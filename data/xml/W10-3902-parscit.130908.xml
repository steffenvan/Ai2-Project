<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000457">
<title confidence="0.9989375">
Exploiting Term Importance Categories and
Dependency Relations for Natural Language Search
</title>
<author confidence="0.982622">
Keiji Shinzato Sadao Kurohashi
</author>
<affiliation confidence="0.9973475">
Graduate School of Informatics, Graduate School of Informatics,
Kyoto University Kyoto University
</affiliation>
<email confidence="0.997783">
shinzato@i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.997367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999605">
In this paper, we propose a method that
clearly separates terms (words and de-
pendency relations) in a natural language
query into important and other terms, and
differently handles the terms according to
their importance. The proposed method
uses three types of term importance: nec-
essary, optional, and unnecessary. The
importance are detected using linguistic
clues. We evaluated the proposed method
using a test collection for Japanese infor-
mation retrieval. Performance was resul-
tantly improved by differently handling
terms according to their importance.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999606">
Currently, search engines that receive a couple of
keywords reflecting users’ information needs pre-
dominate. These keyword-based searches have
been focused on evaluation conferences for infor-
mation retrieval (IR) such as TREC and NTCIR.
Search engines based on keywords, however, have
a crucial problem that it is difficult for their users
to represent complex needs, such as “I want to
know what Steve Jobs said about the iPod.” A
natural language sentence can more adeptly ac-
commodate such information needs than a couple
of keywords because users can straightforwardly
present their needs. We call a query represented
by a sentence a natural language query (NLQ).
The other advantage of NLQs is that search
engines can leverage dependency relations be-
tween words in a given query. Dependency rela-
tions allow search engines to retrieve documents
with a similar linguistic structure to that of the
query. Search performance improvement can be
expected through the use of dependency relations.
For handling an NLQ, we can consider a con-
junctive search (AND search) that retrieves docu-
ments that include all terms in the query, a simple
methodology similar to real-world Web searches.
This methodology, however, often leads to insuf-
ficient amounts of search results. In some in-
stances, no documents match the query. This
problem occurs because the amount of search re-
sults is inversely proportional to the number of
terms used in a search; and an NLQ includes many
terms. Hence, a conjunctive search simply using
all terms in an NLQ is problematic.
Apart from this, we can consider conventional
IR methodology. This approach performs a dis-
junctive search (OR search), and then ranks re-
trieved documents according to scores that are
computed by term weights derived from retrieval
models. The methodology attempts to use term
weights to distinguish important terms and other
items. However, a problem arises in that irrelevant
documents are more highly ranked than relevant
ones when giving NLQs. This is because an NLQ
tends to contain some important terms and many
noisy (redundant) terms and document relevancy
is calculated from the combinations of these term
weights.
Avoiding the above problems, we define three
discrete categories of term importance: necessary;
optional, and unnecessary, and propose a method
that classifies words and dependency relations in
an NLQ into term importance, and then, when per-
forming document retrieval, differently handles
the terms according to their importance. The nec-
essary type includes expressions in Named Enti-
</bodyText>
<page confidence="0.951019">
2
</page>
<note confidence="0.6170865">
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 2–11,
Beijing, August 2010
</note>
<bodyText confidence="0.999853">
ties (NEs) and compound nouns, the optional in-
cludes redundant verbs and the unnecessary in-
cludes expressions that express inquiries such as
“I want to find.” The process of IR consists of two
steps: document collecting and document scor-
ing. The proposed method uses only necessary
terms for document collecting and necessary and
optional terms for document scoring.
We evaluated the proposed method using
the test collections built at the NTCIR-3 and
NTCIR-4 conferences for evaluating Japanese IR.
Search performance was resultantly improved by
differently handling terms (words and dependency
relations) according to their importance.
This paper is organized as follows. Section 2
shows related work, and section 3 describes how
to leverage dependency relations in our retrieval
method. Section 4 presents term importance cate-
gories, and section 5 gives methodology for de-
tecting such categories. Experiment results are
shown in section 6.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999982546875">
A large amount of the IR methodology that has
been proposed (Robertson et al., 1992; Ponte and
Croft, 1998) depends on retrieval models such as
probabilistic and language models. Bendersky
and Croft (Bendersky and Croft, 2008), for in-
stance, proposed a new language model in which
important noun phrases can be considered.
IR methodology based on important term detec-
tion has also been proposed (Callan et al., 1995;
Allan et al., 1997; Liu et al., 2004; Wei et al.,
2007). These previous methods have commonly
focused on noun phrases because the methods as-
sumed that a document relates to a query if the
two have common noun phrases. Liu et al. (Liu et
al., 2004) classified noun phrases into four types:
proper nouns, dictionary phrases (e.g., computer
monitor), simple phrases, and complex phrases,
and detected them from a keyword-based query
by using named entity taggers, part-of-speech pat-
terns, and dictionaries such as WordNet. The
detected phrases were assigned different window
sizes in a proximity operator according to their
types. Wei et al. (Wei et al., 2007) extended Liu’s
work for precisely detecting noun phrases. Their
method used hit counts obtained from Google and
Wikipedia in addition to clues used in Liu’s work.
The differences between the proposed method and
these methods are (i) the proposed method fo-
cuses on an NLQ while the previous methods fo-
cus on a keyword-based query, (ii) the proposed
method needs no dictionaries, and (iii) while the
previous methods retrieve documents by proxim-
ity searches of words in phrases, the proposed
method retrieves them by dependency relations
in phrases. Therefore, the proposed method does
not need to adjust window size, and naturally per-
forms document retrieval based on noun phrases
by using dependency relations.
Linguistically motivated IR research pointed
out that dependency relations did not con-
tribute to significantly improving performance
due to low accuracy and robustness of syntac-
tic parsers (Jones, 1999). Current state-of-the-art
parsers, however, can perform high accuracy for
real-world sentences. Therefore, dependency re-
lations are remarked in IR (N iyao et al., 2006;
Shinzato et al., 2008b). For instance, N iyao et
al. (N iyao et al., 2006) proposed an IR system for
a biomedical domain that performs deep linguis-
tic analysis on a query and each document. Their
system represented relations between words by a
predicate-argument structure, and used ontologi-
cal databases for handling synonyms. Their ex-
periments using a small number of short queries
showed that their proposed system significantly
improved search performance versus a system not
performing deep linguistic analysis. Shinzato
et al. (Shinzato et al., 2008b) proposed a Web
search system that handles not only words but
also dependency relations as terms; yet they did
not discuss the effectiveness of dependency rela-
tions. This paper reveals the effectiveness of de-
pendency relations through experiments using test
collections for Japanese Web searches.
</bodyText>
<sectionHeader confidence="0.953929" genericHeader="method">
3 Exploitation of Dependency Relation
</sectionHeader>
<bodyText confidence="0.999847285714286">
One of the advantages of an NLQ is leveraging
dependency relations between words in the query.
We can expect that search performance improves
because the dependency relations allow systems
to retrieve documents that have similar linguistic
structure to that of the query. Therefore the pro-
posed method exploits dependency relations for
</bodyText>
<page confidence="0.985602">
3
</page>
<figure confidence="0.970474333333333">
✆
spectacular
✝ ☞
✆
active
✕✔
(Michael Jordan
✖
✆
✝
✆
time
</figure>
<figureCaption confidence="0.999897">
Figure 1: Parsing result of an NLQ.
</figureCaption>
<bodyText confidence="0.997697689655172">
retrieving documents. Though a dependency re-
lation is generally a relation between two clauses,
we regard a relation between two content words
as a dependency relation. More precisely, we rep-
resent a dependency relation by a directed binary
relation of content words, and discard the case
marker between content words. Also, (compound)
functional words such as “ʹ͍ͭͯ (about)” and
“ʹैͬͯ (according to)” are attached to the for-
mer content word. Figure 1 shows the parsing re-
sult of the query “෮ؼޙɼΊ͟·͍͠׆༂Λ͠
͍ͯΔϚΠέϧδϣʔμϯͷେֶ࣌୅ͷ׆༂ʹ
͍ͭͯௐ΂͍ͨ.1” The pair of content words (େ
ֶ (university), ࣌୅ (time)) is extracted as a de-
pendency relation from the parsing result. Note
that the pair of content words ( ࣌୅ (time), େ
ֶ (university)) is not extracted as a dependency
relation because a dependency relation is repre-
sented by a directed binary relation.
We used Okapi BM25 (Robertson et al., 1992)
for estimating relevancy between a query and a
document, which is how it is used in most case,
though we slightly extend this measure for esti-
mating relevancy for dependency relations. We
denote a set of words in a query q as Tqword, and
also denote a set of dependency relations in q as
Tqdpnd. The relevancy between query q and docu-
ment d is as follows:
where β is a parameter for adjusting the ratio of a
</bodyText>
<footnote confidence="0.985377666666667">
1This means that Michael Jordan’s performance has been
spectacular since his return to NBA, and I want to learn about
his activities when he was a university student.
</footnote>
<bodyText confidence="0.813036">
score calculated from dependency relations. The
score BM(t, d) is defined as:
</bodyText>
<equation confidence="0.9847492">
BM(t, d w x (k1 + 1)Fdt × (k3 + 1)Fqt
( ) = K + Fdt k3 + Fqt ,
N − n + 0.5
w = log K = k1((1 − b) + b ld )
n + 0.5 ,lave
</equation>
<bodyText confidence="0.99288675">
Here, Fdt is the frequency with which t appears
in document d, Fqt is the frequency that t ap-
pears in q, N is the number of documents being
searched, n is the document frequency of t, ld is
the length of document d (words), and lave is the
average document length. Finally, k1, k3, and b,
are Okapi parameters, for which we use values
k1 = 1, k3 = 0 and b = 0.6.
</bodyText>
<sectionHeader confidence="0.997338" genericHeader="method">
4 Term Importance Category
</sectionHeader>
<bodyText confidence="0.9995845">
Conventional IR methodology regards weights es-
timated by retrieval models, such as probabilistic
and language models, as term importance. The
methods depending on the term weights, however,
cause a problem in that irrelevant documents are
more highly ranked than relevant ones when an
NLQ is given. This is because (i) NLQs tend to
contain some important terms and a large quan-
tity noise (redundant terms) and (ii) document rel-
evancy is estimated by the combinations of these
term weights.
Avoiding this problem, term importance is
clearly separated, instead of representing by
weights. We propose three term-importance cat-
egories and methodology that differently handles
terms according to their importance categories.
These categories are defined as follows:
Necessary: Terms that must be in retrieved doc-
uments. We can also consider a prox-
imity constraint so that all retrieved docu-
ments must contain necessary terms within
N words.
Optional: Terms preferable for inclusion in re-
trieved documents.
Unnecessary: Terms for which it does not matter
if they are included in retrieved documents.
In this paper, terms in necessary, optional and un-
necessary categories are referred to as necessary
terms, optional terms, and unnecessary terms, re-
spectively.
</bodyText>
<figure confidence="0.999114568627451">
✁
✂ ✄
� ✆
re
er
o
turn to
✝s
✧
✦ A
a
✝
✆
university
p
✜
☛
N
✌
☞ ✆ba
about activities
☛
01h:
want to learn
✆
✥
✝
R(q, d) = (1 − β) � BM(t, d) + β � BM(t, d),
t∈Tqword t∈Tqdpnd
ar
✟
✡
☛
✠
✍
✡
☛
Kd
N
✌
✒✓
✝
7 �(fllL�3-3
I
✚
✙
G
G
★
☆
✝
</figure>
<page confidence="0.971767">
4
</page>
<bodyText confidence="0.99993115">
IR methodology consists of two steps: docu-
ment collecting and document scoring. In the pro-
posed method, document collecting is performed
using only necessary terms, document scoring
is performed using both necessary and optional
terms, and neither step uses unnecessary terms.
As mentioned, the proposed method retrieves
documents exploiting not only words but also de-
pendency relations. Though a conjunctive search
with words and dependency relations can be con-
sidered, the proposed method basically only uses
words. In short, words are handled as necessary
terms, while dependency relations are handled as
optional terms. This is because the number of
documents that include all dependency relations
tends to be small. Importance of words and de-
pendency relations is, however, revised depending
on whether they can be regarded as important ex-
pressions. The revision methodology is described
in the next section.
</bodyText>
<sectionHeader confidence="0.959583" genericHeader="method">
5 Revision of Term Importance
</sectionHeader>
<bodyText confidence="0.9992342">
The proposed method basically deals with words
and dependency relations as necessary terms and
optional terms, respectively. However, the term
importance of the following words and depen-
dency relations are revised.
</bodyText>
<listItem confidence="0.971783166666667">
1. Dependency relations in NEs and strongly
connected compound nouns.
2. Redundant verbs, verbs whose meaning can
be inferred from surrounding nouns.
3. Words and dependency relations in inquiry
expressions and functional expressions.
</listItem>
<bodyText confidence="0.997115333333333">
This section describes how to recognize the above
expressions and revise the term importance of the
recognized expressions.
</bodyText>
<subsectionHeader confidence="0.995574">
5.1 Named Entity and Strongly Connected
Compound Noun
</subsectionHeader>
<bodyText confidence="0.999928954545454">
The term importance of all dependency relations
in Named Entities (NEs) is revised to a necessary
category. We believe that a user entering a search
engine query including an NE expects to obtain
documents that include the NE. For instance, if a
user’s query includes “American Bank,” the user
prefers documents that include “American Bank”
to those with the individual words “American”
and “Bank.” That is why the proposed method re-
vises the term importance of all dependency re-
lations in an NE to a necessary category. This
revision guarantees that search engine users will
obtain documents including the NEs in a query.
In addition to NEs, for some compound nouns
a search engine user prefers to obtain documents
that include the compound noun rather than the in-
dividual words in the compound noun. We refer to
this as a Strongly Connected Compound Noun
(SCCN). An example of an SCCN is “information
science.” In the same way as “American Bank,” a
user whose search engine query contains “infor-
mation science” expects to obtain documents that
include “information science” rather than with the
individual words “information” and “science.”
On the other hand, there are also compound
nouns, such as “Kyoto sightseeing”, that do not
need to be included in retrieved documents as
a single phrase. For these, a user approves
of retrieved documents that include “Kyoto” and
“sightseeing” separately. We therefore need crite-
ria for distinguishing such compound nouns and
SCCNs.
The problem is how to compute the connec-
tion strength of words in a compound noun N
(i.e., w1, ..., w|N|). For computing the connec-
tion strength among words in N, we assumed that
words in an SCCN are unlikely to occur in docu-
ments as “wi ͷ wi+1 (wi+1 of wi)”. This assump-
tion reflects the observation that “Kyoto sightsee-
ing” is likely to be expressed as “sightseeing of
Kyoto” and that “information science” is unlikely
to be expressed by “science of information.” In
line with this assumption, the connection strength
is calculated as follows:
</bodyText>
<equation confidence="0.995458">
|N|−1
1 DF(wi wi+1)
5corestrength(N) = |N |− 1 E DF(wi+1ͷ wi)
</equation>
<bodyText confidence="0.997324285714286">
Here, DF(X) is the document frequency of X
computed from hundreds of millions Japanese
Web pages (Shinzato et al., 2008a). The proposed
method regards a compound noun N as an SCCN
if the value of 5corestrength(N) exceeds a thresh-
old Tp. We used the value of 300 as the thresh-
old. In addition to dependency relations in NEs,
</bodyText>
<page confidence="0.922503">
5
</page>
<bodyText confidence="0.581454666666667">
the term importance of dependency relations in an
SCCN is also revised from an optional category to
a necessary category.
</bodyText>
<figure confidence="0.934595">
a
w
ua
</figure>
<subsectionHeader confidence="0.986079">
5.2 Redundant Verb
</subsectionHeader>
<bodyText confidence="0.971424">
The proposed method deals with a verb whose
meaning is inferable from the surrounding nouns
as an optional term. We refer to such a verb a re-
dundant verb.
Consider the following two expressions:
</bodyText>
<figure confidence="0.90553425">
(A) ࡞Ո (author) ͷ (of) ॻ͍ͨ (wrote) ຊ (book)
(A book written by an author)
(B) ࡞Ո (author) ͷ (of) ຊ (book)
(A book of an author)
</figure>
<bodyText confidence="0.999114411764706">
The expression (A) is often paraphrased as the ex-
pression (B) which omits the verb “write.” How-
ever, we can recognize that (A) is equivalent to
(B). This is because the meaning of the verb
“write” can be inferred from the noun “author.” In
other words, the noun “author” can be considered
to imply the meaning of the verb “write.” Accord-
ing to this observation, we assumed that a verb
whose meaning is inferable from the surrounding
nouns does not need to be included in retrieved
documents.
For computing redundancy of verbs, we made
the assumption that a noun n implies the meaning
of a verb v if a syntactic dependency relation be-
tween a noun n and a verb v frequently occurs in
corpora. We defined the following score function
according to the assumption.
</bodyText>
<equation confidence="0.9946095">
P (n,Scorecooc(n, v) = P(n, v) · log2 P(n) v)
(n)
</equation>
<bodyText confidence="0.965632153846154">
(v),
where P(n) and P(v) indicate the probabilities
of a noun n and a verb v respectively. P(n, v) is
the probability of a dependency relation between
a noun n and a verb v. These probabilities were
estimated from 1.6 billion Japanese sentences ex-
tracted from the hundreds of millions of Japanese
pages used for computing DF(X) in the previous
section.
For each noun n that is the parent-of or child-of
dependency relation of a verb v, the above score
is calculated. We consider that the meaning of a
verb v can be inferred from a noun n if the value
</bodyText>
<figure confidence="0.703746">
Dependency relation
Added dependency relation
</figure>
<figureCaption confidence="0.930737333333333">
Figure 2: Structural difference between “࡞Ոͷ
ॻ͍ͨຊ (a book written by an author)” and “࡞
Ոͷຊ (a book of an author)”.
</figureCaption>
<bodyText confidence="0.999759428571429">
of Scorecooc(n, v) exceeds a threshold T,,. The
value of the threshold is used 1 × 10−6 which was
decided empirically. For instance, the nouns au-
thor and book in Figure 2 (a) are used for comput-
ing the above score with respect to the verb wrote,
and then wrote is regarded as a redundant verb if
either one exceeds the threshold.
When a verb v is regarded as an optional term
(i.e., v is a redundant verb), the proposed method
appends a new dependency relation consisting of
the parent-of and child-of dependency relation of
the redundant verb v. Figure 2 (a) shows the pars-
ing result of the expression (A). A new depen-
dency relation between “author” and “book” is
depicted by a dashed arrow. Figure 2 (b) shows
the parsing result of the expression (B). Though
there is a structural gap between the expressions
(A) and (B), this gap is bridged by the new de-
pendency relation because the dependency rela-
tion (author, book) is contained in the both ex-
pressions.
</bodyText>
<subsectionHeader confidence="0.9135845">
5.3 Inquiry Expressions and Functional
Words
</subsectionHeader>
<bodyText confidence="0.999733363636363">
An NLQ tends to contain expressions, such as “I
want to find” and “I want to know,” and such ex-
pressions almost never relate to users’ informa-
tion needs. Therefore we regard words and de-
pendency relations in these expressions as unnec-
essary terms. To do so, we crafted the inquiry
pattern shown in Figure 3. The importance of
words and dependency relations in the matched
expressions is revised to an unnecessary category
if expressions in a query matched the pattern. The
spelling variations of words, such as “୳͢ (find)”
</bodyText>
<figure confidence="0.894569813559322">
✝ ✁
wrote
✞
✟
✂
The meaning is inferable
from ̏author’’
boo
boo
✍
✎
(a)
(a book written by an author)
✠✡
✍✎
✡
☛☞✌
☞
✠
☛
✌
✆
☎
✂
author
✄✁
lQ ✁
book
✂
✎
(a book of an author)
☛
✡
(b)✠✡☛✎
✠
✆
☎
✂
author
✄ ✁
✂
✁
book
�
6
INQUIRY PATTERN:
&lt;EPITHET&gt;?&lt;EXPOSITION&gt;? &lt;DOC&gt;?(iL-)
L&apos;1-C (about))?&lt;PREDICATE&gt;;
&lt;EPITHET&gt;: [ 3*LL&apos;1 (in detail)  |3* WWf;� (in detail) ];
&lt;EXPOSITION&gt;: [ RM (explain)|f&lt; (write) |
r,HLIN (describe)  |�rH Pc (mention)  |�rH1- (write
down) |LIN­Zra (express)][�ra (do)]? [(L&apos;1ra (be)
 |bra (be) |tbra (reru)|i�)tbra (rareru)]?;
&lt;DOC&gt;: [r7=7 (Web)|WEB (Web)]? [_ZR (docu-
ment) |(page)|HP (homepage) |&apos;h* (in-
formation)|_Z1 (sentences)|1-zk-A� (text)];
&lt;PREDICATE&gt;: [0ra (know)|91- (look for)|
�­Zra (find)|J,ra (watch)|A-)4tra (find out)|
wt1 (read)][ftL&apos;1 (tai)|L&apos;1ra (iru)];
</figure>
<figureCaption confidence="0.950347">
Figure 3: Inquiry patterns. The notation [A|B] in-
dicates Aor B and the symbol ‘?’ indicates that an
</figureCaption>
<bodyText confidence="0.950833727272727">
expression in front of the symbol may be omitted.
The words reru, rareru, tai and iru are Japanese
functional words.
and “tz�at (find)” are properly handled when
matching an inquiry pattern.
In addition to the inquiry expressions, we can
consider that content words that play a role like
functional words, such as bra (be), 4�ra (be-
come), and 6�5 (use), are unnecessary for retriev-
ing documents. To detect these words we con-
structed an unnecessary content word list.
</bodyText>
<sectionHeader confidence="0.999417" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.98948">
6.1 Settings
</subsectionHeader>
<bodyText confidence="0.992439565217391">
We evaluated the proposed method by using the
test collections built at the NTCIR-3 (Eguchi et
al., 2003) and NTCIR-4 (Eguchi et al., 2004)
conferences. These share a target document
set, which consists of 11,038,720 Japanese Web
pages. For the evaluation, we used 127 infor-
mational topics defined in the test collections (47
from NTCIR-3 and 80 from NTCIR-4). An exam-
ple of the informational topic definition is shown
in Figure 4. &lt;DESC&gt; includes a sentence reflect-
ing the user’s information needs; the sentence can
be regarded as an NLQ. Therefore, we used only
&lt;DESC&gt; as a query in the experiments. The rel-
evance of each document with respect to a topic
was judged as highly relevant, relevant, partially
relevant, irrelevant or unjudged. We regarded
the highly relevant, relevant, and partially relevant
documents as correct answers.
The process of IR consists of two steps: doc-
&lt;TOPIC&gt;&lt;NUM&gt; 0008 &lt;/NUM&gt;&lt;TITLE&gt; Salsa,
learn, methods &lt;/TITLE&gt;&lt;DESC&gt; I want to
find out about methods for learning how
to dance the salsa &lt;/DESC&gt; .. &lt;/TOPIC&gt;
</bodyText>
<figureCaption confidence="0.994499">
Figure 4: Example of a search topic.
</figureCaption>
<bodyText confidence="0.999956032258065">
ument collecting and document scoring. In both
steps, the proposed method considered synonyms
automatically extracted from ordinary dictionaries
and Web pages (Shibata et al., 2008). For calcu-
lating the scores, we selected the value of 0.2 as
the parameter β. This value was estimated using
the dry-run data set of NTCIR-3.
For each topic, we retrieved 1,000 docu-
ments and then assessed search performance
according to MRR, P@10, R-prec, MAP,
DCGN (Jarvelin and Kekalainen, 2002), and Q-
Measure (QM) (Sakai, 2004). We calculated these
scores for each topic then averaged them. Note
that unjudged documents were treated as irrele-
vant when computing the scores. As the graded
relevance for DCGN and QM, we mapped highly
relevant, relevant and partially relevant to 3, 2 and
1, respectively.
The proposed method often leads to an insuffi-
cient number of search results because the method
performs a conjunctive search using necessary
terms. Therefore, evaluation measures, such as
QM, which utilize low-ranked search results for
computing their scores, give low scores in the pro-
posed method. To avoid this problem we combine
the proposed method with an OR (dpnd) search,
which is described in the next section. More pre-
cisely, let R(d) denote the rank given by the pro-
posed method for a document d, and ROR(d) de-
note the rank given by the OR(dpnd) search. The
final score for a document d is defined as:
</bodyText>
<equation confidence="0.9972345">
1 1
S(d) = R(d) + ROR(d)
</equation>
<bodyText confidence="0.999176666666667">
The documents collected by the proposed method
and the OR(dpnd) search are sorted according to
values of S(d), and then the top 1,000 of the
sorted documents are regarded as the search re-
sult of the proposed method. Note that the search
result of the OR(dpnd) search is dealt with fusing
the proposed method when the number of search
results of the proposed method is zero.
All NLQs extracted from &lt;DESC&gt; were an-
</bodyText>
<page confidence="0.999823">
7
</page>
<tableCaption confidence="0.999974">
Table 1: Comparison between the proposed method and alternative methods.
</tableCaption>
<table confidence="0.959749105263158">
Methods AND OR OR (dpnd) Proposed method
ANDprox+
OR (dpnd)
Prox. &amp; Prox. Word Prox. Word Prox. Word Terms Prox. Word Dpnd. Prox. Word Dpnd.
Dpnd.
Normal RV Normal NEs &amp;
SCCNs
Search No Q No D No 4 0 Yes Q 0 Yes Q 0 0 Q
conditions
No 0 0 No 0 0 0 0
MRR 0.533 0.538 0.503 0.547 0.537
P@10 0.328 0.337 0.352 0.352 0.357
DCG10 3.469 3.497 3.583 3.634 3.713
DCG100 7.191 8.898 9.167 9.045 9.280
DCG1000 8.956 16.221 16.553 16.678 16.866
R-prec 0.174 0.207 0.212 0.217 0.221
MAP 0.120 0.151 0.158 0.161 0.164
QM 0.095 0.168 0.175 0.179 0.183
Prox: Proximity, Dpnd: Dependency relation, RV: Redundant verb.
</table>
<bodyText confidence="0.999542818181818">
alyzed by the JUMAN2, Japanese morphologi-
cal analyzer and KNP3, Japanese syntactic parser
which implemented the named entity recog-
nition feature proposed by Sasano and Kuro-
hashi (Sasano and Kurohashi, 2008). All doc-
uments were also analyzed by JUMAN and
KNP, and then words and dependency rela-
tions in the documents were indexed as index
terms. For instance, the dependency relation
(university, time) shown in Figure 1 is in-
dexed as university ˠ time.
</bodyText>
<subsectionHeader confidence="0.999964">
6.2 Comparison with Alternative Searches
</subsectionHeader>
<bodyText confidence="0.9967725">
We first investigated the effectiveness of clear
boundaries of term importance and differently
handling of terms according to their importance.
We compared the proposed method with the fol-
lowing alternative search methods (see Table 1):
AND: Conjunctive search only using words. We
do nothing even if the number of retrieved doc-
uments is less than 1,000. Retrieved documents
are ranked according to Okapi BM25 scores. This
is the same equation when the parameter 0 is re-
garded as zero in R(q, d). The Prox. column in
Table 1 indicates whether a proximity operator
is imposed. The symbol Q in the Word column
means that words in a query are handled as neces-
sary terms.
OR: Disjunctive search only using words. Re-
trieved documents are ranked according to
Okapi BM25 scores. The symbol A in the Word
column means that words in a query are handled
as optional terms.
</bodyText>
<footnote confidence="0.999044">
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
</footnote>
<bodyText confidence="0.9897982">
OR (dpnd): Disjunctive search using both words
and dependency relations. Retrieved documents
are ranked according to scores of R(q, d). We
used the value of 0.2 as the parameter 0.
ANDAro,,+OR(dpnd): In the same way as the
proposed method, this search consists of conjunc-
tive search and OR search. The conjunctive search
uses only words with a proximity operator. Re-
trieved documents must contain words in a search
query within 75 words (regardless of order). The
parameter value was decided by the results of pilot
studies. Retrieved documents are ranked accord-
ing to Okapi BM25 scores. These scores are cal-
culated by both words and dependency relations.
On the other hand, the OR(dpnd) search described
above is used as an OR search. Let Rprox(d) de-
note the rank given by the conjunctive search, and
ROR(d) denote the rank given by the OR(dpnd)
search, and the final score for a document d is de-
fined as:
</bodyText>
<equation confidence="0.9972365">
1 1
S(d) = Rprox(d) + ROR(d).
</equation>
<bodyText confidence="0.999958307692308">
The documents collected by the conjunctive and
OR(dpnd) searches are sorted according to the
above values, then the top 1,000 documents are
regarded as the search result of this search.
In the above methods, the unnecessary expres-
sions described in Section 5.3 are not used.
The proposed method exploits dependency re-
lations in NEs and SCCNs as necessary terms, and
the other dependency relations are handled as op-
tional terms. Redundant verbs are handled as op-
tional terms and the others are necessary terms.
The proposed method imposes the same proxim-
ity operator as the ANDAro,,+OR (dpnd) search.
</bodyText>
<page confidence="0.999376">
8
</page>
<tableCaption confidence="0.999917">
Table 2: Comparison with systems in NTCIR3
</tableCaption>
<table confidence="0.9992435">
(a) For MRR and P@10. (b) For R-prec and MAP.
System MRR P@10 System R-prec MAP
GRACE 0.502 0.330 GRACE 0.230 0.208
UAIFI5 0.383 0.289 OKSAT 0.156 0.190
NAICR 0.468 0.249 NAICR 0.115 0.180
Ours 0.431 0.313 Ours 0.208 0.156
</table>
<tableCaption confidence="0.999769">
Table 3: Comparison with systems in NTCIR4.
</tableCaption>
<table confidence="0.9999236">
System MRR P@10 R-prec MAP
GRACE 0.645 0.501 0.278 0.216
DBLAB 0.613 0.435 0.254 0.212
SSTUT 0.562 0.370 0.189 0.132
Ours 0.600 0.383 0.229 0.169
</table>
<tableCaption confidence="0.998569">
Table 1 shows performance of the proposed
</tableCaption>
<bodyText confidence="0.999795111111111">
method and alternative methods. We can
see that the proposed method outperforms not
only AND and OR searches which are sim-
ple and conventional methodology but also the
ANDD,,,+OR(dpnd) search. A small number of
documents is returned by the AND search since
the documents must include all necessary terms in
a query. Because of this, the AND search indi-
cates the worst performance in almost all evalua-
tion measures. Though the proposed method also
retrieves documents that must include all neces-
sary terms in a query, the method achieves high
performance because of its combination with the
OR(dpnd) search.
From the difference between the OR and
OR (dpnd) searches, we can see that dependency
relations improve the performance of the OR
search.
</bodyText>
<subsectionHeader confidence="0.999699">
6.3 Comparison with Systems in NTCIR
</subsectionHeader>
<bodyText confidence="0.999022576923077">
Next we compared the search performance of the
proposed method and that of systems participated
in NTCIR 3 and NTCIR 4. In NTCIR 3, the mea-
sures MRR and P@10 and measures MAP and R-
prec were used in different tasks. Therefore we
selected the top three systems for each evaluation
measure. In NTCIR 4, we selected the top three
systems according to MAP.
Tables 2 and 3 show the comparison results
for NTCIR3 and 4. Note that although GRACE,
DBLAB and SSTUT in the tables used pseudo-
relevance feedback, the proposed method did
not. Tables 2 (a) and (b) show that the pro-
posed method achieves the close performance of
GRACE, the best system in NTCIR 3, in terms of
P@10 and R-prec.
On the other hand, Table 3 shows that the pro-
posed method outperforms SSTUT, the third sys-
tem in NTCIR 4. The difference between the
performance of the proposed method and that of
GRACE and DBLAB is derived from pseudo-
relevance feedback. We expect that the proposed
method achieves similar performance to GRACE
and DBLAB if it utilizes pseudo-relevance feed-
back. Usage of of pseudo-relevance feedback is
our future work.
</bodyText>
<subsectionHeader confidence="0.9991135">
6.4 Effectiveness of Dependency Relation in
Document Scoring
</subsectionHeader>
<bodyText confidence="0.999996846153846">
We investigated the optimized value of the param-
eter 0 used to regulate the extent to which depen-
dency relations are used in the document scoring.
For estimating the value, we investigated the per-
formance when changing the value of 0 from 0.0
to 0.9 at increments of 0.1.
The performance is shown in Table 4. The
“0.0” row means that document scoring is per-
formed without using dependency relations. We
can see that dependency relations contribute to
improved search performance. In particular, max-
imum values of most evaluation measure are indi-
cated when the value of 0 is 0.2.
</bodyText>
<subsectionHeader confidence="0.975295">
6.5 Influence of Redundant Verb
</subsectionHeader>
<bodyText confidence="0.999954842105263">
Next we classified all verbs in queries into re-
dundant verbs and other verbs, then examined the
search performance when changing their term im-
portance. The result is shown in Table 5. The
proposed method deals with redundant verbs as
optional terms, and the others as necessary terms
(Normal: Q, Redundant: A in the table). The
proposed method outperforms methods that han-
dle all verbs as necessary terms (Normal: Q, Re-
dundant: Q).
An example of a query that includes a redun-
dant verb and contributes to improved search per-
formance is “I want to find shops that make bread
with natural yeast.” In this query, the proposed
method found a document that describes “... is a
well-known bakery. Bread with natural yeast is a
popular item.” Though this document did not in-
clude the verb “make,” we were able to find it be-
cause the redundant verb detection procedure de-
</bodyText>
<page confidence="0.998084">
9
</page>
<tableCaption confidence="0.99979">
Table 4: Changes in search performance, when varying the parameter β in document scoring.
</tableCaption>
<table confidence="0.99858">
β MRR P@10 DCG10 DCG100 DCG1000 R-prec MAP QM
0.0 0.548 0.341 3.528 9.108 17.209 0.208 0.151 0.170
0.1 0.529 0.350 3.619 9.265 17.454 0.214 0.155 0.173
0.2 0.537 0.357 3.713 9.280 16.866 0.221 0.164 0.183
0.3 0.497 0.338 3.446 9.174 17.418 0.209 0.152 0.171
0.4 0.507 0.339 3.335 8.791 17.038 0.199 0.145 0.164
0.5 0.486 0.320 3.150 8.307 16.482 0.191 0.136 0.154
0.6 0.467 0.303 2.988 7.793 15.645 0.174 0.126 0.143
0.7 0.458 0.292 2.873 7.384 14.777 0.166 0.118 0.133
0.8 0.456 0.278 2.790 7.059 14.216 0.157 0.110 0.124
0.9 0.447 0.263 2.646 6.681 13.569 0.148 0.104 0.117
</table>
<bodyText confidence="0.99765752173913">
scribed in Section 5.2 judged that the meaning of
“make” is inferable from “bread.”
The highest performance, however, was
achieved when regarding all verbs as optional
terms (Normal: A, Redundant: A). In this
setting, the example of a query that contributes
to improved search performance is “I want to
find out how the heliocentric theory of Coper-
nicus was accepted by Christian society.” The
redundant verb detection procedure judged that
the meaning of “accept” is not inferable from
“society.” Consequently, the verb “accept” is han-
dled as a necessary term. Though this judgement
is correct, the handling of verbs as necessary
terms means that the possibility of the same event
being expressed by different expressions such as
synonyms is discarded. In general, a verb has
multiple synonyms, and multiple expressions
can be considered for describing the identical
event. The handling of verbs as necessary terms
can thereby be a cause of decreased search
performance. We cope with the side effect of
verbs by expanding synonym databases.
</bodyText>
<subsectionHeader confidence="0.986769">
6.6 Influence of Dependency Relation Usage
</subsectionHeader>
<bodyText confidence="0.999910714285714">
Finally we investigated search performance when
changing importance of dependency relations.
Table 6 shows that scores of all evaluation mea-
sures are close to each other when we simply
used all dependency relations as necessary, op-
tional or unnecessary terms. On the other hand,
the proposed method handles dependency rela-
tions in NEs and SCCNs as necessary terms, and
handles the other dependency relations as optional
terms. This setting achieves relatively higher per-
formance than the other settings. This means that
the different handling of dependency relations ac-
cording to their categories improves search perfor-
mance.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999955176470588">
In this paper, we defined three term importance
categories: necessary; optional and unnecessary,
and proposed a method that classifies terms in
an NLQ into a category. The term importance
is detected by word co-occurrence frequencies
estimated from large-scale Web documents and
NE recognition. The proposed method also han-
dles dependency relations in a query as terms for
achieving high performance.
We evaluated the proposed method using
the NTCIR-3 and NTCIR-4 test collections for
Japanese information retrieval. The search per-
formance resultantly improved by regarding terms
(words and dependency relations) in the named
entities and compound nouns as necessary terms.
Moreover, the performance was partially im-
proved by regarding redundant verbs as optional.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999766111111111">
Allan, James, Jamie Callan, W. Bruce Croft, Lisa
Ballesteros, John Broglio, Jinxi Xu, and Hongmin
Shu. 1997. Inquery at trec-5. In NIST, pages 119–
132.
Bendersky, Michael and W. Bruce Croft. 2008. Dis-
covering key concepts in verbose queries query. In
Proceedings of the 31st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval 2008, pages 491–498.
Callan, James P., W. Bruce Croft, and John Broglio.
1995. Trec and tipster experiments with inquery.
Inf. Process. Manage., 31(3):327–343.
Eguchi, Koji, Keizo Oyama, Emi Ishida, Noriko
Kando, and Kazuko Kuriyama. 2003. The web re-
trieval task and its evaluation in the third ntcir work-
shop. In Proceedings of the 25th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval.
</reference>
<page confidence="0.999322">
10
</page>
<tableCaption confidence="0.999732">
Table 5: Changes in search performance, when varying term importance of verbs.
</tableCaption>
<table confidence="0.999862">
Verbs MRR P@10 DCG10 DCG100 DCG1000 R-prec MAP QM
Normal Redundant
Q Q 0.525 0.352 3.640 9.110 16.734 0.217 0.161 0.180
Q A 0.537 0.357 3.713 9.280 16.866 0.221 0.164 0.183
O × 0.534 0.354 3.664 9.273 16.832 0.221 0.164 0.183
A A 0.537 0.360 3.755 9.404 17.053 0.221 0.165 0.184
A × 0.534 0.357 3.709 9.399 17.019 0.221 0.165 0.184
× × 0.533 0.356 3.703 9.401 17.018 0.221 0.165 0.184
</table>
<tableCaption confidence="0.993654">
Table 6: Changes in search performance, when varying the importance of dependency relations.
</tableCaption>
<table confidence="0.999293222222222">
Dependency relations MRR P@10 DCG10 DCG100 DCG1000 R-prec MAP QM
Outside of Inside of
NEs &amp; SCCNs NEs &amp; SCCNs
Q Q 0.513 0.338 3.474 8.987 16.650 0.211 0.155 0.174
A O 0.537 0.357 3.713 9.280 16.866 0.221 0.164 0.183
× O 0.561 0.349 3.642 9.072 16.547 0.213 0.159 0.177
A A 0.552 0.347 3.647 9.073 16.565 0.215 0.159 0.177
× A 0.539 0.359 3.725 9.223 16.827 0.221 0.164 0.182
× × 0.561 0.344 3.655 9.059 16.545 0.214 0.159 0.177
</table>
<reference confidence="0.999487362318841">
Eguchi, Koji, Keizo Oyama, Akiko Aizawa, and
Haruko Ishikawa. 2004. Overview of web task
at the fourth ntcir workshop. In Proceedings of
the Fourth NTCIR Workshop on Research in Infor-
mation Access Technologies Information Retrieval,
Question Answering and Summarization.
Jarvelin, Kalervo and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of ir techniques. ACM
Transactions on Information Systems, 20:422–446.
Jones, Karen Sparck. 1999. What is the role of nlp in
text retrieval? In Strzalkowski, T., editor, Natural
language information retrieval, pages 1–24. Kluwer
Academic Publishers.
Liu, Shuang, Fang Liu, Clement Yu, and Weiyi Meng.
2004. An effective approach to document retrieval
via utilizing wordnet and recognizing phrases. In
Proceedings of the 27th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 266–272.
Miyao, Yusuke, Tomoko Ohta, Katsuya Masuda,
Yoshimasa Tsuruoka, Kazuhiro Yoshida, Takashi
Ninomiya, and Jun’ichi Tsujii. 2006. Seman-
tic retrieval for the accurate identification of rela-
tional concepts in massive textbases. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the ACL, pages 1017–1024.
Ponte, Jay M. and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 275–281.
Robertson, Stephen E., Steve Walker, Micheline
Hancock-Beaulieu, Aarron Gull, and Marianna Lau.
1992. Okapi at TREC. In Text REtrieval Confer-
ence, pages 21–30.
Sakai, Tetsuya. 2004. New performance metrics based
on multigrade relevance: Their application to ques-
tion answering. In Proceedings of the Fourth NT-
CIR Workshop Meeting.
Sasano, Ryohei and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural
language processing. In Proceedings of Third In-
ternational Joint Conference on Natural Language
Processing, pages 607–612.
Shibata, Tomohide, Michitaka Odani, Jun Harashima,
Takashi Oonishi, and Sadao Kurohashi. 2008.
SYNGRAPH: A flexible matching method based
on synonymous expression extraction from an ordi-
nary dictionary and a web corpus. In Proc. of IJC-
NLP2008, pages 787–792.
Shinzato, Keiji, Daisuke Kawahara, Chikara
Hashimoto, and Sadao Kurohashi. 2008a. A
large-scale web data collection as a natural lan-
guage processing infrastructure. In Proceedings
of the 6th International Conference on Language
Resources and Evaluation (LREC08).
Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-
hara, Chikara Hashimoto, and Sadao Kurohashi.
2008b. TSUBAKI: An open search engine in-
frastructure for developing new information access
methodology. In Proc. of IJCNLP2008, pages 189–
196.
Wei, Zhang, Liu Shuang, Yu Clement, Sun Chaojing,
Liu Fang, and Meng Weiyi. 2007. Recognition and
classification of noun phrases in queries for effective
retrieval. In Proceedings of the sixteenth ACM con-
ference on Conference on information and knowl-
edge management, pages 711–720.
</reference>
<page confidence="0.999481">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.581116">
<title confidence="0.986761">Exploiting Term Importance Categories Dependency Relations for Natural Language Search</title>
<author confidence="0.975056">Keiji Shinzato Sadao Kurohashi</author>
<affiliation confidence="0.9993425">Graduate School of Informatics, Graduate School of Informatics, Kyoto University Kyoto University</affiliation>
<email confidence="0.612776">shinzato@i.kyoto-u.ac.jpkuro@i.kyoto-u.ac.jp</email>
<abstract confidence="0.999381866666667">In this paper, we propose a method that clearly separates terms (words and dependency relations) in a natural language query into important and other terms, and differently handles the terms according to their importance. The proposed method uses three types of term importance: necessary, optional, and unnecessary. The importance are detected using linguistic clues. We evaluated the proposed method using a test collection for Japanese information retrieval. Performance was resultantly improved by differently handling terms according to their importance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jamie Callan</author>
<author>W Bruce Croft</author>
<author>Lisa Ballesteros</author>
<author>John Broglio</author>
<author>Jinxi Xu</author>
<author>Hongmin Shu</author>
</authors>
<title>Inquery at trec-5. In</title>
<date>1997</date>
<booktitle>NIST,</booktitle>
<pages>119--132</pages>
<contexts>
<context position="4932" citStr="Allan et al., 1997" startWordPosition="753" endWordPosition="756">rieval method. Section 4 presents term importance categories, and section 5 gives methodology for detecting such categories. Experiment results are shown in section 6. 2 Related Work A large amount of the IR methodology that has been proposed (Robertson et al., 1992; Ponte and Croft, 1998) depends on retrieval models such as probabilistic and language models. Bendersky and Croft (Bendersky and Croft, 2008), for instance, proposed a new language model in which important noun phrases can be considered. IR methodology based on important term detection has also been proposed (Callan et al., 1995; Allan et al., 1997; Liu et al., 2004; Wei et al., 2007). These previous methods have commonly focused on noun phrases because the methods assumed that a document relates to a query if the two have common noun phrases. Liu et al. (Liu et al., 2004) classified noun phrases into four types: proper nouns, dictionary phrases (e.g., computer monitor), simple phrases, and complex phrases, and detected them from a keyword-based query by using named entity taggers, part-of-speech patterns, and dictionaries such as WordNet. The detected phrases were assigned different window sizes in a proximity operator according to the</context>
</contexts>
<marker>Allan, Callan, Croft, Ballesteros, Broglio, Xu, Shu, 1997</marker>
<rawString>Allan, James, Jamie Callan, W. Bruce Croft, Lisa Ballesteros, John Broglio, Jinxi Xu, and Hongmin Shu. 1997. Inquery at trec-5. In NIST, pages 119– 132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bendersky</author>
<author>W Bruce Croft</author>
</authors>
<title>Discovering key concepts in verbose queries query.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</booktitle>
<pages>491--498</pages>
<contexts>
<context position="4723" citStr="Bendersky and Croft, 2008" startWordPosition="717" endWordPosition="720"> handling terms (words and dependency relations) according to their importance. This paper is organized as follows. Section 2 shows related work, and section 3 describes how to leverage dependency relations in our retrieval method. Section 4 presents term importance categories, and section 5 gives methodology for detecting such categories. Experiment results are shown in section 6. 2 Related Work A large amount of the IR methodology that has been proposed (Robertson et al., 1992; Ponte and Croft, 1998) depends on retrieval models such as probabilistic and language models. Bendersky and Croft (Bendersky and Croft, 2008), for instance, proposed a new language model in which important noun phrases can be considered. IR methodology based on important term detection has also been proposed (Callan et al., 1995; Allan et al., 1997; Liu et al., 2004; Wei et al., 2007). These previous methods have commonly focused on noun phrases because the methods assumed that a document relates to a query if the two have common noun phrases. Liu et al. (Liu et al., 2004) classified noun phrases into four types: proper nouns, dictionary phrases (e.g., computer monitor), simple phrases, and complex phrases, and detected them from a</context>
</contexts>
<marker>Bendersky, Croft, 2008</marker>
<rawString>Bendersky, Michael and W. Bruce Croft. 2008. Discovering key concepts in verbose queries query. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval 2008, pages 491–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James P Callan</author>
<author>W Bruce Croft</author>
<author>John Broglio</author>
</authors>
<title>Trec and tipster experiments with inquery.</title>
<date>1995</date>
<journal>Inf. Process. Manage.,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="4912" citStr="Callan et al., 1995" startWordPosition="749" endWordPosition="752"> relations in our retrieval method. Section 4 presents term importance categories, and section 5 gives methodology for detecting such categories. Experiment results are shown in section 6. 2 Related Work A large amount of the IR methodology that has been proposed (Robertson et al., 1992; Ponte and Croft, 1998) depends on retrieval models such as probabilistic and language models. Bendersky and Croft (Bendersky and Croft, 2008), for instance, proposed a new language model in which important noun phrases can be considered. IR methodology based on important term detection has also been proposed (Callan et al., 1995; Allan et al., 1997; Liu et al., 2004; Wei et al., 2007). These previous methods have commonly focused on noun phrases because the methods assumed that a document relates to a query if the two have common noun phrases. Liu et al. (Liu et al., 2004) classified noun phrases into four types: proper nouns, dictionary phrases (e.g., computer monitor), simple phrases, and complex phrases, and detected them from a keyword-based query by using named entity taggers, part-of-speech patterns, and dictionaries such as WordNet. The detected phrases were assigned different window sizes in a proximity opera</context>
</contexts>
<marker>Callan, Croft, Broglio, 1995</marker>
<rawString>Callan, James P., W. Bruce Croft, and John Broglio. 1995. Trec and tipster experiments with inquery. Inf. Process. Manage., 31(3):327–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Eguchi</author>
<author>Keizo Oyama</author>
<author>Emi Ishida</author>
<author>Noriko Kando</author>
<author>Kazuko Kuriyama</author>
</authors>
<title>The web retrieval task and its evaluation in the third ntcir workshop.</title>
<date>2003</date>
<booktitle>In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="20508" citStr="Eguchi et al., 2003" startWordPosition="3401" endWordPosition="3404">?’ indicates that an expression in front of the symbol may be omitted. The words reru, rareru, tai and iru are Japanese functional words. and “tz�at (find)” are properly handled when matching an inquiry pattern. In addition to the inquiry expressions, we can consider that content words that play a role like functional words, such as bra (be), 4�ra (become), and 6�5 (use), are unnecessary for retrieving documents. To detect these words we constructed an unnecessary content word list. 6 Experiments 6.1 Settings We evaluated the proposed method by using the test collections built at the NTCIR-3 (Eguchi et al., 2003) and NTCIR-4 (Eguchi et al., 2004) conferences. These share a target document set, which consists of 11,038,720 Japanese Web pages. For the evaluation, we used 127 informational topics defined in the test collections (47 from NTCIR-3 and 80 from NTCIR-4). An example of the informational topic definition is shown in Figure 4. &lt;DESC&gt; includes a sentence reflecting the user’s information needs; the sentence can be regarded as an NLQ. Therefore, we used only &lt;DESC&gt; as a query in the experiments. The relevance of each document with respect to a topic was judged as highly relevant, relevant, partial</context>
</contexts>
<marker>Eguchi, Oyama, Ishida, Kando, Kuriyama, 2003</marker>
<rawString>Eguchi, Koji, Keizo Oyama, Emi Ishida, Noriko Kando, and Kazuko Kuriyama. 2003. The web retrieval task and its evaluation in the third ntcir workshop. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Eguchi</author>
<author>Keizo Oyama</author>
<author>Akiko Aizawa</author>
<author>Haruko Ishikawa</author>
</authors>
<title>Overview of web task at the fourth ntcir workshop.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth NTCIR Workshop on Research in Information Access Technologies Information Retrieval, Question Answering and Summarization.</booktitle>
<contexts>
<context position="20542" citStr="Eguchi et al., 2004" startWordPosition="3407" endWordPosition="3410"> front of the symbol may be omitted. The words reru, rareru, tai and iru are Japanese functional words. and “tz�at (find)” are properly handled when matching an inquiry pattern. In addition to the inquiry expressions, we can consider that content words that play a role like functional words, such as bra (be), 4�ra (become), and 6�5 (use), are unnecessary for retrieving documents. To detect these words we constructed an unnecessary content word list. 6 Experiments 6.1 Settings We evaluated the proposed method by using the test collections built at the NTCIR-3 (Eguchi et al., 2003) and NTCIR-4 (Eguchi et al., 2004) conferences. These share a target document set, which consists of 11,038,720 Japanese Web pages. For the evaluation, we used 127 informational topics defined in the test collections (47 from NTCIR-3 and 80 from NTCIR-4). An example of the informational topic definition is shown in Figure 4. &lt;DESC&gt; includes a sentence reflecting the user’s information needs; the sentence can be regarded as an NLQ. Therefore, we used only &lt;DESC&gt; as a query in the experiments. The relevance of each document with respect to a topic was judged as highly relevant, relevant, partially relevant, irrelevant or unjudge</context>
</contexts>
<marker>Eguchi, Oyama, Aizawa, Ishikawa, 2004</marker>
<rawString>Eguchi, Koji, Keizo Oyama, Akiko Aizawa, and Haruko Ishikawa. 2004. Overview of web task at the fourth ntcir workshop. In Proceedings of the Fourth NTCIR Workshop on Research in Information Access Technologies Information Retrieval, Question Answering and Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo Jarvelin</author>
<author>Jaana Kekalainen</author>
</authors>
<title>Cumulated gain-based evaluation of ir techniques.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>20--422</pages>
<contexts>
<context position="21959" citStr="Jarvelin and Kekalainen, 2002" startWordPosition="3634" endWordPosition="3637">, methods &lt;/TITLE&gt;&lt;DESC&gt; I want to find out about methods for learning how to dance the salsa &lt;/DESC&gt; .. &lt;/TOPIC&gt; Figure 4: Example of a search topic. ument collecting and document scoring. In both steps, the proposed method considered synonyms automatically extracted from ordinary dictionaries and Web pages (Shibata et al., 2008). For calculating the scores, we selected the value of 0.2 as the parameter β. This value was estimated using the dry-run data set of NTCIR-3. For each topic, we retrieved 1,000 documents and then assessed search performance according to MRR, P@10, R-prec, MAP, DCGN (Jarvelin and Kekalainen, 2002), and QMeasure (QM) (Sakai, 2004). We calculated these scores for each topic then averaged them. Note that unjudged documents were treated as irrelevant when computing the scores. As the graded relevance for DCGN and QM, we mapped highly relevant, relevant and partially relevant to 3, 2 and 1, respectively. The proposed method often leads to an insufficient number of search results because the method performs a conjunctive search using necessary terms. Therefore, evaluation measures, such as QM, which utilize low-ranked search results for computing their scores, give low scores in the proposed</context>
</contexts>
<marker>Jarvelin, Kekalainen, 2002</marker>
<rawString>Jarvelin, Kalervo and Jaana Kekalainen. 2002. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems, 20:422–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<title>What is the role of nlp in text retrieval?</title>
<date>1999</date>
<booktitle>Natural language information retrieval,</booktitle>
<pages>1--24</pages>
<editor>In Strzalkowski, T., editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="6485" citStr="Jones, 1999" startWordPosition="1001" endWordPosition="1002">us on a keyword-based query, (ii) the proposed method needs no dictionaries, and (iii) while the previous methods retrieve documents by proximity searches of words in phrases, the proposed method retrieves them by dependency relations in phrases. Therefore, the proposed method does not need to adjust window size, and naturally performs document retrieval based on noun phrases by using dependency relations. Linguistically motivated IR research pointed out that dependency relations did not contribute to significantly improving performance due to low accuracy and robustness of syntactic parsers (Jones, 1999). Current state-of-the-art parsers, however, can perform high accuracy for real-world sentences. Therefore, dependency relations are remarked in IR (N iyao et al., 2006; Shinzato et al., 2008b). For instance, N iyao et al. (N iyao et al., 2006) proposed an IR system for a biomedical domain that performs deep linguistic analysis on a query and each document. Their system represented relations between words by a predicate-argument structure, and used ontological databases for handling synonyms. Their experiments using a small number of short queries showed that their proposed system significantl</context>
</contexts>
<marker>Jones, 1999</marker>
<rawString>Jones, Karen Sparck. 1999. What is the role of nlp in text retrieval? In Strzalkowski, T., editor, Natural language information retrieval, pages 1–24. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuang Liu</author>
<author>Fang Liu</author>
<author>Clement Yu</author>
<author>Weiyi Meng</author>
</authors>
<title>An effective approach to document retrieval via utilizing wordnet and recognizing phrases.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>266--272</pages>
<contexts>
<context position="4950" citStr="Liu et al., 2004" startWordPosition="757" endWordPosition="760">on 4 presents term importance categories, and section 5 gives methodology for detecting such categories. Experiment results are shown in section 6. 2 Related Work A large amount of the IR methodology that has been proposed (Robertson et al., 1992; Ponte and Croft, 1998) depends on retrieval models such as probabilistic and language models. Bendersky and Croft (Bendersky and Croft, 2008), for instance, proposed a new language model in which important noun phrases can be considered. IR methodology based on important term detection has also been proposed (Callan et al., 1995; Allan et al., 1997; Liu et al., 2004; Wei et al., 2007). These previous methods have commonly focused on noun phrases because the methods assumed that a document relates to a query if the two have common noun phrases. Liu et al. (Liu et al., 2004) classified noun phrases into four types: proper nouns, dictionary phrases (e.g., computer monitor), simple phrases, and complex phrases, and detected them from a keyword-based query by using named entity taggers, part-of-speech patterns, and dictionaries such as WordNet. The detected phrases were assigned different window sizes in a proximity operator according to their types. Wei et a</context>
</contexts>
<marker>Liu, Liu, Yu, Meng, 2004</marker>
<rawString>Liu, Shuang, Fang Liu, Clement Yu, and Weiyi Meng. 2004. An effective approach to document retrieval via utilizing wordnet and recognizing phrases. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 266–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Tomoko Ohta</author>
<author>Katsuya Masuda</author>
<author>Yoshimasa Tsuruoka</author>
<author>Kazuhiro Yoshida</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Semantic retrieval for the accurate identification of relational concepts in massive textbases.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL,</booktitle>
<pages>1017--1024</pages>
<marker>Miyao, Ohta, Masuda, Tsuruoka, Yoshida, Ninomiya, Tsujii, 2006</marker>
<rawString>Miyao, Yusuke, Tomoko Ohta, Katsuya Masuda, Yoshimasa Tsuruoka, Kazuhiro Yoshida, Takashi Ninomiya, and Jun’ichi Tsujii. 2006. Semantic retrieval for the accurate identification of relational concepts in massive textbases. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pages 1017–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="4604" citStr="Ponte and Croft, 1998" startWordPosition="700" endWordPosition="703">IR-3 and NTCIR-4 conferences for evaluating Japanese IR. Search performance was resultantly improved by differently handling terms (words and dependency relations) according to their importance. This paper is organized as follows. Section 2 shows related work, and section 3 describes how to leverage dependency relations in our retrieval method. Section 4 presents term importance categories, and section 5 gives methodology for detecting such categories. Experiment results are shown in section 6. 2 Related Work A large amount of the IR methodology that has been proposed (Robertson et al., 1992; Ponte and Croft, 1998) depends on retrieval models such as probabilistic and language models. Bendersky and Croft (Bendersky and Croft, 2008), for instance, proposed a new language model in which important noun phrases can be considered. IR methodology based on important term detection has also been proposed (Callan et al., 1995; Allan et al., 1997; Liu et al., 2004; Wei et al., 2007). These previous methods have commonly focused on noun phrases because the methods assumed that a document relates to a query if the two have common noun phrases. Liu et al. (Liu et al., 2004) classified noun phrases into four types: p</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Ponte, Jay M. and W. Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
<author>Micheline Hancock-Beaulieu</author>
<author>Aarron Gull</author>
<author>Marianna Lau</author>
</authors>
<title>Okapi at TREC.</title>
<date>1992</date>
<booktitle>In Text REtrieval Conference,</booktitle>
<pages>21--30</pages>
<contexts>
<context position="4580" citStr="Robertson et al., 1992" startWordPosition="696" endWordPosition="699">ections built at the NTCIR-3 and NTCIR-4 conferences for evaluating Japanese IR. Search performance was resultantly improved by differently handling terms (words and dependency relations) according to their importance. This paper is organized as follows. Section 2 shows related work, and section 3 describes how to leverage dependency relations in our retrieval method. Section 4 presents term importance categories, and section 5 gives methodology for detecting such categories. Experiment results are shown in section 6. 2 Related Work A large amount of the IR methodology that has been proposed (Robertson et al., 1992; Ponte and Croft, 1998) depends on retrieval models such as probabilistic and language models. Bendersky and Croft (Bendersky and Croft, 2008), for instance, proposed a new language model in which important noun phrases can be considered. IR methodology based on important term detection has also been proposed (Callan et al., 1995; Allan et al., 1997; Liu et al., 2004; Wei et al., 2007). These previous methods have commonly focused on noun phrases because the methods assumed that a document relates to a query if the two have common noun phrases. Liu et al. (Liu et al., 2004) classified noun ph</context>
<context position="8865" citStr="Robertson et al., 1992" startWordPosition="1377" endWordPosition="1380">ords, and discard the case marker between content words. Also, (compound) functional words such as “ʹ͍ͭͯ (about)” and “ʹैͬͯ (according to)” are attached to the former content word. Figure 1 shows the parsing result of the query “ޙɼΊ͟·͍͠׆༂Λ͠ ͍ͯΔϚΠέϧδϣʔμϯେֶ׆༂ʹ ͍͍ͭͯͨ.1” The pair of content words (େ ֶ (university),  (time)) is extracted as a dependency relation from the parsing result. Note that the pair of content words (  (time), େ ֶ (university)) is not extracted as a dependency relation because a dependency relation is represented by a directed binary relation. We used Okapi BM25 (Robertson et al., 1992) for estimating relevancy between a query and a document, which is how it is used in most case, though we slightly extend this measure for estimating relevancy for dependency relations. We denote a set of words in a query q as Tqword, and also denote a set of dependency relations in q as Tqdpnd. The relevancy between query q and document d is as follows: where β is a parameter for adjusting the ratio of a 1This means that Michael Jordan’s performance has been spectacular since his return to NBA, and I want to learn about his activities when he was a university student. score calculated from de</context>
</contexts>
<marker>Robertson, Walker, Hancock-Beaulieu, Gull, Lau, 1992</marker>
<rawString>Robertson, Stephen E., Steve Walker, Micheline Hancock-Beaulieu, Aarron Gull, and Marianna Lau. 1992. Okapi at TREC. In Text REtrieval Conference, pages 21–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
</authors>
<title>New performance metrics based on multigrade relevance: Their application to question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth NTCIR Workshop Meeting.</booktitle>
<contexts>
<context position="21992" citStr="Sakai, 2004" startWordPosition="3642" endWordPosition="3643">ethods for learning how to dance the salsa &lt;/DESC&gt; .. &lt;/TOPIC&gt; Figure 4: Example of a search topic. ument collecting and document scoring. In both steps, the proposed method considered synonyms automatically extracted from ordinary dictionaries and Web pages (Shibata et al., 2008). For calculating the scores, we selected the value of 0.2 as the parameter β. This value was estimated using the dry-run data set of NTCIR-3. For each topic, we retrieved 1,000 documents and then assessed search performance according to MRR, P@10, R-prec, MAP, DCGN (Jarvelin and Kekalainen, 2002), and QMeasure (QM) (Sakai, 2004). We calculated these scores for each topic then averaged them. Note that unjudged documents were treated as irrelevant when computing the scores. As the graded relevance for DCGN and QM, we mapped highly relevant, relevant and partially relevant to 3, 2 and 1, respectively. The proposed method often leads to an insufficient number of search results because the method performs a conjunctive search using necessary terms. Therefore, evaluation measures, such as QM, which utilize low-ranked search results for computing their scores, give low scores in the proposed method. To avoid this problem we</context>
</contexts>
<marker>Sakai, 2004</marker>
<rawString>Sakai, Tetsuya. 2004. New performance metrics based on multigrade relevance: Their application to question answering. In Proceedings of the Fourth NTCIR Workshop Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Japanese named entity recognition using structural natural language processing.</title>
<date>2008</date>
<booktitle>In Proceedings of Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>607--612</pages>
<contexts>
<context position="24199" citStr="Sasano and Kurohashi, 2008" startWordPosition="4026" endWordPosition="4029">ch No Q No D No 4 0 Yes Q 0 Yes Q 0 0 Q conditions No 0 0 No 0 0 0 0 MRR 0.533 0.538 0.503 0.547 0.537 P@10 0.328 0.337 0.352 0.352 0.357 DCG10 3.469 3.497 3.583 3.634 3.713 DCG100 7.191 8.898 9.167 9.045 9.280 DCG1000 8.956 16.221 16.553 16.678 16.866 R-prec 0.174 0.207 0.212 0.217 0.221 MAP 0.120 0.151 0.158 0.161 0.164 QM 0.095 0.168 0.175 0.179 0.183 Prox: Proximity, Dpnd: Dependency relation, RV: Redundant verb. alyzed by the JUMAN2, Japanese morphological analyzer and KNP3, Japanese syntactic parser which implemented the named entity recognition feature proposed by Sasano and Kurohashi (Sasano and Kurohashi, 2008). All documents were also analyzed by JUMAN and KNP, and then words and dependency relations in the documents were indexed as index terms. For instance, the dependency relation (university, time) shown in Figure 1 is indexed as university ˠ time. 6.2 Comparison with Alternative Searches We first investigated the effectiveness of clear boundaries of term importance and differently handling of terms according to their importance. We compared the proposed method with the following alternative search methods (see Table 1): AND: Conjunctive search only using words. We do nothing even if the number </context>
</contexts>
<marker>Sasano, Kurohashi, 2008</marker>
<rawString>Sasano, Ryohei and Sadao Kurohashi. 2008. Japanese named entity recognition using structural natural language processing. In Proceedings of Third International Joint Conference on Natural Language Processing, pages 607–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomohide Shibata</author>
<author>Michitaka Odani</author>
<author>Jun Harashima</author>
<author>Takashi Oonishi</author>
<author>Sadao Kurohashi</author>
</authors>
<title>SYNGRAPH: A flexible matching method based on synonymous expression extraction from an ordinary dictionary and a web corpus.</title>
<date>2008</date>
<booktitle>In Proc. of IJCNLP2008,</booktitle>
<pages>787--792</pages>
<contexts>
<context position="21661" citStr="Shibata et al., 2008" startWordPosition="3584" endWordPosition="3587"> respect to a topic was judged as highly relevant, relevant, partially relevant, irrelevant or unjudged. We regarded the highly relevant, relevant, and partially relevant documents as correct answers. The process of IR consists of two steps: doc&lt;TOPIC&gt;&lt;NUM&gt; 0008 &lt;/NUM&gt;&lt;TITLE&gt; Salsa, learn, methods &lt;/TITLE&gt;&lt;DESC&gt; I want to find out about methods for learning how to dance the salsa &lt;/DESC&gt; .. &lt;/TOPIC&gt; Figure 4: Example of a search topic. ument collecting and document scoring. In both steps, the proposed method considered synonyms automatically extracted from ordinary dictionaries and Web pages (Shibata et al., 2008). For calculating the scores, we selected the value of 0.2 as the parameter β. This value was estimated using the dry-run data set of NTCIR-3. For each topic, we retrieved 1,000 documents and then assessed search performance according to MRR, P@10, R-prec, MAP, DCGN (Jarvelin and Kekalainen, 2002), and QMeasure (QM) (Sakai, 2004). We calculated these scores for each topic then averaged them. Note that unjudged documents were treated as irrelevant when computing the scores. As the graded relevance for DCGN and QM, we mapped highly relevant, relevant and partially relevant to 3, 2 and 1, respect</context>
</contexts>
<marker>Shibata, Odani, Harashima, Oonishi, Kurohashi, 2008</marker>
<rawString>Shibata, Tomohide, Michitaka Odani, Jun Harashima, Takashi Oonishi, and Sadao Kurohashi. 2008. SYNGRAPH: A flexible matching method based on synonymous expression extraction from an ordinary dictionary and a web corpus. In Proc. of IJCNLP2008, pages 787–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Shinzato</author>
<author>Daisuke Kawahara</author>
<author>Chikara Hashimoto</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A large-scale web data collection as a natural language processing infrastructure.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC08).</booktitle>
<contexts>
<context position="6676" citStr="Shinzato et al., 2008" startWordPosition="1027" endWordPosition="1030">oposed method retrieves them by dependency relations in phrases. Therefore, the proposed method does not need to adjust window size, and naturally performs document retrieval based on noun phrases by using dependency relations. Linguistically motivated IR research pointed out that dependency relations did not contribute to significantly improving performance due to low accuracy and robustness of syntactic parsers (Jones, 1999). Current state-of-the-art parsers, however, can perform high accuracy for real-world sentences. Therefore, dependency relations are remarked in IR (N iyao et al., 2006; Shinzato et al., 2008b). For instance, N iyao et al. (N iyao et al., 2006) proposed an IR system for a biomedical domain that performs deep linguistic analysis on a query and each document. Their system represented relations between words by a predicate-argument structure, and used ontological databases for handling synonyms. Their experiments using a small number of short queries showed that their proposed system significantly improved search performance versus a system not performing deep linguistic analysis. Shinzato et al. (Shinzato et al., 2008b) proposed a Web search system that handles not only words but al</context>
<context position="15293" citStr="Shinzato et al., 2008" startWordPosition="2486" endWordPosition="2489"> For computing the connection strength among words in N, we assumed that words in an SCCN are unlikely to occur in documents as “wi  wi+1 (wi+1 of wi)”. This assumption reflects the observation that “Kyoto sightseeing” is likely to be expressed as “sightseeing of Kyoto” and that “information science” is unlikely to be expressed by “science of information.” In line with this assumption, the connection strength is calculated as follows: |N|−1 1 DF(wi wi+1) 5corestrength(N) = |N |− 1 E DF(wi+1 wi) Here, DF(X) is the document frequency of X computed from hundreds of millions Japanese Web pages (Shinzato et al., 2008a). The proposed method regards a compound noun N as an SCCN if the value of 5corestrength(N) exceeds a threshold Tp. We used the value of 300 as the threshold. In addition to dependency relations in NEs, 5 the term importance of dependency relations in an SCCN is also revised from an optional category to a necessary category. a w ua 5.2 Redundant Verb The proposed method deals with a verb whose meaning is inferable from the surrounding nouns as an optional term. We refer to such a verb a redundant verb. Consider the following two expressions: (A) Ո (author)  (of) ॻ͍ͨ (wrote) ຊ (book) (A boo</context>
</contexts>
<marker>Shinzato, Kawahara, Hashimoto, Kurohashi, 2008</marker>
<rawString>Shinzato, Keiji, Daisuke Kawahara, Chikara Hashimoto, and Sadao Kurohashi. 2008a. A large-scale web data collection as a natural language processing infrastructure. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Shinzato</author>
<author>Tomohide Shibata</author>
<author>Daisuke Kawahara</author>
<author>Chikara Hashimoto</author>
<author>Sadao Kurohashi</author>
</authors>
<title>TSUBAKI: An open search engine infrastructure for developing new information access methodology.</title>
<date>2008</date>
<booktitle>In Proc. of IJCNLP2008,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="6676" citStr="Shinzato et al., 2008" startWordPosition="1027" endWordPosition="1030">oposed method retrieves them by dependency relations in phrases. Therefore, the proposed method does not need to adjust window size, and naturally performs document retrieval based on noun phrases by using dependency relations. Linguistically motivated IR research pointed out that dependency relations did not contribute to significantly improving performance due to low accuracy and robustness of syntactic parsers (Jones, 1999). Current state-of-the-art parsers, however, can perform high accuracy for real-world sentences. Therefore, dependency relations are remarked in IR (N iyao et al., 2006; Shinzato et al., 2008b). For instance, N iyao et al. (N iyao et al., 2006) proposed an IR system for a biomedical domain that performs deep linguistic analysis on a query and each document. Their system represented relations between words by a predicate-argument structure, and used ontological databases for handling synonyms. Their experiments using a small number of short queries showed that their proposed system significantly improved search performance versus a system not performing deep linguistic analysis. Shinzato et al. (Shinzato et al., 2008b) proposed a Web search system that handles not only words but al</context>
<context position="15293" citStr="Shinzato et al., 2008" startWordPosition="2486" endWordPosition="2489"> For computing the connection strength among words in N, we assumed that words in an SCCN are unlikely to occur in documents as “wi  wi+1 (wi+1 of wi)”. This assumption reflects the observation that “Kyoto sightseeing” is likely to be expressed as “sightseeing of Kyoto” and that “information science” is unlikely to be expressed by “science of information.” In line with this assumption, the connection strength is calculated as follows: |N|−1 1 DF(wi wi+1) 5corestrength(N) = |N |− 1 E DF(wi+1 wi) Here, DF(X) is the document frequency of X computed from hundreds of millions Japanese Web pages (Shinzato et al., 2008a). The proposed method regards a compound noun N as an SCCN if the value of 5corestrength(N) exceeds a threshold Tp. We used the value of 300 as the threshold. In addition to dependency relations in NEs, 5 the term importance of dependency relations in an SCCN is also revised from an optional category to a necessary category. a w ua 5.2 Redundant Verb The proposed method deals with a verb whose meaning is inferable from the surrounding nouns as an optional term. We refer to such a verb a redundant verb. Consider the following two expressions: (A) Ո (author)  (of) ॻ͍ͨ (wrote) ຊ (book) (A boo</context>
</contexts>
<marker>Shinzato, Shibata, Kawahara, Hashimoto, Kurohashi, 2008</marker>
<rawString>Shinzato, Keiji, Tomohide Shibata, Daisuke Kawahara, Chikara Hashimoto, and Sadao Kurohashi. 2008b. TSUBAKI: An open search engine infrastructure for developing new information access methodology. In Proc. of IJCNLP2008, pages 189– 196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Wei</author>
<author>Liu Shuang</author>
<author>Yu Clement</author>
<author>Sun Chaojing</author>
<author>Liu Fang</author>
<author>Meng Weiyi</author>
</authors>
<title>Recognition and classification of noun phrases in queries for effective retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>711--720</pages>
<contexts>
<context position="4969" citStr="Wei et al., 2007" startWordPosition="761" endWordPosition="764"> importance categories, and section 5 gives methodology for detecting such categories. Experiment results are shown in section 6. 2 Related Work A large amount of the IR methodology that has been proposed (Robertson et al., 1992; Ponte and Croft, 1998) depends on retrieval models such as probabilistic and language models. Bendersky and Croft (Bendersky and Croft, 2008), for instance, proposed a new language model in which important noun phrases can be considered. IR methodology based on important term detection has also been proposed (Callan et al., 1995; Allan et al., 1997; Liu et al., 2004; Wei et al., 2007). These previous methods have commonly focused on noun phrases because the methods assumed that a document relates to a query if the two have common noun phrases. Liu et al. (Liu et al., 2004) classified noun phrases into four types: proper nouns, dictionary phrases (e.g., computer monitor), simple phrases, and complex phrases, and detected them from a keyword-based query by using named entity taggers, part-of-speech patterns, and dictionaries such as WordNet. The detected phrases were assigned different window sizes in a proximity operator according to their types. Wei et al. (Wei et al., 200</context>
</contexts>
<marker>Wei, Shuang, Clement, Chaojing, Fang, Weiyi, 2007</marker>
<rawString>Wei, Zhang, Liu Shuang, Yu Clement, Sun Chaojing, Liu Fang, and Meng Weiyi. 2007. Recognition and classification of noun phrases in queries for effective retrieval. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 711–720.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>