<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013847">
<title confidence="0.998539">
A Framework for Model-based Evaluation of Spoken Dialog Systems
</title>
<author confidence="0.96731">
Sebastian M¨oller
</author>
<affiliation confidence="0.8202425">
Deutsche Telekom Laboratories
Technische Universit¨at Berlin
</affiliation>
<address confidence="0.700861">
10587 Berlin, Germany
</address>
<email confidence="0.994738">
sebastian.moeller@telekom.de
</email>
<author confidence="0.968617">
Nigel G. Ward
</author>
<affiliation confidence="0.9939935">
Computer Science Department
University of Texas at El Paso
</affiliation>
<address confidence="0.731661">
El Paso, Texas 79968, USA
</address>
<email confidence="0.996755">
nigelward@acm.org
</email>
<sectionHeader confidence="0.995603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912647058823">
Improvements in the quality, usability and ac-
ceptability of spoken dialog systems can be
facilitated by better evaluation methods. To
support early and efficient evaluation of dia-
log systems and their components, this paper
presents a tripartite framework describing the
evaluation problem. One part models the be-
havior of user and system during the interac-
tion, the second one the perception and judg-
ment processes taking place inside the user,
and the third part models what matters to sys-
tem designers and service providers. The pa-
per reviews available approaches for some of
the model parts, and indicates how anticipated
improvements may serve not only developers
and users but also researchers working on ad-
vanced dialog functions and features.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961854166667">
Despite the utility of many spoken dialog systems
today, the user experience is seldom satisfactory.
Improving this is a matter of great intellectual in-
terest and practical importance. However improve-
ments can be difficult to evaluate effectively, and this
may be limiting the pace of innovation: today, valid
and reliable evaluations still require subjective ex-
periments to be carried out, and these are expensive
and time-consuming. Thus, the needs of system de-
velopers, of service operators, and of the final users
of spoken dialog systems argue for the development
of additional evaluation methods.
In this paper we focus on the prospects for an
early and model-based evaluation of dialog systems.
Doing evaluation as early as possible in the de-
sign and development process is critical for improv-
ing quality, reducing costs and fostering innovation.
Early evaluation renders the process more efficient
and less dependent on experience, hunches and intu-
itions. With the help of such models predicting the
outcome of user tests, the need for subjective test-
ing can be reduced, restricting it to that subset of the
possible systems which have already been vetted in
an automatic or semi-automatic way.
Several approaches have already been presented
for semi-automatic evaluation. For example, the
PARADISE framework (Walker et al., 1997) predicts
the effects of system changes, quantified in terms of
interaction parameters, on an average user judgment.
Others (Araki and Doshita, 1997; L´opez-C´ozar et
al., 2003; M¨oller et al., 2006) have developed dialog
simulations to aid system optimization. However the
big picture has been missing: there has been no clear
view of how these methods relate to each other, and
how they might be improved and joined to support
efficient early evaluation.
The remainder of this paper is organized as fol-
lows. Section 2 gives a brief review of different
evaluation purposes and terminology, and outlines a
new tripartite decomposition of the evaluation prob-
lem. One part of our framework models the behav-
ior of user and system during the interaction, and
describes the impact of system changes on the inter-
action flow. The second part models the perception
and judgment processes taking place inside the user,
and tries to predict user ratings on various percep-
tual dimensions. The third part models what mat-
ters to system designers and service providers for
</bodyText>
<page confidence="0.964908">
182
</page>
<note confidence="0.8699625">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 182–189,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9989975">
a specific application. Sections 3, 4, and 5 go into
specifics on the three parts of the framework, dis-
cussing which components are already available or
conceivable. Finally, Section 6 discusses the poten-
tial impact of the approach, and Section 7 lists the
issues to be resolved in future work.
</bodyText>
<sectionHeader confidence="0.990695" genericHeader="introduction">
2 Performance, Quality, Usability and
Acceptability Evaluation
</sectionHeader>
<bodyText confidence="0.999859941860465">
Developers tend to use indices of performance to as-
sess their systems. The performance indicates the
“ability of a system to provide the function it has
been designed for” (M¨oller, 2005). The function
and an appropriate measure for quantifying the de-
gree of fulfillment may easily be determined for cer-
tain components — e.g. word accuracy for a speech
recognizer or concept error rate for a speech under-
standing module — but it is harder to specify for
other components, such as a dialog manager or an
output generation module. However, definitive mea-
sures of component quality are not always neces-
sary: what matters for such a module is its contri-
bution to the quality of the entire interaction, as it is
perceived by the user.
We follow the definition of the term quality as
introduced by Jekosch (2000) and now accepted
for telephone-based spoken dialog services by the
International Telecommunication Union in ITU-T
Rec. P.851 (2003): “Result of judgment of the per-
ceived composition of an entity with respect to its
desired composition”. Quality thus involves a per-
ception process and a judgment process, during
which the perceiving person compares the percep-
tual event with a (typically implicit) reference. It is
the comparison with a reference which associates a
user-specific value to the perceptual event. The per-
ception and the comparison processes take place in a
particular context of use. Thus, both perception and
quality should be regarded as “events” which hap-
pen in a particular personal, spatial, temporal and
functional context.
Usability is one sub-aspect of the quality of the
system. Following the definition in ISO 9241 Part
11 (1998), usability is considered as the “extent to
which a product can be used by specified users to
achieve specified goals with effectiveness, efficiency
and satisfaction in a specified context of use”. Us-
ability is degraded when interaction problems oc-
cur. Such problems influence the perceptual event
of the user interacting with the system, and conse-
quently the quality s/he associates with the system
as a whole. This may have consequences for the
acceptability or the system or service, that is, how
readily a customer will use the system or service.
This can be quantified, for example as the ratio of
the potential user population to the size of the target
group.
It is the task of any evaluation to quantify as-
pects of system performance, quality, usability or
acceptability. The exact target depends on the pur-
pose of the evaluation (Paek, 2007). For example,
the system developer might be most interested in
quantifying the performance of the system and its
components; s/he might further need to know how
the performance affects the quality perceived by the
user. In contrast, the service operator might instead
be most interested in the acceptability of the ser-
vice. S/he might further want to know about the
satisfaction of the user, influenced by the usability
of the system, and also by other (e.g. hedonic) as-
pects like comfort, joy-of-use, fashion, etc. Differ-
ent evaluation approaches may be complementary,
in the sense that metrics determined for one purpose
may be helpful for other purposes as well. Thus, it is
useful to describe the components of different eval-
uation approaches in a single framework.
Figure 1 summarizes our view of the evaluation
landscape. At the lower left corner is what we can
change (the dialog system), at the right is what the
service operator might be interested in (a metric for
the value of the system). In between are three com-
ponents of a model of the processes taking place in
the evaluation. The behavior model describes how
system and user characteristics determine the flow
of the interaction and translate this to quantitative
descriptors. The perception and judgment model
describes how the interaction influences the percep-
tual and quality events felt by the user, and trans-
lates these to observable user judgments. Finally the
value model associates a certain value to the qual-
ity judgments, depending on the application. The
model properties have been grouped in three layers:
aspects of the user and his/her behavior, aspects of
the system in its context-of-use, and the work of an
external observer (expert) carrying out the evalua-
</bodyText>
<page confidence="0.996314">
183
</page>
<figure confidence="0.9958955">
Behavior Model Perception and Judgment Model Value Model
System
Behavior
User
Behavior
Interacion
Behavior
Interaction
Phenomena
Interaction
Parameters
Perceptual
Event
Reference
Perceptual
Quality
Dimensions
Quality
Event
Dimension
Descriptors
Quality
Judgments
Designer
and Operator
Requirements
Quality
Aspects
Value
Description
</figure>
<figureCaption confidence="0.775967">
Figure 1: Tripartite view of a model-based evaluation. Observable properties are in boxes, inferred or hidden properties
are in ovals. The layers organize the properties as mostly user-related, mostly system-related, and mostly expert-
related, and mostly system-related.
</figureCaption>
<equation confidence="0.6489635">
i
tsy
</equation>
<bodyText confidence="0.979319571428571">
tion. They have further been classified as to whether
they are observable (boxes) or hidden from the eval-
uator (ovals).
The next three sections go through the three parts
of the model left-to-right, explaining the needs, cur-
rent status, and prospects.
y
</bodyText>
<sectionHeader confidence="0.998192" genericHeader="method">
3 Behavior Model
</sectionHeader>
<bodyText confidence="0.99998386">
The behavior model translates the characteristics of
the system and the user into predicted interaction be-
havior. In order to be useful, the representations of
this behavior must be concise.
One way to describe dialog behavior is with in-
teraction parameters which quantify the behavior of
the user and/or the system during the interaction.
Such parameters may be measured instrumentally or
given by expert annotation. In an attempt to sys-
tematize best practice, the ITU-T has proposed a
common set of interaction parameters suitable for
the evaluation of telephone-based spoken dialog sys-
tems in ITU-T Suppl. 24 (2005). These parameters
have been developed bottom-up from a collection of
evaluation reports over the last 15 years, and include
metrics related to dialog and communication in gen-
eral, meta-communication, cooperativity, task, and
speech-input performance (M¨oller, 2005). Unfortu-
nately, it as is yet unclear which of these parameters
relate to quality from a user’s point-of-view. In addi-
tion, some metrics are missing which address critical
aspects for the user, e.g. parameters for the quality
and attractiveness of the speech output.
Another manageable way to describe system be-
havior is to focus on interaction phenomena. Sev-
eral schemes have been developed for classifying
such phenomena, such as system errors, user errors,
points of confusion, dead time, and so on (Bernsen et
al., 1998; Ward et al., 2005; Oulasvirta et al., 2006).
Patterns of interaction phenomena may be reflected
in interaction parameter values, and may be identi-
fied on that basis. Otherwise, they have to be deter-
mined by experts and/or users, by means of obser-
vation, interviews, thinking-aloud, and other tech-
niques from usability engineering. (Using this ter-
minology we can understand the practice of usability
testing as being the identification of interaction phe-
nomena, also known as “usability events” or “criti-
cal incidences”, and using these to estimate specific
quality aspects or the overall value of the system.)
Obtaining the interaction parameters and classi-
fying the interaction phenomena can be done, ob-
viously, from a corpus of user-system interactions.
The challenge for early evaluation is to obtain these
without actually running user tests. Thus, we would
like to have a system behavior model and a user be-
havior model to simulate interaction behavior, and
to map from system parameters and user properties
to interaction parameters or phenomena. The value
of such models for a developer is clear: they could
</bodyText>
<page confidence="0.993847">
184
</page>
<bodyText confidence="0.999955650000001">
enable estimation of how a change in the system
(e.g. a change in the vocabulary) might affect the
interaction properties. In addition to the desired ef-
fects, the side-effects of system changes are also im-
portant. Predicting such side-effects will substan-
tially decrease the risk and uncertainty involved in
dialogue design, thereby decreasing the gap between
research and commercial work on dialog system us-
ability (Heisterkamp, 2003; Pieraccini and Huerta,
2005).
Whereas modeling system behavior in response to
user input is clearly possible (since in the last resort
it is possible to fully implement the system), user be-
havior can probably not be modeled in closed form,
because it unavoidably relates to the intricacies of
the user and reflects the time-flow of the interaction.
Thus, it seems necessary to employ a simulation
of the interaction, as has been proposed by Araki
and Doshita (1997) and L´opez-C´ozar et al. (2003),
among others.
One embodiment of this idea is the MeMo work-
bench (M¨oller et al., 2006), which is based on
the idea of running models of the system and of
the user in a dedicated usability testing workbench.
The system model is a description of the possi-
ble tasks (system task model) plus a description of
the system’s interaction behavior (system interac-
tion model). The user model is a description of the
tasks a user would want to carry out with the sys-
tem (user task model) plus a description of the steps
s/he would take to reach the goal when faced with
the system (user interaction model). Currently the
workbench uses simple attribute-value descriptions
of tasks the system is able to carry out. From these,
user-desired tasks may be derived, given some back-
ground knowledge of the domain and possible tasks.
The system interaction model is described by a state
diagram which models interactions as paths through
a number of dialog states. The system designer pro-
vides one or several ‘intended paths’ through the in-
teraction, which lead easily and/or effectively to the
task goal.
The user’s interaction behavior will strongly de-
pend on the system output in the previous turn.
Thus, it is reasonable to build the user interaction
model on top of the system interaction model: The
user mainly follows the ‘intended path’, but at cer-
tain points deviations from this path are generated in
a probabilistic rule-based manner. For example, the
user might deviate from the intended path, because
s/he does not understand a long system prompt, or
because s/he is irritated by a large number of op-
tions. Each deviation from the intended path has
an associated probability; these are calculated from
system characteristics (e.g. prompt length, number
of options) and user characteristics (e.g. experience
with dialog systems, command of foreign languages,
assumed task and domain knowledge).
After the models have been defined, simulations
of user-system interactions can be generated. These
interactions are logged and annotated on different
levels in order to detect interaction problems. Us-
ability predictions are obtained from the (simulated)
interaction problems. The simulations can also sup-
port reinforcement learning or other methods for au-
tomatically determining the best dialog strategy.
Building user interaction models by hand is
costly. As an alternative to explicitly defining rules
and probabilities, simulations can be based on data
sets of actual interactions, augmented with annota-
tions such as indications of the dialog state, current
subtask, inferred user state, and interaction phenom-
ena. Annotations can be generated by the dialog
participants themselves, e.g. by re-listening after the
fact (Ward and Tsukahara, 2003), or by top com-
municators, decision-makers, trend-setters, experts
in linguistics and communication, and the like. Ma-
chine learning techniques can help by providing pre-
dictions of how users tend to react in various situa-
tions from lightly annotated data.
</bodyText>
<sectionHeader confidence="0.994004" genericHeader="method">
4 Perception and Judgment Model
</sectionHeader>
<bodyText confidence="0.999889538461538">
Once the interaction behavior is determined, the
evaluator needs to know about the impact it has on
the quality perceived by the user. As pointed out in
Section 2, the perception and judgments processes
take place in the human user and are thus hidden
from the observer. The evaluator may, however, ask
the user to describe the perceptual event and/or the
quality event, either qualitatively in an open form or
quantitatively on rating scales. Provided that the ex-
periment is properly planned and carried out, user
quality judgments can be considered as direct qual-
ity measurements, reflecting the user’s quality per-
ception.
</bodyText>
<page confidence="0.995534">
185
</page>
<bodyText confidence="0.999973718750001">
Whereas user judgments on quality will reflect the
internal reference and thus depend heavily on the
specific context and application, it may be assumed
that the characteristics of the perceptual event are
more universal. For example, it is likely that sam-
ples of observers and/or users would generally agree
on whether a given system could be characterized
as responsive, smooth, or predictable, etc. regardless
of what they feel about the importance of each such
quality aspect. We may take advantage of this by
defining a small set of universal perceptual quality
dimensions, that together are sufficient for predict-
ing system value from the user’s point-of-view.
In order to quantify the quality event and to iden-
tify perceptual quality dimensions, psychometric
measurement methods are needed, e.g. interaction
experiments with appropriate measurement scales.
Several attempts have been made to come up with
a common questionnaire for user perception mea-
surement related to spoken dialog systems, for ex-
ample the SASSI questionnaire (Hone and Graham,
2000) for systems using speech input, and the ITU-
standard augmented framework for questionnaires
(ITU-T Rec. P.851, 2003) for systems with both
speech-input and speech-output capabilities. Studies
of the validity and the reliability of these question-
naires (M¨oller et al., 2007) show that both SASSI
and P.851 can cover a large number of different qual-
ity and usability dimensions with a high validity, and
mainly with adequate reliability, although the gener-
alizability of these results remains to be shown.
On the basis of batteries of user judgments ob-
tained with these questionnaires, dimension descrip-
tors of the perceptual quality dimensions can be ex-
tracted by means of factor analysis. A summary of
such multidimensional analyses in M¨oller (2005b)
reveals that users’ perceptions of quality and usabil-
ity can be decomposed into around 5 to 8 dimen-
sions. The resulting dimensions include factors such
as overall acceptability, task effectiveness, speed,
cognitive effort, and joy-of-use. It should be noted
that most such efforts have considered task-oriented
systems, where effectiveness, efficiency, and suc-
cess are obviously important, however these dimen-
sions may be less relevant to systems designed for
other purposes, for example tutoring or “edutain-
ment” (Bernsen et al., 2004), and additional factors
may be needed for such applications.
In order to describe the impact of the interac-
tion flow on user-perceived quality, or on some of
its sub-dimensions, we would ideally model the hu-
man perception and judgment processes. Such an
approach has the clear advantage that the resulting
model would be generic, i.e. applicable to differ-
ent systems and potentially for different user groups,
and also analytic, i.e. able to explain why certain in-
teraction characteristics have a positive or negative
impact on perceived quality. Unfortunately, the per-
ception and judgment processes involved in spoken-
dialog interaction are not yet well understood, as
compared, for example, to those involved in listen-
ing to transmitted speech samples and judging their
quality. For the latter, models are available which
estimate quality with the help of peripheral audi-
tory perception models and a signal-based compar-
ison of representations of the perceptual event and
the assumed reference (Rix et al., 2006). They are
able to estimate user judgments on “overall quality”
with an average correlation of around 0.93, and are
widely used for planning, implementing and moni-
toring telephone networks.
For interactions with spoken dialog systems, the
situation is more complicated, as the perceptual
events depend on the interaction between user and
systems, and not on one speech signal alone. A way
out is not to worry about the perception processes,
and instead to use simple linear regression models
for predicting an average user judgment from vari-
ous interaction parameters. The most widely used
framework designed to support this sort of early
evaluation is PARADISE (Walker et al., 1997). The
target variable of PARADISE is an average of several
user judgments (labeled “user satisfaction”) of dif-
ferent system and interaction aspects, such as system
voice, perceived system understanding, task ease,
interaction pace, or the transparency of the interac-
tion. The interaction parameters are of three types,
those relating to efficiency (including elapsed time
and the number of turns), those relating to “dialog
quality” (including mean recognition score and the
number of timeouts and rejections), and a measure
of effectiveness (task success). The model can be
trained on data, and the results are readily inter-
pretable: they can indicate which features of the in-
teraction are most critical for improving user satis-
faction.
</bodyText>
<page confidence="0.997714">
186
</page>
<bodyText confidence="0.999959441860465">
PARADISE-style models can be very helpful tools
for system developers. For example, a recent inves-
tigation showed that the model can be used to ef-
fectively determine the minimum acceptable recog-
nition rate for a smart-home system, leading to
the same critical threshold as that obtained from
user judgments (Engelbrecht and M¨oller, 2007).
However, experience also shows that the PARADISE
framework does not reliably give valid predictions of
individual user judgments, typically covering only
around 40-50% of the variance in the data it is
trained on. The generality is also limited: cross-
system extrapolation works sometimes but other
times has low accuracy (Walker et al., 2000; M¨oller,
2005). These limitations are easy to understand in
terms of Figure 1: over-ambitious attempts to di-
rectly relate interaction parameters to a measure of
overall system value seem unlikely to succeed in
general. Thus it seems wise to limit the scope of the
perception and judgment component to the predic-
tion of values on the perceptual quality dimensions.
In any case, there are several ways in which such
models could be improved. One issue is that a linear
combination of factors is probably not generally ad-
equate. For example, parameters like the number of
turns required to execute a specific task will have a
non-zero optimum value, at least for inexperienced
users. An excessively low number of turns will be
as sure a sign of interaction problems as an exces-
sively large number. Such non-linear effects can-
not be handled by linear models which only support
relationships like “the-more-the-better” or “the-less-
the-better”. Non-linear algorithms may overcome
these limitations. A second issue is that of tempo-
ral context: instead of using a single input vector
of interaction parameters for each dialog, it may be
possible to apply a sequence of feature vectors, one
for each exchange (user-system utterance pair). The
features may consist not only of numeric measures
but also of categories encoding interaction phenom-
ena. Using this input one could then perhaps use a
neural network or Hidden-Markov Model to predict
various user judgments at the end of the interaction.
</bodyText>
<sectionHeader confidence="0.997881" genericHeader="method">
5 Value Model
</sectionHeader>
<bodyText confidence="0.999988095238095">
Even if a model can predict user judgments of “over-
all quality” with high validity and reliability, this is
not necessarily a good indicator of the acceptability
of a service. For example, systems with a sophis-
ticated and smooth dialog flow may be unaccept-
able for frequent users because what counts for them
is effectiveness and efficiency only. Different users
may focus on different quality dimensions in differ-
ent contexts, and weight them according to the task,
context of use, price, etc.
A first step towards addressing this problem
is to define quality aspects that a system devel-
oper or service operator might be concerned about.
There can be many such, but in usability engineer-
ing they are typically categorized into “effective-
ness”, “efficiency” and “satisfaction”. A more de-
tailed taxonomy of quality aspects can be found in
M¨oller (2005). On the basis of this or other tax-
onomizations, value prediction models can be de-
veloped. For example, a system enabling 5-year
old girls to “talk to Barbie” might ascribe little im-
portance to task completion, speech recognition ac-
curacy, or efficiency, but high importance to voice
quality, responsiveness, and unpredictability. The
value model will derive a value description which
takes such a weighting into account. A model for
systems enabling police officers on patrol to obtain
information over the telephone would have very dif-
ferent weights.
Unfortunately, there appear to be no published de-
scriptions of value prediction models, perhaps be-
cause they are very specific or even proprietary, de-
pending on a company’s business logic and cus-
tomer base. Such models probably need not be very
complex: it likely will suffice to ascribe weights to
the perceptual quality dimensions, or to quality as-
pects derived from system developer and/or service
operator requirements. Appropriate weights may be
uncovered in stakeholder workshops, where design-
ers, vendors, usability experts, marketing strategists,
user representatives and so on come together and
discuss what they desire or expect.
</bodyText>
<sectionHeader confidence="0.992571" genericHeader="method">
6 Broader Impacts
</sectionHeader>
<bodyText confidence="0.9997946">
We have presented a tripartite evaluation framework
which shows the relationship between user and sys-
tem characteristics, interaction behavior, perceptual
and quality events, their descriptions, and the final
value of the system or service. In doing so, we
</bodyText>
<page confidence="0.995577">
187
</page>
<bodyText confidence="0.999915333333333">
have mainly considered the needs of system devel-
opers. However, an evaluation framework that sup-
ports judgments of perceived quality could provide
additional benefits for users. We can imagine user-
specific value models, representing what is impor-
tant to specified user groups. These could be so-
licited for an entire group, or inferred from each
user’s own personal history of interactions and deci-
sions, e.g, through a personalization database avail-
able to the service operator. The models could also
be used to support system selection, or to inform
real-time system customization or adaptation.
Better evaluation will also support the needs of
the research community. With the help of model-
based evaluation, it will become easier for re-
searchers not only to do evaluation more efficiently,
but also to to produce more meaningful evaluation
results; saying not just “this feature was useful” but
also providing quantitative statements of how much
the feature affects various interaction parameters,
and from that how much it impacts the various qual-
ity dimensions, and ultimately the value itself. This
will make evaluation more meaningful and make it
easy for others to determine when an innovation is
worth adopting, speeding technology transfer.
One might worry that a standardized framework
might only be useful for evaluating incremental im-
provements, thereby discouraging work on radically
different dialog design concepts. However well-
designed evaluation components should enable this
framework to work for systems of any type, meaning
that it may be easier to explore new regions of the
design space. In particular it may enable more ac-
curate prediction of the value of design innovations
which in isolation may not be effective, but which in
combination may be.
</bodyText>
<sectionHeader confidence="0.999564" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.92324669047619">
Although examples of some model components are
available today, notably several interaction simula-
tions and the PARADISE framework for predicting
user judgments from interaction parameters, these
are limited. To realize a complete and generally use-
ful evaluation model will require considerable work,
for example, on:
• User behavior model: Of the three compo-
nents, perhaps the greatest challenges are in
the development of user behavior models. We
need to develop methods which produce simu-
lated behavior which is realistic (congruent to
the behavior of real users), and/or which pro-
duce interaction parameters and/or quality in-
dicators comparable to those obtained by sub-
jective interaction experiments. It is yet un-
clear whether realistic user behavior can also be
generated for more advanced systems and do-
mains, such as computer games, collaborative
problem solving systems, or educational sys-
tems. We also need to develop models that ac-
curately represent the behavior patterns of var-
ious user groups.
• Interaction parameters: Several quality aspects
are still not reflected in the current parameter
sets, e.g. indices for the quality of speech out-
put. Some approaches are described in M¨oller
and Heimansberg (2006), but the predictive
power is still too limited. In addition, many pa-
rameters still have to be derived by expert an-
notation. It may be possible to automatically
infer values for some parameters from proper-
ties of the user’s and system’s speech signals,
and such analyses may be a source for new pa-
rameters, covering new quality aspects.
• Perceptual and quality events and reference:
These items are subject of ongoing research in
related disciplines, such as speech quality as-
sessment, sound quality assessment, and prod-
uct sound design. Ideas for better, more realis-
tic modeling may be derived from cooperations
with these disciplines.
</bodyText>
<listItem confidence="0.838108">
• Quality judgments and dimension descriptors:
</listItem>
<bodyText confidence="0.998997454545455">
In addition to the aspects covered by the SASSI
and P.851 questionnaires, psychologists have
defined methods for assessing cognitive load,
affect, affinity towards technology, etc. Input
from such questionnaires may provide a better
basis for developing value models.
Although a full model may be out of reach for the
next decade, a more thorough understanding of hu-
man behavior, perception and judgment processes is
not only of intrinsic interest but promises benefits
enough to make this a goal worth working towards.
</bodyText>
<page confidence="0.99794">
188
</page>
<sectionHeader confidence="0.998308" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9976855">
This work was supported in part by NSF Grant No.
0415150.
</bodyText>
<sectionHeader confidence="0.990206" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999791461538462">
M. Araki, and S. Doshita. 1997. Automatic Evaluation
Environment for Spoken Dialogue Systems. Dialogue
Processing in Spoken Language Systems, ECAI’96
Workshop Proceedings, Springer Lecture Notes in
Artificial Intelligence No. 1236, 183-194, Springer,
Berlin.
N. O. Bernsen, H. Dybkjær, and L. Dybkjær. 1998. De-
signing Interactive Speech Systems: From First Ideas
to User Testing. Springer, Berlin.
N. O. Bernsen, L. Dybkjær, L., and S. Kiilerich. 2004.
Evaluating Conversation with Hans Christian Ander-
sen. Proc. 4th Int. Conf. on Language Resources and
Evaluation (LREC 2004), 3, pp. 1011-1014, Lisbon.
K.-P. Engelbrecht, and S. M¨oller. 2007. Using Linear Re-
gression Models for the Prediction of Data Distribu-
tions. Proc. 8th SIGdial Workshop on Discourse and
Dialogue, Antwerp, pp. 291-294.
P. Heisterkamp. 2003. “Do not attempt to light with
match!”: Some Thoughts on Progress and Research
Goals in Spoken Dialog Systems. Proc. 8th Europ.
Conf. on Speech Communication and Technology (Eu-
rospeech 2003 – Switzerland).
K. S. Hone, and R. Graham. 2000. Towards a Tool for the
Subjective Assessment of Speech System Interfaces
(SASSI). Natural Language Engineering, 3(3-4): 287-
303.
ITU-T Rec. P.851. 2003. Subjective Quality Eval-
uation of Telephone Services Based on Spoken
Dialogue Systems. International Telecommunication
Union, Geneva.
ITU-T Suppl. 24 to P-Series Rec. 2005. Parameters
Describing the Interaction with Spoken Dialogue
Systems. International Telecommunication Union,
Geneva.
ISO Standard 9241 Part 11. 1998. Ergonomic Require-
ments for Office Work with Visual Display Terminals
(VDTs) – Part 11: Guidance on Usability. Interna-
tional Organization for Standardization, Geneva.
U. Jekosch. 2000. Sprache h¨oren und beur-
teilen: Ein Ansatz zur Grundlegung der
Sprachqualit¨atsbeurteilung. Habilitation thesis
(unpublished), Universit¨at/Gesamthochschule, Essen.
R. L´opez-C´ozar, A. De la Torre, J. Segura, and A. Rubio.
2003. Assessment of Dialog Systems by Means of a
New Simulation Technique. Speech Communication,
40: 387-407.
S. M¨oller, P. Smeele, H. Boland, and J. Krebber. 2007.
Evaluating Spoken Dialogue Systems According to
De-Facto Standards: A Case Study. Computer Speech
and Language, 21: 26-53.
S. M¨oller, R. Englert, K.-P. Engelbrecht, V. Hafner,
A. Jameson, A. Oulasvirta, A. Raake, and N. Rei-
thinger. 2006. MeMo: Towards Automatic Usability
Evaluation of Spoken Dialogue Services by User Er-
ror Simulations. Proc. 9th Int. Conf. on Spoken Lan-
guage Processing (Interspeech 2006 – ICSLP), Pitts-
burgh PA, pp. 1786-1789.
S. M¨oller, and J. Heimansberg. 2006. Estimation of
TTS Quality in Telephone Environments Using a
Reference-free Quality Prediction Model. Proc. 2nd
ISCA/DEGA Tutorial and Research Workshop on Per-
ceptual Quality of Systems, Berlin, pp. 56-60.
S. M¨oller. 2005. Quality of Telephone-Based Spoken Di-
alogue Systems. Springer, New York NY.
S. M¨oller. 2005b. Perceptual Quality Dimensions of Spo-
ken Dialogue Systems: A Review and New Exper-
imental Results. Proc. 4th European Congress on
Acoustics (Forum Acusticum Budapest 2005), Bu-
dapest, pp. 2681-2686.
A. Oulasvirta, S. M¨oller, K.-P. Engelbrecht, and A. Jame-
son. 2006. The Relationship of User Errors to Per-
ceived Usability of a Spoken Dialogue System. Proc.
2nd ISCA/DEGA Tutorial and Research Workshop on
Perceptual Quality of Systems, Berlin, pp. 61-67.
T. Paek. 2007. Toward Evaluation that Leads to Best
Practices: Reconciling Dialog Evaluation in Research
and Industry. Bridging the Gap: Academic and Indus-
trial Research in Dialog Technologies Workshop Pro-
ceedings, Rochester, pp. 40-47.
R. Pieraccini, J. Huerta. 2005. Where Do We and Com-
mercial Spoken Dialog Systems. Proc. 6th SIGdial
Workshop on Discourse and Dialogue, Lisbon, pp. 1-
10.
A. W. Rix, J. G. Beerends, D.-S. Kim, P. Kroon, and
O. Ghitza. 2006. Objective Assessment of Speech and
Audio Quality – Technology and Applications. IEEE
Trans. Audio, Speech, Lang. Process, 14: 1890-1901.
M. Walker, C. Kamm, and D. Litman. 2000. Towards
Developing General Models of Usability with PAR-
ADISE. Natural Language Engineering, 6: 363-377.
M. A. Walker, D. J. Litman, C. A. Kamm, and A. Abella.
1997. PARADISE: A Framework for Evaluating Spo-
ken Dialogue Agents. Proc. of the ACL/EACL 35th
Ann. Meeting of the Assoc. for Computational Linguis-
tics, Madrid, Morgan Kaufmann, San Francisco CA,
pp. 271-280.
N. Ward, A. G. Rivera, K. Ward, and D. G. Novick. 2005.
Root Causes of Lost Time and User Stress in a Simple
Dialog System. Proc. 9th European Conf. on Speech
Communication and Technology (Interspeech 2005),
Lisboa.
N. Ward, and W. Tsukahara. 2003. A Study in Respon-
siveness in Spoken Dialogue. International Journal of
Human-Computer Studies, 59: 603-630.
</reference>
<page confidence="0.998924">
189
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.230484">
<title confidence="0.999916">A Framework for Model-based Evaluation of Spoken Dialog Systems</title>
<author confidence="0.911296">Sebastian</author>
<affiliation confidence="0.704524">Deutsche Telekom Technische Universit¨at</affiliation>
<address confidence="0.934922">10587 Berlin,</address>
<email confidence="0.997437">sebastian.moeller@telekom.de</email>
<affiliation confidence="0.868351333333333">Nigel G. Computer Science University of Texas at El</affiliation>
<address confidence="0.900421">El Paso, Texas 79968,</address>
<email confidence="0.98497">nigelward@acm.org</email>
<abstract confidence="0.996140388888889">Improvements in the quality, usability and acceptability of spoken dialog systems can be facilitated by better evaluation methods. To support early and efficient evaluation of dialog systems and their components, this paper presents a tripartite framework describing the evaluation problem. One part models the behavior of user and system during the interaction, the second one the perception and judgment processes taking place inside the user, and the third part models what matters to system designers and service providers. The paper reviews available approaches for some of the model parts, and indicates how anticipated improvements may serve not only developers and users but also researchers working on advanced dialog functions and features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Araki</author>
<author>S Doshita</author>
</authors>
<title>Automatic Evaluation Environment for Spoken Dialogue Systems.</title>
<date>1997</date>
<booktitle>Dialogue Processing in Spoken Language Systems, ECAI’96 Workshop Proceedings, Springer Lecture Notes in Artificial Intelligence No.</booktitle>
<volume>1236</volume>
<pages>183--194</pages>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="2572" citStr="Araki and Doshita, 1997" startWordPosition="388" endWordPosition="391">luation renders the process more efficient and less dependent on experience, hunches and intuitions. With the help of such models predicting the outcome of user tests, the need for subjective testing can be reduced, restricting it to that subset of the possible systems which have already been vetted in an automatic or semi-automatic way. Several approaches have already been presented for semi-automatic evaluation. For example, the PARADISE framework (Walker et al., 1997) predicts the effects of system changes, quantified in terms of interaction parameters, on an average user judgment. Others (Araki and Doshita, 1997; L´opez-C´ozar et al., 2003; M¨oller et al., 2006) have developed dialog simulations to aid system optimization. However the big picture has been missing: there has been no clear view of how these methods relate to each other, and how they might be improved and joined to support efficient early evaluation. The remainder of this paper is organized as follows. Section 2 gives a brief review of different evaluation purposes and terminology, and outlines a new tripartite decomposition of the evaluation problem. One part of our framework models the behavior of user and system during the interactio</context>
<context position="12559" citStr="Araki and Doshita (1997)" startWordPosition="1979" endWordPosition="1982">sk and uncertainty involved in dialogue design, thereby decreasing the gap between research and commercial work on dialog system usability (Heisterkamp, 2003; Pieraccini and Huerta, 2005). Whereas modeling system behavior in response to user input is clearly possible (since in the last resort it is possible to fully implement the system), user behavior can probably not be modeled in closed form, because it unavoidably relates to the intricacies of the user and reflects the time-flow of the interaction. Thus, it seems necessary to employ a simulation of the interaction, as has been proposed by Araki and Doshita (1997) and L´opez-C´ozar et al. (2003), among others. One embodiment of this idea is the MeMo workbench (M¨oller et al., 2006), which is based on the idea of running models of the system and of the user in a dedicated usability testing workbench. The system model is a description of the possible tasks (system task model) plus a description of the system’s interaction behavior (system interaction model). The user model is a description of the tasks a user would want to carry out with the system (user task model) plus a description of the steps s/he would take to reach the goal when faced with the sys</context>
</contexts>
<marker>Araki, Doshita, 1997</marker>
<rawString>M. Araki, and S. Doshita. 1997. Automatic Evaluation Environment for Spoken Dialogue Systems. Dialogue Processing in Spoken Language Systems, ECAI’96 Workshop Proceedings, Springer Lecture Notes in Artificial Intelligence No. 1236, 183-194, Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N O Bernsen</author>
<author>H Dybkjær</author>
<author>L Dybkjær</author>
</authors>
<title>Designing Interactive Speech Systems: From First Ideas to User Testing.</title>
<date>1998</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="10537" citStr="Bernsen et al., 1998" startWordPosition="1657" endWordPosition="1660">in general, meta-communication, cooperativity, task, and speech-input performance (M¨oller, 2005). Unfortunately, it as is yet unclear which of these parameters relate to quality from a user’s point-of-view. In addition, some metrics are missing which address critical aspects for the user, e.g. parameters for the quality and attractiveness of the speech output. Another manageable way to describe system behavior is to focus on interaction phenomena. Several schemes have been developed for classifying such phenomena, such as system errors, user errors, points of confusion, dead time, and so on (Bernsen et al., 1998; Ward et al., 2005; Oulasvirta et al., 2006). Patterns of interaction phenomena may be reflected in interaction parameter values, and may be identified on that basis. Otherwise, they have to be determined by experts and/or users, by means of observation, interviews, thinking-aloud, and other techniques from usability engineering. (Using this terminology we can understand the practice of usability testing as being the identification of interaction phenomena, also known as “usability events” or “critical incidences”, and using these to estimate specific quality aspects or the overall value of t</context>
</contexts>
<marker>Bernsen, Dybkjær, Dybkjær, 1998</marker>
<rawString>N. O. Bernsen, H. Dybkjær, and L. Dybkjær. 1998. Designing Interactive Speech Systems: From First Ideas to User Testing. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N O Bernsen</author>
<author>L Dybkjær</author>
<author>L</author>
<author>S Kiilerich</author>
</authors>
<title>Evaluating Conversation with Hans Christian Andersen.</title>
<date>2004</date>
<booktitle>Proc. 4th Int. Conf. on Language Resources and Evaluation (LREC 2004), 3,</booktitle>
<pages>1011--1014</pages>
<location>Lisbon.</location>
<contexts>
<context position="18596" citStr="Bernsen et al., 2004" startWordPosition="2926" endWordPosition="2929">s of factor analysis. A summary of such multidimensional analyses in M¨oller (2005b) reveals that users’ perceptions of quality and usability can be decomposed into around 5 to 8 dimensions. The resulting dimensions include factors such as overall acceptability, task effectiveness, speed, cognitive effort, and joy-of-use. It should be noted that most such efforts have considered task-oriented systems, where effectiveness, efficiency, and success are obviously important, however these dimensions may be less relevant to systems designed for other purposes, for example tutoring or “edutainment” (Bernsen et al., 2004), and additional factors may be needed for such applications. In order to describe the impact of the interaction flow on user-perceived quality, or on some of its sub-dimensions, we would ideally model the human perception and judgment processes. Such an approach has the clear advantage that the resulting model would be generic, i.e. applicable to different systems and potentially for different user groups, and also analytic, i.e. able to explain why certain interaction characteristics have a positive or negative impact on perceived quality. Unfortunately, the perception and judgment processes</context>
</contexts>
<marker>Bernsen, Dybkjær, L, Kiilerich, 2004</marker>
<rawString>N. O. Bernsen, L. Dybkjær, L., and S. Kiilerich. 2004. Evaluating Conversation with Hans Christian Andersen. Proc. 4th Int. Conf. on Language Resources and Evaluation (LREC 2004), 3, pp. 1011-1014, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-P Engelbrecht</author>
<author>S M¨oller</author>
</authors>
<title>Using Linear Regression Models for the Prediction of Data Distributions.</title>
<date>2007</date>
<booktitle>Proc. 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>291--294</pages>
<location>Antwerp,</location>
<marker>Engelbrecht, M¨oller, 2007</marker>
<rawString>K.-P. Engelbrecht, and S. M¨oller. 2007. Using Linear Regression Models for the Prediction of Data Distributions. Proc. 8th SIGdial Workshop on Discourse and Dialogue, Antwerp, pp. 291-294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Heisterkamp</author>
</authors>
<title>Do not attempt to light with match!”:</title>
<date>2003</date>
<booktitle>Some Thoughts on Progress and Research Goals in Spoken Dialog Systems. Proc. 8th Europ. Conf. on Speech Communication and Technology (Eurospeech</booktitle>
<contexts>
<context position="12092" citStr="Heisterkamp, 2003" startWordPosition="1904" endWordPosition="1905"> interaction behavior, and to map from system parameters and user properties to interaction parameters or phenomena. The value of such models for a developer is clear: they could 184 enable estimation of how a change in the system (e.g. a change in the vocabulary) might affect the interaction properties. In addition to the desired effects, the side-effects of system changes are also important. Predicting such side-effects will substantially decrease the risk and uncertainty involved in dialogue design, thereby decreasing the gap between research and commercial work on dialog system usability (Heisterkamp, 2003; Pieraccini and Huerta, 2005). Whereas modeling system behavior in response to user input is clearly possible (since in the last resort it is possible to fully implement the system), user behavior can probably not be modeled in closed form, because it unavoidably relates to the intricacies of the user and reflects the time-flow of the interaction. Thus, it seems necessary to employ a simulation of the interaction, as has been proposed by Araki and Doshita (1997) and L´opez-C´ozar et al. (2003), among others. One embodiment of this idea is the MeMo workbench (M¨oller et al., 2006), which is ba</context>
</contexts>
<marker>Heisterkamp, 2003</marker>
<rawString>P. Heisterkamp. 2003. “Do not attempt to light with match!”: Some Thoughts on Progress and Research Goals in Spoken Dialog Systems. Proc. 8th Europ. Conf. on Speech Communication and Technology (Eurospeech 2003 – Switzerland).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Hone</author>
<author>R Graham</author>
</authors>
<title>Towards a Tool for the Subjective Assessment of Speech System Interfaces (SASSI).</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>3</volume>
<issue>3</issue>
<pages>287--303</pages>
<contexts>
<context position="17307" citStr="Hone and Graham, 2000" startWordPosition="2729" endWordPosition="2732">he importance of each such quality aspect. We may take advantage of this by defining a small set of universal perceptual quality dimensions, that together are sufficient for predicting system value from the user’s point-of-view. In order to quantify the quality event and to identify perceptual quality dimensions, psychometric measurement methods are needed, e.g. interaction experiments with appropriate measurement scales. Several attempts have been made to come up with a common questionnaire for user perception measurement related to spoken dialog systems, for example the SASSI questionnaire (Hone and Graham, 2000) for systems using speech input, and the ITUstandard augmented framework for questionnaires (ITU-T Rec. P.851, 2003) for systems with both speech-input and speech-output capabilities. Studies of the validity and the reliability of these questionnaires (M¨oller et al., 2007) show that both SASSI and P.851 can cover a large number of different quality and usability dimensions with a high validity, and mainly with adequate reliability, although the generalizability of these results remains to be shown. On the basis of batteries of user judgments obtained with these questionnaires, dimension descr</context>
</contexts>
<marker>Hone, Graham, 2000</marker>
<rawString>K. S. Hone, and R. Graham. 2000. Towards a Tool for the Subjective Assessment of Speech System Interfaces (SASSI). Natural Language Engineering, 3(3-4): 287-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ITU-T Rec</author>
</authors>
<title>Subjective Quality Evaluation of Telephone Services Based on Spoken Dialogue Systems. International Telecommunication</title>
<date>2003</date>
<location>Union, Geneva.</location>
<marker>Rec, 2003</marker>
<rawString>ITU-T Rec. P.851. 2003. Subjective Quality Evaluation of Telephone Services Based on Spoken Dialogue Systems. International Telecommunication Union, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ITU-T Suppl</author>
</authors>
<title>24 to P-Series Rec.</title>
<date>2005</date>
<location>Union, Geneva.</location>
<marker>Suppl, 2005</marker>
<rawString>ITU-T Suppl. 24 to P-Series Rec. 2005. Parameters Describing the Interaction with Spoken Dialogue Systems. International Telecommunication Union, Geneva.</rawString>
</citation>
<citation valid="true">
<title>Ergonomic Requirements for Office Work with Visual Display Terminals (VDTs) – Part 11: Guidance on Usability. International Organization for Standardization,</title>
<date>1998</date>
<journal>ISO Standard 9241 Part</journal>
<volume>11</volume>
<location>Geneva.</location>
<contexts>
<context position="5656" citStr="(1998)" startWordPosition="891" endWordPosition="891">hus involves a perception process and a judgment process, during which the perceiving person compares the perceptual event with a (typically implicit) reference. It is the comparison with a reference which associates a user-specific value to the perceptual event. The perception and the comparison processes take place in a particular context of use. Thus, both perception and quality should be regarded as “events” which happen in a particular personal, spatial, temporal and functional context. Usability is one sub-aspect of the quality of the system. Following the definition in ISO 9241 Part 11 (1998), usability is considered as the “extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use”. Usability is degraded when interaction problems occur. Such problems influence the perceptual event of the user interacting with the system, and consequently the quality s/he associates with the system as a whole. This may have consequences for the acceptability or the system or service, that is, how readily a customer will use the system or service. This can be quantified, for example as the ratio </context>
</contexts>
<marker>1998</marker>
<rawString>ISO Standard 9241 Part 11. 1998. Ergonomic Requirements for Office Work with Visual Display Terminals (VDTs) – Part 11: Guidance on Usability. International Organization for Standardization, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Jekosch</author>
</authors>
<title>Sprache h¨oren und beurteilen: Ein Ansatz zur Grundlegung der Sprachqualit¨atsbeurteilung. Habilitation thesis (unpublished),</title>
<date>2000</date>
<location>Universit¨at/Gesamthochschule, Essen.</location>
<contexts>
<context position="4803" citStr="Jekosch (2000)" startWordPosition="757" endWordPosition="758"> function and an appropriate measure for quantifying the degree of fulfillment may easily be determined for certain components — e.g. word accuracy for a speech recognizer or concept error rate for a speech understanding module — but it is harder to specify for other components, such as a dialog manager or an output generation module. However, definitive measures of component quality are not always necessary: what matters for such a module is its contribution to the quality of the entire interaction, as it is perceived by the user. We follow the definition of the term quality as introduced by Jekosch (2000) and now accepted for telephone-based spoken dialog services by the International Telecommunication Union in ITU-T Rec. P.851 (2003): “Result of judgment of the perceived composition of an entity with respect to its desired composition”. Quality thus involves a perception process and a judgment process, during which the perceiving person compares the perceptual event with a (typically implicit) reference. It is the comparison with a reference which associates a user-specific value to the perceptual event. The perception and the comparison processes take place in a particular context of use. Th</context>
</contexts>
<marker>Jekosch, 2000</marker>
<rawString>U. Jekosch. 2000. Sprache h¨oren und beurteilen: Ein Ansatz zur Grundlegung der Sprachqualit¨atsbeurteilung. Habilitation thesis (unpublished), Universit¨at/Gesamthochschule, Essen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L´opez-C´ozar</author>
<author>A De la Torre</author>
<author>J Segura</author>
<author>A Rubio</author>
</authors>
<title>Assessment of Dialog Systems by Means of a New Simulation Technique.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<volume>40</volume>
<pages>387--407</pages>
<marker>L´opez-C´ozar, Torre, Segura, Rubio, 2003</marker>
<rawString>R. L´opez-C´ozar, A. De la Torre, J. Segura, and A. Rubio. 2003. Assessment of Dialog Systems by Means of a New Simulation Technique. Speech Communication, 40: 387-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M¨oller</author>
<author>P Smeele</author>
<author>H Boland</author>
<author>J Krebber</author>
</authors>
<title>Evaluating Spoken Dialogue Systems According to De-Facto Standards: A Case Study.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<pages>26--53</pages>
<marker>M¨oller, Smeele, Boland, Krebber, 2007</marker>
<rawString>S. M¨oller, P. Smeele, H. Boland, and J. Krebber. 2007. Evaluating Spoken Dialogue Systems According to De-Facto Standards: A Case Study. Computer Speech and Language, 21: 26-53.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S M¨oller</author>
<author>R Englert</author>
<author>K-P Engelbrecht</author>
<author>V Hafner</author>
</authors>
<marker>M¨oller, Englert, Engelbrecht, Hafner, </marker>
<rawString>S. M¨oller, R. Englert, K.-P. Engelbrecht, V. Hafner,</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jameson</author>
<author>A Oulasvirta</author>
<author>A Raake</author>
<author>N Reithinger</author>
</authors>
<title>MeMo: Towards Automatic Usability Evaluation of Spoken Dialogue Services by User Error Simulations.</title>
<date>2006</date>
<booktitle>Proc. 9th Int. Conf. on Spoken Language Processing (Interspeech 2006 – ICSLP),</booktitle>
<pages>1786--1789</pages>
<location>Pittsburgh PA,</location>
<marker>Jameson, Oulasvirta, Raake, Reithinger, 2006</marker>
<rawString>A. Jameson, A. Oulasvirta, A. Raake, and N. Reithinger. 2006. MeMo: Towards Automatic Usability Evaluation of Spoken Dialogue Services by User Error Simulations. Proc. 9th Int. Conf. on Spoken Language Processing (Interspeech 2006 – ICSLP), Pittsburgh PA, pp. 1786-1789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M¨oller</author>
<author>J Heimansberg</author>
</authors>
<title>Estimation of TTS Quality in Telephone Environments Using a Reference-free Quality Prediction Model.</title>
<date>2006</date>
<booktitle>Proc. 2nd ISCA/DEGA Tutorial and Research Workshop on Perceptual Quality of Systems,</booktitle>
<pages>56--60</pages>
<location>Berlin,</location>
<marker>M¨oller, Heimansberg, 2006</marker>
<rawString>S. M¨oller, and J. Heimansberg. 2006. Estimation of TTS Quality in Telephone Environments Using a Reference-free Quality Prediction Model. Proc. 2nd ISCA/DEGA Tutorial and Research Workshop on Perceptual Quality of Systems, Berlin, pp. 56-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M¨oller</author>
</authors>
<title>Quality of Telephone-Based Spoken Dialogue Systems.</title>
<date>2005</date>
<publisher>Springer,</publisher>
<location>New York NY.</location>
<marker>M¨oller, 2005</marker>
<rawString>S. M¨oller. 2005. Quality of Telephone-Based Spoken Dialogue Systems. Springer, New York NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M¨oller</author>
</authors>
<title>Perceptual Quality Dimensions of Spoken Dialogue Systems: A Review and New Experimental Results.</title>
<date>2005</date>
<booktitle>Proc. 4th European Congress on Acoustics (Forum Acusticum</booktitle>
<pages>2681--2686</pages>
<location>Budapest</location>
<marker>M¨oller, 2005</marker>
<rawString>S. M¨oller. 2005b. Perceptual Quality Dimensions of Spoken Dialogue Systems: A Review and New Experimental Results. Proc. 4th European Congress on Acoustics (Forum Acusticum Budapest 2005), Budapest, pp. 2681-2686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Oulasvirta</author>
<author>S M¨oller</author>
<author>K-P Engelbrecht</author>
<author>A Jameson</author>
</authors>
<title>The Relationship of User Errors to Perceived Usability of a Spoken Dialogue System.</title>
<date>2006</date>
<booktitle>Proc. 2nd ISCA/DEGA Tutorial and Research Workshop on Perceptual Quality of Systems,</booktitle>
<pages>61--67</pages>
<location>Berlin,</location>
<marker>Oulasvirta, M¨oller, Engelbrecht, Jameson, 2006</marker>
<rawString>A. Oulasvirta, S. M¨oller, K.-P. Engelbrecht, and A. Jameson. 2006. The Relationship of User Errors to Perceived Usability of a Spoken Dialogue System. Proc. 2nd ISCA/DEGA Tutorial and Research Workshop on Perceptual Quality of Systems, Berlin, pp. 61-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
</authors>
<title>Toward Evaluation that Leads to Best Practices: Reconciling Dialog Evaluation</title>
<date>2007</date>
<booktitle>in Research and Industry. Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings,</booktitle>
<pages>40--47</pages>
<location>Rochester,</location>
<contexts>
<context position="6505" citStr="Paek, 2007" startWordPosition="1032" endWordPosition="1033">roblems occur. Such problems influence the perceptual event of the user interacting with the system, and consequently the quality s/he associates with the system as a whole. This may have consequences for the acceptability or the system or service, that is, how readily a customer will use the system or service. This can be quantified, for example as the ratio of the potential user population to the size of the target group. It is the task of any evaluation to quantify aspects of system performance, quality, usability or acceptability. The exact target depends on the purpose of the evaluation (Paek, 2007). For example, the system developer might be most interested in quantifying the performance of the system and its components; s/he might further need to know how the performance affects the quality perceived by the user. In contrast, the service operator might instead be most interested in the acceptability of the service. S/he might further want to know about the satisfaction of the user, influenced by the usability of the system, and also by other (e.g. hedonic) aspects like comfort, joy-of-use, fashion, etc. Different evaluation approaches may be complementary, in the sense that metrics det</context>
</contexts>
<marker>Paek, 2007</marker>
<rawString>T. Paek. 2007. Toward Evaluation that Leads to Best Practices: Reconciling Dialog Evaluation in Research and Industry. Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, Rochester, pp. 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pieraccini</author>
<author>J Huerta</author>
</authors>
<title>Where Do We and Commercial Spoken Dialog Systems.</title>
<date>2005</date>
<booktitle>Proc. 6th SIGdial Workshop on Discourse and Dialogue, Lisbon,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="12122" citStr="Pieraccini and Huerta, 2005" startWordPosition="1906" endWordPosition="1909">or, and to map from system parameters and user properties to interaction parameters or phenomena. The value of such models for a developer is clear: they could 184 enable estimation of how a change in the system (e.g. a change in the vocabulary) might affect the interaction properties. In addition to the desired effects, the side-effects of system changes are also important. Predicting such side-effects will substantially decrease the risk and uncertainty involved in dialogue design, thereby decreasing the gap between research and commercial work on dialog system usability (Heisterkamp, 2003; Pieraccini and Huerta, 2005). Whereas modeling system behavior in response to user input is clearly possible (since in the last resort it is possible to fully implement the system), user behavior can probably not be modeled in closed form, because it unavoidably relates to the intricacies of the user and reflects the time-flow of the interaction. Thus, it seems necessary to employ a simulation of the interaction, as has been proposed by Araki and Doshita (1997) and L´opez-C´ozar et al. (2003), among others. One embodiment of this idea is the MeMo workbench (M¨oller et al., 2006), which is based on the idea of running mod</context>
</contexts>
<marker>Pieraccini, Huerta, 2005</marker>
<rawString>R. Pieraccini, J. Huerta. 2005. Where Do We and Commercial Spoken Dialog Systems. Proc. 6th SIGdial Workshop on Discourse and Dialogue, Lisbon, pp. 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A W Rix</author>
<author>J G Beerends</author>
<author>D-S Kim</author>
<author>P Kroon</author>
<author>O Ghitza</author>
</authors>
<title>Objective Assessment of Speech and Audio Quality – Technology and Applications.</title>
<date>2006</date>
<journal>IEEE Trans. Audio, Speech, Lang. Process,</journal>
<volume>14</volume>
<pages>1890--1901</pages>
<contexts>
<context position="19609" citStr="Rix et al., 2006" startWordPosition="3085" endWordPosition="3088">t user groups, and also analytic, i.e. able to explain why certain interaction characteristics have a positive or negative impact on perceived quality. Unfortunately, the perception and judgment processes involved in spokendialog interaction are not yet well understood, as compared, for example, to those involved in listening to transmitted speech samples and judging their quality. For the latter, models are available which estimate quality with the help of peripheral auditory perception models and a signal-based comparison of representations of the perceptual event and the assumed reference (Rix et al., 2006). They are able to estimate user judgments on “overall quality” with an average correlation of around 0.93, and are widely used for planning, implementing and monitoring telephone networks. For interactions with spoken dialog systems, the situation is more complicated, as the perceptual events depend on the interaction between user and systems, and not on one speech signal alone. A way out is not to worry about the perception processes, and instead to use simple linear regression models for predicting an average user judgment from various interaction parameters. The most widely used framework </context>
</contexts>
<marker>Rix, Beerends, Kim, Kroon, Ghitza, 2006</marker>
<rawString>A. W. Rix, J. G. Beerends, D.-S. Kim, P. Kroon, and O. Ghitza. 2006. Objective Assessment of Speech and Audio Quality – Technology and Applications. IEEE Trans. Audio, Speech, Lang. Process, 14: 1890-1901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>C Kamm</author>
<author>D Litman</author>
</authors>
<title>Towards Developing General Models of Usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<pages>363--377</pages>
<contexts>
<context position="21715" citStr="Walker et al., 2000" startWordPosition="3413" endWordPosition="3416">elopers. For example, a recent investigation showed that the model can be used to effectively determine the minimum acceptable recognition rate for a smart-home system, leading to the same critical threshold as that obtained from user judgments (Engelbrecht and M¨oller, 2007). However, experience also shows that the PARADISE framework does not reliably give valid predictions of individual user judgments, typically covering only around 40-50% of the variance in the data it is trained on. The generality is also limited: crosssystem extrapolation works sometimes but other times has low accuracy (Walker et al., 2000; M¨oller, 2005). These limitations are easy to understand in terms of Figure 1: over-ambitious attempts to directly relate interaction parameters to a measure of overall system value seem unlikely to succeed in general. Thus it seems wise to limit the scope of the perception and judgment component to the prediction of values on the perceptual quality dimensions. In any case, there are several ways in which such models could be improved. One issue is that a linear combination of factors is probably not generally adequate. For example, parameters like the number of turns required to execute a s</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>M. Walker, C. Kamm, and D. Litman. 2000. Towards Developing General Models of Usability with PARADISE. Natural Language Engineering, 6: 363-377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>D J Litman</author>
<author>C A Kamm</author>
<author>A Abella</author>
</authors>
<title>PARADISE: A Framework for Evaluating Spoken Dialogue Agents.</title>
<date>1997</date>
<booktitle>Proc. of the ACL/EACL 35th Ann. Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>271--280</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>Madrid,</location>
<contexts>
<context position="2424" citStr="Walker et al., 1997" startWordPosition="366" endWordPosition="369"> as early as possible in the design and development process is critical for improving quality, reducing costs and fostering innovation. Early evaluation renders the process more efficient and less dependent on experience, hunches and intuitions. With the help of such models predicting the outcome of user tests, the need for subjective testing can be reduced, restricting it to that subset of the possible systems which have already been vetted in an automatic or semi-automatic way. Several approaches have already been presented for semi-automatic evaluation. For example, the PARADISE framework (Walker et al., 1997) predicts the effects of system changes, quantified in terms of interaction parameters, on an average user judgment. Others (Araki and Doshita, 1997; L´opez-C´ozar et al., 2003; M¨oller et al., 2006) have developed dialog simulations to aid system optimization. However the big picture has been missing: there has been no clear view of how these methods relate to each other, and how they might be improved and joined to support efficient early evaluation. The remainder of this paper is organized as follows. Section 2 gives a brief review of different evaluation purposes and terminology, and outli</context>
<context position="20292" citStr="Walker et al., 1997" startWordPosition="3193" endWordPosition="3196"> with an average correlation of around 0.93, and are widely used for planning, implementing and monitoring telephone networks. For interactions with spoken dialog systems, the situation is more complicated, as the perceptual events depend on the interaction between user and systems, and not on one speech signal alone. A way out is not to worry about the perception processes, and instead to use simple linear regression models for predicting an average user judgment from various interaction parameters. The most widely used framework designed to support this sort of early evaluation is PARADISE (Walker et al., 1997). The target variable of PARADISE is an average of several user judgments (labeled “user satisfaction”) of different system and interaction aspects, such as system voice, perceived system understanding, task ease, interaction pace, or the transparency of the interaction. The interaction parameters are of three types, those relating to efficiency (including elapsed time and the number of turns), those relating to “dialog quality” (including mean recognition score and the number of timeouts and rejections), and a measure of effectiveness (task success). The model can be trained on data, and the </context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>M. A. Walker, D. J. Litman, C. A. Kamm, and A. Abella. 1997. PARADISE: A Framework for Evaluating Spoken Dialogue Agents. Proc. of the ACL/EACL 35th Ann. Meeting of the Assoc. for Computational Linguistics, Madrid, Morgan Kaufmann, San Francisco CA, pp. 271-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ward</author>
<author>A G Rivera</author>
<author>K Ward</author>
<author>D G Novick</author>
</authors>
<title>Root Causes of Lost Time and User Stress in a Simple Dialog System.</title>
<date>2005</date>
<booktitle>Proc. 9th European Conf. on Speech Communication and Technology (Interspeech 2005),</booktitle>
<location>Lisboa.</location>
<contexts>
<context position="10556" citStr="Ward et al., 2005" startWordPosition="1661" endWordPosition="1664">nication, cooperativity, task, and speech-input performance (M¨oller, 2005). Unfortunately, it as is yet unclear which of these parameters relate to quality from a user’s point-of-view. In addition, some metrics are missing which address critical aspects for the user, e.g. parameters for the quality and attractiveness of the speech output. Another manageable way to describe system behavior is to focus on interaction phenomena. Several schemes have been developed for classifying such phenomena, such as system errors, user errors, points of confusion, dead time, and so on (Bernsen et al., 1998; Ward et al., 2005; Oulasvirta et al., 2006). Patterns of interaction phenomena may be reflected in interaction parameter values, and may be identified on that basis. Otherwise, they have to be determined by experts and/or users, by means of observation, interviews, thinking-aloud, and other techniques from usability engineering. (Using this terminology we can understand the practice of usability testing as being the identification of interaction phenomena, also known as “usability events” or “critical incidences”, and using these to estimate specific quality aspects or the overall value of the system.) Obtaini</context>
</contexts>
<marker>Ward, Rivera, Ward, Novick, 2005</marker>
<rawString>N. Ward, A. G. Rivera, K. Ward, and D. G. Novick. 2005. Root Causes of Lost Time and User Stress in a Simple Dialog System. Proc. 9th European Conf. on Speech Communication and Technology (Interspeech 2005), Lisboa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ward</author>
<author>W Tsukahara</author>
</authors>
<title>A Study in Responsiveness in Spoken Dialogue.</title>
<date>2003</date>
<journal>International Journal of Human-Computer Studies,</journal>
<volume>59</volume>
<pages>603--630</pages>
<contexts>
<context position="15331" citStr="Ward and Tsukahara, 2003" startWordPosition="2419" endWordPosition="2422"> obtained from the (simulated) interaction problems. The simulations can also support reinforcement learning or other methods for automatically determining the best dialog strategy. Building user interaction models by hand is costly. As an alternative to explicitly defining rules and probabilities, simulations can be based on data sets of actual interactions, augmented with annotations such as indications of the dialog state, current subtask, inferred user state, and interaction phenomena. Annotations can be generated by the dialog participants themselves, e.g. by re-listening after the fact (Ward and Tsukahara, 2003), or by top communicators, decision-makers, trend-setters, experts in linguistics and communication, and the like. Machine learning techniques can help by providing predictions of how users tend to react in various situations from lightly annotated data. 4 Perception and Judgment Model Once the interaction behavior is determined, the evaluator needs to know about the impact it has on the quality perceived by the user. As pointed out in Section 2, the perception and judgments processes take place in the human user and are thus hidden from the observer. The evaluator may, however, ask the user t</context>
</contexts>
<marker>Ward, Tsukahara, 2003</marker>
<rawString>N. Ward, and W. Tsukahara. 2003. A Study in Responsiveness in Spoken Dialogue. International Journal of Human-Computer Studies, 59: 603-630.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>