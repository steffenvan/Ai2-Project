<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006894">
<title confidence="0.9979295">
Data Quality from Crowdsourcing:
A Study of Annotation Selection Criteria
</title>
<author confidence="0.91975">
Pei-Yun Hsueh, Prem Melville, Vikas Sindhwani
</author>
<affiliation confidence="0.72013">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.828242">
1101 Kitchawan Road, Route 134
Yorktown Heights, NY 10598, USA
</address>
<sectionHeader confidence="0.970803" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999861739130435">
Annotation acquisition is an essential step in
training supervised classifiers. However, man-
ual annotation is often time-consuming and
expensive. The possibility of recruiting anno-
tators through Internet services (e.g., Amazon
Mechanic Turk) is an appealing option that al-
lows multiple labeling tasks to be outsourced
in bulk, typically with low overall costs and
fast completion rates. In this paper, we con-
sider the difficult problem of classifying sen-
timent in political blog snippets. Annotation
data from both expert annotators in a research
lab and non-expert annotators recruited from
the Internet are examined. Three selection cri-
teria are identified to select high-quality anno-
tations: noise level, sentiment ambiguity, and
lexical uncertainty. Analysis confirm the util-
ity of these criteria on improving data quality.
We conduct an empirical study to examine the
effect of noisy annotations on the performance
of sentiment classification models, and evalu-
ate the utility of annotation selection on clas-
sification accuracy and efficiency.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903046511627">
Crowdsourcing (Howe, 2008) is an attractive solu-
tion to the problem of cheaply and quickly acquir-
ing annotations for the purposes of constructing all
kinds of predictive models. To sense the potential of
crowdsourcing, consider an observation in von Ahn
et al. (2004): a crowd of 5,000 people playing an
appropriately designed computer game 24 hours a
day, could be made to label all images on Google
(425,000,000 images in 2005) in a matter of just 31
days. Several recent papers have studied the use
of annotations obtained from Amazon Mechanical
Turk, a marketplace for recruiting online workers
(Su et al., 2007; Kaisser et al., 2008; Kittur et al.,
2008; Sheng et al., 2008; Snow et al., 2008; Sorokin
and Forsyth, 2008).
With efficiency and cost-effectiveness, online re-
cruitment of anonymous annotators brings a new set
of issues to the table. These workers are not usually
specifically trained for annotation, and might not be
highly invested in producing good-quality annota-
tions. Consequently, the obtained annotations may
be noisy by nature, and might require additional val-
idation or scrutiny. Several interesting questions im-
mediately arise in how to optimally utilize annota-
tions in this setting: How does one handle differ-
ences among workers in terms of the quality of an-
notations they provide? How useful are noisy anno-
tations for the end task of creating a model? Is it pos-
sible to identify genuinely ambiguous examples via
annotator disagreements? How should these consid-
erations be treated with respect to intrinsic informa-
tiveness of examples? These questions also hint at a
strong connection to active learning, with annotation
quality as a new dimension to the problem.
As a challenging empirical testbed for these is-
sues, we consider the problem of sentiment classi-
fication on political blogs. Given a snippet drawn
from a political blog post, the desired output is a
polarity score that indicates whether the sentiment
expressed is positive or negative. Such an analysis
provides a view of the opinion around a subject of
interest, e.g., US Presidential candidates, aggregated
across the blogsphere. Recently, sentiment analy-
</bodyText>
<page confidence="0.983976">
27
</page>
<note confidence="0.9924055">
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 27–35,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.910626333333333">
sis is emerging as a critical methodology for social
media analytics. Previous research has focused on
classifying subjective-versus-objective expressions
</bodyText>
<note confidence="0.9421115">
(Wiebe et al., 2004), and also on accurate sentiment
polarity assignment (Turney, 2002; Yi et al., 2003;
Pang and Lee, 2004; Sindhwani and Melville, 2008;
Melville et al., 2009).
</note>
<bodyText confidence="0.999759970588236">
The success of most prior work relies on the qual-
ity of their knowledge bases; either lexicons defin-
ing the sentiment polarity of words around a topic
(Yi et al., 2003), or quality annotation data for sta-
tistical training. While manual intervention for com-
piling lexicons has been significantly lessened by
bootstrapping techniques (Yu and Hatzivassiloglou,
2003; Wiebe and Riloff, 2005), manual intervention
in the annotation process is harder to avoid. More-
over, the task of annotating blog-post snippets is
challenging, particularly in a charged political at-
mosphere with complex discourse spanning many
issues, use of cynicism and sarcasm, and highly
domain-specific and contextual cues. The downside
is that high-performance models are generally dif-
ficult to construct, but the upside is that annotation
and data-quality issues are more clearly exposed.
In this paper we aim to provide an empirical basis
for the use of data selection criteria in the context
of sentiment analysis in political blogs. Specifically,
we highlight the need for a set of criteria that can be
applied to screen untrustworthy annotators and se-
lect informative yet unambiguous examples for the
end goal of predictive modeling. In Section 2, we
first examine annotation data obtained by both the
expert and non-expert annotators to quantify the im-
pact of including non-experts. Then, in Section 3,
we quantify criteria that can be used to select anno-
tators and examples for selective sampling. Next, in
Section 4, we address the questions of whether the
noisy annotations are still useful for this task and
study the effect of the different selection criteria on
the performance of this task. Finally, in Section 5
we present conclusion and future work.
</bodyText>
<sectionHeader confidence="0.934865" genericHeader="method">
2 Annotating Blog Sentiment
</sectionHeader>
<bodyText confidence="0.99985475">
This section introduces the Political Blog Snippet
(PBS) corpus, describes our annotation procedure
and the sources of noise, and gives an overview of
the experiments on political snippet sentiments.
</bodyText>
<subsectionHeader confidence="0.997782">
2.1 The Political Blog Snippet Corpus
</subsectionHeader>
<bodyText confidence="0.999938260869565">
Our dataset comprises of a collection of snippets ex-
tracted from over 500,000 blog posts, spanning the
activity of 16,741 political bloggers in the time pe-
riod of Aug 15, 2008 to the election day Nov 4,
2008. A snippet was defined as a window of text
containing four consecutive sentences such that the
head sentence contained either the term “Obama”
or the term “McCain”, but both candidates were
not mentioned in the same window. The global
discourse structure of a typical political blog post
can be highly complicated with latent topics ranging
from policies (e.g., financial situation, economics,
the Iraq war) to personalities to voting preferences.
We therefore expected sentiment to be highly non-
uniform over a blog post. This snippetization proce-
dure attempts to localize the text around a presiden-
tial candidate with the objective of better estimat-
ing aggregate sentiment around them. In all, we ex-
tracted 631,224 snippets. For learning classifiers, we
passed the snippets through a stopword filter, pruned
all words that occur in less than 3 snippets and cre-
ated normalized term-frequency feature vectors over
a vocabulary of 3,812 words.
</bodyText>
<subsectionHeader confidence="0.999403">
2.2 Annotation Procedure
</subsectionHeader>
<bodyText confidence="0.999639142857143">
The annotation process consists of two steps:
Sentiment-class annotation: In the first step, as
we are only interested in detecting sentiments re-
lated to the named candidate, the annotators were
first asked to mark up the snippets irrelevant to the
named candidate’s election campaign. Then, the an-
notators were instructed to tag each relevant snippet
with one of the following four sentiment polarity la-
bels: Positive, Negative, Both, or Neutral.
Alignment annotation: In the second step, the
annotators were instructed to mark up whether each
snippet was written to support or oppose the target
candidate therein named. The motivation of adding
this tag comes from our interest in building a classi-
fication system to detect positive and negative men-
tions of each candidate. For the snippets that do
not contain a clear political alignment, the annota-
tors had the freedom to mark it as neutral or simply
not alignment-revealing.
In our pilot study many bloggers were observed
to endorse a named candidate by using negative ex-
</bodyText>
<page confidence="0.997514">
28
</page>
<bodyText confidence="0.9999154">
pressions to denounce his opponent. Therefore, in
our annotation procedure, the distinction is made
between the coding of manifest content, i.e., sen-
timents “on the surface”, and latent political align-
ment under these surface elements.
</bodyText>
<subsectionHeader confidence="0.999904">
2.3 Agreement Study
</subsectionHeader>
<bodyText confidence="0.997472333333333">
In this section, we compare the annotations obtained
from the on-site expert annotators and those from the
non-expert AMT annotators.
</bodyText>
<subsectionHeader confidence="0.89535">
2.3.1 Expert (On-site) Annotation
</subsectionHeader>
<bodyText confidence="0.999379583333333">
To assess the reliability of the sentiment annota-
tion procedure, we conducted an agreement study
with three expert annotators in our site, using 36
snippets randomly chosen from the PBS Corpus.
Overall agreement among the three annotators on
the relevance of snippets is 77.8%. Overall agree-
ment on the four-class sentiment codings is 70.4%.
Analysis indicate that the annotators agreed better
on some codings than the others. For the task of
determining whether a snippet is subjective or not1,
the annotators agreed 86.1% of the time. For the
task of determining whether a snippet is positive or
negative, they agreed 94.9% of the time.
To examine which pair of codings is the most dif-
ficult to distinguish, Table 1 summarizes the confu-
sion matrix for the three pairs of annotator’s judge-
ments on sentiment codings. Each column describes
the marginal probability of a coding and the prob-
ability distribution for this coding being recognized
as another coding (including itself). As many blog-
gers use cynical expressions in their writings, the
most confusing cases occur when the annotators
have to determine whether a snippet is “negative”
or “neutral”. The effect of cynical expressions on
</bodyText>
<table confidence="0.998642166666667">
% Neu Pos Both Neg
Marginal 21.9 20.0 10.5 47.6
Neutral (Neu) 47.8 14.3 9.1 16.0
Positive (Pos) 13.0 61.9 18.2 6.0
Both (Both) 4.4 9.5 9.1 14.0
Negative (Neg) 34.8 14.3 63.6 64.0
</table>
<tableCaption confidence="0.9927935">
Table 1: Summary matrix for the three on-site annotators’
sentiment codings.
</tableCaption>
<footnote confidence="0.9613465">
1This is done by grouping the codings of Positive, Negative,
and Both into the subjective class.
</footnote>
<bodyText confidence="0.9989112">
sentiment analysis in the political domain is also re-
vealed in the second step of alignment annotation.
Only 42.5% of the snippets have been coded with
alignment coding in the same direction as its senti-
ment coding – i.e., if a snippet is intended to support
(oppose) a target candidate, it will contain positive
(negative) sentiment. The alignment coding task has
been shown to be reliable, with the annotators agree-
ing 76.8% of the time overall on the three-level cod-
ings: Support/Against/Neutral.
</bodyText>
<subsectionHeader confidence="0.919288">
2.3.2 Amazon Mechanical Turk Annotation
</subsectionHeader>
<bodyText confidence="0.99999268">
To compare the annotation reliability between
expert and non-expert annotators, we further con-
ducted an agreement study with the annotators re-
cruited from Amazon Mechanical Turk (AMT). We
have collected 1,000 snippets overnight, with the
cost of 4 cents per annotation.
In the agreement study, a subset of 100 snippets
is used, and each snippet is annotated by five AMT
annotators. These annotations were completed by
25 annotators whom were selected based on the ap-
proval rate of their previous AMT tasks (over 95%
of times).2 The AMT annotators spent on average
40 seconds per snippet, shorter than the average of
two minutes reported by the on-site annotators. The
lower overall agreement on all four-class sentiment
codings, 35.3%, conforms to the expectation that the
non-expert annotators are less reliable. The Turk an-
notators also agreed less on the three-level alignment
codings, achieving only 47.2% of agreement.
However, a finer-grained analysis reveals that they
still agree well on some codings: The overall agree-
ment on whether a snippet is relevant, whether a
snippet is subjective or not, and whether a snippet
is positive or negative remain within a reasonable
range: 81.0%, 81.8% and 61.9% respectively.
</bodyText>
<subsectionHeader confidence="0.998804">
2.4 Gold Standard
</subsectionHeader>
<bodyText confidence="0.999929">
We defined the gold standard (GS) label of a snip-
pet in terms of the coding that receives the major-
ity votes.3 Column 1 in Table 2 (onsite-GS predic-
</bodyText>
<footnote confidence="0.804545">
2Note that we do not enforce these snippets to be annotated
by the same group of annotators. However, Kappa statistics
requires to compute the chance agreement of each annotator.
Due to the violation of this assumption, we do not measure the
intercoder agreement with Kappa in this agreement study.
3In this study, we excluded 6 snippets whose annotations
failed to reach majority vote by the three onsite annotators.
</footnote>
<page confidence="0.992541">
29
</page>
<table confidence="0.996498333333333">
onsite-GS prediction onsite agreement AMT-GS prediction AMT agreement
Sentiment (4-class) 0.767 0.704 0.614 0.353
Alignment (3-level) 0.884 0.768 0.669 0.472
Relevant or not 0.889 0.778 0.893 0.810
Subjective or not 0.931 0.861 0.898 0.818
Positive or negative 0.974 0.949 0.714 0.619
</table>
<tableCaption confidence="0.999247">
Table 2: Average prediction accuracy on gold standard (GS) using one-coder strategy and inter-coder agreement.
</tableCaption>
<bodyText confidence="0.99967496">
tion) shows the ratio of the onsite expert annotations
that are consistent with the gold standard, and Col-
umn 3 (AMT-GS prediction) shows the same for the
AMT annotations. The level of consistency, i.e., the
percentage agreement with the gold standard labels,
can be viewed as a proxy of the quality of the an-
notations. Among the AMT annotations, Columns
2 (onsite agreement) and 4 (AMT agreement) show
the pair-wise intercoder agreement in the on-site ex-
pert and AMT annotations respectively.
The results suggest that it is possible to take one
single expert annotator’s coding as the gold standard
in a number of annotation tasks using binary clas-
sification. For example, there is a 97.4% chance
that one expert’s coding on the polarity of a snip-
pet, i.e., whether it is positive or negative, will be
consistent with the gold standard coding. However,
this one-annotator strategy is less reliable with the
introduction of non-expert annotators. Take the task
of polarity annotation as an example, the intercoder
agreement among the AMT workers goes down to
61.9% and the “one-coder” strategy can only yield
71.4% accuracy. To determine reliable gold stan-
dard codings, multiple annotators are still necessary
when non-expert annotators are recruited.
</bodyText>
<sectionHeader confidence="0.940072" genericHeader="method">
3 Annotation Quality Measures
</sectionHeader>
<bodyText confidence="0.999534">
Given the noisy AMT annotations, in this section we
discuss some summary statistics that are needed to
control the quality of annotations.
</bodyText>
<subsectionHeader confidence="0.996818">
3.1 Annotator-level noise
</subsectionHeader>
<bodyText confidence="0.999989304347826">
To study the question of whether there exists a group
of annotators who tend to yield more noisy annota-
tions, we evaluate the accumulated noise level intro-
duced by each of the annotators. We define the noise
level as the deviation from the gold standard labels.
Similar to the measure of individual error rates pro-
posed in (Dawid and Skene, 1979), the noise level of
a particular annotator j, i.e., noise(annoj), is then
estimated by summing up the deviation of the an-
notations received from this annotator, with a small
sampling correction for chance disagreement. Anal-
ysis results demonstrate that there does exist a subset
of annotators who yield more noisy annotations than
the others. 20% of the annotators (who exceed the
noise level 60%) result in annotations that have 70%
disagreement with the gold standard.
In addition, we also evaluate how inclusion of
noisy annotators reduces the mean agreement with
Gold Standard. The plot (left) in Figure 1 plots the
mean agreement rate with GS over the subset of an-
notators that pass a noise threshold. These results
show that the data quality decreases with the inclu-
sion of more untrustworthy annotators.
</bodyText>
<subsectionHeader confidence="0.999379">
3.2 Snippet-level sentiment ambiguity
</subsectionHeader>
<bodyText confidence="0.999721">
We have observed that not all snippets are equally
easy to annotate, with some containing more am-
biguous expressions. To incorporate this concern in
the selection process, a key question to be answered
is whether there exist snippets whose sentiment is
substantially less distinguishable than the others.
We address this question by quantifying ambigu-
ity measures with the two key properties shown as
important in evaluating the controversiality of anno-
tation snippets (Carenini and Cheung, 2008): (1) the
strength of the annotators’ judgements and (2) the
polarity of the annotations. The measurement needs
to satisfy the constraints demonstrated in the follow-
ing snippets: (1) An example that has received three
positive codings are more ambiguous than that has
received five, and (2) an example that has received
five positive codings is more ambiguous than the one
that has received four positive and one negative cod-
ing. In addition, as some snippets were shown to
</bodyText>
<page confidence="0.954303">
30
</page>
<figure confidence="0.985232333333333">
Sentiment Ambigity
Lexical Uncertainty
Annotator Noise
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8
Prediction Accuracy
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
Prediction Accuracy
0.0 0.2 0.4 0.6 0.8 1.0
Annotator noise level Lexical Uncertainty
</figure>
<figureCaption confidence="0.994947">
Figure 1: Data quality (consistency with GS) as a function of noise level (left), sentiment ambiguity (middle), and
lexical uncertainty (right).
</figureCaption>
<bodyText confidence="0.9990533125">
be difficult to tell whether they contain negative or
neutral sentiment, the measure of example ambigu-
ity has to go beyond controversiality and incorporate
codings of “neutral” and “both”.
To satisfy these constraints, we first enumerated
through the codings of each snippet and counted
the number of neutral, positive, both, and negative
codings: We added (1) one to the positive (nega-
tive) category for each positive (negative) coding,
(2) 0.5 to the neutral category with each neutral cod-
ing, and (3) 0.5 to both the positive and negative
categories with each both coding. The strength of
codings in the three categories, i.e., str+(snipi),
strneu(snipi), and str−(snipi), were then summed
up into str(snipi). The distribution were parame-
terized with
</bodyText>
<equation confidence="0.999853333333333">
θ+(snipi) = str+(snipi)/str(snipi)
θneu(snipi) = strneu(snipi)/str(snipi)
θ−(snipi) = str−(snipi)/str(snipi)
</equation>
<bodyText confidence="0.991241">
We then quantify the level of ambiguity in the an-
notator’s judgement as follows:
</bodyText>
<equation confidence="0.999651666666667">
H(θ(snipi)) = −θ+(snipi)log(θ+(snipi))
−θneu(snipi)log(θneu(snipi))
−θ−(snipi)log(θ−(snipi))
</equation>
<bodyText confidence="0.999963166666667">
where strmax is the maximum value of str among
all the snippets in the collection. The plot (middle)
in Figure 1 shows that with the inclusion of snip-
pets that are more ambiguous in sentiment disam-
biguation, the mean agreement with Gold Standard
decreases as expected.
</bodyText>
<subsectionHeader confidence="0.969458">
3.3 Combining measures on multiple
annotations
</subsectionHeader>
<bodyText confidence="0.999918">
Having established the impact of noise and senti-
ment ambiguity on annotation quality, we then set
out to explore how to integrate them for selection.
First, the ambiguity scores for each of the snippets
are reweighed with respect to the noise level.
</bodyText>
<equation confidence="0.999509">
noise(annoj) x (1)θ(ij)
e
w(snipi)
Conf(snipi) = �i w(snipi) x Amb(snipi),
</equation>
<bodyText confidence="0.999987304347826">
where θ(ij) is an indicator function of whether a
coding of snipi from annotator j agrees with its gold
standard coding. w(expi) is thus computed as the
aggregated noise level of all the annotators who la-
beled the ith snippet.
To understand the baseline performance of the se-
lection procedure, we evaluate the the true predic-
tions versus the false alarms resulting from using
each of the quality measures separately to select an-
notations for label predictions. In this context, a true
prediction occurs when an annotation suggested by
our measure as high-quality indeed matches the GS
label, and a false alarm occurs when a high quality
annotation suggested by our measure does not match
the GS label. The ROC (receiver operating charac-
teristics) curves in Figure 2 reflect all the potential
operating points with the different measures.
We used data from 2,895 AMT annotations on
579 snippets, including 63 snippets used in the
agreement study. This dataset is obtained by filter-
ing out the snippets with their GS labels as 1 (“ir-
relevant”) and the snippets that do not receive any
coding that has more than two votes.
</bodyText>
<equation confidence="0.967851714285714">
str(snipi)
Amb(snipi) =
x H(θ(snipi)),
strmax
�
w(snipi) =
j
</equation>
<page confidence="0.999406">
31
</page>
<figure confidence="0.999794854166666">
(a) Match Prediction Before Removing Divisive Snippets
(b) Match Prediction After Removing Divisive Snippets
True Prediction Rate
0.0 0.2 0.4 0.6 0.8 1.0
●
●
0.9
1
●
0.8
●
1−confusion
1−ambiguity
1−noise
●
0.7
0.6
●
●
0.5
●
●
0.4
True Prediction Rate
0.0 0.2 0.4 0.6 0.8 1.0
●
●
●
0.9
1
0.8
●
0.7
1−confusion(all4codings)
1−confusion(pos/neg)
1−ambiguity(all4codings)
1−ambiguity(pos/neg)
1−noise
●
0.6
●
●0.5
0.4 ●●●
0
0.0 0.2 0.4 0.6 0.8 1.0
False Alarm Rate
0.0 0.2 0.4 0.6 0.8 1.0
False Alarm Rate
</figure>
<figureCaption confidence="0.9824085">
Figure 2: Modified ROC curves for quality measures: (a) before removing divisive snippets, (b) after removing divisive
snippets. The numbers shown with the ROC curve are the values of the aggregated quality measure (1-confusion).
</figureCaption>
<bodyText confidence="0.999910285714286">
Initially, three quality measures are tested: 1-
noise, 1-ambiguity, 1-confusion. Examination of the
snippet-level sentiment codings reveals that some
snippets (12%) result in “divisive” codings, i.e.,
equal number of votes on two codings.
The ROC curves in Figure 2 (a) plot the base-
line performance of the different quality measures.
Results show that before removing the subset of di-
visive snippets, the only effective selection criteria
is obtained by monitoring the noise level of anno-
tators. Figure 2 (b) plots the performance after re-
moving the divisive snippets. In addition, our am-
biguity scores are computed under two settings: (1)
with only the polar codings (pos/neg), and (2) with
all the four codings (all4codings). The ROC curves
reveal that analyzing only the polar codings is not
sufficient for annotation selection.
The results also demonstrate that confusion, an in-
tegrated measure, does perform best. Confusion is
just one way of combining these measures. One may
chose alternative combinations – the results here pri-
marily illustrate the benefit of considering these dif-
ferent dimensions in tandem. Moreover, the differ-
ence between plot (a) and (b) suggests that removing
divisive snippets is essential for the quality measures
to work well. How to automatically identify the di-
visive snippets is therefore important to the success
of the annotation selection process.
</bodyText>
<subsectionHeader confidence="0.98024">
3.4 Effect of lexical uncertainty on divisive
snippet detection
</subsectionHeader>
<bodyText confidence="0.999992473684211">
In search of measures that can help identify the di-
visive snippets automatically, we consider the inher-
ent lexical uncertainty of an example. Uncertainty
Sampling (Lewis and Catlett, 1994) is one common
heuristic for the selection of informative instances,
which select instances that the current classifier is
most uncertain about. Following on these lines we
measure the uncertainty on instances, with the as-
sumption that the most uncertain snippets are likely
to be divisive.
In particular, we applied a lexical sentiment clas-
sifier (c.f. Section 4.1.1) to estimate the likelihood of
an unseen snippet being of positive or negative sen-
timent, i.e., P+(expz), P−(expz), by counting the
sentiment-indicative word occurrences in the snip-
pet. As in our dataset the negative snippets far ex-
ceed the positive ones, we also take the prior proba-
bility into account to avoid class bias. We then mea-
sure lexical uncertainty as follows.
</bodyText>
<equation confidence="0.9924798">
Deviation(snipz) =
1
C
+(log(P+(snipz))−log(P−(snipz)))|,
Uncertainty(snipz) =1 − Deviation(snipz),
</equation>
<bodyText confidence="0.9999184">
where class priors, P(+) and P(−), are estimated
with the dataset used in the agreement studies, and
C is the normalization constant.
We then examine not only the utility of lexical un-
certainty in identifying high-quality annotations, but
</bodyText>
<equation confidence="0.536309">
× |(log(P(+))−log(P(−)))
</equation>
<page confidence="0.962073">
32
</page>
<table confidence="0.9975894">
Classifier Accuracy AUC
LC 49.60 0.614
NB 83.53 0.653
SVM 83.89 0.647
Pooling 84.51 0.700
</table>
<tableCaption confidence="0.999908">
Table 3: Accuracy of sentiment classification methods.
</tableCaption>
<bodyText confidence="0.995497428571429">
also the utility of such measure in identifying divi-
sive snippets. Figure 1 (right) shows the effect of
lexical uncertainty on filtering out low-quality anno-
tations. Figure 3 demonstrates the effect of lexical
uncertainty on divisive snippet detection, suggesting
the potential use of lexical uncertainty measures in
the selection process.
</bodyText>
<figure confidence="0.9290355">
0.0 0.2 0.4 0.6 0.8 1.0
Lexical Uncertai nty
</figure>
<figureCaption confidence="0.985213">
Figure 3: Divisive snippet detection accuracy as a func-
tion of lexical uncertainty.
</figureCaption>
<sectionHeader confidence="0.985618" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.9999062">
The analysis in Sec. 3 raises two important ques-
tions: (1) how useful are noisy annotations for sen-
timent analysis, and (2) what is the effect of online
annotation selection on improving sentiment polar-
ity classification?
</bodyText>
<subsectionHeader confidence="0.998498">
4.1 Polarity Classifier with Noisy Annotations
</subsectionHeader>
<bodyText confidence="0.999989714285714">
To answer the first question raised above, we train
classifiers based on the noisy AMT annotations to
classify positive and negative snippets. Four dif-
ferent types of classifiers are used: SVMs, Naive
Bayes (NB), a lexical classifier (LC), and the lexi-
cal knowledge-enhanced Pooling Multinomial clas-
sifier, described below.
</bodyText>
<subsectionHeader confidence="0.829277">
4.1.1 Lexical Classifier
</subsectionHeader>
<bodyText confidence="0.998850411764706">
In the absence of any labeled data in a domain,
one can build sentiment-classification models that
rely solely on background knowledge, such as a lex-
icon defining the polarity of words. Given a lexi-
con of positive and negative terms, one straightfor-
ward approach to using this information is to mea-
sure the frequency of occurrence of these terms in
each document. The probability that a test document
belongs to the positive class can then be computed
as P(+|D) = a
a+b; where a and b are the number
of occurrences of positive and negative terms in the
document respectively. A document is then classi-
fied as positive if P(+|D) &gt; P(−|D); otherwise,
the document is classified as negative. For this study,
we used a lexicon of 1,267 positive and 1,701 nega-
tive terms, as labeled by human experts.
</bodyText>
<subsectionHeader confidence="0.512266">
4.1.2 Pooling Multinomials
</subsectionHeader>
<bodyText confidence="0.9999043125">
The Pooling Multinomials classifier was intro-
duced by the authors as an approach to incorpo-
rate prior lexical knowledge into supervised learn-
ing for better text classification. In the context
of sentiment analysis, such lexical knowledge is
available in terms of the prior sentiment-polarity of
words. Pooling Multinomials classifies unlabeled
examples just as in multinomial Naive Bayes clas-
sification (McCallum and Nigam, 1998), by predict-
ing the class with the maximum likelihood, given by
argmaxcjP(cj) Hi P(wi|cj); where P(cj) is the
prior probability of class cj, and P(wi|cj) is the
probability of word wi appearing in a snippet of
class cj. In the absence of background knowledge
about the class distribution, we estimate the class
priors P(cj) solely from the training data. However,
unlike regular Naive Bayes, the conditional prob-
abilities P (wi|cj) are computed using both the la-
beled examples and the lexicon of labeled features.
Given two models built using labeled examples and
labeled features, the multinomial parameters of such
models can be aggregated through a convex combi-
nation, P(wi|cj) = αPe(wi|cj)+(1−α)Pf(wi|cj);
where Pe(wi|cj) and Pf(wi|cj) represent the proba-
bility assigned by using the example labels and fea-
ture labels respectively, and α is the weight for com-
bining these distributions. The weight indicates a
level of confidence in each source of information,
and can be computed based on the training set accu-
racy of the two components. The derivation and de-
tails of these models are not directly relevant to this
paper, but can be found in (Melville et al., 2009).
</bodyText>
<figure confidence="0.734813">
Divisive Snippet Detection Accuracy
0.0 0.2 0.4 0.6 0.8 1.0
</figure>
<page confidence="0.808695">
33
</page>
<table confidence="0.9974214">
Q1 Q2 Q3 Q4
Accuracy AUC Accuracy AUC Accuracy AUC Accuracy AUC
Noise 84.62% 0.688 74.36% 0.588 74.36% 0.512 79.49% 0.441
Ambiguity 84.21% 0.715 78.95% 0.618 68.42% 0.624 84.21% 0.691
Confusion 82.50% 0.831 82.50% 0.762 80.00% 0.814 80.00% 0.645
</table>
<tableCaption confidence="0.999414">
Table 4: Effect of annotation selection on classification accuracy.
</tableCaption>
<sectionHeader confidence="0.470538" genericHeader="evaluation">
4.1.3 Results on Polarity Classification
</sectionHeader>
<bodyText confidence="0.997695461538462">
We generated a data set of 504 snippets that had
3 or more labels for either the positive or negative
class. We compare the different classification ap-
proaches using 10-fold cross-validation and present
our results in Table 3. Results show that the Pool-
ing Multinomial classifier, which makes predictions
based on both the prior lexical knowledge and the
training data, can learn the most from the labeled
data to classify sentiments of the political blog snip-
pets. We observe that despite the significant level
of noise and ambiguity in the training data, using
majority-labeled data for training still results in clas-
sifiers with reasonable accuracy.
</bodyText>
<subsectionHeader confidence="0.998725">
4.2 Effect of Annotation Selection
</subsectionHeader>
<bodyText confidence="0.999894590909091">
We then evaluate the utility of the quality measures
in a randomly split dataset (with 7.5% of the data in
the test set). We applied each of the measures to rank
the annotation examples and then divide them into
4 equal-sized training sets based on their rankings.
For example, Noise-Q1 contains only the least noisy
quarter of annotations and Q4 the most noisy ones.
Results in Table 4 demonstrate that the classi-
fication performance declines with the decrease of
each quality measure in general, despite exceptions
in the subset with the highest sentiment ambiguity
(Ambiguity-Q4), the most noisy subset Q4 (Noise-
Q4), and the subset yielding less overall confusion
(Confusion-Q2). The results also reveal the benefits
of annotation selection on efficiency: using the sub-
set of annotations predicted in the top quality quar-
ter achieves similar performance as using the whole
training set. These preliminary results suggest that
an active learning scheme which considers all three
quality measures may indeed be effective in improv-
ing label quality and subsequent classification accu-
racy.
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999964918918919">
In this paper, we have analyzed the difference be-
tween expert and non-expert annotators in terms of
annotation quality, and showed that having a single
non-expert annotator is detrimental for annotating
sentiment in political snippets. However, we con-
firmed that using multiple noisy annotations from
different non-experts can still be very useful for
modeling. This finding is consistent with the sim-
ulated results reported in (Sheng et al., 2008). Given
the availability of many non-expert annotators on-
demand, we studied three important dimensions to
consider when acquiring annotations: (1) the noise
level of an annotator compared to others, (2) the in-
herent ambiguity of an example’s class label, and
(3) the informativeness of an example to the current
classification model. While the first measure has
been studied with annotations obtained from experts
(Dawid and Skene, 1979; Clemen and Reilly, 1999),
the applicability of their findings on non-expert an-
notation selection has not been examined.
We showed how quality of labels can be improved
by eliminating noisy annotators and ambiguous ex-
amples. Furthermore, we demonstrated the quality
measures are useful for selecting annotations that
lead to more accurate classification models. Our re-
sults suggest that a good active learning or online
learning scheme in this setting should really con-
sider all three dimensions. The way we use to in-
tegrate the different dimensions now is still prelimi-
nary. Also, our empirical findings suggest that some
of the dimensions may have to be considered sepa-
rately. For example, due to the divisive tendency of
the most informative examples, these examples may
have to be disregarded in the initial stage of anno-
tation selection. Also, the way we use to combine
these measures is still preliminary. The design and
testing of such schemes are avenues for future work.
</bodyText>
<page confidence="0.998572">
34
</page>
<sectionHeader confidence="0.995818" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884164383562">
Giuseppe Carenini and Jackie C. K. Cheung. 2008. Ex-
tractive vs. NLG-based abstractive summarization of
evaluative text: The effect of corpus controversiality.
In Proceedings of the Fifth International Natural Lan-
guage Generation Conference.
R.T. Clemen and T. Reilly. 1999. Correlations and cop-
ulas for decision and risk analysis. Management Sci-
ence, 45:208–224.
A. P. Dawid and A. .M. Skene. 1979. Maximum likli-
hood estimation of observer error-rates using the em
algorithm. Applied Statistics, 28(1):20–28.
Jeff Howe. 2008. Crowdsourcing: Why the Power of
the Crowd Is Driving the Future of Business. Crown
Business, 1 edition, August.
Michael Kaisser, Marti Hearst, and John B. Lowe. 2008.
Evidence for varying search results summary lengths.
In Proceedings of ACL 2008.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with mechanical turk. In
Proceedings of CHI 2008.
David D. Lewis and Jason Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. pages
148–156, San Francisco, CA, July.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive Bayes text classifi-
cation. In AAAI Workshop on Text Categorization.
Prem Melville, Wojciech Gryc, and Richard Lawrence.
2009. Sentiment analysis of blogs by combining lexi-
cal knowledge with text classification. In KDD.
Bo Pang and Lillian Lee. 2004. A sentiment education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL 2004.
Victor Sheng, Foster Provost, and G. Panagiotis Ipeiro-
tis. 2008. Get another label? Improving data quality
and data mining using multiple, noisy labelers. In Pro-
ceeding of KDD 2008, pages 614–622.
Vikas Sindhwani and Prem Melville. 2008. Document-
word co-regularization for semi-supervised sentiment
analysis. In Proceedings of IEEE International Con-
ference on Data Mining (ICDM), pages 1025–1030,
Los Alamitos, CA, USA. IEEE Computer Society.
R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast—but is it good? evaluating non-expert
annotations for natural language tasks. In Proceedings
of EMNLP 2008.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation via amazon mechanical turk. In IEEE
Workshop on Internet Vision at CVPR 08.
Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell
C.Baker. 2007. Internet-scale collection of human-
reviewed data. In Proceedings of WWW 2007.
Peter D. Turney. 2002. Thumbs up or thumbs down:
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of ACL 2002.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of CHI
2004, pages 319–326.
Janyce Wiebe and E. Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of CICLing 2005.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational Linguistics, 30 (3).
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extract-
ing sentiments about a given topic using natural lan-
guage processing technique. In Proceedings of the
International Conference on Data Mining (ICDM),
pages 427–434.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP 2003.
</reference>
<page confidence="0.999326">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.926194">
<title confidence="0.9993605">Data Quality from Crowdsourcing: A Study of Annotation Selection Criteria</title>
<author confidence="0.989056">Pei-Yun Hsueh</author>
<author confidence="0.989056">Prem Melville</author>
<author confidence="0.989056">Vikas</author>
<affiliation confidence="0.995398">IBM T.J. Watson Research</affiliation>
<address confidence="0.9948575">1101 Kitchawan Road, Route Yorktown Heights, NY 10598, USA</address>
<abstract confidence="0.997899458333333">Annotation acquisition is an essential step in training supervised classifiers. However, manual annotation is often time-consuming and expensive. The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates. In this paper, we consider the difficult problem of classifying sentiment in political blog snippets. Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined. Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty. Analysis confirm the utility of these criteria on improving data quality. We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Jackie C K Cheung</author>
</authors>
<title>Extractive vs. NLG-based abstractive summarization of evaluative text: The effect of corpus controversiality.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference.</booktitle>
<contexts>
<context position="16014" citStr="Carenini and Cheung, 2008" startWordPosition="2531" endWordPosition="2534">s show that the data quality decreases with the inclusion of more untrustworthy annotators. 3.2 Snippet-level sentiment ambiguity We have observed that not all snippets are equally easy to annotate, with some containing more ambiguous expressions. To incorporate this concern in the selection process, a key question to be answered is whether there exist snippets whose sentiment is substantially less distinguishable than the others. We address this question by quantifying ambiguity measures with the two key properties shown as important in evaluating the controversiality of annotation snippets (Carenini and Cheung, 2008): (1) the strength of the annotators’ judgements and (2) the polarity of the annotations. The measurement needs to satisfy the constraints demonstrated in the following snippets: (1) An example that has received three positive codings are more ambiguous than that has received five, and (2) an example that has received five positive codings is more ambiguous than the one that has received four positive and one negative coding. In addition, as some snippets were shown to 30 Sentiment Ambigity Lexical Uncertainty Annotator Noise 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 Prediction Accuracy 0.0 </context>
</contexts>
<marker>Carenini, Cheung, 2008</marker>
<rawString>Giuseppe Carenini and Jackie C. K. Cheung. 2008. Extractive vs. NLG-based abstractive summarization of evaluative text: The effect of corpus controversiality. In Proceedings of the Fifth International Natural Language Generation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T Clemen</author>
<author>T Reilly</author>
</authors>
<title>Correlations and copulas for decision and risk analysis.</title>
<date>1999</date>
<journal>Management Science,</journal>
<pages>45--208</pages>
<contexts>
<context position="30054" citStr="Clemen and Reilly, 1999" startWordPosition="4753" endWordPosition="4756">notations from different non-experts can still be very useful for modeling. This finding is consistent with the simulated results reported in (Sheng et al., 2008). Given the availability of many non-expert annotators ondemand, we studied three important dimensions to consider when acquiring annotations: (1) the noise level of an annotator compared to others, (2) the inherent ambiguity of an example’s class label, and (3) the informativeness of an example to the current classification model. While the first measure has been studied with annotations obtained from experts (Dawid and Skene, 1979; Clemen and Reilly, 1999), the applicability of their findings on non-expert annotation selection has not been examined. We showed how quality of labels can be improved by eliminating noisy annotators and ambiguous examples. Furthermore, we demonstrated the quality measures are useful for selecting annotations that lead to more accurate classification models. Our results suggest that a good active learning or online learning scheme in this setting should really consider all three dimensions. The way we use to integrate the different dimensions now is still preliminary. Also, our empirical findings suggest that some of</context>
</contexts>
<marker>Clemen, Reilly, 1999</marker>
<rawString>R.T. Clemen and T. Reilly. 1999. Correlations and copulas for decision and risk analysis. Management Science, 45:208–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dawid</author>
<author>A M Skene</author>
</authors>
<title>Maximum liklihood estimation of observer error-rates using the em algorithm.</title>
<date>1979</date>
<journal>Applied Statistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="14668" citStr="Dawid and Skene, 1979" startWordPosition="2319" endWordPosition="2322">ultiple annotators are still necessary when non-expert annotators are recruited. 3 Annotation Quality Measures Given the noisy AMT annotations, in this section we discuss some summary statistics that are needed to control the quality of annotations. 3.1 Annotator-level noise To study the question of whether there exists a group of annotators who tend to yield more noisy annotations, we evaluate the accumulated noise level introduced by each of the annotators. We define the noise level as the deviation from the gold standard labels. Similar to the measure of individual error rates proposed in (Dawid and Skene, 1979), the noise level of a particular annotator j, i.e., noise(annoj), is then estimated by summing up the deviation of the annotations received from this annotator, with a small sampling correction for chance disagreement. Analysis results demonstrate that there does exist a subset of annotators who yield more noisy annotations than the others. 20% of the annotators (who exceed the noise level 60%) result in annotations that have 70% disagreement with the gold standard. In addition, we also evaluate how inclusion of noisy annotators reduces the mean agreement with Gold Standard. The plot (left) i</context>
<context position="30028" citStr="Dawid and Skene, 1979" startWordPosition="4749" endWordPosition="4752">using multiple noisy annotations from different non-experts can still be very useful for modeling. This finding is consistent with the simulated results reported in (Sheng et al., 2008). Given the availability of many non-expert annotators ondemand, we studied three important dimensions to consider when acquiring annotations: (1) the noise level of an annotator compared to others, (2) the inherent ambiguity of an example’s class label, and (3) the informativeness of an example to the current classification model. While the first measure has been studied with annotations obtained from experts (Dawid and Skene, 1979; Clemen and Reilly, 1999), the applicability of their findings on non-expert annotation selection has not been examined. We showed how quality of labels can be improved by eliminating noisy annotators and ambiguous examples. Furthermore, we demonstrated the quality measures are useful for selecting annotations that lead to more accurate classification models. Our results suggest that a good active learning or online learning scheme in this setting should really consider all three dimensions. The way we use to integrate the different dimensions now is still preliminary. Also, our empirical fin</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>A. P. Dawid and A. .M. Skene. 1979. Maximum liklihood estimation of observer error-rates using the em algorithm. Applied Statistics, 28(1):20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Howe</author>
</authors>
<title>Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business.</title>
<date>2008</date>
<journal>Crown Business,</journal>
<volume>1</volume>
<pages>edition,</pages>
<contexts>
<context position="1313" citStr="Howe, 2008" startWordPosition="190" endWordPosition="191"> snippets. Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined. Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty. Analysis confirm the utility of these criteria on improving data quality. We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency. 1 Introduction Crowdsourcing (Howe, 2008) is an attractive solution to the problem of cheaply and quickly acquiring annotations for the purposes of constructing all kinds of predictive models. To sense the potential of crowdsourcing, consider an observation in von Ahn et al. (2004): a crowd of 5,000 people playing an appropriately designed computer game 24 hours a day, could be made to label all images on Google (425,000,000 images in 2005) in a matter of just 31 days. Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk, a marketplace for recruiting online workers (Su et al., 2007; Kaisser e</context>
</contexts>
<marker>Howe, 2008</marker>
<rawString>Jeff Howe. 2008. Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business. Crown Business, 1 edition, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
<author>Marti Hearst</author>
<author>John B Lowe</author>
</authors>
<title>Evidence for varying search results summary lengths.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1924" citStr="Kaisser et al., 2008" startWordPosition="291" endWordPosition="294">we, 2008) is an attractive solution to the problem of cheaply and quickly acquiring annotations for the purposes of constructing all kinds of predictive models. To sense the potential of crowdsourcing, consider an observation in von Ahn et al. (2004): a crowd of 5,000 people playing an appropriately designed computer game 24 hours a day, could be made to label all images on Google (425,000,000 images in 2005) in a matter of just 31 days. Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk, a marketplace for recruiting online workers (Su et al., 2007; Kaisser et al., 2008; Kittur et al., 2008; Sheng et al., 2008; Snow et al., 2008; Sorokin and Forsyth, 2008). With efficiency and cost-effectiveness, online recruitment of anonymous annotators brings a new set of issues to the table. These workers are not usually specifically trained for annotation, and might not be highly invested in producing good-quality annotations. Consequently, the obtained annotations may be noisy by nature, and might require additional validation or scrutiny. Several interesting questions immediately arise in how to optimally utilize annotations in this setting: How does one handle differ</context>
</contexts>
<marker>Kaisser, Hearst, Lowe, 2008</marker>
<rawString>Michael Kaisser, Marti Hearst, and John B. Lowe. 2008. Evidence for varying search results summary lengths. In Proceedings of ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniket Kittur</author>
<author>Ed H Chi</author>
<author>Bongwon Suh</author>
</authors>
<title>Crowdsourcing user studies with mechanical turk.</title>
<date>2008</date>
<booktitle>In Proceedings of CHI</booktitle>
<contexts>
<context position="1945" citStr="Kittur et al., 2008" startWordPosition="295" endWordPosition="298">tive solution to the problem of cheaply and quickly acquiring annotations for the purposes of constructing all kinds of predictive models. To sense the potential of crowdsourcing, consider an observation in von Ahn et al. (2004): a crowd of 5,000 people playing an appropriately designed computer game 24 hours a day, could be made to label all images on Google (425,000,000 images in 2005) in a matter of just 31 days. Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk, a marketplace for recruiting online workers (Su et al., 2007; Kaisser et al., 2008; Kittur et al., 2008; Sheng et al., 2008; Snow et al., 2008; Sorokin and Forsyth, 2008). With efficiency and cost-effectiveness, online recruitment of anonymous annotators brings a new set of issues to the table. These workers are not usually specifically trained for annotation, and might not be highly invested in producing good-quality annotations. Consequently, the obtained annotations may be noisy by nature, and might require additional validation or scrutiny. Several interesting questions immediately arise in how to optimally utilize annotations in this setting: How does one handle differences among workers i</context>
</contexts>
<marker>Kittur, Chi, Suh, 2008</marker>
<rawString>Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008. Crowdsourcing user studies with mechanical turk. In Proceedings of CHI 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Jason Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<pages>148--156</pages>
<location>San Francisco, CA,</location>
<contexts>
<context position="22149" citStr="Lewis and Catlett, 1994" startWordPosition="3509" endWordPosition="3512">he results here primarily illustrate the benefit of considering these different dimensions in tandem. Moreover, the difference between plot (a) and (b) suggests that removing divisive snippets is essential for the quality measures to work well. How to automatically identify the divisive snippets is therefore important to the success of the annotation selection process. 3.4 Effect of lexical uncertainty on divisive snippet detection In search of measures that can help identify the divisive snippets automatically, we consider the inherent lexical uncertainty of an example. Uncertainty Sampling (Lewis and Catlett, 1994) is one common heuristic for the selection of informative instances, which select instances that the current classifier is most uncertain about. Following on these lines we measure the uncertainty on instances, with the assumption that the most uncertain snippets are likely to be divisive. In particular, we applied a lexical sentiment classifier (c.f. Section 4.1.1) to estimate the likelihood of an unseen snippet being of positive or negative sentiment, i.e., P+(expz), P−(expz), by counting the sentiment-indicative word occurrences in the snippet. As in our dataset the negative snippets far ex</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>David D. Lewis and Jason Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. pages 148–156, San Francisco, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for naive Bayes text classification.</title>
<date>1998</date>
<booktitle>In AAAI Workshop on Text Categorization.</booktitle>
<contexts>
<context position="25764" citStr="McCallum and Nigam, 1998" startWordPosition="4074" endWordPosition="4077"> &gt; P(−|D); otherwise, the document is classified as negative. For this study, we used a lexicon of 1,267 positive and 1,701 negative terms, as labeled by human experts. 4.1.2 Pooling Multinomials The Pooling Multinomials classifier was introduced by the authors as an approach to incorporate prior lexical knowledge into supervised learning for better text classification. In the context of sentiment analysis, such lexical knowledge is available in terms of the prior sentiment-polarity of words. Pooling Multinomials classifies unlabeled examples just as in multinomial Naive Bayes classification (McCallum and Nigam, 1998), by predicting the class with the maximum likelihood, given by argmaxcjP(cj) Hi P(wi|cj); where P(cj) is the prior probability of class cj, and P(wi|cj) is the probability of word wi appearing in a snippet of class cj. In the absence of background knowledge about the class distribution, we estimate the class priors P(cj) solely from the training data. However, unlike regular Naive Bayes, the conditional probabilities P (wi|cj) are computed using both the labeled examples and the lexicon of labeled features. Given two models built using labeled examples and labeled features, the multinomial pa</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for naive Bayes text classification. In AAAI Workshop on Text Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prem Melville</author>
<author>Wojciech Gryc</author>
<author>Richard Lawrence</author>
</authors>
<title>Sentiment analysis of blogs by combining lexical knowledge with text classification.</title>
<date>2009</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="3953" citStr="Melville et al., 2009" startWordPosition="603" endWordPosition="606">f interest, e.g., US Presidential candidates, aggregated across the blogsphere. Recently, sentiment analy27 Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 27–35, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics sis is emerging as a critical methodology for social media analytics. Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al., 2004), and also on accurate sentiment polarity assignment (Turney, 2002; Yi et al., 2003; Pang and Lee, 2004; Sindhwani and Melville, 2008; Melville et al., 2009). The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al., 2003), or quality annotation data for statistical training. While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005), manual intervention in the annotation process is harder to avoid. Moreover, the task of annotating blog-post snippets is challenging, particularly in a charged political atmosphere with complex discourse spann</context>
<context position="26934" citStr="Melville et al., 2009" startWordPosition="4265" endWordPosition="4268">ed examples and labeled features, the multinomial parameters of such models can be aggregated through a convex combination, P(wi|cj) = αPe(wi|cj)+(1−α)Pf(wi|cj); where Pe(wi|cj) and Pf(wi|cj) represent the probability assigned by using the example labels and feature labels respectively, and α is the weight for combining these distributions. The weight indicates a level of confidence in each source of information, and can be computed based on the training set accuracy of the two components. The derivation and details of these models are not directly relevant to this paper, but can be found in (Melville et al., 2009). Divisive Snippet Detection Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 33 Q1 Q2 Q3 Q4 Accuracy AUC Accuracy AUC Accuracy AUC Accuracy AUC Noise 84.62% 0.688 74.36% 0.588 74.36% 0.512 79.49% 0.441 Ambiguity 84.21% 0.715 78.95% 0.618 68.42% 0.624 84.21% 0.691 Confusion 82.50% 0.831 82.50% 0.762 80.00% 0.814 80.00% 0.645 Table 4: Effect of annotation selection on classification accuracy. 4.1.3 Results on Polarity Classification We generated a data set of 504 snippets that had 3 or more labels for either the positive or negative class. We compare the different classification approaches using 10-fold cross-</context>
</contexts>
<marker>Melville, Gryc, Lawrence, 2009</marker>
<rawString>Prem Melville, Wojciech Gryc, and Richard Lawrence. 2009. Sentiment analysis of blogs by combining lexical knowledge with text classification. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentiment education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="3899" citStr="Pang and Lee, 2004" startWordPosition="595" endWordPosition="598"> provides a view of the opinion around a subject of interest, e.g., US Presidential candidates, aggregated across the blogsphere. Recently, sentiment analy27 Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 27–35, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics sis is emerging as a critical methodology for social media analytics. Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al., 2004), and also on accurate sentiment polarity assignment (Turney, 2002; Yi et al., 2003; Pang and Lee, 2004; Sindhwani and Melville, 2008; Melville et al., 2009). The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al., 2003), or quality annotation data for statistical training. While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005), manual intervention in the annotation process is harder to avoid. Moreover, the task of annotating blog-post snippets is challenging, particularly in a cha</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentiment education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Sheng</author>
<author>Foster Provost</author>
<author>G Panagiotis Ipeirotis</author>
</authors>
<title>Get another label? Improving data quality and data mining using multiple, noisy labelers.</title>
<date>2008</date>
<booktitle>In Proceeding of KDD</booktitle>
<pages>614--622</pages>
<contexts>
<context position="1965" citStr="Sheng et al., 2008" startWordPosition="299" endWordPosition="302">problem of cheaply and quickly acquiring annotations for the purposes of constructing all kinds of predictive models. To sense the potential of crowdsourcing, consider an observation in von Ahn et al. (2004): a crowd of 5,000 people playing an appropriately designed computer game 24 hours a day, could be made to label all images on Google (425,000,000 images in 2005) in a matter of just 31 days. Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk, a marketplace for recruiting online workers (Su et al., 2007; Kaisser et al., 2008; Kittur et al., 2008; Sheng et al., 2008; Snow et al., 2008; Sorokin and Forsyth, 2008). With efficiency and cost-effectiveness, online recruitment of anonymous annotators brings a new set of issues to the table. These workers are not usually specifically trained for annotation, and might not be highly invested in producing good-quality annotations. Consequently, the obtained annotations may be noisy by nature, and might require additional validation or scrutiny. Several interesting questions immediately arise in how to optimally utilize annotations in this setting: How does one handle differences among workers in terms of the quali</context>
<context position="29592" citStr="Sheng et al., 2008" startWordPosition="4682" endWordPosition="4685"> active learning scheme which considers all three quality measures may indeed be effective in improving label quality and subsequent classification accuracy. 5 Conclusion In this paper, we have analyzed the difference between expert and non-expert annotators in terms of annotation quality, and showed that having a single non-expert annotator is detrimental for annotating sentiment in political snippets. However, we confirmed that using multiple noisy annotations from different non-experts can still be very useful for modeling. This finding is consistent with the simulated results reported in (Sheng et al., 2008). Given the availability of many non-expert annotators ondemand, we studied three important dimensions to consider when acquiring annotations: (1) the noise level of an annotator compared to others, (2) the inherent ambiguity of an example’s class label, and (3) the informativeness of an example to the current classification model. While the first measure has been studied with annotations obtained from experts (Dawid and Skene, 1979; Clemen and Reilly, 1999), the applicability of their findings on non-expert annotation selection has not been examined. We showed how quality of labels can be imp</context>
</contexts>
<marker>Sheng, Provost, Ipeirotis, 2008</marker>
<rawString>Victor Sheng, Foster Provost, and G. Panagiotis Ipeirotis. 2008. Get another label? Improving data quality and data mining using multiple, noisy labelers. In Proceeding of KDD 2008, pages 614–622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Prem Melville</author>
</authors>
<title>Documentword co-regularization for semi-supervised sentiment analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of IEEE International Conference on Data Mining (ICDM),</booktitle>
<pages>1025--1030</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Los Alamitos, CA, USA.</location>
<contexts>
<context position="3929" citStr="Sindhwani and Melville, 2008" startWordPosition="599" endWordPosition="602">the opinion around a subject of interest, e.g., US Presidential candidates, aggregated across the blogsphere. Recently, sentiment analy27 Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 27–35, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics sis is emerging as a critical methodology for social media analytics. Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al., 2004), and also on accurate sentiment polarity assignment (Turney, 2002; Yi et al., 2003; Pang and Lee, 2004; Sindhwani and Melville, 2008; Melville et al., 2009). The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al., 2003), or quality annotation data for statistical training. While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005), manual intervention in the annotation process is harder to avoid. Moreover, the task of annotating blog-post snippets is challenging, particularly in a charged political atmosphere with</context>
</contexts>
<marker>Sindhwani, Melville, 2008</marker>
<rawString>Vikas Sindhwani and Prem Melville. 2008. Documentword co-regularization for semi-supervised sentiment analysis. In Proceedings of IEEE International Conference on Data Mining (ICDM), pages 1025–1030, Los Alamitos, CA, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Cheap and fast—but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. 2008. Cheap and fast—but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Sorokin</author>
<author>David Forsyth</author>
</authors>
<title>Utility data annotation via amazon mechanical turk.</title>
<date>2008</date>
<booktitle>In IEEE Workshop on Internet Vision at CVPR 08.</booktitle>
<contexts>
<context position="2012" citStr="Sorokin and Forsyth, 2008" startWordPosition="307" endWordPosition="310">g annotations for the purposes of constructing all kinds of predictive models. To sense the potential of crowdsourcing, consider an observation in von Ahn et al. (2004): a crowd of 5,000 people playing an appropriately designed computer game 24 hours a day, could be made to label all images on Google (425,000,000 images in 2005) in a matter of just 31 days. Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk, a marketplace for recruiting online workers (Su et al., 2007; Kaisser et al., 2008; Kittur et al., 2008; Sheng et al., 2008; Snow et al., 2008; Sorokin and Forsyth, 2008). With efficiency and cost-effectiveness, online recruitment of anonymous annotators brings a new set of issues to the table. These workers are not usually specifically trained for annotation, and might not be highly invested in producing good-quality annotations. Consequently, the obtained annotations may be noisy by nature, and might require additional validation or scrutiny. Several interesting questions immediately arise in how to optimally utilize annotations in this setting: How does one handle differences among workers in terms of the quality of annotations they provide? How useful are </context>
</contexts>
<marker>Sorokin, Forsyth, 2008</marker>
<rawString>Alexander Sorokin and David Forsyth. 2008. Utility data annotation via amazon mechanical turk. In IEEE Workshop on Internet Vision at CVPR 08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Su</author>
<author>Dmitry Pavlov</author>
<author>Jyh-Herng Chow</author>
<author>Wendell C Baker</author>
</authors>
<title>Internet-scale collection of humanreviewed data.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW</booktitle>
<contexts>
<context position="1902" citStr="Su et al., 2007" startWordPosition="287" endWordPosition="290">Crowdsourcing (Howe, 2008) is an attractive solution to the problem of cheaply and quickly acquiring annotations for the purposes of constructing all kinds of predictive models. To sense the potential of crowdsourcing, consider an observation in von Ahn et al. (2004): a crowd of 5,000 people playing an appropriately designed computer game 24 hours a day, could be made to label all images on Google (425,000,000 images in 2005) in a matter of just 31 days. Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk, a marketplace for recruiting online workers (Su et al., 2007; Kaisser et al., 2008; Kittur et al., 2008; Sheng et al., 2008; Snow et al., 2008; Sorokin and Forsyth, 2008). With efficiency and cost-effectiveness, online recruitment of anonymous annotators brings a new set of issues to the table. These workers are not usually specifically trained for annotation, and might not be highly invested in producing good-quality annotations. Consequently, the obtained annotations may be noisy by nature, and might require additional validation or scrutiny. Several interesting questions immediately arise in how to optimally utilize annotations in this setting: How </context>
</contexts>
<marker>Su, Pavlov, Chow, Baker, 2007</marker>
<rawString>Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell C.Baker. 2007. Internet-scale collection of humanreviewed data. In Proceedings of WWW 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down: Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="3862" citStr="Turney, 2002" startWordPosition="589" endWordPosition="590">e or negative. Such an analysis provides a view of the opinion around a subject of interest, e.g., US Presidential candidates, aggregated across the blogsphere. Recently, sentiment analy27 Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 27–35, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics sis is emerging as a critical methodology for social media analytics. Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al., 2004), and also on accurate sentiment polarity assignment (Turney, 2002; Yi et al., 2003; Pang and Lee, 2004; Sindhwani and Melville, 2008; Melville et al., 2009). The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al., 2003), or quality annotation data for statistical training. While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005), manual intervention in the annotation process is harder to avoid. Moreover, the task of annotating blog-post snippets </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down: Semantic orientation applied to unsupervised classification of reviews. In Proceedings of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of CHI</booktitle>
<pages>319--326</pages>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of CHI 2004, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>E Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing</booktitle>
<contexts>
<context position="4342" citStr="Wiebe and Riloff, 2005" startWordPosition="664" endWordPosition="667">d on classifying subjective-versus-objective expressions (Wiebe et al., 2004), and also on accurate sentiment polarity assignment (Turney, 2002; Yi et al., 2003; Pang and Lee, 2004; Sindhwani and Melville, 2008; Melville et al., 2009). The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al., 2003), or quality annotation data for statistical training. While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005), manual intervention in the annotation process is harder to avoid. Moreover, the task of annotating blog-post snippets is challenging, particularly in a charged political atmosphere with complex discourse spanning many issues, use of cynicism and sarcasm, and highly domain-specific and contextual cues. The downside is that high-performance models are generally difficult to construct, but the upside is that annotation and data-quality issues are more clearly exposed. In this paper we aim to provide an empirical basis for the use of data selection criteria in the context of sentiment analysis i</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Janyce Wiebe and E. Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of CICLing 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="3796" citStr="Wiebe et al., 2004" startWordPosition="578" endWordPosition="581"> polarity score that indicates whether the sentiment expressed is positive or negative. Such an analysis provides a view of the opinion around a subject of interest, e.g., US Presidential candidates, aggregated across the blogsphere. Recently, sentiment analy27 Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 27–35, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics sis is emerging as a critical methodology for social media analytics. Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al., 2004), and also on accurate sentiment polarity assignment (Turney, 2002; Yi et al., 2003; Pang and Lee, 2004; Sindhwani and Melville, 2008; Melville et al., 2009). The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al., 2003), or quality annotation data for statistical training. While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005), manual intervention in the annotation process is har</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computational Linguistics, 30 (3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeonghee Yi</author>
<author>Tetsuya Nasukawa</author>
<author>Razvan Bunescu</author>
<author>Wayne Niblack</author>
</authors>
<title>Sentiment analyzer: Extracting sentiments about a given topic using natural language processing technique.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Data Mining (ICDM),</booktitle>
<pages>427--434</pages>
<contexts>
<context position="3879" citStr="Yi et al., 2003" startWordPosition="591" endWordPosition="594"> Such an analysis provides a view of the opinion around a subject of interest, e.g., US Presidential candidates, aggregated across the blogsphere. Recently, sentiment analy27 Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 27–35, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics sis is emerging as a critical methodology for social media analytics. Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al., 2004), and also on accurate sentiment polarity assignment (Turney, 2002; Yi et al., 2003; Pang and Lee, 2004; Sindhwani and Melville, 2008; Melville et al., 2009). The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al., 2003), or quality annotation data for statistical training. While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005), manual intervention in the annotation process is harder to avoid. Moreover, the task of annotating blog-post snippets is challenging, p</context>
</contexts>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and Wayne Niblack. 2003. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing technique. In Proceedings of the International Conference on Data Mining (ICDM), pages 427–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="4317" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="660" endWordPosition="663">s. Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al., 2004), and also on accurate sentiment polarity assignment (Turney, 2002; Yi et al., 2003; Pang and Lee, 2004; Sindhwani and Melville, 2008; Melville et al., 2009). The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al., 2003), or quality annotation data for statistical training. While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005), manual intervention in the annotation process is harder to avoid. Moreover, the task of annotating blog-post snippets is challenging, particularly in a charged political atmosphere with complex discourse spanning many issues, use of cynicism and sarcasm, and highly domain-specific and contextual cues. The downside is that high-performance models are generally difficult to construct, but the upside is that annotation and data-quality issues are more clearly exposed. In this paper we aim to provide an empirical basis for the use of data selection criteria in the contex</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>