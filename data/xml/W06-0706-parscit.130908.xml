<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9936345">
Automating Help-desk Responses: A Comparative Study of
Information-gathering Approaches
</title>
<author confidence="0.803823">
Yuval Marom and Ingrid Zukerman
</author>
<affiliation confidence="0.8574145">
Faculty of Information Technology
Monash University
</affiliation>
<address confidence="0.764699">
Clayton, VICTORIA 3800, AUSTRALIA
</address>
<email confidence="0.998264">
lyuvalm,ingridl@csse.monash.edu.au
</email>
<sectionHeader confidence="0.997382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998834">
We present a comparative study of corpus-
based methods for the automatic synthe-
sis of email responses to help-desk re-
quests. Our methods were developed by
considering two operational dimensions:
</bodyText>
<listItem confidence="0.840838">
(1) information-gathering technique, and
</listItem>
<bodyText confidence="0.962359545454545">
(2) granularity of the information. In par-
ticular, we investigate two techniques – re-
trieval and prediction – applied to infor-
mation represented at two levels of granu-
larity – sentence-level and document level.
We also developed a hybrid method that
combines prediction with retrieval. Our
results show that the different approaches
are applicable in different situations, ad-
dressing a combined 72% of the requests
with either complete or partial responses.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976857142857">
Email inquiries sent to help desks often “revolve
around a small set of common questions and is-
sues”.1 This means that help-desk operators spend
most of their time dealing with problems that
have been previously addressed. Further, a sig-
nificant proportion of help-desk responses contain
a low level of technical content, corresponding,
for example, to inquires addressed to the wrong
group, or insufficient detail provided by the cus-
tomer about his or her problem. Organizations and
clients would benefit if the efforts of human oper-
ators were focused on difficult, atypical problems,
and an automated process was employed to deal
with the easier problems.
</bodyText>
<footnote confidence="0.8650625">
1http://customercare.telephonyonline.
com/ar/telecom-next-generation-customer.
</footnote>
<bodyText confidence="0.999734782608696">
In this paper, we report on our experiments with
corpus-based approaches to the automation of
help-desk responses. Our study is based on a cor-
pus of 30,000 email dialogues between users and
help-desk operators at Hewlett-Packard. These di-
alogues deal with a variety of user requests, which
include requests for technical assistance, inquiries
about products, and queries about how to return
faulty products or parts.
In order to restrict the scope of our study, we
considered two-turn short dialogues, comprising a
request followed by an answer, where the answer
has at most 15 lines. This yields a sub-corpus of
6659 dialogues. As a first step, we have automat-
ically clustered the corpus according to the sub-
ject line of the first email. This process yielded
15 topic-based datasets that contain between 135
and 1200 email dialogues. Owing to time limita-
tions, the procedures described in this paper were
applied to 8 of the datasets, corresponding to ap-
proximately 75% of the dialogues.
Analysis of our corpus yields the following ob-
servations.
</bodyText>
<listItem confidence="0.701618">
• O1: Requests containing precise information,
</listItem>
<bodyText confidence="0.9480638">
such as product names or part specifications,
sometimes elicit helpful, precise answers re-
ferring to this information, while other times
they elicit answers that do not refer to the query
terms, but contain generic information (e.g.,
referring customers to another help group or
asking them to call a particular phone num-
ber). Request-answer pair RA1 in Figure 1 il-
lustrates the first situation, while the pair RA2
illustrates the second.2
</bodyText>
<note confidence="0.570705">
2Our examples are reproduced verbatim from the corpus
(except for URLs and phone numbers which have been dis-
guised by us), and some have user or operator errors.
</note>
<page confidence="0.955215">
40
</page>
<note confidence="0.912148363636364">
Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 40–47,
Sydney, July 2006. c�2006 Association for Computational Linguistics
|XML |xmlLoc_0 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_continue bi_xmlPara_new
Do I need Compaq driver software for my armada 1500
docking station? This in order to be able to re-install win
98?
I would recommend to install the latest system rompaq,
on the laptop and the docking station. Just select the
model of computer and the operating system you have.
http://www.thislink.com.
|XML |xmlLoc_1 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_new bi_xmlPara_new
</note>
<bodyText confidence="0.850393714285714">
Is there a way to disable the NAT firewall on the Com-
paq CP-2W so I don’t get a private ip address through
the wireless network?
Unfortunately, you have reached the incorrect eResponse
queue for your unit. Your device is supported at the fol-
lowing link, or at 888-phone-number. We apologize
for the inconvenience.
</bodyText>
<figureCaption confidence="0.988107">
Figure 1: Sample request-answer pairs.
</figureCaption>
<bodyText confidence="0.717415833333333">
• O2: Operators tend to re-use the same sen-
tences in different responses. This is partly a
result of companies having in-house manuals
that prescribe how to generate an answer. For
instance, answers A3 and A4 in Figure 2 share
the sentence in italics.
These observations prompt us to consider com-
plementary approaches along two separate dimen-
sions of our problem. The first dimension pertains
to the technique applied to determine the informa-
tion in an answer, and the second dimension per-
tains to the granularity of the information.
• Observation O1 leads us to consider two tech-
niques for obtaining information: retrieval and
prediction. Retrieval returns an information
item by matching its terms to query terms
(Salton and McGill, 1983). Hence, it is likely
to obtain precise information if available. In
contrast, prediction uses features of requests
and responses to select an information item.
For example, the absence of a particular term
in a request may be a good predictive feature
(which cannot be considered in traditional re-
trieval). Thus, prediction could yield replies
that do not match particular query terms.
• Observation O2 leads us to consider two levels
of granularity: document and sentence. That is,
we can obtain a document comprising a com-
plete answer on the basis of a request (i.e., re-
use an answer to a previous request), or we can
obtain individual sentences and then combine
them to compose an answer, as is done in multi-
document summarization (Filatova and Hatzi-
vassiloglou, 2004). The sentence-level granu-
|XML |xmlLoc_0 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_new bi_xmlPara_new
If you are able to see the Internet then it sounds like
it is working, you may want to get in touch with your IT
department to see if you need to make any changes to your
settings to get it to work. Try performing a soft reset, by
pressing the stylus pen in the small hole on the bottom left
hand side of the Ipaq and then release.
|XML |xmlLoc_0 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_new bi_xmlPara_new
I would recommend doing a soft reset by pressing the
stylus pen in the small hole on the left hand side of the
Ipaq and then release. Then charge the unit overnight
to make sure it has been long enough and then see what
happens. If the battery is not charging then the unit will
need to be sent in for repair.
</bodyText>
<figureCaption confidence="0.966082">
Figure 2: Sample answers that share a sentence.
</figureCaption>
<bodyText confidence="0.99982140625">
larity enables the re-use of a sentence for dif-
ferent responses, as well as the composition of
partial responses.
The methods developed on the basis of these
two dimensions are: Retrieve Answer, Predict An-
swer, Predict Sentences, Retrieve Sentences and
Hybrid Predict-Retrieve Sentences. The first four
methods represent the possible combinations of
information-gathering technique and level of gran-
ularity; the fifth method is a hybrid where the
two information-gathering techniques are applied
at the sentence level. The generation of re-
sponses under these different methods combines
different aspects of document retrieval, question-
answering, and multi-document summarization.
Our aim in this paper is to investigate when the
different methods are applicable, and whether in-
dividual methods are uniquely successful in cer-
tain situations. For this purpose, we decided to
assign a level of success not only to complete re-
sponses, but also to partial ones (obtained with the
sentence-based methods). The rationale for this
is that we believe that a partial high-precision re-
sponse is better than no response, and better than
a complete response that contains incorrect infor-
mation. We plan to test these assumptions in fu-
ture user studies.
The rest of this paper is organized as follows.
In the next section, we describe our five meth-
ods, followed by the evaluation of their results. In
Section 4, we discuss related research, and then
present our conclusions and plans for future work.
</bodyText>
<page confidence="0.999644">
41
</page>
<sectionHeader confidence="0.999905" genericHeader="method">
2 Information-gathering Methods
</sectionHeader>
<subsectionHeader confidence="0.991804">
2.1 Retrieve a Complete Answer
</subsectionHeader>
<bodyText confidence="0.999969647058824">
This method retrieves a complete document (an-
swer) on the basis of request lemmas. We use co-
sine similarity to determine a retrieval score, and
use a minimal retrieval threshold that must be sur-
passed for a response to be accepted.
We have considered three approaches to index-
ing the answers in our corpus: according to the
content lemmas in (1) requests, (2) answers, or
(3) requests&amp;answers. The results in Section 3 are
for the third approach, which proved best. To il-
lustrate the difference between these approaches,
consider request-answer pair RA2. If we received
a new request similar to that in RA2, the answer
in RA2 would be retrieved if we had indexed ac-
cording to requests or requests&amp;answers. How-
ever, if we had indexed only on answers, then the
response would not be retrieved.
</bodyText>
<subsectionHeader confidence="0.999046">
2.2 Predict a Complete Answer
</subsectionHeader>
<bodyText confidence="0.985283076923077">
This prediction method first groups similar an-
swers in the corpus into answer clusters. For each
request, we then predict an answer cluster on the
basis of the request features, and select the answer
that is most representative of the cluster (closest to
the centroid). This method would predict a group
of answers similar to the answer in RA2 from the
input lemmas “compaq” and “cp-2w”.
The clustering is performed in advance of the
prediction process by the intrinsic classification
program Snob (Wallace and Boulton, 1968), us-
ing the content lemmas (unigrams) in the answers
as features. The predictive model is a Decision
Graph (Oliver, 1993) trained on (1) input features:
unigram and bigram lemmas in the request,3 and
(2) target feature – the identifier of the answer
cluster that contains the actual answer for the re-
quest.4 The model provides a prediction of which
response cluster is most suitable for a given re-
quest, as well as a level of confidence in this pre-
diction. We do not attempt to produce an answer
if the confidence is not sufficiently high.
In principle, rather than clustering the answers,
the predictive model could have been trained on
individual answers. However, on one hand, the
3Significant bigrams are obtained using the NSP package
(http://www.d.umn.edu/˜tpederse/nsp.html).
4At present, the clustering features differ from the predic-
tion features because these parts of the system were devel-
oped at different times. In the near future, we will align these
features.
dimensionality of this task is very high, and on
the other hand, answers that share significant fea-
tures would be predicted together, effectively act-
ing as a cluster. By clustering answers in advance,
we reduce the dimensionality of the problem, at
the expense of some loss of information (since
somewhat dissimilar answers may be grouped to-
gether).
</bodyText>
<subsectionHeader confidence="0.999148">
2.3 Predict Sentences
</subsectionHeader>
<bodyText confidence="0.999986272727273">
This method looks at each answer sentence as
though it were a separate document, and groups
similar sentences into clusters in order to obtain
meaningful sentence abstractions and avoid redun-
dancy.5 For instance, the last sentence in A3 and
the first sentence in A4 are assigned to the same
sentence cluster. As for Answer Prediction (Sec-
tion 2.2), this clustering process also reduces the
dimensionality of the problem.
Each request is used to predict promising clus-
ters of answer sentences, and an answer is com-
posed by extracting a sentence from such clus-
ters. Because the sentences in each cluster orig-
inate in different response documents, the pro-
cess of selecting them for a new response corre-
sponds to multi-document summarization. In fact,
our selection mechanism, described in more de-
tail in (Marom and Zukerman, 2005), is based on
a multi-document summarization formulation pro-
posed by Filatova and Hatzivassiloglou (2004).
In order to be able to generate appropriate an-
swers in this manner, the sentence clusters should
be cohesive, and they should be predicted with
high confidence. A cluster is cohesive if the sen-
tences in it are similar to each other. This means
that it is possible to obtain a sentence that repre-
sents the cluster adequately (which is not the case
for an uncohesive cluster). A high-confidence pre-
diction indicates that the sentence is relevant to
many requests that share certain regularities. Ow-
ing to these requirements, the Sentence Prediction
method will often produce partial answers (i.e., it
will have a high precision, but often a low recall).
</bodyText>
<subsectionHeader confidence="0.864378">
2.3.1 Sentence clustering
</subsectionHeader>
<bodyText confidence="0.998552571428571">
The clustering is performed by applying Snob
using the following sentence-based and word-
based features, all of which proved significant for
5We did not cluster request sentences, as requests are of-
ten ungrammatical, which makes it hard to segment them into
sentences, and the language used in requests is more diverse
than the corporate language used in responses.
</bodyText>
<page confidence="0.993887">
42
</page>
<bodyText confidence="0.7678015">
at least some datasets. The sentence-based fea-
tures are:
</bodyText>
<listItem confidence="0.986782923076923">
• Number of syntactic phrases in the sentence
(e.g., prepositional, subordinate) – gives an
idea of sentence complexity.
• Grammatical mood of the main clause (5
states: imperative, imperative-step, declara-
tive, declarative-step, unknown) – indicates the
function of the sentence in the answer, e.g., an
isolated instruction, part of a sequence of steps,
part of a list of options.
• Grammatical person in the subject of the main
clause (4 states: first, second, third, unknown)
– indicates the agent (e.g., organization or
client) or patient (e.g., product).
</listItem>
<bodyText confidence="0.660672">
The word-based features are binary:
</bodyText>
<listItem confidence="0.9899323">
• Significant lemma bigrams in the subject of the
main clause and in the “augmented” object in
the main clause. This is the syntactic object if
it exists or the subject of a prepositional phrase
in an imperative sentence with no object, e.g.,
“click on the following link.”
• The verbs in the sentence and their polarity (as-
serted or negated).
• All unigrams in the sentence, excluding verbs.
2.3.2 Calculation of cluster cohesion
</listItem>
<bodyText confidence="0.999909473684211">
To measure the textual cohesion of a cluster, we
inspect the centroid values corresponding to the
word features. Due to their binary representation,
the centroid values correspond to probabilities of
the words appearing in the cluster. Our measure is
similar to entropy, in the sense that it yields non-
zero values for extreme probabilities (Marom and
Zukerman, 2005). It implements that idea that a
cohesive group of sentences should agree strongly
on both the words that appear in these sentences
and the words that are omitted. Hence, it is pos-
sible to obtain a sentence that adequately repre-
sents a cohesive sentence cluster, while this is not
the case for a loose sentence cluster. For exam-
ple, the italicized sentences in A3 and A4 belong
to a highly cohesive sentence cluster (0.93), while
the opening answer sentence in RA1 belongs to
a less cohesive cluster (0.7) that contains diverse
sentences about the Rompaq power management.
</bodyText>
<subsubsectionHeader confidence="0.674536">
2.3.3 Sentence-cluster prediction
</subsubsectionHeader>
<bodyText confidence="0.995275733333333">
Unlike Answer Prediction, we use a Support
Vector Machine (SVM) for predicting sentence
clusters. A separate SVM is trained for each sen-
tence cluster, with unigram and bigram lemmas in
a request as input features, and a binary target fea-
ture specifying whether the cluster contains a sen-
tence from the response to this request.
During the prediction stage, the SVMs predict
zero or more clusters for each request. One repre-
sentative sentence (closest to the centroid) is then
extracted from each highly cohesive cluster pre-
dicted with high confidence. These sentences will
appear in the answer (at present, these sentences
are treated as a set, and are not organized into a
coherent reply).
</bodyText>
<subsectionHeader confidence="0.965193">
2.4 Retrieve Sentences
</subsectionHeader>
<bodyText confidence="0.999988722222222">
As for Sentence Prediction (Section 2.3), this
method looks at each answer sentence as though it
were a separate document. For each request sen-
tence, we retrieve candidate answer sentences on
the basis of the match between the content lem-
mas in the request sentence and the answer sen-
tence. For example, while the first answer sen-
tence in RA1 might match the first request sen-
tence in RA1, an answer sentence from a different
response (about re-installing Win98) might match
the second request sentence. The selection of in-
dividual text units from documents implements
ideas from question-answering approaches.
We are mainly interested in answer sentences
that “cover” request sentences, i.e., the terms in
the request should appear in the answer. Hence,
we use recall as the measure for the goodness of a
match, where recall is defined as follows.
</bodyText>
<subsectionHeader confidence="0.44667">
TF.IDF of lemmas in request sent &amp; answer sent
</subsectionHeader>
<bodyText confidence="0.530953">
recall =
</bodyText>
<subsectionHeader confidence="0.607036">
TF.IDF of lemmas in request sentence
</subsectionHeader>
<bodyText confidence="0.877886153846154">
We initially retain the answer sentences whose re-
call exceeds a threshold.6
Once we have the set of candidate answer sen-
tences, we attempt to remove redundant sentences.
This requires the identification of sentences that
are similar to each other — a task for which we
use the sentence clusters described in Section 2.3.
Again, this redundancy-removal step essentially
casts the task as multi-document summarization.
Given a group of answer sentences that belong to
6To assess the goodness of a sentence, we experimented
with f-scores that had different weights for recall and preci-
sion. Our results were insensitive to these variations.
</bodyText>
<page confidence="0.999314">
43
</page>
<bodyText confidence="0.999968571428571">
the same cohesive cluster, we retain the sentence
with the highest recall (in our current trials, a clus-
ter is sufficiently cohesive for this purpose if its
cohesion &gt; 0.7). In addition, we retain all the an-
swer sentences that do not belong to a cohesive
cluster. All the retained sentences will appear in
the answer.
</bodyText>
<subsectionHeader confidence="0.996424">
2.5 Hybrid Predict-Retrieve Sentences
</subsectionHeader>
<bodyText confidence="0.999856">
It is possible that the Sentence Prediction method
predicts a sentence cluster that is not sufficiently
cohesive for a confident selection of a representa-
tive sentence, but instead the ambiguity can be re-
solved through cues in the request. For example,
selecting between a group of sentences concerning
the installation of different drivers might be possi-
ble if the request mentions a specific driver. Thus
the Sentence Prediction method is complemented
with the Sentence Retrieval method to form a hy-
brid, as follows.
</bodyText>
<listItem confidence="0.98693325">
• For highly cohesive clusters predicted with
high confidence, we select a representative sen-
tence as before.
• For clusters with medium cohesion predicted
</listItem>
<bodyText confidence="0.9601732">
with high confidence, we attempt to match the
sentences with the request sentences, using the
Sentence Retrieval method but with a lower re-
call threshold. This reduction takes place be-
cause the high prediction confidence provides
a guarantee that the sentences in the cluster are
suitable for the request, so there is no need for
a convervative recall threshold. The role of re-
trieval is now to select the sentence whose con-
tent lemmas best match the request.
• For uncohesive clusters or clusters predicted
with low confidence, we have to resort to word
matches, which means reverting to the higher,
more convervative recall threshold, because we
no longer have the prediction confidence.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999965875">
As mentioned in Section 1, our corpus was divided
into topic-based datasets. We have observed that
the different datasets lend themselves differently
to the various information-gathering methods de-
scribed in the previous section. In this section, we
examine the overall performance of the five meth-
ods across the corpus, as well as their performance
for different datasets.
</bodyText>
<subsectionHeader confidence="0.989746">
3.1 Measures
</subsectionHeader>
<bodyText confidence="0.999945184210526">
We are interested in two performance indicators:
coverage and quality.
Coverage is the proportion of requests for
which a response can be generated. The various
information gathering methods presented in the
previous section have acceptance criteria that indi-
cate that there is some level of confidence in gen-
erating a response. A request for which a planned
response fails to meet these criteria is not covered,
or addressed, by the system. We are interested in
seeing if the different methods are applicable in
different situations, that is, how exclusively they
address different requests. Note that the sentence-
based methods generate partial responses, which
are considered acceptable so long as they contain
at least one sentence generated with high confi-
dence. In many cases these methods produce obvi-
ous and non-informative sentences such as “Thank
you for contacting HP”, which would be deemed
an acceptable response. We have manually ex-
cluded such sentences from the calculation of cov-
erage, in order to have a more informative compar-
ison between the different methods.
Ideally, the quality of the generated responses
should be measured through a user study, where
people judge the correctness and appropriateness
of answers generated by the different methods.
However, we intend to refine our methods fur-
ther before we conduct such a study. Hence, at
present we rely on a text-based quantitative mea-
sure. Our experimental setup involves a standard
10-fold validation procedure, where we repeatedly
train on 90% of a dataset and test on the remaining
10%. We then evaluate the quality of the answers
generated for the requests in each test split, by
comparing them with the actual responses given
by the help-desk operator for these requests.
We are interested in two quality measures:
</bodyText>
<listItem confidence="0.565983">
(1) the precision of a generated response, and
(2) its overall similarity to the actual response. The
reason for this distinction is that the former does
not penalize for a low recall — it simply mea-
sures how correct the generated text is. As stated
in Section 1, a partial but correct response may be
better than a complete response that contains in-
correct units of information. On the other hand,
more complete responses are favoured over par-
tial ones, and so we use the second measure to get
an overall indication of how correct and complete
a response is. We use the traditional Information
</listItem>
<page confidence="0.999676">
44
</page>
<tableCaption confidence="0.999869">
Table 1: Performance of the different methods, measured as coverage, precision and f-score.
</tableCaption>
<table confidence="0.998958142857143">
Method Coverage Precision Ave (stdev) F-score Ave (stdev)
Answer Retrieval 43% 0.37 (0.34) 0.35 (0.33)
Answer Prediction 29% 0.82 (0.21) 0.82 (0.24)
Sentence Prediction 34% 0.94 (0.13) 0.78 (0.18)
Sentence Retrieval 9% 0.19 (0.19) 0.12 (0.11)
Sentence Hybrid 43% 0.81 (0.29) 0.66 (0.25)
Combined 72% 0.80 (0.25) 0.50 (0.33)
</table>
<bodyText confidence="0.9917615">
Retrieval precision and f-score measures (Salton
and McGill, 1983), employed on a word-by-word
basis, to evaluate the quality of the generated re-
sponses.7
</bodyText>
<subsectionHeader confidence="0.904849">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.99990440625">
Table 1 shows the overall results obtained using
the different methods. We see that combined the
different methods can address 72% of the requests.
That is, at least one of these methods can produce
some non-empty response to 72% of the requests.
Looking at the individual coverages of the differ-
ent methods we see that they must be applicable in
different situations, because the highest individual
coverage is 43%.
The Answer Retrieval method addresses 43% of
the requests, and in fact, about half of these (22%)
are uniquely addressed by this method. However,
in terms of the quality of the generated response,
we see that the performance is very poor (both pre-
cision and f-score have very low averages). Nev-
ertheless, there are some cases where this method
uniquely addresses requests quite well. In three of
the datasets, Answer Retrieval is the only method
that produces good answers, successfully address-
ing 15-20 requests (about 5% of the requests in
these datasets). These requests include several
cases similar to RA2, where the request was sent
to the wrong place. We would expect Answer Pre-
diction to be able to handle such cases as well.
However, when there are not enough similar cases
in the dataset (as is the case with the three datasets
referred to above), Answer Prediction is not able
to generalize from them, and therefore we can only
rely on a new request closely matching an old re-
quest or an old answer.
The Answer Prediction method can address
29% of the requests. Only about a tenth of these
</bodyText>
<footnote confidence="0.540143666666667">
7We have also employed sequence-based measures using
the ROUGE tool set (Lin and Hovy, 2003), with similar re-
sults to those obtained with the word-by-word measure.
</footnote>
<bodyText confidence="0.991937585365854">
are uniquely addressed by this method, but the
generated responses are of a fairly high quality,
with an average precision and f-score of 0.82.
Notice the large standard deviation of these
averages, suggesting a somewhat inconsistent
behaviour. This is due to the fact that this method
gives good results only when complete template
responses are found. In this case, any re-used
response will have a high similarity to the actual
response. However, when this is not the case,
the performance degrades substantially, resulting
in inconsistent behaviour. This behaviour is par-
ticularly prevalent for the “product replacement”
dataset, which comprises 18% of the requests.
The vast majority of the requests in this dataset
ask for a return shipping label to be mailed to the
customer, so that he or she can return a faulty
product. Although these requests often contain
detailed product descriptions, the responses rarely
refer to the actual products, and often contain the
following generic answer.
A5:
Your request for a return airbill has been received and has
been sent for processing. Your replacement airbill will be
sent to you via email within 24 hours.
Answer Retrieval fails in such cases, because each
request has precise information about the actual
product, so a new request can neither match an
old request (about a different product) nor can it
match the generic response. In contrast, Answer
Prediction can ignore the precise information
in the request, and infer from the mention of
a shipping label that the generic response is
appropriate. When we exclude this dataset from
the calculations, both the average precision and
f-score for the Answer Prediction method fall be-
low those of the Sentence Prediction and Hybrid
methods. This means that Answer Prediction is
suitable when requests that share some regularity
receive a complete template answer.
The Sentence Prediction method can find reg-
</bodyText>
<page confidence="0.998344">
45
</page>
<bodyText confidence="0.9714627">
ularities at the sub-document level, and therefore
deal with cases when partial responses can be gen-
erated. It produces such responses for 34% of
the requests, and does so with a consistently high
precision (average 0.94, standard deviation 0.13).
Only an overall 1% of the requests are uniquely
addressed by this method, however, for the cases
that are shared between this method and other
ones, it is useful to compare the actual quality of
the generated response. In 5% of the cases, the
Sentence Prediction method either uniquely ad-
dresses requests, or jointly addresses requests to-
gether with other methods but has a higher f-score.
This means that in some cases a partial response
has a higher quality than a complete one.
Like the document-level Answer Retrieval
method, the Sentence Retrieval method performs
poorly. It is difficult to find an answer sentence
that closely matches a request sentence, and even
when this is possible, the selected sentences tend
to be different to the ones used by the help-desk
operators, hence the low precision and f-score.
This is discussed further below in the context of
the Sentence Hybrid method.
The Sentence Hybrid method extends the Sen-
tence Prediction method by employing sentence
retrieval as well, and thus has a higher coverage
(45%). In fact, the retrieval component serves to
disambiguate between groups of candidate sen-
tences, thus enabling more sentences to be in-
cluded in the generated response. This, however,
is at the expense of precision, as we also saw for
the pure Sentence Retrieval method. Although re-
trieval selects sentences that match closely a given
request, this selection can differ from the “selec-
tions” made by the operator in the actual response.
Precision (and hence f-score) penalizes such sen-
tences, even when they are more appropriate than
those in the model response. For example, con-
sider request-answer pair RA6. The answer is
quite generic, and is used almost identically for
several other requests. The Hybrid method al-
most reproduces this answer, replacing the first
sentence with A7. This sentence, which matches
more request words than the first sentence in the
model answer, was selected from a sentence clus-
ter that is not highly cohesive, and contains sen-
tences that describe different reasons for setting
up a repair (the matching word in A7 is “screen”).
The Hybrid method outperforms the other meth-
ods in about 10% of the cases, where it either
RA6:
My screen is coming up reversed (mirrored). There must
be something loose electronically because if I put the
stylus in it’s hole and move it back and forth, I can get the
screen to display properly momentarily. Please advise
where to send for repairs.
To get the iPAQ serviced, you can call
1-800-phone-number, options 3, 1 (enter a 10
digit phone number), 2. Enter your phone number twice
and then wait for the routing center to put you through
to a technician with Technical Support. They can get the
unit picked up and brought to our service center.
A7:
To get the iPAQ repaired (battery, stylus lock and
screen), please call 1-800-phone-number, options
3, 1 (enter a 10 digit phone number), 2.
uniquely addresses requests, or addresses them
jointly with other methods but produces responses
with a higher f-score.
</bodyText>
<subsectionHeader confidence="0.972398">
3.3 Summary
</subsectionHeader>
<bodyText confidence="0.999988111111111">
In summary, our results show that each of the dif-
ferent methods is applicable in different situations,
all occurring significantly in the corpus, with the
exception of the Sentence Retrieval method. The
Answer Retrieval method uniquely addresses a
large portion of the requests, but many of its at-
tempts are spurious, thus lowering the combined
overall quality shown at the bottom of Table 1 (av-
erage f-score 0.50), calculated by using the best
performing method for each request. The Answer
Prediction method is good at addressing situations
that warrant complete template responses. How-
ever, its confidence criteria might need refining
to lower the variability in quality. The combined
contribution of the sentence-based methods is sub-
stantial (about 15%), suggesting that partial re-
sponses of high precision may be better than com-
plete responses with a lower precision.
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="method">
4 Related Research
</sectionHeader>
<bodyText confidence="0.999934461538462">
There are very few reported attempts at corpus-
based automation of help-desk responses. The re-
trieval system eResponder (Carmel et al., 2000)
is similar to our Answer Retrieval method, where
the system retrieves a list of request-response pairs
and presents a ranked list of responses to the
user. Our results show that due to the repeti-
tions in the responses, multi-document summa-
rization can be used to produce a single (possi-
bly partial) representative response. This is rec-
ognized by Berger and Mittal (2000), who em-
ploy query-relevant summarization to generate re-
sponses. However, their corpus consists of FAQ
</bodyText>
<page confidence="0.99769">
46
</page>
<bodyText confidence="0.999855583333333">
request-response pairs — a significantly different
corpus to ours in that it lacks repetition and redun-
dancy, and where the responses are not personal-
ized. Lapalme and Kosseim (2003) propose a re-
trieval approach similar to our Answer Retrieval
method, and a question-answering approach, but
applied to a corpus of technical documents rather
than request-response pairs. The methods pre-
sented in this paper combine different aspects of
document retrieval, question-answering and multi-
document summarization, applied to a corpus of
repetitive request-response pairs.
</bodyText>
<sectionHeader confidence="0.993708" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999994394736842">
We have presented four basic methods and one
hybrid method for addressing help-desk requests.
The basic methods represent the four ways of com-
bining level of granularity (sentence and docu-
ment) with information-gathering technique (pre-
diction and retrieval). The hybrid method applies
prediction possibly followed by retrieval to infor-
mation at the sentence level. The results show that
with the exception of Sentence Retrieval, the dif-
ferent methods can address a significant portion of
the requests. A future avenue of research is thus
to characterize situations where different methods
are applicable, in order to derive decision proce-
dures that determine the best method automati-
cally. We have also started to investigate an in-
termediate level of granularity: paragraphs.
Our results suggest that the automatic evalua-
tion method requires further consideration. As
seen in Section 3, our f-score penalizes the Sen-
tence Prediction and Hybrid methods when they
produce good answers that are more informative
than the model answer. As mentioned previously,
a user study would provide a more conclusive
evaluation of the system, and could be used to de-
termine preferences regarding partial responses.
Finally, we propose the following extensions to
our current implementation. First, we would like
to improve the representation used for clustering,
prediction and retrieval by using features that in-
corporate word-based similarity metrics (Pedersen
et al., 2004). Secondly, we intend to investigate
a more focused sentence retrieval approach that
utilizes syntactic matching of sentences. For ex-
ample, if a sentence cluster is strongly predicted
by a request, but the cluster is uncohesive because
of a low verb agreement, then the retrieval should
favour the sentences whose verbs match those in
the request.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999915375">
This research was supported in part by grant
LP0347470 from the Australian Research Coun-
cil and by an endowment from Hewlett-Packard.
The authors also thank Hewlett-Packard for the
extensive help-desk data, and Tony Tony for as-
sistance with the sentence-segmentation software,
and Kerri Morgan and Michael Niemann for de-
veloping the syntactic feature extraction code.
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999917452380952">
A. Berger and V.O. Mittal. 2000. Query-relevant sum-
marization using FAQs. In ACL2000 – Proceedings
of the 38th Annual Meeting of the Association for
Computational Linguistics, pages 294–301, Hong
Kong.
D. Carmel, M. Shtalhaim, and A. Soffer. 2000. eRe-
sponder: Electronic question responder. In CoopIS
’02: Proceedings of the 7th International Confer-
ence on Cooperative Information Systems, pages
150–161, Eilat, Israel.
E. Filatova and V. Hatzivassiloglou. 2004. A formal
model for information selection in multi-sentence
text extraction. In COLING’04 – Proceedings of
the 20th International Conference on Computational
Linguistics, pages 397–403, Geneva, Switzerland.
G. Lapalme and L. Kosseim. 2003. Mercure: Towards
an automatic e-mail follow-up system. IEEE Com-
putational Intelligence Bulletin, 2(1):14–18.
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Language Tech-
nology Conference (HLT-NAACL 2003), Edmonton,
Canada.
Y. Marom and I. Zukerman. 2005. Towards a frame-
work for collating help-desk responses from mul-
tiple documents. In Proceedings of the IJCAI05
Workshop on Knowledge and Reasoning forAnswer-
ing Questions, pages 32–39, Edinburgh, Scotland.
J.J. Oliver. 1993. Decision graphs – an extension of
decision trees. In Proceedings of the Fourth Interna-
tional Workshop on Artificial Intelligence and Statis-
tics, pages 343–350, Fort Lauderdale, Florida.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity – measuring the relatedness
of concepts. In AAAI-04 – Proceedings of the
Nineteenth National Conference on Artificial Intel-
ligence, pages 25–29, San Jose, California.
G. Salton and M.J. McGill. 1983. An Introduction to
Modern Information Retrieval. McGraw Hill.
C.S. Wallace and D.M. Boulton. 1968. An information
measure for classification. The Computer Journal,
11(2):185–194.
</reference>
<page confidence="0.99949">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.422966">
<title confidence="0.7598946">Automating Help-desk Responses: A Comparative Study Information-gathering Approaches Yuval Marom and Ingrid Faculty of Information Monash</title>
<address confidence="0.930001">Clayton, VICTORIA 3800,</address>
<abstract confidence="0.998957055555556">We present a comparative study of corpusbased methods for the automatic synthesis of email responses to help-desk requests. Our methods were developed by considering two operational dimensions: (1) information-gathering technique, and (2) granularity of the information. In particular, we investigate two techniques – retrieval and prediction – applied to information represented at two levels of granularity – sentence-level and document level. We also developed a hybrid method that combines prediction with retrieval. Our results show that the different approaches are applicable in different situations, addressing a combined 72% of the requests with either complete or partial responses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>V O Mittal</author>
</authors>
<title>Query-relevant summarization using FAQs.</title>
<date>2000</date>
<booktitle>In ACL2000 – Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>294--301</pages>
<location>Hong Kong.</location>
<contexts>
<context position="30904" citStr="Berger and Mittal (2000)" startWordPosition="4970" endWordPosition="4973">ial responses of high precision may be better than complete responses with a lower precision. 4 Related Research There are very few reported attempts at corpusbased automation of help-desk responses. The retrieval system eResponder (Carmel et al., 2000) is similar to our Answer Retrieval method, where the system retrieves a list of request-response pairs and presents a ranked list of responses to the user. Our results show that due to the repetitions in the responses, multi-document summarization can be used to produce a single (possibly partial) representative response. This is recognized by Berger and Mittal (2000), who employ query-relevant summarization to generate responses. However, their corpus consists of FAQ 46 request-response pairs — a significantly different corpus to ours in that it lacks repetition and redundancy, and where the responses are not personalized. Lapalme and Kosseim (2003) propose a retrieval approach similar to our Answer Retrieval method, and a question-answering approach, but applied to a corpus of technical documents rather than request-response pairs. The methods presented in this paper combine different aspects of document retrieval, question-answering and multidocument su</context>
</contexts>
<marker>Berger, Mittal, 2000</marker>
<rawString>A. Berger and V.O. Mittal. 2000. Query-relevant summarization using FAQs. In ACL2000 – Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 294–301, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Carmel</author>
<author>M Shtalhaim</author>
<author>A Soffer</author>
</authors>
<title>eResponder: Electronic question responder.</title>
<date>2000</date>
<booktitle>In CoopIS ’02: Proceedings of the 7th International Conference on Cooperative Information Systems,</booktitle>
<pages>150--161</pages>
<location>Eilat,</location>
<contexts>
<context position="30533" citStr="Carmel et al., 2000" startWordPosition="4908" endWordPosition="4911">0), calculated by using the best performing method for each request. The Answer Prediction method is good at addressing situations that warrant complete template responses. However, its confidence criteria might need refining to lower the variability in quality. The combined contribution of the sentence-based methods is substantial (about 15%), suggesting that partial responses of high precision may be better than complete responses with a lower precision. 4 Related Research There are very few reported attempts at corpusbased automation of help-desk responses. The retrieval system eResponder (Carmel et al., 2000) is similar to our Answer Retrieval method, where the system retrieves a list of request-response pairs and presents a ranked list of responses to the user. Our results show that due to the repetitions in the responses, multi-document summarization can be used to produce a single (possibly partial) representative response. This is recognized by Berger and Mittal (2000), who employ query-relevant summarization to generate responses. However, their corpus consists of FAQ 46 request-response pairs — a significantly different corpus to ours in that it lacks repetition and redundancy, and where the</context>
</contexts>
<marker>Carmel, Shtalhaim, Soffer, 2000</marker>
<rawString>D. Carmel, M. Shtalhaim, and A. Soffer. 2000. eResponder: Electronic question responder. In CoopIS ’02: Proceedings of the 7th International Conference on Cooperative Information Systems, pages 150–161, Eilat, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filatova</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multi-sentence text extraction.</title>
<date>2004</date>
<booktitle>In COLING’04 – Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>397--403</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="5983" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="909" endWordPosition="913">ect an information item. For example, the absence of a particular term in a request may be a good predictive feature (which cannot be considered in traditional retrieval). Thus, prediction could yield replies that do not match particular query terms. • Observation O2 leads us to consider two levels of granularity: document and sentence. That is, we can obtain a document comprising a complete answer on the basis of a request (i.e., reuse an answer to a previous request), or we can obtain individual sentences and then combine them to compose an answer, as is done in multidocument summarization (Filatova and Hatzivassiloglou, 2004). The sentence-level granu|XML |xmlLoc_0 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_new bi_xmlPara_new If you are able to see the Internet then it sounds like it is working, you may want to get in touch with your IT department to see if you need to make any changes to your settings to get it to work. Try performing a soft reset, by pressing the stylus pen in the small hole on the bottom left hand side of the Ipaq and then release. |XML |xmlLoc_0 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_new bi_xm</context>
<context position="12130" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="1909" endWordPosition="1912">nce cluster. As for Answer Prediction (Section 2.2), this clustering process also reduces the dimensionality of the problem. Each request is used to predict promising clusters of answer sentences, and an answer is composed by extracting a sentence from such clusters. Because the sentences in each cluster originate in different response documents, the process of selecting them for a new response corresponds to multi-document summarization. In fact, our selection mechanism, described in more detail in (Marom and Zukerman, 2005), is based on a multi-document summarization formulation proposed by Filatova and Hatzivassiloglou (2004). In order to be able to generate appropriate answers in this manner, the sentence clusters should be cohesive, and they should be predicted with high confidence. A cluster is cohesive if the sentences in it are similar to each other. This means that it is possible to obtain a sentence that represents the cluster adequately (which is not the case for an uncohesive cluster). A high-confidence prediction indicates that the sentence is relevant to many requests that share certain regularities. Owing to these requirements, the Sentence Prediction method will often produce partial answers (i.e., it</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>E. Filatova and V. Hatzivassiloglou. 2004. A formal model for information selection in multi-sentence text extraction. In COLING’04 – Proceedings of the 20th International Conference on Computational Linguistics, pages 397–403, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lapalme</author>
<author>L Kosseim</author>
</authors>
<title>Mercure: Towards an automatic e-mail follow-up system.</title>
<date>2003</date>
<journal>IEEE Computational Intelligence Bulletin,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="31192" citStr="Lapalme and Kosseim (2003)" startWordPosition="5015" endWordPosition="5018">al method, where the system retrieves a list of request-response pairs and presents a ranked list of responses to the user. Our results show that due to the repetitions in the responses, multi-document summarization can be used to produce a single (possibly partial) representative response. This is recognized by Berger and Mittal (2000), who employ query-relevant summarization to generate responses. However, their corpus consists of FAQ 46 request-response pairs — a significantly different corpus to ours in that it lacks repetition and redundancy, and where the responses are not personalized. Lapalme and Kosseim (2003) propose a retrieval approach similar to our Answer Retrieval method, and a question-answering approach, but applied to a corpus of technical documents rather than request-response pairs. The methods presented in this paper combine different aspects of document retrieval, question-answering and multidocument summarization, applied to a corpus of repetitive request-response pairs. 5 Conclusion and Future Work We have presented four basic methods and one hybrid method for addressing help-desk requests. The basic methods represent the four ways of combining level of granularity (sentence and docu</context>
</contexts>
<marker>Lapalme, Kosseim, 2003</marker>
<rawString>G. Lapalme and L. Kosseim. 2003. Mercure: Towards an automatic e-mail follow-up system. IEEE Computational Intelligence Bulletin, 2(1):14–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Language Technology Conference (HLT-NAACL</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="24239" citStr="Lin and Hovy, 2003" startWordPosition="3886" endWordPosition="3889">include several cases similar to RA2, where the request was sent to the wrong place. We would expect Answer Prediction to be able to handle such cases as well. However, when there are not enough similar cases in the dataset (as is the case with the three datasets referred to above), Answer Prediction is not able to generalize from them, and therefore we can only rely on a new request closely matching an old request or an old answer. The Answer Prediction method can address 29% of the requests. Only about a tenth of these 7We have also employed sequence-based measures using the ROUGE tool set (Lin and Hovy, 2003), with similar results to those obtained with the word-by-word measure. are uniquely addressed by this method, but the generated responses are of a fairly high quality, with an average precision and f-score of 0.82. Notice the large standard deviation of these averages, suggesting a somewhat inconsistent behaviour. This is due to the fact that this method gives good results only when complete template responses are found. In this case, any re-used response will have a high similarity to the actual response. However, when this is not the case, the performance degrades substantially, resulting i</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Marom</author>
<author>I Zukerman</author>
</authors>
<title>Towards a framework for collating help-desk responses from multiple documents.</title>
<date>2005</date>
<booktitle>In Proceedings of the IJCAI05 Workshop on Knowledge and Reasoning forAnswering Questions,</booktitle>
<pages>32--39</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="12025" citStr="Marom and Zukerman, 2005" startWordPosition="1895" endWordPosition="1898"> instance, the last sentence in A3 and the first sentence in A4 are assigned to the same sentence cluster. As for Answer Prediction (Section 2.2), this clustering process also reduces the dimensionality of the problem. Each request is used to predict promising clusters of answer sentences, and an answer is composed by extracting a sentence from such clusters. Because the sentences in each cluster originate in different response documents, the process of selecting them for a new response corresponds to multi-document summarization. In fact, our selection mechanism, described in more detail in (Marom and Zukerman, 2005), is based on a multi-document summarization formulation proposed by Filatova and Hatzivassiloglou (2004). In order to be able to generate appropriate answers in this manner, the sentence clusters should be cohesive, and they should be predicted with high confidence. A cluster is cohesive if the sentences in it are similar to each other. This means that it is possible to obtain a sentence that represents the cluster adequately (which is not the case for an uncohesive cluster). A high-confidence prediction indicates that the sentence is relevant to many requests that share certain regularities.</context>
<context position="14628" citStr="Marom and Zukerman, 2005" startWordPosition="2314" endWordPosition="2317">a prepositional phrase in an imperative sentence with no object, e.g., “click on the following link.” • The verbs in the sentence and their polarity (asserted or negated). • All unigrams in the sentence, excluding verbs. 2.3.2 Calculation of cluster cohesion To measure the textual cohesion of a cluster, we inspect the centroid values corresponding to the word features. Due to their binary representation, the centroid values correspond to probabilities of the words appearing in the cluster. Our measure is similar to entropy, in the sense that it yields nonzero values for extreme probabilities (Marom and Zukerman, 2005). It implements that idea that a cohesive group of sentences should agree strongly on both the words that appear in these sentences and the words that are omitted. Hence, it is possible to obtain a sentence that adequately represents a cohesive sentence cluster, while this is not the case for a loose sentence cluster. For example, the italicized sentences in A3 and A4 belong to a highly cohesive sentence cluster (0.93), while the opening answer sentence in RA1 belongs to a less cohesive cluster (0.7) that contains diverse sentences about the Rompaq power management. 2.3.3 Sentence-cluster pred</context>
</contexts>
<marker>Marom, Zukerman, 2005</marker>
<rawString>Y. Marom and I. Zukerman. 2005. Towards a framework for collating help-desk responses from multiple documents. In Proceedings of the IJCAI05 Workshop on Knowledge and Reasoning forAnswering Questions, pages 32–39, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Oliver</author>
</authors>
<title>Decision graphs – an extension of decision trees.</title>
<date>1993</date>
<booktitle>In Proceedings of the Fourth International Workshop on Artificial Intelligence and Statistics,</booktitle>
<pages>343--350</pages>
<location>Fort Lauderdale, Florida.</location>
<contexts>
<context position="9972" citStr="Oliver, 1993" startWordPosition="1563" endWordPosition="1564">ps similar answers in the corpus into answer clusters. For each request, we then predict an answer cluster on the basis of the request features, and select the answer that is most representative of the cluster (closest to the centroid). This method would predict a group of answers similar to the answer in RA2 from the input lemmas “compaq” and “cp-2w”. The clustering is performed in advance of the prediction process by the intrinsic classification program Snob (Wallace and Boulton, 1968), using the content lemmas (unigrams) in the answers as features. The predictive model is a Decision Graph (Oliver, 1993) trained on (1) input features: unigram and bigram lemmas in the request,3 and (2) target feature – the identifier of the answer cluster that contains the actual answer for the request.4 The model provides a prediction of which response cluster is most suitable for a given request, as well as a level of confidence in this prediction. We do not attempt to produce an answer if the confidence is not sufficiently high. In principle, rather than clustering the answers, the predictive model could have been trained on individual answers. However, on one hand, the 3Significant bigrams are obtained usi</context>
</contexts>
<marker>Oliver, 1993</marker>
<rawString>J.J. Oliver. 1993. Decision graphs – an extension of decision trees. In Proceedings of the Fourth International Workshop on Artificial Intelligence and Statistics, pages 343–350, Fort Lauderdale, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet::Similarity – measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In AAAI-04 – Proceedings of the Nineteenth National Conference on Artificial Intelligence,</booktitle>
<pages>25--29</pages>
<location>San Jose, California.</location>
<contexts>
<context position="33065" citStr="Pedersen et al., 2004" startWordPosition="5294" endWordPosition="5297">es further consideration. As seen in Section 3, our f-score penalizes the Sentence Prediction and Hybrid methods when they produce good answers that are more informative than the model answer. As mentioned previously, a user study would provide a more conclusive evaluation of the system, and could be used to determine preferences regarding partial responses. Finally, we propose the following extensions to our current implementation. First, we would like to improve the representation used for clustering, prediction and retrieval by using features that incorporate word-based similarity metrics (Pedersen et al., 2004). Secondly, we intend to investigate a more focused sentence retrieval approach that utilizes syntactic matching of sentences. For example, if a sentence cluster is strongly predicted by a request, but the cluster is uncohesive because of a low verb agreement, then the retrieval should favour the sentences whose verbs match those in the request. Acknowledgments This research was supported in part by grant LP0347470 from the Australian Research Council and by an endowment from Hewlett-Packard. The authors also thank Hewlett-Packard for the extensive help-desk data, and Tony Tony for assistance </context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet::Similarity – measuring the relatedness of concepts. In AAAI-04 – Proceedings of the Nineteenth National Conference on Artificial Intelligence, pages 25–29, San Jose, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>An Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="5211" citStr="Salton and McGill, 1983" startWordPosition="782" endWordPosition="785">g in-house manuals that prescribe how to generate an answer. For instance, answers A3 and A4 in Figure 2 share the sentence in italics. These observations prompt us to consider complementary approaches along two separate dimensions of our problem. The first dimension pertains to the technique applied to determine the information in an answer, and the second dimension pertains to the granularity of the information. • Observation O1 leads us to consider two techniques for obtaining information: retrieval and prediction. Retrieval returns an information item by matching its terms to query terms (Salton and McGill, 1983). Hence, it is likely to obtain precise information if available. In contrast, prediction uses features of requests and responses to select an information item. For example, the absence of a particular term in a request may be a good predictive feature (which cannot be considered in traditional retrieval). Thus, prediction could yield replies that do not match particular query terms. • Observation O2 leads us to consider two levels of granularity: document and sentence. That is, we can obtain a document comprising a complete answer on the basis of a request (i.e., reuse an answer to a previous</context>
<context position="22528" citStr="Salton and McGill, 1983" startWordPosition="3595" endWordPosition="3598">nd so we use the second measure to get an overall indication of how correct and complete a response is. We use the traditional Information 44 Table 1: Performance of the different methods, measured as coverage, precision and f-score. Method Coverage Precision Ave (stdev) F-score Ave (stdev) Answer Retrieval 43% 0.37 (0.34) 0.35 (0.33) Answer Prediction 29% 0.82 (0.21) 0.82 (0.24) Sentence Prediction 34% 0.94 (0.13) 0.78 (0.18) Sentence Retrieval 9% 0.19 (0.19) 0.12 (0.11) Sentence Hybrid 43% 0.81 (0.29) 0.66 (0.25) Combined 72% 0.80 (0.25) 0.50 (0.33) Retrieval precision and f-score measures (Salton and McGill, 1983), employed on a word-by-word basis, to evaluate the quality of the generated responses.7 3.2 Results Table 1 shows the overall results obtained using the different methods. We see that combined the different methods can address 72% of the requests. That is, at least one of these methods can produce some non-empty response to 72% of the requests. Looking at the individual coverages of the different methods we see that they must be applicable in different situations, because the highest individual coverage is 43%. The Answer Retrieval method addresses 43% of the requests, and in fact, about half</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill. 1983. An Introduction to Modern Information Retrieval. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wallace</author>
<author>D M Boulton</author>
</authors>
<title>An information measure for classification.</title>
<date>1968</date>
<journal>The Computer Journal,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="9851" citStr="Wallace and Boulton, 1968" startWordPosition="1541" endWordPosition="1544">had indexed only on answers, then the response would not be retrieved. 2.2 Predict a Complete Answer This prediction method first groups similar answers in the corpus into answer clusters. For each request, we then predict an answer cluster on the basis of the request features, and select the answer that is most representative of the cluster (closest to the centroid). This method would predict a group of answers similar to the answer in RA2 from the input lemmas “compaq” and “cp-2w”. The clustering is performed in advance of the prediction process by the intrinsic classification program Snob (Wallace and Boulton, 1968), using the content lemmas (unigrams) in the answers as features. The predictive model is a Decision Graph (Oliver, 1993) trained on (1) input features: unigram and bigram lemmas in the request,3 and (2) target feature – the identifier of the answer cluster that contains the actual answer for the request.4 The model provides a prediction of which response cluster is most suitable for a given request, as well as a level of confidence in this prediction. We do not attempt to produce an answer if the confidence is not sufficiently high. In principle, rather than clustering the answers, the predic</context>
</contexts>
<marker>Wallace, Boulton, 1968</marker>
<rawString>C.S. Wallace and D.M. Boulton. 1968. An information measure for classification. The Computer Journal, 11(2):185–194.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>