<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018893">
<title confidence="0.981751">
BALI: Automatic weighting of text window distances
</title>
<author confidence="0.944232">
Bernard Brosseau-Villeneuve*#, Noriko Kando#, Jian-Yun Nie*
</author>
<affiliation confidence="0.6276975">
* Université de Montréal, Email: {brosseab, nie}@iro.umontreal.ca
# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jp
</affiliation>
<sectionHeader confidence="0.975844" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999826384615385">
Systems using text windows to model
word contexts have mostly been using
fixed-sized windows and uniform weights.
The window size is often selected by trial
and error to maximize task results. We
propose a non-supervised method for se-
lecting weights for each window distance,
effectively removing the need to limit win-
dow sizes, by maximizing the mutual gen-
eration of two sets of samples of the same
word. Experiments on Semeval Word
Sense Disambiguation tasks showed con-
siderable improvements.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998144520000001">
The meaning of a word can be defined by the
words that accompany it in the text. This is the
principle often used in previous studies on Word
Sense Disambiguation (WSD) (Ide and Véronis,
1998; Navigli, 2009). In general, the accompa-
nying words form a context vector of the target
word, or a probability distribution of the context
words. For example, under the unigram bag-of-
word assumption, this means building p(x1t) _
∑�� count�x�,t), where count(x, t) is the count of
count�x,t)
co-occurrences of word x with the target word t
under a certain criterion. In most studies, x and t
should co-occur within a window of up to k words
or sentences. The bounds are usually selected as
to maximize system performance. Occurrences in-
side the window usually weight the same with-
out regard to their position. This is counterintu-
itive. Indeed, a word closer to the target word usu-
ally has a greater semantic constraint on the tar-
get word than a more distant word. Some studies
have also proposed decaying factors to decrease
the importance of more distant words in the con-
text vector. However, the decaying functions are
defined manually. It is unclear that the functions
defined can capture the true impact of the con-
text words on the target word. In this paper, we
propose an unsupervised method to automatically
learn the optimal weight of a word according to its
distance to the target word. The general idea used
to determine such weight is that, if we randomly
determine two sets of texts containing the target
word, the resulting probability distributions for its
context words in the two sets should be similar.
Therefore, the weights of context words at differ-
ent distance are determined so as to maximize the
mutual generation probabilities of two sets of sam-
ples. Experimentation on Semeval-2007 English
and Semeval-2010 Japanese lexical sample task
data shows that improvements can automatically
be attained on simple Naive Bayes (NB) systems
in comparison to the best manually selected fixed
window system.
The remainder of this paper is organized as fol-
lows: example uses of text windows and related
work are presented in Section 2. Our method
is presented in Section 3. In Section 4 and
5, we show experimental results on English and
Japanese WSD. We conclude in Section 6 with
discussion and further possible extensions.
</bodyText>
<sectionHeader confidence="0.755002" genericHeader="method">
2 Uses of text windows
</sectionHeader>
<bodyText confidence="0.999762785714286">
Modeling the distribution of words around one
target word has many uses. For instance, the
Xu&amp;Croft co-occurrence-based stemmer (Xu and
Croft, 1998) uses window co-occurrence statis-
tics to calculate the best equivalence classes for
a group of word forms. They suggest using win-
dows of up to 100 words. Another example can be
found in WSD systems, where a shorter window is
preferred. In Semeval-2007, top performing sys-
tems on WSD tasks, such as NUS-ML (Cai et al.,
2007), made use of bag-of-word features around
the target word. In this case, they found that the
best results can be achieved using a window size
of 3.
</bodyText>
<page confidence="0.987325">
375
</page>
<bodyText confidence="0.963883434782609">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 375–378,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
Both these systems limit the size of their win-
dows for different purposes. The former aims to
model the topic of the documents containing the
word rather than the word’s meaning. The latter
limits the size because bag-of-word features fur-
ther from the target word would not be sufficiently
related to its meaning (Ide and Véronis, 1998). We
see that because of sparsity issues, there is a com-
promise between taking few, highly related words,
or taking several, lower quality words.
In most current systems, all words in a window
are given equal weight, but we can easily under-
stand that the occurrences of words should gener-
ally count less as they become farther; they form
a long tail that we should use. Previous work pro-
posed using non-linear functions of the distance
to model the relation between two words. For in-
stance, improvements can be obtained by using an
exponential function (Gao et al., 2002). Yet, there
is no evidence that the exponential – with its man-
ually selected parameter – is the best function.
</bodyText>
<sectionHeader confidence="0.931703" genericHeader="method">
3 Computing weights for distances
</sectionHeader>
<bodyText confidence="0.999976428571429">
In this section, we present our method for choos-
ing how much a word should count according to its
distance to the target word. First, for some defini-
tions, let C be a corpus, W a set of text windows,
cW,i,x the count of occurrences of word x at dis-
tance i in W, cW,i the sum of these counts, and αi
the weight put on one word at distance i. Then,
</bodyText>
<equation confidence="0.901481666666667">
∑i αicW,i,x
PML,W (x) = (1)
∑i αicW,i
</equation>
<bodyText confidence="0.99986425">
is the maximum likelihood estimator for x. To
counter the zero-probability problem, we apply
Dirichlet smoothing with the collection language
model as a prior:
</bodyText>
<equation confidence="0.999305">
∑i αicW,i,x + µW P(x|C)
PDir,W (x) = ∑ (2)
i αicW,i + µW
</equation>
<bodyText confidence="0.9991544">
The pseudo-count µW is found by using Newton’s
method via leave-one-out estimation. We follow
the procedure shown in (Zhai and Lafferty, 2002),
but since occurrences have different weights, the
log-likelihood is changed to
</bodyText>
<equation confidence="0.996314666666667">
G−1(µ|W,C) = (3)
&amp; ∑xEV αicW,i,x log ai+ ∑. a c
� � W,j− a µ
</equation>
<bodyText confidence="0.997913">
To find the best weights for our model we pro-
pose the following:
</bodyText>
<listItem confidence="0.988815166666667">
• Let T be the set of all windows containing
the target word. We randomly split this set
into two sets A and B.
• We want to find α? that maximizes the mu-
tual generation of the two sets, by minimizing
their cross-entropy:
</listItem>
<equation confidence="0.8754305">
l(α) = H(PML,A, PDir,B) + H(PML,B, PDir,A)
(4)
</equation>
<bodyText confidence="0.9992185">
In other words, we want αi to represent how
much an occurrence at distance i models the con-
text better than the collection language model,
whose counts are controlled by the Dirichlet
pseudo-count. We hypothesize that target words
occurs in limited contexts, and as we get farther
from them, the possibilities become greater, re-
sulting in sparse and less related counts.
</bodyText>
<subsectionHeader confidence="0.996921">
3.1 Gradient descent
</subsectionHeader>
<bodyText confidence="0.999997833333333">
We propose a simple gradient descent minimiz-
ing (4) over α. For the following experiments,
we used one single curve for all words in a task.
We used the mini-batch type of gradient descent:
the gradients of a fixed amount of target words are
summed, a gradient step is done, and the proces
is repeated while cycling the data. The starting
state was with all αi to one, the batch size of 50
and a learning rate of 1. We notice that as the al-
gorithm progress, weights on close distances in-
crease and the farthest decrease. As further dis-
tances contribute less and less, middle distances
start to decay more and more, until at some point,
all distances but the closest start to decrease, head-
ing towards a degenerate solution. We therefore
suggest using the observation of several consecu-
tive decreases of all except α1 as an end criterion.
We used 10 consecutive steps for our experiments.
</bodyText>
<sectionHeader confidence="0.993954" genericHeader="method">
4 Experiments on Semeval-2007 English
</sectionHeader>
<subsectionHeader confidence="0.599282">
Lexical Sample
</subsectionHeader>
<bodyText confidence="0.956655666666667">
The Semeval workshop holds WSD tasks such as
the English Lexical Sample (ELS) (Pradhan et al.,
2007). It consists of a selected set of polysemous
words, contained within passages where a sense
taken from a sense inventory is manually anno-
tated. The task is to create supervised classifiers
maximizing accuracy on test data.
Since there are only 50 words and instances are
few, we judged there was not enough data to com-
pute weights. Instead, we used the AP Newswire
corpus of the TREC collection (CD 1 &amp; 2). Words
αicW,i,x−αi+µP(xJC)
</bodyText>
<page confidence="0.994259">
376
</page>
<bodyText confidence="0.9998544">
were stemmed with the Porter stemmer and text
windows were grouped for all words. For sim-
plicity and efficiency, windows to the right and to
the left were considered independent, and we only
kept words with between 30 and 1000 windows.
Also, only windows with a size of 100, which was
considered big enough without any doubt, were
kept. A stop list of the top 10 frequent words was
used, but place holders were left in the windows to
preserve the distances. Multiple consecutive stop
words (ex: “of the”) were merged, and the tar-
get word, being the same for all samples of a set,
was ignored. This results in 32,650 sets contain-
ing 5,870,604 windows. In Figure 1, we can see
the resulting weight curve.
</bodyText>
<figure confidence="0.998733125">
weight 1.0
0.8
0.6
0.4
0.2
0.0
0 20 40 60 80 100
distance
</figure>
<figureCaption confidence="0.99999">
Figure 1: Weight curve for AP Newswire
</figureCaption>
<bodyText confidence="0.999853181818182">
Since the curve converges, words over the 100th
distance were assigned the minimum weight found
in the curve. From this we constructed NB models
whose class priors used an absolute discounting of
0.5. The collection language model used the con-
catenation of the AP collection and the Semeval
data. As the unstemmed target word is an impor-
tant feature it was added to the models. It’s weight
was chosen to be 0.7 by maximizing accuracy on
one-held-out cross-validation of the training data.
The results are listed in Table 1.
</bodyText>
<table confidence="0.9916605">
System Cross-Val (%) Test set (%)
Prior only 78.66 77.76
Best uniform 85.48 83.28
RALI-2 88.23 86.45
</table>
<tableCaption confidence="0.999857">
Table 1: WSD accuracy on Semeval-2007 ELC
</tableCaption>
<bodyText confidence="0.999930529411765">
We used two baselines: most frequent sense
(prior only), and the best uniform (except target
word) fixed size window found from extensive
search on the training data. The best settings were
a window of size 4, with a weight of 4.4 on the
target word and a Laplace smoothing of 2.9. The
improvements seen using our system are substan-
tial, beating most of the systems originally pro-
posed for the task (Pradhan et al., 2007). Out
of 15 systems, the best results had accuracies of
89.1*, 89.1*, 88.7, 86.9 and 86.4 (* indicates post-
competition submissions). Notice that most were
using Support Vector Machine (SVM) with bag-
of-word features in a very small window, local col-
locations and POS tags. In our future work, we
will investigate the applications of SVM with our
new term weighting scheme.
</bodyText>
<sectionHeader confidence="0.901204" genericHeader="method">
5 Experiments on Semeval-2010
Japanese WSD
</sectionHeader>
<bodyText confidence="0.999894774193548">
The Semeval-2010 Japanese WSD task (Okumura
et al., 2010) consists of 50 polysemous words
for which examples were taken from the BC-
CWJ tagged corpus. It was manually segmented,
tagged, and annotated with senses taken from the
Iwanami Kokugo dictionary. The task is identical
to the ELS of the previous experiment.
Since the data was again insufficient to com-
pute curves, we used the Mainichi-2005 corpus of
NTCIR-8. We tried to reproduce the same kind
of segmentation as the training data by using the
Chasen parser with UniDic. For the corpus and
Semeval data, conjugations (setsuzoku-to, jodô-
shi, etc.), particles (all jo-shi), symbols (blanks,
kigô, etc.), and numbers were stripped. When a
base-form reading was present (for verbs and ad-
jectives), the token was replaced by the Kanjis
(chinese characters) in the word writing concate-
nated with the base-form reading. This treatment
is somewhat equivalent to the stemming+stop list
of the ELS tasks. The resulting curve can be seen
in Figure 2.
The NB models are the same as in the previous
experiments. Target words were again added the
same way as in the ELS task. The best fixed win-
dow model was found to have a window size of 1
with a target word weight of 0.6 and used manual
Dirichlet smoothing with a pseudo-count of 110.
We submited two systems with the following set-
tings: RALI-1 used manual Dirichlet smoothing
and 0.9 for the target word. RALI-2 used auto-
</bodyText>
<page confidence="0.973101">
377
</page>
<figure confidence="0.9978945">
weight 1.0
0.8
0.6
0.4
0.2
0.0
0 20 40 60 80 100
distance
</figure>
<figureCaption confidence="0.924683">
Figure 2: Weight curve for Mainichi Shinbun 2005
matic Dirichlet smoothing and 1.7 for the target
word weight. Results are listed in Table 2.
</figureCaption>
<table confidence="0.9974002">
System Cross-Val (%) Test set (%)
prior only 75.23 68.96
Best uniform 82.29 76.12
RALI-1 82.77 75.92
RALI-2 83.05 76.36
</table>
<tableCaption confidence="0.99885">
Table 2: WSD accuracy on Semeval-2010 JWSD
</tableCaption>
<bodyText confidence="0.999986888888889">
As we can see, the results are not significantly
different from the best uniform model. This may
be due to differences in the segmentation parame-
ters of our external corpus. Another reason could
be that the systems use almost the same weights:
the best fixed window had size 1, and the Japanese
curve is steeper than the English one.
This steeper curve can be explained by the
grammatical structure of the Japanese language.
While English can be considered a Subject-
Verb-Complement language, Japanese is consid-
ered Subject-Complement-Verb. Verbs are mostly
found at the end of the sentence, far from their sub-
ject, and vice versa. The window distance is there-
fore less useful in Japanese than in English since
it has more non-local dependencies. These results
show that the curves work as expected even in dif-
ferent languages.
</bodyText>
<sectionHeader confidence="0.999666" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999784083333333">
This paper proposed an unsupervised method for
finding weights for counts in text windows ac-
cording to their distance to the target word. Re-
sults from the Semeval-2007 English lexical sam-
ple showed a substantial improvement in preci-
sion. Yet, as we have seen with the Japanese task,
window distance is not always a good indicator of
word relatedness. Fortunately, we can easily imag-
ine extensions to the current scheme that bins word
counts by factors other than word distance. For in-
stance, we could also bin counts by parsing tree
distance, sentence distance or POS-tags.
</bodyText>
<sectionHeader confidence="0.998279" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999102857142857">
The authors would like to thank Florian Boudin
and Satoko Fujisawa for helpful comments on
this work. This work is partially supported
by Japanese MEXT Grant-in-Aid for Scientific
Research on Info-plosion (#21013046) and the
Japanese MEXT Research Student Scholarship
program.
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99822396969697">
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml: improving word sense disambiguation us-
ing topic features. In SemEval ’07 Proceedings,
pages 249–252, Morristown, NJ, USA. Association
for Computational Linguistics.
Jianfeng Gao, Ming Zhou, Jian-Yun Nie, Hongzhao
He, and Weijun Chen. 2002. Resolving query trans-
lation ambiguity using a decaying co-occurrence
model and syntactic dependence relations. In SI-
GIR ’02 Proceedings, pages 183–190, New York,
NY, USA. ACM.
Nancy Ide and Jean Véronis. 1998. Introduction to
the special issue on word sense disambiguation: the
state of the art. Comput. Linguist., 24(1):2–40.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Comput. Surv., 41(2):1–69.
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese wsd. In SemEval ’10 Proceedings. Associ-
ation for Computational Linguistics.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Se-
mEval ’07 Proceedings, pages 87–92, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Jinxi Xu and W. Bruce Croft. 1998. Corpus-
based stemming using cooccurrence of word vari-
ants. ACM Trans. Inf. Syst., 16(1):61–81.
ChengXiang Zhai and John Lafferty. 2002. Two-stage
language models for information retrieval. In SIGIR
’02 Proceedings, pages 49–56, New York, NY, USA.
ACM.
</reference>
<page confidence="0.998223">
378
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.857631">
<title confidence="0.998896">BALI: Automatic weighting of text window distances</title>
<author confidence="0.999268">Bernard Brosseau-Villeneuve</author>
<author confidence="0.999268">Noriko Kando</author>
<author confidence="0.999268">Jian-Yun Nie</author>
<affiliation confidence="0.9415095">Université de Montréal, Email: nie}@iro.umontreal.ca National Institute of Informatics, Email: kando}@nii.ac.jp</affiliation>
<abstract confidence="0.9968435">Systems using text windows to model word contexts have mostly been using fixed-sized windows and uniform weights. The window size is often selected by trial and error to maximize task results. We propose a non-supervised method for selecting weights for each window distance, effectively removing the need to limit window sizes, by maximizing the mutual generation of two sets of samples of the same word. Experiments on Semeval Word Sense Disambiguation tasks showed considerable improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jun Fu Cai</author>
<author>Wee Sun Lee</author>
<author>Yee Whye Teh</author>
</authors>
<title>Nus-ml: improving word sense disambiguation using topic features.</title>
<date>2007</date>
<booktitle>In SemEval ’07 Proceedings,</booktitle>
<pages>249--252</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3591" citStr="Cai et al., 2007" startWordPosition="582" endWordPosition="585">xperimental results on English and Japanese WSD. We conclude in Section 6 with discussion and further possible extensions. 2 Uses of text windows Modeling the distribution of words around one target word has many uses. For instance, the Xu&amp;Croft co-occurrence-based stemmer (Xu and Croft, 1998) uses window co-occurrence statistics to calculate the best equivalence classes for a group of word forms. They suggest using windows of up to 100 words. Another example can be found in WSD systems, where a shorter window is preferred. In Semeval-2007, top performing systems on WSD tasks, such as NUS-ML (Cai et al., 2007), made use of bag-of-word features around the target word. In this case, they found that the best results can be achieved using a window size of 3. 375 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 375–378, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Both these systems limit the size of their windows for different purposes. The former aims to model the topic of the documents containing the word rather than the word’s meaning. The latter limits the size because bag-of-word features further from the target word would </context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. Nus-ml: improving word sense disambiguation using topic features. In SemEval ’07 Proceedings, pages 249–252, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Ming Zhou</author>
<author>Jian-Yun Nie</author>
<author>Hongzhao He</author>
<author>Weijun Chen</author>
</authors>
<title>Resolving query translation ambiguity using a decaying co-occurrence model and syntactic dependence relations.</title>
<date>2002</date>
<booktitle>In SIGIR ’02 Proceedings,</booktitle>
<pages>183--190</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4828" citStr="Gao et al., 2002" startWordPosition="789" endWordPosition="792">related to its meaning (Ide and Véronis, 1998). We see that because of sparsity issues, there is a compromise between taking few, highly related words, or taking several, lower quality words. In most current systems, all words in a window are given equal weight, but we can easily understand that the occurrences of words should generally count less as they become farther; they form a long tail that we should use. Previous work proposed using non-linear functions of the distance to model the relation between two words. For instance, improvements can be obtained by using an exponential function (Gao et al., 2002). Yet, there is no evidence that the exponential – with its manually selected parameter – is the best function. 3 Computing weights for distances In this section, we present our method for choosing how much a word should count according to its distance to the target word. First, for some definitions, let C be a corpus, W a set of text windows, cW,i,x the count of occurrences of word x at distance i in W, cW,i the sum of these counts, and αi the weight put on one word at distance i. Then, ∑i αicW,i,x PML,W (x) = (1) ∑i αicW,i is the maximum likelihood estimator for x. To counter the zero-probab</context>
</contexts>
<marker>Gao, Zhou, Nie, He, Chen, 2002</marker>
<rawString>Jianfeng Gao, Ming Zhou, Jian-Yun Nie, Hongzhao He, and Weijun Chen. 2002. Resolving query translation ambiguity using a decaying co-occurrence model and syntactic dependence relations. In SIGIR ’02 Proceedings, pages 183–190, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Jean Véronis</author>
</authors>
<title>Introduction to the special issue on word sense disambiguation: the state of the art.</title>
<date>1998</date>
<journal>Comput. Linguist.,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="960" citStr="Ide and Véronis, 1998" startWordPosition="141" endWordPosition="144">ized windows and uniform weights. The window size is often selected by trial and error to maximize task results. We propose a non-supervised method for selecting weights for each window distance, effectively removing the need to limit window sizes, by maximizing the mutual generation of two sets of samples of the same word. Experiments on Semeval Word Sense Disambiguation tasks showed considerable improvements. 1 Introduction The meaning of a word can be defined by the words that accompany it in the text. This is the principle often used in previous studies on Word Sense Disambiguation (WSD) (Ide and Véronis, 1998; Navigli, 2009). In general, the accompanying words form a context vector of the target word, or a probability distribution of the context words. For example, under the unigram bag-ofword assumption, this means building p(x1t) _ ∑�� count�x�,t), where count(x, t) is the count of count�x,t) co-occurrences of word x with the target word t under a certain criterion. In most studies, x and t should co-occur within a window of up to k words or sentences. The bounds are usually selected as to maximize system performance. Occurrences inside the window usually weight the same without regard to their </context>
<context position="4257" citStr="Ide and Véronis, 1998" startWordPosition="690" endWordPosition="693">e target word. In this case, they found that the best results can be achieved using a window size of 3. 375 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 375–378, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Both these systems limit the size of their windows for different purposes. The former aims to model the topic of the documents containing the word rather than the word’s meaning. The latter limits the size because bag-of-word features further from the target word would not be sufficiently related to its meaning (Ide and Véronis, 1998). We see that because of sparsity issues, there is a compromise between taking few, highly related words, or taking several, lower quality words. In most current systems, all words in a window are given equal weight, but we can easily understand that the occurrences of words should generally count less as they become farther; they form a long tail that we should use. Previous work proposed using non-linear functions of the distance to model the relation between two words. For instance, improvements can be obtained by using an exponential function (Gao et al., 2002). Yet, there is no evidence t</context>
</contexts>
<marker>Ide, Véronis, 1998</marker>
<rawString>Nancy Ide and Jean Véronis. 1998. Introduction to the special issue on word sense disambiguation: the state of the art. Comput. Linguist., 24(1):2–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Comput. Surv.,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="976" citStr="Navigli, 2009" startWordPosition="145" endWordPosition="146">m weights. The window size is often selected by trial and error to maximize task results. We propose a non-supervised method for selecting weights for each window distance, effectively removing the need to limit window sizes, by maximizing the mutual generation of two sets of samples of the same word. Experiments on Semeval Word Sense Disambiguation tasks showed considerable improvements. 1 Introduction The meaning of a word can be defined by the words that accompany it in the text. This is the principle often used in previous studies on Word Sense Disambiguation (WSD) (Ide and Véronis, 1998; Navigli, 2009). In general, the accompanying words form a context vector of the target word, or a probability distribution of the context words. For example, under the unigram bag-ofword assumption, this means building p(x1t) _ ∑�� count�x�,t), where count(x, t) is the count of count�x,t) co-occurrences of word x with the target word t under a certain criterion. In most studies, x and t should co-occur within a window of up to k words or sentences. The bounds are usually selected as to maximize system performance. Occurrences inside the window usually weight the same without regard to their position. This i</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Comput. Surv., 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Okumura</author>
<author>Kiyoaki Shirai</author>
<author>Kanako Komiya</author>
<author>Hikaru Yokono</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task: Japanese wsd. In SemEval ’10 Proceedings. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10425" citStr="Okumura et al., 2010" startWordPosition="1779" endWordPosition="1782">g of 2.9. The improvements seen using our system are substantial, beating most of the systems originally proposed for the task (Pradhan et al., 2007). Out of 15 systems, the best results had accuracies of 89.1*, 89.1*, 88.7, 86.9 and 86.4 (* indicates postcompetition submissions). Notice that most were using Support Vector Machine (SVM) with bagof-word features in a very small window, local collocations and POS tags. In our future work, we will investigate the applications of SVM with our new term weighting scheme. 5 Experiments on Semeval-2010 Japanese WSD The Semeval-2010 Japanese WSD task (Okumura et al., 2010) consists of 50 polysemous words for which examples were taken from the BCCWJ tagged corpus. It was manually segmented, tagged, and annotated with senses taken from the Iwanami Kokugo dictionary. The task is identical to the ELS of the previous experiment. Since the data was again insufficient to compute curves, we used the Mainichi-2005 corpus of NTCIR-8. We tried to reproduce the same kind of segmentation as the training data by using the Chasen parser with UniDic. For the corpus and Semeval data, conjugations (setsuzoku-to, jodô- shi, etc.), particles (all jo-shi), symbols (blanks, kigô, et</context>
</contexts>
<marker>Okumura, Shirai, Komiya, Yokono, 2010</marker>
<rawString>Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, and Hikaru Yokono. 2010. Semeval-2010 task: Japanese wsd. In SemEval ’10 Proceedings. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2007 task 17: English lexical sample, srl and all words.</title>
<date>2007</date>
<booktitle>In SemEval ’07 Proceedings,</booktitle>
<pages>87--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7627" citStr="Pradhan et al., 2007" startWordPosition="1292" endWordPosition="1295">otice that as the algorithm progress, weights on close distances increase and the farthest decrease. As further distances contribute less and less, middle distances start to decay more and more, until at some point, all distances but the closest start to decrease, heading towards a degenerate solution. We therefore suggest using the observation of several consecutive decreases of all except α1 as an end criterion. We used 10 consecutive steps for our experiments. 4 Experiments on Semeval-2007 English Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al., 2007). It consists of a selected set of polysemous words, contained within passages where a sense taken from a sense inventory is manually annotated. The task is to create supervised classifiers maximizing accuracy on test data. Since there are only 50 words and instances are few, we judged there was not enough data to compute weights. Instead, we used the AP Newswire corpus of the TREC collection (CD 1 &amp; 2). Words αicW,i,x−αi+µP(xJC) 376 were stemmed with the Porter stemmer and text windows were grouped for all words. For simplicity and efficiency, windows to the right and to the left were conside</context>
<context position="9953" citStr="Pradhan et al., 2007" startWordPosition="1702" endWordPosition="1705">of the training data. The results are listed in Table 1. System Cross-Val (%) Test set (%) Prior only 78.66 77.76 Best uniform 85.48 83.28 RALI-2 88.23 86.45 Table 1: WSD accuracy on Semeval-2007 ELC We used two baselines: most frequent sense (prior only), and the best uniform (except target word) fixed size window found from extensive search on the training data. The best settings were a window of size 4, with a weight of 4.4 on the target word and a Laplace smoothing of 2.9. The improvements seen using our system are substantial, beating most of the systems originally proposed for the task (Pradhan et al., 2007). Out of 15 systems, the best results had accuracies of 89.1*, 89.1*, 88.7, 86.9 and 86.4 (* indicates postcompetition submissions). Notice that most were using Support Vector Machine (SVM) with bagof-word features in a very small window, local collocations and POS tags. In our future work, we will investigate the applications of SVM with our new term weighting scheme. 5 Experiments on Semeval-2010 Japanese WSD The Semeval-2010 Japanese WSD task (Okumura et al., 2010) consists of 50 polysemous words for which examples were taken from the BCCWJ tagged corpus. It was manually segmented, tagged, </context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer S. Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. Semeval-2007 task 17: English lexical sample, srl and all words. In SemEval ’07 Proceedings, pages 87–92, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>W Bruce Croft</author>
</authors>
<title>Corpusbased stemming using cooccurrence of word variants.</title>
<date>1998</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="3268" citStr="Xu and Croft, 1998" startWordPosition="525" endWordPosition="528"> can automatically be attained on simple Naive Bayes (NB) systems in comparison to the best manually selected fixed window system. The remainder of this paper is organized as follows: example uses of text windows and related work are presented in Section 2. Our method is presented in Section 3. In Section 4 and 5, we show experimental results on English and Japanese WSD. We conclude in Section 6 with discussion and further possible extensions. 2 Uses of text windows Modeling the distribution of words around one target word has many uses. For instance, the Xu&amp;Croft co-occurrence-based stemmer (Xu and Croft, 1998) uses window co-occurrence statistics to calculate the best equivalence classes for a group of word forms. They suggest using windows of up to 100 words. Another example can be found in WSD systems, where a shorter window is preferred. In Semeval-2007, top performing systems on WSD tasks, such as NUS-ML (Cai et al., 2007), made use of bag-of-word features around the target word. In this case, they found that the best results can be achieved using a window size of 3. 375 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 375–378, Uppsala, Sweden, 15-16 July 20</context>
</contexts>
<marker>Xu, Croft, 1998</marker>
<rawString>Jinxi Xu and W. Bruce Croft. 1998. Corpusbased stemming using cooccurrence of word variants. ACM Trans. Inf. Syst., 16(1):61–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>Two-stage language models for information retrieval.</title>
<date>2002</date>
<booktitle>In SIGIR ’02 Proceedings,</booktitle>
<pages>49--56</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5718" citStr="Zhai and Lafferty, 2002" startWordPosition="954" endWordPosition="957"> word. First, for some definitions, let C be a corpus, W a set of text windows, cW,i,x the count of occurrences of word x at distance i in W, cW,i the sum of these counts, and αi the weight put on one word at distance i. Then, ∑i αicW,i,x PML,W (x) = (1) ∑i αicW,i is the maximum likelihood estimator for x. To counter the zero-probability problem, we apply Dirichlet smoothing with the collection language model as a prior: ∑i αicW,i,x + µW P(x|C) PDir,W (x) = ∑ (2) i αicW,i + µW The pseudo-count µW is found by using Newton’s method via leave-one-out estimation. We follow the procedure shown in (Zhai and Lafferty, 2002), but since occurrences have different weights, the log-likelihood is changed to G−1(µ|W,C) = (3) &amp; ∑xEV αicW,i,x log ai+ ∑. a c � � W,j− a µ To find the best weights for our model we propose the following: • Let T be the set of all windows containing the target word. We randomly split this set into two sets A and B. • We want to find α? that maximizes the mutual generation of the two sets, by minimizing their cross-entropy: l(α) = H(PML,A, PDir,B) + H(PML,B, PDir,A) (4) In other words, we want αi to represent how much an occurrence at distance i models the context better than the collection l</context>
</contexts>
<marker>Zhai, Lafferty, 2002</marker>
<rawString>ChengXiang Zhai and John Lafferty. 2002. Two-stage language models for information retrieval. In SIGIR ’02 Proceedings, pages 49–56, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>