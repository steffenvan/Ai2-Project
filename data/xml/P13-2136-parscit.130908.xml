<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000141">
<title confidence="0.995889">
An Information Theoretic Approach to Bilingual Word Clustering
</title>
<author confidence="0.969718">
Manaal Faruqui and Chris Dyer
</author>
<affiliation confidence="0.975891">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.571584">
Pittsburgh, PA, 15213, USA
</address>
<email confidence="0.998243">
{mfaruqui, cdyer}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993845" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993905882353">
We present an information theoretic objec-
tive for bilingual word clustering that in-
corporates both monolingual distributional
evidence as well as cross-lingual evidence
from parallel corpora to learn high qual-
ity word clusters jointly in any number of
languages. The monolingual component
of our objective is the average mutual in-
formation of clusters of adjacent words in
each language, while the bilingual com-
ponent is the average mutual information
of the aligned clusters. To evaluate our
method, we use the word clusters in an
NER system and demonstrate a statisti-
cally significant improvement in F1 score
when using bilingual word clusters instead
of monolingual clusters.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999109375">
A word cluster is a group of words which ideally
captures syntactic, semantic, and distributional
regularities among the words belonging to the
group. Word clustering is widely used to reduce
the number of parameters in statistical models
which leads to improved generalization (Brown et
al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo
et al., 2008; Turian et al., 2010), and multilingual
clustering has been proposed as a means to im-
prove modeling of translational correspondences
and to facilitate projection of linguistic resource
across languages (Och, 1999; T¨ackstr¨om et al.,
2012). In this paper, we argue that generally more
informative clusters can be learned when evidence
from multiple languages is considered while cre-
ating the clusters.
We propose a novel bilingual word clustering
objective (§2). The first term deals with each
language independently and ensures that the data
is well-explained by the clustering in a sequence
model (§2.1). The second term ensures that the
cluster alignments induced by a word alignment
have high mutual information across languages
(§2.2). Since the objective consists of terms rep-
resenting the entropy monolingual data (for each
language) and parallel bilingual data, it is partic-
ularly attractive for the usual situation in which
there is much more monolingual data available
than parallel data. Because of its similarity to the
variation of information metric (Meilˇa, 2003), we
call this bilingual term in the objective the aligned
variation of information.
</bodyText>
<sectionHeader confidence="0.974686" genericHeader="introduction">
2 Word Clustering
</sectionHeader>
<bodyText confidence="0.98423225">
A word clustering C is a partition of a vocabulary
Σ = {x1, x2,... , x|Σ|} into K disjoint subsets,
C1,C2,...,CK. That is, C = {C1,C2,...,CK};
Ci ∩ Cj = ∅ for all i =�j and UKk=1 Ck = Σ.
</bodyText>
<subsectionHeader confidence="0.989941">
2.1 Monolingual objective
</subsectionHeader>
<bodyText confidence="0.9990494">
We use the average surprisal in a probabilistic se-
quence model to define the monolingual clustering
objective. Let ci denote the word class of word
wi. Our objective assumes that the probability of
a word sequence w = (w1, w2, ... , wM) is
</bodyText>
<equation confidence="0.995026333333333">
M
p(w) = p(ci  |ci−1) x p(wi  |ci), (2.1)
i=1
</equation>
<bodyText confidence="0.9999014">
where c0 is a special start symbol. The term p(ci |
ci−1) is the probability of class ci following class
ci−1, and p(wi  |ci) is the probability of class ci
emitting word wi. Using the MLE esitmates after
taking the negative logarithm, this term reduces to
</bodyText>
<page confidence="0.954179">
777
</page>
<bodyText confidence="0.518243333333333">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 777–783,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
the following as shown in (Brown et al., 1992):
</bodyText>
<equation confidence="0.9986718">
#(Ck) #(Ck)
M log M
#(Ci, Cj)
log
M
</equation>
<bodyText confidence="0.997934">
where #(Ck) is the count of Ck in the corpus w
under the clustering C, #(Ci, Cj) is the count of
the number of times that cluster Ci precedes Cj
and M is the size of the corpus. Using the mono-
lingual objective to cluster, we solve the following
search problem:
</bodyText>
<equation confidence="0.762673">
Cˆ = arg min H(C; w). (2.2)
C
</equation>
<subsectionHeader confidence="0.999018">
2.2 Bilingual objective
</subsectionHeader>
<bodyText confidence="0.999568857142857">
Now let us suppose we have a second lan-
guage with vocabulary Q = {y1, y2, ... , yJΩJ},
which is clustered into K disjoint subsets D =
{D1, D2, ... , DK}, and a corpus of text in the
second language, v = (v1, v2, ... , vN). Obvi-
ously we can cluster both languages using the
monolingual objective above:
</bodyText>
<equation confidence="0.958551">
H(C; w) + H(D; v).
</equation>
<bodyText confidence="0.9859584">
This joint minimization for the clusterings for both
languages clearly has no benefit since the two
terms of the objective are independent. We must
alter the object by further assuming that we have
a priori beliefs that some of the words in w and v
have the same meaning.
To encode this belief, we introduce the notion
of a weighted vocabulary alignment A, which is
a function on pairs of words in vocabularies E and
Q to a value greater than or equal to 0, i.e., A :
E x Q �-+ R&gt;0. For concreteness, A(x, y) will be
the number of times that x is aligned to y in a word
aligned parallel corpus. By abuse of notation, we
write marginal weights A(x) = PyEΩ A(x, y)
and A(y) = PxEΣ A(x, y). We also define the
</bodyText>
<equation confidence="0.3922">
set marginals A(C, D) = P PyED A(x, y).
xEC
</equation>
<bodyText confidence="0.977164909090909">
Using this weighted vocabulary alignment, we
state an objective that encourages clusterings to
have high average mutual information when align-
ment links are followed; that is, on average how
much information does knowing the cluster of a
word x E E impart about the clustering of y E Q,
and vice-versa?
Figure 1: Factor graphs of the monolingual (left)
&amp; proposed bilingual clustering problem (right).
We call this quantity the aligned variation of
information (AVI).
</bodyText>
<equation confidence="0.9198348">
AVI(C, D; A) =
EA(x,y) [−log p(cx  |dy) − logp(dy  |cx)]
Writing out the expectation and gathering terms,
we obtain
X
AVI(C, D; A) = −
xEΣ
�2 log A(C, D) − log p(C) − log p(D)I
,
A(·, ·)
</equation>
<bodyText confidence="0.924847625">
where it is assumed that 0 log x = 0.
Our bilingual clustering objective can therefore
be stated as the following search problem over a
linear combination of the monolingual and bilin-
gual objectives:
arg min monolingual βxbilingual
C,D z } |{ z }|{
H(C; w) + H(D; v) + OAVI(C, D) .
(2.3)
Understanding AVI. Intuitively, we can imag-
ine sampling a random alignment from the distri-
bution obtained by normalizing A(·, ·). AVI gives
us a measure of how much information do we ob-
tain, on average, from knowing the cluster in one
language about the clustering of a linked element
chosen at random proportional to A(x, ·) (or con-
ditioned the other way around). In the following
sections, we denote AVI(C, D; A) by AVI(C, D).
To further understand AVI, we remark that AVI re-
duces to the VI metric when the alignment maps
words to themselves in the same language. As a
proper metric, VI has a number of attractive prop-
erties, and these can be generalized to AVI (with-
out restriction on the alignment map), namely:
</bodyText>
<listItem confidence="0.999988">
• Non-negativity: AVI(C, D) &gt; 0;
• Symmetry: AVI(C, D) = AVI(D, C);
• Triangle inequality:
</listItem>
<equation confidence="0.992824466666667">
AVI(C, D) + AVI(D, E) &gt; AVI(C, E);
C C D
H(C; w) = 2 XK
k=1
X X
−
i jai
#(Ci, Cj)
M
ˆC, Dˆ = arg min
C,D
A(x, y)
A(·,·) x
X
yEΩ
</equation>
<page confidence="0.975011">
778
</page>
<listItem confidence="0.957857">
• Identity of indiscernables:
AVI(C, D) = 0 iff C ≡ D.1
</listItem>
<subsectionHeader confidence="0.987247">
2.3 Example
</subsectionHeader>
<bodyText confidence="0.999981184210526">
Figure 2 provides an example illustrating the dif-
ference between the bilingual vs. monolingual
clustering objectives. We compare two different
clusterings of a two-sentence Arabic-English par-
allel corpus (the English half of the corpus con-
tains the same sentence, twice, while the Ara-
bic half has two variants with the same mean-
ing). While English has a relatively rigid SVO
word order, Arabic can alternate between the tradi-
tional VSO order and an more modern SVO order.
Since our monolingual clustering objective relies
exclusively on the distribution of clusters before
and after each token, flexible word order alterna-
tions like this can cause unintuitive results. To
further complicate matters, verbs can inflect dif-
ferently depending on whether their subject pre-
cedes or follows them (Haywood and Nahmad,
1999), so a monolingual model, which knows
nothing about morphology and may only rely
on distributional clues, has little chance of per-
forming well without help. This is indeed what
we observe in the monolingual objective opti-
mal solution (center), in which AwlAd (boys) and
yElbwn (play+PRES + 3PL) are grouped into a
single class, while yElb (play+PRES +3SG) is in
its own class. However, the AVI term (which is of
course not included) has a value of 1.0, reflecting
the relatively disordered clustering relative to the
given alignment. On the right, we see the optimal
solution that includes the AVI term in the cluster-
ing objective. This has an AVI of 0, indicating that
knowing the clustering of any word is completely
informative about the words it is aligned to. By in-
cluding this term, a slightly worse monolingual so-
lution is chosen, but the clustering corresponds to
the reasonable intuition that words with the same
meaning (i.e., the two variants of to play) should
be clustered together.
</bodyText>
<subsectionHeader confidence="0.785608">
2.4 Inference
</subsectionHeader>
<bodyText confidence="0.999820714285714">
Figure 1 shows the factor graph representation
of our clustering models. Finding the optimal
clustering under both the monolingual and bilin-
gual objectives is a computationally hard combi-
natorial optimization problem (Och, 1995). We
use a greedy hill-climbing word exchange algo-
rithm (Martin et al., 1995) to find a minimum
</bodyText>
<footnote confidence="0.402763">
1C ≡ D iff ∀i|{D(y)|∀(x, y) E A, C(x) = i} |= 1
</footnote>
<bodyText confidence="0.999904111111111">
value for our objective. We terminate the opti-
mization procedure when the number of words
exchanged at the end of one complete iteration
through both the languages is less than 0.1% of
the sum of vocabulary of the two languages and
at least five complete iterations have been com-
pleted.2 For every language the word clusters are
initialised in a round robin order according to the
token frequency.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999639457142857">
Evaluation of clustering is not a trivial problem.
One branch of work seeks to recast the problem
as the of part-of-speech (POS) induction and at-
tempts to match linguistic intuitions. However,
hard clusters are particularly useful for down-
stream tasks (Turian et al., 2010). We therefore
chose to focus our evaluation on the latter prob-
lem. For our evaluation, we use our word clusters
as an input to a named entity recognizer which
uses these clusters as a source of features. Our
evaluation task is the German corpus with NER
annotation that was created for the shared task
at CoNLL-2003 3. The training set contains ap-
proximately 220,000 tokens and the development
set and test set contains 55,000 tokens each. We
use Stanford’s Named Entity Recognition system4
which uses a linear-chain conditional random field
to predict the most likely sequence of NE la-
bels (Finkel and Manning, 2009).
Corpora for Clustering: We used parallel cor-
pora for {Arabic, English, French, Korean &amp;
Turkish}-German pairs from WIT-3 corpus (Cet-
tolo et al., 2012) 5, which is a collection of trans-
lated transcriptions of TED talks. Each language
pair contained around 1.5 million German words.
The corpus was word aligned in two directions
using an unsupervised word aligner (Dyer et al.,
2013), then the intersected alignment points were
taken.
Monolingual Clustering: For every language
pair, we train German word clusters on the mono-
lingual German data from the parallel data. Note
that the parallel corpora are of different sizes and
hence the monolingual German data from every
parallel corpus is different. We treat the Fi score
</bodyText>
<footnote confidence="0.9998992">
2In practice, the number of exchanged words drops of exponentially,
so this threshold is typically reached in not many iterations.
3http://www.cnts.ua.ac.be/conll2003/ner/
4http://nlp.stanford.edu/ner/index.shtml
5https://wit3.fbk.eu/mt.php?release=2012-03
</footnote>
<page confidence="0.993292">
779
</page>
<figure confidence="0.999543258064516">
ylEb
ylEbwn
Al-
are
playing
The
,IyJY
I ,aL )
(
ylEb Al- AwlAd
Al-
The
boys
AwlAd
ylEbwn
H(C;w) + H(D;v)= 7.88
AVI(C,D)= 1.0
)
,IyJY I
(������
H(D;v) = 3.88
Al- AwlAd ylEbwn
ylEb
boys
AwlAd
are
playing
The boys are playing
H(C;w) = 4 H(C;w) = 4.56 H(D;v) = 4
H(C;w) + H(D;v)= 8.56
AVI(C,D)= 0
</figure>
<figureCaption confidence="0.908261333333333">
Figure 2: A two-sentence English-Arabic parallel corpus (left); a 3-class clustering that maximizes the
monolingual objective (6 = 0; center); and a 3-class clustering that maximizes the joint monolingual
and bilingual objective (any 6 &gt; 0.68; right).
</figureCaption>
<bodyText confidence="0.99538715625">
obtained using monolingual word clusters (6 = 0)
as the baseline. Table 1 shows the F1 score of
NER6 when trained on these monolingual German
word clusters.
Bilingual Clustering: While we have formu-
lated a joint objective that enables using both
monolingual and bilingual evidence, it is possible
to create word clusters using the bilingual signal
only by removing the first term in Eq. 2.3. Ta-
ble 1 shows the performance of NER when the
word clusters are obtained using only the bilingual
information for different language pairs. As can
be seen, these clusters are helpful for all the lan-
guage pairs. For Turkish the F1 score improves
by 1.0 point over when there are no distributional
clusters which clearly shows that the word align-
ment information improves the clustering quality.
We now need to supplement the bilingual infor-
mation with monolingual information to see if the
improvement sustains.
We varied the weight of the bilingual objec-
tive (6) from 0.05 to 0.9 and observed the ef-
fect in NER performance on English-German lan-
guage pair. The F1 score is maximum for 6 =
0.1 and decreases monotonically when 6 is ei-
ther increased or decreased. This indicates that
bilingual information is helpful, but less valuable
than monolingual information. Preliminary exper-
iments showed that the value of 6 = 0.1 is fairly
robust across other language pairs and hence we
fix it to that for all the experiments.
We run our bilingual clustering model (6 =
</bodyText>
<footnote confidence="0.605616">
6Faruqui and Pad´o (2010) show that for the size of our generalization
data in German-NER, K = 100 should give us the optimum value.
</footnote>
<bodyText confidence="0.984651085714286">
0.1) across all language pairs and note the F1
scores. Table 1 (unrefined) shows that except for
Arabic-German &amp; French-German, all other lan-
guage pairs deliver a better F1 score than only us-
ing monolingual German data. In case of Arabic-
German there is a drop in score by 0.25 points.
Although, we have observed improvement in F1
score over the monolingual case, the gains do
not reach significance according to McNemar’s
test (Dietterich, 1998).
Thus we propose to further refine the quality of
word alignment links as follows: Let x be a word
in language Σ and y be a word in language Q and
let there exists an alignment link between x and
y. Recall that A(x, y) is the count of the align-
ment links between x and y observed in the par-
allel data, and A(x) and A(y) are the respective
marginal counts. Then we define an edge associ-
ation weight e(x, y) = 2xA(x,y) This quantity
A(x)+A(y)
is an association of the strength of the relationship
between x and y, and we use it to remove all align-
ment links whose e(x, y) is below a given thresh-
old before running the bilingual clustering model.
We vary e from 0.1 to 0.7 and observe the new F1
scores on the development data. Table 1 (refined)
shows the results obtained by our refined model.
The values shown in bold are the highest improve-
ments over the monolingual model.
For English and Turkish we observe a statisti-
cally significant improvement over the monolin-
gual model (cf. Table 1) with p &lt; 0.007 and
p &lt; 0.001 according to McNemar’s test. Ara-
bic improves least with just an improvement of
0.02 F1 points over the monolingual baseline. We
</bodyText>
<page confidence="0.988639">
780
</page>
<table confidence="0.9998554">
Dev Test
Language Pair — 0 = 0 0 = 0.1 0 = 0.1 0 = 0 0 = 0.1
(only bi) (only mono) (unrefined) (refined) (only mono) (refined)
No clusters 68.27 72.32
En-De 68.95 70.04 70.33 70.64† 72.30 72.98†
Fr-De 69.16 69.74 69.69 69.89 72.66 72.83
Ar-De 69.01 69.65 69.40 69.67 72.90 72.37
Tr-De 69.29 69.46 69.64 70.05† 72.41 72.54
Ko-De 68.95 69.70 69.78 69.95 72.71 72.54
Average 69.07 69.71 69.76 70.04† 72.59 72.65
</table>
<tableCaption confidence="0.999698">
Table 1: NER performance using different word clustering models. Bold indicates an improvement over
</tableCaption>
<bodyText confidence="0.984887344827586">
the monolingual (β = 0) baseline; † indicates a significant improvement (McNemar’s test, p &lt; 0.01).
see that the optimal value of e changes from one
language pair to another. For French and English
e = 0.1 gives the best results whereas for Turk-
ish and Arabic e = 0.5 and for Korean e = 0.7.
Are these thresholds correlated with anything? We
suggest that higher values of e correspond to more
intrinsically noisy alignments. Since alignment
models are parameterized based on the vocabu-
laries of the languages they are aligning, larger
vocabularies are more prone to degenerate solu-
tions resulting from overfitting. So we are not
surprised to see that sparser alignments (resulting
from higher values of e) are required by languages
like Korean, while languages like French and En-
glish make due with denser alignments.
Evaluation on Test Set: We now verify our re-
sults on the test set. We take the best bilin-
gual word clustering model obtained for every lan-
guage pair (e = 0.1 for En, Fr. e = 0.5 for Ar,
Tr. e = 0.7 for Ko) and train NER classifiers
using these. Table 1 shows the performance of
German NER classifiers on the test set. All the
values shown in bold are better than the mono-
lingual baselines. English again has a statistically
significant improvement over the baseline. French
and Turkish show the next best improvements.
The English-German cluster model performs bet-
ter than the mkcls7 tool (72.83%).
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.99659025">
Our monolingual clustering model is purely distri-
butional in nature. Other extensions to word clus-
tering have incorporated morphological and or-
thographic information (Clark, 2003). The work
of Snyder and Barzilay (2010), which focused on
POS induction is very closely related. The ear-
liest work on bilingual word clustering was pro-
posed by (Och, 1999) which, like us, uses a lan-
</bodyText>
<footnote confidence="0.722308">
7http://www.statmt.org/moses/giza/mkcls.html
</footnote>
<bodyText confidence="0.999920409090909">
guage modeling approach (Brown et al., 1992;
Kneser and Ney, 1993) for monolingual optimiza-
tion and a similarity function for bilingual simi-
larity. T¨ackstr¨om et al. (2012) use cross-lingual
word clusters to show transfer of linguistic struc-
ture. While their clustering method is superficially
similar, the objective function is more heuristic in
nature than our information-theoretic conception
of the problem. Multilingual learning has been
applied to a number of unsupervised and super-
vised learning problems, including word sense dis-
ambiguation (Diab, 2003; Guo and Diab, 2010),
topic modeling (Mimno et al., 2009; Boyd-Graber
and Blei, 2009), and morphological segmenta-
tion (Snyder and Barzilay, 2008).
Also closely related is the technique of cross-
lingual annotation projection. This has been
applied to bootstrapping syntactic parsers (Hwa
et al., 2005; Smith and Smith, 2007; Co-
hen et al., 2011), morphology (Fraser, 2009),
tense (Schiehlen, 1998) and T/V pronoun us-
age (Faruqui and Pad´o, 2012).
</bodyText>
<sectionHeader confidence="0.996491" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999691">
We presented a novel information theoretic model
for bilingual word clustering which seeks a clus-
tering with high average mutual information be-
tween clusters of adjacent words, and also high
mutual information across observed word align-
ment links. We have shown that improvement in
clustering can be obtained across a range of lan-
guage pairs, evaluated in terms of their value as
features in an extrinsic NER task. Our model can
be extended for clustering any number of given
languages together in a joint framework, and in-
corporate both monolingual and parallel data.
Acknowledgement: We woud like to thank W.
Ammar, V. Chahuneau and W. Ling for valuable
discussions.
</bodyText>
<page confidence="0.99637">
781
</page>
<sectionHeader confidence="0.986225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999019867924529">
J. Boyd-Graber and D. M. Blei. 2009. Multilingual
topic models for unaligned text. In Proceedings of
the Twenty-Fifth Conference on Uncertainty in Arti-
ficialIntelligence, UAI ’09, pages 75–82, Arlington,
Virginia, United States. AUAI Press.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,
and J. C. Lai. 1992. Class-based n-gram models
of natural language. Comput. Linguist., 18(4):467–
479, December.
M. Cettolo, C. Girardi, and M. Federico. 2012. Wit3:
Web inventory of transcribed and translated talks. In
Proceedings of the 16th Conference of the European
Association for Machine Translation (EAMT), pages
261–268, Trento, Italy, May.
A. Clark. 2003. Combining distributional and mor-
phological information for part of speech induction.
In Proceedings of the tenth conference on Euro-
pean chapter of the Association for Computational
Linguistics - Volume 1, EACL ’03, pages 59–66,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsuper-
vised structure prediction with non-parallel multilin-
gual guidance. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 50–61, Stroudsburg, PA,
USA. Association for Computational Linguistics.
M. T. Diab. 2003. Word sense disambiguation within a
multilingual framework. Ph.D. thesis, University of
Maryland at College Park, College Park, MD, USA.
AAI3115805.
T. G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning al-
gorithms. Neural Computation, 10:1895–1923.
C. Dyer, V. Chahuneau, and N. A. Smith. 2013.
A simple, fast, and effective reparameterization of
IBM Model 2. In Proc. NAACL.
M. Faruqui and S. Pad´o. 2010. Training and Evalu-
ating a German Named Entity Recognizer with Se-
mantic Generalization. In Proceedings of KON-
VENS 2010, Saarbr¨ucken, Germany.
M. Faruqui and S. Pad´o. 2012. Towards a model of for-
mal and informal address in english. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics.
J. R. Finkel and C. D. Manning. 2009. Nested named
entity recognition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1 - Volume 1, EMNLP ’09,
pages 141–150, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Fraser. 2009. Experiments in morphosyntactic pro-
cessing for translating to and from German. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 115–119, Athens, Greece,
March. Association for Computational Linguistics.
W. Guo and M. Diab. 2010. Combining orthogonal
monolingual and multilingual sources of evidence
for all words wsd. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ’10, pages 1542–1551, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
J. A. Haywood and H. M. Nahmad. 1999. A new
Arabic grammar of the written language. Lund
Humphries Publishers.
R. Hwa, P. Resnik, A. Weinberg, C. I. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural Language
Engineering, pages 311–325.
R. Kneser and H. Ney. 1993. Forming word classes
by statistical clustering for statistical language mod-
elling. In R. Khler and B. Rieger, editors, Contri-
butions to Quantitative Linguistics, pages 221–226.
Springer Netherlands.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proc. of
ACL.
S. Martin, J. Liermann, and H. Ney. 1995. Algorithms
for bigram and trigram word clustering. In Speech
Communication, pages 1253–1256.
M. Meilˇa. 2003. Comparing Clusterings by the Varia-
tion of Information. In Learning Theory and Kernel
Machines, pages 173–187.
D. Mimno, H. M. Wallach, J. Naradowsky, D. A.
Smith, and A. McCallum. 2009. Polylingual topic
models. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 2 - Volume 2, EMNLP ’09, pages 880–
889, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
F. J. Och. 1995. Maximum-Likelihood-Sch¨atzung von
Wortkategorien mit Verfahren der kombinatorischen
Optimierung. Studienarbeit, University of Erlangen.
F. J. Och. 1999. An efficient method for determin-
ing bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, EACL ’99,
pages 71–76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
M. Schiehlen. 1998. Learning tense translation from
bilingual corpora.
D. A. Smith and N. A. Smith. 2007. Probabilis-
tic Models of Nonprojective Dependency Trees.
In Proceedings of the 2007 Joint Conference on
</reference>
<page confidence="0.969478">
782
</page>
<reference confidence="0.997527862068966">
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 132–140, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
B. Snyder and R. Barzilay. 2008. Unsupervised mul-
tilingual learning for morphological segmentation.
In In The Annual Conference of the Association for
Computational Linguistics.
B. Snyder and R. Barzilay. 2010. Climbing the tower
of babel: Unsupervised multilingual learning. In
J. Frnkranz and T. Joachims, editors, Proceedings
of the 27th International Conference on Machine
Learning (ICML-10), June 21-24, 2010, Haifa, Is-
rael, pages 29–36. Omnipress.
O. T¨ackstr¨om, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of
linguistic structure. In The 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, volume 1, page 11. Association for Com-
putational Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 384–394, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.998935">
783
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.638349">
<title confidence="0.820483">An Information Theoretic Approach to Bilingual Word Clustering Faruqui Language Technologies</title>
<affiliation confidence="0.968622">Carnegie Mellon</affiliation>
<address confidence="0.997663">Pittsburgh, PA, 15213,</address>
<abstract confidence="0.998902">We present an information theoretic objective for bilingual word clustering that incorporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages. The monolingual component of our objective is the average mutual information of clusters of adjacent words in each language, while the bilingual component is the average mutual information of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistisignificant improvement in when using bilingual word clusters instead of monolingual clusters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D M Blei</author>
</authors>
<title>Multilingual topic models for unaligned text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Fifth Conference on Uncertainty in ArtificialIntelligence, UAI ’09,</booktitle>
<pages>75--82</pages>
<publisher>AUAI Press.</publisher>
<location>Arlington, Virginia, United States.</location>
<contexts>
<context position="18027" citStr="Boyd-Graber and Blei, 2009" startWordPosition="3062" endWordPosition="3065">ach (Brown et al., 1992; Kneser and Ney, 1993) for monolingual optimization and a similarity function for bilingual similarity. T¨ackstr¨om et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmentation (Snyder and Barzilay, 2008). Also closely related is the technique of crosslingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun usage (Faruqui and Pad´o, 2012). 5 Conclusions We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information between clusters of adjacent words, and also high mutual information across observed w</context>
</contexts>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>J. Boyd-Graber and D. M. Blei. 2009. Multilingual topic models for unaligned text. In Proceedings of the Twenty-Fifth Conference on Uncertainty in ArtificialIntelligence, UAI ’09, pages 75–82, Arlington, Virginia, United States. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
<author>V J D Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Comput. Linguist.,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>479</pages>
<contexts>
<context position="1208" citStr="Brown et al., 1992" startWordPosition="175" endWordPosition="178">each language, while the bilingual component is the average mutual information of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically significant improvement in F1 score when using bilingual word clusters instead of monolingual clusters. 1 Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each language independently and ensures that the data is </context>
<context position="3433" citStr="Brown et al., 1992" startWordPosition="542" endWordPosition="545">jective assumes that the probability of a word sequence w = (w1, w2, ... , wM) is M p(w) = p(ci |ci−1) x p(wi |ci), (2.1) i=1 where c0 is a special start symbol. The term p(ci | ci−1) is the probability of class ci following class ci−1, and p(wi |ci) is the probability of class ci emitting word wi. Using the MLE esitmates after taking the negative logarithm, this term reduces to 777 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 777–783, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics the following as shown in (Brown et al., 1992): #(Ck) #(Ck) M log M #(Ci, Cj) log M where #(Ck) is the count of Ck in the corpus w under the clustering C, #(Ci, Cj) is the count of the number of times that cluster Ci precedes Cj and M is the size of the corpus. Using the monolingual objective to cluster, we solve the following search problem: Cˆ = arg min H(C; w). (2.2) C 2.2 Bilingual objective Now let us suppose we have a second language with vocabulary Q = {y1, y2, ... , yJΩJ}, which is clustered into K disjoint subsets D = {D1, D2, ... , DK}, and a corpus of text in the second language, v = (v1, v2, ... , vN). Obviously we can cluster</context>
<context position="17423" citStr="Brown et al., 1992" startWordPosition="2973" endWordPosition="2976">ine. French and Turkish show the next best improvements. The English-German cluster model performs better than the mkcls7 tool (72.83%). 4 Related Work Our monolingual clustering model is purely distributional in nature. Other extensions to word clustering have incorporated morphological and orthographic information (Clark, 2003). The work of Snyder and Barzilay (2010), which focused on POS induction is very closely related. The earliest work on bilingual word clustering was proposed by (Och, 1999) which, like us, uses a lan7http://www.statmt.org/moses/giza/mkcls.html guage modeling approach (Brown et al., 1992; Kneser and Ney, 1993) for monolingual optimization and a similarity function for bilingual similarity. T¨ackstr¨om et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai. 1992. Class-based n-gram models of natural language. Comput. Linguist., 18(4):467– 479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cettolo</author>
<author>C Girardi</author>
<author>M Federico</author>
</authors>
<title>Wit3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>261--268</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="10459" citStr="Cettolo et al., 2012" startWordPosition="1793" endWordPosition="1797">izer which uses these clusters as a source of features. Our evaluation task is the German corpus with NER annotation that was created for the shared task at CoNLL-2003 3. The training set contains approximately 220,000 tokens and the development set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean &amp; Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5, which is a collection of translated transcriptions of TED talks. Each language pair contained around 1.5 million German words. The corpus was word aligned in two directions using an unsupervised word aligner (Dyer et al., 2013), then the intersected alignment points were taken. Monolingual Clustering: For every language pair, we train German word clusters on the monolingual German data from the parallel data. Note that the parallel corpora are of different sizes and hence the monolingual German data from every parallel corpus is different. We treat the Fi score 2In practice, the number of </context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>M. Cettolo, C. Girardi, and M. Federico. 2012. Wit3: Web inventory of transcribed and translated talks. In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT), pages 261–268, Trento, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics - Volume 1, EACL ’03,</booktitle>
<pages>59--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1243" citStr="Clark, 2003" startWordPosition="183" endWordPosition="184">nt is the average mutual information of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically significant improvement in F1 score when using bilingual word clusters instead of monolingual clusters. 1 Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each language independently and ensures that the data is well-explained by the clustering in</context>
<context position="17136" citStr="Clark, 2003" startWordPosition="2931" endWordPosition="2932"> Ar, Tr. e = 0.7 for Ko) and train NER classifiers using these. Table 1 shows the performance of German NER classifiers on the test set. All the values shown in bold are better than the monolingual baselines. English again has a statistically significant improvement over the baseline. French and Turkish show the next best improvements. The English-German cluster model performs better than the mkcls7 tool (72.83%). 4 Related Work Our monolingual clustering model is purely distributional in nature. Other extensions to word clustering have incorporated morphological and orthographic information (Clark, 2003). The work of Snyder and Barzilay (2010), which focused on POS induction is very closely related. The earliest work on bilingual word clustering was proposed by (Och, 1999) which, like us, uses a lan7http://www.statmt.org/moses/giza/mkcls.html guage modeling approach (Brown et al., 1992; Kneser and Ney, 1993) for monolingual optimization and a similarity function for bilingual similarity. T¨ackstr¨om et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature t</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>A. Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics - Volume 1, EACL ’03, pages 59–66, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Unsupervised structure prediction with non-parallel multilingual guidance.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>50--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18284" citStr="Cohen et al., 2011" startWordPosition="3101" endWordPosition="3105">rficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmentation (Snyder and Barzilay, 2008). Also closely related is the technique of crosslingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun usage (Faruqui and Pad´o, 2012). 5 Conclusions We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information between clusters of adjacent words, and also high mutual information across observed word alignment links. We have shown that improvement in clustering can be obtained across a range of language pairs, evaluated in terms of their value as features in an extrinsic NER task. Our model can be extended for clustering any number of given language</context>
</contexts>
<marker>Cohen, Das, Smith, 2011</marker>
<rawString>S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 50–61, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Diab</author>
</authors>
<title>Word sense disambiguation within a multilingual framework.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland at College Park,</institution>
<location>College Park, MD, USA. AAI3115805.</location>
<contexts>
<context position="17941" citStr="Diab, 2003" startWordPosition="3050" endWordPosition="3051"> lan7http://www.statmt.org/moses/giza/mkcls.html guage modeling approach (Brown et al., 1992; Kneser and Ney, 1993) for monolingual optimization and a similarity function for bilingual similarity. T¨ackstr¨om et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmentation (Snyder and Barzilay, 2008). Also closely related is the technique of crosslingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun usage (Faruqui and Pad´o, 2012). 5 Conclusions We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information </context>
</contexts>
<marker>Diab, 2003</marker>
<rawString>M. T. Diab. 2003. Word sense disambiguation within a multilingual framework. Ph.D. thesis, University of Maryland at College Park, College Park, MD, USA. AAI3115805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<pages>10--1895</pages>
<contexts>
<context position="13864" citStr="Dietterich, 1998" startWordPosition="2353" endWordPosition="2354">un our bilingual clustering model (6 = 6Faruqui and Pad´o (2010) show that for the size of our generalization data in German-NER, K = 100 should give us the optimum value. 0.1) across all language pairs and note the F1 scores. Table 1 (unrefined) shows that except for Arabic-German &amp; French-German, all other language pairs deliver a better F1 score than only using monolingual German data. In case of ArabicGerman there is a drop in score by 0.25 points. Although, we have observed improvement in F1 score over the monolingual case, the gains do not reach significance according to McNemar’s test (Dietterich, 1998). Thus we propose to further refine the quality of word alignment links as follows: Let x be a word in language Σ and y be a word in language Q and let there exists an alignment link between x and y. Recall that A(x, y) is the count of the alignment links between x and y observed in the parallel data, and A(x) and A(y) are the respective marginal counts. Then we define an edge association weight e(x, y) = 2xA(x,y) This quantity A(x)+A(y) is an association of the strength of the relationship between x and y, and we use it to remove all alignment links whose e(x, y) is below a given threshold be</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>T. G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10:1895–1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>V Chahuneau</author>
<author>N A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of IBM Model 2. In</title>
<date>2013</date>
<booktitle>Proc. NAACL.</booktitle>
<contexts>
<context position="10690" citStr="Dyer et al., 2013" startWordPosition="1832" endWordPosition="1835">development set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean &amp; Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5, which is a collection of translated transcriptions of TED talks. Each language pair contained around 1.5 million German words. The corpus was word aligned in two directions using an unsupervised word aligner (Dyer et al., 2013), then the intersected alignment points were taken. Monolingual Clustering: For every language pair, we train German word clusters on the monolingual German data from the parallel data. Note that the parallel corpora are of different sizes and hence the monolingual German data from every parallel corpus is different. We treat the Fi score 2In practice, the number of exchanged words drops of exponentially, so this threshold is typically reached in not many iterations. 3http://www.cnts.ua.ac.be/conll2003/ner/ 4http://nlp.stanford.edu/ner/index.shtml 5https://wit3.fbk.eu/mt.php?release=2012-03 77</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>C. Dyer, V. Chahuneau, and N. A. Smith. 2013. A simple, fast, and effective reparameterization of IBM Model 2. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Faruqui</author>
<author>S Pad´o</author>
</authors>
<title>Training and Evaluating a German Named Entity Recognizer with Semantic Generalization.</title>
<date>2010</date>
<booktitle>In Proceedings of KONVENS</booktitle>
<location>Saarbr¨ucken, Germany.</location>
<marker>Faruqui, Pad´o, 2010</marker>
<rawString>M. Faruqui and S. Pad´o. 2010. Training and Evaluating a German Named Entity Recognizer with Semantic Generalization. In Proceedings of KONVENS 2010, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Faruqui</author>
<author>S Pad´o</author>
</authors>
<title>Towards a model of formal and informal address in english.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<marker>Faruqui, Pad´o, 2012</marker>
<rawString>M. Faruqui and S. Pad´o. 2012. Towards a model of formal and informal address in english. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Nested named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09,</booktitle>
<pages>141--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10307" citStr="Finkel and Manning, 2009" startWordPosition="1770" endWordPosition="1773">, 2010). We therefore chose to focus our evaluation on the latter problem. For our evaluation, we use our word clusters as an input to a named entity recognizer which uses these clusters as a source of features. Our evaluation task is the German corpus with NER annotation that was created for the shared task at CoNLL-2003 3. The training set contains approximately 220,000 tokens and the development set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean &amp; Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5, which is a collection of translated transcriptions of TED talks. Each language pair contained around 1.5 million German words. The corpus was word aligned in two directions using an unsupervised word aligner (Dyer et al., 2013), then the intersected alignment points were taken. Monolingual Clustering: For every language pair, we train German word clusters on the monolingual German data from the parallel data. Note that the parallel corpora </context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>J. R. Finkel and C. D. Manning. 2009. Nested named entity recognition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 141–150, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
</authors>
<title>Experiments in morphosyntactic processing for translating to and from German.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>115--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="18311" citStr="Fraser, 2009" startWordPosition="3107" endWordPosition="3108">function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmentation (Snyder and Barzilay, 2008). Also closely related is the technique of crosslingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun usage (Faruqui and Pad´o, 2012). 5 Conclusions We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information between clusters of adjacent words, and also high mutual information across observed word alignment links. We have shown that improvement in clustering can be obtained across a range of language pairs, evaluated in terms of their value as features in an extrinsic NER task. Our model can be extended for clustering any number of given languages together in a joint frame</context>
</contexts>
<marker>Fraser, 2009</marker>
<rawString>A. Fraser. 2009. Experiments in morphosyntactic processing for translating to and from German. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 115–119, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Guo</author>
<author>M Diab</author>
</authors>
<title>Combining orthogonal monolingual and multilingual sources of evidence for all words wsd.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1542--1551</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17962" citStr="Guo and Diab, 2010" startWordPosition="3052" endWordPosition="3055">www.statmt.org/moses/giza/mkcls.html guage modeling approach (Brown et al., 1992; Kneser and Ney, 1993) for monolingual optimization and a similarity function for bilingual similarity. T¨ackstr¨om et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmentation (Snyder and Barzilay, 2008). Also closely related is the technique of crosslingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun usage (Faruqui and Pad´o, 2012). 5 Conclusions We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information between clusters of a</context>
</contexts>
<marker>Guo, Diab, 2010</marker>
<rawString>W. Guo and M. Diab. 2010. Combining orthogonal monolingual and multilingual sources of evidence for all words wsd. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1542–1551, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Haywood</author>
<author>H M Nahmad</author>
</authors>
<title>A new Arabic grammar of the written language.</title>
<date>1999</date>
<publisher>Lund Humphries Publishers.</publisher>
<contexts>
<context position="7622" citStr="Haywood and Nahmad, 1999" startWordPosition="1319" endWordPosition="1322">allel corpus (the English half of the corpus contains the same sentence, twice, while the Arabic half has two variants with the same meaning). While English has a relatively rigid SVO word order, Arabic can alternate between the traditional VSO order and an more modern SVO order. Since our monolingual clustering objective relies exclusively on the distribution of clusters before and after each token, flexible word order alternations like this can cause unintuitive results. To further complicate matters, verbs can inflect differently depending on whether their subject precedes or follows them (Haywood and Nahmad, 1999), so a monolingual model, which knows nothing about morphology and may only rely on distributional clues, has little chance of performing well without help. This is indeed what we observe in the monolingual objective optimal solution (center), in which AwlAd (boys) and yElbwn (play+PRES + 3PL) are grouped into a single class, while yElb (play+PRES +3SG) is in its own class. However, the AVI term (which is of course not included) has a value of 1.0, reflecting the relatively disordered clustering relative to the given alignment. On the right, we see the optimal solution that includes the AVI te</context>
</contexts>
<marker>Haywood, Nahmad, 1999</marker>
<rawString>J. A. Haywood and H. M. Nahmad. 1999. A new Arabic grammar of the written language. Lund Humphries Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
<author>P Resnik</author>
<author>A Weinberg</author>
<author>C I Cabezas</author>
<author>O Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering,</title>
<date>2005</date>
<pages>311--325</pages>
<contexts>
<context position="18240" citStr="Hwa et al., 2005" startWordPosition="3093" endWordPosition="3096">re. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmentation (Snyder and Barzilay, 2008). Also closely related is the technique of crosslingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun usage (Faruqui and Pad´o, 2012). 5 Conclusions We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information between clusters of adjacent words, and also high mutual information across observed word alignment links. We have shown that improvement in clustering can be obtained across a range of language pairs, evaluated in terms of their value as features in an extrinsic NER task. Our model can be extended</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>R. Hwa, P. Resnik, A. Weinberg, C. I. Cabezas, and O. Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering, pages 311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Forming word classes by statistical clustering for statistical language modelling.</title>
<date>1993</date>
<booktitle>Contributions to Quantitative Linguistics,</booktitle>
<pages>221--226</pages>
<editor>In R. Khler and B. Rieger, editors,</editor>
<publisher>Springer Netherlands.</publisher>
<contexts>
<context position="1230" citStr="Kneser and Ney, 1993" startWordPosition="179" endWordPosition="182"> the bilingual component is the average mutual information of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically significant improvement in F1 score when using bilingual word clusters instead of monolingual clusters. 1 Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each language independently and ensures that the data is well-explained by the </context>
<context position="17446" citStr="Kneser and Ney, 1993" startWordPosition="2977" endWordPosition="2980">ish show the next best improvements. The English-German cluster model performs better than the mkcls7 tool (72.83%). 4 Related Work Our monolingual clustering model is purely distributional in nature. Other extensions to word clustering have incorporated morphological and orthographic information (Clark, 2003). The work of Snyder and Barzilay (2010), which focused on POS induction is very closely related. The earliest work on bilingual word clustering was proposed by (Och, 1999) which, like us, uses a lan7http://www.statmt.org/moses/giza/mkcls.html guage modeling approach (Brown et al., 1992; Kneser and Ney, 1993) for monolingual optimization and a similarity function for bilingual similarity. T¨ackstr¨om et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological</context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>R. Kneser and H. Ney. 1993. Forming word classes by statistical clustering for statistical language modelling. In R. Khler and B. Rieger, editors, Contributions to Quantitative Linguistics, pages 221–226. Springer Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1261" citStr="Koo et al., 2008" startWordPosition="185" endWordPosition="188">rage mutual information of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically significant improvement in F1 score when using bilingual word clusters instead of monolingual clusters. 1 Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each language independently and ensures that the data is well-explained by the clustering in a sequence model </context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Martin</author>
<author>J Liermann</author>
<author>H Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering. In Speech Communication,</title>
<date>1995</date>
<pages>1253--1256</pages>
<contexts>
<context position="8937" citStr="Martin et al., 1995" startWordPosition="1535" endWordPosition="1538"> any word is completely informative about the words it is aligned to. By including this term, a slightly worse monolingual solution is chosen, but the clustering corresponds to the reasonable intuition that words with the same meaning (i.e., the two variants of to play) should be clustered together. 2.4 Inference Figure 1 shows the factor graph representation of our clustering models. Finding the optimal clustering under both the monolingual and bilingual objectives is a computationally hard combinatorial optimization problem (Och, 1995). We use a greedy hill-climbing word exchange algorithm (Martin et al., 1995) to find a minimum 1C ≡ D iff ∀i|{D(y)|∀(x, y) E A, C(x) = i} |= 1 value for our objective. We terminate the optimization procedure when the number of words exchanged at the end of one complete iteration through both the languages is less than 0.1% of the sum of vocabulary of the two languages and at least five complete iterations have been completed.2 For every language the word clusters are initialised in a round robin order according to the token frequency. 3 Experiments Evaluation of clustering is not a trivial problem. One branch of work seeks to recast the problem as the of part-of-speec</context>
</contexts>
<marker>Martin, Liermann, Ney, 1995</marker>
<rawString>S. Martin, J. Liermann, and H. Ney. 1995. Algorithms for bigram and trigram word clustering. In Speech Communication, pages 1253–1256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meilˇa</author>
</authors>
<title>Comparing Clusterings by the Variation of Information.</title>
<date>2003</date>
<booktitle>In Learning Theory and Kernel Machines,</booktitle>
<pages>173--187</pages>
<marker>Meilˇa, 2003</marker>
<rawString>M. Meilˇa. 2003. Comparing Clusterings by the Variation of Information. In Learning Theory and Kernel Machines, pages 173–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>H M Wallach</author>
<author>J Naradowsky</author>
<author>D A Smith</author>
<author>A McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>880--889</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17998" citStr="Mimno et al., 2009" startWordPosition="3058" endWordPosition="3061">guage modeling approach (Brown et al., 1992; Kneser and Ney, 1993) for monolingual optimization and a similarity function for bilingual similarity. T¨ackstr¨om et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmentation (Snyder and Barzilay, 2008). Also closely related is the technique of crosslingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun usage (Faruqui and Pad´o, 2012). 5 Conclusions We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information between clusters of adjacent words, and also high mutual </context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>D. Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, and A. McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 880– 889, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Maximum-Likelihood-Sch¨atzung von Wortkategorien mit Verfahren der kombinatorischen Optimierung.</title>
<date>1995</date>
<tech>Studienarbeit,</tech>
<institution>University of Erlangen.</institution>
<contexts>
<context position="8860" citStr="Och, 1995" startWordPosition="1524" endWordPosition="1525">ve. This has an AVI of 0, indicating that knowing the clustering of any word is completely informative about the words it is aligned to. By including this term, a slightly worse monolingual solution is chosen, but the clustering corresponds to the reasonable intuition that words with the same meaning (i.e., the two variants of to play) should be clustered together. 2.4 Inference Figure 1 shows the factor graph representation of our clustering models. Finding the optimal clustering under both the monolingual and bilingual objectives is a computationally hard combinatorial optimization problem (Och, 1995). We use a greedy hill-climbing word exchange algorithm (Martin et al., 1995) to find a minimum 1C ≡ D iff ∀i|{D(y)|∀(x, y) E A, C(x) = i} |= 1 value for our objective. We terminate the optimization procedure when the number of words exchanged at the end of one complete iteration through both the languages is less than 0.1% of the sum of vocabulary of the two languages and at least five complete iterations have been completed.2 For every language the word clusters are initialised in a round robin order according to the token frequency. 3 Experiments Evaluation of clustering is not a trivial pr</context>
</contexts>
<marker>Och, 1995</marker>
<rawString>F. J. Och. 1995. Maximum-Likelihood-Sch¨atzung von Wortkategorien mit Verfahren der kombinatorischen Optimierung. Studienarbeit, University of Erlangen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, EACL ’99,</booktitle>
<pages>71--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1474" citStr="Och, 1999" startWordPosition="218" endWordPosition="219">stead of monolingual clusters. 1 Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each language independently and ensures that the data is well-explained by the clustering in a sequence model (§2.1). The second term ensures that the cluster alignments induced by a word alignment have high mutual information across languages (§2.2). Since the objective consists of terms representing the entropy monoling</context>
<context position="17308" citStr="Och, 1999" startWordPosition="2961" endWordPosition="2962">er than the monolingual baselines. English again has a statistically significant improvement over the baseline. French and Turkish show the next best improvements. The English-German cluster model performs better than the mkcls7 tool (72.83%). 4 Related Work Our monolingual clustering model is purely distributional in nature. Other extensions to word clustering have incorporated morphological and orthographic information (Clark, 2003). The work of Snyder and Barzilay (2010), which focused on POS induction is very closely related. The earliest work on bilingual word clustering was proposed by (Och, 1999) which, like us, uses a lan7http://www.statmt.org/moses/giza/mkcls.html guage modeling approach (Brown et al., 1992; Kneser and Ney, 1993) for monolingual optimization and a similarity function for bilingual similarity. T¨ackstr¨om et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>F. J. Och. 1999. An efficient method for determining bilingual word classes. In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, EACL ’99, pages 71–76, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schiehlen</author>
</authors>
<title>Learning tense translation from bilingual corpora.</title>
<date>1998</date>
<contexts>
<context position="18336" citStr="Schiehlen, 1998" startWordPosition="3110" endWordPosition="3111">stic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmentation (Snyder and Barzilay, 2008). Also closely related is the technique of crosslingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun usage (Faruqui and Pad´o, 2012). 5 Conclusions We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information between clusters of adjacent words, and also high mutual information across observed word alignment links. We have shown that improvement in clustering can be obtained across a range of language pairs, evaluated in terms of their value as features in an extrinsic NER task. Our model can be extended for clustering any number of given languages together in a joint framework, and incorporate bot</context>
</contexts>
<marker>Schiehlen, 1998</marker>
<rawString>M. Schiehlen. 1998. Learning tense translation from bilingual corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>N A Smith</author>
</authors>
<title>Probabilistic Models of Nonprojective Dependency Trees.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>132--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="18263" citStr="Smith and Smith, 2007" startWordPosition="3097" endWordPosition="3100">ustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmentation (Snyder and Barzilay, 2008). Also closely related is the technique of crosslingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun usage (Faruqui and Pad´o, 2012). 5 Conclusions We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information between clusters of adjacent words, and also high mutual information across observed word alignment links. We have shown that improvement in clustering can be obtained across a range of language pairs, evaluated in terms of their value as features in an extrinsic NER task. Our model can be extended for clustering any num</context>
</contexts>
<marker>Smith, Smith, 2007</marker>
<rawString>D. A. Smith and N. A. Smith. 2007. Probabilistic Models of Nonprojective Dependency Trees. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 132–140, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation. In</title>
<date>2008</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18087" citStr="Snyder and Barzilay, 2008" startWordPosition="3070" endWordPosition="3073">l optimization and a similarity function for bilingual similarity. T¨ackstr¨om et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmentation (Snyder and Barzilay, 2008). Also closely related is the technique of crosslingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun usage (Faruqui and Pad´o, 2012). 5 Conclusions We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information between clusters of adjacent words, and also high mutual information across observed word alignment links. We have shown that improvement in clust</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>B. Snyder and R. Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Climbing the tower of babel: Unsupervised multilingual learning.</title>
<date>2010</date>
<booktitle>Proceedings of the 27th International Conference on Machine Learning (ICML-10),</booktitle>
<pages>29--36</pages>
<editor>In J. Frnkranz and T. Joachims, editors,</editor>
<publisher>Omnipress.</publisher>
<location>Haifa, Israel,</location>
<contexts>
<context position="17176" citStr="Snyder and Barzilay (2010)" startWordPosition="2936" endWordPosition="2939">nd train NER classifiers using these. Table 1 shows the performance of German NER classifiers on the test set. All the values shown in bold are better than the monolingual baselines. English again has a statistically significant improvement over the baseline. French and Turkish show the next best improvements. The English-German cluster model performs better than the mkcls7 tool (72.83%). 4 Related Work Our monolingual clustering model is purely distributional in nature. Other extensions to word clustering have incorporated morphological and orthographic information (Clark, 2003). The work of Snyder and Barzilay (2010), which focused on POS induction is very closely related. The earliest work on bilingual word clustering was proposed by (Och, 1999) which, like us, uses a lan7http://www.statmt.org/moses/giza/mkcls.html guage modeling approach (Brown et al., 1992; Kneser and Ney, 1993) for monolingual optimization and a similarity function for bilingual similarity. T¨ackstr¨om et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception</context>
</contexts>
<marker>Snyder, Barzilay, 2010</marker>
<rawString>B. Snyder and R. Barzilay. 2010. Climbing the tower of babel: Unsupervised multilingual learning. In J. Frnkranz and T. Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 29–36. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O T¨ackstr¨om</author>
<author>R McDonald</author>
<author>J Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>page</pages>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>O. T¨ackstr¨om, R. McDonald, and J. Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, volume 1, page 11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1283" citStr="Turian et al., 2010" startWordPosition="189" endWordPosition="192">ation of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically significant improvement in F1 score when using bilingual word clusters instead of monolingual clusters. 1 Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each language independently and ensures that the data is well-explained by the clustering in a sequence model (§2.1). The second ter</context>
<context position="9689" citStr="Turian et al., 2010" startWordPosition="1666" endWordPosition="1669">when the number of words exchanged at the end of one complete iteration through both the languages is less than 0.1% of the sum of vocabulary of the two languages and at least five complete iterations have been completed.2 For every language the word clusters are initialised in a round robin order according to the token frequency. 3 Experiments Evaluation of clustering is not a trivial problem. One branch of work seeks to recast the problem as the of part-of-speech (POS) induction and attempts to match linguistic intuitions. However, hard clusters are particularly useful for downstream tasks (Turian et al., 2010). We therefore chose to focus our evaluation on the latter problem. For our evaluation, we use our word clusters as an input to a named entity recognizer which uses these clusters as a source of features. Our evaluation task is the German corpus with NER annotation that was created for the shared task at CoNLL-2003 3. The training set contains approximately 220,000 tokens and the development set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semisupervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>