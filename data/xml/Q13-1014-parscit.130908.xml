<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.9938595">
Learning to translate with products of novices: a suite of open-ended
challenge problems for teaching MT
</title>
<author confidence="0.9914185">
Adam Lopez1, Matt Post1, Chris Callison-Burch1 2, Jonathan Weese, Juri Ganitkevitch,
Narges Ahmidi, Olivia Buzek, Leah Hanson, Beenish Jamil, Matthias Lee, Ya-Ting Lin,
Henry Pao, Fatima Rivera, Leili Shahriyari, Debu Sinha, Adam Teichert,
Stephen Wampler, Michael Weinberger, Daguang Xu, Lin Yang, and Shang Zhao∗
</author>
<affiliation confidence="0.986031">
Department of Computer Science, Johns Hopkins University
1Human Language Technology Center of Excellence, Johns Hopkins University
2Computer and Information Science Department, University of Pennsylvania
</affiliation>
<sectionHeader confidence="0.979054" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999288285714286">
Machine translation (MT) draws from several
different disciplines, making it a complex sub-
ject to teach. There are excellent pedagogical
texts, but problems in MT and current algo-
rithms for solving them are best learned by
doing. As a centerpiece of our MT course,
we devised a series of open-ended challenges
for students in which the goal was to im-
prove performance on carefully constrained
instances of four key MT tasks: alignment,
decoding, evaluation, and reranking. Students
brought a diverse set of techniques to the prob-
lems, including some novel solutions which
performed remarkably well. A surprising and
exciting outcome was that student solutions
or their combinations fared competitively on
some tasks, demonstrating that even newcom-
ers to the field can help improve the state-of-
the-art on hard NLP problems while simulta-
neously learning a great deal. The problems,
baseline code, and results are freely available.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99973825">
A decade ago, students interested in natural lan-
guage processing arrived at universities having been
exposed to the idea of machine translation (MT)
primarily through science fiction. Today, incoming
students have been exposed to services like Google
Translate since they were in secondary school or ear-
lier. For them, MT is science fact. So it makes sense
to teach statistical MT, either on its own or as a unit
∗ The first five authors were instructors and the remaining au-
thors were students in the worked described here. This research
was conducted while Chris Callison-Burch was at Johns Hop-
kins University.
in a class on natural language processing (NLP), ma-
chine learning (ML), or artificial intelligence (AI). A
course that promises to show students how Google
Translate works and teach them how to build some-
thing like it is especially appealing, and several uni-
versities and summer schools now offer such classes.
There are excellent introductory texts—depending
on the level of detail required, instructors can choose
from a comprehensive MT textbook (Koehn, 2010),
a chapter of a popular NLP textbook (Jurafsky and
Martin, 2009), a tutorial survey (Lopez, 2008), or
an intuitive tutorial on the IBM Models (Knight,
1999b), among many others.
But MT is not just an object of academic study.
It’s a real application that isn’t fully perfected, and
the best way to learn about it is to build an MT sys-
tem. This can be done with open-source toolkits
such as Moses (Koehn et al., 2007), cdec (Dyer et
al., 2010), or Joshua (Ganitkevitch et al., 2012), but
these systems are not designed for pedagogy. They
are mature codebases featuring tens of thousands of
source code lines, making it difficult to focus on
their core algorithms. Most tutorials present them
as black boxes. But our goal is for students to learn
the key techniques in MT, and ideally to learn by
doing. Black boxes are incompatible with this goal.
We solve this dilemma by presenting students
with concise, fully-functioning, self-contained com-
ponents of a statistical MT system: word alignment,
decoding, evaluation, and reranking. Each imple-
mentation consists of a naive baseline algorithm in
less than 150 lines of Python code. We assign them
to students as open-ended challenges in which the
goal is to improve performance on objective eval-
uation metrics as much as possible. This setting
mirrors evaluations conducted by the NLP research
</bodyText>
<page confidence="0.991898">
165
</page>
<bodyText confidence="0.904566214285714">
Transactions of the Association for Computational Linguistics, 1 (2013) 165–178. Action Editor: David Chiang.
Submitted 11/2012; Revised 3/2013; Published 5/2013. c�2013 Association for Computational Linguistics.
community and by the engineering teams behind
high-profile NLP projects such as Google Translate
and IBM’s Watson. While we designate specific al-
gorithms as benchmarks for each task, we encour-
age creativity by awarding more points for the best
systems. As additional incentive, we provide a web-
based leaderboard to display standings in real time.
In our graduate class on MT, students took a va-
riety of different approaches to the tasks, in some
cases devising novel algorithms. A more exciting re-
sult is that some student systems or combinations of
systems rivaled the state of the art on some datasets.
</bodyText>
<sectionHeader confidence="0.990419" genericHeader="introduction">
2 Designing MT Challenge Problems
</sectionHeader>
<bodyText confidence="0.9995347">
Our goal was for students to freely experiment with
different ways of solving MT problems on real data,
and our approach consisted of two separable com-
ponents. First, we provided a framework that strips
key MT problems down to their essence so students
could focus on understanding classic algorithms or
invent new ones. Second, we designed incentives
that motivated them to improve their solutions as
much as possible, encouraging experimentation with
approaches beyond what we taught in class.
</bodyText>
<sectionHeader confidence="0.726149" genericHeader="method">
2.1 Decoding, Reranking, Evaluation, and
Alignment for MT (DREAMT)
</sectionHeader>
<bodyText confidence="0.999854052631579">
We designed four assignments, each corresponding
to a real subproblem in MT: alignment, decoding,
evaluation, and reranking.1 From the more general
perspective of AI, they emphasize the key problems
of unsupervised learning, search, evaluation design,
and supervised learning, respectively. In real MT
systems, these problems are highly interdependent,
a point we emphasized in class and at the end of each
assignment—for example, that alignment is an exer-
cise in parameter estimation for translation models,
that model choice is a tradeoff between expressivity
and efficient inference, and that optimal search does
not guarantee optimal accuracy. However, present-
ing each problem independently and holding all else
constant enables more focused exploration.
For each problem we provided data, a naive solu-
tion, and an evaluation program. Following Bird et
al. (2008) and Madnani and Dorr (2008), we imple-
mented the challenges in Python, a high-level pro-
</bodyText>
<footnote confidence="0.896059">
1http://alopez.github.io/dreamt
</footnote>
<bodyText confidence="0.985723466666667">
gramming language that can be used to write very
concise programs resembling pseudocode.2,3 By de-
fault, each baseline system reads the test data and
generates output in the evaluation format, so setup
required zero configuration, and students could be-
gin experimenting immediately. For example, on re-
ceipt of the alignment code, aligning data and eval-
uating results required only typing:
&gt; align  |grade
Students could then run experiments within minutes
of beginning the assignment.
Three of the four challenges also included unla-
beled test data (except the decoding assignment, as
explained in §4). We evaluated test results against a
hidden key when assignments were submitted.
</bodyText>
<subsectionHeader confidence="0.972369">
2.2 Incentive Design
</subsectionHeader>
<bodyText confidence="0.999900777777778">
We wanted to balance several pedagogical goals: un-
derstanding of classic algorithms, free exploration
of alternatives, experience with typical experimental
design, and unhindered collaboration.
Machine translation is far from solved, so we ex-
pected more than reimplementation of prescribed al-
gorithms; we wanted students to really explore the
problems. To motivate exploration, we made the as-
signments competitive. Competition is a powerful
force, but must be applied with care in an educa-
tional setting.4 We did not want the consequences
of ambitious but failed experiments to be too dire,
and we did not want to discourage collaboration.
For each assignment, we guaranteed a passing
grade for matching the performance of a specific tar-
get algorithm. Typically, the target was important
but not state-of-the-art: we left substantial room for
improvement, and thus competition. We told stu-
dents the exact algorithm that produced the target ac-
curacy (though we expected them to derive it them-
selves based on lectures, notes, or literature). We
did not specifically require them to implement it, but
the guarantee of a passing grade provided a power-
ful incentive for this to be the first step of each as-
signment. Submissions that beat this target received
additional credit. The top five submissions received
full credit, while the top three received extra credit.
</bodyText>
<footnote confidence="0.9953635">
2http://python.org
3Some well-known MT systems have been implemented in
Python (Chiang, 2007; Huang and Chiang, 2007).
4Thanks to an anonymous reviewer for this turn of phrase.
</footnote>
<page confidence="0.998488">
166
</page>
<bodyText confidence="0.999210575">
This scheme provided strong incentive to continue
experimentation beyond the target algorith5
m.
For each assignment, students could form teams
of any size, under three rules: each team had to pub-
licize its formation to the class, all team members
agreed to receive the same grade, and teams could
not drop members. Our hope was that these require-
ments would balance the perceived competitive ad-
vantage of collaboration against a reluctance to take
(and thus support) teammates who did not contribute
to the competitive effort.6 This strategy worked: out
of sixteen students, ten opted to work collaboratively
on at least one assignment, always in pairs.
We provided a web-based leaderboard that dis-
played standings on the test data in real time, iden-
tifying each submission by a pseudonymous han-
dle known only to the team and instructors. Teams
could upload solutions as often as they liked before
the assignment deadline. The leaderboard displayed
scores of the default and target algorithms. This in-
centivized an early start, since teams could verify
for themselves when they met the threshold for a
passing grade. Though effective, it also detracted
from realism in one important way: it enabled hill-
climbing on the evaluation metric. In early assign-
ments, we observed a few cases of this behavior,
so for the remaining assignments, we modified the
leaderboard so that changes in score would only be
reflected once every twelve hours. This strategy
trades some amount of scientific realism for some
measure of incentive, a strategy that has proven
effective in other pedagogical tools with real-time
feedback (Spacco et al., 2006).
To obtain a grade, teams were required to sub-
mit their results, share their code privately with the
instructors, and publicly describe their experimen-
tal process to the class so that everyone could learn
from their collective effort. Teams were free (but not
required) to share their code publicly at any time.
</bodyText>
<footnote confidence="0.7178444">
5Grades depend on institutional norms. In our case, high grades
in the rest of class combined with matching all assignment tar-
get algorithms would earn a B+; beating two target algorithms
would earn an A-; top five placement on any assignment would
earn an A; and top three placement compensated for weaker
grades in other course criteria. Everyone who completed all
four assignments placed in the top five at least once.
6The equilibrium point is a single team, though this team would
still need to decide on a division of labor. One student contem-
plated organizing this team, but decided against it.
</footnote>
<bodyText confidence="0.876183">
Some did so after the assignment deadline.
</bodyText>
<sectionHeader confidence="0.972128" genericHeader="method">
3 The Alignment Challenge
</sectionHeader>
<bodyText confidence="0.999904">
The first challenge was word alignment: given a par-
allel text, students were challenged to produce word-
to-word alignments with low alignment error rate
(AER; Och and Ney, 2000). This is a variant of a
classic assignment not just in MT, but in NLP gen-
erally. Klein (2005) describes a version of it, and we
know several other instructors who use it.7 In most
of these, the object is to implement IBM Model 1
or 2, or a hidden Markov model. Our version makes
it open-ended by asking students to match or beat an
IBM Model 1 baseline.
</bodyText>
<subsectionHeader confidence="0.992711">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999993882352941">
We provided 100,000 sentences of parallel data from
the Canadian Hansards, totaling around two million
words.8 This dataset is small enough to align in
a few minutes with our implementation—enabling
rapid experimentation—yet large enough to obtain
reasonable results. In fact, Liang et al. (2006) report
alignment accuracy on data of this size that is within
a fraction of a point of their accuracy on the com-
plete Hansards data. To evaluate, we used manual
alignments of a small fraction of sentences, devel-
oped by Och and Ney (2000), which we obtained
from the shared task resources organized by Mihal-
cea and Pedersen (2003). The first 37 sentences
of the corpus were development data, with manual
alignments provided in a separate file. Test data con-
sisted of an additional 447 sentences, for which we
did not provide alignments.9
</bodyText>
<subsectionHeader confidence="0.998526">
3.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999612">
We distributed three Python programs with the
data. The first, align, computes Dice’s coefficient
(1945) for every pair of French and English words,
then aligns every pair for which its value is above an
adjustable threshold. Our implementation (most of
</bodyText>
<footnote confidence="0.994796285714286">
7Among them, Jordan Boyd-Graber, John DeNero, Philipp
Koehn, and Slav Petrov (personal communication).
8http://www.isi.edu/natural-language/download/hansard/
9This invited the possibility of cheating, since alignments of the
test data are publicly available on the web. We did not adver-
tise this, but as an added safeguard we obfuscated the data by
distributing the test sentences randomly throughout the file.
</footnote>
<page confidence="0.996161">
167
</page>
<bodyText confidence="0.989465">
Listing 1 The default aligner in DREAMT: thresh-
olding Dice’s coefficient.
</bodyText>
<equation confidence="0.910232272727273">
for (f, e) in bitext:
for f_i in set(f):
f_count[f_i] += 1
for e_j in set(e):
fe_count[(f_i,e_j)] += 1
for e_j in set(e):
e_count[e_j] += 1
for (f_i, e_j) in fe_count.keys():
dice[(f_i,e_j)] = \
2.0 * fe_count[(f_i, e_j)] / \
(f_count[f_i] + e_count[e_j])
</equation>
<bodyText confidence="0.9823381">
for (f, e) in bitext:
for (i, f_i) in enumerate(f):
for (j, e_j) in enumerate(e):
if dice[(f_i,e_j)] &gt;= cutoff:
print &amp;quot;%i-%i &amp;quot; % (i,j)
which is shown in Listing 1) is quite close to pseu-
docode, making it easy to focus on the algorithm,
one of our pedagogical goals. The grade program
computes AER and optionally prints an alignment
grid for sentences in the development data, showing
both human and automatic alignments. Finally the
check program verifies that the results represent
a valid solution, reporting an error if not—enabling
students to diagnose bugs in their submissions.
The default implementation enabled immediate
experimentation. On receipt of the code, students
were instructed to align the first 1,000 sentences and
compute AER using a simple command.
&gt; align -n 1000  |grade By varying the
number of input sentences and the threshold for an
alignment, students could immediately see the effect
of various parameters on alignment quality.
We privately implemented IBM Model 1 (Brown
et al., 1993) as the target algorithm for a passing
grade. We ran it for five iterations with English
as the target language and French as the source.
Our implementation did not use null alignment
or symmetrization—leaving out these common im-
provements offered students the possibility of dis-
covering them independently, and thereby rewarded.
</bodyText>
<figureCaption confidence="0.9563692">
Figure 1: Submission history for the alignment challenge.
Dashed lines represent the default and baseline system
performance. Each colored line represents a student, and
each dot represents a submission. For clarity, we show
only submissions that improved the student’s AER.
</figureCaption>
<subsectionHeader confidence="0.999571">
3.3 Challenge Results
</subsectionHeader>
<bodyText confidence="0.999987666666667">
We received 209 submissions from 11 teams over a
period of two weeks (Figure 1). Everyone eventually
matched or exceeded IBM Model 1 AER of 31.26.
Most students implemented IBM Model 1, but we
saw many other solutions, indicating that many truly
experimented with the problem:
</bodyText>
<listItem confidence="0.997445625">
• Implementing heuristic constraints to require
alignment of proper names and punctuation.
• Running the algorithm on stems rather than sur-
face words.
• Initializing the first iteration of Model 1 with
parameters estimated on the observed align-
ments in the development data.
• Running Model 1 for many iterations. Most re-
</listItem>
<bodyText confidence="0.598124">
searchers typically run Model 1 for five itera-
tions or fewer, and there are few experiments
in the literature on its behavior over many iter-
ations, as there are for hidden Markov model
taggers (Johnson, 2007). Our students carried
out these experiments, reporting runs of 5, 20,
100, and even 2000 iterations. No improve-
ment was observed after 20 iterations.
</bodyText>
<figure confidence="0.985763666666667">
20
50
60
AER × 100
30
40
</figure>
<page confidence="0.959419">
168
</page>
<bodyText confidence="0.982596810810811">
• Implementing various alternative approaches
from the literature, including IBM Model 2
(Brown et al., 1993), competitive linking
(Melamed, 2000), and smoothing (Moore,
2004).
One of the best solutions was competitive linking
with Dice’s coefficient, modified to incorporate the
observation that alignments tend to be monotonic by
restricting possible alignment points to a window of
eight words around the diagonal. Although simple,
it acheived an AER of 18.41, an error reduction over
Model 1 of more than 40%.
The best score compares unfavorably against a
state-of-the-art AER of 3.6 (Liu et al., 2010). But
under a different view, it still represents a significant
amount of progress for an effort taking just over two
weeks: on the original challenge from which we ob-
tained the data (Mihalcea and Pedersen, 2003) the
best student system would have placed fifth out of
fifteen systems. Consider also the combined effort of
all the students: when we trained a perceptron clas-
sifier on the development data, taking each student’s
prediction as a feature, we obtained an AER of 15.4,
which would have placed fourth on the original chal-
lenge. This is notable since none of the systems
incorporated first-order dependencies on the align-
ments of adjacent words, long noted as an impor-
tant feature of the best alignment models (Och and
Ney, 2003). Yet a simple system combination of stu-
dent assignments is as effective as a hidden Markov
Model trained on a comparable amount of data (Och
and Ney, 2003).
It is important to note that AER does not neces-
sarily correlate with downstream performance, par-
ticularly on the Hansards dataset (Fraser and Marcu,
2007). We used the conclusion of the assignment as
an opportunity to emphasize this point.
</bodyText>
<sectionHeader confidence="0.983173" genericHeader="method">
4 The Decoding Challenge
</sectionHeader>
<bodyText confidence="0.9912142">
The second challenge was decoding: given a fixed
translation model and a set of input sentences, stu-
dents were challenged to produce translations with
the highest model score. This challenge introduced
the difficulties of combinatorial optimization under
a deceptively simple setup: the model we provided
was a simple phrase-based translation model (Koehn
et al., 2003) consisting only of a phrase table and tri-
gram language model. Under this simple model, for
a French sentence f of length I, English sentence
e of length J, and alignment a where each element
consists of a span in both e and f such that every
word in both e and f is aligned exactly once, the
conditional probability of e and a given f is as fol-
lows.10
</bodyText>
<equation confidence="0.993454">
�
p(e,a|f) =
(i,i,,j,j,)∈a
(1)
</equation>
<bodyText confidence="0.9278745">
To evaluate output, we compute the conditional
probability of e as follows.
</bodyText>
<equation confidence="0.981407666666667">
�
p(e|f) = p(e, a|f) (2)
a
</equation>
<bodyText confidence="0.999893037037037">
Note that this formulation is different from the typ-
ical Viterbi objective of standard beam search de-
coders, which do not sum over all alignments, but
approximate p(e|f) by maxa p(e, a|f). Though the
computation in Equation 2 is intractable (DeNero
and Klein, 2008), it can be computed in a few min-
utes via dynamic programming on reasonably short
sentences. We ensured that our data met this crite-
rion. The corpus-level probability is then the prod-
uct of all sentence-level probabilities in the data.
The model includes no distortion limit or distor-
tion model, for two reasons. First, leaving out the
distortion model slightly simplifies the implementa-
tion, since it is not necessary to keep track of the last
word translated in a beam decoder; we felt that this
detail was secondary to understanding the difficulty
of search over phrase permutations. Second, it actu-
ally makes the problem more difficult, since a simple
distance-based distortion model prefers translations
with fewer permutations; without it, the model may
easily prefer any permutation of the target phrases,
making even the Viterbi search problem exhibit its
true NP-hardness (Knight, 1999a; Zaslavskiy et al.,
2009).
Since the goal was to find the translation with the
highest probability, we did not provide a held-out
test set; with access to both the input sentences and
</bodyText>
<footnote confidence="0.98439325">
10For simplicity, this formula assumes that a is padded with two
sentence-initial symbols and one sentence-final symbol, and
ignores the probability of sentence segmentation, which we
take to be uniform.
</footnote>
<table confidence="0.962346">
p(fi, J+1H p(ej|ej−1, ej−2)
i |ej, j=1
j )
</table>
<page confidence="0.994042">
169
</page>
<bodyText confidence="0.999890833333333">
the model, students had enough information to com-
pute the evaluation score on any dataset themselves.
The difficulty of the challenge lies simply in finding
the translation that maximizes the evaluation. In-
deed, since the problem is intractable, even the in-
structors did not know the true solution.11
</bodyText>
<subsectionHeader confidence="0.968891">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999979375">
We chose 48 French sentences totaling 716 words
from the Canadian Hansards to serve as test data.
To create a simple translation model, we used the
Berkeley aligner to align the parallel text from the
first assignment, and extracted a phrase table using
the method of Lopez (2007), as implemented in cdec
(Dyer et al., 2010). To create a simple language
model, we used SRILM (Stolcke, 2002).
</bodyText>
<subsectionHeader confidence="0.88975">
4.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999931243243243">
We distributed two Python programs. The first,
decode, decodes the test data monotonically—
using both the language model and translation
model, but without permuting phrases. The imple-
mentation is completely self-contained with no ex-
ternal dependencies: it implements both models and
a simple stack decoding algorithm for monotonic
translation. It contains only 122 lines of Python—
orders of magnitude fewer than most full-featured
decoders. To see its similarity to pseudocode, com-
pare the decoding algorithm (Listing 2) with the
pseudocode in Koehn’s (2010) popular textbook (re-
produced here as Algorithm 1). The second pro-
gram, grade, computes the log-probability of a set
of translations, as outline above.
We privately implemented a simple stack decoder
that searched over permutations of phrases, similar
to Koehn (2004). Our implementation increased the
codebase by 44 lines of code and included param-
eters for beam size, distortion limit, and the maxi-
mum number of translations considered for each in-
put phrase. We posted a baseline to the leaderboard
using values of 50, 3, and 20 for these, respectively.
11 We implemented a version of the Lagrangian relaxation algo-
rithm of Chang and Collins (2011), but found it difficult to
obtain tight (optimal) solutions without iteratively reintroduc-
ing all of the original constraints. We suspect this is due to
the lack of a distortion penalty, which enforces a strong pref-
erence towards translations with little reordering. However,
the solution found by this algorithm is only approximates the
objective implied by Equation 2, which sums over alignments.
We also posted an oracle containing the most prob-
able output for each sentence, selected from among
all submissions received so far. The intent of this
oracle was to provide a lower bound on the best pos-
sible output, giving students additional incentive to
continue improving their systems.
</bodyText>
<subsectionHeader confidence="0.999163">
4.3 Challenge Results
</subsectionHeader>
<bodyText confidence="0.9997465">
We received 71 submissions from 10 teams (Fig-
ure 2), again exhibiting variety of solutions.
</bodyText>
<listItem confidence="0.922707777777778">
• Implementation of greedy decoder which at
each step chooses the most probable translation
from among those reachable by a single swap
or retranslation (Germann et al., 2001; Langlais
et al., 2007).
• Inclusion of heuristic estimates of future cost.
• Implementation of a private oracle. Some stu-
dents observed that the ideal beam setting was
not uniform across the corpus. They ran their
</listItem>
<bodyText confidence="0.956327473684211">
decoder under different settings, and then se-
lected the most probable translation of each
sentence.
Many teams who implemented the standard stack
decoding algorithm experimented heavily with its
pruning parameters. The best submission used ex-
tremely wide beam settings in conjunction with a
reimplementation of the future cost estimate used in
Moses (Koehn et al., 2007). Five of the submissions
beat Moses using its standard beam settings after it
had been configured to decode with our model.
We used this assignment to emphasize the im-
portance of good models: the model score of the
submissions was generally inversely correlated with
BLEU, possibly because our simple model had no
distortion limits. We used this to illustrate the differ-
ence between model error and search error, includ-
ing fortuitous search error (Germann et al., 2001)
made by decoders with less accurate search.
</bodyText>
<sectionHeader confidence="0.994796" genericHeader="method">
5 The Evaluation Challenge
</sectionHeader>
<bodyText confidence="0.99976">
The third challenge was evaluation: given a test cor-
pus with reference translations and the output of sev-
eral MT systems, students were challenged to pro-
duce a ranking of the systems that closely correlated
with a human ranking.
</bodyText>
<page confidence="0.988291">
170
</page>
<bodyText confidence="0.912167">
Listing 2 The default decoder in DREAMT: a stack decoder for monotonic translation.
</bodyText>
<equation confidence="0.963431727272727">
stacks = [{} for _ in f] + [{}]
stacks[0][lm.begin()] = initial_hypothesis
for i, stack in enumerate(stacks[:-1]):
for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:alpha]:
for j in xrange(i+1,len(f)+1):
if f[i:j] in tm:
for phrase in tm[f[i:j]]:
logprob = h.logprob + phrase.logprob
lm_state = h.lm_state
for word in phrase.english.split():
(lm_state, word_logprob) = lm.score(lm_state, word)
logprob += word_logprob
logprob += lm.end(lm_state) if j == len(f) else 0.0
new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
if lm_state not in stacks[j] or \
stacks[j][lm_state].logprob &lt; logprob:
stacks[j][lm_state] = new_hypothesis
winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
def extract_english(h):
return &amp;quot;&amp;quot; if h.predecessor is None else &amp;quot;%s%s &amp;quot; %
(extract_english(h.predecessor), h.phrase.english)
print extract_english(winner)
</equation>
<figureCaption confidence="0.688563666666667">
Algorithm 1 Basic stack decoding algorithm, -1200
adapted from Koehn (2010), p. 165.
place empty hypothesis into stack 0 log10 p(e|f) − C -1250
for all stacks 0...n − 1 do -1300
for all hypotheses in stack do -1350
for all translation options do -1400
if applicable then
create new hypothesis
place in stack
recombine with existing hypothesis
prune stack if too big
5.1 Data
</figureCaption>
<bodyText confidence="0.999714444444444">
We chose the English-to-German translation sys-
tems from the 2009 and 2011 shared task at the an-
nual Workshop for Machine Translation (Callison-
Burch et al., 2009; Callison-Burch et al., 2011), pro-
viding the first as development data and the second
as test data. We chose these sets because BLEU
(Papineni et al., 2002), our baseline metric, per-
formed particularly poorly on them; this left room
for improvement in addition to highlighting some
</bodyText>
<figureCaption confidence="0.914766666666667">
Figure 2: Submission history for the decoding challenge.
The dotted green line represents the oracle over submis-
sions.
</figureCaption>
<bodyText confidence="0.99954875">
deficiencies of BLEU. For each dataset we pro-
vided the source and reference sentences along with
anonymized system outputs. For the development
data we also provided the human ranking of the sys-
</bodyText>
<page confidence="0.996417">
171
</page>
<bodyText confidence="0.999581333333333">
tems, computed from pairwise human judgements
according to a formula recommended by Bojar et al.
(2011).12
</bodyText>
<subsectionHeader confidence="0.993777">
5.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999942681818182">
We provided three simple Python programs:
evaluate implements a simple ranking of the sys-
tems based on position-independent word error rate
(PER; Tillmann et al., 1997), which computes a bag-
of-words overlap between the system translations
and the reference. The grade program computes
Spearman’s ρ between the human ranking and an
output ranking. The check program simply ensures
that a submission contains a valid ranking.
We were concerned about hill-climbing on the test
data, so we modified the leaderboard to report new
results only twice a day. This encouraged students to
experiment on the development data before posting
new submissions, while still providing intermittent
feedback.
We privately implemented a version of BLEU,
which obtained a correlation of 38.6 with the human
rankings, a modest improvement over the baseline
of 34.0. Our implementation underperforms the one
reported in Callison-Burch et al. (2011) since it per-
forms no tokenization or normalization of the data.
This also left room for improvement.
</bodyText>
<subsectionHeader confidence="0.992478">
5.3 Evaluation Challenge Results
</subsectionHeader>
<bodyText confidence="0.999359">
We received 212 submissions from 12 teams (Fig-
ure 3), again demonstrating a wide range of tech-
niques.
</bodyText>
<listItem confidence="0.991892">
• Experimentation with the maximum n-gram
length and weights in BLEU.
• Implementation of smoothed versions of BLEU
(Lin and Och, 2004).
• Implementation of weighted F-measure to bal-
ance both precision and recall.
• Careful normalization of the reference and ma-
chine translations, including lowercasing and
punctuation-stripping.
</listItem>
<footnote confidence="0.92684175">
12This ranking has been disputed over a series of papers (Lopez,
2012; Callison-Burch et al., 2012; Koehn, 2012). The paper
which initiated the dispute, written by the first author, was di-
rectly inspired by the experience of designing this assignment.
</footnote>
<figure confidence="0.861559">
Spearman’s ρ 0.8
0.6
0.4
</figure>
<figureCaption confidence="0.9875555">
Figure 3: Submission history for the evaluation chal-
lenge.
</figureCaption>
<listItem confidence="0.9934335">
• Implementation of several techniques used in
AMBER (Chen and Kuhn, 2005).
</listItem>
<bodyText confidence="0.9999146875">
The best submission, obtaining a correlation of
83.5, relied on the idea that the reference and ma-
chine translation should be good paraphrases of each
other (Owczarzak et al., 2006; Kauchak and Barzi-
lay, 2006). It employed a simple paraphrase sys-
tem trained on the alignment challenge data, us-
ing the pivot technique of Bannard and Callison-
Burch (2005), and computing the optimal alignment
between machine translation and reference under a
simple model in which words could align if they
were paraphrases. When compared with the 20
systems submitted to the original task from which
the data was obtained (Callison-Burch et al., 2011),
this system would have ranked fifth, quite near the
top-scoring competitors, whose correlations ranged
from 88 to 94.
</bodyText>
<sectionHeader confidence="0.984867" genericHeader="method">
6 The Reranking Challenge
</sectionHeader>
<bodyText confidence="0.998963666666667">
The fourth challenge was reranking: given a test cor-
pus and a large N-best list of candidate translations
for each sentence, students were challenged to select
a candidate translation for each sentence to produce
a high corpus-level BLEU score. Due to an error
our data preparation, this assignment had a simple
solution that was very difficult to improve on. Nev-
ertheless, it featured several elements that may be
useful for future courses.
</bodyText>
<page confidence="0.993405">
172
</page>
<subsectionHeader confidence="0.988495">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999967538461538">
We obtained 300-best lists from a Spanish-English
translation system built with the Joshua toolkit
(Ganitkevitch et al., 2012) using data and resources
from the 2011 Workshop on Machine Translation
(Callison-Burch et al., 2011). We provided 1989
training sentences, consisting of source and refer-
ence sentences along with the candidate translations.
We also included a test set of 250 sentences, for
which we provided only the source and candidate
translations. Each candidate translation included six
features from the underlying translation system, out
of an original 21; our hope was that students might
rediscover some features through experimentation.
</bodyText>
<subsectionHeader confidence="0.997304">
6.2 Implementation
</subsectionHeader>
<bodyText confidence="0.99996055">
We conceived of the assignment as one in which stu-
dents could apply machine learning or feature engi-
neering to the task of reranking the systems, so we
provided several tools. The first of these, learn,
was a simple program that produced a vector of
feature weights using pairwise ranking optimization
(PRO; Hopkins and May, 2011), with a perceptron
as the underlying learning algorithm. A second,
rerank, takes a weight vector as input and reranks
the sentences; both programs were designed to work
with arbitrary numbers of features. The grade pro-
gram computed the BLEU score on development
data, while check ensured that a test submission
is valid. Finally, we provided an oracle program,
which computed a lower bound on the achievable
BLEU score on the development data using a greedy
approximation (Och et al., 2004). The leaderboard
likewise displayed an oracle on test data. We did
not assign a target algorithm, but left the assignment
fully open-ended.
</bodyText>
<subsectionHeader confidence="0.999758">
6.3 Reranking Challenge Outcome
</subsectionHeader>
<bodyText confidence="0.9996159375">
For each assignment, we made an effort to create
room for competition above the target algorithm.
However, we did not accomplish this in the rerank-
ing challenge: we had removed most of the features
from the candidate translations, in hopes that stu-
dents might reinvent some of them, but we left one
highly predictive implicit feature in the data: the
rank order of the underlying translation system. Stu-
dents discovered that simply returning the first can-
didate earned a very high score, and most of them
quickly converged to this solution. Unfortunately,
the high accuracy of this baseline left little room for
additional competition. Nevertheless, we were en-
couraged that most students discovered this by acci-
dent while attempting other strategies to rerank the
translations.
</bodyText>
<listItem confidence="0.9815732">
• Experimentation with parameters of the PRO
algorithm.
• Substitution of alternative learning algorithms.
• Implementation of a simplified minimum
Bayes risk reranker (Kumar and Byrne, 2004).
</listItem>
<bodyText confidence="0.9478425">
Over a baseline of 24.02, the latter approach ob-
tained a BLEU of 27.08, nearly matching the score
of 27.39 from the underlying system despite an im-
poverished feature set.
</bodyText>
<sectionHeader confidence="0.983349" genericHeader="method">
7 Pedagogical Outcomes
</sectionHeader>
<bodyText confidence="0.999759363636364">
Could our students have obtained similar results by
running standard toolkits? Undoubtedly. However,
our goal was for students to learn by doing: they
obtained these results by implementing key MT al-
gorithms, observing their behavior on real data, and
improving them. This left them with much more in-
sight into how MT systems actually work, and in
this sense, DREAMT was a success. At the end of
class, we requested written feedback on the design
of the assignments. Many commented positively on
the motivation provided by the challenge problems:
</bodyText>
<listItem confidence="0.98543675">
• The immediate feedback of the automatic grad-
ing was really nice.
• Fast feedback on my submissions and my rela-
tive position on the leaderboard kept me both
motivated to start the assignments early and to
constantly improve them. Also knowing how
well others were doing was a good way to
gauge whether I was completely off track or not
when I got bad results.
• The homework assignments were very engag-
ing thanks to the clear yet open-ended setup
and their competitive aspects.
</listItem>
<bodyText confidence="0.7014245">
Students also commented that they learned a lot
about MT and even research in general:
</bodyText>
<page confidence="0.996836">
173
</page>
<note confidence="0.457598">
Question
Feedback on my work for this course is useful
This course enhanced my ability to work effectively in a team
Compared to other courses at this level, the workload for this course is high
</note>
<table confidence="0.63885775">
1 2 3 4 5
- - - 4 9
1 - 5 8 2
- 1 7 6 1
N/A
3
-
1
</table>
<tableCaption confidence="0.998854">
Table 1: Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).
</tableCaption>
<listItem confidence="0.9888225">
• I learned the most from the assignments.
• The assignments always pushed me one step
more towards thinking out loud how the par-
ticular task can be completed.
• I appreciated the setup of the homework prob-
lems. I think it has helped me learn how to
set up and attack research questions in an or-
ganized way. I have a much better sense for
what goes into an MT system and what prob-
lems aren’t solved.
</listItem>
<bodyText confidence="0.99994125">
We also received feedback through an anonymous
survey conducted at the end of the course before
posting final grades. Each student rated aspects
of the course on a five point Likert scale, from 1
(strongly disagree) to 5 (strongly agree). Several
questions pertained to assignments (Table 1), and al-
lay two possible concerns about competition: most
students felt that the assignments enhanced their col-
laborative skills, and that their open-endedness did
not result in an overload of work. For all survey
questions, student satisfaction was higher than av-
erage for courses in our department.
</bodyText>
<sectionHeader confidence="0.996102" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999975072727273">
DREAMT is inspired by several different ap-
proaches to teaching NLP, AI, and computer sci-
ence. Eisner and Smith (2008) teach NLP using
a competitive game in which students aim to write
fragments of English grammar. Charniak et al.
(2000) improve the state-of-the-art in a reading com-
prehension task as part of a group project. Christo-
pher et al. (1993) use NACHOS, a classic tool for
teaching operating systems by providing a rudimen-
tary system that students then augment. DeNero and
Klein (2010) devise a series of assignments based
on Pac-Man, for which students implement several
classic AI techniques. A crucial element in such ap-
proaches is a highly functional but simple scaffold-
ing. The DREAMT codebase, including grading and
validation scripts, consists of only 656 lines of code
(LOC) over four assignments: 141 LOC for align-
ment, 237 LOC for decoding, 86 LOC for evalua-
tion, and 192 LOC for reranking. To simplify imple-
mentation further, the optional leaderboard could be
delegated to Kaggle.com, a company that organizes
machine learning competitions using a model sim-
ilar to the Netflix Challenge (Bennet and Lanning,
2007), and offers pro bono use of its services for
educational challenge problems. A recent machine
learning class at Oxford hosted its assignments on
Kaggle (Phil Blunsom, personal communication).
We imagine other uses of DREAMT. It could be
used in an inverted classroom, where students view
lecture material outside of class and work on prac-
tical problems in class. It might also be useful in
massive open online courses (MOOCs). In this for-
mat, course material (primarily lectures and quizzes)
is distributed over the internet to an arbitrarily large
number of interested students through sites such as
coursera.org, udacity.com, and khanacademy.org. In
many cases, material and problem sets focus on spe-
cific techniques. Although this is important, there is
also a place for open-ended problems on which stu-
dents apply a full range of problem-solving skills.
Automatic grading enables them to scale easily to
large numbers of students.
On the scientific side, the scale of MOOCs might
make it possible to empirically measure the effec-
tiveness of hands-on or competitive assignments,
by comparing course performance of students who
work on them against that of those who do not.
Though there is some empirical work on competi-
tive assignments in the computer science education
literature (Lawrence, 2004; Garlick and Akl, 2006;
Regueras et al., 2008; Ribeiro et al., 2009), they
generally measure student satisfaction and retention
rather than the more difficult question of whether
such assignments actually improve student learning.
However, it might be feasible to answer such ques-
</bodyText>
<page confidence="0.994421">
174
</page>
<bodyText confidence="0.999956391304348">
tions in large, data-rich virtual classrooms offered
by MOOCs. This is an interesting potential avenue
for future work.
Because our class came within reach of state-of-
the-art on each problem within a matter of weeks,
we wonder what might happen with a very large
body of competitors. Could real innovation oc-
cur? Could we solve large-scale problems? It may
be interesting to adopt a different incentive struc-
ture, such as one posed by Abernethy and Frongillo
(2011) for crowdsourcing machine learning prob-
lems: rather than competing, everyone works to-
gether to solve a shared task, with credit awarded
proportional to the contribution that each individual
makes. In this setting, everyone stands to gain: stu-
dents learn to solve problems as they are found in
the real world, instructors learn new insights into the
problems they pose, and, in the long run, users of
AI technology benefit from overall improvements.
Hence it is possible that posing open-ended, real-
world problems to students might be a small piece
of the puzzle of providing high-quality NLP tech-
nologies.
</bodyText>
<sectionHeader confidence="0.997474" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99999">
We are grateful to Colin Cherry and Chris Dyer
for testing the assignments in different settings and
providing valuable feedback, and to Jessie Young
for implementing a dual decomposition solution to
the decoding assignment. We thank Jason Eis-
ner, Frank Ferraro, Yoav Goldberg, Matt Gormley,
Ann Irvine, Rebecca Knowles, Ben Mitchell, Court-
ney Napoles, Michael Rushanan, Joanne Selinski,
Svitlana Volkova, and the anonymous reviewers for
lively discussion and helpful comments on previous
drafts of this paper. Any errors are our own.
</bodyText>
<sectionHeader confidence="0.99859" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999662540983607">
J. Abernethy and R. M. Frongillo. 2011. A collaborative
mechanism for crowdsourcing prediction problems. In
Proc. ofNIPS.
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. ofACL.
J. Bennet and S. Lanning. 2007. The netflix prize. In
Proc. of the KDD Cup and Workshop.
S. Bird, E. Klein, E. Loper, and J. Baldridge. 2008.
Multidisciplinary instruction with the natural language
toolkit. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
O. Bojar, M. Ercegovˇcevi´c, M. Popel, and O. Zaidan.
2011. A grain of salt for the WMT manual evaluation.
In Proc. of WMT.
P. E. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2).
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-
cut, and L. Specia. 2012. Findings of the 2012 work-
shop on statistical machine translation. In Proc. of
WMT.
Y.-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. In Proc. of EMNLP.
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett,
M. Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun,
W. Wy, Z. Yang, S. Zeiler, and L. Zorn. 2000. Read-
ing comprehension programs in a statistical-language-
processing class. In Proc. of Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
B. Chen and R. Kuhn. 2005. AMBER: A modified
BLEU, enhanced ranking metric. In Proc. of WMT.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
W. A. Christopher, S. J. Procter, and T. E. Anderson.
1993. The nachos instructional operating system. In
Proc. of USENIX.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. ofACL.
J. DeNero and D. Klein. 2010. Teaching introductory
articial intelligence with Pac-Man. In Proc. of Sym-
posium on Educational Advances in Artificial Intelli-
gence.
L. R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297–302.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. ofACL.
J. Eisner and N. A. Smith. 2008. Competitive grammar
writing. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
</reference>
<page confidence="0.984485">
175
</page>
<reference confidence="0.999842074766355">
A. Fraser and D. Marcu. 2007. Measuring word align-
ment quality for statistical machine translation. Com-
putational Linguistics, 33(3).
J. Ganitkevitch, Y. Cao, J. Weese, M. Post, and
C. Callison-Burch. 2012. Joshua 4.0: Packing, PRO,
and paraphrases. In Proc. of WMT.
R. Garlick and R. Akl. 2006. Intra-class competitive
assignments in CS2: A one-year study. In Proc. of
International Conference on Engineering Education.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding for
machine translation. In Proc. ofACL.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. of
ACL.
M. Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In Proc. of EMNLP.
D. Jurafsky and J. H. Martin. 2009. Speech and Lan-
guage Processing. Prentice Hall, 2nd edition.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. ofHLT-NAACL.
D. Klein. 2005. A core-tools statistical NLP course. In
Proc. of Workshop on Effective Tools and Methodolo-
gies for Teaching NLP and CL.
K. Knight. 1999a. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4).
K. Knight. 1999b. A statistical MT tutorial workbook.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. ofNAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. ofACL.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proc. ofAMTA.
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Koehn. 2012. Simulating human judgment in machine
translation evaluation campaigns. In Proc. ofIWSLT.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Proc.
ofHLT-NAACL.
P. Langlais, A. Patry, and F. Gotti. 2007. A greedy de-
coder for phrase-based statistical machine translation.
In Proc. of TMI.
R. Lawrence. 2004. Teaching data structures using
competitive games. IEEE Transactions on Education,
47(4).
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. ofNAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In Proc. of COLING.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3).
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proc. of EMNLP.
A. Lopez. 2008. Statistical machine translation. ACM
Computing Surveys, 40(3).
A. Lopez. 2012. Putting human assessments of machine
translation systems in order. In Proc. of WMT.
N. Madnani and B. Dorr. 2008. Combining open-source
with research to re-engineer a hands-on introductory
NLP course. In Proc. of Workshop on Issues in Teach-
ing Computational Linguistics.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2).
R. Mihalcea and T. Pedersen. 2003. An evaluation ex-
ercise for word alignment. In Proc. on Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2004. Improving IBM word alignment
model 1. In Proc. ofACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. ofACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In Proc. of
NAACL.
K. Owczarzak, D. Groves, J. V. Genabith, and A. Way.
2006. Contextual bitext-derived paraphrases in auto-
matic MT evaluation. In Proc. of WMT.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. ofACL.
L. Regueras, E. Verd´u, M. Verd´u, M. P´erez, J. de Castro,
and M. Mu˜noz. 2008. Motivating students through
on-line competition: An analysis of satisfaction and
learning styles.
P. Ribeiro, M. Ferreira, and H. Sim˜oes. 2009. Teach-
ing artificial intelligence and logic programming in a
competitive environment. Informatics in Education,
(Vol 8 1):85.
J. Spacco, D. Hovemeyer, W. Pugh, J. Hollingsworth,
N. Padua-Perez, and F. Emad. 2006. Experiences with
marmoset: Designing and using an advanced submis-
sion and testing system for programming courses. In
Proc. of Innovation and technology in computer sci-
ence education.
</reference>
<page confidence="0.985933">
176
</page>
<reference confidence="0.999252">
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. ofICSLP.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In Proc. of European Conf. on Speech
Communication and Technology.
M. Zaslavskiy, M. Dymetman, and N. Cancedda. 2009.
Phrase-based statistical machine translation as a trav-
eling salesman problem. In Proc. ofACL.
</reference>
<page confidence="0.999206">
177
178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.558380">
<title confidence="0.992811">Learning to translate with products of novices: a suite of challenge problems for teaching MT</title>
<author confidence="0.8859935">Matt Chris Jonathan Weese</author>
<author confidence="0.8859935">Juri Narges Ahmidi</author>
<author confidence="0.8859935">Olivia Buzek</author>
<author confidence="0.8859935">Leah Hanson</author>
<author confidence="0.8859935">Beenish Jamil</author>
<author confidence="0.8859935">Matthias Lee</author>
<author confidence="0.8859935">Ya-Ting Henry Pao</author>
<author confidence="0.8859935">Fatima Rivera</author>
<author confidence="0.8859935">Leili Shahriyari</author>
<author confidence="0.8859935">Debu Sinha</author>
<author confidence="0.8859935">Adam Wampler</author>
<author confidence="0.8859935">Michael Weinberger</author>
<author confidence="0.8859935">Daguang Xu</author>
<author confidence="0.8859935">Lin Yang</author>
<affiliation confidence="0.985962333333333">Department of Computer Science, Johns Hopkins Language Technology Center of Excellence, Johns Hopkins and Information Science Department, University of Pennsylvania</affiliation>
<abstract confidence="0.996935227272727">Machine translation (MT) draws from several different disciplines, making it a complex subject to teach. There are excellent pedagogical texts, but problems in MT and current algorithms for solving them are best learned by doing. As a centerpiece of our MT course, we devised a series of open-ended challenges for students in which the goal was to improve performance on carefully constrained instances of four key MT tasks: alignment, decoding, evaluation, and reranking. Students brought a diverse set of techniques to the problems, including some novel solutions which performed remarkably well. A surprising and exciting outcome was that student solutions or their combinations fared competitively on some tasks, demonstrating that even newcomers to the field can help improve the state-ofthe-art on hard NLP problems while simultaneously learning a great deal. The problems, baseline code, and results are freely available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Abernethy</author>
<author>R M Frongillo</author>
</authors>
<title>A collaborative mechanism for crowdsourcing prediction problems.</title>
<date>2011</date>
<booktitle>In Proc. ofNIPS.</booktitle>
<contexts>
<context position="38625" citStr="Abernethy and Frongillo (2011)" startWordPosition="6176" endWordPosition="6179">etention rather than the more difficult question of whether such assignments actually improve student learning. However, it might be feasible to answer such ques174 tions in large, data-rich virtual classrooms offered by MOOCs. This is an interesting potential avenue for future work. Because our class came within reach of state-ofthe-art on each problem within a matter of weeks, we wonder what might happen with a very large body of competitors. Could real innovation occur? Could we solve large-scale problems? It may be interesting to adopt a different incentive structure, such as one posed by Abernethy and Frongillo (2011) for crowdsourcing machine learning problems: rather than competing, everyone works together to solve a shared task, with credit awarded proportional to the contribution that each individual makes. In this setting, everyone stands to gain: students learn to solve problems as they are found in the real world, instructors learn new insights into the problems they pose, and, in the long run, users of AI technology benefit from overall improvements. Hence it is possible that posing open-ended, realworld problems to students might be a small piece of the puzzle of providing high-quality NLP technol</context>
</contexts>
<marker>Abernethy, Frongillo, 2011</marker>
<rawString>J. Abernethy and R. M. Frongillo. 2011. A collaborative mechanism for crowdsourcing prediction problems. In Proc. ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>C Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proc. ofACL.</booktitle>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>C. Bannard and C. Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bennet</author>
<author>S Lanning</author>
</authors>
<title>The netflix prize.</title>
<date>2007</date>
<booktitle>In Proc. of the KDD Cup and Workshop.</booktitle>
<contexts>
<context position="36574" citStr="Bennet and Lanning, 2007" startWordPosition="5851" endWordPosition="5854">es of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, including grading and validation scripts, consists of only 656 lines of code (LOC) over four assignments: 141 LOC for alignment, 237 LOC for decoding, 86 LOC for evaluation, and 192 LOC for reranking. To simplify implementation further, the optional leaderboard could be delegated to Kaggle.com, a company that organizes machine learning competitions using a model similar to the Netflix Challenge (Bennet and Lanning, 2007), and offers pro bono use of its services for educational challenge problems. A recent machine learning class at Oxford hosted its assignments on Kaggle (Phil Blunsom, personal communication). We imagine other uses of DREAMT. It could be used in an inverted classroom, where students view lecture material outside of class and work on practical problems in class. It might also be useful in massive open online courses (MOOCs). In this format, course material (primarily lectures and quizzes) is distributed over the internet to an arbitrarily large number of interested students through sites such a</context>
</contexts>
<marker>Bennet, Lanning, 2007</marker>
<rawString>J. Bennet and S. Lanning. 2007. The netflix prize. In Proc. of the KDD Cup and Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bird</author>
<author>E Klein</author>
<author>E Loper</author>
<author>J Baldridge</author>
</authors>
<title>Multidisciplinary instruction with the natural language toolkit.</title>
<date>2008</date>
<booktitle>In Proc. of Workshop on Issues in Teaching Computational Linguistics.</booktitle>
<contexts>
<context position="6281" citStr="Bird et al. (2008)" startWordPosition="973" endWordPosition="976">n, and supervised learning, respectively. In real MT systems, these problems are highly interdependent, a point we emphasized in class and at the end of each assignment—for example, that alignment is an exercise in parameter estimation for translation models, that model choice is a tradeoff between expressivity and efficient inference, and that optimal search does not guarantee optimal accuracy. However, presenting each problem independently and holding all else constant enables more focused exploration. For each problem we provided data, a naive solution, and an evaluation program. Following Bird et al. (2008) and Madnani and Dorr (2008), we implemented the challenges in Python, a high-level pro1http://alopez.github.io/dreamt gramming language that can be used to write very concise programs resembling pseudocode.2,3 By default, each baseline system reads the test data and generates output in the evaluation format, so setup required zero configuration, and students could begin experimenting immediately. For example, on receipt of the alignment code, aligning data and evaluating results required only typing: &gt; align |grade Students could then run experiments within minutes of beginning the assignment</context>
</contexts>
<marker>Bird, Klein, Loper, Baldridge, 2008</marker>
<rawString>S. Bird, E. Klein, E. Loper, and J. Baldridge. 2008. Multidisciplinary instruction with the natural language toolkit. In Proc. of Workshop on Issues in Teaching Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>M Ercegovˇcevi´c</author>
<author>M Popel</author>
<author>O Zaidan</author>
</authors>
<title>A grain of salt for the WMT manual evaluation.</title>
<date>2011</date>
<booktitle>In Proc. of WMT.</booktitle>
<marker>Bojar, Ercegovˇcevi´c, Popel, Zaidan, 2011</marker>
<rawString>O. Bojar, M. Ercegovˇcevi´c, M. Popel, and O. Zaidan. 2011. A grain of salt for the WMT manual evaluation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P E Brown</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="14668" citStr="Brown et al., 1993" startWordPosition="2320" endWordPosition="2323"> and automatic alignments. Finally the check program verifies that the results represent a valid solution, reporting an error if not—enabling students to diagnose bugs in their submissions. The default implementation enabled immediate experimentation. On receipt of the code, students were instructed to align the first 1,000 sentences and compute AER using a simple command. &gt; align -n 1000 |grade By varying the number of input sentences and the threshold for an alignment, students could immediately see the effect of various parameters on alignment quality. We privately implemented IBM Model 1 (Brown et al., 1993) as the target algorithm for a passing grade. We ran it for five iterations with English as the target language and French as the source. Our implementation did not use null alignment or symmetrization—leaving out these common improvements offered students the possibility of discovering them independently, and thereby rewarded. Figure 1: Submission history for the alignment challenge. Dashed lines represent the default and baseline system performance. Each colored line represents a student, and each dot represents a submission. For clarity, we show only submissions that improved the student’s </context>
<context position="16390" citStr="Brown et al., 1993" startWordPosition="2597" endWordPosition="2600">th parameters estimated on the observed alignments in the development data. • Running Model 1 for many iterations. Most researchers typically run Model 1 for five iterations or fewer, and there are few experiments in the literature on its behavior over many iterations, as there are for hidden Markov model taggers (Johnson, 2007). Our students carried out these experiments, reporting runs of 5, 20, 100, and even 2000 iterations. No improvement was observed after 20 iterations. 20 50 60 AER × 100 30 40 168 • Implementing various alternative approaches from the literature, including IBM Model 2 (Brown et al., 1993), competitive linking (Melamed, 2000), and smoothing (Moore, 2004). One of the best solutions was competitive linking with Dice’s coefficient, modified to incorporate the observation that alignments tend to be monotonic by restricting possible alignment points to a window of eight words around the diagonal. Although simple, it acheived an AER of 18.41, an error reduction over Model 1 of more than 40%. The best score compares unfavorably against a state-of-the-art AER of 3.6 (Liu et al., 2010). But under a different view, it still represents a significant amount of progress for an effort taking</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. E. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<title>Findings of the 2009 workshop on statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of WMT.</booktitle>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder. 2009. Findings of the 2009 workshop on statistical machine translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>O Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="26289" citStr="Callison-Burch et al., 2011" startWordPosition="4174" endWordPosition="4177">.predecessor), h.phrase.english) print extract_english(winner) Algorithm 1 Basic stack decoding algorithm, -1200 adapted from Koehn (2010), p. 165. place empty hypothesis into stack 0 log10 p(e|f) − C -1250 for all stacks 0...n − 1 do -1300 for all hypotheses in stack do -1350 for all translation options do -1400 if applicable then create new hypothesis place in stack recombine with existing hypothesis prune stack if too big 5.1 Data We chose the English-to-German translation systems from the 2009 and 2011 shared task at the annual Workshop for Machine Translation (CallisonBurch et al., 2009; Callison-Burch et al., 2011), providing the first as development data and the second as test data. We chose these sets because BLEU (Papineni et al., 2002), our baseline metric, performed particularly poorly on them; this left room for improvement in addition to highlighting some Figure 2: Submission history for the decoding challenge. The dotted green line represents the oracle over submissions. deficiencies of BLEU. For each dataset we provided the source and reference sentences along with anonymized system outputs. For the development data we also provided the human ranking of the sys171 tems, computed from pairwise h</context>
<context position="27911" citStr="Callison-Burch et al. (2011)" startWordPosition="4426" endWordPosition="4429">between the human ranking and an output ranking. The check program simply ensures that a submission contains a valid ranking. We were concerned about hill-climbing on the test data, so we modified the leaderboard to report new results only twice a day. This encouraged students to experiment on the development data before posting new submissions, while still providing intermittent feedback. We privately implemented a version of BLEU, which obtained a correlation of 38.6 with the human rankings, a modest improvement over the baseline of 34.0. Our implementation underperforms the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 5.3 Evaluation Challenge Results We received 212 submissions from 12 teams (Figure 3), again demonstrating a wide range of techniques. • Experimentation with the maximum n-gram length and weights in BLEU. • Implementation of smoothed versions of BLEU (Lin and Och, 2004). • Implementation of weighted F-measure to balance both precision and recall. • Careful normalization of the reference and machine translations, including lowercasing and punctuation-stripping. 12This ranking has been disputed </context>
<context position="29523" citStr="Callison-Burch et al., 2011" startWordPosition="4682" endWordPosition="4685">est submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of Bannard and CallisonBurch (2005), and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases. When compared with the 20 systems submitted to the original task from which the data was obtained (Callison-Burch et al., 2011), this system would have ranked fifth, quite near the top-scoring competitors, whose correlations ranged from 88 to 94. 6 The Reranking Challenge The fourth challenge was reranking: given a test corpus and a large N-best list of candidate translations for each sentence, students were challenged to select a candidate translation for each sentence to produce a high corpus-level BLEU score. Due to an error our data preparation, this assignment had a simple solution that was very difficult to improve on. Nevertheless, it featured several elements that may be useful for future courses. 172 6.1 Data</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="28576" citStr="Callison-Burch et al., 2012" startWordPosition="4529" endWordPosition="4532">normalization of the data. This also left room for improvement. 5.3 Evaluation Challenge Results We received 212 submissions from 12 teams (Figure 3), again demonstrating a wide range of techniques. • Experimentation with the maximum n-gram length and weights in BLEU. • Implementation of smoothed versions of BLEU (Lin and Och, 2004). • Implementation of weighted F-measure to balance both precision and recall. • Careful normalization of the reference and machine translations, including lowercasing and punctuation-stripping. 12This ranking has been disputed over a series of papers (Lopez, 2012; Callison-Burch et al., 2012; Koehn, 2012). The paper which initiated the dispute, written by the first author, was directly inspired by the experience of designing this assignment. Spearman’s ρ 0.8 0.6 0.4 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge d</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-W Chang</author>
<author>M Collins</author>
</authors>
<title>Exact decoding of phrase-based translation models through Lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="22432" citStr="Chang and Collins (2011)" startWordPosition="3585" endWordPosition="3588">ced here as Algorithm 1). The second program, grade, computes the log-probability of a set of translations, as outline above. We privately implemented a simple stack decoder that searched over permutations of phrases, similar to Koehn (2004). Our implementation increased the codebase by 44 lines of code and included parameters for beam size, distortion limit, and the maximum number of translations considered for each input phrase. We posted a baseline to the leaderboard using values of 50, 3, and 20 for these, respectively. 11 We implemented a version of the Lagrangian relaxation algorithm of Chang and Collins (2011), but found it difficult to obtain tight (optimal) solutions without iteratively reintroducing all of the original constraints. We suspect this is due to the lack of a distortion penalty, which enforces a strong preference towards translations with little reordering. However, the solution found by this algorithm is only approximates the objective implied by Equation 2, which sums over alignments. We also posted an oracle containing the most probable output for each sentence, selected from among all submissions received so far. The intent of this oracle was to provide a lower bound on the best </context>
</contexts>
<marker>Chang, Collins, 2011</marker>
<rawString>Y.-W. Chang and M. Collins. 2011. Exact decoding of phrase-based translation models through Lagrangian relaxation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>Y Altun</author>
<author>R de Salvo Braz</author>
<author>B Garrett</author>
<author>M Kosmala</author>
<author>T Moscovich</author>
<author>L Pang</author>
<author>C Pyo</author>
<author>Y Sun</author>
<author>W Wy</author>
<author>Z Yang</author>
<author>S Zeiler</author>
<author>L Zorn</author>
</authors>
<title>Reading comprehension programs in a statistical-languageprocessing class.</title>
<date>2000</date>
<booktitle>In Proc. of Workshop on Reading Comprehension Tests</booktitle>
<contexts>
<context position="35676" citStr="Charniak et al. (2000)" startWordPosition="5706" endWordPosition="5709">5 (strongly agree). Several questions pertained to assignments (Table 1), and allay two possible concerns about competition: most students felt that the assignments enhanced their collaborative skills, and that their open-endedness did not result in an overload of work. For all survey questions, student satisfaction was higher than average for courses in our department. 8 Discussion DREAMT is inspired by several different approaches to teaching NLP, AI, and computer science. Eisner and Smith (2008) teach NLP using a competitive game in which students aim to write fragments of English grammar. Charniak et al. (2000) improve the state-of-the-art in a reading comprehension task as part of a group project. Christopher et al. (1993) use NACHOS, a classic tool for teaching operating systems by providing a rudimentary system that students then augment. DeNero and Klein (2010) devise a series of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, including grading and validation scripts, consists of only 656 lines of code (LOC) over four assignments: 141 LOC for alignmen</context>
</contexts>
<marker>Charniak, Altun, Braz, Garrett, Kosmala, Moscovich, Pang, Pyo, Sun, Wy, Yang, Zeiler, Zorn, 2000</marker>
<rawString>E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett, M. Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun, W. Wy, Z. Yang, S. Zeiler, and L. Zorn. 2000. Reading comprehension programs in a statistical-languageprocessing class. In Proc. of Workshop on Reading Comprehension Tests as Evaluation for ComputerBased Language Understanding Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Chen</author>
<author>R Kuhn</author>
</authors>
<title>AMBER: A modified BLEU, enhanced ranking metric.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<booktitle>In Proc. of</booktitle>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="28888" citStr="Chen and Kuhn, 2005" startWordPosition="4579" endWordPosition="4582"> and Och, 2004). • Implementation of weighted F-measure to balance both precision and recall. • Careful normalization of the reference and machine translations, including lowercasing and punctuation-stripping. 12This ranking has been disputed over a series of papers (Lopez, 2012; Callison-Burch et al., 2012; Koehn, 2012). The paper which initiated the dispute, written by the first author, was directly inspired by the experience of designing this assignment. Spearman’s ρ 0.8 0.6 0.4 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of Bannard and CallisonBurch (2005), and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases. When compared with the 20 systems submitted to the original task from which the data was obt</context>
</contexts>
<marker>Chen, Kuhn, 2005</marker>
<rawString>B. Chen and R. Kuhn. 2005. AMBER: A modified BLEU, enhanced ranking metric. In Proc. of WMT. D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Christopher</author>
<author>S J Procter</author>
<author>T E Anderson</author>
</authors>
<title>The nachos instructional operating system.</title>
<date>1993</date>
<booktitle>In Proc. of USENIX.</booktitle>
<contexts>
<context position="35791" citStr="Christopher et al. (1993)" startWordPosition="5725" endWordPosition="5729">competition: most students felt that the assignments enhanced their collaborative skills, and that their open-endedness did not result in an overload of work. For all survey questions, student satisfaction was higher than average for courses in our department. 8 Discussion DREAMT is inspired by several different approaches to teaching NLP, AI, and computer science. Eisner and Smith (2008) teach NLP using a competitive game in which students aim to write fragments of English grammar. Charniak et al. (2000) improve the state-of-the-art in a reading comprehension task as part of a group project. Christopher et al. (1993) use NACHOS, a classic tool for teaching operating systems by providing a rudimentary system that students then augment. DeNero and Klein (2010) devise a series of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, including grading and validation scripts, consists of only 656 lines of code (LOC) over four assignments: 141 LOC for alignment, 237 LOC for decoding, 86 LOC for evaluation, and 192 LOC for reranking. To simplify implementation further, the </context>
</contexts>
<marker>Christopher, Procter, Anderson, 1993</marker>
<rawString>W. A. Christopher, S. J. Procter, and T. E. Anderson. 1993. The nachos instructional operating system. In Proc. of USENIX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="19169" citStr="DeNero and Klein, 2008" startWordPosition="3063" endWordPosition="3066">ish sentence e of length J, and alignment a where each element consists of a span in both e and f such that every word in both e and f is aligned exactly once, the conditional probability of e and a given f is as follows.10 � p(e,a|f) = (i,i,,j,j,)∈a (1) To evaluate output, we compute the conditional probability of e as follows. � p(e|f) = p(e, a|f) (2) a Note that this formulation is different from the typical Viterbi objective of standard beam search decoders, which do not sum over all alignments, but approximate p(e|f) by maxa p(e, a|f). Though the computation in Equation 2 is intractable (DeNero and Klein, 2008), it can be computed in a few minutes via dynamic programming on reasonably short sentences. We ensured that our data met this criterion. The corpus-level probability is then the product of all sentence-level probabilities in the data. The model includes no distortion limit or distortion model, for two reasons. First, leaving out the distortion model slightly simplifies the implementation, since it is not necessary to keep track of the last word translated in a beam decoder; we felt that this detail was secondary to understanding the difficulty of search over phrase permutations. Second, it ac</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>J. DeNero and D. Klein. 2008. The complexity of phrase alignment problems. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Teaching introductory articial intelligence with Pac-Man.</title>
<date>2010</date>
<booktitle>In Proc. of Symposium on Educational Advances in Artificial Intelligence.</booktitle>
<contexts>
<context position="35935" citStr="DeNero and Klein (2010)" startWordPosition="5749" endWordPosition="5752">oad of work. For all survey questions, student satisfaction was higher than average for courses in our department. 8 Discussion DREAMT is inspired by several different approaches to teaching NLP, AI, and computer science. Eisner and Smith (2008) teach NLP using a competitive game in which students aim to write fragments of English grammar. Charniak et al. (2000) improve the state-of-the-art in a reading comprehension task as part of a group project. Christopher et al. (1993) use NACHOS, a classic tool for teaching operating systems by providing a rudimentary system that students then augment. DeNero and Klein (2010) devise a series of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, including grading and validation scripts, consists of only 656 lines of code (LOC) over four assignments: 141 LOC for alignment, 237 LOC for decoding, 86 LOC for evaluation, and 192 LOC for reranking. To simplify implementation further, the optional leaderboard could be delegated to Kaggle.com, a company that organizes machine learning competitions using a model similar to the Netfl</context>
</contexts>
<marker>DeNero, Klein, 2010</marker>
<rawString>J. DeNero and D. Klein. 2010. Teaching introductory articial intelligence with Pac-Man. In Proc. of Symposium on Educational Advances in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Dice</author>
</authors>
<title>Measures of the amount of ecologic association between species.</title>
<date>1945</date>
<journal>Ecology,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Dice, 1945</marker>
<rawString>L. R. Dice. 1945. Measures of the amount of ecologic association between species. Ecology, 26(3):297–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>A Lopez</author>
<author>J Ganitkevitch</author>
<author>J Weese</author>
<author>F Ture</author>
<author>P Blunsom</author>
<author>H Setiawan</author>
<author>V Eidelman</author>
<author>P Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="3095" citStr="Dyer et al., 2010" startWordPosition="483" endWordPosition="486">now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation, and reranking. Each imple</context>
<context position="21134" citStr="Dyer et al., 2010" startWordPosition="3383" endWordPosition="3386">nts had enough information to compute the evaluation score on any dataset themselves. The difficulty of the challenge lies simply in finding the translation that maximizes the evaluation. Indeed, since the problem is intractable, even the instructors did not know the true solution.11 4.1 Data We chose 48 French sentences totaling 716 words from the Canadian Hansards to serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). 4.2 Implementation We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the language model and translation model, but without permuting phrases. The implementation is completely self-contained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding algorithm (</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>N A Smith</author>
</authors>
<title>Competitive grammar writing.</title>
<date>2008</date>
<booktitle>In Proc. of Workshop on Issues in Teaching Computational Linguistics.</booktitle>
<contexts>
<context position="35557" citStr="Eisner and Smith (2008)" startWordPosition="5686" endWordPosition="5689">ting final grades. Each student rated aspects of the course on a five point Likert scale, from 1 (strongly disagree) to 5 (strongly agree). Several questions pertained to assignments (Table 1), and allay two possible concerns about competition: most students felt that the assignments enhanced their collaborative skills, and that their open-endedness did not result in an overload of work. For all survey questions, student satisfaction was higher than average for courses in our department. 8 Discussion DREAMT is inspired by several different approaches to teaching NLP, AI, and computer science. Eisner and Smith (2008) teach NLP using a competitive game in which students aim to write fragments of English grammar. Charniak et al. (2000) improve the state-of-the-art in a reading comprehension task as part of a group project. Christopher et al. (1993) use NACHOS, a classic tool for teaching operating systems by providing a rudimentary system that students then augment. DeNero and Klein (2010) devise a series of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, includi</context>
</contexts>
<marker>Eisner, Smith, 2008</marker>
<rawString>J. Eisner and N. A. Smith. 2008. Competitive grammar writing. In Proc. of Workshop on Issues in Teaching Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="17937" citStr="Fraser and Marcu, 2007" startWordPosition="2852" endWordPosition="2855">prediction as a feature, we obtained an AER of 15.4, which would have placed fourth on the original challenge. This is notable since none of the systems incorporated first-order dependencies on the alignments of adjacent words, long noted as an important feature of the best alignment models (Och and Ney, 2003). Yet a simple system combination of student assignments is as effective as a hidden Markov Model trained on a comparable amount of data (Och and Ney, 2003). It is important to note that AER does not necessarily correlate with downstream performance, particularly on the Hansards dataset (Fraser and Marcu, 2007). We used the conclusion of the assignment as an opportunity to emphasize this point. 4 The Decoding Challenge The second challenge was decoding: given a fixed translation model and a set of input sentences, students were challenged to produce translations with the highest model score. This challenge introduced the difficulties of combinatorial optimization under a deceptively simple setup: the model we provided was a simple phrase-based translation model (Koehn et al., 2003) consisting only of a phrase table and trigram language model. Under this simple model, for a French sentence f of lengt</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>A. Fraser and D. Marcu. 2007. Measuring word alignment quality for statistical machine translation. Computational Linguistics, 33(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ganitkevitch</author>
<author>Y Cao</author>
<author>J Weese</author>
<author>M Post</author>
<author>C Callison-Burch</author>
</authors>
<title>Joshua 4.0: Packing, PRO, and paraphrases.</title>
<date>2012</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="3134" citStr="Ganitkevitch et al., 2012" startWordPosition="489" endWordPosition="492">re excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation, and reranking. Each implementation consists of a naive baseline </context>
<context position="30250" citStr="Ganitkevitch et al., 2012" startWordPosition="4797" endWordPosition="4800">ged from 88 to 94. 6 The Reranking Challenge The fourth challenge was reranking: given a test corpus and a large N-best list of candidate translations for each sentence, students were challenged to select a candidate translation for each sentence to produce a high corpus-level BLEU score. Due to an error our data preparation, this assignment had a simple solution that was very difficult to improve on. Nevertheless, it featured several elements that may be useful for future courses. 172 6.1 Data We obtained 300-best lists from a Spanish-English translation system built with the Joshua toolkit (Ganitkevitch et al., 2012) using data and resources from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). We provided 1989 training sentences, consisting of source and reference sentences along with the candidate translations. We also included a test set of 250 sentences, for which we provided only the source and candidate translations. Each candidate translation included six features from the underlying translation system, out of an original 21; our hope was that students might rediscover some features through experimentation. 6.2 Implementation We conceived of the assignment as one in which stu</context>
</contexts>
<marker>Ganitkevitch, Cao, Weese, Post, Callison-Burch, 2012</marker>
<rawString>J. Ganitkevitch, Y. Cao, J. Weese, M. Post, and C. Callison-Burch. 2012. Joshua 4.0: Packing, PRO, and paraphrases. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garlick</author>
<author>R Akl</author>
</authors>
<title>Intra-class competitive assignments in CS2: A one-year study.</title>
<date>2006</date>
<booktitle>In Proc. of International Conference on Engineering Education.</booktitle>
<contexts>
<context position="37898" citStr="Garlick and Akl, 2006" startWordPosition="6060" endWordPosition="6063">cific techniques. Although this is important, there is also a place for open-ended problems on which students apply a full range of problem-solving skills. Automatic grading enables them to scale easily to large numbers of students. On the scientific side, the scale of MOOCs might make it possible to empirically measure the effectiveness of hands-on or competitive assignments, by comparing course performance of students who work on them against that of those who do not. Though there is some empirical work on competitive assignments in the computer science education literature (Lawrence, 2004; Garlick and Akl, 2006; Regueras et al., 2008; Ribeiro et al., 2009), they generally measure student satisfaction and retention rather than the more difficult question of whether such assignments actually improve student learning. However, it might be feasible to answer such ques174 tions in large, data-rich virtual classrooms offered by MOOCs. This is an interesting potential avenue for future work. Because our class came within reach of state-ofthe-art on each problem within a matter of weeks, we wonder what might happen with a very large body of competitors. Could real innovation occur? Could we solve large-scal</context>
</contexts>
<marker>Garlick, Akl, 2006</marker>
<rawString>R. Garlick and R. Akl. 2006. Intra-class competitive assignments in CS2: A one-year study. In Proc. of International Conference on Engineering Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Germann</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>K Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation. In</title>
<date>2001</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="23411" citStr="Germann et al., 2001" startWordPosition="3740" endWordPosition="3743"> Equation 2, which sums over alignments. We also posted an oracle containing the most probable output for each sentence, selected from among all submissions received so far. The intent of this oracle was to provide a lower bound on the best possible output, giving students additional incentive to continue improving their systems. 4.3 Challenge Results We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). • Inclusion of heuristic estimates of future cost. • Implementation of a private oracle. Some students observed that the ideal beam setting was not uniform across the corpus. They ran their decoder under different settings, and then selected the most probable translation of each sentence. Many teams who implemented the standard stack decoding algorithm experimented heavily with its pruning parameters. The best submission used extremely wide beam settings in conjunction with a reimplementation of the future cost estimate used in Moses (Koehn et al., 2007). Five of the </context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="8581" citStr="Huang and Chiang, 2007" startWordPosition="1327" endWordPosition="1330">nd thus competition. We told students the exact algorithm that produced the target accuracy (though we expected them to derive it themselves based on lectures, notes, or literature). We did not specifically require them to implement it, but the guarantee of a passing grade provided a powerful incentive for this to be the first step of each assignment. Submissions that beat this target received additional credit. The top five submissions received full credit, while the top three received extra credit. 2http://python.org 3Some well-known MT systems have been implemented in Python (Chiang, 2007; Huang and Chiang, 2007). 4Thanks to an anonymous reviewer for this turn of phrase. 166 This scheme provided strong incentive to continue experimentation beyond the target algorith5 m. For each assignment, students could form teams of any size, under three rules: each team had to publicize its formation to the class, all team members agreed to receive the same grade, and teams could not drop members. Our hope was that these requirements would balance the perceived competitive advantage of collaboration against a reluctance to take (and thus support) teammates who did not contribute to the competitive effort.6 This st</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>L. Huang and D. Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="16101" citStr="Johnson, 2007" startWordPosition="2550" endWordPosition="2551"> saw many other solutions, indicating that many truly experimented with the problem: • Implementing heuristic constraints to require alignment of proper names and punctuation. • Running the algorithm on stems rather than surface words. • Initializing the first iteration of Model 1 with parameters estimated on the observed alignments in the development data. • Running Model 1 for many iterations. Most researchers typically run Model 1 for five iterations or fewer, and there are few experiments in the literature on its behavior over many iterations, as there are for hidden Markov model taggers (Johnson, 2007). Our students carried out these experiments, reporting runs of 5, 20, 100, and even 2000 iterations. No improvement was observed after 20 iterations. 20 50 60 AER × 100 30 40 168 • Implementing various alternative approaches from the literature, including IBM Model 2 (Brown et al., 1993), competitive linking (Melamed, 2000), and smoothing (Moore, 2004). One of the best solutions was competitive linking with Dice’s coefficient, modified to incorporate the observation that alignments tend to be monotonic by restricting possible alignment points to a window of eight words around the diagonal. Al</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>M. Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2009</date>
<publisher>Prentice Hall,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="2717" citStr="Jurafsky and Martin, 2009" startWordPosition="413" endWordPosition="416">ibed here. This research was conducted while Chris Callison-Burch was at Johns Hopkins University. in a class on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. M</context>
</contexts>
<marker>Jurafsky, Martin, 2009</marker>
<rawString>D. Jurafsky and J. H. Martin. 2009. Speech and Language Processing. Prentice Hall, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kauchak</author>
<author>R Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proc. ofHLT-NAACL.</booktitle>
<contexts>
<context position="29099" citStr="Kauchak and Barzilay, 2006" startWordPosition="4613" endWordPosition="4617">ping. 12This ranking has been disputed over a series of papers (Lopez, 2012; Callison-Burch et al., 2012; Koehn, 2012). The paper which initiated the dispute, written by the first author, was directly inspired by the experience of designing this assignment. Spearman’s ρ 0.8 0.6 0.4 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of Bannard and CallisonBurch (2005), and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases. When compared with the 20 systems submitted to the original task from which the data was obtained (Callison-Burch et al., 2011), this system would have ranked fifth, quite near the top-scoring competitors, whose correlations ranged from 88 to 94. 6 The Reranking Challenge The fourth challenge was reran</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>D. Kauchak and R. Barzilay. 2006. Paraphrasing for automatic evaluation. In Proc. ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
</authors>
<title>A core-tools statistical NLP course.</title>
<date>2005</date>
<booktitle>In Proc. of Workshop on Effective Tools and Methodologies for Teaching NLP and CL.</booktitle>
<contexts>
<context position="11533" citStr="Klein (2005)" startWordPosition="1817" endWordPosition="1818"> criteria. Everyone who completed all four assignments placed in the top five at least once. 6The equilibrium point is a single team, though this team would still need to decide on a division of labor. One student contemplated organizing this team, but decided against it. Some did so after the assignment deadline. 3 The Alignment Challenge The first challenge was word alignment: given a parallel text, students were challenged to produce wordto-word alignments with low alignment error rate (AER; Och and Ney, 2000). This is a variant of a classic assignment not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data</context>
</contexts>
<marker>Klein, 2005</marker>
<rawString>D. Klein. 2005. A core-tools statistical NLP course. In Proc. of Workshop on Effective Tools and Methodologies for Teaching NLP and CL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="2808" citStr="Knight, 1999" startWordPosition="430" endWordPosition="431">lass on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key te</context>
<context position="20063" citStr="Knight, 1999" startWordPosition="3207" endWordPosition="3208">ortion model, for two reasons. First, leaving out the distortion model slightly simplifies the implementation, since it is not necessary to keep track of the last word translated in a beam decoder; we felt that this detail was secondary to understanding the difficulty of search over phrase permutations. Second, it actually makes the problem more difficult, since a simple distance-based distortion model prefers translations with fewer permutations; without it, the model may easily prefer any permutation of the target phrases, making even the Viterbi search problem exhibit its true NP-hardness (Knight, 1999a; Zaslavskiy et al., 2009). Since the goal was to find the translation with the highest probability, we did not provide a held-out test set; with access to both the input sentences and 10For simplicity, this formula assumes that a is padded with two sentence-initial symbols and one sentence-final symbol, and ignores the probability of sentence segmentation, which we take to be uniform. p(fi, J+1H p(ej|ej−1, ej−2) i |ej, j=1 j ) 169 the model, students had enough information to compute the evaluation score on any dataset themselves. The difficulty of the challenge lies simply in finding the tr</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>K. Knight. 1999a. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<date>1999</date>
<note>A statistical MT tutorial workbook.</note>
<contexts>
<context position="2808" citStr="Knight, 1999" startWordPosition="430" endWordPosition="431">lass on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key te</context>
<context position="20063" citStr="Knight, 1999" startWordPosition="3207" endWordPosition="3208">ortion model, for two reasons. First, leaving out the distortion model slightly simplifies the implementation, since it is not necessary to keep track of the last word translated in a beam decoder; we felt that this detail was secondary to understanding the difficulty of search over phrase permutations. Second, it actually makes the problem more difficult, since a simple distance-based distortion model prefers translations with fewer permutations; without it, the model may easily prefer any permutation of the target phrases, making even the Viterbi search problem exhibit its true NP-hardness (Knight, 1999a; Zaslavskiy et al., 2009). Since the goal was to find the translation with the highest probability, we did not provide a held-out test set; with access to both the input sentences and 10For simplicity, this formula assumes that a is padded with two sentence-initial symbols and one sentence-final symbol, and ignores the probability of sentence segmentation, which we take to be uniform. p(fi, J+1H p(ej|ej−1, ej−2) i |ej, j=1 j ) 169 the model, students had enough information to compute the evaluation score on any dataset themselves. The difficulty of the challenge lies simply in finding the tr</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>K. Knight. 1999b. A statistical MT tutorial workbook.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. ofNAACL.</booktitle>
<contexts>
<context position="18417" citStr="Koehn et al., 2003" startWordPosition="2925" endWordPosition="2928"> to note that AER does not necessarily correlate with downstream performance, particularly on the Hansards dataset (Fraser and Marcu, 2007). We used the conclusion of the assignment as an opportunity to emphasize this point. 4 The Decoding Challenge The second challenge was decoding: given a fixed translation model and a set of input sentences, students were challenged to produce translations with the highest model score. This challenge introduced the difficulties of combinatorial optimization under a deceptively simple setup: the model we provided was a simple phrase-based translation model (Koehn et al., 2003) consisting only of a phrase table and trigram language model. Under this simple model, for a French sentence f of length I, English sentence e of length J, and alignment a where each element consists of a span in both e and f such that every word in both e and f is aligned exactly once, the conditional probability of e and a given f is as follows.10 � p(e,a|f) = (i,i,,j,j,)∈a (1) To evaluate output, we compute the conditional probability of e as follows. � p(e|f) = p(e, a|f) (2) a Note that this formulation is different from the typical Viterbi objective of standard beam search decoders, whic</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. ofNAACL. P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. ofAMTA.</booktitle>
<contexts>
<context position="22049" citStr="Koehn (2004)" startWordPosition="3522" endWordPosition="3523">ntained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding algorithm (Listing 2) with the pseudocode in Koehn’s (2010) popular textbook (reproduced here as Algorithm 1). The second program, grade, computes the log-probability of a set of translations, as outline above. We privately implemented a simple stack decoder that searched over permutations of phrases, similar to Koehn (2004). Our implementation increased the codebase by 44 lines of code and included parameters for beam size, distortion limit, and the maximum number of translations considered for each input phrase. We posted a baseline to the leaderboard using values of 50, 3, and 20 for these, respectively. 11 We implemented a version of the Lagrangian relaxation algorithm of Chang and Collins (2011), but found it difficult to obtain tight (optimal) solutions without iteratively reintroducing all of the original constraints. We suspect this is due to the lack of a distortion penalty, which enforces a strong prefe</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proc. ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2652" citStr="Koehn, 2010" startWordPosition="404" endWordPosition="405">remaining authors were students in the worked described here. This research was conducted while Chris Callison-Burch was at Johns Hopkins University. in a class on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source cod</context>
<context position="25799" citStr="Koehn (2010)" startWordPosition="4091" endWordPosition="4092">hrase.english.split(): (lm_state, word_logprob) = lm.score(lm_state, word) logprob += word_logprob logprob += lm.end(lm_state) if j == len(f) else 0.0 new_hypothesis = hypothesis(logprob, lm_state, h, phrase) if lm_state not in stacks[j] or \ stacks[j][lm_state].logprob &lt; logprob: stacks[j][lm_state] = new_hypothesis winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob) def extract_english(h): return &amp;quot;&amp;quot; if h.predecessor is None else &amp;quot;%s%s &amp;quot; % (extract_english(h.predecessor), h.phrase.english) print extract_english(winner) Algorithm 1 Basic stack decoding algorithm, -1200 adapted from Koehn (2010), p. 165. place empty hypothesis into stack 0 log10 p(e|f) − C -1250 for all stacks 0...n − 1 do -1300 for all hypotheses in stack do -1350 for all translation options do -1400 if applicable then create new hypothesis place in stack recombine with existing hypothesis prune stack if too big 5.1 Data We chose the English-to-German translation systems from the 2009 and 2011 shared task at the annual Workshop for Machine Translation (CallisonBurch et al., 2009; Callison-Burch et al., 2011), providing the first as development data and the second as test data. We chose these sets because BLEU (Papin</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>P. Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Simulating human judgment in machine translation evaluation campaigns.</title>
<date>2012</date>
<booktitle>In Proc. ofIWSLT.</booktitle>
<contexts>
<context position="28590" citStr="Koehn, 2012" startWordPosition="4533" endWordPosition="4534">is also left room for improvement. 5.3 Evaluation Challenge Results We received 212 submissions from 12 teams (Figure 3), again demonstrating a wide range of techniques. • Experimentation with the maximum n-gram length and weights in BLEU. • Implementation of smoothed versions of BLEU (Lin and Och, 2004). • Implementation of weighted F-measure to balance both precision and recall. • Careful normalization of the reference and machine translations, including lowercasing and punctuation-stripping. 12This ranking has been disputed over a series of papers (Lopez, 2012; Callison-Burch et al., 2012; Koehn, 2012). The paper which initiated the dispute, written by the first author, was directly inspired by the experience of designing this assignment. Spearman’s ρ 0.8 0.6 0.4 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the</context>
</contexts>
<marker>Koehn, 2012</marker>
<rawString>P. Koehn. 2012. Simulating human judgment in machine translation evaluation campaigns. In Proc. ofIWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. ofHLT-NAACL.</booktitle>
<contexts>
<context position="32763" citStr="Kumar and Byrne, 2004" startWordPosition="5191" endWordPosition="5194">the rank order of the underlying translation system. Students discovered that simply returning the first candidate earned a very high score, and most of them quickly converged to this solution. Unfortunately, the high accuracy of this baseline left little room for additional competition. Nevertheless, we were encouraged that most students discovered this by accident while attempting other strategies to rerank the translations. • Experimentation with parameters of the PRO algorithm. • Substitution of alternative learning algorithms. • Implementation of a simplified minimum Bayes risk reranker (Kumar and Byrne, 2004). Over a baseline of 24.02, the latter approach obtained a BLEU of 27.08, nearly matching the score of 27.39 from the underlying system despite an impoverished feature set. 7 Pedagogical Outcomes Could our students have obtained similar results by running standard toolkits? Undoubtedly. However, our goal was for students to learn by doing: they obtained these results by implementing key MT algorithms, observing their behavior on real data, and improving them. This left them with much more insight into how MT systems actually work, and in this sense, DREAMT was a success. At the end of class, w</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Proc. ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langlais</author>
<author>A Patry</author>
<author>F Gotti</author>
</authors>
<title>A greedy decoder for phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of TMI.</booktitle>
<contexts>
<context position="23435" citStr="Langlais et al., 2007" startWordPosition="3744" endWordPosition="3747">s over alignments. We also posted an oracle containing the most probable output for each sentence, selected from among all submissions received so far. The intent of this oracle was to provide a lower bound on the best possible output, giving students additional incentive to continue improving their systems. 4.3 Challenge Results We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). • Inclusion of heuristic estimates of future cost. • Implementation of a private oracle. Some students observed that the ideal beam setting was not uniform across the corpus. They ran their decoder under different settings, and then selected the most probable translation of each sentence. Many teams who implemented the standard stack decoding algorithm experimented heavily with its pruning parameters. The best submission used extremely wide beam settings in conjunction with a reimplementation of the future cost estimate used in Moses (Koehn et al., 2007). Five of the submissions beat Moses u</context>
</contexts>
<marker>Langlais, Patry, Gotti, 2007</marker>
<rawString>P. Langlais, A. Patry, and F. Gotti. 2007. A greedy decoder for phrase-based statistical machine translation. In Proc. of TMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lawrence</author>
</authors>
<title>Teaching data structures using competitive games.</title>
<date>2004</date>
<journal>IEEE Transactions on Education,</journal>
<volume>47</volume>
<issue>4</issue>
<contexts>
<context position="37875" citStr="Lawrence, 2004" startWordPosition="6058" endWordPosition="6059">ets focus on specific techniques. Although this is important, there is also a place for open-ended problems on which students apply a full range of problem-solving skills. Automatic grading enables them to scale easily to large numbers of students. On the scientific side, the scale of MOOCs might make it possible to empirically measure the effectiveness of hands-on or competitive assignments, by comparing course performance of students who work on them against that of those who do not. Though there is some empirical work on competitive assignments in the computer science education literature (Lawrence, 2004; Garlick and Akl, 2006; Regueras et al., 2008; Ribeiro et al., 2009), they generally measure student satisfaction and retention rather than the more difficult question of whether such assignments actually improve student learning. However, it might be feasible to answer such ques174 tions in large, data-rich virtual classrooms offered by MOOCs. This is an interesting potential avenue for future work. Because our class came within reach of state-ofthe-art on each problem within a matter of weeks, we wonder what might happen with a very large body of competitors. Could real innovation occur? Co</context>
</contexts>
<marker>Lawrence, 2004</marker>
<rawString>R. Lawrence. 2004. Teaching data structures using competitive games. IEEE Transactions on Education, 47(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proc. ofNAACL.</booktitle>
<contexts>
<context position="12099" citStr="Liang et al. (2006)" startWordPosition="1911" endWordPosition="1914">nt not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data of this size that is within a fraction of a point of their accuracy on the complete Hansards data. To evaluate, we used manual alignments of a small fraction of sentences, developed by Och and Ney (2000), which we obtained from the shared task resources organized by Mihalcea and Pedersen (2003). The first 37 sentences of the corpus were development data, with manual alignments provided in a separate file. Test data consisted of an additional 447 sentences, for which we did not provide alignments.9 3.2 Implementation We distributed three Python programs with t</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In Proc. ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>F J Och</author>
</authors>
<title>ORANGE: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="28283" citStr="Lin and Och, 2004" startWordPosition="4487" endWordPosition="4490"> feedback. We privately implemented a version of BLEU, which obtained a correlation of 38.6 with the human rankings, a modest improvement over the baseline of 34.0. Our implementation underperforms the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 5.3 Evaluation Challenge Results We received 212 submissions from 12 teams (Figure 3), again demonstrating a wide range of techniques. • Experimentation with the maximum n-gram length and weights in BLEU. • Implementation of smoothed versions of BLEU (Lin and Och, 2004). • Implementation of weighted F-measure to balance both precision and recall. • Careful normalization of the reference and machine translations, including lowercasing and punctuation-stripping. 12This ranking has been disputed over a series of papers (Lopez, 2012; Callison-Burch et al., 2012; Koehn, 2012). The paper which initiated the dispute, written by the first author, was directly inspired by the experience of designing this assignment. Spearman’s ρ 0.8 0.6 0.4 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, </context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Discriminative word alignment by linear modeling.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="16887" citStr="Liu et al., 2010" startWordPosition="2674" endWordPosition="2677"> 30 40 168 • Implementing various alternative approaches from the literature, including IBM Model 2 (Brown et al., 1993), competitive linking (Melamed, 2000), and smoothing (Moore, 2004). One of the best solutions was competitive linking with Dice’s coefficient, modified to incorporate the observation that alignments tend to be monotonic by restricting possible alignment points to a window of eight words around the diagonal. Although simple, it acheived an AER of 18.41, an error reduction over Model 1 of more than 40%. The best score compares unfavorably against a state-of-the-art AER of 3.6 (Liu et al., 2010). But under a different view, it still represents a significant amount of progress for an effort taking just over two weeks: on the original challenge from which we obtained the data (Mihalcea and Pedersen, 2003) the best student system would have placed fifth out of fifteen systems. Consider also the combined effort of all the students: when we trained a perceptron classifier on the development data, taking each student’s prediction as a feature, we obtained an AER of 15.4, which would have placed fourth on the original challenge. This is notable since none of the systems incorporated first-o</context>
</contexts>
<marker>Liu, Liu, Lin, 2010</marker>
<rawString>Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word alignment by linear modeling. Computational Linguistics, 36(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="21090" citStr="Lopez (2007)" startWordPosition="3377" endWordPosition="3378">2) i |ej, j=1 j ) 169 the model, students had enough information to compute the evaluation score on any dataset themselves. The difficulty of the challenge lies simply in finding the translation that maximizes the evaluation. Indeed, since the problem is intractable, even the instructors did not know the true solution.11 4.1 Data We chose 48 French sentences totaling 716 words from the Canadian Hansards to serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). 4.2 Implementation We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the language model and translation model, but without permuting phrases. The implementation is completely self-contained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to </context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>A. Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Statistical machine translation.</title>
<date>2008</date>
<journal>ACM Computing Surveys,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="2750" citStr="Lopez, 2008" startWordPosition="420" endWordPosition="421">ris Callison-Burch was at Johns Hopkins University. in a class on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as bla</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>A. Lopez. 2008. Statistical machine translation. ACM Computing Surveys, 40(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Putting human assessments of machine translation systems in order.</title>
<date>2012</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="28547" citStr="Lopez, 2012" startWordPosition="4527" endWordPosition="4528">enization or normalization of the data. This also left room for improvement. 5.3 Evaluation Challenge Results We received 212 submissions from 12 teams (Figure 3), again demonstrating a wide range of techniques. • Experimentation with the maximum n-gram length and weights in BLEU. • Implementation of smoothed versions of BLEU (Lin and Och, 2004). • Implementation of weighted F-measure to balance both precision and recall. • Careful normalization of the reference and machine translations, including lowercasing and punctuation-stripping. 12This ranking has been disputed over a series of papers (Lopez, 2012; Callison-Burch et al., 2012; Koehn, 2012). The paper which initiated the dispute, written by the first author, was directly inspired by the experience of designing this assignment. Spearman’s ρ 0.8 0.6 0.4 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained</context>
</contexts>
<marker>Lopez, 2012</marker>
<rawString>A. Lopez. 2012. Putting human assessments of machine translation systems in order. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>B Dorr</author>
</authors>
<title>Combining open-source with research to re-engineer a hands-on introductory NLP course.</title>
<date>2008</date>
<booktitle>In Proc. of Workshop on Issues in Teaching Computational Linguistics.</booktitle>
<contexts>
<context position="6309" citStr="Madnani and Dorr (2008)" startWordPosition="978" endWordPosition="981">ing, respectively. In real MT systems, these problems are highly interdependent, a point we emphasized in class and at the end of each assignment—for example, that alignment is an exercise in parameter estimation for translation models, that model choice is a tradeoff between expressivity and efficient inference, and that optimal search does not guarantee optimal accuracy. However, presenting each problem independently and holding all else constant enables more focused exploration. For each problem we provided data, a naive solution, and an evaluation program. Following Bird et al. (2008) and Madnani and Dorr (2008), we implemented the challenges in Python, a high-level pro1http://alopez.github.io/dreamt gramming language that can be used to write very concise programs resembling pseudocode.2,3 By default, each baseline system reads the test data and generates output in the evaluation format, so setup required zero configuration, and students could begin experimenting immediately. For example, on receipt of the alignment code, aligning data and evaluating results required only typing: &gt; align |grade Students could then run experiments within minutes of beginning the assignment. Three of the four challeng</context>
</contexts>
<marker>Madnani, Dorr, 2008</marker>
<rawString>N. Madnani and B. Dorr. 2008. Combining open-source with research to re-engineer a hands-on introductory NLP course. In Proc. of Workshop on Issues in Teaching Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="16427" citStr="Melamed, 2000" startWordPosition="2603" endWordPosition="2604">ignments in the development data. • Running Model 1 for many iterations. Most researchers typically run Model 1 for five iterations or fewer, and there are few experiments in the literature on its behavior over many iterations, as there are for hidden Markov model taggers (Johnson, 2007). Our students carried out these experiments, reporting runs of 5, 20, 100, and even 2000 iterations. No improvement was observed after 20 iterations. 20 50 60 AER × 100 30 40 168 • Implementing various alternative approaches from the literature, including IBM Model 2 (Brown et al., 1993), competitive linking (Melamed, 2000), and smoothing (Moore, 2004). One of the best solutions was competitive linking with Dice’s coefficient, modified to incorporate the observation that alignments tend to be monotonic by restricting possible alignment points to a window of eight words around the diagonal. Although simple, it acheived an AER of 18.41, an error reduction over Model 1 of more than 40%. The best score compares unfavorably against a state-of-the-art AER of 3.6 (Liu et al., 2010). But under a different view, it still represents a significant amount of progress for an effort taking just over two weeks: on the original</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. D. Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In Proc. on Workshop on Building and Using Parallel Texts.</booktitle>
<contexts>
<context position="12429" citStr="Mihalcea and Pedersen (2003)" startWordPosition="1969" endWordPosition="1973">a We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data of this size that is within a fraction of a point of their accuracy on the complete Hansards data. To evaluate, we used manual alignments of a small fraction of sentences, developed by Och and Ney (2000), which we obtained from the shared task resources organized by Mihalcea and Pedersen (2003). The first 37 sentences of the corpus were development data, with manual alignments provided in a separate file. Test data consisted of an additional 447 sentences, for which we did not provide alignments.9 3.2 Implementation We distributed three Python programs with the data. The first, align, computes Dice’s coefficient (1945) for every pair of French and English words, then aligns every pair for which its value is above an adjustable threshold. Our implementation (most of 7Among them, Jordan Boyd-Graber, John DeNero, Philipp Koehn, and Slav Petrov (personal communication). 8http://www.isi.</context>
<context position="17099" citStr="Mihalcea and Pedersen, 2003" startWordPosition="2710" endWordPosition="2713">st solutions was competitive linking with Dice’s coefficient, modified to incorporate the observation that alignments tend to be monotonic by restricting possible alignment points to a window of eight words around the diagonal. Although simple, it acheived an AER of 18.41, an error reduction over Model 1 of more than 40%. The best score compares unfavorably against a state-of-the-art AER of 3.6 (Liu et al., 2010). But under a different view, it still represents a significant amount of progress for an effort taking just over two weeks: on the original challenge from which we obtained the data (Mihalcea and Pedersen, 2003) the best student system would have placed fifth out of fifteen systems. Consider also the combined effort of all the students: when we trained a perceptron classifier on the development data, taking each student’s prediction as a feature, we obtained an AER of 15.4, which would have placed fourth on the original challenge. This is notable since none of the systems incorporated first-order dependencies on the alignments of adjacent words, long noted as an important feature of the best alignment models (Och and Ney, 2003). Yet a simple system combination of student assignments is as effective a</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>R. Mihalcea and T. Pedersen. 2003. An evaluation exercise for word alignment. In Proc. on Workshop on Building and Using Parallel Texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
</authors>
<title>Improving IBM word alignment model 1. In</title>
<date>2004</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="16456" citStr="Moore, 2004" startWordPosition="2607" endWordPosition="2608">a. • Running Model 1 for many iterations. Most researchers typically run Model 1 for five iterations or fewer, and there are few experiments in the literature on its behavior over many iterations, as there are for hidden Markov model taggers (Johnson, 2007). Our students carried out these experiments, reporting runs of 5, 20, 100, and even 2000 iterations. No improvement was observed after 20 iterations. 20 50 60 AER × 100 30 40 168 • Implementing various alternative approaches from the literature, including IBM Model 2 (Brown et al., 1993), competitive linking (Melamed, 2000), and smoothing (Moore, 2004). One of the best solutions was competitive linking with Dice’s coefficient, modified to incorporate the observation that alignments tend to be monotonic by restricting possible alignment points to a window of eight words around the diagonal. Although simple, it acheived an AER of 18.41, an error reduction over Model 1 of more than 40%. The best score compares unfavorably against a state-of-the-art AER of 3.6 (Liu et al., 2010). But under a different view, it still represents a significant amount of progress for an effort taking just over two weeks: on the original challenge from which we obta</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>R. C. Moore. 2004. Improving IBM word alignment model 1. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="11439" citStr="Och and Ney, 2000" startWordPosition="1796" endWordPosition="1799">ny assignment would earn an A; and top three placement compensated for weaker grades in other course criteria. Everyone who completed all four assignments placed in the top five at least once. 6The equilibrium point is a single team, though this team would still need to decide on a division of labor. One student contemplated organizing this team, but decided against it. Some did so after the assignment deadline. 3 The Alignment Challenge The first challenge was word alignment: given a parallel text, students were challenged to produce wordto-word alignments with low alignment error rate (AER; Och and Ney, 2000). This is a variant of a classic assignment not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enoug</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<contexts>
<context position="17625" citStr="Och and Ney, 2003" startWordPosition="2799" endWordPosition="2802">eks: on the original challenge from which we obtained the data (Mihalcea and Pedersen, 2003) the best student system would have placed fifth out of fifteen systems. Consider also the combined effort of all the students: when we trained a perceptron classifier on the development data, taking each student’s prediction as a feature, we obtained an AER of 15.4, which would have placed fourth on the original challenge. This is notable since none of the systems incorporated first-order dependencies on the alignments of adjacent words, long noted as an important feature of the best alignment models (Och and Ney, 2003). Yet a simple system combination of student assignments is as effective as a hidden Markov Model trained on a comparable amount of data (Och and Ney, 2003). It is important to note that AER does not necessarily correlate with downstream performance, particularly on the Hansards dataset (Fraser and Marcu, 2007). We used the conclusion of the assignment as an opportunity to emphasize this point. 4 The Decoding Challenge The second challenge was decoding: given a fixed translation model and a set of input sentences, students were challenged to produce translations with the highest model score. T</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="31621" citStr="Och et al., 2004" startWordPosition="5013" endWordPosition="5016">as a simple program that produced a vector of feature weights using pairwise ranking optimization (PRO; Hopkins and May, 2011), with a perceptron as the underlying learning algorithm. A second, rerank, takes a weight vector as input and reranks the sentences; both programs were designed to work with arbitrary numbers of features. The grade program computed the BLEU score on development data, while check ensured that a test submission is valid. Finally, we provided an oracle program, which computed a lower bound on the achievable BLEU score on the development data using a greedy approximation (Och et al., 2004). The leaderboard likewise displayed an oracle on test data. We did not assign a target algorithm, but left the assignment fully open-ended. 6.3 Reranking Challenge Outcome For each assignment, we made an effort to create room for competition above the target algorithm. However, we did not accomplish this in the reranking challenge: we had removed most of the features from the candidate translations, in hopes that students might reinvent some of them, but we left one highly predictive implicit feature in the data: the rank order of the underlying translation system. Students discovered that si</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of features for statistical machine translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Owczarzak</author>
<author>D Groves</author>
<author>J V Genabith</author>
<author>A Way</author>
</authors>
<title>Contextual bitext-derived paraphrases in automatic MT evaluation.</title>
<date>2006</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="29070" citStr="Owczarzak et al., 2006" startWordPosition="4609" endWordPosition="4612">ng and punctuation-stripping. 12This ranking has been disputed over a series of papers (Lopez, 2012; Callison-Burch et al., 2012; Koehn, 2012). The paper which initiated the dispute, written by the first author, was directly inspired by the experience of designing this assignment. Spearman’s ρ 0.8 0.6 0.4 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of Bannard and CallisonBurch (2005), and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases. When compared with the 20 systems submitted to the original task from which the data was obtained (Callison-Burch et al., 2011), this system would have ranked fifth, quite near the top-scoring competitors, whose correlations ranged from 88 to 94. 6 The Reranking Challenge T</context>
</contexts>
<marker>Owczarzak, Groves, Genabith, Way, 2006</marker>
<rawString>K. Owczarzak, D. Groves, J. V. Genabith, and A. Way. 2006. Contextual bitext-derived paraphrases in automatic MT evaluation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="26416" citStr="Papineni et al., 2002" startWordPosition="4197" endWordPosition="4200">2010), p. 165. place empty hypothesis into stack 0 log10 p(e|f) − C -1250 for all stacks 0...n − 1 do -1300 for all hypotheses in stack do -1350 for all translation options do -1400 if applicable then create new hypothesis place in stack recombine with existing hypothesis prune stack if too big 5.1 Data We chose the English-to-German translation systems from the 2009 and 2011 shared task at the annual Workshop for Machine Translation (CallisonBurch et al., 2009; Callison-Burch et al., 2011), providing the first as development data and the second as test data. We chose these sets because BLEU (Papineni et al., 2002), our baseline metric, performed particularly poorly on them; this left room for improvement in addition to highlighting some Figure 2: Submission history for the decoding challenge. The dotted green line represents the oracle over submissions. deficiencies of BLEU. For each dataset we provided the source and reference sentences along with anonymized system outputs. For the development data we also provided the human ranking of the sys171 tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 5.2 Implementation We provided three simple Python</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Regueras</author>
<author>E Verd´u</author>
<author>M Verd´u</author>
<author>M P´erez</author>
<author>J de Castro</author>
<author>M Mu˜noz</author>
</authors>
<title>Motivating students through on-line competition: An analysis of satisfaction and learning styles.</title>
<date>2008</date>
<marker>Regueras, Verd´u, Verd´u, P´erez, de Castro, Mu˜noz, 2008</marker>
<rawString>L. Regueras, E. Verd´u, M. Verd´u, M. P´erez, J. de Castro, and M. Mu˜noz. 2008. Motivating students through on-line competition: An analysis of satisfaction and learning styles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ribeiro</author>
<author>M Ferreira</author>
<author>H Sim˜oes</author>
</authors>
<title>Teaching artificial intelligence and logic programming in a competitive environment.</title>
<date>2009</date>
<journal>Informatics in Education, (Vol</journal>
<volume>8</volume>
<pages>1--85</pages>
<marker>Ribeiro, Ferreira, Sim˜oes, 2009</marker>
<rawString>P. Ribeiro, M. Ferreira, and H. Sim˜oes. 2009. Teaching artificial intelligence and logic programming in a competitive environment. Informatics in Education, (Vol 8 1):85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Spacco</author>
<author>D Hovemeyer</author>
<author>W Pugh</author>
<author>J Hollingsworth</author>
<author>N Padua-Perez</author>
<author>F Emad</author>
</authors>
<title>Experiences with marmoset: Designing and using an advanced submission and testing system for programming courses.</title>
<date>2006</date>
<booktitle>In Proc. of Innovation and technology in computer science education.</booktitle>
<contexts>
<context position="10279" citStr="Spacco et al., 2006" startWordPosition="1602" endWordPosition="1605">n early start, since teams could verify for themselves when they met the threshold for a passing grade. Though effective, it also detracted from realism in one important way: it enabled hillclimbing on the evaluation metric. In early assignments, we observed a few cases of this behavior, so for the remaining assignments, we modified the leaderboard so that changes in score would only be reflected once every twelve hours. This strategy trades some amount of scientific realism for some measure of incentive, a strategy that has proven effective in other pedagogical tools with real-time feedback (Spacco et al., 2006). To obtain a grade, teams were required to submit their results, share their code privately with the instructors, and publicly describe their experimental process to the class so that everyone could learn from their collective effort. Teams were free (but not required) to share their code publicly at any time. 5Grades depend on institutional norms. In our case, high grades in the rest of class combined with matching all assignment target algorithms would earn a B+; beating two target algorithms would earn an A-; top five placement on any assignment would earn an A; and top three placement com</context>
</contexts>
<marker>Spacco, Hovemeyer, Pugh, Hollingsworth, Padua-Perez, Emad, 2006</marker>
<rawString>J. Spacco, D. Hovemeyer, W. Pugh, J. Hollingsworth, N. Padua-Perez, and F. Emad. 2006. Experiences with marmoset: Designing and using an advanced submission and testing system for programming courses. In Proc. of Innovation and technology in computer science education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ofICSLP.</booktitle>
<contexts>
<context position="21200" citStr="Stolcke, 2002" startWordPosition="3396" endWordPosition="3397">set themselves. The difficulty of the challenge lies simply in finding the translation that maximizes the evaluation. Indeed, since the problem is intractable, even the instructors did not know the true solution.11 4.1 Data We chose 48 French sentences totaling 716 words from the Canadian Hansards to serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). 4.2 Implementation We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the language model and translation model, but without permuting phrases. The implementation is completely self-contained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding algorithm (Listing 2) with the pseudocode in Koehn’s (2010) popular textbook </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proc. ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
<author>H Sawaf</author>
</authors>
<title>Accelerated DP based search for statistical translation.</title>
<date>1997</date>
<booktitle>In Proc. of European Conf. on Speech Communication and Technology.</booktitle>
<contexts>
<context position="27153" citStr="Tillmann et al., 1997" startWordPosition="4311" endWordPosition="4314">hting some Figure 2: Submission history for the decoding challenge. The dotted green line represents the oracle over submissions. deficiencies of BLEU. For each dataset we provided the source and reference sentences along with anonymized system outputs. For the development data we also provided the human ranking of the sys171 tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 5.2 Implementation We provided three simple Python programs: evaluate implements a simple ranking of the systems based on position-independent word error rate (PER; Tillmann et al., 1997), which computes a bagof-words overlap between the system translations and the reference. The grade program computes Spearman’s ρ between the human ranking and an output ranking. The check program simply ensures that a submission contains a valid ranking. We were concerned about hill-climbing on the test data, so we modified the leaderboard to report new results only twice a day. This encouraged students to experiment on the development data before posting new submissions, while still providing intermittent feedback. We privately implemented a version of BLEU, which obtained a correlation of 3</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP based search for statistical translation. In Proc. of European Conf. on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zaslavskiy</author>
<author>M Dymetman</author>
<author>N Cancedda</author>
</authors>
<title>Phrase-based statistical machine translation as a traveling salesman problem.</title>
<date>2009</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="20090" citStr="Zaslavskiy et al., 2009" startWordPosition="3209" endWordPosition="3212">or two reasons. First, leaving out the distortion model slightly simplifies the implementation, since it is not necessary to keep track of the last word translated in a beam decoder; we felt that this detail was secondary to understanding the difficulty of search over phrase permutations. Second, it actually makes the problem more difficult, since a simple distance-based distortion model prefers translations with fewer permutations; without it, the model may easily prefer any permutation of the target phrases, making even the Viterbi search problem exhibit its true NP-hardness (Knight, 1999a; Zaslavskiy et al., 2009). Since the goal was to find the translation with the highest probability, we did not provide a held-out test set; with access to both the input sentences and 10For simplicity, this formula assumes that a is padded with two sentence-initial symbols and one sentence-final symbol, and ignores the probability of sentence segmentation, which we take to be uniform. p(fi, J+1H p(ej|ej−1, ej−2) i |ej, j=1 j ) 169 the model, students had enough information to compute the evaluation score on any dataset themselves. The difficulty of the challenge lies simply in finding the translation that maximizes th</context>
</contexts>
<marker>Zaslavskiy, Dymetman, Cancedda, 2009</marker>
<rawString>M. Zaslavskiy, M. Dymetman, and N. Cancedda. 2009. Phrase-based statistical machine translation as a traveling salesman problem. In Proc. ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>