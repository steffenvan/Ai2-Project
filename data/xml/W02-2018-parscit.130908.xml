<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003764">
<title confidence="0.993388">
A comparison of algorithms for maximum entropy parameter estimation
</title>
<author confidence="0.983289">
Robert Malouf
</author>
<affiliation confidence="0.854738">
Alfa-Informatica
</affiliation>
<address confidence="0.772758">
Rijksuniversiteit Groningen
Postbus 716
9700AS Groningen
The Netherlands
</address>
<email confidence="0.994376">
malouf@let.rug.nl
</email>
<sectionHeader confidence="0.993715" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977175">
Conditional maximum entropy (ME) models pro-
vide a general purpose machine learning technique
which has been successfully applied to fields as
diverse as computer vision and econometrics, and
which is used for a wide variety of classification
problems in natural language processing. However,
the flexibility of ME models is not without cost.
While parameter estimation for ME models is con-
ceptually straightforward, in practice ME models
for typical natural language tasks are very large, and
may well contain many thousands of free parame-
ters. In this paper, we consider a number of algo-
rithms for estimating the parameters of ME mod-
els, including iterative scaling, gradient ascent, con-
jugate gradient, and variable metric methods. Sur-
prisingly, the standardly used iterative scaling algo-
rithms perform quite poorly in comparison to the
others, and for all of the test problems, a limited-
memory variable metric algorithm outperformed the
other choices.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925512195122">
Maximum entropy (ME) models, variously known
as log-linear, Gibbs, exponential, and multinomial
logit models, provide a general purpose machine
learning technique for classification and prediction
which has been successfully applied to fields as di-
verse as computer vision and econometrics. In natu-
ral language processing, recent years have seen ME
techniques used for sentence boundary detection,
part of speech tagging, parse selection and ambigu-
ity resolution, and stochastic attribute-value gram-
mars, to name just a few applications (Abney, 1997;
Berger et al., 1996; Ratnaparkhi, 1998; Johnson et
al., 1999).
A leading advantage of ME models is their flex-
ibility: they allow stochastic rule systems to be
augmented with additional syntactic, semantic, and
pragmatic features. However, the richness of the
representations is not without cost. Even mod-
est ME models can require considerable computa-
tional resources and very large quantities of anno-
tated training data in order to accurately estimate
the model’s parameters. While parameter estima-
tion for ME models is conceptually straightforward,
in practice ME models for typical natural language
tasks are usually quite large, and frequently contain
hundreds of thousands of free parameters. Estima-
tion of such large models is not only expensive, but
also, due to sparsely distributed features, sensitive
to round-off errors. Thus, highly efficient, accurate,
scalable methods are required for estimating the pa-
rameters of practical models.
In this paper, we consider a number of algorithms
for estimating the parameters of ME models, in-
cluding Generalized Iterative Scaling and Improved
Iterative Scaling, as well as general purpose opti-
mization techniques such as gradient ascent, conju-
gate gradient, and variable metric methods. Sur-
prisingly, the widely used iterative scaling algo-
rithms perform quite poorly, and for all of the test
problems, a limited memory variable metric algo-
rithm outperformed the other choices.
</bodyText>
<sectionHeader confidence="0.947847" genericHeader="method">
2 Maximum likelihood estimation
</sectionHeader>
<bodyText confidence="0.999985153846154">
Suppose we are given a probability distribution p
over a set of events X which are characterized by a
d dimensional feature vector function f : X → Rd.
In addition, we have also a set of contexts W and a
function Y which partitions the members of X. In
the case of a stochastic context-free grammar, for
example, X might be the set of possible trees, the
feature vectors might represent the number of times
each rule applied in the derivation of each tree, W
might be the set of possible strings of words, and
Y(w) the set of trees whose yield is w ∈ W. A con-
ditional maximum entropy model qθ(x|w) for p has
the parametric form (Berger et al., 1996; Chi, 1998;
</bodyText>
<equation confidence="0.861992333333333">
Johnson et al., 1999):
expθT f (x)~
q0(x|w) = ∑y∈Y(w) exp(θT f (y)) (1)
</equation>
<bodyText confidence="0.990783444444445">
where θ is a d-dimensional parameter vector and
θT f (x) is the inner product of the parameter vector
and a feature vector.
Given the parametric form of an ME model in
(1), fitting an ME model to a collection of training
data entails finding values for the parameter vector
θ which minimize the Kullback-Leibler divergence
between the model q0 and the empirical distribu-
tion p:
</bodyText>
<equation confidence="0.981611">
p(x,w)log p(x|w)
q0(x|w)
or, equivalently, which maximize the log likelihood:
L(θ) = ∑ p(w,x)logq0(x|w) (2)
w,x
</equation>
<bodyText confidence="0.999836333333333">
ratio of Ep[f] to Eq(k)[f], with the restriction that
∑j fj(x) = C for each event x in the training data
(a condition which can be easily satisfied by the ad-
dition of a correction feature). We can adapt GIS
to estimate the model parameters θ rather than the
model probabilities q, yielding the update rule:
</bodyText>
<equation confidence="0.994135">
Ep[f] !
δ(k) = log
Eq(k)[f]
</equation>
<bodyText confidence="0.9997428">
The step size, and thus the rate of convergence,
depends on the constant C: the larger the value of
C, the smaller the step size. In case not all rows of
the training data sum to a constant, the addition of a
correction feature effectively slows convergence to
match the most difficult case. To avoid this slowed
convergence and the need for a correction feature,
Della Pietra et al. (1997) propose an Improved Iter-
ative Scaling (IIS) algorithm, whose update rule is
the solution to the equation:
</bodyText>
<equation confidence="0.99672425">
D(p||q0) = ∑
w,x
1
C
</equation>
<bodyText confidence="0.961417666666667">
The gradient of the log likelihood function, or the Ep[f] = ∑w,x p(w)q(k)(x|w)f(x)exp(M(x)δ(k))
vector of its first derivatives with respect to the pa-
rameter θ is:
</bodyText>
<equation confidence="0.998564">
G(θ) = Ep[f]−Eqθ[f] (3)
</equation>
<bodyText confidence="0.999971647058823">
Since the likelihood function (2) is concave over
the parameter space, it has a global maximum where
the gradient is zero. Unfortunately, simply setting
G(θ) = 0 and solving for θ does not yield a closed
form solution, so we proceed iteratively. At each
step, we adjust an estimate of the parameters θ(k)
to a new estimate θ(k+1) based on the divergence
between the estimated probability distribution q(k)
and the empirical distribution p. We continue until
successive improvements fail to yield a sufficiently
large decrease in the divergence.
While all parameter estimation algorithms we
will consider take the same general form, the
method for computing the updates δ(k) at each
search step differs substantially. As we shall see,
this difference can have a dramatic impact on the
number of updates required to reach convergence.
</bodyText>
<subsectionHeader confidence="0.973509">
2.1 Iterative Scaling
</subsectionHeader>
<bodyText confidence="0.999877727272727">
One popular method for iteratively refining the
model parameters is Generalized Iterative Scaling
(GIS), due to Darroch and Ratcliff (1972). An
extension of Iterative Proportional Fitting (Dem-
ing and Stephan, 1940), GIS scales the probabil-
ity distribution q(k) by a factor proportional to the
where M(x) is the sum of the feature values for an
event x in the training data. This is a polynomial in
exp(δ(k)), and the solution can be found straight-
forwardly using, for example, the Newton-Raphson
method.
</bodyText>
<subsectionHeader confidence="0.999186">
2.2 First order methods
</subsectionHeader>
<bodyText confidence="0.984666619047619">
Iterative scaling algorithms have a long tradition in
statistics and are still widely used for analysis of
contingency tables. Their primary strength is that
on each iteration they only require computation of
the expected values Eq(k). They do not depend on
evaluation of the gradient of the log-likelihood func-
tion, which, depending on the distribution, could be
prohibitively expensive. In the case of ME models,
however, the vector of expected values required by
iterative scaling essentially is the gradient G. Thus,
it makes sense to consider methods which use the
gradient directly.
The most obvious way of making explicit use of
the gradient is by Cauchy’s method, or the method
of steepest ascent. The gradient of a function is a
vector which points in the direction in which the
function’s value increases most rapidly. Since our
goal is to maximize the log-likelihood function, a
natural strategy is to shift our current estimate of
the parameters in the direction of the gradient via
the update rule:
</bodyText>
<equation confidence="0.990539">
δ(k) = α(k)G(θ(k))
</equation>
<bodyText confidence="0.999985333333333">
where the step size α(k) is chosen to maximize
L(θ(k) +δ(k)). Finding the optimal step size is itself
an optimization problem, though only in one dimen-
sion and, in practice, only an approximate solution
is required to guarantee global convergence.
Since the log-likelihood function is concave, the
method of steepest ascent is guaranteed to find the
global maximum. However, while the steps taken
on each iteration are in a very narrow sense locally
optimal, the global convergence rate of steepest as-
cent is very poor. Each new search direction is or-
thogonal (or, if an approximate line search is used,
nearly so) to the previous direction. This leads to
a characteristic “zig-zag” ascent, with convergence
slowing as the maximum is approached.
One way of looking at the problem with steep-
est ascent is that it considers the same search di-
rections many times. We would prefer an algo-
rithm which considered each possible search direc-
tion only once, in each iteration taking a step of ex-
actly the right length in a direction orthogonal to all
previous search directions. This intuition underlies
conjugate gradient methods, which choose a search
direction which is a linear combination of the steep-
est ascent direction and the previous search direc-
tion. The step size is selected by an approximate
line search, as in the steepest ascent method. Sev-
eral non-linear conjugate gradient methods, such as
the Fletcher-Reeves (cg-fr) and the Polak-Ribi`ere-
Positive (cf-prp) algorithms, have been proposed.
While theoretically equivalent, they use slighly dif-
ferent update rules and thus show different numeric
properties.
</bodyText>
<subsectionHeader confidence="0.999436">
2.3 Second order methods
</subsectionHeader>
<bodyText confidence="0.999917">
Another way of looking at the problem with steep-
est ascent is that while it takes into account the gra-
dient of the log-likelihood function, it fails to take
into account its curvature, or the gradient of the gra-
dient. The usefulness of the curvature is made clear
if we consider a second-order Taylor series approx-
imation of L(θ +δ):
</bodyText>
<equation confidence="0.933362">
L(θ+δ) ≈L(θ)+δTG(θ)+ 12δT H(θ)δ (4)
</equation>
<bodyText confidence="0.9999648">
where H is Hessian matrix of the log-likelihood
function, the d × d matrix of its second partial
derivatives with respect to θ. If we set the deriva-
tive of (4) to zero and solve for δ, we get the update
rule for Newton’s method:
</bodyText>
<equation confidence="0.999491">
δ(k) = H−1(θ(k))G(θ(k)) (5)
</equation>
<bodyText confidence="0.9989814">
Newton’s method converges very quickly (for
quadratic objective functions, in one step), but it re-
quires the computation of the inverse of the Hessian
matrix on each iteration.
While the log-likelihood function for ME models
in (2) is twice differentiable, for large scale prob-
lems the evaluation of the Hessian matrix is com-
putationally impractical, and Newton’s method is
not competitive with iterative scaling or first order
methods. Variable metric or quasi-Newton methods
avoid explicit evaluation of the Hessian by building
up an approximation of it using successive evalua-
tions of the gradient. That is, we replace H−1(θ(k))
in (5) with a local approximation of the inverse Hes-
sian B(k):
</bodyText>
<equation confidence="0.988985">
δ(k) = B(k)G(θ(k))
</equation>
<bodyText confidence="0.9779405">
with B(k) a symmatric, positive definite matrix
which satisfies the equation:
</bodyText>
<equation confidence="0.993679">
B(k)y(k) = δ(k−1)
</equation>
<bodyText confidence="0.973191933333333">
where y(k) = G(θ(k)) − G(θ(k−1)).
Variable metric methods also show excellent con-
vergence properties and can be much more efficient
than using true Newton updates, but for large scale
problems with hundreds of thousands of parame-
ters, even storing the approximate Hessian is pro-
hibitively expensive. For such cases, we can apply
limited memory variable metric methods, which im-
plicitly approximate the Hessian matrix in the vicin-
ity of the current estimate of θ(k) using the previous
m values of y(k) and δ(k). Since in practical applica-
tions values of m between 3 and 10 suffice, this can
offer a substantial savings in storage requirements
over variable metric methods, while still giving fa-
vorable convergence properties.1
</bodyText>
<sectionHeader confidence="0.981444" genericHeader="method">
3 Comparing estimation techniques
</sectionHeader>
<bodyText confidence="0.998991451612903">
The performance of optimization algorithms is
highly dependent on the specific properties of the
problem to be solved. Worst-case analysis typically
&apos;Space constraints preclude a more detailed discussion of
these methods here. For algorithmic details and theoretical
analysis of first and second order methods, see, e.g., Nocedal
(1997) or Nocedal and Wright (1999).
does not reflect the actual behavior on actual prob-
lems. Therefore, in order to evaluate the perfor-
mance of the optimization techniques sketched in
previous section when applied to the problem of pa-
rameter estimation, we need to compare the perfor-
mance of actual implementations on realistic data
sets (Dolan and Mor´e, 2002).
Minka (2001) offers a comparison of iterative
scaling with other algorithms for parameter esti-
mation in logistic regression, a problem similar to
the one considered here, but it is difficult to trans-
fer Minka’s results to ME models. For one, he
evaluates the algorithms with randomly generated
training data. However, the performance and accu-
racy of optimization algorithms can be sensitive to
the specific numerical properties of the function be-
ing optimized; results based on random data may
or may not carry over to more realistic problems.
And, the test problems Minka considers are rela-
tively small (100–500 dimensions). As we have
seen, though, algorithms which perform well for
small and medium scale problems may not always
be applicable to problems with many thousands of
dimensions.
</bodyText>
<subsectionHeader confidence="0.988728">
3.1 Implementation
</subsectionHeader>
<bodyText confidence="0.999994166666667">
As a basis for the implementation, we have used
PETSc (the “Portable, Extensible Toolkit for Sci-
entific Computation”), a software library designed
to ease development of programs which solve large
systems of partial differential equations (Balay et
al., 2001; Balay et al., 1997; Balay et al., 2002).
PETSc offers data structures and routines for paral-
lel and sequential storage, manipulation, and visu-
alization of very large sparse matrices.
For any of the estimation techniques, the most ex-
pensive operation is computing the probability dis-
tribution q and the expectations Eq[f] for each it-
eration. In order to make use of the facilities pro-
vided by PETSc, we can store the training data as
a (sparse) matrix F, with rows corresponding to
events and columns to features. Then given a pa-
rameter vector θ, the unnormalized probabilities ˙q0
are the matrix-vector product:
</bodyText>
<equation confidence="0.590134">
˙q0 = expFθ
</equation>
<bodyText confidence="0.6884965">
and the feature expectations are the transposed
matrix-vector product:
</bodyText>
<equation confidence="0.984872">
Eqθ[f] = FT q0
</equation>
<bodyText confidence="0.999329833333333">
By expressing these computations as matrix-vector
operations, we can take advantage of the high per-
formance sparse matrix primitives of PETSc.
For the comparison, we implemented both Gener-
alized and Improved Iterative Scaling in C++ using
the primitives provided by PETSc. For the other op-
timization techniques, we used TAO (the “Toolkit
for Advanced Optimization”), a library layered on
top of the foundation of PETSc for solving non-
linear optimization problems (Benson et al., 2002).
TAO offers the building blocks for writing optimiza-
tion programs (such as line searches and conver-
gence tests) as well as high-quality implementations
of standard optimization algorithms (including con-
jugate gradient and variable metric methods).
Before turning to the results of the comparison,
two additional points need to be made. First, in
order to assure a consistent comparison, we need
to use the same stopping rule for each algorithm.
For these experiments, we judged that convergence
was reached when the relative change in the log-
likelihood between iterations fell below a predeter-
mined threshold. That is, each run was stopped
when:
</bodyText>
<equation confidence="0.993534">
&lt; ε (6)
L(θ(k))
</equation>
<bodyText confidence="0.999940210526316">
where the relative tolerance ε = 10−7. For any par-
ticular application, this may or may not be an appro-
priate stopping rule, but is only used here for pur-
poses of comparison.
Finally, it should be noted that in the current im-
plementation, we have not applied any of the possi-
ble optimizations that appear in the literature (Laf-
ferty and Suhm, 1996; Wu and Khudanpur, 2000;
Lafferty et al., 2001) to speed up normalization of
the probability distribution q. These improvements
take advantage of a model’s structure to simplify the
evaluation of the denominator in (1). The particular
data sets examined here are unstructured, and such
optimizations are unlikely to give any improvement.
However, when these optimizations are appropriate,
they will give a proportional speed-up to all of the
algorithms. Thus, the use of such optimizations is
independent of the choice of parameter estimation
method.
</bodyText>
<subsectionHeader confidence="0.948346">
3.2 Experiments
</subsectionHeader>
<bodyText confidence="0.9989376">
To compare the algorithms described in §2, we ap-
plied the implementation outlined in the previous
section to four training data sets (described in Table
1) drawn from the domain of natural language pro-
cessing. The ‘rules’ and ‘lex’ datasets are examples
</bodyText>
<equation confidence="0.657658">
|L(θ(k)) −L(θ(k−1))|
</equation>
<table confidence="0.8264406">
dataset classes contexts features non-zeros
rules 29,602 2,525 246 732,384
lex 42,509 2,547 135,182 3,930,406
summary 24,044 12,022 198,467 396,626
shallow 8,625,782 375,034 264,142 55,192,723
</table>
<tableCaption confidence="0.999439">
Table 1: Datasets used in experiments
</tableCaption>
<bodyText confidence="0.999943888888889">
of stochastic attribute value grammars, one with a
small set of SCFG-like features, and with a very
large set of fine-grained lexical features (Bouma
et al., 2001). The ‘summary’ dataset is part of a
sentence extraction task (Osborne, to appear), and
the ‘shallow’ dataset is drawn from a text chunking
application (Osborne, 2002). These datasets vary
widely in their size and composition, and are repre-
sentative of the kinds of datasets typically encoun-
tered in applying ME models to NLP classification
tasks.
The results of applying each of the parameter es-
timation algorithms to each of the datasets is sum-
marized in Table 2. For each run, we report the KL
divergence between the fitted model and the train-
ing data at convergence, the prediction accuracy of
fitted model on a held-out test set (the fraction of
contexts for which the event with the highest prob-
ability under the model also had the highest proba-
bility under the reference distribution), the number
of iterations required, the number of log-likelihood
and gradient evaluations required (algorithms which
use a line search may require several function eval-
uations per iteration), and the total elapsed time (in
seconds).2
There are a few things to observe about these
results. First, while IIS converges in fewer steps
the GIS, it takes substantially more time. At least
for this implementation, the additional bookkeeping
overhead required by IIS more than cancels any im-
provements in speed offered by accelerated conver-
gence. This may be a misleading conclusion, how-
ever, since a more finely tuned implementation of
IIS may well take much less time per iteration than
the one used for these experiments. However, even
if each iteration of IIS could be made as fast as an
</bodyText>
<footnote confidence="0.985747428571429">
2The reported time does not include the time required to in-
put the training data, which is difficult to reproduce and which
is the same for all the algorithms being tested. All tests were
run using one CPU of a dual processor 1700MHz Pentium 4
with 2 gigabytes of main memory at the Center for High Per-
formance Computing and Visualisation, University of Gronin-
gen.
</footnote>
<bodyText confidence="0.999202404761905">
iteration of GIS (which seems unlikely), the bene-
fits of IIS over GIS would in these cases be quite
modest.
Second, note that for three of the four datasets,
the KL divergence at convergence is roughly the
same for all of the algorithms. For the ‘summary’
dataset, however, they differ by up to two orders of
magnitude. This is an indication that the conver-
gence test in (6) is sensitive to the rate of conver-
gence and thus to the choice of algorithm. Any de-
gree of precision desired could be reached by any
of the algorithms, with the appropriate value of ε.
However, GIS, say, would require many more itera-
tions than reported in Table 2 to reach the precision
achieved by the limited memory variable metric al-
gorithm.
Third, the prediction accuracy is, in most cases,
more or less the same for all of the algorithms.
Some variability is to be expected—all of the data
sets being considered here are badly ill-conditioned,
and many different models will yield the same like-
lihood. In a few cases, however, the prediction
accuracy differs more substantially. For the two
SAVG data sets (‘rules’ and ‘lex’), GIS has a small
advantage over the other methods. More dramati-
cally, both iterative scaling methods perform very
poorly on the ‘shallow’ dataset. In this case, the
training data is very sparse. Many features are
nearly ‘pseudo-minimal’ in the sense of Johnson et
al. (1999), and so receive weights approaching −∞.
Smoothing the reference probabilities would likely
improve the results for all of the methods and re-
duce the observed differences. However, this does
suggest that gradient-based methods are robust to
certain problems with the training data.
Finally, the most significant lesson to be drawn
from these results is that, with the exception of
steepest ascent, gradient-based methods outperform
iterative scaling by a wide margin for almost all the
datasets, as measured by both number of function
evaluations and by the total elapsed time. And, in
each case, the limited memory variable metric algo-
</bodyText>
<table confidence="0.99077136">
KL Div. Acc Iters Evals Time
5.124x10−2 47.00 1186 1187 16.68
5.079x10−2 43.82 917 918 31.36
5.065x10−2 44.88 224 350 4.80
5.007x10−2 44.17 66 181 2.57
5.013x10−2 46.29 59 142 1.93
5.007x10−2 44.52 72 81 1.13
1.573x10−3 46.74 363 364 31.69
1.487x10−3 42.15 235 236 95.09
3.341x10−3 42.92 980 1545 114.21
1.377x10−3 43.30 148 408 30.36
1.893x10−3 44.06 114 281 21.72
1.366x10−3 43.30 168 176 20.02
1.857x10−3 96.10 1424 1425 107.05
1.081x10−3 96.10 593 594 188.54
2.489x10−3 96.33 1094 3321 190.22
9.053x10−5 95.87 157 849 49.48
3.297x10−4 96.10 112 537 31.66
5.598x10−5 95.54 63 69 8.52
3.314x10−2 14.19 3494 3495 21223.86
3.238x10−2 5.42 3264 3265 66855.92
7.303x10−2 26.74 3677 14527 85062.53
2.585x10−2 24.72 1157 6823 39038.31
3.534x10−2 24.72 536 2813 16251.12
3.024x10−2 23.82 403 421 2420.30
</table>
<figure confidence="0.97643892">
Dataset Method
rules gis
iis
steepest ascent
conjugate gradient (fr)
conjugate gradient (prp)
limited memory variable metric
lex gis
iis
steepest ascent
conjugate gradient (fr)
conjugate gradient (prp)
limited memory variable metric
summary gis
iis
steepest ascent
conjugate gradient (fr)
conjugate gradient (prp)
limited memory variable metric
shallow gis
iis
steepest ascent
conjugate gradient (fr)
conjugate gradient (prp)
limited memory variable metric
</figure>
<tableCaption confidence="0.944921">
Table 2: Results of comparison.
</tableCaption>
<bodyText confidence="0.934651">
rithm performs substantially better than any of the
competing methods.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999968361111111">
In this paper, we have described experiments com-
paring the performance of a number of different al-
gorithms for estimating the parameters of a con-
ditional ME model. The results show that vari-
ants of iterative scaling, the algorithms which are
most widely used in the literature, perform quite
poorly when compared to general function opti-
mization algorithms such as conjugate gradient and
variable metric methods. And, more specifically,
for the NLP classification tasks considered, the lim-
ited memory variable metric algorithm of Benson
and Mor´e (2001) outperforms the other choices by
a substantial margin.
This conclusion has obvious consequences for the
field. ME modeling is a commonly used machine
learning technique, and the application of improved
parameter estimation algorithms will it practical to
construct larger, more complex models. And, since
the parameters of individual models can be esti-
mated quite quickly, this will further open up the
possibility for more sophisticated model and feature
selection techniques which compare large numbers
of alternative model specifications. This suggests
that more comprehensive experiments to compare
the convergence rate and accuracy of various algo-
rithms on a wider range of problems is called for.
In addition, there is a larger lesson to be drawn
from these results. We typically think of computa-
tional linguistics as being primarily a symbolic dis-
cipline. However, statistical natural language pro-
cessing involves non-trivial numeric computations.
As these results show, natural language processing
can take great advantage of the algorithms and soft-
ware libraries developed by and for more quantita-
tively oriented engineering and computational sci-
ences.
</bodyText>
<sectionHeader confidence="0.990794" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.927141571428571">
The research of Dr. Malouf has been made possible by
a fellowship of the Royal Netherlands Academy of Arts
and Sciences and by the NWO PIONIER project Algo-
rithms for Linguistic Processing. Thanks also to Stephen
Clark, Andreas Eisele, Detlef Prescher, Miles Osborne,
and Gertjan van Noord for helpful comments and test
data.
</bodyText>
<sectionHeader confidence="0.995676" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.979763571428572">
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23:597–
618.
Satish Balay, William D. Gropp, Lois Curfman
McInnes, and Barry F. Smith. 1997. Efficienct
management of parallelism in object oriented nu-
merical software libraries. In E. Arge, A. M. Bru-
aset, and H. P. Langtangen, editors, Modern Soft-
ware Tools in Scientific Computing, pages 163–
202. Birkhauser Press.
Satish Balay, Kris Buschelman, William D. Gropp,
Dinesh Kaushik, Lois Curfman McInnes, and
Barry F. Smith. 2001. PETSc home page.
http://www.mcs.anl.gov/petsc.
Satish Balay, William D. Gropp, Lois Curfman
McInnes, and Barry F. Smith. 2002. PETSc users
manual. Technical Report ANL-95/11–Revision
2.1.2, Argonne National Laboratory.
Steven J. Benson and Jorge J. Mor´e. 2001. A lim-
ited memory variable metric method for bound
constrained minimization. Preprint ANL/ACS-
P909-0901, Argonne National Laboratory.
Steven J. Benson, Lois Curfman McInnes, Jorge J.
Mor´e, and Jason Sarich. 2002. TAO users
manual. Technical Report ANL/MCS-TM-242–
Revision 1.4, Argonne National Laboratory.
Adam Berger, Stephen Della Pietra, and Vincent
Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Compu-
tational Linguistics, 22.
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. 2001. Alpino: wide coverage computational
analysis of Dutch. In W. Daelemans, K. Sima’an,
J. Veenstra, and J. Zavrel, editors, Computational
Linguistics in the Netherlands 2000, pages 45–
59. Rodolpi, Amsterdam.
Zhiyi Chi. 1998. Probability models for complex
systems. Ph.D. thesis, Brown University.
J. Darroch and D. Ratcliff. 1972. Generalized it-
erative scaling for log-linear models. Ann. Math.
Statistics, 43:1470–1480.
Stephen Della Pietra, Vincent Della Pietra, and
John Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 19:380–393.
W.E. Deming and F.F. Stephan. 1940. On a least
squares adjustment of a sampled frequency table
when the expected marginals are known. Annals
ofMathematical Statistics, 11:427–444.
Elizabeth D. Dolan and Jorge J. Mor´e. 2002.
Benchmarking optimization software with per-
formance profiles. Mathematical Programming,
91:201–213.
Mark Johnson, Stuart Geman, Stephen Canon,
Zhiyi Chi, and Stefan Riezler. 1999. Estimators
for stochastic “unification-based” grammars. In
Proceedings of the 37th Annual Meeting of the
ACL, pages 535–541, College Park, Maryland.
John Lafferty and Bernhard Suhm. 1996. Cluster
expansions and iterative scaling for maximum en-
tropy language models. In K. Hanson and R. Sil-
ver, editors, Maximum Entropy and Bayesian
Methods. Kluwer.
John Lafferty, Fernando Pereira, and Andrew Mc-
Callum. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning (ICML).
Thomas P. Minka. 2001. Algorithms for
maximum-likelihood logistic regression. Statis-
tics Tech Report 758, CMU.
Jorge Nocedal and Stephen J. Wright. 1999. Nu-
merical Optimization. Springer, New York.
Jorge Nocedal. 1997. Large scale unconstrained
optimization. In A. Watson and I. Duff, editors,
The State of the Art in Numerical Analysis, pages
311–338. Oxford University Press.
Miles Osborne. 2002. Shallow parsing using noisy
and non-stationary training material. Journal of
Machine Learning Research, 2:695–719.
Miles Osborne. to appear. Using maximum entropy
for sentence extraction. In Proceedings of the
ACL 2002 Workshop on Automatic Summariza-
tion, Philadelphia.
Adwait Ratnaparkhi. 1998. Maximum entropy
models for natural language ambiguity resolu-
tion. Ph.D. thesis, University of Pennsylvania.
Jun Wu and Sanjeev Khudanpur. 2000. Efficient
training methods for maximum entropy language
modelling. In Proceedings of ICSLP2000, vol-
ume 3, pages 114–117, Beijing.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.149245">
<title confidence="0.9871">A comparison of algorithms for maximum entropy parameter estimation</title>
<author confidence="0.896182">Robert</author>
<email confidence="0.341035">Rijksuniversiteit</email>
<note confidence="0.595560333333333">Postbus 9700AS The</note>
<email confidence="0.837071">malouf@let.rug.nl</email>
<abstract confidence="0.999638857142857">Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<pages>618</pages>
<contexts>
<context position="1718" citStr="Abney, 1997" startWordPosition="249" endWordPosition="250">tric algorithm outperformed the other choices. 1 Introduction Maximum entropy (ME) models, variously known as log-linear, Gibbs, exponential, and multinomial logit models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to fields as diverse as computer vision and econometrics. In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications (Abney, 1997; Berger et al., 1996; Ratnaparkhi, 1998; Johnson et al., 1999). A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features. However, the richness of the representations is not without cost. Even modest ME models can require considerable computational resources and very large quantities of annotated training data in order to accurately estimate the model’s parameters. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural langua</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven P. Abney. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23:597– 618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satish Balay</author>
<author>William D Gropp</author>
<author>Lois Curfman McInnes</author>
<author>Barry F Smith</author>
</authors>
<title>Efficienct management of parallelism in object oriented numerical software libraries.</title>
<date>1997</date>
<booktitle>Modern Software Tools in Scientific Computing,</booktitle>
<pages>163--202</pages>
<editor>In E. Arge, A. M. Bruaset, and H. P. Langtangen, editors,</editor>
<publisher>Birkhauser Press.</publisher>
<contexts>
<context position="13478" citStr="Balay et al., 1997" startWordPosition="2166" endWordPosition="2169">ed on random data may or may not carry over to more realistic problems. And, the test problems Minka considers are relatively small (100–500 dimensions). As we have seen, though, algorithms which perform well for small and medium scale problems may not always be applicable to problems with many thousands of dimensions. 3.1 Implementation As a basis for the implementation, we have used PETSc (the “Portable, Extensible Toolkit for Scientific Computation”), a software library designed to ease development of programs which solve large systems of partial differential equations (Balay et al., 2001; Balay et al., 1997; Balay et al., 2002). PETSc offers data structures and routines for parallel and sequential storage, manipulation, and visualization of very large sparse matrices. For any of the estimation techniques, the most expensive operation is computing the probability distribution q and the expectations Eq[f] for each iteration. In order to make use of the facilities provided by PETSc, we can store the training data as a (sparse) matrix F, with rows corresponding to events and columns to features. Then given a parameter vector θ, the unnormalized probabilities ˙q0 are the matrix-vector product: ˙q0 = </context>
</contexts>
<marker>Balay, Gropp, McInnes, Smith, 1997</marker>
<rawString>Satish Balay, William D. Gropp, Lois Curfman McInnes, and Barry F. Smith. 1997. Efficienct management of parallelism in object oriented numerical software libraries. In E. Arge, A. M. Bruaset, and H. P. Langtangen, editors, Modern Software Tools in Scientific Computing, pages 163– 202. Birkhauser Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satish Balay</author>
<author>Kris Buschelman</author>
<author>William D Gropp</author>
<author>Dinesh Kaushik</author>
<author>Lois Curfman McInnes</author>
<author>Barry F Smith</author>
</authors>
<date>2001</date>
<note>PETSc home page. http://www.mcs.anl.gov/petsc.</note>
<contexts>
<context position="13458" citStr="Balay et al., 2001" startWordPosition="2162" endWordPosition="2165">timized; results based on random data may or may not carry over to more realistic problems. And, the test problems Minka considers are relatively small (100–500 dimensions). As we have seen, though, algorithms which perform well for small and medium scale problems may not always be applicable to problems with many thousands of dimensions. 3.1 Implementation As a basis for the implementation, we have used PETSc (the “Portable, Extensible Toolkit for Scientific Computation”), a software library designed to ease development of programs which solve large systems of partial differential equations (Balay et al., 2001; Balay et al., 1997; Balay et al., 2002). PETSc offers data structures and routines for parallel and sequential storage, manipulation, and visualization of very large sparse matrices. For any of the estimation techniques, the most expensive operation is computing the probability distribution q and the expectations Eq[f] for each iteration. In order to make use of the facilities provided by PETSc, we can store the training data as a (sparse) matrix F, with rows corresponding to events and columns to features. Then given a parameter vector θ, the unnormalized probabilities ˙q0 are the matrix-ve</context>
</contexts>
<marker>Balay, Buschelman, Gropp, Kaushik, McInnes, Smith, 2001</marker>
<rawString>Satish Balay, Kris Buschelman, William D. Gropp, Dinesh Kaushik, Lois Curfman McInnes, and Barry F. Smith. 2001. PETSc home page. http://www.mcs.anl.gov/petsc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satish Balay</author>
<author>William D Gropp</author>
<author>Lois Curfman McInnes</author>
<author>Barry F Smith</author>
</authors>
<title>PETSc users manual.</title>
<date>2002</date>
<tech>Technical Report ANL-95/11–Revision 2.1.2,</tech>
<institution>Argonne National Laboratory.</institution>
<contexts>
<context position="13499" citStr="Balay et al., 2002" startWordPosition="2170" endWordPosition="2173">y or may not carry over to more realistic problems. And, the test problems Minka considers are relatively small (100–500 dimensions). As we have seen, though, algorithms which perform well for small and medium scale problems may not always be applicable to problems with many thousands of dimensions. 3.1 Implementation As a basis for the implementation, we have used PETSc (the “Portable, Extensible Toolkit for Scientific Computation”), a software library designed to ease development of programs which solve large systems of partial differential equations (Balay et al., 2001; Balay et al., 1997; Balay et al., 2002). PETSc offers data structures and routines for parallel and sequential storage, manipulation, and visualization of very large sparse matrices. For any of the estimation techniques, the most expensive operation is computing the probability distribution q and the expectations Eq[f] for each iteration. In order to make use of the facilities provided by PETSc, we can store the training data as a (sparse) matrix F, with rows corresponding to events and columns to features. Then given a parameter vector θ, the unnormalized probabilities ˙q0 are the matrix-vector product: ˙q0 = expFθ and the feature</context>
</contexts>
<marker>Balay, Gropp, McInnes, Smith, 2002</marker>
<rawString>Satish Balay, William D. Gropp, Lois Curfman McInnes, and Barry F. Smith. 2002. PETSc users manual. Technical Report ANL-95/11–Revision 2.1.2, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J Benson</author>
<author>Jorge J Mor´e</author>
</authors>
<title>A limited memory variable metric method for bound constrained minimization.</title>
<date>2001</date>
<tech>Preprint ANL/ACSP909-0901,</tech>
<institution>Argonne National Laboratory.</institution>
<marker>Benson, Mor´e, 2001</marker>
<rawString>Steven J. Benson and Jorge J. Mor´e. 2001. A limited memory variable metric method for bound constrained minimization. Preprint ANL/ACSP909-0901, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J Benson</author>
<author>Lois Curfman McInnes</author>
<author>Jorge J Mor´e</author>
<author>Jason Sarich</author>
</authors>
<title>TAO users manual.</title>
<date>2002</date>
<tech>Technical Report ANL/MCS-TM-242– Revision 1.4,</tech>
<institution>Argonne National Laboratory.</institution>
<marker>Benson, McInnes, Mor´e, Sarich, 2002</marker>
<rawString>Steven J. Benson, Lois Curfman McInnes, Jorge J. Mor´e, and Jason Sarich. 2002. TAO users manual. Technical Report ANL/MCS-TM-242– Revision 1.4, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<contexts>
<context position="1739" citStr="Berger et al., 1996" startWordPosition="251" endWordPosition="254">m outperformed the other choices. 1 Introduction Maximum entropy (ME) models, variously known as log-linear, Gibbs, exponential, and multinomial logit models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to fields as diverse as computer vision and econometrics. In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications (Abney, 1997; Berger et al., 1996; Ratnaparkhi, 1998; Johnson et al., 1999). A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features. However, the richness of the representations is not without cost. Even modest ME models can require considerable computational resources and very large quantities of annotated training data in order to accurately estimate the model’s parameters. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are usually </context>
<context position="3827" citStr="Berger et al., 1996" startWordPosition="591" endWordPosition="594">ility distribution p over a set of events X which are characterized by a d dimensional feature vector function f : X → Rd. In addition, we have also a set of contexts W and a function Y which partitions the members of X. In the case of a stochastic context-free grammar, for example, X might be the set of possible trees, the feature vectors might represent the number of times each rule applied in the derivation of each tree, W might be the set of possible strings of words, and Y(w) the set of trees whose yield is w ∈ W. A conditional maximum entropy model qθ(x|w) for p has the parametric form (Berger et al., 1996; Chi, 1998; Johnson et al., 1999): expθT f (x)~ q0(x|w) = ∑y∈Y(w) exp(θT f (y)) (1) where θ is a d-dimensional parameter vector and θT f (x) is the inner product of the parameter vector and a feature vector. Given the parametric form of an ME model in (1), fitting an ME model to a collection of training data entails finding values for the parameter vector θ which minimize the Kullback-Leibler divergence between the model q0 and the empirical distribution p: p(x,w)log p(x|w) q0(x|w) or, equivalently, which maximize the log likelihood: L(θ) = ∑ p(w,x)logq0(x|w) (2) w,x ratio of Ep[f] to Eq(k)[f</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Gertjan van Noord</author>
<author>Robert Malouf</author>
</authors>
<title>Alpino: wide coverage computational analysis of Dutch. In</title>
<date>2001</date>
<booktitle>Computational Linguistics in the Netherlands</booktitle>
<pages>45--59</pages>
<editor>W. Daelemans, K. Sima’an, J. Veenstra, and J. Zavrel, editors,</editor>
<location>Rodolpi, Amsterdam.</location>
<marker>Bouma, van Noord, Malouf, 2001</marker>
<rawString>Gosse Bouma, Gertjan van Noord, and Robert Malouf. 2001. Alpino: wide coverage computational analysis of Dutch. In W. Daelemans, K. Sima’an, J. Veenstra, and J. Zavrel, editors, Computational Linguistics in the Netherlands 2000, pages 45– 59. Rodolpi, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyi Chi</author>
</authors>
<title>Probability models for complex systems.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="3838" citStr="Chi, 1998" startWordPosition="595" endWordPosition="596">over a set of events X which are characterized by a d dimensional feature vector function f : X → Rd. In addition, we have also a set of contexts W and a function Y which partitions the members of X. In the case of a stochastic context-free grammar, for example, X might be the set of possible trees, the feature vectors might represent the number of times each rule applied in the derivation of each tree, W might be the set of possible strings of words, and Y(w) the set of trees whose yield is w ∈ W. A conditional maximum entropy model qθ(x|w) for p has the parametric form (Berger et al., 1996; Chi, 1998; Johnson et al., 1999): expθT f (x)~ q0(x|w) = ∑y∈Y(w) exp(θT f (y)) (1) where θ is a d-dimensional parameter vector and θT f (x) is the inner product of the parameter vector and a feature vector. Given the parametric form of an ME model in (1), fitting an ME model to a collection of training data entails finding values for the parameter vector θ which minimize the Kullback-Leibler divergence between the model q0 and the empirical distribution p: p(x,w)log p(x|w) q0(x|w) or, equivalently, which maximize the log likelihood: L(θ) = ∑ p(w,x)logq0(x|w) (2) w,x ratio of Ep[f] to Eq(k)[f], with the</context>
</contexts>
<marker>Chi, 1998</marker>
<rawString>Zhiyi Chi. 1998. Probability models for complex systems. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Ann. Math. Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="6437" citStr="Darroch and Ratcliff (1972)" startWordPosition="1031" endWordPosition="1034">stimated probability distribution q(k) and the empirical distribution p. We continue until successive improvements fail to yield a sufficiently large decrease in the divergence. While all parameter estimation algorithms we will consider take the same general form, the method for computing the updates δ(k) at each search step differs substantially. As we shall see, this difference can have a dramatic impact on the number of updates required to reach convergence. 2.1 Iterative Scaling One popular method for iteratively refining the model parameters is Generalized Iterative Scaling (GIS), due to Darroch and Ratcliff (1972). An extension of Iterative Proportional Fitting (Deming and Stephan, 1940), GIS scales the probability distribution q(k) by a factor proportional to the where M(x) is the sum of the feature values for an event x in the training data. This is a polynomial in exp(δ(k)), and the solution can be found straightforwardly using, for example, the Newton-Raphson method. 2.2 First order methods Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables. Their primary strength is that on each iteration they only require computation of th</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. Ann. Math. Statistics, 43:1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--380</pages>
<contexts>
<context position="5127" citStr="Pietra et al. (1997)" startWordPosition="821" endWordPosition="824">(a condition which can be easily satisfied by the addition of a correction feature). We can adapt GIS to estimate the model parameters θ rather than the model probabilities q, yielding the update rule: Ep[f] ! δ(k) = log Eq(k)[f] The step size, and thus the rate of convergence, depends on the constant C: the larger the value of C, the smaller the step size. In case not all rows of the training data sum to a constant, the addition of a correction feature effectively slows convergence to match the most difficult case. To avoid this slowed convergence and the need for a correction feature, Della Pietra et al. (1997) propose an Improved Iterative Scaling (IIS) algorithm, whose update rule is the solution to the equation: D(p||q0) = ∑ w,x 1 C The gradient of the log likelihood function, or the Ep[f] = ∑w,x p(w)q(k)(x|w)f(x)exp(M(x)δ(k)) vector of its first derivatives with respect to the parameter θ is: G(θ) = Ep[f]−Eqθ[f] (3) Since the likelihood function (2) is concave over the parameter space, it has a global maximum where the gradient is zero. Unfortunately, simply setting G(θ) = 0 and solving for θ does not yield a closed form solution, so we proceed iteratively. At each step, we adjust an estimate of</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W E Deming</author>
<author>F F Stephan</author>
</authors>
<title>On a least squares adjustment of a sampled frequency table when the expected marginals are known. Annals ofMathematical Statistics,</title>
<date>1940</date>
<pages>11--427</pages>
<contexts>
<context position="6512" citStr="Deming and Stephan, 1940" startWordPosition="1041" endWordPosition="1045">ontinue until successive improvements fail to yield a sufficiently large decrease in the divergence. While all parameter estimation algorithms we will consider take the same general form, the method for computing the updates δ(k) at each search step differs substantially. As we shall see, this difference can have a dramatic impact on the number of updates required to reach convergence. 2.1 Iterative Scaling One popular method for iteratively refining the model parameters is Generalized Iterative Scaling (GIS), due to Darroch and Ratcliff (1972). An extension of Iterative Proportional Fitting (Deming and Stephan, 1940), GIS scales the probability distribution q(k) by a factor proportional to the where M(x) is the sum of the feature values for an event x in the training data. This is a polynomial in exp(δ(k)), and the solution can be found straightforwardly using, for example, the Newton-Raphson method. 2.2 First order methods Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables. Their primary strength is that on each iteration they only require computation of the expected values Eq(k). They do not depend on evaluation of the gradient o</context>
</contexts>
<marker>Deming, Stephan, 1940</marker>
<rawString>W.E. Deming and F.F. Stephan. 1940. On a least squares adjustment of a sampled frequency table when the expected marginals are known. Annals ofMathematical Statistics, 11:427–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth D Dolan</author>
<author>Jorge J Mor´e</author>
</authors>
<title>Benchmarking optimization software with performance profiles.</title>
<date>2002</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>91--201</pages>
<marker>Dolan, Mor´e, 2002</marker>
<rawString>Elizabeth D. Dolan and Jorge J. Mor´e. 2002. Benchmarking optimization software with performance profiles. Mathematical Programming, 91:201–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL,</booktitle>
<pages>535--541</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="1781" citStr="Johnson et al., 1999" startWordPosition="257" endWordPosition="260">oduction Maximum entropy (ME) models, variously known as log-linear, Gibbs, exponential, and multinomial logit models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to fields as diverse as computer vision and econometrics. In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications (Abney, 1997; Berger et al., 1996; Ratnaparkhi, 1998; Johnson et al., 1999). A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features. However, the richness of the representations is not without cost. Even modest ME models can require considerable computational resources and very large quantities of annotated training data in order to accurately estimate the model’s parameters. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are usually quite large, and frequently contain hundre</context>
<context position="3861" citStr="Johnson et al., 1999" startWordPosition="597" endWordPosition="600">of events X which are characterized by a d dimensional feature vector function f : X → Rd. In addition, we have also a set of contexts W and a function Y which partitions the members of X. In the case of a stochastic context-free grammar, for example, X might be the set of possible trees, the feature vectors might represent the number of times each rule applied in the derivation of each tree, W might be the set of possible strings of words, and Y(w) the set of trees whose yield is w ∈ W. A conditional maximum entropy model qθ(x|w) for p has the parametric form (Berger et al., 1996; Chi, 1998; Johnson et al., 1999): expθT f (x)~ q0(x|w) = ∑y∈Y(w) exp(θT f (y)) (1) where θ is a d-dimensional parameter vector and θT f (x) is the inner product of the parameter vector and a feature vector. Given the parametric form of an ME model in (1), fitting an ME model to a collection of training data entails finding values for the parameter vector θ which minimize the Kullback-Leibler divergence between the model q0 and the empirical distribution p: p(x,w)log p(x|w) q0(x|w) or, equivalently, which maximize the log likelihood: L(θ) = ∑ p(w,x)logq0(x|w) (2) w,x ratio of Ep[f] to Eq(k)[f], with the restriction that ∑j fj</context>
<context position="20222" citStr="Johnson et al. (1999)" startWordPosition="3276" endWordPosition="3279">ost cases, more or less the same for all of the algorithms. Some variability is to be expected—all of the data sets being considered here are badly ill-conditioned, and many different models will yield the same likelihood. In a few cases, however, the prediction accuracy differs more substantially. For the two SAVG data sets (‘rules’ and ‘lex’), GIS has a small advantage over the other methods. More dramatically, both iterative scaling methods perform very poorly on the ‘shallow’ dataset. In this case, the training data is very sparse. Many features are nearly ‘pseudo-minimal’ in the sense of Johnson et al. (1999), and so receive weights approaching −∞. Smoothing the reference probabilities would likely improve the results for all of the methods and reduce the observed differences. However, this does suggest that gradient-based methods are robust to certain problems with the training data. Finally, the most significant lesson to be drawn from these results is that, with the exception of steepest ascent, gradient-based methods outperform iterative scaling by a wide margin for almost all the datasets, as measured by both number of function evaluations and by the total elapsed time. And, in each case, the</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In Proceedings of the 37th Annual Meeting of the ACL, pages 535–541, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Bernhard Suhm</author>
</authors>
<title>Cluster expansions and iterative scaling for maximum entropy language models.</title>
<date>1996</date>
<booktitle>Maximum Entropy and Bayesian Methods.</booktitle>
<editor>In K. Hanson and R. Silver, editors,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="15663" citStr="Lafferty and Suhm, 1996" startWordPosition="2523" endWordPosition="2527">, we need to use the same stopping rule for each algorithm. For these experiments, we judged that convergence was reached when the relative change in the loglikelihood between iterations fell below a predetermined threshold. That is, each run was stopped when: &lt; ε (6) L(θ(k)) where the relative tolerance ε = 10−7. For any particular application, this may or may not be an appropriate stopping rule, but is only used here for purposes of comparison. Finally, it should be noted that in the current implementation, we have not applied any of the possible optimizations that appear in the literature (Lafferty and Suhm, 1996; Wu and Khudanpur, 2000; Lafferty et al., 2001) to speed up normalization of the probability distribution q. These improvements take advantage of a model’s structure to simplify the evaluation of the denominator in (1). The particular data sets examined here are unstructured, and such optimizations are unlikely to give any improvement. However, when these optimizations are appropriate, they will give a proportional speed-up to all of the algorithms. Thus, the use of such optimizations is independent of the choice of parameter estimation method. 3.2 Experiments To compare the algorithms descri</context>
</contexts>
<marker>Lafferty, Suhm, 1996</marker>
<rawString>John Lafferty and Bernhard Suhm. 1996. Cluster expansions and iterative scaling for maximum entropy language models. In K. Hanson and R. Silver, editors, Maximum Entropy and Bayesian Methods. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Fernando Pereira</author>
<author>Andrew McCallum</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="15711" citStr="Lafferty et al., 2001" startWordPosition="2532" endWordPosition="2535">algorithm. For these experiments, we judged that convergence was reached when the relative change in the loglikelihood between iterations fell below a predetermined threshold. That is, each run was stopped when: &lt; ε (6) L(θ(k)) where the relative tolerance ε = 10−7. For any particular application, this may or may not be an appropriate stopping rule, but is only used here for purposes of comparison. Finally, it should be noted that in the current implementation, we have not applied any of the possible optimizations that appear in the literature (Lafferty and Suhm, 1996; Wu and Khudanpur, 2000; Lafferty et al., 2001) to speed up normalization of the probability distribution q. These improvements take advantage of a model’s structure to simplify the evaluation of the denominator in (1). The particular data sets examined here are unstructured, and such optimizations are unlikely to give any improvement. However, when these optimizations are appropriate, they will give a proportional speed-up to all of the algorithms. Thus, the use of such optimizations is independent of the choice of parameter estimation method. 3.2 Experiments To compare the algorithms described in §2, we applied the implementation outline</context>
</contexts>
<marker>Lafferty, Pereira, McCallum, 2001</marker>
<rawString>John Lafferty, Fernando Pereira, and Andrew McCallum. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Algorithms for maximum-likelihood logistic regression.</title>
<date>2001</date>
<tech>Statistics Tech Report 758, CMU.</tech>
<contexts>
<context position="12400" citStr="Minka (2001)" startWordPosition="1999" endWordPosition="2000">he problem to be solved. Worst-case analysis typically &apos;Space constraints preclude a more detailed discussion of these methods here. For algorithmic details and theoretical analysis of first and second order methods, see, e.g., Nocedal (1997) or Nocedal and Wright (1999). does not reflect the actual behavior on actual problems. Therefore, in order to evaluate the performance of the optimization techniques sketched in previous section when applied to the problem of parameter estimation, we need to compare the performance of actual implementations on realistic data sets (Dolan and Mor´e, 2002). Minka (2001) offers a comparison of iterative scaling with other algorithms for parameter estimation in logistic regression, a problem similar to the one considered here, but it is difficult to transfer Minka’s results to ME models. For one, he evaluates the algorithms with randomly generated training data. However, the performance and accuracy of optimization algorithms can be sensitive to the specific numerical properties of the function being optimized; results based on random data may or may not carry over to more realistic problems. And, the test problems Minka considers are relatively small (100–500</context>
</contexts>
<marker>Minka, 2001</marker>
<rawString>Thomas P. Minka. 2001. Algorithms for maximum-likelihood logistic regression. Statistics Tech Report 758, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen J Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>1999</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="12059" citStr="Nocedal and Wright (1999)" startWordPosition="1942" endWordPosition="1945"> and δ(k). Since in practical applications values of m between 3 and 10 suffice, this can offer a substantial savings in storage requirements over variable metric methods, while still giving favorable convergence properties.1 3 Comparing estimation techniques The performance of optimization algorithms is highly dependent on the specific properties of the problem to be solved. Worst-case analysis typically &apos;Space constraints preclude a more detailed discussion of these methods here. For algorithmic details and theoretical analysis of first and second order methods, see, e.g., Nocedal (1997) or Nocedal and Wright (1999). does not reflect the actual behavior on actual problems. Therefore, in order to evaluate the performance of the optimization techniques sketched in previous section when applied to the problem of parameter estimation, we need to compare the performance of actual implementations on realistic data sets (Dolan and Mor´e, 2002). Minka (2001) offers a comparison of iterative scaling with other algorithms for parameter estimation in logistic regression, a problem similar to the one considered here, but it is difficult to transfer Minka’s results to ME models. For one, he evaluates the algorithms w</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Jorge Nocedal and Stephen J. Wright. 1999. Numerical Optimization. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
</authors>
<title>Large scale unconstrained optimization.</title>
<date>1997</date>
<booktitle>The State of the Art in Numerical Analysis,</booktitle>
<pages>311--338</pages>
<editor>In A. Watson and I. Duff, editors,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="12030" citStr="Nocedal (1997)" startWordPosition="1939" endWordPosition="1940">s m values of y(k) and δ(k). Since in practical applications values of m between 3 and 10 suffice, this can offer a substantial savings in storage requirements over variable metric methods, while still giving favorable convergence properties.1 3 Comparing estimation techniques The performance of optimization algorithms is highly dependent on the specific properties of the problem to be solved. Worst-case analysis typically &apos;Space constraints preclude a more detailed discussion of these methods here. For algorithmic details and theoretical analysis of first and second order methods, see, e.g., Nocedal (1997) or Nocedal and Wright (1999). does not reflect the actual behavior on actual problems. Therefore, in order to evaluate the performance of the optimization techniques sketched in previous section when applied to the problem of parameter estimation, we need to compare the performance of actual implementations on realistic data sets (Dolan and Mor´e, 2002). Minka (2001) offers a comparison of iterative scaling with other algorithms for parameter estimation in logistic regression, a problem similar to the one considered here, but it is difficult to transfer Minka’s results to ME models. For one, </context>
</contexts>
<marker>Nocedal, 1997</marker>
<rawString>Jorge Nocedal. 1997. Large scale unconstrained optimization. In A. Watson and I. Duff, editors, The State of the Art in Numerical Analysis, pages 311–338. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
</authors>
<title>Shallow parsing using noisy and non-stationary training material.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--695</pages>
<contexts>
<context position="17067" citStr="Osborne, 2002" startWordPosition="2739" endWordPosition="2740">and ‘lex’ datasets are examples |L(θ(k)) −L(θ(k−1))| dataset classes contexts features non-zeros rules 29,602 2,525 246 732,384 lex 42,509 2,547 135,182 3,930,406 summary 24,044 12,022 198,467 396,626 shallow 8,625,782 375,034 264,142 55,192,723 Table 1: Datasets used in experiments of stochastic attribute value grammars, one with a small set of SCFG-like features, and with a very large set of fine-grained lexical features (Bouma et al., 2001). The ‘summary’ dataset is part of a sentence extraction task (Osborne, to appear), and the ‘shallow’ dataset is drawn from a text chunking application (Osborne, 2002). These datasets vary widely in their size and composition, and are representative of the kinds of datasets typically encountered in applying ME models to NLP classification tasks. The results of applying each of the parameter estimation algorithms to each of the datasets is summarized in Table 2. For each run, we report the KL divergence between the fitted model and the training data at convergence, the prediction accuracy of fitted model on a held-out test set (the fraction of contexts for which the event with the highest probability under the model also had the highest probability under the</context>
</contexts>
<marker>Osborne, 2002</marker>
<rawString>Miles Osborne. 2002. Shallow parsing using noisy and non-stationary training material. Journal of Machine Learning Research, 2:695–719.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Miles Osborne</author>
</authors>
<title>to appear. Using maximum entropy for sentence extraction.</title>
<booktitle>In Proceedings of the ACL 2002 Workshop on Automatic Summarization,</booktitle>
<location>Philadelphia.</location>
<marker>Osborne, </marker>
<rawString>Miles Osborne. to appear. Using maximum entropy for sentence extraction. In Proceedings of the ACL 2002 Workshop on Automatic Summarization, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum entropy models for natural language ambiguity resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1758" citStr="Ratnaparkhi, 1998" startWordPosition="255" endWordPosition="256">her choices. 1 Introduction Maximum entropy (ME) models, variously known as log-linear, Gibbs, exponential, and multinomial logit models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to fields as diverse as computer vision and econometrics. In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications (Abney, 1997; Berger et al., 1996; Ratnaparkhi, 1998; Johnson et al., 1999). A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features. However, the richness of the representations is not without cost. Even modest ME models can require considerable computational resources and very large quantities of annotated training data in order to accurately estimate the model’s parameters. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are usually quite large, and fr</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum entropy models for natural language ambiguity resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Wu</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient training methods for maximum entropy language modelling.</title>
<date>2000</date>
<booktitle>In Proceedings of ICSLP2000,</booktitle>
<volume>3</volume>
<pages>114--117</pages>
<location>Beijing.</location>
<contexts>
<context position="15687" citStr="Wu and Khudanpur, 2000" startWordPosition="2528" endWordPosition="2531"> stopping rule for each algorithm. For these experiments, we judged that convergence was reached when the relative change in the loglikelihood between iterations fell below a predetermined threshold. That is, each run was stopped when: &lt; ε (6) L(θ(k)) where the relative tolerance ε = 10−7. For any particular application, this may or may not be an appropriate stopping rule, but is only used here for purposes of comparison. Finally, it should be noted that in the current implementation, we have not applied any of the possible optimizations that appear in the literature (Lafferty and Suhm, 1996; Wu and Khudanpur, 2000; Lafferty et al., 2001) to speed up normalization of the probability distribution q. These improvements take advantage of a model’s structure to simplify the evaluation of the denominator in (1). The particular data sets examined here are unstructured, and such optimizations are unlikely to give any improvement. However, when these optimizations are appropriate, they will give a proportional speed-up to all of the algorithms. Thus, the use of such optimizations is independent of the choice of parameter estimation method. 3.2 Experiments To compare the algorithms described in §2, we applied th</context>
</contexts>
<marker>Wu, Khudanpur, 2000</marker>
<rawString>Jun Wu and Sanjeev Khudanpur. 2000. Efficient training methods for maximum entropy language modelling. In Proceedings of ICSLP2000, volume 3, pages 114–117, Beijing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>