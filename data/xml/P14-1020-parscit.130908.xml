<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000082">
<title confidence="0.849999">
Sparser, Better, Faster GPU Parsing
</title>
<author confidence="0.837652">
David Hall Taylor Berg-Kirkpatrick John Canny Dan Klein
</author>
<affiliation confidence="0.9185505">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.997593">
{dlwh,tberg,jfc,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970357142857">
Due to their origin in computer graph-
ics, graphics processing units (GPUs)
are highly optimized for dense problems,
where the exact same operation is applied
repeatedly to all data points. Natural lan-
guage processing algorithms, on the other
hand, are traditionally constructed in ways
that exploit structural sparsity. Recently,
Canny et al. (2013) presented an approach
to GPU parsing that sacrifices traditional
sparsity in exchange for raw computa-
tional power, obtaining a system that can
compute Viterbi parses for a high-quality
grammar at about 164 sentences per sec-
ond on a mid-range GPU. In this work,
we reintroduce sparsity to GPU parsing
by adapting a coarse-to-fine pruning ap-
proach to the constraints of a GPU. The
resulting system is capable of computing
over 404 Viterbi parses per second—more
than a 2x speedup—on the same hard-
ware. Moreover, our approach allows us
to efficiently implement less GPU-friendly
minimum Bayes risk inference, improv-
ing throughput for this more accurate algo-
rithm from only 32 sentences per second
unpruned to over 190 sentences per second
using pruning—nearly a 6x speedup.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999014">
Because NLP models typically treat sentences in-
dependently, NLP problems have long been seen
as “embarrassingly parallel” – large corpora can
be processed arbitrarily fast by simply sending dif-
ferent sentences to different machines. However,
recent trends in computer architecture, particularly
the development of powerful “general purpose”
GPUs, have changed the landscape even for prob-
lems that parallelize at the sentence level. First,
classic single-core processors and main memory
architectures are no longer getting substantially
faster over time, so speed gains must now come
from parallelism within a single machine. Second,
compared to CPUs, GPUs devote a much larger
fraction of their computational power to actual
arithmetic. Since tasks like parsing boil down to
repeated read-multiply-write loops, GPUs should
be many times more efficient in time, power, or
cost. The challenge is that GPUs are not a good
fit for the kinds of sparse computations that most
current CPU-based NLP algorithms rely on.
Recently, Canny et al. (2013) proposed a GPU
implementation of a constituency parser that sac-
rifices all sparsity in exchange for the sheer horse-
power that GPUs can provide. Their system uses a
grammar based on the Berkeley parser (Petrov and
Klein, 2007) (which is particularly amenable to
GPU processing), “compiling” the grammar into a
sequence of GPU kernels that are applied densely
to every item in the parse chart. Together these
kernels implement the Viterbi inside algorithm.
On a mid-range GPU, their system can compute
Viterbi derivations at 164 sentences per second on
sentences of length 40 or less (see timing details
below).
In this paper, we develop algorithms that can
exploit sparsity on a GPU by adapting coarse-to-
fine pruning to a GPU setting. On a CPU, pruning
methods can give speedups of up to 100x. Such
extreme speedups over a dense GPU baseline cur-
rently seem unlikely because fine-grained sparsity
appears to be directly at odds with dense paral-
lelism. However, in this paper, we present a sys-
tem that finds a middle ground, where some level
of sparsity can be maintained without losing the
parallelism of the GPU. We use a coarse-to-fine
approach as in Petrov and Klein (2007), but with
only one coarse pass. Figure 1 shows an overview
of the approach: we first parse densely with a
coarse grammar and then parse sparsely with the
</bodyText>
<page confidence="0.972164">
208
</page>
<note confidence="0.831077">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 208–217,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999851142857143">
fine grammar, skipping symbols that the coarse
pass deemed sufficiently unlikely. Using this ap-
proach, we see a gain of more than 2x over the
dense GPU implementation, resulting in overall
speeds of up to 404 sentences per second. For
comparison, the publicly available CPU imple-
mentation of Petrov and Klein (2007) parses ap-
proximately 7 sentences per second per core on a
modern CPU.
A further drawback of the dense approach in
Canny et al. (2013) is that it only computes
Viterbi parses. As with other grammars with
a parse/derivation distinction, the grammars of
Petrov and Klein (2007) only achieve their full
accuracy using minimum-Bayes-risk parsing, with
improvements of over 1.5 F1 over best-derivation
Viterbi parsing on the Penn Treebank (Marcus et
al., 1993). To that end, we extend our coarse-to-
fine GPU approach to computing marginals, along
the way proposing a new way to exploit the coarse
pass to avoid expensive log-domain computations
in the fine pass. We then implement minimum-
Bayes-risk parsing via the max recall algorithm of
Goodman (1996). Without the coarse pass, the
dense marginal computation is not efficient on a
GPU, processing only 32 sentences per second.
However, our approach allows us to process over
190 sentences per second, almost a 6x speedup.
</bodyText>
<sectionHeader confidence="0.971687" genericHeader="introduction">
2 A Note on Experiments
</sectionHeader>
<bodyText confidence="0.961431379310345">
We build up our approach incrementally, with ex-
periments interspersed throughout the paper, and
summarized in Tables 1 and 2. In this paper, we
focus our attention on current-generation NVIDIA
GPUs. Many of the ideas described here apply to
other GPUs (such as those from AMD), but some
specifics will differ. All experiments are run with
an NVIDIA GeForce GTX 680, a mid-range GPU
that costs around $500 at time of writing. Unless
otherwise noted, all experiments are conducted on
sentences of length &lt; 40 words, and we estimate
times based on batches of 20K sentences.1 We
should note that our experimental condition dif-
fers from that of Canny et al. (2013): they evaluate
on sentences of length &lt; 30. Furthermore, they
1The implementation of Canny et al. (2013) cannot han-
dle batches so large, and so we tested it on batches of 1200
sentences. Our reimplementation is approximately the same
speed for the same batch sizes. For batches of 20K sentences,
we used sentences from the training set. We verified that there
was no significant difference in speed for sentences from the
training set and from the test set.
use two NVIDIA GeForce GTX 690s—each of
which is essentially a repackaging of two 680s—
meaning that our system and experiments would
run approximately four times faster on their hard-
ware. (This expected 4x factor is empirically con-
sistent with the result of running their system on
our hardware.)
</bodyText>
<sectionHeader confidence="0.971349" genericHeader="method">
3 Sparsity and CPUs
</sectionHeader>
<bodyText confidence="0.991361085714286">
One successful approach for speeding up con-
stituency parsers has been to use coarse-to-fine
inference (Charniak et al., 2006). In coarse-to-
fine inference, we have a sequence of increasingly
complex grammars Gt. Typically, each succes-
sive grammar Gt is a refinement of the preceding
grammar Gt−1. That is, for each symbol A,, in
the fine grammar, there is some symbol A in the
coarse grammar. For instance, in a latent variable
parser, the coarse grammar would have symbols
like NP, V P, etc., and the fine pass would have
refined symbols NP0, NP1, V P4, and so on.
In coarse-to-fine inference, one applies the
grammars in sequence, computing inside and out-
side scores. Next, one computes (max) marginals
for every labeled span (A, i, j) in a sentence.
These max marginals are used to compute a prun-
ing mask for every span (i, j). This mask is the set
of symbols allowed for that span. Then, in the next
pass, one only processes rules that are licensed by
the pruning mask computed at the previous level.
This approach works because a low quality
coarse grammar can still reliably be used to prune
many symbols from the fine chart without loss of
accuracy. Petrov and Klein (2007) found that over
98% of symbols can be pruned from typical charts
using a simple X-bar grammar without any loss
of accuracy. Thus, the vast majority of rules can
be skipped, and therefore most computation can
be avoided. It is worth pointing out that although
98% of labeled spans can be skipped due to X-bar
pruning, we found that only about 79% of binary
rule applications can be skipped, because the un-
pruned symbols tend to be the ones with a larger
grammar footprint.
</bodyText>
<sectionHeader confidence="0.998224" genericHeader="method">
4 GPU Architectures
</sectionHeader>
<bodyText confidence="0.9815566">
Unfortunately, the standard coarse-to-fine ap-
proach does not naively translate to GPU archi-
tectures. GPUs work by executing thousands of
threads at once, but impose the constraint that
large blocks of threads must be executing the same
</bodyText>
<page confidence="0.996316">
209
</page>
<figure confidence="0.9975826">
GPU
CPU
Sentences
Masks
Queue
Trees
Queue
RAM
Parse Charts
Work Array
Instruction Cache
Grammar
RAM
Masks
Queue
</figure>
<figureCaption confidence="0.9394166">
Figure 1: Overview of the architecture of our system, which is an extension of Canny et al. (2013)’s
system. The GPU and CPU communicate via a work queue, which ferries parse items from the CPU to
the GPU. Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask
that is used by the CPU when deciding which items to queue during the fine pass. The original system
of Canny et al. (2013) only used the fine pass, with no pruning.
</figureCaption>
<bodyText confidence="0.985629945945946">
instructions in lockstep, differing only in their in-
put data. Thus sparsely skipping rules and sym-
bols will not save any work. Indeed, it may ac-
tually slow the system down. In this section, we
provide an overview of GPU architectures, focus-
ing on the details that are relevant to building an
efficient parser.
The large number of threads that a GPU exe-
cutes are packaged into blocks of 32 threads called
warps. All threads in a warp must execute the
same instruction at every clock cycle: if one thread
takes a branch the others do not, then all threads in
the warp must follow both code paths. This situa-
tion is called warp divergence. Because all threads
execute all code paths that any thread takes, time
can only be saved if an entire warp agrees to skip
any particular branch.
NVIDIA GPUs have 8-15 processors called
streaming multi-processors or SMs.2 Each SM
can process up to 48 different warps at a time:
it interleaves the execution of each warp, so that
when one warp is stalled another warp can exe-
cute. Unlike threads within a single warp, the 48
warps do not have to execute the same instruc-
tions. However, the memory architecture is such
that they will be faster if they access related mem-
ory locations.
2Older hardware (600 series or older) has 8 SMs. Newer
hardware has more.
A further consideration is that the number of
registers available to a thread in a warp is rather
limited compared to a CPU. On the 600 series,
maximum occupancy can only be achieved if each
thread uses at most 63 registers (Nvidia, 2008).3
Registers are many times faster than variables lo-
cated in thread-local memory, which is actually
the same speed as global memory.
</bodyText>
<sectionHeader confidence="0.59208" genericHeader="method">
5 Anatomy of a Dense GPU Parser
</sectionHeader>
<bodyText confidence="0.999931411764706">
This architecture environment puts very different
constraints on parsing algorithms from a CPU en-
vironment. Canny et al. (2013) proposed an imple-
mentation of a PCFG parser that sacrifices stan-
dard sparse methods like coarse-to-fine pruning,
focusing instead on maximizing the instruction
and memory throughput of the parser. They as-
sume that they are parsing many sentences at once,
with throughput being more important than la-
tency. In this section, we describe their dense algo-
rithm, which we take as the baseline for our work;
we present it in a way that sets up the changes to
follow.
At the top level, the CPU and GPU communi-
cate via a work queue of parse items of the form
(s, i, k, j), where s is an identifier of a sentence,
i is the start of a span, k is the split point, and j
</bodyText>
<footnote confidence="0.850278">
3A thread can use more registers than this, but the full
complement of 48 warps cannot execute if too many are used.
</footnote>
<page confidence="0.985126">
210
</page>
<table confidence="0.999714888888889">
Clustering Pruning Sent/Sec Speedup
Canny et al. – 164.0 –
Reimpl – 192.9 1.0x
Reimpl Empty, Coarse 185.5 0.96x
Reimpl Labeled, Coarse 187.5 0.97x
Parent – 158.6 0.82x
Parent Labeled, Coarse 278.9 1.4x
Parent Labeled, 1-split 404.7 2.1x
Parent Labeled, 2-split 343.6 1.8x
</table>
<tableCaption confidence="0.999051">
Table 1: Performance numbers for computing
</tableCaption>
<bodyText confidence="0.9880415">
Viterbi inside charts on 20,000 sentences of length
≤40 from the Penn Treebank. All times are
measured on an NVIDIA GeForce GTX 680.
‘Reimpl’ is our reimplementation of their ap-
proach. Speedups are measured in reference to this
reimplementation. See Section 7 for discussion of
the clustering algorithms and Section 6 for a de-
scription of the pruning methods. The Canny et al.
(2013) system is benchmarked on a batch size of
1200 sentences, the others on 20,000.
is the end point. The GPU takes large numbers of
parse items and applies the entire grammar to them
in parallel. These parse items are enqueued in or-
der of increasing span size, blocking until all items
of a given length are complete. This approach is
diagrammed in Figure 2.
Because all rules are applied to all parse items,
all threads are executing the same sequence of in-
structions. Thus, there is no concern of warp di-
vergence.
</bodyText>
<subsectionHeader confidence="0.977586">
5.1 Grammar Compilation
</subsectionHeader>
<bodyText confidence="0.99998375">
One important feature of Canny et al. (2013)’s sys-
tem is grammar compilation. Because registers
are so much faster than thread-local memory, it
is critical to keep as many variables in registers
as possible. One way to accomplish this is to un-
roll loops at compilation time. Therefore, they in-
lined the iteration over the grammar directly into
the GPU kernels (i.e. the code itself), which al-
lows the compiler to more effectively use all of its
registers.
However, register space is limited on GPUs.
Because the Berkeley grammar is so large, the
compiler is not able to efficiently schedule all of
the operations in the grammar, resulting in regis-
ter spills. Canny et al. (2013) found they had to
partition the grammar into multiple different ker-
nels. We discuss this partitioning in more detail in
Section 7. However, in short, the entire grammar
G is broken into multiple clusters Gi where each
rule belongs to exactly one cluster.
</bodyText>
<equation confidence="0.7097305">
Grammar Queue
(i, k, j)
</equation>
<figureCaption confidence="0.713898">
Figure 2: Schematic representation of the work
queue used in Canny et al. (2013). The Viterbi
inside loop for the grammar is inlined into a ker-
nel. The kernel is applied to all items in the queue
in a blockwise manner.
Figure 3: Schematic representation of the work
</figureCaption>
<bodyText confidence="0.996541416666667">
queue and grammar clusters used in the fine pass
of our work. Here, the rules of the grammar are
clustered by their coarse parent symbol. We then
have multiple work queues, with parse items only
being enqueued if the span (i, j) allows that sym-
bol in its pruning mask.
All in all, Canny et al. (2013)’s system is able
to compute Viterbi charts at 164 sentences per sec-
ond, for sentences up to length 40. On larger batch
sizes, our reimplementation of their approach is
able to achieve 193 sentences per second on the
same hardware. (See Table 1.)
</bodyText>
<sectionHeader confidence="0.938793" genericHeader="method">
6 Pruning on a GPU
</sectionHeader>
<bodyText confidence="0.988248">
Now we turn to the algorithmic and architectural
changes in our approach. First, consider trying to
</bodyText>
<figure confidence="0.713513066666666">
NP VP
S
VP
PP
NP
(0, , 3)
(0, 2, 3)
( , 2,4)
( , 3,4)
(2, 3, )
(2, 4, )
Grammar Clusters Queues
(i, k, j)
VP
NP
PP
VB
DT NN
IN
VP
VP
VP
NP
NP
NP
PP
PP
PP
NP
NP
</figure>
<equation confidence="0.906991375">
(0, 2, 3)
(2, 4, )
(3, 4, )
( ,3,4)
( ,2,4)
(3, , )
(0, , 3)
( , 2,4)
</equation>
<page confidence="0.989998">
211
</page>
<bodyText confidence="0.99907146031746">
directly apply the coarse-to-fine method sketched
in Section 3 to the dense baseline described above.
The natural implementation would be for each
thread to check if each rule is licensed before
applying it. However, we would only avoid the
work of applying the rule if all threads in the warp
agreed to skip it. Since each thread in the warp is
processing a different span (perhaps even from a
different sentence), consensus from all 32 threads
on any skip would be unlikely.
Another approach would be to skip enqueu-
ing any parse item (s, i, k, j) where the pruning
mask for any of (i, j), (i, k), or (k, j) is entirely
empty (i.e. all symbols are pruned in this cell by
the coarse grammar). However, our experiments
showed that only 40% of parse items are pruned in
this manner. Because of the overhead associated
with creating pruning masks and the further over-
head of GPU communication, we found that this
method did not actually produce any time savings
at all. The result is a parsing speed of 185.5 sen-
tences per second, as shown in Table 1 on the row
labeled ‘Reimpl’ with ‘Empty, Coarse’ pruning.
Instead, we take advantage of the partitioned
structure of the grammar and organize our com-
putation around the coarse symbol set. Recall that
the baseline already partitions the grammar G into
rule clusters GZ to improve register sharing. (See
Section 7 for more on the baseline clustering.) We
create a separate work queue for each partition.
We call each such queue a labeled work queue, and
each one only queues items to which some rule in
the corresponding partition applies. We call the set
of coarse symbols for a partition (and therefore the
corresponding labeled work queue) a signature.
During parsing, we only enqueue items
(s, i, k, j) to a labeled queue if two conditions are
met. First, the span (i, j)’s pruning mask must
have a non-empty intersection with the signature
of the queue. Second, the pruning mask for the
children (i, k) and (k, j) must be non-empty.
Once on the GPU, parse items are processed us-
ing the same style of compiled kernel as in Canny
et al. (2013). Because the entire partition (though
not necessarily the entire grammar) is applied to
each item in the queue, we still do not need to
worry about warp divergence.
At the top level, our system first computes prun-
ing masks with a coarse grammar. Then it pro-
cesses the same sentences with the fine gram-
mar. However, to the extent that the signatures
are small, items can be selectively queued only to
certain queues. This approach is diagrammed in
Figure 3.
We tested our new pruning approach using an
X-bar grammar as the coarse pass. The result-
ing speed is 187.5 sentences per second, labeled
in Table 1 as row labeled ‘Reimpl’ with ‘Labeled,
Coarse’ pruning. Unfortunately, this approach
again does not produce a speedup relative to our
reimplemented baseline. To improve upon this re-
sult, we need to consider how the grammar clus-
tering interacts with the coarse pruning phase.
</bodyText>
<sectionHeader confidence="0.994723" genericHeader="method">
7 Grammar Clustering
</sectionHeader>
<bodyText confidence="0.999987027027027">
Recall that the rules in the grammar are partitioned
into a set of clusters, and that these clusters are
further divided into subclusters. How can we best
cluster and subcluster the grammar so as to maxi-
mize performance? A good clustering will group
rules together that use the same symbols, since
this means fewer memory accesses to read and
write scores for symbols. Moreover, we would
like the time spent processing each of the subclus-
ters within a cluster to be about the same. We can-
not move on to the next cluster until all threads
from a cluster are finished, which means that the
time a cluster takes is the amount of time taken
by the longest-running subcluster. Finally, when
pruning, it is best if symbols that have the same
coarse projection are clustered together. That way,
we are more likely to be able to skip a subcluster,
since fewer distinct symbols need to be “off” for a
parse item to be skipped in a given subcluster.
Canny et al. (2013) clustered symbols of the
grammar using a sophisticated spectral clustering
algorithm to obtain a permutation of the symbols.
Then the rules of the grammar were laid out in
a (sparse) three-dimensional tensor, with one di-
mension representing the parent of the rule, one
representing the left child, and one representing
the right child. They then split the cube into 6x2x2
contiguous “major cubes,” giving a partition of the
rules into 24 clusters. They then further subdi-
vided these cubes into 2x2x2 minor cubes, giv-
ing 8 subclusters that executed in parallel. Note
that the clusters induced by these major and minor
cubes need not be of similar sizes; indeed, they of-
ten are not. Clustering using this method is labeled
‘Reimplementation’ in Table 1.
The addition of pruning introduces further con-
siderations. First, we have a coarse grammar, with
</bodyText>
<page confidence="0.996051">
212
</page>
<bodyText confidence="0.999984894736842">
many fewer rules and symbols. Second, we are
able to skip a parse item for an entire cluster if that
item’s pruning mask does not intersect the clus-
ter’s signature. Spreading symbols across clusters
may be inefficient: if a parse item licenses a given
symbol, we will have to enqueue that item to any
queue that has the symbol in its signature, no mat-
ter how many other symbols are in that cluster.
Thus, it makes sense to choose a clustering al-
gorithm that exploits the structure introduced by
the pruning masks. We use a very simple method:
we cluster the rules in the grammar by coarse par-
ent symbol. When coarse symbols are extremely
unlikely (and therefore have few corresponding
rules), we merge their clusters to avoid the over-
head of beginning work on clusters where little
work has to be done.4 In order to subcluster, we
divide up rules among subclusters so that each
subcluster has the same number of active parent
symbols. We found this approach to subclustering
worked well in practice.
Clustering using this method is labeled ‘Parent’
in Table 1. Now, when we use a coarse pruning
pass, we are able to parse nearly 280 sentences
per second, a 70% increase in parsing performance
relative to Canny et al. (2013)’s system, and nearly
50% over our reimplemented baseline.
It turns out that this simple clustering algorithm
produces relatively efficient kernels even in the un-
pruned case. The unpruned Viterbi computations
in a fine grammar using the clustering method of
Canny et al. (2013) yields a speed of 193 sen-
tences per second, whereas the same computation
using coarse parent clustering has a speed of 159
sentences per second. (See Table 1.) This is not
as efficient as Canny et al. (2013)’s highly tuned
method, but it is still fairly fast, and much simpler
to implement.
</bodyText>
<sectionHeader confidence="0.702541" genericHeader="method">
8 Pruning with Finer Grammars
</sectionHeader>
<bodyText confidence="0.992764387096774">
The coarse to fine pruning approach of Petrov and
Klein (2007) employs an X-bar grammar as its
first pruning phase, but there is no reason why
we cannot begin with a more complex grammar
for our initial pass. As Petrov and Klein (2007)
have shown, intermediate-sized Berkeley gram-
mars prune many more symbols than the X-bar
system. However, they are slower to parse with
4Specifically, after clustering based on the coarse parent
symbol, we merge all clusters with less than 300 rules in them
into one large cluster.
in a CPU context, and so they begin with an X-bar
grammar.
Because of the overhead associated with trans-
ferring work items to GPU, using a very small
grammar may not be an efficient use of the GPU’s
computational resources. To that end, we tried
computing pruning masks with one-split and two-
split Berkeley grammars. The X-bar grammar can
compute pruning masks at just over 1000 sen-
tences per second, the 1-split grammar parses 858
sentences per second, and the 2-split grammar
parses 526 sentences per second.
Because parsing with these grammars is still
quite fast, we tried using them as the coarse pass
instead. As shown in Table 1, using a 1-split gram-
mar as a coarse pass allows us to produce over 400
sentences per second, a full 2x improvement over
our original system. Conducting a coarse pass
with a 2-split grammar is somewhat slower, at a
“mere” 343 sentences per second.
</bodyText>
<sectionHeader confidence="0.970491" genericHeader="method">
9 Minimum Bayes risk parsing
</sectionHeader>
<bodyText confidence="0.999968862068966">
The Viterbi algorithm is a reasonably effective
method for parsing. However, many authors
have noted that parsers benefit substantially from
minimum Bayes risk decoding (Goodman, 1996;
Simaan, 2003; Matsuzaki et al., 2005; Titov and
Henderson, 2006; Petrov and Klein, 2007). MBR
algorithms for parsing do not compute the best
derivation, as in Viterbi parsing, but instead the
parse tree that maximizes the expected count of
some figure of merit. For instance, one might want
to maximize the expected number of correct con-
stituents (Goodman, 1996), or the expected rule
counts (Simaan, 2003; Petrov and Klein, 2007).
MBR parsing has proven especially useful in la-
tent variable grammars. Petrov and Klein (2007)
showed that MBR trees substantially improved
performance over Viterbi parses for latent variable
grammars, earning up to 1.5F1.
Here, we implement the Max Recall algorithm
of Goodman (1996). This algorithm maximizes
the expected number of correct coarse symbols
(A, i, j) with respect to the posterior distribution
over parses for a sentence.
This particular MBR algorithm has the advan-
tage that it is relatively straightforward to imple-
ment. In essence, we must compute the marginal
probability of each fine-labeled span µ(AX, i, j),
and then marginalize to obtain µ(A, i, j). Then,
for each span (i, j), we find the best possible split
</bodyText>
<page confidence="0.997885">
213
</page>
<bodyText confidence="0.9851665">
point k that maximizes C(i, j) = µ(A, i, j) +
maxk (C(i, k) + C(k, j)). Parse extraction is
then just a matter of following back pointers from
the root, as in the Viterbi algorithm.
</bodyText>
<subsectionHeader confidence="0.998705">
9.1 Computing marginal probabilities
</subsectionHeader>
<bodyText confidence="0.999990405405405">
The easiest way to compute marginal probabilities
is to use the log space semiring rather than the
Viterbi semiring, and then to run the inside and
outside algorithms as before. We should expect
this algorithm to be at least a factor of two slower:
the outside pass performs at least as much work as
the inside pass. Moreover, it typically has worse
memory access patterns, leading to slower perfor-
mance.
Without pruning, our approach does not han-
dle these log domain computations well at all:
we are only able to compute marginals for 32.1
sentences/second, more than a factor of 5 slower
than our coarse pass. To begin, log space addition
requires significantly more operations than max,
which is a primitive operation on GPUs. Beyond
the obvious consequence that executing more op-
erations means more time taken, the sheer number
of operations becomes too much for the compiler
to handle. Because the grammars are compiled
into code, the additional operations are all inlined
into the kernels, producing much larger kernels.
Indeed, in practice the compiler will often hang if
we use the same size grammar clusters as we did
for Viterbi. In practice, we found there is an effec-
tive maximum of 2000 rules per kernel using log
sums, while we can use more than 10,000 rules
rules in a single kernel with Viterbi.
With coarse pruning, however, we can avoid
much of the increased cost associated with log
domain computations. Because so many labeled
spans are pruned, we are able to skip many of the
grammar clusters and thus avoid many of the ex-
pensive operations. Using coarse pruning and log
domain calculations, our system produces MBR
trees at a rate of 130.4 sentences per second, a
four-fold increase.
</bodyText>
<subsectionHeader confidence="0.999517">
9.2 Scaling with the Coarse Pass
</subsectionHeader>
<bodyText confidence="0.999897571428571">
One way to avoid the expense of log domain com-
putations is to use scaled probabilities rather than
log probabilities. Scaling is one of the folk tech-
niques that are commonly used in the NLP com-
munity, but not generally written about. Recall
that floating point numbers are composed of a
mantissa m and an exponent e, giving a number
</bodyText>
<table confidence="0.9971022">
System Sent/Sec Speedup
Unpruned Log Sum MBR 32.1 –
Pruned Log Sum MBR 130.4 4.1x
Pruned Scaling MBR 190.6 5.9x
Pruned Viterbi 404.7 12.6x
</table>
<tableCaption confidence="0.991541">
Table 2: Performance numbers for computing max
</tableCaption>
<bodyText confidence="0.977661886363636">
constituent (Goodman, 1996) trees on 20,000 sen-
tences of length 40 or less from the Penn Tree-
bank. For convenience, we have copied our pruned
Viterbi system’s result.
f = m · 2e. When a float underflows, the ex-
ponent becomes too low to represent the available
number of bits. In scaling, floating point numbers
are paired with an additional number that extends
the exponent. That is, the number is represented
as f� = f · exp(s). Whenever f becomes either
too big or too small, the number is rescaled back
to a less “dangerous” range by shifting mass from
the exponent e to the scaling factor s.
In practice, one scale s is used for an entire span
(i, j), and all scores for that span are rescaled in
concert. In our GPU system, multiple scores in
any given span are being updated at the same time,
which makes this dynamic rescaling tricky and ex-
pensive, especially since inter-warp communica-
tion is fairly limited.
We propose a much simpler static solution that
exploits the coarse pass. In the coarse pass, we
compute Viterbi inside and outside scores for ev-
ery span. Because the grammar used in the coarse
pass is a projection of the grammar used in the
fine pass, these coarse scores correlate reasonably
closely with the probabilities computed in the fine
pass: If a span has a very high or very low score
in the coarse pass, it typically has a similar score
in the fine pass. Thus, we can use the coarse
pass’s inside and outside scores as the scaling val-
ues for the fine pass’s scores. That is, in addition
to computing a pruning mask, in the coarse pass
we store the maximum inside and outside score in
each span, giving two arrays of scores sIi,j and sOi,j.
Then, when applying rules in the fine pass, each
fine inside score over a split span (i, k, j) is scaled
to the appropriate sIi,j by multiplying the score by
exp (sIi, k + sIk,j − sIj), where sZ k, sIk,j SZ j are
the scaling factors for the left child, right child,
and parent, respectively. The outside scores are
scaled analogously.
By itself, this approach works on nearly ev-
ery sentence. However, scores for approximately
</bodyText>
<page confidence="0.995256">
214
</page>
<bodyText confidence="0.999917714285714">
0.5% of sentences overflow (sic). Because we are
summing instead of maxing scores in the fine pass,
the scaling factors computed using max scores are
not quite large enough, and so the rescaled inside
probabilities grow too large when multiplied to-
gether. Most of this difference arises at the leaves,
where the lexicon typically has more uncertainty
than higher up in the tree. Therefore, in the fine
pass, we normalize the inside scores at the leaves
to sum to 1.0.5 Using this slight modification, no
sentences from the Treebank under- or overflow.
We know of no reason why this same trick can-
not be employed in more traditional parsers, but
it is especially useful here: with this static scal-
ing, we can avoid the costly log sums without in-
troducing any additional inter-thread communica-
tion, making the kernels much smaller and much
faster. Using scaling, we are able to push our
parser to 190.6 sentences/second for MBR extrac-
tion, just under half the speed of the Viterbi sys-
tem.
</bodyText>
<subsectionHeader confidence="0.998869">
9.3 Parsing Accuracies
</subsectionHeader>
<bodyText confidence="0.999989066666667">
It is of course important verify the correctness of
our system; one easy way to do so is to exam-
ine parsing accuracy, as compared to the original
Berkeley parser. We measured parsing accuracy
on sentences of length ≤ 40 from section 22 of the
Penn Treebank. Our Viterbi parser achieves 89.7
F1, while our MBR parser scores 91.0. These re-
sults are nearly identical to the Berkeley parsers
most comparable numbers: 89.8 for Viterbi, and
90.9 for their “Max-Rule-Sum” MBR algorithm.
These slight differences arise from the usual mi-
nor variation in implementation details. In partic-
ular, we use one coarse pass instead of several, and
a different MBR algorithm. In addition, there are
some differences in unary processing.
</bodyText>
<subsectionHeader confidence="0.540999">
10 Analyzing System Performance
</subsectionHeader>
<bodyText confidence="0.999982875">
In this section we attempt to break down how ex-
actly our system is spending its time. We do this in
an effort to give a sense of how time is spent dur-
ing computation on GPUs. These timing numbers
are computed using the built-in profiling capabil-
ities of the programming environment. As usual,
profiles exhibit an observer effect, where the act of
measuring the system changes the execution. Nev-
</bodyText>
<footnote confidence="0.691888">
5One can instead interpret this approach as changing the
scaling factors to sl&apos;. = sz13 J · �i≤�&lt;� Ea inside(A, k, k +
1), where inside is the array of scores for the fine pass.
</footnote>
<table confidence="0.9971836">
System Coarse Pass Fine Pass
Unpruned Viterbi – 6.4
Pruned Viterbi 1.2 1.5
Unpruned Logsum MBR — 28.6
Pruned Scaling MBR 1.2 4.3
</table>
<tableCaption confidence="0.998386">
Table 3: Time spent in the passes of our differ-
</tableCaption>
<bodyText confidence="0.976223794871795">
ent systems, in seconds per 1000 sentences. Prun-
ing refers to using a 1-split grammar for the coarse
pass.
ertheless, the general trends should more or less be
preserved as compared to the unprofiled code.
To begin, we can compute the number of sec-
onds needed to parse 1000 sentences. (We use sec-
onds per sentence rather than sentences per second
because the former measure is additive.) The re-
sults are in Table 3. In the case of pruned Viterbi,
pruning reduces the amount of time spent in the
fine pass by more than 4x, though half of those
gains are lost to computing the pruning masks.
In Table 4, we break down the time taken by
our system into individual components. As ex-
pected, binary rules account for the vast majority
of the time in the unpruned Viterbi case, but much
less time in the pruned case, with the total time
taken for binary rules in the coarse and fine passes
taking about 1/5 of the time taken by binaries in
the unpruned version. Queueing, which involves
copying memory around within the GPU to pro-
cess the individual parse items, takes a fairly con-
sistent amount of time in all systems. Overhead,
which includes transport time between the CPU
and GPU and other processing on the CPU, is rela-
tively small for most system configurations. There
is greater overhead in the scaling system, because
scaling factors are copied to the CPU between the
coarse and fine passes.
A final question is: how many sentences per
second do we need to process to saturate the
GPU’s processing power? We computed Viterbi
parses of successive powers of 10, from 1 to
100,000 sentences.6 In Figure 4, we then plotted
the throughput, in terms of number of sentences
per second. Throughput increases through parsing
10,000 sentences, and then levels off by the time it
reaches 100,000 sentences.
</bodyText>
<footnote confidence="0.982889">
6We replicated the Treebank for the 100,000 sentences
pass.
</footnote>
<page confidence="0.99546">
215
</page>
<table confidence="0.9987818">
System Binary Unary Coarse Pass Overhead Binary Fine Pass Overhead
Queueing Masks Unary Queueing
Unpruned Viterbi – – – – – 5.42 0.14 0.33 0.40
Pruned Viterbi 0.59 0.02 0.19 0.04 0.22 0.56 0.10 0.34 0.22
Pruned Scaling 0.59 0.02 0.19 0.04 0.20 1.74 0.24 0.46 0.84
</table>
<tableCaption confidence="0.966505">
Table 4: Breakdown of time spent in our different systems, in seconds per 1000 sentences. Binary and
</tableCaption>
<bodyText confidence="0.695990666666667">
Unary refer to spent processing binary rules. Queueing refers to the amount of time used to move memory
around within the GPU for processing. Overhead includes all other time, which includes communication
between the GPU and the CPU.
</bodyText>
<figure confidence="0.979409625">
400
300
200
100
0
1 10 100 1K 10K 100K
Number of Sentences
Sentences/Second
</figure>
<figureCaption confidence="0.656833">
Figure 4: Plot of speeds (sentences / second) for
various sizes of input corpora. The full power of
the GPU parser is only reached when run on large
numbers of sentences.
</figureCaption>
<sectionHeader confidence="0.999772" genericHeader="related work">
11 Related Work
</sectionHeader>
<bodyText confidence="0.9999897">
Apart from the model of Canny et al. (2013), there
have been a few attempts at using GPUs in NLP
contexts before. Johnson (2011) and Yi et al.
(2011) both had early attempts at porting pars-
ing algorithms to the GPU. However, they did
not demonstrate significantly increased speed over
a CPU implementation. In machine translation,
He et al. (2013) adapted algorithms designed for
GPUs in the computational biology literature to
speed up on-demand phrase table extraction.
</bodyText>
<sectionHeader confidence="0.976304" genericHeader="conclusions">
12 Conclusion
</sectionHeader>
<bodyText confidence="0.999959578947368">
GPUs represent a challenging opportunity for nat-
ural language processing. By carefully design-
ing within the constraints imposed by the architec-
ture, we have created a parser that can exploit the
same kinds of sparsity that have been developed
for more traditional architectures.
One of the key remaining challenges going
forward is confronting the kind of lexicalized
sparsity common in other NLP models. The
Berkeley parser’s grammars—by virtue of being
unlexicalized—can be applied uniformly to all
parse items. The bilexical features needed by
dependency models and lexicalized constituency
models are not directly amenable to acceleration
using the techniques we described here. Deter-
mining how to efficiently implement these kinds
of models is a promising area for new research.
Our system is available as open-source at
https://www.github.com/dlwh/puck.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999312666666667">
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014, by a
Google PhD fellowship to the first author, and
an NSF fellowship to the second. We further
gratefully acknowledge a hardware donation by
NVIDIA Corporation.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998151423076923">
John Canny, David Hall, and Dan Klein. 2013. A
multi-teraflop constituency parser using GPUs. In
Proceedings of EMNLP, pages 1898–1907, October.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine
Hill, R Shrivaths, Jeremy Moore, Michael Pozar,
et al. 2006. Multilevel coarse-to-fine pcfg pars-
ing. In Proceedings of the main conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 168–175. Association for
Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In ACL, pages 177–183.
Hua He, Jimmy Lin, and Adam Lopez. 2013. Mas-
sively parallel suffix array queries and on-demand
phrase extraction for statistical machine translation
using gpus. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 325–334, Atlanta, Geor-
gia, June. Association for Computational Linguis-
tics.
Mark Johnson. 2011. Parsing in parallel on multiple
cores and gpus. In Proceedings of the Australasian
Language Technology Association Workshop.
</reference>
<page confidence="0.986544">
216
</page>
<reference confidence="0.99953125">
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75–82, Morristown, NJ, USA.
CUDA Nvidia. 2008. Programming guide.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT.
Khalil Simaan. 2003. On maximizing metrics for syn-
tactic disambiguation. In Proceedings of IWPT.
Ivan Titov and James Henderson. 2006. Loss min-
imization in parse reranking. In Proceedings of
EMNLP, pages 560–567. Association for Computa-
tional Linguistics.
Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt
Keutzer. 2011. Efficient parallel cky parsing on
gpus. In Proceedings of the 2011 Conference on
Parsing Technologies, Dublin, Ireland, October.
</reference>
<page confidence="0.9984">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973308">
<title confidence="0.999516">Sparser, Better, Faster GPU Parsing</title>
<author confidence="0.999929">David Hall Taylor Berg-Kirkpatrick John Canny Dan Klein</author>
<affiliation confidence="0.9999165">Computer Science Division University of California,</affiliation>
<abstract confidence="0.998908655172414">Due to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points. Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per second—more than a 2x speedup—on the same hardware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning—nearly a 6x speedup.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Canny</author>
<author>David Hall</author>
<author>Dan Klein</author>
</authors>
<title>A multi-teraflop constituency parser using GPUs.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1898--1907</pages>
<contexts>
<context position="2378" citStr="Canny et al. (2013)" startWordPosition="355" endWordPosition="358">ce level. First, classic single-core processors and main memory architectures are no longer getting substantially faster over time, so speed gains must now come from parallelism within a single machine. Second, compared to CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic. Since tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost. The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on. Recently, Canny et al. (2013) proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide. Their system uses a grammar based on the Berkeley parser (Petrov and Klein, 2007) (which is particularly amenable to GPU processing), “compiling” the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart. Together these kernels implement the Viterbi inside algorithm. On a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing detai</context>
<context position="4357" citStr="Canny et al. (2013)" startWordPosition="684" endWordPosition="687">nual Meeting of the Association for Computational Linguistics, pages 208–217, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics fine grammar, skipping symbols that the coarse pass deemed sufficiently unlikely. Using this approach, we see a gain of more than 2x over the dense GPU implementation, resulting in overall speeds of up to 404 sentences per second. For comparison, the publicly available CPU implementation of Petrov and Klein (2007) parses approximately 7 sentences per second per core on a modern CPU. A further drawback of the dense approach in Canny et al. (2013) is that it only computes Viterbi parses. As with other grammars with a parse/derivation distinction, the grammars of Petrov and Klein (2007) only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (Marcus et al., 1993). To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass. We then implement minimumBayes-risk parsing via the max recall algorithm of G</context>
<context position="5874" citStr="Canny et al. (2013)" startWordPosition="933" endWordPosition="936">with experiments interspersed throughout the paper, and summarized in Tables 1 and 2. In this paper, we focus our attention on current-generation NVIDIA GPUs. Many of the ideas described here apply to other GPUs (such as those from AMD), but some specifics will differ. All experiments are run with an NVIDIA GeForce GTX 680, a mid-range GPU that costs around $500 at time of writing. Unless otherwise noted, all experiments are conducted on sentences of length &lt; 40 words, and we estimate times based on batches of 20K sentences.1 We should note that our experimental condition differs from that of Canny et al. (2013): they evaluate on sentences of length &lt; 30. Furthermore, they 1The implementation of Canny et al. (2013) cannot handle batches so large, and so we tested it on batches of 1200 sentences. Our reimplementation is approximately the same speed for the same batch sizes. For batches of 20K sentences, we used sentences from the training set. We verified that there was no significant difference in speed for sentences from the training set and from the test set. use two NVIDIA GeForce GTX 690s—each of which is essentially a repackaging of two 680s— meaning that our system and experiments would run app</context>
<context position="8773" citStr="Canny et al. (2013)" startWordPosition="1431" endWordPosition="1434">nd that only about 79% of binary rule applications can be skipped, because the unpruned symbols tend to be the ones with a larger grammar footprint. 4 GPU Architectures Unfortunately, the standard coarse-to-fine approach does not naively translate to GPU architectures. GPUs work by executing thousands of threads at once, but impose the constraint that large blocks of threads must be executing the same 209 GPU CPU Sentences Masks Queue Trees Queue RAM Parse Charts Work Array Instruction Cache Grammar RAM Masks Queue Figure 1: Overview of the architecture of our system, which is an extension of Canny et al. (2013)’s system. The GPU and CPU communicate via a work queue, which ferries parse items from the CPU to the GPU. Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask that is used by the CPU when deciding which items to queue during the fine pass. The original system of Canny et al. (2013) only used the fine pass, with no pruning. instructions in lockstep, differing only in their input data. Thus sparsely skipping rules and symbols will not save any work. Indeed, it may actually slow the system down. In this section, we provide an overview of GPU architectures, fo</context>
<context position="10959" citStr="Canny et al. (2013)" startWordPosition="1818" endWordPosition="1821">ory locations. 2Older hardware (600 series or older) has 8 SMs. Newer hardware has more. A further consideration is that the number of registers available to a thread in a warp is rather limited compared to a CPU. On the 600 series, maximum occupancy can only be achieved if each thread uses at most 63 registers (Nvidia, 2008).3 Registers are many times faster than variables located in thread-local memory, which is actually the same speed as global memory. 5 Anatomy of a Dense GPU Parser This architecture environment puts very different constraints on parsing algorithms from a CPU environment. Canny et al. (2013) proposed an implementation of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser. They assume that they are parsing many sentences at once, with throughput being more important than latency. In this section, we describe their dense algorithm, which we take as the baseline for our work; we present it in a way that sets up the changes to follow. At the top level, the CPU and GPU communicate via a work queue of parse items of the form (s, i, k, j), where s is an identifier of a sent</context>
<context position="12438" citStr="Canny et al. (2013)" startWordPosition="2079" endWordPosition="2082">Reimpl Empty, Coarse 185.5 0.96x Reimpl Labeled, Coarse 187.5 0.97x Parent – 158.6 0.82x Parent Labeled, Coarse 278.9 1.4x Parent Labeled, 1-split 404.7 2.1x Parent Labeled, 2-split 343.6 1.8x Table 1: Performance numbers for computing Viterbi inside charts on 20,000 sentences of length ≤40 from the Penn Treebank. All times are measured on an NVIDIA GeForce GTX 680. ‘Reimpl’ is our reimplementation of their approach. Speedups are measured in reference to this reimplementation. See Section 7 for discussion of the clustering algorithms and Section 6 for a description of the pruning methods. The Canny et al. (2013) system is benchmarked on a batch size of 1200 sentences, the others on 20,000. is the end point. The GPU takes large numbers of parse items and applies the entire grammar to them in parallel. These parse items are enqueued in order of increasing span size, blocking until all items of a given length are complete. This approach is diagrammed in Figure 2. Because all rules are applied to all parse items, all threads are executing the same sequence of instructions. Thus, there is no concern of warp divergence. 5.1 Grammar Compilation One important feature of Canny et al. (2013)’s system is gramma</context>
<context position="14013" citStr="Canny et al. (2013)" startWordPosition="2352" endWordPosition="2355">ffectively use all of its registers. However, register space is limited on GPUs. Because the Berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in register spills. Canny et al. (2013) found they had to partition the grammar into multiple different kernels. We discuss this partitioning in more detail in Section 7. However, in short, the entire grammar G is broken into multiple clusters Gi where each rule belongs to exactly one cluster. Grammar Queue (i, k, j) Figure 2: Schematic representation of the work queue used in Canny et al. (2013). The Viterbi inside loop for the grammar is inlined into a kernel. The kernel is applied to all items in the queue in a blockwise manner. Figure 3: Schematic representation of the work queue and grammar clusters used in the fine pass of our work. Here, the rules of the grammar are clustered by their coarse parent symbol. We then have multiple work queues, with parse items only being enqueued if the span (i, j) allows that symbol in its pruning mask. All in all, Canny et al. (2013)’s system is able to compute Viterbi charts at 164 sentences per second, for sentences up to length 40. On larger </context>
<context position="17191" citStr="Canny et al. (2013)" startWordPosition="2949" endWordPosition="2952"> a labeled work queue, and each one only queues items to which some rule in the corresponding partition applies. We call the set of coarse symbols for a partition (and therefore the corresponding labeled work queue) a signature. During parsing, we only enqueue items (s, i, k, j) to a labeled queue if two conditions are met. First, the span (i, j)’s pruning mask must have a non-empty intersection with the signature of the queue. Second, the pruning mask for the children (i, k) and (k, j) must be non-empty. Once on the GPU, parse items are processed using the same style of compiled kernel as in Canny et al. (2013). Because the entire partition (though not necessarily the entire grammar) is applied to each item in the queue, we still do not need to worry about warp divergence. At the top level, our system first computes pruning masks with a coarse grammar. Then it processes the same sentences with the fine grammar. However, to the extent that the signatures are small, items can be selectively queued only to certain queues. This approach is diagrammed in Figure 3. We tested our new pruning approach using an X-bar grammar as the coarse pass. The resulting speed is 187.5 sentences per second, labeled in Ta</context>
<context position="19052" citStr="Canny et al. (2013)" startWordPosition="3274" endWordPosition="3277">nd write scores for symbols. Moreover, we would like the time spent processing each of the subclusters within a cluster to be about the same. We cannot move on to the next cluster until all threads from a cluster are finished, which means that the time a cluster takes is the amount of time taken by the longest-running subcluster. Finally, when pruning, it is best if symbols that have the same coarse projection are clustered together. That way, we are more likely to be able to skip a subcluster, since fewer distinct symbols need to be “off” for a parse item to be skipped in a given subcluster. Canny et al. (2013) clustered symbols of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols. Then the rules of the grammar were laid out in a (sparse) three-dimensional tensor, with one dimension representing the parent of the rule, one representing the left child, and one representing the right child. They then split the cube into 6x2x2 contiguous “major cubes,” giving a partition of the rules into 24 clusters. They then further subdivided these cubes into 2x2x2 minor cubes, giving 8 subclusters that executed in parallel. Note that the clusters induced by thes</context>
<context position="21130" citStr="Canny et al. (2013)" startWordPosition="3629" endWordPosition="3632">hen coarse symbols are extremely unlikely (and therefore have few corresponding rules), we merge their clusters to avoid the overhead of beginning work on clusters where little work has to be done.4 In order to subcluster, we divide up rules among subclusters so that each subcluster has the same number of active parent symbols. We found this approach to subclustering worked well in practice. Clustering using this method is labeled ‘Parent’ in Table 1. Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to Canny et al. (2013)’s system, and nearly 50% over our reimplemented baseline. It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case. The unpruned Viterbi computations in a fine grammar using the clustering method of Canny et al. (2013) yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second. (See Table 1.) This is not as efficient as Canny et al. (2013)’s highly tuned method, but it is still fairly fast, and much simpler to implement. 8 Pruning with Finer Grammars T</context>
</contexts>
<marker>Canny, Hall, Klein, 2013</marker>
<rawString>John Canny, David Hall, and Dan Klein. 2013. A multi-teraflop constituency parser using GPUs. In Proceedings of EMNLP, pages 1898–1907, October.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>Micha Elsner</author>
<author>Joseph Austerweil</author>
<author>David Ellis</author>
<author>Isaac Haxton</author>
<author>Catherine Hill</author>
<author>R Shrivaths</author>
<author>Jeremy Moore</author>
<author>Michael Pozar</author>
</authors>
<title>Multilevel coarse-to-fine pcfg parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>168--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6776" citStr="Charniak et al., 2006" startWordPosition="1083" endWordPosition="1086">f 20K sentences, we used sentences from the training set. We verified that there was no significant difference in speed for sentences from the training set and from the test set. use two NVIDIA GeForce GTX 690s—each of which is essentially a repackaging of two 680s— meaning that our system and experiments would run approximately four times faster on their hardware. (This expected 4x factor is empirically consistent with the result of running their system on our hardware.) 3 Sparsity and CPUs One successful approach for speeding up constituency parsers has been to use coarse-to-fine inference (Charniak et al., 2006). In coarse-tofine inference, we have a sequence of increasingly complex grammars Gt. Typically, each successive grammar Gt is a refinement of the preceding grammar Gt−1. That is, for each symbol A,, in the fine grammar, there is some symbol A in the coarse grammar. For instance, in a latent variable parser, the coarse grammar would have symbols like NP, V P, etc., and the fine pass would have refined symbols NP0, NP1, V P4, and so on. In coarse-to-fine inference, one applies the grammars in sequence, computing inside and outside scores. Next, one computes (max) marginals for every labeled spa</context>
</contexts>
<marker>Charniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, 2006</marker>
<rawString>Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil, David Ellis, Isaac Haxton, Catherine Hill, R Shrivaths, Jeremy Moore, Michael Pozar, et al. 2006. Multilevel coarse-to-fine pcfg parsing. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 168–175. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In ACL,</booktitle>
<pages>177--183</pages>
<contexts>
<context position="4970" citStr="Goodman (1996)" startWordPosition="782" endWordPosition="783">) is that it only computes Viterbi parses. As with other grammars with a parse/derivation distinction, the grammars of Petrov and Klein (2007) only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (Marcus et al., 1993). To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass. We then implement minimumBayes-risk parsing via the max recall algorithm of Goodman (1996). Without the coarse pass, the dense marginal computation is not efficient on a GPU, processing only 32 sentences per second. However, our approach allows us to process over 190 sentences per second, almost a 6x speedup. 2 A Note on Experiments We build up our approach incrementally, with experiments interspersed throughout the paper, and summarized in Tables 1 and 2. In this paper, we focus our attention on current-generation NVIDIA GPUs. Many of the ideas described here apply to other GPUs (such as those from AMD), but some specifics will differ. All experiments are run with an NVIDIA GeForc</context>
<context position="23342" citStr="Goodman, 1996" startWordPosition="4004" endWordPosition="4005">s 526 sentences per second. Because parsing with these grammars is still quite fast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi pa</context>
<context position="27016" citStr="Goodman, 1996" startWordPosition="4623" endWordPosition="4624"> per second, a four-fold increase. 9.2 Scaling with the Coarse Pass One way to avoid the expense of log domain computations is to use scaled probabilities rather than log probabilities. Scaling is one of the folk techniques that are commonly used in the NLP community, but not generally written about. Recall that floating point numbers are composed of a mantissa m and an exponent e, giving a number System Sent/Sec Speedup Unpruned Log Sum MBR 32.1 – Pruned Log Sum MBR 130.4 4.1x Pruned Scaling MBR 190.6 5.9x Pruned Viterbi 404.7 12.6x Table 2: Performance numbers for computing max constituent (Goodman, 1996) trees on 20,000 sentences of length 40 or less from the Penn Treebank. For convenience, we have copied our pruned Viterbi system’s result. f = m · 2e. When a float underflows, the exponent becomes too low to represent the available number of bits. In scaling, floating point numbers are paired with an additional number that extends the exponent. That is, the number is represented as f� = f · exp(s). Whenever f becomes either too big or too small, the number is rescaled back to a less “dangerous” range by shifting mass from the exponent e to the scaling factor s. In practice, one scale s is use</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and metrics. In ACL, pages 177–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua He</author>
<author>Jimmy Lin</author>
<author>Adam Lopez</author>
</authors>
<title>Massively parallel suffix array queries and on-demand phrase extraction for statistical machine translation using gpus.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>325--334</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="34666" citStr="He et al. (2013)" startWordPosition="5968" endWordPosition="5971"> CPU. 400 300 200 100 0 1 10 100 1K 10K 100K Number of Sentences Sentences/Second Figure 4: Plot of speeds (sentences / second) for various sizes of input corpora. The full power of the GPU parser is only reached when run on large numbers of sentences. 11 Related Work Apart from the model of Canny et al. (2013), there have been a few attempts at using GPUs in NLP contexts before. Johnson (2011) and Yi et al. (2011) both had early attempts at porting parsing algorithms to the GPU. However, they did not demonstrate significantly increased speed over a CPU implementation. In machine translation, He et al. (2013) adapted algorithms designed for GPUs in the computational biology literature to speed up on-demand phrase table extraction. 12 Conclusion GPUs represent a challenging opportunity for natural language processing. By carefully designing within the constraints imposed by the architecture, we have created a parser that can exploit the same kinds of sparsity that have been developed for more traditional architectures. One of the key remaining challenges going forward is confronting the kind of lexicalized sparsity common in other NLP models. The Berkeley parser’s grammars—by virtue of being unlexi</context>
</contexts>
<marker>He, Lin, Lopez, 2013</marker>
<rawString>Hua He, Jimmy Lin, and Adam Lopez. 2013. Massively parallel suffix array queries and on-demand phrase extraction for statistical machine translation using gpus. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 325–334, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Parsing in parallel on multiple cores and gpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop.</booktitle>
<contexts>
<context position="34447" citStr="Johnson (2011)" startWordPosition="5934" endWordPosition="5935"> to spent processing binary rules. Queueing refers to the amount of time used to move memory around within the GPU for processing. Overhead includes all other time, which includes communication between the GPU and the CPU. 400 300 200 100 0 1 10 100 1K 10K 100K Number of Sentences Sentences/Second Figure 4: Plot of speeds (sentences / second) for various sizes of input corpora. The full power of the GPU parser is only reached when run on large numbers of sentences. 11 Related Work Apart from the model of Canny et al. (2013), there have been a few attempts at using GPUs in NLP contexts before. Johnson (2011) and Yi et al. (2011) both had early attempts at porting parsing algorithms to the GPU. However, they did not demonstrate significantly increased speed over a CPU implementation. In machine translation, He et al. (2013) adapted algorithms designed for GPUs in the computational biology literature to speed up on-demand phrase table extraction. 12 Conclusion GPUs represent a challenging opportunity for natural language processing. By carefully designing within the constraints imposed by the architecture, we have created a parser that can exploit the same kinds of sparsity that have been developed</context>
</contexts>
<marker>Johnson, 2011</marker>
<rawString>Mark Johnson. 2011. Parsing in parallel on multiple cores and gpus. In Proceedings of the Australasian Language Technology Association Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="4678" citStr="Marcus et al., 1993" startWordPosition="732" endWordPosition="735">U implementation, resulting in overall speeds of up to 404 sentences per second. For comparison, the publicly available CPU implementation of Petrov and Klein (2007) parses approximately 7 sentences per second per core on a modern CPU. A further drawback of the dense approach in Canny et al. (2013) is that it only computes Viterbi parses. As with other grammars with a parse/derivation distinction, the grammars of Petrov and Klein (2007) only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (Marcus et al., 1993). To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass. We then implement minimumBayes-risk parsing via the max recall algorithm of Goodman (1996). Without the coarse pass, the dense marginal computation is not efficient on a GPU, processing only 32 sentences per second. However, our approach allows us to process over 190 sentences per second, almost a 6x speedup. 2 A Note on Experiments We build up our approach incrementally, with experiments inters</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<journal>CUDA Nvidia.</journal>
<booktitle>In ACL,</booktitle>
<pages>75--82</pages>
<location>Morristown, NJ, USA.</location>
<note>Programming guide.</note>
<contexts>
<context position="23380" citStr="Matsuzaki et al., 2005" startWordPosition="4008" endWordPosition="4011">ecause parsing with these grammars is still quite fast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, ear</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL, pages 75–82, Morristown, NJ, USA. CUDA Nvidia. 2008. Programming guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="2604" citStr="Petrov and Klein, 2007" startWordPosition="393" endWordPosition="396"> CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic. Since tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost. The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on. Recently, Canny et al. (2013) proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide. Their system uses a grammar based on the Berkeley parser (Petrov and Klein, 2007) (which is particularly amenable to GPU processing), “compiling” the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart. Together these kernels implement the Viterbi inside algorithm. On a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing details below). In this paper, we develop algorithms that can exploit sparsity on a GPU by adapting coarse-tofine pruning to a GPU setting. On a CPU, pruning methods can give speedups of up to 100x. Such extreme speedups over a den</context>
<context position="4223" citStr="Petrov and Klein (2007)" startWordPosition="659" endWordPosition="662"> an overview of the approach: we first parse densely with a coarse grammar and then parse sparsely with the 208 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 208–217, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics fine grammar, skipping symbols that the coarse pass deemed sufficiently unlikely. Using this approach, we see a gain of more than 2x over the dense GPU implementation, resulting in overall speeds of up to 404 sentences per second. For comparison, the publicly available CPU implementation of Petrov and Klein (2007) parses approximately 7 sentences per second per core on a modern CPU. A further drawback of the dense approach in Canny et al. (2013) is that it only computes Viterbi parses. As with other grammars with a parse/derivation distinction, the grammars of Petrov and Klein (2007) only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (Marcus et al., 1993). To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to a</context>
<context position="7830" citStr="Petrov and Klein (2007)" startWordPosition="1270" endWordPosition="1273">d so on. In coarse-to-fine inference, one applies the grammars in sequence, computing inside and outside scores. Next, one computes (max) marginals for every labeled span (A, i, j) in a sentence. These max marginals are used to compute a pruning mask for every span (i, j). This mask is the set of symbols allowed for that span. Then, in the next pass, one only processes rules that are licensed by the pruning mask computed at the previous level. This approach works because a low quality coarse grammar can still reliably be used to prune many symbols from the fine chart without loss of accuracy. Petrov and Klein (2007) found that over 98% of symbols can be pruned from typical charts using a simple X-bar grammar without any loss of accuracy. Thus, the vast majority of rules can be skipped, and therefore most computation can be avoided. It is worth pointing out that although 98% of labeled spans can be skipped due to X-bar pruning, we found that only about 79% of binary rule applications can be skipped, because the unpruned symbols tend to be the ones with a larger grammar footprint. 4 GPU Architectures Unfortunately, the standard coarse-to-fine approach does not naively translate to GPU architectures. GPUs w</context>
<context position="21791" citStr="Petrov and Klein (2007)" startWordPosition="3740" endWordPosition="3743">mplemented baseline. It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case. The unpruned Viterbi computations in a fine grammar using the clustering method of Canny et al. (2013) yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second. (See Table 1.) This is not as efficient as Canny et al. (2013)’s highly tuned method, but it is still fairly fast, and much simpler to implement. 8 Pruning with Finer Grammars The coarse to fine pruning approach of Petrov and Klein (2007) employs an X-bar grammar as its first pruning phase, but there is no reason why we cannot begin with a more complex grammar for our initial pass. As Petrov and Klein (2007) have shown, intermediate-sized Berkeley grammars prune many more symbols than the X-bar system. However, they are slower to parse with 4Specifically, after clustering based on the coarse parent symbol, we merge all clusters with less than 300 rules in them into one large cluster. in a CPU context, and so they begin with an X-bar grammar. Because of the overhead associated with transferring work items to GPU, using a very s</context>
<context position="23432" citStr="Petrov and Klein, 2007" startWordPosition="4016" endWordPosition="4019">ast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, earning up to 1.5F1. Here, we implement the Max Recall </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Simaan</author>
</authors>
<title>On maximizing metrics for syntactic disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="23356" citStr="Simaan, 2003" startWordPosition="4006" endWordPosition="4007"> per second. Because parsing with these grammars is still quite fast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi parses for laten</context>
</contexts>
<marker>Simaan, 2003</marker>
<rawString>Khalil Simaan. 2003. On maximizing metrics for syntactic disambiguation. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Loss minimization in parse reranking.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>560--567</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23407" citStr="Titov and Henderson, 2006" startWordPosition="4012" endWordPosition="4015">e grammars is still quite fast, we tried using them as the coarse pass instead. As shown in Table 1, using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a “mere” 343 sentences per second. 9 Minimum Bayes risk parsing The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding (Goodman, 1996; Simaan, 2003; Matsuzaki et al., 2005; Titov and Henderson, 2006; Petrov and Klein, 2007). MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct constituents (Goodman, 1996), or the expected rule counts (Simaan, 2003; Petrov and Klein, 2007). MBR parsing has proven especially useful in latent variable grammars. Petrov and Klein (2007) showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, earning up to 1.5F1. Here, we </context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>Ivan Titov and James Henderson. 2006. Loss minimization in parse reranking. In Proceedings of EMNLP, pages 560–567. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youngmin Yi</author>
<author>Chao-Yue Lai</author>
<author>Slav Petrov</author>
<author>Kurt Keutzer</author>
</authors>
<title>Efficient parallel cky parsing on gpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Parsing Technologies,</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="34468" citStr="Yi et al. (2011)" startWordPosition="5937" endWordPosition="5940">g binary rules. Queueing refers to the amount of time used to move memory around within the GPU for processing. Overhead includes all other time, which includes communication between the GPU and the CPU. 400 300 200 100 0 1 10 100 1K 10K 100K Number of Sentences Sentences/Second Figure 4: Plot of speeds (sentences / second) for various sizes of input corpora. The full power of the GPU parser is only reached when run on large numbers of sentences. 11 Related Work Apart from the model of Canny et al. (2013), there have been a few attempts at using GPUs in NLP contexts before. Johnson (2011) and Yi et al. (2011) both had early attempts at porting parsing algorithms to the GPU. However, they did not demonstrate significantly increased speed over a CPU implementation. In machine translation, He et al. (2013) adapted algorithms designed for GPUs in the computational biology literature to speed up on-demand phrase table extraction. 12 Conclusion GPUs represent a challenging opportunity for natural language processing. By carefully designing within the constraints imposed by the architecture, we have created a parser that can exploit the same kinds of sparsity that have been developed for more traditional</context>
</contexts>
<marker>Yi, Lai, Petrov, Keutzer, 2011</marker>
<rawString>Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt Keutzer. 2011. Efficient parallel cky parsing on gpus. In Proceedings of the 2011 Conference on Parsing Technologies, Dublin, Ireland, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>