<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000084">
<title confidence="0.99729">
A Unified Learning Framework of Skip-Grams and Global Vectors
</title>
<author confidence="0.800755">
Jun Suzuki and Masaaki Nagata
</author>
<affiliation confidence="0.643914">
NTT Communication Science Laboratories, NTT Corporation
</affiliation>
<address confidence="0.745581">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
</address>
<email confidence="0.994991">
{suzuki.jun, nagata.masaaki}@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.993809" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999747533333333">
Log-bilinear language models such as
SkipGram and GloVe have been proven to
capture high quality syntactic and seman-
tic relationships between words in a vector
space. We revisit the relationship between
SkipGram and GloVe models from a ma-
chine learning viewpoint, and show that
these two methods are easily merged into
a unified form. Then, by using the unified
form, we extract the factors of the config-
urations that they use differently. We also
empirically investigate which factor is re-
sponsible for the performance difference
often observed in widely examined word
similarity and analogy tasks.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988143742857143">
Neural-network-inspired word embedding meth-
ods such as Skip-Gram (SkipGram) have been
proven to capture high quality syntactic and se-
mantic relationships between words in a vector
space (Mikolov et al., 2013a). A similar embed-
ding method, called ‘Global Vector (GloVe)’, was
recently proposed. It has demonstrated significant
improvements over SkipGram on the widely used
‘Word Analogy’ and ‘Word Similarity’ benchmark
datasets (Pennington et al., 2014). Unfortunately,
a later deep re-evaluation has revealed that GloVe
does not consistently outperform SkipGram (Levy
et al., 2015); both methods provided basically the
same level of performance, and SkipGram even
seems ‘more robust (not yielding very poor re-
sults)’ than GloVe. Moreover, some other papers,
i.e., (Shi and Liu, 2014), and some researchers
in the community have discussed a relationship,
and/or which is superior, SkipGram or GloVe.
From this background, we revisit the relation-
ship between SkipGram and GloVe from a ma-
chine learning viewpoint. We show that it is nat-
V :set of vocabulary (set of words)
|V |:vocabulary size, or number of words in V
i :index of the input vector, where i ∈ {1, ... , |V|}
j :index of the output vector, where j ∈ {1, ... , |V|}
ei :input vector of the i-th word in V
oj :output vector of the j-th word in V
If i = j, then ei and oj are the input and output vec-
tors of the same word in V, respectively.
D :number of dimensions in input and output vectors
mi,j :(i, j)-factor of matrix M
si,j :dot product of input and output vectors, si,j = ei · oj
D :training data, D = {(in, jn)}Nn=1
Ψ(·):objective function
</bodyText>
<equation confidence="0.980009666666667">
1
v(·) :sigmoid function, v(x) =
1+exp(−x)
</equation>
<bodyText confidence="0.800925666666667">
ci,j :co-occurrence of the i-th and j-th words in D
D&apos; :(virtual) negative sampling data
c&apos;i,j :co-occurrence of the i-th and j-th words in D&apos;
k :hyper-parameter of the negative sampling
β(·) :‘weighting factor’ of loss function
Φ(·) :loss function
</bodyText>
<tableCaption confidence="0.989294">
Table 1: List of notations used in this paper.
</tableCaption>
<bodyText confidence="0.999695272727273">
ural to think that these two methods are essen-
tially identical, with the chief difference being
their learning configurations.
The final goal of this paper is to provide a uni-
fied learning framework that encompasses the con-
figurations used in SkipGram and GloVe to gain a
deeper understanding of the behavior of these em-
bedding methods. We also empirically investigate
which learning configuration most clearly eluci-
dates the performance difference often observed in
word similarity and analogy tasks.
</bodyText>
<sectionHeader confidence="0.868204" genericHeader="introduction">
2 SkipGram and GloVe
</sectionHeader>
<bodyText confidence="0.785849">
Table 1 shows the notations used in this paper.
</bodyText>
<subsectionHeader confidence="0.956412">
2.1 Matrix factorization view of SkipGram
</subsectionHeader>
<bodyText confidence="0.957129166666666">
SkipGram can be categorized as one of the
simplest neural language models (Mnih and
Kavukcuoglu, 2013). It generally assigns two dis-
tinct D-dimensional vectors to each word in vo-
cabulary V; one is ‘input vector’, and the other is
‘output vector’1.
</bodyText>
<footnote confidence="0.980586">
1These two vectors are generally referred to as ‘word (or
target) vector’ and ‘context vector’. We use the terms ‘in-
</footnote>
<page confidence="0.764328">
186
</page>
<bodyText confidence="0.885731533333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 186–191,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
Roughly speaking, SkipGram models word-to-
word co-occurrences, which are extracted within
the predefined context window size, by the in-
put and output vectors. Recently, SkipGram
has been interpreted as implicitly factorizing the
matrix, where the factors are calculated from
co-occurrence information (Levy and Goldberg,
2014). Let mi,j be the (i, j)-factor of matrix M
to be ‘implicitly’ factorized by SkipGram. Skip-
Gram approximates each mi,j by the inner prod-
uct of the corresponding input and output vectors,
that is:
</bodyText>
<equation confidence="0.730452">
mi,j ≈ ei · oj, (1)
</equation>
<subsectionHeader confidence="0.923943">
2.1.1 SkipGram with negative sampling
</subsectionHeader>
<bodyText confidence="0.9993395">
The primitive training sample for SkipGram is a
pair of a target word and its corresponding con-
text word. Thus, we can represent the training
data of SkipGram as a list of input and output in-
dex pairs, that is, D = {(in, jn)}Nn=1. Thus the
estimation problem of ‘SkipGram with negative
sampling (SGNS)’ is defined as the minimization
problem of objective function Ψ:
</bodyText>
<equation confidence="0.9996935">
Ψ = − � log (σ(ein · ojn))
�−
(in,jn)∈D log �1 − σ(ein · ojn)�, (2)
(in,jn)∈D0
</equation>
<bodyText confidence="0.997429">
where the optimization parameters are ei and oj
for all i and j. Note that we explicitly represent the
negative sampling data D0 (Goldberg and Levy,
2014).
Let us assume that, in a preliminary step, we
count all co-occurrences in D. Then, the SGNS
objective in Eq. 2 can be rewritten as follows by a
simple reformulation:
</bodyText>
<equation confidence="0.999590333333333">
Ψ = − � � (ci,j log (σ(ei · oj))
i j
+c0i,j log (1 − σ(ei · oj))) .
</equation>
<bodyText confidence="0.999860333333333">
Here, let us substitute ei · oj in Eq. 3 for si,j,
and then assume that all si,j are free parameters.
Namely, we can freely select the value of si,j in-
dependent from any other si0,j0, where i =6 i0 and
j =6 j0, respectively. The partial derivatives of Ψ
with respect to si,j take the following form:
</bodyText>
<equation confidence="0.994089333333333">
� �
∂si,jΨ = − ci,j�1 − σ(si,j)� − c0 i,jσ(si,j) .
(4)
</equation>
<bodyText confidence="0.998200333333333">
put’ and ‘output’ to reduce the ambiguity since ‘word’ and
‘context’ are exchangeable by the definition of model (i.e.,
SkipGram or CBoW).
The minimizer can be obtained when ∂si,jΨ = 0
for all si,j. By using this relation, we can obtain
the following closed form solution:
</bodyText>
<equation confidence="0.9896415">
�ci,j /
si,j = log
0. (5)
ci j
</equation>
<bodyText confidence="0.998569714285714">
Overall, SGNS approximates the log of the co-
occurrence ratio between ‘real’ training data D
and ‘virtual’ negative sampling data D0 by the in-
ner product of the corresponding input and output
vectors in terms of minimizing the SGNS objec-
tive written in Eq. 2, and Eq. 3 as well. Therefore,
we can obtain the following relation for SGNS:
</bodyText>
<equation confidence="0.8909955">
mi,j = log(c0 ci,j ≈ ei · oj . (6)
i,j/
</equation>
<bodyText confidence="0.932986">
Note that the expectation of c0i is kcicj if the
</bodyText>
<equation confidence="0.696790333333333">
j |D|
negative sampling is assumed to follow unigram
probability cj
</equation>
<bodyText confidence="0.999769357142857">
|D|, and the negative sampling data is
k-times larger than the training data D, where ci =
E j ci,j and cj = Ei ci,j2. The above matches
‘shifted PMI’ as described in (Levy and Goldberg,
2014) when we substitute c0i,j for kcicj
|D |in Eq. 6,
In addition, the word2vec implementation
uses a smoothing factor α to reduce the selec-
tion of high-occurrence-frequency words during
the negative sampling. The expectation of c0i,j
can then be written as: kci �(cj)α
j0(cj0)α . We refer
to log (ci,j kei (c; )α) as ‘α-parameterized shifted
PMI (SPMIk,α)’.
</bodyText>
<subsectionHeader confidence="0.995383">
2.2 Matrix factorization view of GloVe
</subsectionHeader>
<bodyText confidence="0.999854">
The GloVe objective is defined in the following
form (Pennington et al., 2014):
</bodyText>
<equation confidence="0.9869225">
2
β(ci,j) (ei · oj − log(ci,j )) , (7)
</equation>
<bodyText confidence="0.9999418">
where β(·) represent a ‘weighting function’. In
particular, β(·) satisfies the relations 0 ≤ β(x) &lt;
∞, and β(x) = 0 if x = 0. For example, the
following weighting function has been introduced
in (Pennington et al., 2014):
</bodyText>
<equation confidence="0.991796">
� � �γ�
β(x) = min 1, x/xmax . (8)
</equation>
<bodyText confidence="0.995675">
This is worth noting here that the original GloVe
introduces two bias terms, bi and bj, and defines
</bodyText>
<footnote confidence="0.654568666666667">
2Every input of the i-th word samples k words. Therefore,
the negative sampling number is kci. Finally, the expectation
can be obtained by multiplying count kci by probability cj
</footnote>
<equation confidence="0.95006275">
|D|.
�Ψ = �
i j
(3)
</equation>
<page confidence="0.970937">
187
</page>
<bodyText confidence="0.768166571428571">
configuration SGNS GloVe
training unit sample-wise co-occurrence
loss function logistic (Eq. 11) squared (Eq. 12)
neg. sampling explicit no sampling
weight. func. β(·) fixed to 1 Eq. 8
fitting function SPMIk,α log(ci,j)
bias none bi and bj
</bodyText>
<tableCaption confidence="0.8839875">
Table 2: Comparison of the different configura-
tions used in SGNS and GloVe.
</tableCaption>
<bodyText confidence="0.993884833333334">
ei · oj + bi + bj instead of just ei · oj in Eq. 7. For
simplicity and ease of discussion, we do not ex-
plicitly introduce bias terms in this paper. This is
because, without loss of generality, we can embed
the effect of the bias terms in the input and output
vectors by introducing two additional dimensions
for all ei and oj, and fixing parameters ei,D+1 = 1
and oj,D+2 = 1.
According to Eq. 7, GloVe can also be viewed
as a matrix factorization method. Different
from SGNS, GloVe approximates the log of co-
occurrences:
</bodyText>
<equation confidence="0.982166">
mi,j = log(ci,j) ≈ ei · oj, (9)
</equation>
<sectionHeader confidence="0.971874" genericHeader="method">
3 Unified Form of SkipGram and GloVe
</sectionHeader>
<bodyText confidence="0.99997864">
An examination of the differences between Eqs. 6
and 9 finds that Eq. 6 matches Eq. 9 if ci,j = 1.
Recall that ci,j is the number of co-occurrences
of (i, j) in negative sampling data D�. Therefore,
what GloVe approximates is SGNS when the neg-
ative sampling data D� is constructed as 1 for all
co-occurrences. From the viewpoint of matrix fac-
torization, GloVe can be seen as a special case of
SGNS, in that it utilizes a sort of uniform negative
sampling method.
Our assessment of the original GloVe paper
suggests that the name “Global Vector” mainly
stands for the architecture of the two stage learn-
ing framework. Namely, it first counts all the
co-occurrences in D, and then, it leverages the
gathered co-occurrence information for estimating
(possibly better) parameters. In contrast, the name
“SkipGram” stands mainly for the model type;
how it counts the co-occurrences in D. The key
points of these two methods seems different and
do not conflict. Therefore, it is not surprising to
treat these two similar methods as one method; for
example, SkipGram model with two-stage global
vector learning. The following objective function
is a generalized form that subsumes Eqs. 3 and 7:
</bodyText>
<equation confidence="0.8781835">
XΨ = X β(ci,j)Φ(ei, oj, ci,j, c�i,j). (10)
i j
</equation>
<table confidence="0.990427153846154">
hyper-parameter selected value
word2vec glove
context window (W) 10
sub (Levy et al., 2015) dirty, t = 10−5 –
del (Levy et al., 2015) use 400,000 most frequent words
cds (Levy et al., 2015) α = 3/4 –
w+c (Levy et al., 2015) e + o
weight. func. (γ, xmax) – 3/4, 100
initial learning rate (η) 0.025 0.05
# of neg. sampling (k) 5 –
# of iterations (T) 5 20
# of threads 56
# of dimensions (D) 300
</table>
<tableCaption confidence="0.999883">
Table 3: Hyper-parameters in our experiments.
</tableCaption>
<bodyText confidence="0.98437">
In particular, the original SGNS uses β(ci,j) = 1
for all (i, j), and logistic loss function:
</bodyText>
<equation confidence="0.8535155">
Φ(ei, oj,ci,j,ci,j) = ci,j log (σ(ei · oj))
+c�i,j log (1 − σ(ei · oj)).
</equation>
<bodyText confidence="0.941710166666667">
In contrast, GloVe uses a least squared loss func-
tion:
Φ(ei, oj, ci,j, c�i,j) = ~ei · oj − log Q,j 2. i,j
Table 2 lists the factors of each configuration used
differently in SGNS and GloVe.
Note that this unified form also includes
SkipGram with noise contrastive estimation
(SGNCE) (Mnih and Kavukcuoglu, 2013), which
approximates mi,j = log(ci,j
kcj ) in matrix factoriza-
tion view. This paper omits a detailed discussion
of SGNCE for space restrictions.
</bodyText>
<sectionHeader confidence="0.99972" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999918555555556">
Following the series of neural word embedding pa-
pers, our training data is taken from a Wikipedia
dump (Aug. 2014). We tokenized and lowercased
the data yielding about 1.8B tokens.
For the hyper-parameter selection, we mostly
followed the suggestion made in (Levy et al.,
2015). Table 3 summarizes the default values of
hyper-parameters used consistently in all our ex-
periments unless otherwise noted.
</bodyText>
<subsectionHeader confidence="0.996345">
4.1 Benchmark datasets for evaluation
</subsectionHeader>
<bodyText confidence="0.998259142857143">
We prepared eight word similarity benchmark
datasets (WSimilarity), namely, R&amp;G (Ruben-
stein and Goodenough, 1965), M&amp;C (Miller and
Charles, 1991), WSimS (Agirre et al., 2009),
WSimR (Agirre et al., 2009), MEM (Bruni
et al., 2014), MTurk (Radinsky et al., 2011),
SCWS (Huang et al., 2012), and RARE (Luong
</bodyText>
<page confidence="0.996911">
188
</page>
<table confidence="0.9988152">
method time WSimilarity WAnalogy
SGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8)
GloVe (original) 8243 57.6 (57.5, 57.9) 64.8 (64.6, 65.0)
w/o bias terms 8027 57.6 (57.5, 57.7) 64.8 (64.5, 65.0)
fitting=SPMIk,α 8332 57.5 (57.2, 57.8) 65.0 (64.8. 65.1)
</table>
<tableCaption confidence="0.87564">
Table 4: Results: the micro averages of Spear-
man’s rho (WSimilarity) and accuracy (WAnal-
ogy) for all benchmark datasets.
</tableCaption>
<bodyText confidence="0.9943135">
et al., 2013). Moreover, we also prepared three
analogy benchmark datasets (WAnalogy), that is,
GSEM (Mikolov et al., 2013a), GSYN (Mikolov
et al., 2013a), and MSYN (Mikolov et al., 2013b).
</bodyText>
<subsectionHeader confidence="0.916026">
4.2 SGNS and GloVe Results
</subsectionHeader>
<bodyText confidence="0.999975086956522">
Table 4 shows the training time and performance
results gained from our benchmark data. The col-
umn ‘time’ indicates average elapsed time (sec-
ond) for model learning. All the results are the av-
erage performance of ten runs. This is because
the comparison methods have some randomized
factors, such as initial value (since they are non-
convex optimization problems) and (probabilistic)
sampling method, which significantly impact the
results.
At first, we compared the original SGNS as im-
plemented in the word2vec package3 and the
original GloVe as implemented in the glove
package4. These results are shown in the first and
second rows in Table 4. In our experiments, SGNS
significantly outperformed GloVe in WSimilarity
while GloVe significantly outperformed SGNS in
WAnalogy. As we explained, these two methods
can be easily merged into a unified form. Thus,
there must be some differences in their configura-
tions that yields such a large difference in the re-
sults. Next, we tried to determine the clues as the
differences.
</bodyText>
<subsectionHeader confidence="0.999481">
4.3 Impact of incorporating bias terms
</subsectionHeader>
<bodyText confidence="0.999464">
The third row (w/o bias terms) in Table 4 shows
the results of the configuration without using the
bias terms in the glove package. A comparison
with the results of the second row, finds no mean-
ingful benefit to using the bias terms. In contrast,
obviously, the elapsed time for model learning is
consistently shorter since we can discard the bias
term update.
</bodyText>
<footnote confidence="0.999452">
3https://code.google.com/p/word2vec/
4http://nlp.stanford.edu/projects/glove/
</footnote>
<table confidence="0.9443124">
(a) WSimilarity
method W=2 3 5 10 20
SGNS (original) 64.9 65.1 65.4 65.4 64.9
GloVe (original) 53.6 55.7 57.0 57.6 57.8
w/o harmonic func. 54.6 56.9 57.8 58.2 57.9
(b) WAnalogy
method W=2 3 5 10 20
SGNS (original) 62.8 63.5 63.9 63.0 61.3
GloVe (original) 51.7 58.4 62.3 64.8 66.1
w/o harmonic func. 52.6 58.0 60.5 61.6 60.7
</table>
<tableCaption confidence="0.991759">
Table 5: Impact of the context window size, and
harmonic function.
</tableCaption>
<table confidence="0.999691">
W=2 3 5 10 20
(1) 0&lt;ci,j &lt;1 104M 213M 377M 649M 914M
(2) 1≤ci,j 167M 184M 207M 234M 251M
non-zero ci,j 271M 398M 584M 883M 1165M
ratio of (1) 38.5% 53.6% 64.5% 73.5% 78.4%
</table>
<tableCaption confidence="0.995446">
Table 6: The ratio of entries less than one in co-
occurrence matrix.
</tableCaption>
<subsectionHeader confidence="0.998161">
4.4 Impact of fitting function
</subsectionHeader>
<bodyText confidence="0.999975857142857">
The fourth row (fitting=SPMIk,α) in Table 4 shows
the performance when we substituted the fit-
ting function of GloVe, namely, log(ci,j), for
SPMIk=5,α=3/4 used in SGNS. Clearly, the per-
formance becomes nearly identical to the original
GloVe. Accordingly, the selection of fitting func-
tion has only a small impact.
</bodyText>
<subsectionHeader confidence="0.977722">
4.5 Impact of context window size and
harmonic function
</subsectionHeader>
<bodyText confidence="0.999903375">
Table 5 shows the impact of context window size
W. The results of SGNS seem more stable against
W than those of GloVe.
Additionally, we investigated the impact of the
‘harmonic function’ used in GloVe. The ‘har-
monic function’ uses the inverse of context dis-
tance, i.e., 1/a if the context word is a-word away
from the target word, instead of just count 1 re-
gardless of the distance when calculating the co-
occurrences. Clearly, GloVe without using the
harmonic function shown in the third row of Ta-
ble 5 yielded significantly degraded performance
on WAnalogy, and slight improvement on WSimi-
larity. This fact may imply that the higher WAnal-
ogy performance of GloVe was derived by the ef-
fect of this configuration.
</bodyText>
<subsectionHeader confidence="0.986227">
4.6 Link between harmonic function and
negative sampling
</subsectionHeader>
<bodyText confidence="0.9649536">
This section further discusses a benefit of har-
monic function.
Recall that GloVe does not explicitly consider
‘negative samples’. It fixes cz,j = 1 for all (i, j)
as shown in Eq. 7. However, the co-occurrence
</bodyText>
<page confidence="0.997839">
189
</page>
<bodyText confidence="0.99998541025641">
count given by using the harmonic function can
take values less than 1, i.e., ci,j = 2/3, if the i-
th word and the j-th word co-occurred twice with
distance 3. As a result, the value of the fitting func-
tion of GloVe becomes log(2/3). Interestingly,
this is essentially equivalent to co-occur 3 times in
the negative sampling data and 2 times in the real
data since the fitting function of the unified form
shown in Eq. 12 is log(ci,j/cz,j) = log(2/3) when
ci,j = 2 and c�i,j = 3. It is not surprising that rare
co-occurrence words that occur only in long range
contexts may have almost no correlation between
them. Thus treating them as negative samples will
not create a problem in most cases. Therefore, the
harmonic function seems to ‘unexpectedly’ mimic
a kind of a negative sampling method; it is inter-
preted as ‘implicitly’ generating negative data.
Table 6 shows the ratio of the entries ci,j whose
value is less than one in matrix M. Remember
that vocabulary size was 400,000 in our experi-
ments. Thus, we had a total of 400K×400K=160B
elements in M, and most were 0. Here, we con-
sider only non-zero entries. It is clear that longer
context window sizes generated many more en-
tries categorized in 0 &lt; ci,j &lt; 1 by the har-
monic function. One important observation is that
the ratio of 0 &lt; ci,j &lt; 1 is gradually increas-
ing, which offers a similar effect to increasing the
number of negative samples. This can be a rea-
son why GloVe demonstrated consistent improve-
ments in WAnalogy performance as context win-
dow increased since larger negative sampling size
often improves performance (Levy et al., 2015).
Note also that the number of 0 &lt; ci,j &lt; 1 always
becomes 0 in the configuration without the har-
monic function. This is equivalent to using uni-
form negative sampling cz,j = 1 as described in
Sec. 3. This fact also indicates the importance of
the negative sampling method.
</bodyText>
<subsectionHeader confidence="0.997257">
4.7 Impact of weighting function
</subsectionHeader>
<bodyText confidence="0.99989925">
Table 7 shows the impact of weighting function
used in GloVe, namely, Eq 8. Note that ‘β(·)=1’
column shows the results when we fixed 1 for
all non-zero entries5. This is also clear that the
weighting function Eq 8 with appropriate param-
eters significantly improved the performance of
both WSimilarity and WAnalogy tasks. How-
ever unfortunately, the best parameter values for
</bodyText>
<footnote confidence="0.9760195">
5This is equivalent to set 0 to -x-max option in glove
implementation.
</footnote>
<table confidence="0.896709833333333">
(a) WSimilarity
hyper param. 3(·)=1 xmax = 1 10 100 10000
7 = 0.75 59.4 60.1 60.9 57.7 49.5
w/o harmonic func. 58.2 58.0 60.7 58.2 56.0
7 = 1.0 (59.4) 60.1 59.4 55.9 36.1
w/o harmonic func. (58.2) 58.3 60.7 57.7 46.7
(b) WAnalogy
hyper param. 3(·)=1 xmax = 1 10 100 10000
7 = 0.75 55.7 61.1 64.3 64.8 28.4
w/o harmonic func. 53.4 52.6 60.3 61.6 42.5
7 = 1.0 (55.7) 61.0 63.8 59.1 7.5
w/o harmonic func. (53.4) 54.1 60.8 60.1 20.3
</table>
<tableCaption confidence="0.999232">
Table 7: Impact of the weighting function.
</tableCaption>
<bodyText confidence="0.9709486">
WSimilarity and WAnalogy tasks looks different.
We emphasize that harmonic function discussed
in the previous sub-section was still a necessary
condition to obtain the best performance, and bet-
ter performance in the case of ‘β(·)=1’ as well.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="evaluation">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999992117647059">
This paper reconsidered the relationship between
SkipGram and GloVe models in machine learn-
ing viewpoint. We showed that SGNS and GloVe
can be easily merged into a unified form. We
also extracted the factors of the configurations
that are used differently. We empirically inves-
tigated which learning configuration is responsi-
ble for the performance difference often observed
in widely examined word similarity and analogy
tasks. Finally, we found that at least two config-
urations, namely, the weighting function and har-
monic function, had significant impacts on the per-
formance. Additionally, we revealed a relation-
ship between harmonic function and negative sam-
pling. We hope that our theoretical and empirical
analyses will offer a deeper understanding of these
neural word embedding methods6.
</bodyText>
<sectionHeader confidence="0.937447" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9997065">
We thank three anonymous reviewers for their
helpful comments.
</bodyText>
<sectionHeader confidence="0.978415" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.683037222222222">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ’09, pages 19–27, Stroudsburg, PA, USA.
Association for Computational Linguistics.
</bodyText>
<footnote confidence="0.9979515">
6The modified codes for our experiments will be available
in author’s homepage
</footnote>
<page confidence="0.993616">
190
</page>
<reference confidence="0.999868328571428">
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. J. Artif. Int.
Res., 49(1):1–47, January.
Yoav Goldberg and Omer Levy. 2014. word2vec
Explained: Deriving Mikolov et al.’s Negative-
sampling Word-embedding Method. CoRR,
abs/1402.3722.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Omer Levy and Yoav Goldberg. 2014. Neural
Word Embedding as Implicit Matrix Factorization.
In Z. Ghahramani, M. Welling, C. Cortes, N.D.
Lawrence, and K.Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, pages
2177–2185. Curran Associates, Inc.
Omer Levy, Yoav Goldberg, and Ido Dagan. 2015.
Improving Distributional Similarity with Lessons
Learned from Word Embeddings. Transactions of
the Association for Computational Linguistics, 3.
Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 104–113,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
George A. Miller and Walter G. Charles. 1991. Con-
textual Correlates of Semantic Similarity. Language
&amp; Cognitive Processes, 6(1):1–28.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 2265–2273. Curran Associates, Inc.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar, October. Association for Computational Lin-
guistics.
Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: Computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th International Conference on World Wide Web,
WWW ’11, pages 337–346, New York, NY, USA.
ACM.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627–633, October.
Tianze Shi and Zhiyuan Liu. 2014. Linking GloVe
with word2vec. CoRR, abs/1411.5595.
</reference>
<page confidence="0.99848">
191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923369">
<title confidence="0.999894">A Unified Learning Framework of Skip-Grams and Global Vectors</title>
<author confidence="0.999655">Suzuki Nagata</author>
<affiliation confidence="0.999683">NTT Communication Science Laboratories, NTT Corporation</affiliation>
<address confidence="0.93126">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan</address>
<abstract confidence="0.999499">Log-bilinear language models such as SkipGram and GloVe have been proven to capture high quality syntactic and semantic relationships between words in a vector space. We revisit the relationship between SkipGram and GloVe models from a machine learning viewpoint, and show that these two methods are easily merged into a unified form. Then, by using the unified form, we extract the factors of the configurations that they use differently. We also empirically investigate which factor is responsible for the performance difference often observed in widely examined word similarity and analogy tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>49</volume>
<issue>1</issue>
<contexts>
<context position="11798" citStr="Bruni et al., 2014" startWordPosition="2018" endWordPosition="2021">rs, our training data is taken from a Wikipedia dump (Aug. 2014). We tokenized and lowercased the data yielding about 1.8B tokens. For the hyper-parameter selection, we mostly followed the suggestion made in (Levy et al., 2015). Table 3 summarizes the default values of hyper-parameters used consistently in all our experiments unless otherwise noted. 4.1 Benchmark datasets for evaluation We prepared eight word similarity benchmark datasets (WSimilarity), namely, R&amp;G (Rubenstein and Goodenough, 1965), M&amp;C (Miller and Charles, 1991), WSimS (Agirre et al., 2009), WSimR (Agirre et al., 2009), MEM (Bruni et al., 2014), MTurk (Radinsky et al., 2011), SCWS (Huang et al., 2012), and RARE (Luong 188 method time WSimilarity WAnalogy SGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8) GloVe (original) 8243 57.6 (57.5, 57.9) 64.8 (64.6, 65.0) w/o bias terms 8027 57.6 (57.5, 57.7) 64.8 (64.5, 65.0) fitting=SPMIk,α 8332 57.5 (57.2, 57.8) 65.0 (64.8. 65.1) Table 4: Results: the micro averages of Spearman’s rho (WSimilarity) and accuracy (WAnalogy) for all benchmark datasets. et al., 2013). Moreover, we also prepared three analogy benchmark datasets (WAnalogy), that is, GSEM (Mikolov et al., 2013a), GSYN (Mikolo</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. J. Artif. Int. Res., 49(1):1–47, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Omer Levy</author>
</authors>
<title>word2vec Explained: Deriving Mikolov et al.’s Negativesampling Word-embedding Method.</title>
<date>2014</date>
<location>CoRR, abs/1402.3722.</location>
<contexts>
<context position="5258" citStr="Goldberg and Levy, 2014" startWordPosition="848" endWordPosition="851"> with negative sampling The primitive training sample for SkipGram is a pair of a target word and its corresponding context word. Thus, we can represent the training data of SkipGram as a list of input and output index pairs, that is, D = {(in, jn)}Nn=1. Thus the estimation problem of ‘SkipGram with negative sampling (SGNS)’ is defined as the minimization problem of objective function Ψ: Ψ = − � log (σ(ein · ojn)) �− (in,jn)∈D log �1 − σ(ein · ojn)�, (2) (in,jn)∈D0 where the optimization parameters are ei and oj for all i and j. Note that we explicitly represent the negative sampling data D0 (Goldberg and Levy, 2014). Let us assume that, in a preliminary step, we count all co-occurrences in D. Then, the SGNS objective in Eq. 2 can be rewritten as follows by a simple reformulation: Ψ = − � � (ci,j log (σ(ei · oj)) i j +c0i,j log (1 − σ(ei · oj))) . Here, let us substitute ei · oj in Eq. 3 for si,j, and then assume that all si,j are free parameters. Namely, we can freely select the value of si,j independent from any other si0,j0, where i =6 i0 and j =6 j0, respectively. The partial derivatives of Ψ with respect to si,j take the following form: � � ∂si,jΨ = − ci,j�1 − σ(si,j)� − c0 i,jσ(si,j) . (4) put’ and </context>
</contexts>
<marker>Goldberg, Levy, 2014</marker>
<rawString>Yoav Goldberg and Omer Levy. 2014. word2vec Explained: Deriving Mikolov et al.’s Negativesampling Word-embedding Method. CoRR, abs/1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers -</booktitle>
<volume>1</volume>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11856" citStr="Huang et al., 2012" startWordPosition="2028" endWordPosition="2031"> 2014). We tokenized and lowercased the data yielding about 1.8B tokens. For the hyper-parameter selection, we mostly followed the suggestion made in (Levy et al., 2015). Table 3 summarizes the default values of hyper-parameters used consistently in all our experiments unless otherwise noted. 4.1 Benchmark datasets for evaluation We prepared eight word similarity benchmark datasets (WSimilarity), namely, R&amp;G (Rubenstein and Goodenough, 1965), M&amp;C (Miller and Charles, 1991), WSimS (Agirre et al., 2009), WSimR (Agirre et al., 2009), MEM (Bruni et al., 2014), MTurk (Radinsky et al., 2011), SCWS (Huang et al., 2012), and RARE (Luong 188 method time WSimilarity WAnalogy SGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8) GloVe (original) 8243 57.6 (57.5, 57.9) 64.8 (64.6, 65.0) w/o bias terms 8027 57.6 (57.5, 57.7) 64.8 (64.5, 65.0) fitting=SPMIk,α 8332 57.5 (57.2, 57.8) 65.0 (64.8. 65.1) Table 4: Results: the micro averages of Spearman’s rho (WSimilarity) and accuracy (WAnalogy) for all benchmark datasets. et al., 2013). Moreover, we also prepared three analogy benchmark datasets (WAnalogy), that is, GSEM (Mikolov et al., 2013a), GSYN (Mikolov et al., 2013a), and MSYN (Mikolov et al., 2013b). 4.2 SG</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural Word Embedding as Implicit Matrix Factorization. In</title>
<date>2014</date>
<booktitle>Advances in Neural Information Processing Systems 27,</booktitle>
<pages>2177--2185</pages>
<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="4404" citStr="Levy and Goldberg, 2014" startWordPosition="694" endWordPosition="697">e use the terms ‘in186 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 186–191, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Roughly speaking, SkipGram models word-toword co-occurrences, which are extracted within the predefined context window size, by the input and output vectors. Recently, SkipGram has been interpreted as implicitly factorizing the matrix, where the factors are calculated from co-occurrence information (Levy and Goldberg, 2014). Let mi,j be the (i, j)-factor of matrix M to be ‘implicitly’ factorized by SkipGram. SkipGram approximates each mi,j by the inner product of the corresponding input and output vectors, that is: mi,j ≈ ei · oj, (1) 2.1.1 SkipGram with negative sampling The primitive training sample for SkipGram is a pair of a target word and its corresponding context word. Thus, we can represent the training data of SkipGram as a list of input and output index pairs, that is, D = {(in, jn)}Nn=1. Thus the estimation problem of ‘SkipGram with negative sampling (SGNS)’ is defined as the minimization problem of o</context>
<context position="6838" citStr="Levy and Goldberg, 2014" startWordPosition="1152" endWordPosition="1155">between ‘real’ training data D and ‘virtual’ negative sampling data D0 by the inner product of the corresponding input and output vectors in terms of minimizing the SGNS objective written in Eq. 2, and Eq. 3 as well. Therefore, we can obtain the following relation for SGNS: mi,j = log(c0 ci,j ≈ ei · oj . (6) i,j/ Note that the expectation of c0i is kcicj if the j |D| negative sampling is assumed to follow unigram probability cj |D|, and the negative sampling data is k-times larger than the training data D, where ci = E j ci,j and cj = Ei ci,j2. The above matches ‘shifted PMI’ as described in (Levy and Goldberg, 2014) when we substitute c0i,j for kcicj |D |in Eq. 6, In addition, the word2vec implementation uses a smoothing factor α to reduce the selection of high-occurrence-frequency words during the negative sampling. The expectation of c0i,j can then be written as: kci �(cj)α j0(cj0)α . We refer to log (ci,j kei (c; )α) as ‘α-parameterized shifted PMI (SPMIk,α)’. 2.2 Matrix factorization view of GloVe The GloVe objective is defined in the following form (Pennington et al., 2014): 2 β(ci,j) (ei · oj − log(ci,j )) , (7) where β(·) represent a ‘weighting function’. In particular, β(·) satisfies the relation</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Neural Word Embedding as Implicit Matrix Factorization. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2177–2185. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
</authors>
<title>Improving Distributional Similarity with Lessons Learned from Word Embeddings.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>3</volume>
<contexts>
<context position="1457" citStr="Levy et al., 2015" startWordPosition="206" endWordPosition="209">y and analogy tasks. 1 Introduction Neural-network-inspired word embedding methods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and semantic relationships between words in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers in the community have discussed a relationship, and/or which is superior, SkipGram or GloVe. From this background, we revisit the relationship between SkipGram and GloVe from a machine learning viewpoint. We show that it is natV :set of vocabulary (set of words) |V |:vocabulary size, or number of words in V i :index of the input vector, where i ∈ {1, ... , |V|} j :index of</context>
<context position="10143" citStr="Levy et al., 2015" startWordPosition="1731" endWordPosition="1734">-occurrence information for estimating (possibly better) parameters. In contrast, the name “SkipGram” stands mainly for the model type; how it counts the co-occurrences in D. The key points of these two methods seems different and do not conflict. Therefore, it is not surprising to treat these two similar methods as one method; for example, SkipGram model with two-stage global vector learning. The following objective function is a generalized form that subsumes Eqs. 3 and 7: XΨ = X β(ci,j)Φ(ei, oj, ci,j, c�i,j). (10) i j hyper-parameter selected value word2vec glove context window (W) 10 sub (Levy et al., 2015) dirty, t = 10−5 – del (Levy et al., 2015) use 400,000 most frequent words cds (Levy et al., 2015) α = 3/4 – w+c (Levy et al., 2015) e + o weight. func. (γ, xmax) – 3/4, 100 initial learning rate (η) 0.025 0.05 # of neg. sampling (k) 5 – # of iterations (T) 5 20 # of threads 56 # of dimensions (D) 300 Table 3: Hyper-parameters in our experiments. In particular, the original SGNS uses β(ci,j) = 1 for all (i, j), and logistic loss function: Φ(ei, oj,ci,j,ci,j) = ci,j log (σ(ei · oj)) +c�i,j log (1 − σ(ei · oj)). In contrast, GloVe uses a least squared loss function: Φ(ei, oj, ci,j, c�i,j) = ~ei </context>
<context position="11406" citStr="Levy et al., 2015" startWordPosition="1960" endWordPosition="1963">rs of each configuration used differently in SGNS and GloVe. Note that this unified form also includes SkipGram with noise contrastive estimation (SGNCE) (Mnih and Kavukcuoglu, 2013), which approximates mi,j = log(ci,j kcj ) in matrix factorization view. This paper omits a detailed discussion of SGNCE for space restrictions. 4 Experiments Following the series of neural word embedding papers, our training data is taken from a Wikipedia dump (Aug. 2014). We tokenized and lowercased the data yielding about 1.8B tokens. For the hyper-parameter selection, we mostly followed the suggestion made in (Levy et al., 2015). Table 3 summarizes the default values of hyper-parameters used consistently in all our experiments unless otherwise noted. 4.1 Benchmark datasets for evaluation We prepared eight word similarity benchmark datasets (WSimilarity), namely, R&amp;G (Rubenstein and Goodenough, 1965), M&amp;C (Miller and Charles, 1991), WSimS (Agirre et al., 2009), WSimR (Agirre et al., 2009), MEM (Bruni et al., 2014), MTurk (Radinsky et al., 2011), SCWS (Huang et al., 2012), and RARE (Luong 188 method time WSimilarity WAnalogy SGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8) GloVe (original) 8243 57.6 (57.5, 57.9</context>
<context position="17599" citStr="Levy et al., 2015" startWordPosition="2997" endWordPosition="3000">ur experiments. Thus, we had a total of 400K×400K=160B elements in M, and most were 0. Here, we consider only non-zero entries. It is clear that longer context window sizes generated many more entries categorized in 0 &lt; ci,j &lt; 1 by the harmonic function. One important observation is that the ratio of 0 &lt; ci,j &lt; 1 is gradually increasing, which offers a similar effect to increasing the number of negative samples. This can be a reason why GloVe demonstrated consistent improvements in WAnalogy performance as context window increased since larger negative sampling size often improves performance (Levy et al., 2015). Note also that the number of 0 &lt; ci,j &lt; 1 always becomes 0 in the configuration without the harmonic function. This is equivalent to using uniform negative sampling cz,j = 1 as described in Sec. 3. This fact also indicates the importance of the negative sampling method. 4.7 Impact of weighting function Table 7 shows the impact of weighting function used in GloVe, namely, Eq 8. Note that ‘β(·)=1’ column shows the results when we fixed 1 for all non-zero entries5. This is also clear that the weighting function Eq 8 with appropriate parameters significantly improved the performance of both WSim</context>
</contexts>
<marker>Levy, Goldberg, Dagan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving Distributional Similarity with Lessons Learned from Word Embeddings. Transactions of the Association for Computational Linguistics, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>104--113</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Thang Luong, Richard Socher, and Christopher Manning. 2013. Better word representations with recursive neural networks for morphology. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104–113, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="1082" citStr="Mikolov et al., 2013" startWordPosition="154" endWordPosition="157"> SkipGram and GloVe models from a machine learning viewpoint, and show that these two methods are easily merged into a unified form. Then, by using the unified form, we extract the factors of the configurations that they use differently. We also empirically investigate which factor is responsible for the performance difference often observed in widely examined word similarity and analogy tasks. 1 Introduction Neural-network-inspired word embedding methods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and semantic relationships between words in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers </context>
<context position="12382" citStr="Mikolov et al., 2013" startWordPosition="2110" endWordPosition="2113">al., 2009), MEM (Bruni et al., 2014), MTurk (Radinsky et al., 2011), SCWS (Huang et al., 2012), and RARE (Luong 188 method time WSimilarity WAnalogy SGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8) GloVe (original) 8243 57.6 (57.5, 57.9) 64.8 (64.6, 65.0) w/o bias terms 8027 57.6 (57.5, 57.7) 64.8 (64.5, 65.0) fitting=SPMIk,α 8332 57.5 (57.2, 57.8) 65.0 (64.8. 65.1) Table 4: Results: the micro averages of Spearman’s rho (WSimilarity) and accuracy (WAnalogy) for all benchmark datasets. et al., 2013). Moreover, we also prepared three analogy benchmark datasets (WAnalogy), that is, GSEM (Mikolov et al., 2013a), GSYN (Mikolov et al., 2013a), and MSYN (Mikolov et al., 2013b). 4.2 SGNS and GloVe Results Table 4 shows the training time and performance results gained from our benchmark data. The column ‘time’ indicates average elapsed time (second) for model learning. All the results are the average performance of ten runs. This is because the comparison methods have some randomized factors, such as initial value (since they are nonconvex optimization problems) and (probabilistic) sampling method, which significantly impact the results. At first, we compared the original SGNS as implemented in the wor</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>746--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1082" citStr="Mikolov et al., 2013" startWordPosition="154" endWordPosition="157"> SkipGram and GloVe models from a machine learning viewpoint, and show that these two methods are easily merged into a unified form. Then, by using the unified form, we extract the factors of the configurations that they use differently. We also empirically investigate which factor is responsible for the performance difference often observed in widely examined word similarity and analogy tasks. 1 Introduction Neural-network-inspired word embedding methods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and semantic relationships between words in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers </context>
<context position="12382" citStr="Mikolov et al., 2013" startWordPosition="2110" endWordPosition="2113">al., 2009), MEM (Bruni et al., 2014), MTurk (Radinsky et al., 2011), SCWS (Huang et al., 2012), and RARE (Luong 188 method time WSimilarity WAnalogy SGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8) GloVe (original) 8243 57.6 (57.5, 57.9) 64.8 (64.6, 65.0) w/o bias terms 8027 57.6 (57.5, 57.7) 64.8 (64.5, 65.0) fitting=SPMIk,α 8332 57.5 (57.2, 57.8) 65.0 (64.8. 65.1) Table 4: Results: the micro averages of Spearman’s rho (WSimilarity) and accuracy (WAnalogy) for all benchmark datasets. et al., 2013). Moreover, we also prepared three analogy benchmark datasets (WAnalogy), that is, GSEM (Mikolov et al., 2013a), GSYN (Mikolov et al., 2013a), and MSYN (Mikolov et al., 2013b). 4.2 SGNS and GloVe Results Table 4 shows the training time and performance results gained from our benchmark data. The column ‘time’ indicates average elapsed time (second) for model learning. All the results are the average performance of ten runs. This is because the comparison methods have some randomized factors, such as initial value (since they are nonconvex optimization problems) and (probabilistic) sampling method, which significantly impact the results. At first, we compared the original SGNS as implemented in the wor</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<date>1991</date>
<booktitle>Contextual Correlates of Semantic Similarity. Language &amp; Cognitive Processes,</booktitle>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="11714" citStr="Miller and Charles, 1991" startWordPosition="2003" endWordPosition="2006">E for space restrictions. 4 Experiments Following the series of neural word embedding papers, our training data is taken from a Wikipedia dump (Aug. 2014). We tokenized and lowercased the data yielding about 1.8B tokens. For the hyper-parameter selection, we mostly followed the suggestion made in (Levy et al., 2015). Table 3 summarizes the default values of hyper-parameters used consistently in all our experiments unless otherwise noted. 4.1 Benchmark datasets for evaluation We prepared eight word similarity benchmark datasets (WSimilarity), namely, R&amp;G (Rubenstein and Goodenough, 1965), M&amp;C (Miller and Charles, 1991), WSimS (Agirre et al., 2009), WSimR (Agirre et al., 2009), MEM (Bruni et al., 2014), MTurk (Radinsky et al., 2011), SCWS (Huang et al., 2012), and RARE (Luong 188 method time WSimilarity WAnalogy SGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8) GloVe (original) 8243 57.6 (57.5, 57.9) 64.8 (64.6, 65.0) w/o bias terms 8027 57.6 (57.5, 57.7) 64.8 (64.5, 65.0) fitting=SPMIk,α 8332 57.5 (57.2, 57.8) 65.0 (64.8. 65.1) Table 4: Results: the micro averages of Spearman’s rho (WSimilarity) and accuracy (WAnalogy) for all benchmark datasets. et al., 2013). Moreover, we also prepared three analog</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual Correlates of Semantic Similarity. Language &amp; Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>2265--2273</pages>
<editor>In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="3537" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="567" endWordPosition="570">e being their learning configurations. The final goal of this paper is to provide a unified learning framework that encompasses the configurations used in SkipGram and GloVe to gain a deeper understanding of the behavior of these embedding methods. We also empirically investigate which learning configuration most clearly elucidates the performance difference often observed in word similarity and analogy tasks. 2 SkipGram and GloVe Table 1 shows the notations used in this paper. 2.1 Matrix factorization view of SkipGram SkipGram can be categorized as one of the simplest neural language models (Mnih and Kavukcuoglu, 2013). It generally assigns two distinct D-dimensional vectors to each word in vocabulary V; one is ‘input vector’, and the other is ‘output vector’1. 1These two vectors are generally referred to as ‘word (or target) vector’ and ‘context vector’. We use the terms ‘in186 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 186–191, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Roughly speaking, SkipGram models word-toword co-occurrenc</context>
<context position="10970" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="1890" endWordPosition="1893">0.025 0.05 # of neg. sampling (k) 5 – # of iterations (T) 5 20 # of threads 56 # of dimensions (D) 300 Table 3: Hyper-parameters in our experiments. In particular, the original SGNS uses β(ci,j) = 1 for all (i, j), and logistic loss function: Φ(ei, oj,ci,j,ci,j) = ci,j log (σ(ei · oj)) +c�i,j log (1 − σ(ei · oj)). In contrast, GloVe uses a least squared loss function: Φ(ei, oj, ci,j, c�i,j) = ~ei · oj − log Q,j 2. i,j Table 2 lists the factors of each configuration used differently in SGNS and GloVe. Note that this unified form also includes SkipGram with noise contrastive estimation (SGNCE) (Mnih and Kavukcuoglu, 2013), which approximates mi,j = log(ci,j kcj ) in matrix factorization view. This paper omits a detailed discussion of SGNCE for space restrictions. 4 Experiments Following the series of neural word embedding papers, our training data is taken from a Wikipedia dump (Aug. 2014). We tokenized and lowercased the data yielding about 1.8B tokens. For the hyper-parameter selection, we mostly followed the suggestion made in (Levy et al., 2015). Table 3 summarizes the default values of hyper-parameters used consistently in all our experiments unless otherwise noted. 4.1 Benchmark datasets for evaluation W</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2265–2273. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1532--1543</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="1328" citStr="Pennington et al., 2014" startWordPosition="188" endWordPosition="191">lso empirically investigate which factor is responsible for the performance difference often observed in widely examined word similarity and analogy tasks. 1 Introduction Neural-network-inspired word embedding methods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and semantic relationships between words in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers in the community have discussed a relationship, and/or which is superior, SkipGram or GloVe. From this background, we revisit the relationship between SkipGram and GloVe from a machine learning viewpoint. We show that it is natV :set of vocabular</context>
<context position="7310" citStr="Pennington et al., 2014" startWordPosition="1229" endWordPosition="1232">-times larger than the training data D, where ci = E j ci,j and cj = Ei ci,j2. The above matches ‘shifted PMI’ as described in (Levy and Goldberg, 2014) when we substitute c0i,j for kcicj |D |in Eq. 6, In addition, the word2vec implementation uses a smoothing factor α to reduce the selection of high-occurrence-frequency words during the negative sampling. The expectation of c0i,j can then be written as: kci �(cj)α j0(cj0)α . We refer to log (ci,j kei (c; )α) as ‘α-parameterized shifted PMI (SPMIk,α)’. 2.2 Matrix factorization view of GloVe The GloVe objective is defined in the following form (Pennington et al., 2014): 2 β(ci,j) (ei · oj − log(ci,j )) , (7) where β(·) represent a ‘weighting function’. In particular, β(·) satisfies the relations 0 ≤ β(x) &lt; ∞, and β(x) = 0 if x = 0. For example, the following weighting function has been introduced in (Pennington et al., 2014): � � �γ� β(x) = min 1, x/xmax . (8) This is worth noting here that the original GloVe introduces two bias terms, bi and bj, and defines 2Every input of the i-th word samples k words. Therefore, the negative sampling number is kci. Finally, the expectation can be obtained by multiplying count kci by probability cj |D|. �Ψ = � i j (3) 187</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eugene Agichtein</author>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>A word at a time: Computing word relatedness using temporal semantic analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th International Conference on World Wide Web, WWW ’11,</booktitle>
<pages>337--346</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11829" citStr="Radinsky et al., 2011" startWordPosition="2023" endWordPosition="2026">en from a Wikipedia dump (Aug. 2014). We tokenized and lowercased the data yielding about 1.8B tokens. For the hyper-parameter selection, we mostly followed the suggestion made in (Levy et al., 2015). Table 3 summarizes the default values of hyper-parameters used consistently in all our experiments unless otherwise noted. 4.1 Benchmark datasets for evaluation We prepared eight word similarity benchmark datasets (WSimilarity), namely, R&amp;G (Rubenstein and Goodenough, 1965), M&amp;C (Miller and Charles, 1991), WSimS (Agirre et al., 2009), WSimR (Agirre et al., 2009), MEM (Bruni et al., 2014), MTurk (Radinsky et al., 2011), SCWS (Huang et al., 2012), and RARE (Luong 188 method time WSimilarity WAnalogy SGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8) GloVe (original) 8243 57.6 (57.5, 57.9) 64.8 (64.6, 65.0) w/o bias terms 8027 57.6 (57.5, 57.7) 64.8 (64.5, 65.0) fitting=SPMIk,α 8332 57.5 (57.2, 57.8) 65.0 (64.8. 65.1) Table 4: Results: the micro averages of Spearman’s rho (WSimilarity) and accuracy (WAnalogy) for all benchmark datasets. et al., 2013). Moreover, we also prepared three analogy benchmark datasets (WAnalogy), that is, GSEM (Mikolov et al., 2013a), GSYN (Mikolov et al., 2013a), and MSYN (Mik</context>
</contexts>
<marker>Radinsky, Agichtein, Gabrilovich, Markovitch, 2011</marker>
<rawString>Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a time: Computing word relatedness using temporal semantic analysis. In Proceedings of the 20th International Conference on World Wide Web, WWW ’11, pages 337–346, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Commun. ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="11682" citStr="Rubenstein and Goodenough, 1965" startWordPosition="1997" endWordPosition="2001">per omits a detailed discussion of SGNCE for space restrictions. 4 Experiments Following the series of neural word embedding papers, our training data is taken from a Wikipedia dump (Aug. 2014). We tokenized and lowercased the data yielding about 1.8B tokens. For the hyper-parameter selection, we mostly followed the suggestion made in (Levy et al., 2015). Table 3 summarizes the default values of hyper-parameters used consistently in all our experiments unless otherwise noted. 4.1 Benchmark datasets for evaluation We prepared eight word similarity benchmark datasets (WSimilarity), namely, R&amp;G (Rubenstein and Goodenough, 1965), M&amp;C (Miller and Charles, 1991), WSimS (Agirre et al., 2009), WSimR (Agirre et al., 2009), MEM (Bruni et al., 2014), MTurk (Radinsky et al., 2011), SCWS (Huang et al., 2012), and RARE (Luong 188 method time WSimilarity WAnalogy SGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8) GloVe (original) 8243 57.6 (57.5, 57.9) 64.8 (64.6, 65.0) w/o bias terms 8027 57.6 (57.5, 57.7) 64.8 (64.5, 65.0) fitting=SPMIk,α 8332 57.5 (57.2, 57.8) 65.0 (64.8. 65.1) Table 4: Results: the micro averages of Spearman’s rho (WSimilarity) and accuracy (WAnalogy) for all benchmark datasets. et al., 2013). Moreove</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM, 8(10):627–633, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tianze Shi</author>
<author>Zhiyuan Liu</author>
</authors>
<date>2014</date>
<booktitle>Linking GloVe with word2vec. CoRR, abs/1411.5595.</booktitle>
<contexts>
<context position="1659" citStr="Shi and Liu, 2014" startWordPosition="238" endWordPosition="241">s in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers in the community have discussed a relationship, and/or which is superior, SkipGram or GloVe. From this background, we revisit the relationship between SkipGram and GloVe from a machine learning viewpoint. We show that it is natV :set of vocabulary (set of words) |V |:vocabulary size, or number of words in V i :index of the input vector, where i ∈ {1, ... , |V|} j :index of the output vector, where j ∈ {1, ... , |V|} ei :input vector of the i-th word in V oj :output vector of the j-th word in V If i = j, then ei and oj are the input and output vectors of the same word in </context>
</contexts>
<marker>Shi, Liu, 2014</marker>
<rawString>Tianze Shi and Zhiyuan Liu. 2014. Linking GloVe with word2vec. CoRR, abs/1411.5595.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>