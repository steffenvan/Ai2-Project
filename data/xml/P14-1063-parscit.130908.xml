<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992873">
Online Learning in Tensor Space
</title>
<author confidence="0.938619">
Yuan Cao Sanjeev Khudanpur
</author>
<affiliation confidence="0.885514">
Center for Language &amp; Speech Processing and Human Language Technology Center of Excellence
The Johns Hopkins University
</affiliation>
<address confidence="0.989792">
Baltimore, MD, USA, 21218
</address>
<email confidence="0.999631">
{yuan.cao, khudanpur}@jhu.edu
</email>
<sectionHeader confidence="0.994818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996875">
We propose an online learning algorithm
based on tensor-space models. A tensor-
space model represents data in a compact
way, and via rank-1 approximation the
weight tensor can be made highly struc-
tured, resulting in a significantly smaller
number of free parameters to be estimated
than in comparable vector-space models.
This regularizes the model complexity and
makes the tensor model highly effective in
situations where a large feature set is de-
fined but very limited resources are avail-
able for training. We apply with the pro-
posed algorithm to a parsing task, and
show that even with very little training
data the learning algorithm based on a ten-
sor model performs well, and gives signif-
icantly better results than standard learn-
ing algorithms based on traditional vector-
space models.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952634615385">
Many NLP applications use models that try to in-
corporate a large number of linguistic features so
that as much human knowledge of language can
be brought to bear on the (prediction) task as pos-
sible. This also makes training the model param-
eters a challenging problem, since the amount of
labeled training data is usually small compared to
the size of feature sets: the feature weights cannot
be estimated reliably.
Most traditional models are linear models, in
the sense that both the features of the data and
model parameters are represented as vectors in a
vector space. Many learning algorithms applied
to NLP problems, such as the Perceptron (Collins,
2002), MIRA (Crammer et al., 2006; McDonald
et al., 2005; Chiang et al., 2008), PRO (Hop-
kins and May, 2011), RAMPION (Gimpel and
Smith, 2012) etc., are based on vector-space mod-
els. Such models require learning individual fea-
ture weights directly, so that the number of param-
eters to be estimated is identical to the size of the
feature set. When millions of features are used but
the amount of labeled data is limited, it can be dif-
ficult to precisely estimate each feature weight.
In this paper, we shift the model from vector-
space to tensor-space. Data can be represented
in a compact and structured way using tensors as
containers. Tensor representations have been ap-
plied to computer vision problems (Hazan et al.,
2005; Shashua and Hazan, 2005) and information
retrieval (Cai et al., 2006a) a long time ago. More
recently, it has also been applied to parsing (Cohen
and Collins, 2012; Cohen and Satta, 2013) and se-
mantic analysis (Van de Cruys et al., 2013). A
linear tensor model represents both features and
weights in tensor-space, hence the weight tensor
can be factorized and approximated by a linear
sum of rank-1 tensors. This low-rank approxi-
mation imposes structural constraints on the fea-
ture weights and can be regarded as a form of
regularization. With this representation, we no
longer need to estimate individual feature weights
directly but only a small number of “bases” in-
stead. This property makes the the tensor model
very effective when training a large number of fea-
ture weights in a low-resource environment. On
the other hand, tensor models have many more de-
grees of “design freedom” than vector space mod-
els. While this makes them very flexible, it also
creates much difficulty in designing an optimal
tensor structure for a given training set.
We give detailed description of the tensor space
</bodyText>
<page confidence="0.977479">
666
</page>
<note confidence="0.8385815">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 666–675,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.989361857142857">
written as the outer product of D vectors, i.e.
model in Section 2. Several issues that come
with the tensor model construction are addressed
in Section 3. A tensor weight learning algorithm
is then proposed in 4. Finally we give our exper-
imental results on a parsing task and analysis in
Section 5.
</bodyText>
<equation confidence="0.804699">
A = a1 ⊗ a2⊗, . . . , ⊗aD,
</equation>
<bodyText confidence="0.993457333333333">
where ai ∈ Rnd, 1 ≤ d ≤ D. A Dth order tensor
T ∈ Rn1xn2x...nD can be factorized into a sum of
component rank-1 tensors as
</bodyText>
<sectionHeader confidence="0.984711" genericHeader="method">
2 Tensor Space Representation
</sectionHeader>
<bodyText confidence="0.999885333333333">
Most of the learning algorithms for NLP problems
are based on vector space models, which represent
data as vectors 0 ∈ Rn, and try to learn feature
weight vectors w ∈ Rn such that a linear model
y = w · 0 is able to discriminate between, say,
good and bad hypotheses. While this is a natural
way of representing data, it is not the only choice.
Below, we reformulate the model from vector to
tensor space.
</bodyText>
<subsectionHeader confidence="0.713555">
2.1 Tensor Space Model
</subsectionHeader>
<bodyText confidence="0.999790894736842">
A tensor is a multidimensional array, and is a gen-
eralization of commonly used algebraic objects
such as vectors and matrices. Specifically, a vec-
tor is a 1st order tensor, a matrix is a 2nd order
tensor, and data organized as a rectangular cuboid
is a 3rd order tensor etc. In general, a Dth order
tensor is represented as T ∈ Rn1xn2x...nD, and an
entry in T is denoted by Ti1,i2,...,iD. Different di-
mensions of a tensor 1, 2, ... , D are named modes
of the tensor.
Using a Dth order tensor as container, we can
assign each feature of the task a D-dimensional
index in the tensor and represent the data as ten-
sors. Of course, shifting from a vector to a tensor
representation entails several additional degrees of
freedom, e.g., the order D of the tensor and the
sizes {nd}Dd=1 of the modes, which must be ad-
dressed when selecting a tensor model. This will
be done in Section 3.
</bodyText>
<subsectionHeader confidence="0.996734">
2.2 Tensor Decomposition
</subsectionHeader>
<bodyText confidence="0.999046">
Just as a matrix can be decomposed as a lin-
ear combination of several rank-1 matrices via
SVD, tensors also admit decompositions1 into lin-
ear combinations of “rank-1” tensors. A Dth or-
der tensor A ∈ Rn1xn2x...nD is rank-1 if it can be
</bodyText>
<footnote confidence="0.980884333333333">
1The form of tensor decomposition defined here is named
as CANDECOMP/PARAFAC(CP) decomposition (Kolda
and Bader, 2009). Another popular form of tensor decom-
position is called Tucker decomposition, which decomposes
a tensor into a core tensor multiplied by a matrix along each
mode. We focus only on the CP decomposition in this paper.
</footnote>
<equation confidence="0.998512">
R R
T = Ar = a1r ⊗ a2r⊗,...,⊗aDr
r=1 r=1
</equation>
<bodyText confidence="0.971802">
where R, called the rank of the tensor, is the mini-
mum number of rank-1 tensors whose sum equals
T . Via decomposition, one may approximate a
tensor by the sum of H major rank-1 tensors with
H ≤ R.
</bodyText>
<subsectionHeader confidence="0.996878">
2.3 Linear Tensor Model
</subsectionHeader>
<bodyText confidence="0.999153">
In tensor space, a linear model may be written (ig-
noring a bias term) as
</bodyText>
<equation confidence="0.991106">
f(W) = W ◦ Φ,
</equation>
<bodyText confidence="0.923982833333333">
where Φ ∈ Rn1xn2x...nD is the feature tensor, W
is the corresponding weight tensor, and ◦ denotes
the Hadamard product. If W is further decom-
posed as the sum of H major component rank-1
tensors, i.e. W ≈ EH h=1 w1h ⊗ w2h⊗, ... , ⊗wDh ,
then
</bodyText>
<equation confidence="0.9937126">
1 D 1 D
w1,...,w1 ,...,wh,...,wh )
H
Φ ×1 w1 h ×2 w2 h ... ×D wD h , (1)
h=1
</equation>
<bodyText confidence="0.99756">
where ×l is the l-mode product operator between
a Dth order tensor T and a vector a of dimension
nd, yielding a (D − 1)th order tensor such that
</bodyText>
<equation confidence="0.9982795">
(T ×l a)i1,...,il−1,il+1,...,iD
nd
� Ti1,...,il−1,il,il+1,...,iD · ail.
il=1
</equation>
<bodyText confidence="0.974583">
The linear tensor model is illustrated in Figure 1.
</bodyText>
<subsectionHeader confidence="0.998707">
2.4 Why Learning in Tensor Space?
</subsectionHeader>
<bodyText confidence="0.93344975">
So what is the advantage of learning with a ten-
sor model instead of a vector model? Consider the
case where we have defined 1,000,000 features for
our task. A vector space linear model requires es-
timating 1,000,000 free parameters. However if
we use a 2nd order tensor model, organize the fea-
tures into a 1000 × 1000 matrix Φ, and use just
f(
</bodyText>
<page confidence="0.955839">
667
</page>
<figureCaption confidence="0.998142">
Figure 1: A 3rd order linear tensor model. The
</figureCaption>
<bodyText confidence="0.9684256">
feature weight tensor W can be decomposed as
the sum of a sequence of rank-1 component ten-
sors.
one rank-1 matrix to approximate the weight ten-
sor, then the linear model becomes
</bodyText>
<equation confidence="0.99772">
T
f (w1, w2) = w1 Φw2,
</equation>
<bodyText confidence="0.999964272727273">
where w1, w2 E R1000. That is to say, now we
only need to estimate 2000 parameters!
In general, if V features are defined for a learn-
ing problem, and we (i) organize the feature set
as a tensor Φ E Rn1×n2×...nD and (ii) use H
component rank-1 tensors to approximate the cor-
responding target weight tensor. Then the total
number of parameters to be learned for this ten-
sor model is H EDd=1 nd, which is usually much
smaller than V = HDd=1 nd for a traditional vec-
tor space model. Therefore we expect the tensor
model to be more effective in a low-resource train-
ing environment.
Specifically, a vector space model assumes each
feature weight to be a “free” parameter, and es-
timating them reliably could therefore be hard
when training data are not sufficient or the fea-
ture set is huge. By contrast, a linear tensor model
only needs to learn H ED d=1 nd “bases” of the m
feature weights instead of individual weights di-
rectly. The weight corresponding to the feature
Φi1,i2,...,iD in the tensor model is expressed as
</bodyText>
<equation confidence="0.996398666666667">
H
wi1,i2,...,iD = w1 h,i1w2 h,i2 ... wD h,iD, (2)
h=1
</equation>
<bodyText confidence="0.9999455625">
where wjh i is the itjh element in the vector wjh.
In other words, a true feature weight is now ap-
proximated by a set of bases. This reminds us
of the well-known low-rank matrix approximation
of images via SVD, and we are applying similar
techniques to approximate target feature weights,
which is made possible only after we shift from
vector to tensor space models.
This approximation can be treated as a form of
model regularization, since the weight tensor is
represented in a constrained form and made highly
structured via the rank-1 tensor approximation. Of
course, as we reduce the model complexity, e.g. by
choosing a smaller and smaller H, the model’s ex-
pressive ability is weakened at the same time. We
will elaborate on this point in Section 3.1.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="method">
3 Tensor Model Construction
</sectionHeader>
<bodyText confidence="0.999944875">
To apply a tensor model, we first need to con-
vert the feature vector into a tensor Φ. Once the
structure of Φ is determined, the structure of W
is fixed as well. As mentioned in Section 2.1, a
tensor model has many more degrees of “design
freedom” than a vector model, which makes the
problem of finding a good tensor structure a non-
trivial one.
</bodyText>
<subsectionHeader confidence="0.999222">
3.1 Tensor Order
</subsectionHeader>
<bodyText confidence="0.999968222222222">
The order of a tensor affects the model in two
ways: the expressiveness of the model and the
number of parameters to be estimated. We assume
H = 1 in the analysis below, noting that one can
always add as many rank-1 component tensors as
needed to approximate a tensor with arbitrary pre-
cision.
Obviously, the 1st order tensor (vector) model
is the most expressive, since it is structureless and
any arbitrary set of numbers can always be repre-
sented exactly as a vector. The 2nd order rank-1
tensor (rank-1 matrix) is less expressive because
not every set of numbers can be organized into
a rank-1 matrix. In general, a Dth order rank-1
tensor is more expressive than a (D + 1)th order
rank-1 tensor, as a lower-order tensor imposes less
structural constraints on the set of numbers it can
express. We formally state this fact as follows:
</bodyText>
<construct confidence="0.956622">
Theorem 1. A set of real numbers that can be rep-
resented by a (D + 1)th order tensor Q can also
be represented by a Dth order tensor P, provided
P and Q have the same volume. But the reverse is
not true.
</construct>
<bodyText confidence="0.904723285714286">
Proof. See appendix.
On the other hand, tensor order also affects the
number of parameters to be trained. Assuming
that a Dth order has equal size on each mode (we
will elaborate on this point in Section 3.2) and
the volume (number of entries) of the tensor is
fixed as V , then the total number of parameters
</bodyText>
<page confidence="0.992162">
668
</page>
<bodyText confidence="0.9941994">
of the model is DV1D . This is a convex func-
tion of D, and the minimum2 is reached at either
D* = LlnV] or D* = FlnV].
Therefore, as D increases from 1 to D*, we
lose more and more of the expressive power of the
model but reduce the number of parameters to be
trained. However it would be a bad idea to choose
a D beyond D*. The optimal tensor order depends
on the nature of the actual problem, and we tune
this hyper-parameter on a held-out set.
</bodyText>
<subsectionHeader confidence="0.999922">
3.2 Mode Size
</subsectionHeader>
<bodyText confidence="0.99996504">
The size nd of each tensor mode, d = 1, ... , D,
determines the structure of feature weights a ten-
sor model can precisely represent, as well as the
number of parameters to estimate (we also as-
sume H = 1 in the analysis below). For exam-
ple, if the tensor order is 2 and the volume V is
12, then we can either choose n1 = 3, n2 = 4
or n1 = 2, n2 = 6. For n1 = 3, n2 = 4, the
numbers that can be precisely represented are di-
vided into 3 groups, each having 4 numbers, that
are scaled versions of one another. Similarly for
n1 = 2, n2 = 6, the numbers can be divided into
2 groups with different scales. Obviously, the two
possible choices of (n1, n2) also lead to different
numbers of free parameters (7 vs. 8).
Given D and V , there are many possible combi-
nations of nd, d = 1, ... , D, and the optimal com-
bination should indeed be determined by the struc-
ture of target features weights. However it is hard
to know the structure of target feature weights be-
fore learning, and it would be impractical to try ev-
ery possible combination of mode sizes, therefore
we choose the criterion of determining the mode
sizes as minimization of the total number of pa-
rameters, namely we solve the problem:
</bodyText>
<equation confidence="0.854705555555556">
min
n1,...,nD
The optimal solution is reached when n1 = n2 =
. . . = nD = V D1 . Of course it is not guaran-
teed that V 1D is an integer, therefore we choose
nd = LV D] or FV D ] , d = 1, ... , D such that
1 1
r1Dd= 1 nd &gt; V and [�d 1 nd] − V is minimized.
��D ]
</equation>
<bodyText confidence="0.94902725">
The d=1 nd − V extra entries of the tensor
correspond to no features and are used just for
2The optimal integer solution can be determined simply
by comparing the two function values.
padding. Since for each nd there are only two
possible values to choose, we can simply enumer-
ate all the possible 2D (which is usually a small
number) combinations of values and pick the one
that matches the conditions given above. This way
n1, ... , nD are fully determined.
Here we are only following the principle of min-
imizing the parameter number. While this strat-
egy might work well with small amount of train-
ing data, it is not guaranteed to be the best strategy
in all cases, especially when more data is avail-
able we might want to increase the number of pa-
rameters, making the model more complex so that
the data can be more precisely modeled. Ideally
the mode size needs to be adaptive to the amount
of training data as well as the property of target
weights. A theoretically guaranteed optimal ap-
proach to determining the mode sizes remains an
open problem, and will be explored in our future
work.
</bodyText>
<subsectionHeader confidence="0.998092">
3.3 Number of Rank-1 Tensors
</subsectionHeader>
<bodyText confidence="0.999965444444444">
The impact of using H &gt; 1 rank-1 tensors is ob-
vious: a larger H increases the model complexity
and makes the model more expressive, since we
are able to approximate target weight tensor with
smaller error. As a trade-off, the number of param-
eters and training complexity will be increased. To
find out the optimal value of H for a given prob-
lem, we tune this hyper-parameter too on a held-
out set.
</bodyText>
<subsectionHeader confidence="0.945362">
3.4 Vector to Tensor Mapping
</subsectionHeader>
<bodyText confidence="0.999992263157895">
Finally, we need to find a way to map the orig-
inal feature vector to a tensor, i.e. to associate
each feature with an index in the tensor. Assum-
ing the tensor volume V is the same as the number
of features, then there are in all V ! ways of map-
ping, which is an intractable number of possibili-
ties even for modest sized feature sets, making it
impractical to carry out a brute force search. How-
ever while we are doing the mapping, we hope to
arrange the features in a way such that the corre-
sponding target weight tensor has approximately a
low-rank structure, this way it can be well approx-
imated by very few component rank-1 tensors.
Unfortunately we have no knowledge about the
target weights in advance, since that is what we
need to learn after all. As a way out, we first run
a simple vector-model based learning algorithm
(say the Perceptron) on the training data and es-
timate a weight vector, which serves as a “surro-
</bodyText>
<equation confidence="0.807652333333333">
D D
nd s.t H nd=V
d=1 d=1
</equation>
<page confidence="0.985272">
669
</page>
<bodyText confidence="0.999911857142857">
gate” weight vector. We then use this surrogate
vector to guide the design of the mapping. Ide-
ally we hope to find a permutation of the surro-
gate weights to map to a tensor in such a way that
the tensor has a rank as low as possible. How-
ever matrix rank minimization is in general a hard
problem (Fazel, 2002). Therefore, we follow an
approximate algorithm given in Figure 2a, whose
main idea is illustrated via an example in Figure
2b.
Basically, what the algorithm does is to di-
vide the surrogate weights into hierarchical groups
such that groups on the same level are approx-
imately proportional to each other. Using these
groups as units we are able to “fill” the tensor in a
hierarchical way. The resulting tensor will have an
approximate low-rank structure, provided that the
sorted feature weights have roughly group-wise
proportional relations.
For comparison, we also experimented a trivial
solution which maps each entry of the feature ten-
sor to the tensor just in sequential order, namely
00 is mapped to Φ0,0,...,0, 01 is mapped to Φ0,0,...,1
etc. This of course ignores correlation between
features since the original feature order in the vec-
tor could be totally meaningless, and this strategy
is not expected to be a good solution for vector to
tensor mapping.
</bodyText>
<sectionHeader confidence="0.997943" genericHeader="method">
4 Online Learning Algorithm
</sectionHeader>
<bodyText confidence="0.986733285714286">
We now turn to the problem of learning the feature
weight tensor. Here we propose an online learning
algorithm similar to MIRA but modified to accom-
modate tensor models.
Let the model be f (T) = T o Φ(x, y), where
T = Eh 1 w1h ® w2h®, ... , ®wDh is the weight
tensor, Φ(x, y) is the feature tensor for an input-
output pair (x, y). Training samples (xi, yi), i =
1, ... , m, where xi is the input and yi is the ref-
erence or oracle hypothesis, are fed to the weight
learning algorithm in sequential order. A predic-
tion zt is made by the model Tt at time t from a
set of candidates i(xt), and the model updates the
weight tensor by solving the following problem:
</bodyText>
<equation confidence="0.914911346153846">
2IIT − TtII2 + Cξ (3)
1
s.t.
Lt &lt;- ξ,ξ &gt;&gt; 0
where T is a decomposed weight tensor and
Yt = T o Φ(xt, zt) − T o Φ(xt, yt) + P(yt, zt)
Input:
Tensor order D, tensor volume V , mode size
nd, d = 1, ... , D, surrogate weight vector v
Let
v+ = [v+1 ,..., v+p ] be the non-negative part of
v
v− = [v−1 , ... , v−q ] be the negative part of v
Algorithm:
˜v+ = sort(v+) in descending order
˜v− = sort(v−) in ascending order
u = V/nD
e = p − mod(p, u), f = q − mod(q, u)
Construct vector
X= [˜v+1 ,..., ˜v+ e , ˜v− 1 ,..., ˜v− f ,
˜v+ ˜v+ p , ˜v− f+1, . . . , ˜v
Map Xa, a = 1, ... , p + q to the tensor entry
71,...,iD, such that
D
a = (id − 1)ld−1 + 1
d=1
</equation>
<bodyText confidence="0.818054">
where ld = ld−1nd, and l0 = 1
</bodyText>
<figure confidence="0.995609">
(a) Mapping a surrogate weight vector to a tensor
(b) Illustration of the algorithm
</figure>
<figureCaption confidence="0.674280666666667">
Figure 2: Algorithm for mapping a surrogate
weight vector X to a tensor. (2a) provides the al-
gorithm; (2b) illustrates it by mapping a vector of
</figureCaption>
<bodyText confidence="0.999305333333333">
length V = 12 to a (n1, n2, n3) = (2, 2, 3) ten-
sor. The bars Xi represent the surrogate weights
— after separately sorting the positive and nega-
tive parts — and the labels along a path of the tree
correspond to the tensor-index of the weight rep-
resented by the leaf resulting from the mapping.
</bodyText>
<equation confidence="0.9145213">
min
T ∈Rn1×n2×...nD
e+1, . . .
]
−
q
670
= min
wd∈Rnd
1 2k
</equation>
<bodyText confidence="0.986654045454545">
is the structured hinge loss.
This problem setting follows the same “passive-
aggressive” strategy as in the original MIRA. To
optimize the vectors wdh, h = 1, ... , H, d =
1, ... , D, we use a similar iterative strategy as pro-
posed in (Cai et al., 2006b). Basically, the idea is
that instead of optimizing wdh all together, we op-
timize w11, w21, . . . , wDH in turn. While we are up-
dating one vector, the rest are fixed. For the prob-
lem setting given above, each of the sub-problems
that need to be solved is convex, and according
to (Cai et al., 2006b) the objective function value
will decrease after each individual weight update
and eventually this procedure will converge.
We now give this procedure in more detail.
Denote the weight vector of the dth mode of
the hth tensor at time t as wdh,t. We will up-
date the vectors in turn in the following order:
w1 1,t,...,wD 1,t,w1 2,t,...,wD 2,t,...,w1 H,t, ... ,wD H,t.
Once a vector has been updated, it is fixed for
future updates.
By way of notation, define
</bodyText>
<equation confidence="0.998625238095238">
W dh,t
= w1 h,t+1⊗,..., ⊗wd−1
h,t+1 ⊗ wdh,t⊗, ... , ⊗wD h,t
(and let W D+1
h,t °= w1 h,t+1⊗,... , ⊗wDh,t+1),
cd
W h,t
= w1 h,t+1⊗,..., ⊗wd−1
h,t+1 ⊗ wd⊗, ... , ⊗wDh,t
(where wd ∈ Rnd),
WD+1
h0,t + Wd h,t +
h0=h+1
cWdh,t − Wdh,tk2 + Cξ
2β1
1h,t+1 ... βd−1 h,t+1βd+1
h,t ... βDh,t
kwd − wdh,tk2 + Cξ
s.t.
yd
h,t ≤ ξ, ξ ≥ 0.
</equation>
<bodyText confidence="0.676252">
where
</bodyText>
<equation confidence="0.9852304375">
βdh,t = kwdh,tk2
Ldh,t = Tbdh,t ◦ Φ(xt, zt) − Tbdh,t ◦ Φ(xt, yt)
+P(yt, zt) � �
= wd · φd h,t(xt, zt) − φd h,t(xt, yt)
!
W1 ◦
h0,t
Letting
Δφdh,t φdh,t(xt, yt) − φdh,t(xt, zt)
and
!
W1 ◦
h0,t
(Φ(xt, yt) − Φ(xt, zt))
we may compactly write
Ldh,t= P(yt, zt) − sdh,t − wd · Δφd h,t.
</equation>
<bodyText confidence="0.999926">
This convex optimization problem is just like the
original MIRA and may be solved in a similar way.
The updating strategy for wdh,t is derived as
</bodyText>
<equation confidence="0.987654586206897">
h−1X
h0=1
d
Th,t =
XH W1h0,t(4)
= min
wd∈Rnd
− h−1X WD+1+ XH
h0=1 h0,t h0=h+1
(Φ(xt, yt) − Φ(xt, zt))
+P(yt, zt)
do h−1X WD+1+ XH
sh,t = h0=1 h0,t h0=h+1
d
Th,t =
h−1X
h0=1
XH
1
W h0,t
WD+1
h0,t + Wc d h,t +
h0=h+1
φdh,t(x, y)
= Φ(x, y) ×2 w2h,t+1 ... ×d−1 wd−1
h,t+1 ×d+1
wd+1
h,t ... ×D wD (5)
h,t
</equation>
<bodyText confidence="0.7642845">
In order to update from wdh,t to get wdh,t+1, the
sub-problem to solve is:
min 1 2k bTd h,t − Td h,tk2 + Cξ
wd∈Rnd
</bodyText>
<equation confidence="0.9994035">
wdh,t+1 = wdh,t + TΔφdh,t
T = (6)
min()
C,
P(yt, zt) − Th t ◦ (Φ(xt, yt) − Φ(xt, zt))
kΔφdh,tk2
</equation>
<bodyText confidence="0.999723333333333">
The initial vectors wih,1 cannot be made all zero,
since otherwise the l-mode product in Equation
(5) would yield all zero φd h,t(x, y) and the model
would never get a chance to be updated. There-
fore, we initialize the entries of wih,1 uniformly
such that the Frobenius-norm of the weight tensor
W is unity.
We call the algorithm above “Tensor-MIRA”
and abbreviate it as T-MIRA.
</bodyText>
<page confidence="0.999075">
671
</page>
<sectionHeader confidence="0.998752" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999509571428571">
In this section we shows empirical results of the
training algorithm on a parsing task. We used the
Charniak parser (Charniak et al., 2005) for our ex-
periment, and we used the proposed algorithm to
train the reranking feature weights. For compari-
son, we also investigated training the reranker with
Perceptron and MIRA.
</bodyText>
<subsectionHeader confidence="0.994393">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9984628125">
To simulate a low-resource training environment,
our training sets were selected from sections 2-9
of the Penn WSJ treebank, section 24 was used as
the held-out set and section 23 as the evaluation
set. We applied the default settings of the parser.
There are around V = 1.33 million features in
all defined for reranking, and the n-best size for
reranking is set to 50. We selected the parse with
the highest f-score from the 50-best list as the or-
acle.
We would like to observe from the experiments
how the amount of training data as well as dif-
ferent settings of the tensor degrees of freedom
affects the algorithm performance. Therefore we
tried all combinations of the following experimen-
tal parameters:
</bodyText>
<table confidence="0.837727">
Parameters Settings
Training data (m) Sec. 2, 2-3, 2-5, 2-9
Tensor order (D) 2, 3, 4
# rank-1 tensors (H) 1, 2, 3
Vec. to tensor mapping approximate, sequential
</table>
<bodyText confidence="0.998439875">
Here “approximate” and “sequential” means us-
ing, respectively, the algorithm given in Figure 2
and the sequential mapping mentioned in Section
3.4. According to the strategy given in 3.2, once
the tensor order and number of features are fixed,
the sizes of modes and total number of parameters
to estimate are fixed as well, as shown in the tables
below:
</bodyText>
<table confidence="0.980522">
D Size of modes Number of parameters
2 1155 x 1155 2310
3 110 x 110 x 111 331
4 34 x 34 x 34 x 34 136
</table>
<subsectionHeader confidence="0.940825">
5.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.939560157894737">
The f-scores of the held-out and evaluation set
given by T-MIRA as well as the Perceptron and
MIRA baseline are given in Table 1. From the re-
sults, we have the following observations:
1. When very few labeled data are available for
training (compared with the number of fea-
tures), T-MIRA performs much better than
the vector-based models MIRA and Percep-
tron. However as the amount of training data
increases, the advantage of T-MIRA fades
away, and vector-based models catch up.
This is because the weight tensors learned
by T-MIRA are highly structured, which sig-
nificantly reduces model/training complex-
ity and makes the learning process very ef-
fective in a low-resource environment, but
as the amount of data increases, the more
complex and expressive vector-based models
adapt to the data better, whereas further im-
provements from the tensor model is impeded
by its structural constraints, making it insen-
sitive to the increase of training data.
2. To further contrast the behavior of T-MIRA,
MIRA and Perceptron, we plot the f-scores
on both the training and held-out sets given
by these algorithms after each training epoch
in Figure 3. The plots are for the exper-
imental setting with mapping=surrogate, #
rank-1 tensors=2, tensor order=2, training
data=sections 2-3. It is clearly seen that both
MIRA and Perceptron do much better than T-
MIRA on the training set. Nevertheless, with
a huge number of parameters to fit a limited
amount of data, they tend to over-fit and give
much worse results on the held-out set than
T-MIRA does.
As an aside, observe that MIRA consistently
outperformed Perceptron, as expected.
</bodyText>
<listItem confidence="0.9819332">
3. Properties of linear tensor model: The heuris-
tic vector-to-tensor mapping strategy given
by Figure 2 gives consistently better results
than the sequential mapping strategy, as ex-
pected.
</listItem>
<bodyText confidence="0.999885444444444">
To make further comparison of the two strate-
gies, in Figure 4 we plot the 20 largest sin-
gular values of the matrices which the surro-
gate weights (given by the Perceptron after
running for 1 epoch) are mapped to by both
strategies (from the experiment with training
data sections 2-5). From the contrast between
the largest and the 2nd-largest singular val-
ues, it can be seen that the matrix generated
</bodyText>
<page confidence="0.996911">
672
</page>
<bodyText confidence="0.999647714285714">
by the first strategy approximates a low-rank
structure much better than the second strat-
egy. Therefore, the performance of T-MIRA
is influenced significantly by the way features
are mapped to the tensor. If the correspond-
ing target weight tensor has internal struc-
ture that makes it approximately low-rank,
the learning procedure becomes more effec-
tive.
The best results are consistently given by 2nd
order tensor models, and the differences be-
tween the 3rd and 4th order tensors are not
significant. As discussed in Section 3.1, al-
though 3rd and 4th order tensors have less pa-
rameters, the benefit of reduced training com-
plexity does not compensate for the loss of
expressiveness. A 2nd order tensor has al-
ready reduced the number of parameters from
the original 1.33 million to only 2310, and it
does not help to further reduce the number of
parameters using higher order tensors.
4. As the amount of training data increases,
there is a trend that the best results come from
models with more rank-1 component tensors.
Adding more rank-1 tensors increases the
model’s complexity and ability of expression,
making the model more adaptive to larger
data sets.
</bodyText>
<sectionHeader confidence="0.996124" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999613">
In this paper, we reformulated the traditional lin-
ear vector-space models as tensor-space models,
and proposed an online learning algorithm named
Tensor-MIRA. A tensor-space model is a com-
pact representation of data, and via rank-1 ten-
sor approximation, the weight tensor can be made
highly structured hence the number of parame-
ters to be trained is significantly reduced. This
can be regarded as a form of model regular-
ization.Therefore, compared with the traditional
vector-space models, learning in the tensor space
is very effective when a large feature set is defined,
but only small amount of training data is available.
Our experimental results corroborated this argu-
ment.
As mentioned in Section 3.2, one interesting
problem that merits further investigation is how
to determine optimal mode sizes. The challenge
of applying a tensor model comes from finding a
proper tensor structure for a given problem, and
</bodyText>
<figure confidence="0.999505090909091">
(a) Training set
T
90
89.5
89
88.5
88
87.5
87
1 2 3
(b) Held-out set
</figure>
<figureCaption confidence="0.977773">
Figure 3: f-scores given by three algorithms on
training and held-out set (see text for the setting).
</figureCaption>
<bodyText confidence="0.998097375">
the key to solving this problem is to find a bal-
ance between the model complexity (indicated by
the order and sizes of modes) and the number of
parameters. Developing a theoretically guaran-
teed approach of finding the optimal structure for
a given task will make the tensor model not only
perform well in low-resource environments, but
adaptive to larger data sets.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99981525">
This work was partially supported by IBM via
DARPA/BOLT contract number HR0011-12-C-
0015 and by the National Science Foundation via
award number IIS-0963898.
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.979481222222222">
Deng Cai , Xiaofei He , and Jiawei Han. 2006. Tensor
Space Model for Document Analysis Proceedings
of the 29th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval(SIGIR), 625–626.
Deng Cai, Xiaofei He, and Jiawei Han. 2006. Learn-
ing with Tensor Representation Technical Report,
Department of Computer Science, University of Illi-
nois at Urbana-Champaign.
</reference>
<page confidence="0.998016">
673
</page>
<table confidence="0.995712121212121">
Mapping Approximate Sequential
Rank-1 tensors 1 2 3 1 2 3
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.43 89.16 89.22 89.16 89.21 89.24 89.27 89.14 89.24 89.21 88.90 88.89 89.13 88.88 88.88 89.15 88.87 88.99
Evaluation score 89.83 89.69
MIRA 88.57
Percep 88.23
(a) Training data: Section 2 only
Mapping Approximate Sequential
Rank-1 tensors 1 2 3 1 2 3
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.26 89.06 89.12 89.33 89.11 89.19 89.18 89.14 89.15 89.2 89.01 88.82 89.24 88.94 88.95 89.19 88.91 88.98
Evaluation score 90.02 89.82
MIRA 89.00
Percep 88.59
(b) Training data: Section 2-3
Mapping Approximate Sequential
Rank-1 tensors 1 2 3 1 2 3
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.40 89.44 89.17 89.5 89.37 89.18 89.47 89.32 89.18 89.23 89.03 88.93 89.24 88.98 88.94 89.16 89.01 88.85
Evaluation score 89.96 89.78
MIRA 89.49
Percep 89.10
(c) Training data: Section 2-5
Mapping Approximate Sequential
Rank-1 tensors 2 3 4 2 3 4
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.43 89.23 89.06 89.37 89.23 89.1 89.44 89.22 89.06 89.21 88.92 88.94 89.23 88.94 88.93 89.23 88.95 88.93
Evaluation score 89.95 89.84
MIRA 89.95
Percep 89.77
(d) Training data: Section 2-9
Sgur vue
</table>
<tableCaption confidence="0.996313">
Table 1: Parsing f-scores. Tables (a) to (d) correspond to training data with increasing size. The upper-part of
</tableCaption>
<bodyText confidence="0.83977425">
each table shows the T-MIRA results with different settings, the lower-part shows the MIRA and Perceptron
baselines. The evaluation scores come from the settings indicated by the best held-out scores. The best results
on the held-out and evaluation data are marked in bold.
2 4 6 8 10 12 14 16 18 20
</bodyText>
<figureCaption confidence="0.920236333333333">
Figure 4: The top 20 singular values of the surro-
gate weight matrices given by two mapping algo-
rithms.
</figureCaption>
<reference confidence="0.993821606060606">
Eugene Charniak, and Mark Johnson 2005. Coarse-
to-fine n-Best Parsing and MaxEnt Discriminative
Reranking Proceedings of the 43th Annual Meeting
on Association for Computational Linguistics(ACL)
173–180.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online Large-Margin Training of Syntactic
and Structural Translation Features Proceedings of
Empirical Methods in Natural Language Process-
ing(EMNLP), 224–233.
Shay Cohen and Michael Collins. 2012. Tensor De-
composition for Fast Parsing with Latent-Variable
PCFGs Proceedings of Advances in Neural Infor-
mation Processing Systems(NIPS).
Shay Cohen and Giorgio Satta. 2013. Approximate
PCFG Parsing Using Tensor Decomposition Pro-
ceedings of NAACL-HLT, 487–496.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov Models: Theory and Exper-
iments with Perceptron. Algorithms Proceedings of
Empirical Methods in Natural Language Process-
ing(EMNLP), 10:1–8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithms Journal of Machine
Learning Research(JMLR), 7:551–585.
Maryam Fazel. 2002. Matrix Rank Minimization with
Applications PhD thesis, Stanford University.
Kevin Gimpel, and Noah A. Smith 2012. Structured
Ramp Loss Minimization for Machine Translation
Proceedings of North American Chapter of the As-
sociation for Computational Linguistics(NAACL),
221-231.
</reference>
<figure confidence="0.980264625">
500
400
300
200
100
0
Approximate
Sequential
</figure>
<page confidence="0.989275">
674
</page>
<reference confidence="0.998832541666666">
Tamir Hazan, Simon Polak, and Amnon Shashua 2005.
Sparse Image Coding using a 3D Non-negative Ten-
sor Factorization Proceedings of the International
Conference on Computer Vision (ICCV).
Mark Hopkins and Jonathan May. 2011. Tuning
as Reranking Proceedings of Empirical Methods
in Natural Language Processing(EMNLP), 1352-
1362.
Tamara Kolda and Brett Bader. 2009. Tensor Decom-
positions and Applications SIAM Review, 51:455-
550.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online Large-Margin Training of
Dependency Parsers Proceedings of the 43rd An-
nual Meeting of the ACL, 91–98.
Amnon Shashua, and Tamir Hazan. 2005. Non-
Negative Tensor Factorization with Applications to
Statistics and Computer Vision Proceedings of
the International Conference on Machine Learning
(ICML).
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A Tensor-based Factorization Model of
Semantic Compositionality Proceedings of NAACL-
HLT, 1142–1151.
</reference>
<subsectionHeader confidence="0.484542">
A Proof of Theorem 1
</subsectionHeader>
<bodyText confidence="0.975474">
Proof. For D = 1, it is obvious that if a set of
real numbers {x1, ... , xn} can be represented by
a rank-1 matrix, it can always be represented by a
vector, but the reverse is not true.
</bodyText>
<equation confidence="0.8035768">
For D &gt; 1, if {x1, ... , xn} can be repre-
sented by P = p1 ® p2 ® . . . ® pD, namely
xi = Pi1,...,iD = 7�7 D d then for an compo-
d= o
HD
</equation>
<bodyText confidence="0.9241844">
1d=1 pid+ Y P -
nent vector in mode d,
[pd 1,pd 2, ... ,pd nd] = [sd 1pd 1, sd 2pd 1, ... , sd np dpd 1]
where npd is the size of mode d of P, sdj is a con-
stant and sd = pi1 ,...,i d−1,j,id+1,...,iD Therefore
jpi1 ,...,i d−1,1,id+1 ,...,i D
similar definition as sdj. Then it must be the case
that
then we can just pick d1 E {1, ... , D}, d2 = d1 +
1 and let
</bodyText>
<equation confidence="0.995714333333333">
i d1 d2 d1 d2 d d2
q = [q1 q1 ,q1 q2 ,...,qn I qnq ]
d2 d1
</equation>
<bodyText confidence="0.605591">
and
</bodyText>
<equation confidence="0.970287">
2I = q1 ®...®qd1−1 ®q&apos; ®qd2+1 ®...®qD+1
</equation>
<bodyText confidence="0.9959215">
Hence {x1, ... , xn} can also be represented by a
Dth order tensor 2&apos;.
</bodyText>
<equation confidence="0.67589">
∃d1,d2 E {1,...,D + 1},d E {1,...,D}, d1 =�d2
s.t.
td1
id1 td2
id2 = sd id, (8)
</equation>
<bodyText confidence="0.72844025">
tda
ida = sdb idb, da =� d1, d2, db =� d
since otherwise {x1, ... , xn} would be repre-
sented by a different set of factors than those given
in Equation (7).
Therefore, in order for tensor 2 to represent
the same set of real numbers that P represents,
there needs to exist a vector [sd1, ... , sd ] that can
nd
be represented by a rank-1 matrix as indicated by
Equation (8), which is in general not guaranteed.
On the other hand, if {x1, ... , xn} can be rep-
</bodyText>
<equation confidence="0.883063875">
resented by 2, namely
xi = 2i1,...,iD+1 =
qdid
D+1H
d=1
xi = Pi1,...,iD = x1,...,1 D sd (7)
H id
d=1
</equation>
<bodyText confidence="0.971255">
and this representation is unique for a given D(up
to the ordering of pj and sd j in pj, which simply
assigns {x1, ... , xn} with different indices in the
tensor), due to the pairwise proportional constraint
imposed by xi/xj, i, j = 1, ... , n.
If xi can also be represented by 2, then xi =
2i1,...,iD+1 = x1,...,1 �D+1
</bodyText>
<footnote confidence="0.507609">
d=1 tdid, where tdj has a
</footnote>
<page confidence="0.996735">
675
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.869581">
<title confidence="0.999798">Online Learning in Tensor Space</title>
<author confidence="0.997262">Yuan Cao Sanjeev Khudanpur</author>
<affiliation confidence="0.9532445">Center for Language &amp; Speech Processing and Human Language Technology Center of The Johns Hopkins</affiliation>
<address confidence="0.98807">Baltimore, MD, USA,</address>
<abstract confidence="0.99859519047619">We propose an online learning algorithm based on tensor-space models. A tensorspace model represents data in a compact way, and via rank-1 approximation the weight tensor can be made highly structured, resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models. This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training. We apply with the proposed algorithm to a parsing task, and show that even with very little training data the learning algorithm based on a tensor model performs well, and gives significantly better results than standard learning algorithms based on traditional vectorspace models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xiaofei He</author>
</authors>
<title>Tensor Space Model for Document Analysis</title>
<date>2006</date>
<booktitle>Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval(SIGIR),</booktitle>
<pages>625--626</pages>
<marker>He, 2006</marker>
<rawString>Deng Cai , Xiaofei He , and Jiawei Han. 2006. Tensor Space Model for Document Analysis Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval(SIGIR), 625–626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deng Cai</author>
<author>Xiaofei He</author>
<author>Jiawei Han</author>
</authors>
<title>Learning with Tensor Representation</title>
<date>2006</date>
<tech>Technical Report,</tech>
<institution>Department of Computer Science, University of Illinois at Urbana-Champaign.</institution>
<contexts>
<context position="2501" citStr="Cai et al., 2006" startWordPosition="406" endWordPosition="409">ls. Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representations have been applied to computer vision problems (Hazan et al., 2005; Shashua and Hazan, 2005) and information retrieval (Cai et al., 2006a) a long time ago. More recently, it has also been applied to parsing (Cohen and Collins, 2012; Cohen and Satta, 2013) and semantic analysis (Van de Cruys et al., 2013). A linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors. This low-rank approximation imposes structural constraints on the feature weights and can be regarded as a form of regularization. With this representation, we no longer need to estimate individual feature weights directly but only a small number of “bases” i</context>
<context position="19153" citStr="Cai et al., 2006" startWordPosition="3557" endWordPosition="3560">a vector of length V = 12 to a (n1, n2, n3) = (2, 2, 3) tensor. The bars Xi represent the surrogate weights — after separately sorting the positive and negative parts — and the labels along a path of the tree correspond to the tensor-index of the weight represented by the leaf resulting from the mapping. min T ∈Rn1×n2×...nD e+1, . . . ] − q 670 = min wd∈Rnd 1 2k is the structured hinge loss. This problem setting follows the same “passiveaggressive” strategy as in the original MIRA. To optimize the vectors wdh, h = 1, ... , H, d = 1, ... , D, we use a similar iterative strategy as proposed in (Cai et al., 2006b). Basically, the idea is that instead of optimizing wdh all together, we optimize w11, w21, . . . , wDH in turn. While we are updating one vector, the rest are fixed. For the problem setting given above, each of the sub-problems that need to be solved is convex, and according to (Cai et al., 2006b) the objective function value will decrease after each individual weight update and eventually this procedure will converge. We now give this procedure in more detail. Denote the weight vector of the dth mode of the hth tensor at time t as wdh,t. We will update the vectors in turn in the following </context>
</contexts>
<marker>Cai, He, Han, 2006</marker>
<rawString>Deng Cai, Xiaofei He, and Jiawei Han. 2006. Learning with Tensor Representation Technical Report, Department of Computer Science, University of Illinois at Urbana-Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-Best Parsing and MaxEnt Discriminative Reranking</title>
<date>2005</date>
<booktitle>Proceedings of the 43th Annual Meeting on Association for Computational Linguistics(ACL)</booktitle>
<pages>173--180</pages>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak, and Mark Johnson 2005. Coarseto-fine n-Best Parsing and MaxEnt Discriminative Reranking Proceedings of the 43th Annual Meeting on Association for Computational Linguistics(ACL) 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online Large-Margin Training of Syntactic and Structural Translation Features</title>
<date>2008</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing(EMNLP),</booktitle>
<pages>224--233</pages>
<contexts>
<context position="1785" citStr="Chiang et al., 2008" startWordPosition="284" endWordPosition="287">n knowledge of language can be brought to bear on the (prediction) task as possible. This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets: the feature weights cannot be estimated reliably. Most traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space. Many learning algorithms applied to NLP problems, such as the Perceptron (Collins, 2002), MIRA (Crammer et al., 2006; McDonald et al., 2005; Chiang et al., 2008), PRO (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012) etc., are based on vector-space models. Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representations have been applied t</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online Large-Margin Training of Syntactic and Structural Translation Features Proceedings of Empirical Methods in Natural Language Processing(EMNLP), 224–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay Cohen</author>
<author>Michael Collins</author>
</authors>
<title>Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs</title>
<date>2012</date>
<booktitle>Proceedings of Advances in Neural Information Processing Systems(NIPS).</booktitle>
<contexts>
<context position="2596" citStr="Cohen and Collins, 2012" startWordPosition="423" endWordPosition="426"> of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representations have been applied to computer vision problems (Hazan et al., 2005; Shashua and Hazan, 2005) and information retrieval (Cai et al., 2006a) a long time ago. More recently, it has also been applied to parsing (Cohen and Collins, 2012; Cohen and Satta, 2013) and semantic analysis (Van de Cruys et al., 2013). A linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors. This low-rank approximation imposes structural constraints on the feature weights and can be regarded as a form of regularization. With this representation, we no longer need to estimate individual feature weights directly but only a small number of “bases” instead. This property makes the the tensor model very effective when training a large number of</context>
</contexts>
<marker>Cohen, Collins, 2012</marker>
<rawString>Shay Cohen and Michael Collins. 2012. Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs Proceedings of Advances in Neural Information Processing Systems(NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay Cohen</author>
<author>Giorgio Satta</author>
</authors>
<title>Approximate PCFG Parsing Using Tensor Decomposition</title>
<date>2013</date>
<booktitle>Proceedings of NAACL-HLT,</booktitle>
<pages>487--496</pages>
<contexts>
<context position="2620" citStr="Cohen and Satta, 2013" startWordPosition="427" endWordPosition="430">mated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representations have been applied to computer vision problems (Hazan et al., 2005; Shashua and Hazan, 2005) and information retrieval (Cai et al., 2006a) a long time ago. More recently, it has also been applied to parsing (Cohen and Collins, 2012; Cohen and Satta, 2013) and semantic analysis (Van de Cruys et al., 2013). A linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors. This low-rank approximation imposes structural constraints on the feature weights and can be regarded as a form of regularization. With this representation, we no longer need to estimate individual feature weights directly but only a small number of “bases” instead. This property makes the the tensor model very effective when training a large number of feature weights in a lo</context>
</contexts>
<marker>Cohen, Satta, 2013</marker>
<rawString>Shay Cohen and Giorgio Satta. 2013. Approximate PCFG Parsing Using Tensor Decomposition Proceedings of NAACL-HLT, 487–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov Models: Theory and Experiments with Perceptron. Algorithms</title>
<date>2002</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing(EMNLP),</booktitle>
<pages>10--1</pages>
<contexts>
<context position="1712" citStr="Collins, 2002" startWordPosition="273" endWordPosition="274">orporate a large number of linguistic features so that as much human knowledge of language can be brought to bear on the (prediction) task as possible. This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets: the feature weights cannot be estimated reliably. Most traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space. Many learning algorithms applied to NLP problems, such as the Perceptron (Collins, 2002), MIRA (Crammer et al., 2006; McDonald et al., 2005; Chiang et al., 2008), PRO (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012) etc., are based on vector-space models. Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured wa</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov Models: Theory and Experiments with Perceptron. Algorithms Proceedings of Empirical Methods in Natural Language Processing(EMNLP), 10:1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Schwartz</author>
<author>Yoram Singer</author>
</authors>
<date>2006</date>
<journal>Online Passive-Aggressive Algorithms Journal of Machine Learning Research(JMLR),</journal>
<pages>7--551</pages>
<contexts>
<context position="1740" citStr="Crammer et al., 2006" startWordPosition="276" endWordPosition="279">r of linguistic features so that as much human knowledge of language can be brought to bear on the (prediction) task as possible. This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets: the feature weights cannot be estimated reliably. Most traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space. Many learning algorithms applied to NLP problems, such as the Perceptron (Collins, 2002), MIRA (Crammer et al., 2006; McDonald et al., 2005; Chiang et al., 2008), PRO (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012) etc., are based on vector-space models. Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured way using tensors as container</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Schwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Schwartz, and Yoram Singer. 2006. Online Passive-Aggressive Algorithms Journal of Machine Learning Research(JMLR), 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maryam Fazel</author>
</authors>
<title>Matrix Rank Minimization with Applications PhD thesis,</title>
<date>2002</date>
<institution>Stanford University.</institution>
<contexts>
<context position="15989" citStr="Fazel, 2002" startWordPosition="2930" endWordPosition="2931"> have no knowledge about the target weights in advance, since that is what we need to learn after all. As a way out, we first run a simple vector-model based learning algorithm (say the Perceptron) on the training data and estimate a weight vector, which serves as a “surroD D nd s.t H nd=V d=1 d=1 669 gate” weight vector. We then use this surrogate vector to guide the design of the mapping. Ideally we hope to find a permutation of the surrogate weights to map to a tensor in such a way that the tensor has a rank as low as possible. However matrix rank minimization is in general a hard problem (Fazel, 2002). Therefore, we follow an approximate algorithm given in Figure 2a, whose main idea is illustrated via an example in Figure 2b. Basically, what the algorithm does is to divide the surrogate weights into hierarchical groups such that groups on the same level are approximately proportional to each other. Using these groups as units we are able to “fill” the tensor in a hierarchical way. The resulting tensor will have an approximate low-rank structure, provided that the sorted feature weights have roughly group-wise proportional relations. For comparison, we also experimented a trivial solution w</context>
</contexts>
<marker>Fazel, 2002</marker>
<rawString>Maryam Fazel. 2002. Matrix Rank Minimization with Applications PhD thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Structured Ramp Loss Minimization for Machine Translation</title>
<date>2012</date>
<booktitle>Proceedings of North American Chapter of the Association for Computational Linguistics(NAACL),</booktitle>
<pages>221--231</pages>
<contexts>
<context position="1848" citStr="Gimpel and Smith, 2012" startWordPosition="295" endWordPosition="298">ction) task as possible. This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets: the feature weights cannot be estimated reliably. Most traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space. Many learning algorithms applied to NLP problems, such as the Perceptron (Collins, 2002), MIRA (Crammer et al., 2006; McDonald et al., 2005; Chiang et al., 2008), PRO (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012) etc., are based on vector-space models. Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representations have been applied to computer vision problems (Hazan et al., 2005; Shashua and Haz</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel, and Noah A. Smith 2012. Structured Ramp Loss Minimization for Machine Translation Proceedings of North American Chapter of the Association for Computational Linguistics(NAACL), 221-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamir Hazan</author>
<author>Simon Polak</author>
<author>Amnon Shashua</author>
</authors>
<title>Sparse Image Coding using a 3D Non-negative Tensor Factorization</title>
<date>2005</date>
<booktitle>Proceedings of the International Conference on Computer Vision (ICCV).</booktitle>
<contexts>
<context position="2431" citStr="Hazan et al., 2005" startWordPosition="395" endWordPosition="398">), RAMPION (Gimpel and Smith, 2012) etc., are based on vector-space models. Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representations have been applied to computer vision problems (Hazan et al., 2005; Shashua and Hazan, 2005) and information retrieval (Cai et al., 2006a) a long time ago. More recently, it has also been applied to parsing (Cohen and Collins, 2012; Cohen and Satta, 2013) and semantic analysis (Van de Cruys et al., 2013). A linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors. This low-rank approximation imposes structural constraints on the feature weights and can be regarded as a form of regularization. With this representation, we no longer need to estimate in</context>
</contexts>
<marker>Hazan, Polak, Shashua, 2005</marker>
<rawString>Tamir Hazan, Simon Polak, and Amnon Shashua 2005. Sparse Image Coding using a 3D Non-negative Tensor Factorization Proceedings of the International Conference on Computer Vision (ICCV).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as Reranking</title>
<date>2011</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing(EMNLP),</booktitle>
<pages>1352--1362</pages>
<contexts>
<context position="1814" citStr="Hopkins and May, 2011" startWordPosition="289" endWordPosition="293"> be brought to bear on the (prediction) task as possible. This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets: the feature weights cannot be estimated reliably. Most traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space. Many learning algorithms applied to NLP problems, such as the Perceptron (Collins, 2002), MIRA (Crammer et al., 2006; McDonald et al., 2005; Chiang et al., 2008), PRO (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012) etc., are based on vector-space models. Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representations have been applied to computer vision problems (H</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as Reranking Proceedings of Empirical Methods in Natural Language Processing(EMNLP), 1352-1362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Kolda</author>
<author>Brett Bader</author>
</authors>
<date>2009</date>
<booktitle>Tensor Decompositions and Applications SIAM Review,</booktitle>
<pages>51--455</pages>
<contexts>
<context position="5898" citStr="Kolda and Bader, 2009" startWordPosition="1004" endWordPosition="1007">from a vector to a tensor representation entails several additional degrees of freedom, e.g., the order D of the tensor and the sizes {nd}Dd=1 of the modes, which must be addressed when selecting a tensor model. This will be done in Section 3. 2.2 Tensor Decomposition Just as a matrix can be decomposed as a linear combination of several rank-1 matrices via SVD, tensors also admit decompositions1 into linear combinations of “rank-1” tensors. A Dth order tensor A ∈ Rn1xn2x...nD is rank-1 if it can be 1The form of tensor decomposition defined here is named as CANDECOMP/PARAFAC(CP) decomposition (Kolda and Bader, 2009). Another popular form of tensor decomposition is called Tucker decomposition, which decomposes a tensor into a core tensor multiplied by a matrix along each mode. We focus only on the CP decomposition in this paper. R R T = Ar = a1r ⊗ a2r⊗,...,⊗aDr r=1 r=1 where R, called the rank of the tensor, is the minimum number of rank-1 tensors whose sum equals T . Via decomposition, one may approximate a tensor by the sum of H major rank-1 tensors with H ≤ R. 2.3 Linear Tensor Model In tensor space, a linear model may be written (ignoring a bias term) as f(W) = W ◦ Φ, where Φ ∈ Rn1xn2x...nD is the fea</context>
</contexts>
<marker>Kolda, Bader, 2009</marker>
<rawString>Tamara Kolda and Brett Bader. 2009. Tensor Decompositions and Applications SIAM Review, 51:455-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<date>2005</date>
<booktitle>Online Large-Margin Training of Dependency Parsers Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1763" citStr="McDonald et al., 2005" startWordPosition="280" endWordPosition="283">es so that as much human knowledge of language can be brought to bear on the (prediction) task as possible. This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets: the feature weights cannot be estimated reliably. Most traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space. Many learning algorithms applied to NLP problems, such as the Perceptron (Collins, 2002), MIRA (Crammer et al., 2006; McDonald et al., 2005; Chiang et al., 2008), PRO (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012) etc., are based on vector-space models. Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representatio</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online Large-Margin Training of Dependency Parsers Proceedings of the 43rd Annual Meeting of the ACL, 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amnon Shashua</author>
<author>Tamir Hazan</author>
</authors>
<date>2005</date>
<booktitle>NonNegative Tensor Factorization with Applications to Statistics and Computer Vision Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="2457" citStr="Shashua and Hazan, 2005" startWordPosition="399" endWordPosition="402">nd Smith, 2012) etc., are based on vector-space models. Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight. In this paper, we shift the model from vectorspace to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representations have been applied to computer vision problems (Hazan et al., 2005; Shashua and Hazan, 2005) and information retrieval (Cai et al., 2006a) a long time ago. More recently, it has also been applied to parsing (Cohen and Collins, 2012; Cohen and Satta, 2013) and semantic analysis (Van de Cruys et al., 2013). A linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors. This low-rank approximation imposes structural constraints on the feature weights and can be regarded as a form of regularization. With this representation, we no longer need to estimate individual feature weights d</context>
</contexts>
<marker>Shashua, Hazan, 2005</marker>
<rawString>Amnon Shashua, and Tamir Hazan. 2005. NonNegative Tensor Factorization with Applications to Statistics and Computer Vision Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Thierry Poibeau</author>
<author>Anna Korhonen</author>
</authors>
<title>A Tensor-based Factorization Model of Semantic Compositionality</title>
<date>2013</date>
<booktitle>Proceedings of NAACLHLT,</booktitle>
<pages>1142--1151</pages>
<marker>Van de Cruys, Poibeau, Korhonen, 2013</marker>
<rawString>Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2013. A Tensor-based Factorization Model of Semantic Compositionality Proceedings of NAACLHLT, 1142–1151.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>