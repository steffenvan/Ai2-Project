<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001349">
<title confidence="0.5649">
Language Production: the Source of the Dictionary
</title>
<author confidence="0.956042">
David D. McDonald
</author>
<affiliation confidence="0.933639">
University of Massachusetts at Amherst
</affiliation>
<address confidence="0.364217">
April 1980
</address>
<sectionHeader confidence="0.936582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994805882353">
Ultimately in any natural language production system the largest amount of
human effort will go into the construction of the dictionary: the data base
that associates objects and relations in the program&apos;s domain with the words
and phrases that could be used to describe them. This paper describes a
technique for basing the dictionary directly on the semantic abstraction
network used for the domain knowledge itself, taking advantage of the
inheritance and specialization machanisms of a network formalism such as
KL-ONE. The technique creates considerable economics of scale, and makes
possible the automatic description of individual objects according to their
position in the semantic net. Furthermore, because the process of deciding
what properties to use in an object&apos;s description is now given over to a
common procedure, we can write general-purpose rules to, for example,
avoid redundancy or grammatically awkward constructions.
Regardless of its design, every system for natural !anguage
production begins by selecting objects and relations from the speaker&apos;s
internal model of the world, and proceeds by choosing an English phrase to
describe each selected item, combining them according to the properties of
the phrases and the constraints of the language&apos;s grammar and rhetoric. To
do this, the system must have a data base of some sort, in which the objects
it will talk about are somcwhow associated with the appropriate word or
phrase (or with procedures that will construct them). I will refer to such a
data base asa dictionary.
Every production system has a dictionary in one form or another, and
its compilation is probably the single most tedious job that the human
designer must perform. In the past, typically every object and relation has
been given its own individual &amp;quot;lex&amp;quot; property with the literal phrase to be
used: no attempt was made to share criteria or sub-phrases between
properties: and there was a tacit assumtion that the phrase would have the
right form and content in any of the contexts that the object will be
mentioned. (For a review of this literature, see C13 .) However,
dictionaries built in this way become increasingly harder to maintain as
programs become larger and their discourse more sophisticated. We would
like instead some way to tic the extention of the dictionary directly to the
extcntion of the program&apos;s knowledge base; then, as the knowledge base
expands the dictionary will expand with it with only a minimum of
additional effort.
This paper describes a technique for adapting a semantic abstraction
hierarchy of the sort provided by KL-ONE Cia to function directly as a
dictionary for my production system MO11111 . Its goal is largely
expositional in the sense that while the technique is fully specified and
prototypes have been run, many implementation questions remain to be
explored and it is thus premature to present it as a polished system for
others to use; instead, this paper is intended as a presentation of the
issues—potential economies—that the technique is addressing. In
particular, given the intimate relationship between the choke of
architecture in the network formalism used and the ability of the dictionary
to incorporate linguistically useful generalizations and utilities, this
presentation may suggest additional criteria for network design. namely to
make it easier to talk about the objects the network
The basic idea of &amp;quot;piggybacking&amp;quot; the dictionary onto the speaker&apos;s
regular semantic net can be illustrated very simply: Consider the KL.ONE
network in figure one, a fragment taken from a conceptual taxonomy for
augmented transition nets (given in tklone]). The dictionary will provide
the means to describe individual concepts (filled ellipses) on the basis of
their links to generic concepts (empty ellipses) and their functional roles
(squares), as shown there for the individual concept &amp;quot;C205&amp;quot;. The default
English description of C205 (i.e. &amp;quot;the jump arc from S/NP to 5/0C C&apos;) is
created recursively from descriptions of the three network relations that
C205 participates in: its &amp;quot;superconcept&amp;quot; link to the concept &amp;quot;jump-arc&amp;quot;, and
its two role-value relations: &amp;quot;source-statK205). S/NP&amp;quot; and &amp;quot;next-
state(C205)=S/DCL&amp;quot;. Intuitively, we want to associate each of the
network objects with an English phrase: the concept &amp;quot;arc&amp;quot; with the word
&amp;quot;arc&amp;quot;, the &amp;quot;source-state&amp;quot; role relation with the phrase &amp;quot;C205 comes from
S/NP&amp;quot; (note the embedded references), and so on. The machinery that
actually brings about this association is, of course, much more elaborate,
involving three different meta-level networks describing the whole of the
original, &amp;quot;domain&amp;quot; netwerk, as well as an explicit representation of the
English grammar (i.e. it is itself expressed in kt-oNE).
</bodyText>
<figureCaption confidence="0.705227">
Figure One: the speaker&apos;s original network
</figureCaption>
<bodyText confidence="0.999615545454546">
What does this rather expensive&apos; computational machinery purchase?
There are numrous benefits: The most obvious is the economy of scale
within the dictionary that is gained by drawing directly on the economies
I. What is expensive to represent in an explicit, declarative structure need
not be expensive when translated into procedural Rion. I do not seriously
expect anyone to implement such a dictionary by interpreting the KI.-ONE
structures themselves: given our present hardware such a tact would be
hopelessly inefficient. Instead, a compilation process will in effective
&amp;quot;compact&amp;quot; the explicit version of die dictionary inio an expeditious, space-
expensive (i.e. heavily redundant) version that. perrimus each inheritance
only once and Men runs as an efficient, self-contained procedure.
</bodyText>
<figure confidence="0.9936432">
role links
superconcept links
instance link
test
action
arc-type
urce-state
value-restriction links
value links
&apos;71e jump are front S/NP to S/DCL&amp;quot;
</figure>
<page confidence="0.996678">
57
</page>
<bodyText confidence="0.9949903">
aiready present in the network: a one-time linguistic annotation of the
network&apos;s gcncric concepts and relations can be passed down to describe
arbitrary numbers of instantiating individuals by following general rules
based on the geography of the network. At the same time, the dictionary
&amp;quot;enm&amp;quot; for a object in the network may be specialized and hand-tailored, if
desired, in ordcr to take advantage of special words or idiomatic phrases or
it may inherit partial default realizations, e.g. just for determiners or
adverbial modifiers. while specializing. its other parts. More generally,
because we lime now reified the process of collecting the &amp;quot;raw material&amp;quot; of
the production process (i.e. scanning the network). we can impose rules and
constraints on it just as though it were another part of the production
Planning process; we can develop a dictionary grammar entirely analogous
to our grammar of English. This allows us to filter or transform the
collection process under contextual control according to general rules, and
thereby, among other things, automatically avoid redundancies or violations
of grammatical constraints such as complex-NP.
In order to adapt a semantic net for use as a dictionary we must
determine three points: (1) What type of linguistic annotation to use—just
what is to be associated with the nodes of a network? (2) How annotations
from individual nodes are to be accumulated—what dictates the pattern in
which the network is scanned? (3) How the accumulation process is made
sensitive to context. These will he the focus of the rest of the paper.
The three points of did design are. of course, mutually dependent,
and are further dependent on the requirements of the dictionary&apos;s •
employers, the planning and linguistic realization components of the
production system. In the interests of space I will not go into the details of
these components in this paper, especially as this dictionary design appears
to be useful fbr more than iust inv own particular production system. My
assumptions arc: (1) that the output of the dictionary (the input to my
realization component) is a representation of a natural language phrase as
defined by the grammar and with both words and other objects from the
domain network as its terminals (the embedded domain objects correspond
to the variable parts of the phrase, i.e. the arguments to the original network
relation); and (2) that the planning process (the component-that decides
what to say) will specify that network objects be described either as a
composition of a set of other network relations that it has explicitly selected,
or else will leave the description to a default given in the dictionary.
Meta-level annotation
The basis of the dictionary is a meta-level network constructed so as to
shadow the domain network used by the rest of the speaker&apos;s cognitive
processes. This &amp;quot;dictionary network&amp;quot; describes the domain network from
the point of view of the accumulation procedure and the linguistic
annotation. It is itself an abstraction hierarchy, and is also expressed in
ONE (though see the earlier footnote). Objects in the regular network are
connected by mew-links to their corresponding dictionary &amp;quot;entries&amp;quot;. These
entries arc representations of English phrases (either a single phrase or word
or a cluster of alternative phrases with some decision-criteria to select
among them at run time). When we want to describe an object, we follow
out its meta-link into the dictionary network and then realize the word or
phrase that we find. .
</bodyText>
<subsectionHeader confidence="0.570877">
Specializing Generic Phrases
</subsectionHeader>
<bodyText confidence="0.999910272727273">
The entry for an object may itself have a hierarchical structure that
parallels point for point the hierarchical structure of the object&apos;s description
in the domain. Figure two shows the section of the dictionary network that
annotates the superconcept chain from &amp;quot;jump-arc&amp;quot; to &amp;quot;object&amp;quot;; comparable
dictionary networks can be built for hierarchies of roles or other hierarchical
network structures. Notice how the use of an inheritance mechanism within
the dictionary network (denoted by the vertical links between roles) allows
us on the one hand to state the determiner decision (shown here only as a
cloud) once and for all at the level of the domain concept &amp;quot;object&amp;quot;, while at
the same time we can accumulate or supplant lexical material as we move
down to more specific levels in the domain network.
</bodyText>
<figureCaption confidence="0.750382">
Figure Two: the meta-level dictionary network
</figureCaption>
<bodyText confidence="0.999910583333333">
After all the inheritance is factored in, the entry for. e.g., the generic
concept &amp;quot;jump-arc&amp;quot; will describe a noun phrase (represented by an
indivival concept in ki..oNE.) whose head position, is filled by the word
&amp;quot;arc&amp;quot;, classifier position by &amp;quot;jump&amp;quot;, and whose determiner will be
calculated (at run time) by the same routine that calculated determiners for
objects in general (e.g. it will react to whether the reference is to a generic or
an individual, to how, many other objects have the same description, to
whether any special contrastive effects are intended, etc, see Call !).
Should the planner decide to use this entry by itself, say to produce
&amp;quot;C205 is (a jump arcr, this description from the dictionary network would
be converted to a proper constituent structure and integrated with the rest
of the utterance under production. However, the entry will often be used in
conjunction with the entries for several other domain objects, in which case
it is first manipulated as a description—constraint statement—in order to
determine what grammatical construction(s) would realize the objects as a
group.
The notion of creating a consolidated English phrase out of the
phrases for several different objects is central to the power of this
dictionary. The designer is only expected to explicitly designate words for
the generic objects in the domain network; the entries for the individual
objects that the generic objects describe and even the entries for a
hierarchical chain such as in figure two should typically be constructable by
default by following general-purpose linguistic rules and combination
heuristics.
</bodyText>
<figure confidence="0.999022333333333">
ROPER-NAM
head
determiner
head
modifiers
qualifier
.-40NNECTiNM-71
.--(JUMP-AIRC)
NOUN-PHRAS
.--COBJECT)
&amp;quot;connects&amp;quot;
&amp;quot;jump&amp;quot;
</figure>
<page confidence="0.988532">
58
</page>
<bodyText confidence="0.976246210526316">
Large entries out of small ones
Figure three shows a sketch of the combination process. Here we
need a dictionary entry to describe the relationship between the specific
jump-arc C205 and the state it leads to. S/DCL. i.e. we want something like
the sentence &amp;quot;(C205, goes to &lt;&apos;S/DCL)&amp;quot;, where the references in angle
brackets would be ultimately replaced by their own English phrases. When
the connecting role relation (&amp;quot;next-state&amp;quot;) can be rendered into English by a
conventional pattern, we can use an automatic combination technique as in
the figure to construct a linguistic relationship for the domain one by using
a conventional dictionary entry for the concept-role-value relations as
specialized by the specific entry for the role &amp;quot;next-state&amp;quot;.
The figure shows diagramatically the relationship between the
domain network relation, its meta-level description as an object in the
network formalism (i.e. it is an instance of a concept linked to one of its
roles linked in turn to the role value), and finally the corresponding
conventional linguistic construction. The actual KL-ONE representation of
this relation is considerably more elaborate since the links themselves are
reified, however this sketch shows the relevant level of detail as regards
what kinds of knowledge are needed in order to assemble the entry
</bodyText>
<figureCaption confidence="0.488471">
Figure Three: Combining Entries by Network Relations
</figureCaption>
<bodyText confidence="0.999523068965517">
procedurally. First the domain relation is picked out and categorized: here
this was done by a the conventional meta-level description of the relation in
terms of the ItL.ONE primitives it was built from, below we will see how a
comparable categorization can be done on a purely linguistic basis. With
the relation categorized, we can associated it with an entry in the dictionary
network, in this case an instance of a &amp;quot;basic-clause&amp;quot; (i.e. one without any
adjuncts or root-transformations). We now have determined a mapping
from the entries for the components of the original domain relation to
linguistic roles within a clause and have, in effect, created the relation&apos;s
entry which we could then compile for efficiency,
There is much more to be said about how the &amp;quot;embedded entries&amp;quot;
can be controlled, how, for example, the planner can arrange to say either
&amp;quot;C205 goes to S/DCL&amp;quot; or &amp;quot;There is a jump arc going to S/DCL&amp;quot; by
dynamically specializing the description of the clause, however it would be
taking us too far afield: the interested reader is referred to [thesisj. The
point to be made here is just that the writer of the dictionary has an option
either to write specific dictionary entries for domain relations, or to leave
them to general &amp;quot;macro entries&amp;quot; that will build them out of the entries for
the objects involved as just sketched. Using the macro entries of course
mean that less effort will be needed over all, but using specific entries
permits one to take advantage of special idioms or variable phrases that are
either not productive enough or not easy enough to pick out in a standard
meta-level description of the domain network to be worth writing macro
entries for. A simple example would be a special entry for when one plans
to describe an arc in terms of both its source and its next states: in this case
there is a nice compaction available by using the verb &amp;quot;conned( in a single
clause (instead of one clause for each role). Since the KI.-ONE formalism has
no transparent means of optionally bundling two roles into one, this
compound relation has to be given its own dictionary entry by hand.
</bodyText>
<subsectionHeader confidence="0.967933">
Making combinations linguistically
</subsectionHeader>
<bodyText confidence="0.987970636363636">
Up to this point, we have been looking at associations between
&amp;quot;organic&amp;quot; objects or relations in the domain network and their dictionary
entries for production. It is often the case however, that the speech planner
will want to talk about combinations of objects or complex relations that
have been assembled just for the occasion of one conversation and have no
natural counterpart within the regular domain network. In a case like this
there would not already be an entry in the dictionary for the new relation:
however, in most eases we can still produce an integrated phrase by looking
at how the components of the new relation can combine linguistically.
These linguistic combinations are MX so much the provence of the
dictionary as of my linguistic realization component. MOBILE. MUMBLE
has the ability to perform what in the early days of transformational
generative grammar were referred to as &amp;quot;generalized transformations&amp;quot;: the
combining of two or more phrases into a single phrase on the basis of their
linguistic descriptions. We have an example of this in the original example
of the default description of C205 as &amp;quot;the jump arc from S/NP to S/DCL&amp;quot;.
This phrase was produced by having the default planner construct an
expression indicating which network relations to combine (or more
precisely, which phrases to combine, the phrases being taken from the
entries of the relations), and then pass the expression to MUMBLE which
produces the &amp;quot;compound&amp;quot; phrase on the basis of the linguistic description
of the argument phrases. The expression would look roughly like this:1
(describe C205 al (and [np the jump on
tclause C205 ccducable-vp comes from S/NP 11
(clause C2°5 (reducable-vp goes to S/OCL I
MLIMM.E&apos;s task is the production of an object description front the raw
material of a noun phrase and two clauses. To do this, it will have to match
the three phrases against one of its known linguistic combination patterns,
just as the individual concept, role, and value were matched by a pattern
from the ttuoNc representation formalism. In this case, it characterizes the
trio as combinable through the adjunction of, the two clauses to the noun
phrase as qualifiers. Additionally, the rhetorical label &amp;quot;reducable-vp&amp;quot; in the
clauses indicates that their verbs can be omitted without losing significant
</bodyText>
<footnote confidence="0.653750625">
1. A &amp;quot;phrase&amp;quot; in a dictionary entry docs not consist simply of a string of
words. They are actually schemata specifying the grammatical and
rhetorical relationships that the words and argument domain objects
participate in according to their timetional roles. &apos;the bracketed expressions
Shown in the expression are for expository purposes only and are modeled
on the usual representation for phrase structure. Embedded objects such as
&amp;quot;C205&amp;quot; or &amp;quot;Sils1P&amp;quot; will he replaced by their own English phrases
incrementally as the containing phrases is realized.
</footnote>
<page confidence="0.998727">
59
</page>
<bodyText confidence="0.99018525">
information. triggering a stylistic transformation to shorten and simplify the
phrase. At this point attatittE has a linguistic representation of its decision
which is turned over to the normal realization process for completion.
Exaustive details of these operations may be found in (13 .
</bodyText>
<subsectionHeader confidence="0.765157">
Contextual Effects
</subsectionHeader>
<bodyText confidence="0.997635357142857">
The mechanisms of the dictionary per se perform two functions: (1)
the association of the &amp;quot;ground level&amp;quot; linguistic phrases with the objects of
the domain network. and (2) the proper patterns for accumulating the
linguistic descriptions of other parts of the domain network so as to describe
complex generic relations or to describe individual concepts in terms of
their specific relations and their generic description (as with C205). On top
of these two levels is grafted a third level of contextually-triggered effects;
these effects are carried out by MUMBLE (the component that is maintaining
the linguistic context that is the source of the triggers), acting at the point
where combinations arc submitted to it as just described.
To best illustrate the contextual effects, we should move to a slightly
more complex example, one that is initiated by the speaker&apos;s planning
process rather by than a default. Suppose that the speaker is talking about.
the ArN state &amp;quot;S/DCL&amp;quot; and wants to say in effect that it is part of the
domain relation &amp;quot;next-state(C205)=S/IXL&amp;quot;. The default way to express
this relation is as a fact about the jump arc C205; but what we are doing
now is to use it as fact about S/DCL which will require the production of a
quite different phrase. The planning process expresses this intention to
NIUNIBLE with the following expression:
(say-about C205 that (next-state C205 S/DCL))
The operator &amp;quot;say-about&amp;quot; is responsible for determining, on the basis
of the dictionary&apos;s description of the &amp;quot;next-state&amp;quot; relation, what English
construction to use in order to express the speaker&apos;s intented focus. When
the dictionary contains several possible =tinting phrases for a relation (for
example &amp;quot;not-statOC205) is the next state alter source-state(C205)&amp;quot; or
&amp;quot;next-staugczos) is the target of Cans&amp;quot;), then &amp;quot;say-about&amp;quot; will have to choose
between the realizations on the basis either of some stylistic criteria, for
example whether one of the contained relations had been mentioned
recently or some default (e.g. &amp;quot;souree-staioC205)&amp;quot;). Let us suppose for present
purposes that the only phrase listed in dictionary for the next-state relation
is the one from the first example, Le.
cam [r.ducabiev goes to S/DCL 11
Now, &amp;quot;say-about&apos;&amp;quot;s goal is a sentence that has S/DCL as its subject
It can tell from the dictionary&apos;s annotation and its English grammar that the
phrase as it stands will not permit this since the verb &amp;quot;go to&amp;quot; does not
passivize; however, the phrase is amenable to a kind of clefting
transformation that would yield the text: &amp;quot;S/DCL is where cans goes to&amp;quot;.
&amp;quot;Say-about&amp;quot; arranges for this construction by building the structure below
as its representation of its decision, passing it on to MUNIIILE for realization.
Note that this structure i essentially a linguistic constituent structure of the
usual sort, describing the (annotated) surface structure of&apos; the intended text
to the depth that &amp;quot;say-about&amp;quot; has planned it..
</bodyText>
<figure confidence="0.986940142857143">
clause
subieredicatoi
S/DCL VP
verb -nom!
be
[relative-on] [wh•tracei
S/DCL next-state(C205)=S/DCL
</figure>
<figureCaption confidence="0.996442">
Figure Four tile output of the &amp;quot;say-abgut&amp;quot; operator
</figureCaption>
<bodyText confidence="0.9999874">
The functional labels marking the constituent positions (i.e.
&amp;quot;subject&amp;quot;, &amp;quot;verb&amp;quot;, etc.) control the options for the realization of the
domain-network objects they initially contain. (The objects will be
subsequently replaced by the phrases that realize them, processing front left
to right) Thus the first instance of S/I)CL, in the subject position, is
realized without contextual effects as the name &amp;quot;S/DCL&amp;quot;: while the second
instance, acting as the relative pronoun for the cleft, is realized as the
interrogative pronoun &amp;quot;where&amp;quot;: and the final instance, embedded within the
&amp;quot;next-state&amp;quot; relation, is supressed entirely even though the rest of the
relation is expressed normally. These contextual variations are all entirely
transparent to the dictionary mechanisms and demonstrate how we can
increase the utility of the phrases by carefully annotating them in the
dictionary and using general purpose operations that are triggered by the
descriptions of the phrases alone, therefore not needing to know anything
about their semantic content.
This example was of contextual effects that applied after the domain
objects had been embedded in a linguistic structure. Linguistic context can
have its effect earlier as well by monitoring the accumulation process and
applying its effects at that level. Considering how the phrase for the jump
arc C205 would be formed in this same example. Since the planner&apos;s
original instruction (i.e. &amp;quot;(say-about— )&amp;quot; did not mention C205 specifically,
the description of that object will be left to the default process discussed
earlier. In the original example, C205 was described in issoLation, here it is
part of an ongoing discourse context which must be allowed to influence the
process.
The default description employed all three of the domain-network
relations that C205 is involved in. In this discourse context, however, one of
those relations, &amp;quot;next-state(c205)=S/DCL&amp;quot;, has already be given in the
text: were we to include it in this realization of C205, the result would be
garishly redundant and quite unnatural, i.e. &amp;quot;S/DCL is where the jump arc
from VW? to S/DCL goes to&amp;quot;. To rule out this realization, we can filter the
original set of three relations, eliminating the redundant relation because we
know that it is already mentioned in the text Doing this entails (1) having .
some way to recognize when a relation is already given in the text, and (2) a
predictable point in the process when the filtering can be done. The second
is straight forward, the &amp;quot;describe-as&amp;quot; function is the interface between the
planner and the realization components; we simply add a check in that
function to scan through the list of relation-entries to be combined and
arrange for given relations to be filtered out.
As for the definition of &amp;quot;given&amp;quot;, NIUNIBLE maintains a multi-purpose
record of the current discourse context which, like the dictionary, is a meta-
level network describing the original speaker&apos;s network from yet this other
point of view. Mem-links connect relations in the speaker&apos;s network with
the roles they currentiy play in the ongoing discourse, as illustrated in figure
five. The definition of &amp;quot;given&amp;quot; in terms of properties defined by discourse
</bodyText>
<page confidence="0.993215">
60
</page>
<bodyText confidence="0.634097">
roles such as these in conjunction with heuristics about how much of the
earlier text is likely to still be remembered.
</bodyText>
<figureCaption confidence="0.967276">
Figure Four: using the discourse-context as a filter
</figureCaption>
<bodyText confidence="0.998939761904762">
Once able to refer to a rich, linguistically annotated description of the
context, the powers of the dictionary can be extended still ftuther to
incorporate contextually-triggered transformations to avoid stylistically
awkward or ungrammatical linguistic combinations. This part of the
dictionary design is still being elaborated, so I will say only what sort of
effects are trying to be achieved.
Consider what was done earlier by the &amp;quot;say-about&amp;quot; function: there
the planner proposed to say something about one object by saying a relation
in which the object was involved, the text chooscn for the relation being
specially transformed to insure that its thematic subject was the object in
question. In these situations, the planner decides to use the relations it does
without any particular regard for their potential linguistic structure. This
means that there is a certain potential for linguistic disaster. Suppose we
wanted to use our earlier trio of relations about C205 as the basis of a
question about S/DCI.; that is, suppose our planner is a program that is
building up an augmented transition net in response to a description fed to
it by its human user and that it has reached a point where it knows that
there is a sub-network of the ATN that begins with the state S/DC1. but it
does not yet know how that sub-network is reached. (This would be as if
the network of figure one had the &amp;quot;unknown-state&amp;quot; in place of S/NP.)
Such a planner would be motivated to ask its user:
</bodyText>
<equation confidence="0.524128">
(10at &lt;state) is-such-Ma( next-state(C205)=&lt;state&gt;)
</equation>
<bodyText confidence="0.997432770833333">
Realizing this question will mean coming up with a description of
C205, that name being one made up by the planner rather than the user. It
can of course be described in terms of its properties as already shown:
however, if this description were done without appreciating that it occured
in the middle of a question, it would be possible to produce the nonsense
sentence:
&amp;quot;where does the jump arc from lead to .3/0C L?&amp;quot;
Here the embedded reference to the &amp;quot;unknown-state&amp;quot; (part of the relation,
&amp;quot;sourceitate(C205)= unknown-state&amp;quot;) appeared in the text as a relative
clause qualifying the reference to &amp;quot;the jump arc&amp;quot;. But, because &amp;quot;unknown-
state&amp;quot; was being questioned the English grammar automatically suppressed
it. This lead to the nonsense result shown because, as linguists have noted.
in English one cannot question a noun phrase out of a relative clause—that
would be a violation of an &amp;quot;island constraint&amp;quot; C41.
The problem is, of course, that the critical relation ended up in a
relative clause rather than in a different part of the sentence where is
suppression would have been normal. It was not inevitable that the
nonsense form was chosen; there arc equally expressive ersions of the
same content. e.g. &amp;quot;where does the jump arc to S/ DC come from?&amp;quot;, the
problem is how is a planner who knows nothing about grammatical
principles and does not maintain a linguistic description of the current
context to know not to choose the nonsense form when confronted with
ostensibly synomous alternatives. The answer as I see it is that the selection
should not be the planner&apos;s problem—that we can leave the job to the
linguistic realization component which already maintains the necessary
knowledge base. What we do is to make the violation of a grammatical
constraint such as this one of the criteria for filtering out realizations when a
dictionary entry provides several synonomous choices. In this case, the
choice was made by a general transformation already within the realization
component and the alternative would be taken from a knowledge of
linguistically equivalent ways to ajoin the relations.
A grammatical dictionary filter like this one for island-constraints
could also be use for the maintaince of discourse focus or for stylistic
heuristics such as whether to omit a reducable verb. In general, any
decision criteria that is common to all of the dictionary entries should be
amenable to being abstracted out into a mechanism such as this at which
point they can act transparently to the planner and thereby gain an
important modularity of linguistic and conceptual/pragmatic criteria. The
potential problems with this technique involve questions of how much
information the planner can reasonably be expected to supply the linguistic
component. The above filter would be impossible. for example, if the
macro-entry where it is applied were not able to notice that the embedded
description of C205 could mention the &amp;quot;unknown-state&amp;quot; before it
committed itself to the overall structure of the question. The sort of
indexing required to do this does not seem unreasonable to mc as long as
the indexes are passed up with the ground dictionary entries to the macro-
entries. Exactly how to do this is one of the pending questions of
implementation.
</bodyText>
<figure confidence="0.996974888888889">
Current Discourse Context
here AP
or &amp;quot;S/OCL Li
421ZSM
msource-state
next-state
current-clause
head(cu trent-relative-clause)
subject(current-sentence)
</figure>
<page confidence="0.995152">
61
</page>
<bodyText confidence="0.999715285714286">
The dictionaries of other production systems in the literature have
typically been either trivial, unconditional object to word mappings CC,
C11 , or else been encoded in unextendable procedures C.1.1. A
notable exception is the decision tree technique of [goldmanj and as refined
by researchers at the Yale Artificial Intelligence Protect. The improvements
of the present technique over decision trees (which it otherwise resembles)
can be found (1) in the sophistication of its representation of the target
English phrases, whereby abstract descriptions of the rhetorical and
syntactic structure of the phrases may be manipulated by general rules that
need not know anything about their pragmatic content: and (2) in its ability
to compile decision criteria and candidate phrases dynamically for new
objects or relations in terms of the criteria and phrases from. their generic
descriptions.
The dictionary described in this paper is not critically dependent on
the details of the linguistic realization component or planning component it
is used in conjunction with. It is designed, however, to make maximum use
of whatever constraints may be available from the linguistic context
(broadly construed) or from parallel intentional goals. Consequently,
components that do not employ Nroint.E&apos;s technique of representing the
planned and already spoken parrs of the utterance explicitly along with its
linguistic structure may be unable to use it optimally.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999152">
[11 Brachman (1979) Research in Natural Language Understanding.
Quarterly Technical Progress Report No. 7. lloit Beranek and
Newman Inc.
(21 Davey (1974) Discourse Production Ph.D. Dissertation. Edinburgh
University.
[31 Goldman (1974) Computer Generation of Natural language from a
Deep Conceptual Rase, memo AIM-247. Stanford Artificial
Intelligence Laboratory.
(41 McDonald, DI). (1980) Language Production as a Process of
Decision-making Under Constraints. Ph.D. Dissertation, MIT, to
appear as a technical report from the MIT Artificial Intelligence Lab.
[51 (in preparation) &amp;quot;language Production in A.I. • a review&amp;quot;,
manuscript being revised for publication.
[61 Russ (1968) Constraints on Variables in Syntax. Ph.D. Dissertation,
/All%
171 Swartout (1977) A Digitalis Therapy Advisor with Explanations Masters
Dissertation, Mu.
[81 Winograd (1973) Understanding Natural Language Academic Pre=
</reference>
<page confidence="0.99917">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.018987">
<title confidence="0.999754">Language Production: the Source of the Dictionary</title>
<author confidence="0.999987">David D McDonald</author>
<affiliation confidence="0.998488">University of Massachusetts at Amherst</affiliation>
<date confidence="0.998356">April 1980</date>
<abstract confidence="0.99598530515464">Ultimately in any natural language production system the largest amount of effort will go into the construction of the data base that associates objects and relations in the program&apos;s domain with the words and phrases that could be used to describe them. This paper describes a technique for basing the dictionary directly on the semantic abstraction network used for the domain knowledge itself, taking advantage of the inheritance and specialization machanisms of a network formalism such as technique creates considerable economics of scale, and makes possible the automatic description of individual objects according to their position in the semantic net. Furthermore, because the process of deciding what properties to use in an object&apos;s description is now given over to a common procedure, we can write general-purpose rules to, for example, avoid redundancy or grammatically awkward constructions. Regardless of its design, every system for natural !anguage production begins by selecting objects and relations from the speaker&apos;s internal model of the world, and proceeds by choosing an English phrase to describe each selected item, combining them according to the properties of the phrases and the constraints of the language&apos;s grammar and rhetoric. To do this, the system must have a data base of some sort, in which the objects it will talk about are somcwhow associated with the appropriate word or phrase (or with procedures that will construct them). I will refer to such a base dictionary. Every production system has a dictionary in one form or another, and its compilation is probably the single most tedious job that the human designer must perform. In the past, typically every object and relation has been given its own individual &amp;quot;lex&amp;quot; property with the literal phrase to be used: no attempt was made to share criteria or sub-phrases between properties: and there was a tacit assumtion that the phrase would have the right form and content in any of the contexts that the object will be (For a review of this literature, see .) dictionaries built in this way become increasingly harder to maintain as programs become larger and their discourse more sophisticated. We would like instead some way to tic the extention of the dictionary directly to the extcntion of the program&apos;s knowledge base; then, as the knowledge base expands the dictionary will expand with it with only a minimum of additional effort. This paper describes a technique for adapting a semantic abstraction of the sort provided by Cia function directly as a for my production system . goal is largely expositional in the sense that while the technique is fully specified and prototypes have been run, many implementation questions remain to be explored and it is thus premature to present it as a polished system for others to use; instead, this paper is intended as a presentation of the issues—potential economies—that the technique is addressing. In particular, given the intimate relationship between the choke of architecture in the network formalism used and the ability of the dictionary to incorporate linguistically useful generalizations and utilities, this presentation may suggest additional criteria for network design. namely to make it easier to talk about the objects the network The basic idea of &amp;quot;piggybacking&amp;quot; the dictionary onto the speaker&apos;s semantic net can be illustrated very simply: Consider the network in figure one, a fragment taken from a conceptual taxonomy for augmented transition nets (given in tklone]). The dictionary will provide the means to describe individual concepts (filled ellipses) on the basis of their links to generic concepts (empty ellipses) and their functional roles (squares), as shown there for the individual concept &amp;quot;C205&amp;quot;. The default description of C205 (i.e. arc from S/NP to 5/0C C&apos;) created recursively from descriptions of the three network relations that C205 participates in: its &amp;quot;superconcept&amp;quot; link to the concept &amp;quot;jump-arc&amp;quot;, and its two role-value relations: &amp;quot;source-statK205). S/NP&amp;quot; and &amp;quot;nextstate(C205)=S/DCL&amp;quot;. Intuitively, we want to associate each of the network objects with an English phrase: the concept &amp;quot;arc&amp;quot; with the word &amp;quot;source-state&amp;quot; role relation with the phrase comes from the embedded references), and so on. The machinery that actually brings about this association is, of course, much more elaborate, involving three different meta-level networks describing the whole of the original, &amp;quot;domain&amp;quot; netwerk, as well as an explicit representation of the English grammar (i.e. it is itself expressed in kt-oNE). Figure One: the speaker&apos;s original network What does this rather expensive&apos; computational machinery purchase? There are numrous benefits: The most obvious is the economy of scale within the dictionary that is gained by drawing directly on the economies I. What is expensive to represent in an explicit, declarative structure need not be expensive when translated into procedural Rion. I do not seriously expect anyone to implement such a dictionary by interpreting the KI.-ONE structures themselves: given our present hardware such a tact would be hopelessly inefficient. Instead, a compilation process will in effective the explicit version of die inio expeditious, space- (i.e. heavily that. perrimus each inheritance only once and Men runs as an efficient, self-contained procedure. role links superconcept links instance link test action arc-type urce-state value-restriction links value links &apos;71e jump are front S/NP to S/DCL&amp;quot; 57 aiready present in the network: a one-time linguistic annotation of the gcncric concepts and relations can be passed down arbitrary numbers of instantiating individuals by following general rules based on the geography of the network. At the same time, the dictionary &amp;quot;enm&amp;quot; for a object in the network may be specialized and hand-tailored, if desired, in ordcr to take advantage of special words or idiomatic phrases or it may inherit partial default realizations, e.g. just for determiners or modifiers. while its other parts. More generally, because we lime now reified the process of collecting the &amp;quot;raw material&amp;quot; of the production process (i.e. scanning the network). we can impose rules and constraints on it just as though it were another part of the production process; we can develop a dictionary analogous to our grammar of English. This allows us to filter or transform the collection process under contextual control according to general rules, and thereby, among other things, automatically avoid redundancies or violations of grammatical constraints such as complex-NP. In order to adapt a semantic net for use as a dictionary we must determine three points: (1) What type of linguistic annotation to use—just is to be associated with the nodes network? (2) How annotations from individual nodes are to be accumulated—what dictates the pattern in which the network is scanned? (3) How the accumulation process is made sensitive to context. These will he the focus of the rest of the paper. The three points of did design are. of course, mutually dependent, and are further dependent on the requirements of the dictionary&apos;s • employers, the planning and linguistic realization components of the production system. In the interests of space I will not go into the details of these components in this paper, especially as this dictionary design appears fbr more than iust inv own particular production system. My arc: (1) that the output of the dictionary (the input to realization component) is a representation of a natural language phrase as defined by the grammar and with both words and other objects from the domain network as its terminals (the embedded domain objects correspond the variable parts of the phrase, i.e. arguments the original network and (2) that the planning process (the decides will specify that network objects be described either as a composition of a set of other network relations that it has explicitly selected, or else will leave the description to a default given in the dictionary. Meta-level annotation basis of the dictionary is a network so as to shadow the domain network used by the rest of the speaker&apos;s cognitive processes. This &amp;quot;dictionary network&amp;quot; describes the domain network from the point of view of the accumulation procedure and the linguistic annotation. It is itself an abstraction hierarchy, and is also expressed in see the earlier footnote). Objects in the regular network are connected by mew-links to their corresponding dictionary &amp;quot;entries&amp;quot;. These entries arc representations of English phrases (either a single phrase or word or a cluster of alternative phrases with some decision-criteria to select among them at run time). When we want to describe an object, we follow out its meta-link into the dictionary network and then realize the word or phrase that we find. . Specializing Generic Phrases The entry for an object may itself have a hierarchical structure that parallels point for point the hierarchical structure of the object&apos;s description in the domain. Figure two shows the section of the dictionary network that annotates the superconcept chain from &amp;quot;jump-arc&amp;quot; to &amp;quot;object&amp;quot;; comparable dictionary networks can be built for hierarchies of roles or other hierarchical network structures. Notice how the use of an inheritance mechanism within the dictionary network (denoted by the vertical links between roles) allows us on the one hand to state the determiner decision (shown here only as a cloud) once and for all at the level of the domain concept &amp;quot;object&amp;quot;, while at same time we can accumulate or supplant lexical material as we down to more specific levels in the domain network. Figure Two: the meta-level dictionary network After all the inheritance is factored in, the entry for. e.g., the generic &amp;quot;jump-arc&amp;quot; will describe a noun phrase (represented an indivival concept in ki..oNE.) whose head position, is filled by the word position by whose determiner will (at run time) by the same routine that calculated determiners objects in general (e.g. it will react to whether the reference is to a generic or an individual, to how, many other objects have the same description, to any special contrastive effects are intended, etc, see Call Should the planner decide to use this entry by itself, say to produce is (a arcr, description from the dictionary network would be converted to a proper constituent structure and integrated with the rest of the utterance under production. However, the entry will often be used in conjunction with the entries for several other domain objects, in which case it is first manipulated as a description—constraint statement—in order to what grammatical construction(s) would realize the objects a group. The notion of creating a consolidated English phrase out of the phrases for several different objects is central to the power of this dictionary. The designer is only expected to explicitly designate words for the generic objects in the domain network; the entries for the individual objects that the generic objects describe and even the entries for a hierarchical chain such as in figure two should typically be constructable by default by following general-purpose linguistic rules and combination heuristics. ROPER-NAM head determiner head modifiers qualifier NOUN-PHRAS .--COBJECT) &amp;quot;connects&amp;quot; &amp;quot;jump&amp;quot; 58 Large entries out of small ones Figure three shows a sketch of the combination process. Here we need a dictionary entry to describe the relationship between the specific jump-arc C205 and the state it leads to. S/DCL. i.e. we want something like sentence goes to &lt;&apos;S/DCL)&amp;quot;, the references in angle brackets would be ultimately replaced by their own English phrases. When the connecting role relation (&amp;quot;next-state&amp;quot;) can be rendered into English by a conventional pattern, we can use an automatic combination technique as in the figure to construct a linguistic relationship for the domain one by using a conventional dictionary entry for the concept-role-value relations as specialized by the specific entry for the role &amp;quot;next-state&amp;quot;. The figure shows diagramatically the relationship between the domain network relation, its meta-level description as an object in the network formalism (i.e. it is an instance of a concept linked to one of its roles linked in turn to the role value), and finally the corresponding conventional linguistic construction. The actual KL-ONE representation of this relation is considerably more elaborate since the links themselves are reified, however this sketch shows the relevant level of detail as regards what kinds of knowledge are needed in order to assemble the entry Figure Three: Combining Entries by Network Relations procedurally. First the domain relation is picked out and categorized: here this was done by a the conventional meta-level description of the relation in of the it was built from, below we will see how a comparable categorization can be done on a purely linguistic basis. With the relation categorized, we can associated it with an entry in the dictionary network, in this case an instance of a &amp;quot;basic-clause&amp;quot; (i.e. one without any adjuncts or root-transformations). We now have determined a mapping from the entries for the components of the original domain relation to linguistic roles within a clause and have, in effect, created the relation&apos;s entry which we could then compile for efficiency, There is much more to be said about how the &amp;quot;embedded entries&amp;quot; can be controlled, how, for example, the planner can arrange to say either goes to S/DCL&amp;quot; is a jump arc going to S/DCL&amp;quot; dynamically specializing the description of the clause, however it would be taking us too far afield: the interested reader is referred to [thesisj. The point to be made here is just that the writer of the dictionary has an option either to write specific dictionary entries for domain relations, or to leave them to general &amp;quot;macro entries&amp;quot; that will build them out of the entries for objects involved as just sketched. Using the entries course that less effort will be needed all, specific one to take advantage of special idioms or phrases are not productive enough or not enough to pick in a standard meta-level description of the domain network to be worth writing macro entries for. A simple example would be a special entry for when one plans to describe an arc in terms of both its source and its next states: in this case is a nice compaction available by using the verb a single (instead of one clause for each role). Since the has no transparent means of optionally bundling two roles into one, this compound relation has to be given its own dictionary entry by hand. Making combinations linguistically Up to this point, we have been looking at associations between &amp;quot;organic&amp;quot; objects or relations in the domain network and their dictionary entries for production. It is often the case however, that the speech planner will want to talk about combinations of objects or complex relations that have been assembled just for the occasion of one conversation and have no natural counterpart within the regular domain network. In a case like this there would not already be an entry in the dictionary for the new relation: however, in most eases we can still produce an integrated phrase by looking how the components of the new relation can combine linguistic combinations are much the provence of the as of my linguistic realization component. MUMBLE has the ability to perform what in the early days of transformational generative grammar were referred to as &amp;quot;generalized transformations&amp;quot;: the combining of two or more phrases into a single phrase on the basis of their linguistic descriptions. We have an example of this in the original example the default description of C205 as jump arc from S/NP to S/DCL&amp;quot;. This phrase was produced by having the default planner construct an expression indicating which network relations to combine (or more which combine, the phrases being taken from the of the relations), and then pass the expression to produces the &amp;quot;compound&amp;quot; phrase on the basis of the linguistic description the argument phrases. The expression would look roughly like (describeC205 al (and the jump on C205ccducable-vp from 11 to MLIMM.E&apos;s task is the production of an object description front the raw material of a noun phrase and two clauses. To do this, it will have to match the three phrases against one of its known linguistic combination patterns, just as the individual concept, role, and value were matched by a pattern the ttuoNc representation formalism. In this case, the trio as combinable through the adjunction of, the two clauses to the noun phrase as qualifiers. Additionally, the rhetorical label &amp;quot;reducable-vp&amp;quot; in the clauses indicates that their verbs can be omitted without losing significant 1. A &amp;quot;phrase&amp;quot; in a dictionary entry docs not consist simply of a string of words. They are actually schemata specifying the grammatical and rhetorical relationships that the words and argument domain objects participate in according to their timetional roles. &apos;the bracketed expressions Shown in the expression are for expository purposes only and are modeled on the usual representation for phrase structure. Embedded objects such as &amp;quot;C205&amp;quot; or &amp;quot;Sils1P&amp;quot; will he replaced by their own English phrases incrementally as the containing phrases is realized. 59 information. triggering a stylistic transformation to shorten and simplify the phrase. At this point attatittE has a linguistic representation of its decision which is turned over to the normal realization process for completion. Exaustive details of these operations may be found in (13 . Contextual Effects The mechanisms of the dictionary per se perform two functions: (1) the association of the &amp;quot;ground level&amp;quot; linguistic phrases with the objects of the domain network. and (2) the proper patterns for accumulating the linguistic descriptions of other parts of the domain network so as to describe complex generic relations or to describe individual concepts in terms of their specific relations and their generic description (as with C205). On top of these two levels is grafted a third level of contextually-triggered effects; effects are carried out by component that is maintaining the linguistic context that is the source of the triggers), acting at the point where combinations arc submitted to it as just described. To best illustrate the contextual effects, we should move to a slightly more complex example, one that is initiated by the speaker&apos;s planning process rather by than a default. Suppose that the speaker is talking about. the ArN state &amp;quot;S/DCL&amp;quot; and wants to say in effect that it is part of the domain relation &amp;quot;next-state(C205)=S/IXL&amp;quot;. The default way to express this relation is as a fact about the jump arc C205; but what we are doing now is to use it as fact about S/DCL which will require the production of a different phrase. The planning process expresses this intention the following expression: (say-aboutC205 that(next-state C205 S/DCL)) The operator &amp;quot;say-about&amp;quot; is responsible for determining, on the basis of the dictionary&apos;s description of the &amp;quot;next-state&amp;quot; relation, what English construction to use in order to express the speaker&apos;s intented focus. When the dictionary contains several possible =tinting phrases for a relation (for the next state alter the target of Cans&amp;quot;), &amp;quot;say-about&amp;quot; will have to choose between the realizations on the basis either of some stylistic criteria, for example whether one of the contained relations had been mentioned recently or some default (e.g. &amp;quot;souree-staioC205)&amp;quot;). Let us suppose for present purposes that the only phrase listed in dictionary for the next-state relation is the one from the first example, Le. goes to 11 goal is a sentence that has S/DCL as its subject It can tell from the dictionary&apos;s annotation and its English grammar that the as it stands will not permit this since the verb to&amp;quot; not passivize; however, the phrase is amenable to a kind of clefting that would yield the text: is where to&amp;quot;. &amp;quot;Say-about&amp;quot; arranges for this construction by building the structure below as its representation of its decision, passing it on to MUNIIILE for realization. Note that this structure i essentially a linguistic constituent structure of the usual sort, describing the (annotated) surface structure of&apos; the intended text to the depth that &amp;quot;say-about&amp;quot; has planned it.. clause subieredicatoi be [relative-on] [wh•tracei S/DCL next-state(C205)=S/DCL Four of the &amp;quot;say-abgut&amp;quot; operator The functional labels marking the constituent positions (i.e. &amp;quot;subject&amp;quot;, &amp;quot;verb&amp;quot;, etc.) control the options for the realization of the domain-network objects they initially contain. (The objects will be subsequently replaced by the phrases that realize them, processing front left to right) Thus the first instance of S/I)CL, in the subject position, is without contextual effects as the name the second instance, acting as the relative pronoun for the cleft, is realized as the pronoun the final instance, embedded within the &amp;quot;next-state&amp;quot; relation, is supressed entirely even though the rest of the relation is expressed normally. These contextual variations are all entirely to the dictionary mechanisms and demonstrate how we increase the utility of the phrases by carefully annotating them in the and using general purpose operations that are triggered by the phrases alone, therefore not needing to know anything about their semantic content. example was of contextual effects that applied after the objects had been embedded in a linguistic structure. Linguistic context can have its effect earlier as well by monitoring the accumulation process and applying its effects at that level. Considering how the phrase for the jump arc C205 would be formed in this same example. Since the planner&apos;s original instruction (i.e. &amp;quot;(say-about— )&amp;quot; did not mention C205 specifically, the description of that object will be left to the default process discussed earlier. In the original example, C205 was described in issoLation, here it is part of an ongoing discourse context which must be allowed to influence the process. The default description employed all three of the domain-network relations that C205 is involved in. In this discourse context, however, one of those relations, &amp;quot;next-state(c205)=S/DCL&amp;quot;, has already be given in the text: were we to include it in this realization of C205, the result would be redundant and quite unnatural, i.e. is where the jump arc VW? to S/DCL goes to&amp;quot;. rule out this realization, we can original set of three relations, eliminating the redundant relation because we know that it is already mentioned in the text Doing this entails (1) having . some way to recognize when a relation is already given in the text, and (2) a predictable point in the process when the filtering can be done. The second is straight forward, the &amp;quot;describe-as&amp;quot; function is the interface between the planner and the realization components; we simply add a check in that function to scan through the list of relation-entries to be combined and arrange for given relations to be filtered out. for the definition of &amp;quot;given&amp;quot;, a multi-purpose record of the current discourse context which, like the dictionary, is a metalevel network describing the original speaker&apos;s network from yet this other point of view. Mem-links connect relations in the speaker&apos;s network with the roles they currentiy play in the ongoing discourse, as illustrated in figure five. The definition of &amp;quot;given&amp;quot; in terms of properties defined by discourse 60 roles such as these in conjunction with heuristics about how much of the earlier text is likely to still be remembered. Figure Four: using the discourse-context as a filter Once able to refer to a rich, linguistically annotated description of the context, the powers of the dictionary can be extended still ftuther to incorporate contextually-triggered transformations to avoid stylistically awkward or ungrammatical linguistic combinations. This part of the dictionary design is still being elaborated, so I will say only what sort of effects are trying to be achieved. Consider what was done earlier by the &amp;quot;say-about&amp;quot; function: there the planner proposed to say something about one object by saying a relation in which the object was involved, the text chooscn for the relation being specially transformed to insure that its thematic subject was the object in question. In these situations, the planner decides to use the relations it does without any particular regard for their potential linguistic structure. This means that there is a certain potential for linguistic disaster. Suppose we wanted to use our earlier trio of relations about C205 as the basis of a question about S/DCI.; that is, suppose our planner is a program that is building up an augmented transition net in response to a description fed to it by its human user and that it has reached a point where it knows that is a sub-network of the begins with the state S/DC1. but it does not yet know how that sub-network is reached. (This would be as if the network of figure one had the &amp;quot;unknown-state&amp;quot; in place of S/NP.) Such a planner would be motivated to ask its user: (10at&lt;state) is-such-Ma(next-state(C205)=&lt;state&gt;) Realizing this question will mean coming up with a description of C205, that name being one made up by the planner rather than the user. It can of course be described in terms of its properties as already shown: however, if this description were done without appreciating that it occured in the middle of a question, it would be possible to produce the nonsense sentence: &amp;quot;where does the jump arc from lead to .3/0C L?&amp;quot; Here the embedded reference to the &amp;quot;unknown-state&amp;quot; (part of the relation, &amp;quot;sourceitate(C205)= unknown-state&amp;quot;) appeared in the text as a relative clause qualifying the reference to &amp;quot;the jump arc&amp;quot;. But, because &amp;quot;unknownstate&amp;quot; was being questioned the English grammar automatically suppressed it. This lead to the nonsense result shown because, as linguists have noted. in English one cannot question a noun phrase out of a relative clause—that be a violation of an &amp;quot;island constraint&amp;quot; The problem is, of course, that the critical relation ended up in a relative clause rather than in a different part of the sentence where is suppression would have been normal. It was not inevitable that the nonsense form was chosen; there arc equally expressive ersions of the content. e.g. does the jump arc to S/ DC come from?&amp;quot;, problem is how is a planner who knows nothing about grammatical principles and does not maintain a linguistic description of the current context to know not to choose the nonsense form when confronted with ostensibly synomous alternatives. The answer as I see it is that the selection should not be the planner&apos;s problem—that we can leave the job to the linguistic realization component which already maintains the necessary knowledge base. What we do is to make the violation of a grammatical constraint such as this one of the criteria for filtering out realizations when a dictionary entry provides several synonomous choices. In this case, the choice was made by a general transformation already within the realization component and the alternative would be taken from a knowledge of linguistically equivalent ways to ajoin the relations. dictionary filter this one for island-constraints could also be use for the maintaince of discourse focus or for stylistic heuristics such as whether to omit a reducable verb. In general, any decision criteria that is common to all of the dictionary entries should be amenable to being abstracted out into a mechanism such as this at which point they can act transparently to the planner and thereby gain an important modularity of linguistic and conceptual/pragmatic criteria. The potential problems with this technique involve questions of how much information the planner can reasonably be expected to supply the linguistic component. The above filter would be impossible. for example, if the macro-entry where it is applied were not able to notice that the embedded description of C205 could mention the &amp;quot;unknown-state&amp;quot; before it committed itself to the overall structure of the question. The sort of indexing required to do this does not seem unreasonable to mc as long as the indexes are passed up with the ground dictionary entries to the macroentries. Exactly how to do this is one of the pending questions of implementation. Current Discourse Context here AP 421ZSM next-state current-clause head(cu trent-relative-clause) subject(current-sentence) 61 The dictionaries of other production systems in the literature have typically been either trivial, unconditional object to word mappings CC, C11 , or else been encoded in unextendable procedures C.1.1. A notable exception is the decision tree technique of [goldmanj and as refined by researchers at the Yale Artificial Intelligence Protect. The improvements of the present technique over decision trees (which it otherwise resembles) be found (1) in the sophistication of its representation target English phrases, whereby abstract descriptions of the rhetorical and syntactic structure of the phrases may be manipulated by general rules that need not know anything about their pragmatic content: and (2) in its ability to compile decision criteria and candidate phrases dynamically for new or relations in terms of the criteria and phrases their generic descriptions. The dictionary described in this paper is not critically dependent on the details of the linguistic realization component or planning component it is used in conjunction with. It is designed, however, to make maximum use of whatever constraints may be available from the linguistic context (broadly construed) or from parallel intentional goals. Consequently, that do not employ of representing the planned and already spoken parrs of the utterance explicitly along with its linguistic structure may be unable to use it optimally. References [11 Brachman (1979) Research in Natural Language Understanding.</abstract>
<affiliation confidence="0.844147">Technical Progress Report No. Beranek and Newman Inc.</affiliation>
<address confidence="0.920599">21 Davey (1974) Discourse Production Ph.D. Dissertation. Edinburgh</address>
<note confidence="0.556409933333333">University. [31 Goldman (1974) Computer Generation of Natural language from a Deep Conceptual Rase, memo AIM-247. Stanford Artificial Intelligence Laboratory. (41 McDonald, DI). (1980) Language Production as a Process of Decision-making Under Constraints. Ph.D. Dissertation, MIT, to appear as a technical report from the MIT Artificial Intelligence Lab. preparation) &amp;quot;language Production in A.I. • a review&amp;quot;, manuscript being revised for publication. [61 Russ (1968) Constraints on Variables in Syntax. Ph.D. Dissertation, /All% 171 Swartout (1977) A Digitalis Therapy Advisor with Explanations Masters Dissertation, Mu. [81 Winograd (1973) Understanding Natural Language Academic Pre= 62</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Research in Natural Language Understanding.</title>
<date>1979</date>
<tech>Quarterly Technical Progress Report No. 7. lloit</tech>
<institution>Beranek and Newman Inc.</institution>
<marker>1979</marker>
<rawString>[11 Brachman (1979) Research in Natural Language Understanding. Quarterly Technical Progress Report No. 7. lloit Beranek and Newman Inc.</rawString>
</citation>
<citation valid="false">
<date>1974</date>
<institution>Discourse Production Ph.D. Dissertation. Edinburgh University.</institution>
<marker>1974</marker>
<rawString>(21 Davey (1974) Discourse Production Ph.D. Dissertation. Edinburgh University.</rawString>
</citation>
<citation valid="true">
<title>Computer Generation of Natural language from a Deep Conceptual Rase, memo AIM-247. Stanford Artificial Intelligence Laboratory.</title>
<date>1974</date>
<marker>1974</marker>
<rawString>[31 Goldman (1974) Computer Generation of Natural language from a Deep Conceptual Rase, memo AIM-247. Stanford Artificial Intelligence Laboratory.</rawString>
</citation>
<citation valid="true">
<title>Language Production as a Process of Decision-making Under Constraints. Ph.D. Dissertation, MIT, to appear as a technical report from the MIT Artificial Intelligence Lab. [51 (in preparation) &amp;quot;language Production in A.I. • a review&amp;quot;, manuscript being revised for publication.</title>
<date>1980</date>
<marker>1980</marker>
<rawString>(41 McDonald, DI). (1980) Language Production as a Process of Decision-making Under Constraints. Ph.D. Dissertation, MIT, to appear as a technical report from the MIT Artificial Intelligence Lab. [51 (in preparation) &amp;quot;language Production in A.I. • a review&amp;quot;, manuscript being revised for publication.</rawString>
</citation>
<citation valid="true">
<date>1968</date>
<booktitle>Constraints on Variables in Syntax. Ph.D. Dissertation,</booktitle>
<location>All%</location>
<marker>1968</marker>
<rawString>[61 Russ (1968) Constraints on Variables in Syntax. Ph.D. Dissertation, /All%</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swartout</author>
</authors>
<title>A Digitalis Therapy Advisor with Explanations Masters Dissertation,</title>
<date>1977</date>
<location>Mu.</location>
<marker>Swartout, 1977</marker>
<rawString>171 Swartout (1977) A Digitalis Therapy Advisor with Explanations Masters Dissertation, Mu.</rawString>
</citation>
<citation valid="true">
<title>Understanding Natural Language Academic Pre=</title>
<date>1973</date>
<marker>1973</marker>
<rawString>[81 Winograd (1973) Understanding Natural Language Academic Pre=</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>