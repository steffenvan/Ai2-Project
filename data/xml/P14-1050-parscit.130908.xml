<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.750298">
New Word Detection for Sentiment Analysis
</title>
<author confidence="0.729738">
Minlie Huang, Borui Ye*, Yichen Wang, Haiqiang Chen**, Junjun Cheng**, Xiaoyan Zhu
</author>
<affiliation confidence="0.74986175">
State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science
and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China
*Dept. of Communication Engineering, Beijing University of Posts and Telecommunications
**China Information Technology Security Evaluation Center
</affiliation>
<email confidence="0.990946">
aihuang@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.994665" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999803">
Automatic extraction of new words is
an indispensable precursor to many NLP
tasks such as Chinese word segmentation,
named entity extraction, and sentimen-
t analysis. This paper aims at extract-
ing new sentiment words from large-scale
user-generated content. We propose a ful-
ly unsupervised, purely data-driven frame-
work for this purpose. We design statisti-
cal measures respectively to quantify the
utility of a lexical pattern and to measure
the possibility of a word being a new word.
The method is almost free of linguistic re-
sources (except POS tags), and requires
no elaborated linguistic rules. We also
demonstrate how new sentiment word will
benefit sentiment analysis. Experiment re-
sults demonstrate the effectiveness of the
proposed method.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998918135593221">
New words on the Internet have been emerg-
ing all the time, particularly in user-generated con-
tent. Users like to update and share their infor-
mation on social websites with their own language
styles, among which new political/social/cultural
words are constantly used.
However, such new words have made many
natural language processing tasks more challeng-
ing. Automatic extraction of new words is indis-
pensable to many tasks such as Chinese word seg-
mentation, machine translation, named entity ex-
traction, question answering, and sentiment analy-
sis. New word detection is one of the most critical
issues in Chinese word segmentation. Recent stud-
ies (Sproat and Emerson, 2003) (Chen, 2003) have
shown that more than 60% of word segmentation
errors result from new words. Statistics show that
more than 1000 new Chinese words appear every
year (Thesaurus Research Center, 2003). These
words are mostly domain-specific technical terms
and time-sensitive political/social /cultural terms.
Most of them are not yet correctly recognized by
the segmentation algorithm, and remain as out of
vocabulary (OOV) words.
New word detection is also important for sen-
timent analysis such as opinionated phrase ex-
traction and polarity classification. A sentiment
phrase with complete meaning should have a cor-
rect boundary, however, characters in a new word
may be broken up. For example, in a sentence
&amp;quot; * O/ n 4� S/ adv */ v JJ/ n(artists&apos; perfor-
mance is very impressive)&amp;quot; the two Chinese char-
acters“*/v JJ/n(cool; powerful)”should always
be extracted together. In polarity classification,
new words can be informative features for clas-
sification models. In the previous example, &amp;quot;t
JJ(cool; powerful)&amp;quot; is a strong feature for clas-
sification models while each single character is
not. Adding new words as feature in classification
models will improve the performance of polarity
classification, as demonstrated later in this paper.
This paper aims to detect new word for senti-
ment analysis. We are particulary interested in ex-
tracting new sentiment word that can express opin-
ions or sentiment, which is of high value toward-
s sentiment analysis. New sentiment word, as ex-
emplified in Table 1, is a sub-class of multi-word
expressions which is a sequence of neighboring
words &amp;quot;whose exact and unambiguous meaning
or connotation cannot be derived from the mean-
ing or connotation of its components&amp;quot; (Choueka,
1988). Such new words cannot be directly iden-
tified using grammatical rules, which poses a ma-
jor challenge to automatic analysis. Moreover, ex-
isting lexical resources never have adequate and
timely coverage since new words appear constant-
ly. People thus resort to statistical methods such as
Pointwise Mutual Information (Church and Han-
ks, 1990), Symmetrical Conditional Probability
</bodyText>
<page confidence="0.968952">
531
</page>
<note confidence="0.969590111111111">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 531–541,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
(da Silva and Lopes, 1999), Mutual Expectation
(Dias et al., 2000), Enhanced Mutual Information
(Zhang et al., 2009), and Multi-word Expression
Distance (Bu et al., 2010).
two key issues. We then present the experiments
in Section 4. Finally, the work is summarized in
Section 5.
</note>
<table confidence="0.9942928">
New word English Translation Polarity
QIt lovely positive
4A- tragic/tragedy negative
*JJ very cool; powerful positive
k)-t* reverse one&apos;s expectation negative
</table>
<tableCaption confidence="0.999862">
Table 1: Examples of new sentiment word.
</tableCaption>
<bodyText confidence="0.996842928571429">
Our central idea for new sentiment word de-
tection is as follows: Starting from very few seed
words (for example, just one seed word), we can
extract lexical patterns that have strong statistical
association with the seed words; the extracted lex-
ical patterns can be further used in finding more
new words, and the most probable new words can
be added into the seed word set for the next iter-
ation; and the process can be run iteratively un-
til a stop condition is met. The key issues are to
measure the utility of a pattern and to quantify the
possibility of a word being a new word. The main
contributions of this paper are summarized as fol-
lows:
</bodyText>
<listItem confidence="0.998464705882353">
• We propose a novel framework for new word
detection from large-scale user-generated da-
ta. This framework is fully unsupervised
and purely data-driven, and requires very
lightweight linguistic resources (i.e., only
POS tags).
• We design statistical measures to quantify the
utility of a pattern and to quantify the possi-
bility of a word being a new word, respective-
ly. No elaborated linguistic rules are needed
to filter undesirable results. This feature may
enable our approach to be portable to other
languages.
• We investigate the problem of polarity predic-
tion of new sentiment word and demonstrate
that inclusion of new sentiment word benefits
sentiment classification tasks.
</listItem>
<bodyText confidence="0.9997724">
The rest of the paper is structured as follows:
we will introduce related work in the next section.
We will describe the proposed method in Section 3,
including definitions, the overview of the algorith-
m, and the statistical measures for addressing the
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.993639130434783">
New word detection has been usually inter-
weaved with word segmentation, particularly in
Chinese NLP. In these works, new word detection
is considered as an integral part of segmentation,
where new words are identified as the most proba-
ble segments inferred by the probabilistic models;
and the detected new word can be further used to
improve word segmentation. Typical models in-
clude conditional random fields proposed by (Peng
et al., 2004), and a joint model trained with adap-
tive online gradient descent based on feature fre-
quency information (Sun et al., 2012).
Another line is to treat new word detection as
a separate task, usually preceded by part-of-speech
tagging. The first genre of such studies is to lever-
age complex linguistic rules or knowledge. For
example, Justeson and Katz (1995) extracted tech-
nical terminologies from documents using a regu-
lar expression. Argamon et al. (1998) segmented
the POS sequence of a multi-word into small POS
tiles, counted tile frequency in the new word and
non-new-word on the training set respectively, and
detected new words using these counts. Chen and
Ma (2002) employed morphological and statisti-
cal rules to extract Chinese new word. The sec-
ond genre of the studies is to treat new word de-
tection as a classification problem. Zhou (2005)
proposed a discriminative Markov Model to de-
tect new words by chunking one or more separat-
ed words. In (Li et al., 2005), new word detec-
tion was viewed as a binary classification problem.
However, these supervised models requires not on-
ly heavy engineering of linguistic features, but also
expensive annotation of training data.
User behavior data has recently been explored
for finding new words. Zheng et al. (2009) ex-
plored user typing behaviors in Sogou Chinese
Pinyin input method to detect new words. Zhang
et al. (2010) proposed to use dynamic time warp-
ing to detect new words from query logs. Howev-
er, both of the work are limited due to the public
unavailability of expensive commercial resources.
Statistical methods for new word detection
have been extensively studied, and in some sense
exhibit advantages over linguistics-based method-
s. In this setting, new word detection is mostly
</bodyText>
<page confidence="0.995003">
532
</page>
<bodyText confidence="0.999934608695652">
known as multi-word expression extraction. To
measure multi-word association, the first model
is Pointwise Mutual Information (PMI) (Church
and Hanks, 1990). Since then, a variety of sta-
tistical methods have been proposed to measure
bi-gram association, such as Log-likelihood (Dun-
ning, 1993) and Symmetrical Conditional Proba-
bility (SCP) (da Silva and Lopes, 1999). Among
all the 84 bi-gram association measures, PMI has
been reported to be the best one in Czech data
(Pecina, 2005). In order to measure arbitrary n-
grams, most common strategies are to separate n-
gram into two parts X and Y so that existing bi-
gram methods can be used (da Silva and Lopes,
1999; Dias et al., 2000; Schone and Jurafsky,
2001). Zhang et al. (2009) proposed Enhanced
Mutual Information (EMI) which measures the co-
hesion of n-gram by the frequency of itself and the
frequency of each single word. Based on the in-
formation distance theory, Bu et al. (2010) pro-
posed multi-word expression distance (MED) and
the normalized version, and reported superior per-
formance to EMI, SCP, and other measures.
</bodyText>
<sectionHeader confidence="0.995682" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.989715">
3.1 Definitions
</subsectionHeader>
<bodyText confidence="0.999538894736842">
Definition 3.1 (Adverbial word). Words that are
used mainly to modify a verb or an adjective, such
as &amp;quot;太(too)&amp;quot;, &amp;quot;非常(very)&amp;quot;, &amp;quot;十分(very)&amp;quot;, and &amp;quot;特
别(specially)&amp;quot;.
Definition 3.2 (Auxiliary word). Words that are
auxiliaries, model particles, or punctuation marks.
In Chinese, such words are like &amp;quot;着,了,啦,的,啊&amp;quot;,
and punctuation marks include &amp;quot;,o!?;:&amp;quot; and
so on.
Definition 3.3 (Lexical Pattern). A lexical pat-
tern is a triplet &lt; AD, *, AU &gt;, where AD is an
adverbial word, the wildcard * means an arbitrary
number of words 1, and AU denotes an auxiliary
word.
Table 2 gives some examples of lexical pat-
terns. In order to obtain lexical patterns, we can
define regular expressions with POS tags 2 and ap-
ply the regular expressions on POS tagged texts.
Since the tags of adverbial and auxiliary words are
</bodyText>
<footnote confidence="0.9969786">
1We set the number to 3 words in this work considering
computation costs.
2Such expressions are very simple and easy to write be-
cause we only need to consider POS tags of adverbial and
auxiliary word.
</footnote>
<bodyText confidence="0.6003615">
relatively static and can be easily identified, such
a method can safely obtain lexical patterns.
</bodyText>
<table confidence="0.3671704">
Pattern Frequency
&lt;&amp;quot;都&amp;quot;,*,&amp;quot;了&amp;quot;&gt; 562,057
&lt;&amp;quot;都&amp;quot;,*,&amp;quot;的&amp;quot;&gt; 387,649
&lt;&amp;quot;太&amp;quot;,*,&amp;quot;了&amp;quot;&gt; 380,470
&lt;&amp;quot;不&amp;quot;,*,&amp;quot;,&amp;quot;&gt; 369,702
</table>
<tableCaption confidence="0.746327">
Table 2: Examples of lexical pattern. The frequen-
cy is counted on 237,108,977 Weibo posts.
</tableCaption>
<subsectionHeader confidence="0.999082">
3.2 The Algorithm Overview
</subsectionHeader>
<bodyText confidence="0.999943666666667">
The algorithm works as follows: starting
from very few seed words (for example, a word
in Table 1), the algorithm can find lexical pattern-
s that have strong statistical association with the
seed words in which the likelihood ratio test (L-
RT) is used to quantify the degree of association.
Subsequently, the extracted lexical patterns can be
further used in finding more new words. We de-
sign several measures to quantify the possibility of
a candidate word being a new word, and the top-
ranked words will be added into the seed word set
for the next iteration. The process can be run iter-
atively until a stop condition is met. Note that we
do not augment the pattern set (P) at each iteration,
instead, we keep a fixed small number of patterns
during iteration because this strategy produces op-
timal results.
From linguistic perspectives, new sentiment
words are commonly modified by adverbial words
and thus can be extracted by lexical patterns. This
is the reason why the algorithm will work. Our al-
gorithm is in spirit to double propagation (Qiu et
al., 2011), however, the differences are apparen-
t in that: firstly, we use very lightweight linguis-
tic information (except POS tags); secondly, our
major contributions are to propose statistical mea-
sures to address the following key issues: first, to
measure the utility of lexical patterns; second, to
measure the possibility of a candidate word being
a new word.
</bodyText>
<subsectionHeader confidence="0.999396">
3.3 Measuring the Utility of a Pattern
</subsectionHeader>
<bodyText confidence="0.996570714285714">
The first key issue is to quantify the utility of
a pattern at each iteration. This can be measured
by the association of a pattern to the current word
set used in the algorithm. The likelihood ratio test-
s (Dunning, 1993) is used for this purpose. This
association model has also been used to model as-
sociation between opinion target words by (Hai et
</bodyText>
<page confidence="0.993823">
533
</page>
<bodyText confidence="0.686877">
Algorithm 1: New word detection algorithm
Input:
D: a large set of POS tagged posts
Ws: a set of seed words
kp: the number of patterns chosen at each
iteration
kc: the number of patterns in the candidate
pattern set
kw: the number of words added at each
iteration
K: the number of words returned
Output: A list of ranked new words W
1 Obtain all lexical patterns using regular
expressions on D;
2 Count the frequency of each lexical pattern
and extract words matched by each pattern ;
3 Obtain top kc frequent patterns as candidate
pattern set Pc and top 5,000 frequent words as
candidate word set Wc ;
</bodyText>
<equation confidence="0.832012888888889">
4 P = 4b; W=Ws; t = 0 ;
5 for JWJ &lt; K do
Use W to score each pattern in Pc with
U(p) ;
P = {top kp patterns} ;
Use P to extract new words and if the
words are in Wc, score them with F(w) ;
W = W U{top kw words} ;
Wc = Wc - W ;
</equation>
<bodyText confidence="0.934773166666667">
11 Sort words in W with F(w) ;
12 Output the ranked list of words in W ;
al., 2012).
The LRT is well known for not relying crit-
ically on the assumption of normality, instead, it
uses the asymptotic assumption of the generalized
likelihood ratio. In practice, the use of likelihood
ratios tends to result in significant improvements
in text-analysis performance.
In our problem, LRT computes a contingency
table of a pattern p and a word w, derived from
the corpus statistics, as given in Table 3, where
k1(w, p) is the number of documents that w match-
es pattern p, k2(w, p) is the number of documents
that w occurs while p does not, k3( w, p) is the
number of documents that p occurs while w does
not, and k4( w, p) is the number of documents con-
taining neither p nor w.
</bodyText>
<table confidence="0.482135333333333">
Statistics p p�
w k1(w,p) k2(w, �p)
w� k3(�w, p) k4(�w, �p)
</table>
<tableCaption confidence="0.884041">
Table 3: Contingency table for likelihood ratio test
</tableCaption>
<bodyText confidence="0.9178286">
(LRT).
Based on the statistics shown in Table 3, the
likelihood ratio tests (LRT) model captures the sta-
tistical association between a pattern p and a word
w by employing the following formula:
</bodyText>
<equation confidence="0.989284833333333">
LRT (p, L(P1, k1, n1) * L(P2, k2, n2) 1
(P ) = log L(P, k1, n1) * L(P, k2, n2) )
where:
L(P, k, n) = Pk * (1 − P)n−k; n1 = k1 + k3;
n2 = k2 + k4; P1 = k1/n1; P2 = k2/n2; P =
(k1 + k2)/(n1 + n2).
</equation>
<bodyText confidence="0.9140675">
Thus, the utility of a pattern can be measured
as follows:
</bodyText>
<equation confidence="0.9974065">
U(p) = � LRT(p, wi) (2)
wiEW
</equation>
<bodyText confidence="0.999364">
where W is the current word set used in the algo-
rithm (see Algorithm 1).
</bodyText>
<subsectionHeader confidence="0.9656095">
3.4 Measuring the Possibility of Being New
Words
</subsectionHeader>
<bodyText confidence="0.999907">
Another key issue in the proposed algorithm
is to quantify the possibility of a candidate word
being a new word. We consider several factors for
this purpose.
</bodyText>
<subsectionHeader confidence="0.880165">
3.4.1 Likelihood Ratio Test
</subsectionHeader>
<bodyText confidence="0.999595">
Very similar to the pattern utility measure, L-
RT can also be used to measure the association of
a candidate word to a given pattern set, as follows:
</bodyText>
<equation confidence="0.993513">
LRT (w) = � LRT(w,pi) (3)
piEP
</equation>
<bodyText confidence="0.999873076923077">
where P is the current pattern set used in the algo-
rithm (see Algorithm 1), and pi is a lexical pattern.
This measure only quantifies the association
of a candidate word to the given pattern set. It
tells nothing about the possibility of a word be-
ing a new word, however, a new sentiment word,
should have close association with the lexical pat-
terns. This has linguistic interpretations because
new sentiment words are commonly modified by
adverbial words and thus should have close associ-
ation with lexical patterns. This measure is proved
to be an influential factor by our experiments in
Section 4.3.
</bodyText>
<figure confidence="0.9809804">
6
7
8
9
10
</figure>
<page confidence="0.955663">
534
</page>
<subsectionHeader confidence="0.45834">
3.4.2 Left Pattern Entropy
</subsectionHeader>
<bodyText confidence="0.999517833333333">
If a candidate word is a new word, it will be
more commonly used with diversified lexical pat-
terns since the non-compositionality of new word
means that the word can be used in many differ-
ent linguistic scenarios. This can be measured by
information entropy, as follows:
</bodyText>
<equation confidence="0.998071">
� c(li, w) * log c(li, w) (4)
LPE(w) = − N(w) N(w)
liEL(Pc,w)
</equation>
<bodyText confidence="0.999959111111111">
where L(Pc, w) is the set of left word of all pat-
terns by which word w can be matched in Pc ,
c(li, w) is the count that word w can be matched
by patterns whose left word is li, and N(w) is the
count that word w can be matched by the patterns
in Pc. Note that we use Pc, instead of P, because
the latter set is very small while computing entropy
needs a large number of patterns. Tuning the size
of Pc will be further discussed in Section 4.4.
</bodyText>
<subsectionHeader confidence="0.753029">
3.4.3 New Word Probability
</subsectionHeader>
<bodyText confidence="0.999991">
Some words occur very frequently and can be
widely matched by lexical patterns, but they are
not new words. For example, &amp;quot; IM(love to eat)&amp;quot;
and &amp;quot; I说(love to talk)&amp;quot; can be matched by many
lexical patterns, however, they are not new words
due to the lack of non-compositionality. In such
words, each single character has high probability
to be a word. Thus, we design the following mea-
sure to favor this observation.
</bodyText>
<equation confidence="0.998188">
p(wi) (5)
1 − p(wi)
</equation>
<bodyText confidence="0.999781666666667">
where w = w1w2 ... wn, each wi is a single char-
acter, and p(wi) is the probability of the character
wi being a word, as computed as follows:
</bodyText>
<equation confidence="0.9974435">
all(wi) − s(wi)
p(wi) = all(wi)
</equation>
<bodyText confidence="0.999858285714286">
where all(wi) is the total frequency of wi, and
s(wi) is the frequency of wi being a single char-
acter word. Obviously, in order to obtain the value
of s(wi), some particular Chinese word segmen-
tation tool is required. In this work, we resort to
ICTCLAS (Zhang et al., 2003), a widely used tool
in the literature.
</bodyText>
<subsubsectionHeader confidence="0.480127">
3.4.4 Non-compositionality Measures
</subsubsectionHeader>
<bodyText confidence="0.995986571428571">
New words are usually multi-word expres-
sions, where a variety of statistical measures have
been proposed to detect multi-word expressions.
Thus, such measures can be naturally incorporated
into our algorithm.
The first measure is enhanced mutual infor-
mation (EMI) (Zhang et al., 2009):
</bodyText>
<equation confidence="0.991585">
F EMI (w) = log2 n / N (6)
�i=1 N
</equation>
<bodyText confidence="0.999644153846154">
where F is the number of posts in which a multi-
word expression w = w1w2 ... wn occurs, Fi is
the number of posts where wi occurs, and N is the
total number of posts. The key idea of EMI is to
measure word pair’s dependency as the ratio of its
probability of being a multi-word to its probability
of not being a multi-word. The larger the value, the
more possible the expression will be a multi-word
expression.
The second measure we take into account is
normalized multi-word expression distance (Bu et
al., 2010), which has been proposed to measure the
non-compositionality of multi-word expressions.
</bodyText>
<equation confidence="0.999629333333333">
log|µ(w) |− log|ϕ(w)|
NMED(w) = logN − (7)
log |ϕ(w) |
</equation>
<bodyText confidence="0.999580090909091">
where µ(w) is the set of documents in which all
single words in w = w1w2 ... wn co-occur, ϕ(w)
is the set of documents in which word w occurs
as a whole, and N is the total number of docu-
ments. Different from EMI, this measure is a strict
distance metric, meaning that a smaller value in-
dicates a larger possibility of being a multi-word
expression. As can be seen from the formula, the
key idea of this metric is to compute the ratio of the
co-occurrence of all words in a multi-word expres-
sions to the occurrence of the whole expression.
</bodyText>
<subsectionHeader confidence="0.620049">
3.4.5 Configurations to Combine Various
Factors
</subsectionHeader>
<bodyText confidence="0.999923666666667">
Taking into account the aforementioned fac-
tors, we have different settings to score a new
word, as follows:
</bodyText>
<equation confidence="0.9996099">
FLRT(w) = LRT(w) (8)
FLPE(w) = LRT(w) * LPE(w) (9)
FNWP(w) = LRT(w) * LPE(w) * NWP(w) (10)
FEMI(w) = LRT(w) * LPE(w) * EMI(w) (11)
LRT(w) * LPE(w)
FNMED(w) = (12)
NMED(w)
n
NWP(w) =
i=1
</equation>
<page confidence="0.993912">
535
</page>
<sectionHeader confidence="0.9984" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999935375">
In this section, we will conduct the following
experiments: first, we will compare our method
to several baselines, and perform parameter tun-
ing with extensive experiments; second, we will
classify polarity of new sentiment words using t-
wo methods; third, we will demonstrate how new
sentiment words will benefit sentiment classifica-
tion.
</bodyText>
<subsectionHeader confidence="0.990616">
4.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999995470588235">
We crawled 237,108,977 Weibo posts from
http://www.weibo.com, the largest social website
in China. These posts range from January of 2011
to December of 2012. The posts were then part-of-
speech tagged using a Chinese word segmentation
tool named ICTCLAS (Zhang et al., 2003).
Then, we asked two annotators to label the top
5,000 frequent words that were extracted by lexi-
cal patterns as described in Algorithm 1. The an-
notators were requested to judge whether a candi-
date word is a new word, and also to judge the po-
larity of a new word (positive, negative, and neu-
tral). If there is a disagreement on either of the
two tasks, discussions are required to make the fi-
nal decision. The annotation led to 323 new word-
s, among which there are 116 positive words, 112
negative words, and 95 neutral words3.
</bodyText>
<subsectionHeader confidence="0.973431">
4.2 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.99983375">
As our algorithm outputs a ranked list of
words, we adapt average precision to evaluate
the performance of new sentiment word detection.
The metric is computed as follows:
</bodyText>
<equation confidence="0.999668666666667">
�K 1 P(k) * rel(k)
AP(K) = K
�k=1 rel(k)
</equation>
<bodyText confidence="0.9995526">
where P(k) is the precision at cut-off k, rel(k) is
1 if the word at position k is a new word and 0 oth-
erwise, and K is the number of words in the ranked
list. A perfect list (all top K items are correct) has
an AP value of 1.0.
</bodyText>
<subsectionHeader confidence="0.971295">
4.3 Evaluation of Different Measures and
Comparison to Baselines
</subsectionHeader>
<bodyText confidence="0.999916">
First, we assess the influence of likelihood ra-
tio test, which measures the association of a word
to the pattern set. As can be seen from Table 4,
the association model (LRT) remarkably boosts the
</bodyText>
<footnote confidence="0.44303">
3All the resources are available upon request.
</footnote>
<bodyText confidence="0.998653342857143">
performance of new word detection, indicating L-
RT is a key factor for new sentiment word extrac-
tion. From linguistic perspectives, new sentiment
words are commonly modified by adverbial words
and thus should have close association with lexical
patterns.
Second, we compare different settings of our
method to two baselines. The first one is en-
hanced mutual information (EMI) where we set
F(w) = EMI(w) (Zhang et al., 2009) and the
second baseline is normalized multi-word expres-
sion distance (LAMED) (Bu et al., 2010) where we
set F(w) = NMED(w). The results are shown
in Figure 1. As can be seen, all the proposed
measures outperform the two baselines (EMI and
NMED) remarkably and consistently. The set-
ting of FLAMED produces the best performance.
Adding NMED or EMI leads to remarkable im-
provements because of their capability of measur-
ing non-compositionality of new words. Only us-
ing LRT can obtain a fairly good results when K is
small, however, the performance drops sharply be-
cause it&apos;s unable to measure non-compositionality.
Comparison between LRT + LPE (or LRT +
LPE + NWP) and LRT shows that inclusion
of left pattern entropy also boosts the performance
apparently. However, the new word probabili-
ty (NWP) has only marginal contribution to im-
provement.
In the above experiments, we set kp = 5 (the
number of patterns chosen at each iteration) and
k,,, = 10 (the number of words added at each iter-
ation), which is the optimal setting and will be dis-
cussed in the next subsection. And only one seed
word &amp;quot;坑爹(reverse one&apos;s expectation)&amp;quot; is used.
</bodyText>
<figureCaption confidence="0.620968333333333">
Figure 1: Comparative results of different measure
settings. X-axis is the number of words returned
(K), and Y-axis is average precision (AP(K)).
</figureCaption>
<page confidence="0.987115">
536
</page>
<table confidence="0.999730222222222">
top K words ⇒ 100 200 300 400 500
LPE 0.366 0.324 0.286 0.270 0.259
LRT+LPE 0.743 0.652 0.613 0.582 0.548
LPE+NWP 0.467 0.400 0.350 0.330 0.320
LRT+LPE+NWP 0.755 0.680 0.612 0.571 0.543
LPE+EMI 0.608 0.551 0.519 0.486 0.467
LRT+LPE+EMI 0.859 0.759 0.717 0.662 0.632
LPE+NMED 0.749 0.690 0.641 0.612 0.576
LRT+LPE+NMED 0.907 0.808 0.741 0.723 0.699
</table>
<tableCaption confidence="0.999962">
Table 4: Results with vs. without likelihood ratio test (LRT).
</tableCaption>
<subsectionHeader confidence="0.994309">
4.4 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.999966595238095">
Firstly, we will show how to obtain the op-
timal settings of kp and kw. The measure setting
we take here is FNMED(w), as shown in Formula
(12). Again, we choose only one seed word &amp;quot;坑
爹(reverse one&apos;s expectation)&amp;quot;, and the number of
words returned is set to K = 300. Results in Ta-
ble 5 show that the performance drops consistent-
ly across different kw settings when the number of
patterns increases. Note that at the early stage of
Algorithm 1, larger kp (perhaps with noisy pattern-
s) may lead to lower quality of new words; while
larger kw (perhaps with noisy seed words) may
lead to lower quality of lexical patterns. Therefore,
we choose the optimal setting to small numbers, as
kp = 5, kw = 10.
Secondly, we justify whether the proposed al-
gorithm is sensitive to the number of seed words.
We set kp = 5 and kw = 10, and take FNMED
as the weighting measure of new word. We exper-
imented with only one seed word, two, three, and
four seed words, respectively. The results in Ta-
ble 6 show very stable performance when different
numbers of seed words are chosen. It&apos;s interesting
that the performance is totally the same with dif-
ferent numbers of seed words. By looking into the
pattern set and the selected words at each iteration,
we found that the pattern set (P) converges soon
to the same set after a few iterations; and at the be-
ginning several iterations, the selected words are
almost the same although the order of adding the
words is different. Since the algorithm will finally
sort the words at step (11) and P is the same, the
ranking of the words becomes all the same.
Lastly, we need to decide the optimal number
of patterns in Pc (that is, kc in Algorithm 1) be-
cause the set has been used in computing left pat-
tern entropy, see Formula (4). Too small size of
Pc may lead to insufficient estimation of left pat-
tern entropy. Results in Table 7 shows that larg-
er Pc decrease the performance, particularly when
the number of words returned (K) becomes larger.
Therefore, we set |Pc |= 100.
</bodyText>
<subsectionHeader confidence="0.903987">
4.5 Polarity Prediction of New Sentiment
Words
</subsectionHeader>
<bodyText confidence="0.999079529411765">
In this section, we attempt to classifying the
polarity of the annotated 323 new words. Two
methods are adapted with different settings for this
purpose. The first one is majority vote (MV), and
the second one is pointwise mutual information,
similar to (Turney and Littman, 2003). The ma-
jority vote method is formulated as below:
where PW and NW are a positive and negative
set of emoticons (or seed words) respectively, and
#(w, wp) is the co-occurrence count of the input
word w and the item wp. The polarity is judged ac-
cording to this rule: if MV (w) &gt; th1, the word w
is positive; if MV (w) &lt; −th1 the word negative;
otherwise neutral. The threshold th1 is manually
tuned.
And PMI is computed as follows:
where PMI(x, y) = log2( P r(x,y)
</bodyText>
<subsectionHeader confidence="0.493594">
P r(x)�P r(y)), and
</subsectionHeader>
<bodyText confidence="0.9995675">
Pr(·) denotes probability. The polarity is judged
according to the rule: if PMI(w) &gt; th2, w is
positive; if PMI(w) &lt; −th2 negative; otherwise
neutral. The threshold th2 is manually tuned.
As for the resources PW and NW, we
have three settings. The first setting (denoted by
</bodyText>
<equation confidence="0.959341272727273">
E
MV (w) =
wpEPW
#(w, wp) E
|PW |−
#(w, w�)
|NW|
wnENW
E PMI(w, wp)− E PMI(w, w�)
PMI(w) =  |PW  |wnENW |NW|
wpEPW
</equation>
<page confidence="0.983885">
537
</page>
<table confidence="0.999530833333333">
kp 2 3 4 5 10 20 50
kw
5 0.753 0.738 0.746 0.741 0.741 0.734 0.715
10 0.753 0.738 0.746 0.741 0.741 0.728 0.712
15 0.753 0.738 0.746 0.741 0.754 0.734 0.718
20 0.763 0.738 0.744 0.749 0.749 0.735 0.717
</table>
<tableCaption confidence="0.973925">
Table 5: Parameter tuning results for kp and kw. The measure setting is FNMED(w), the seed word set
is {&amp;quot;k)-t*(reverse one&apos;s expectation)&amp;quot;}, and the number of words returned is K = 300.
</tableCaption>
<table confidence="0.999908">
# seeds =&gt;. 1 2 3 4
K=100 0.907 0.907 0.907 0.907
K=200 0.808 0.808 0.808 0.808
K=300 0.741 0.741 0.741 0.741
K=400 0.709 0.709 0.709 0.709
K=500 0.685 0.685 0.685 0.685
</table>
<tableCaption confidence="0.990234">
Table 6: Performance with different numbers of
</tableCaption>
<bodyText confidence="0.992099689655172">
seed words. The measure setting is FNMED(w),
and kp = 5, kw = 10. The seed words are chosen
from Table 1.
Large_Emo) is a set of most frequent 36 emoticons
in which there are 21 positive and 15 negative e-
moticons respectively. The second one (denoted
by Small_Emo) is a set of 10 emoticons, which
are chosen from the 36 emoticons, as shown in
Table 8. The third one (denoted by Opin_Words)
is two sets of seed opinion words, where PW={
n&amp;quot;appy),)Q)�(generous),aMA(beautiful), I-Iffif
q(kind), Q,HA(smart)} and NW ={4)5,b(sad),/J&apos;,
&apos;I(mean),*&apos;(ugly), SSS; (wicked), *(stupid)}.
The performance of polarity prediction is
shown in Table 9. In two-class polarity classifi-
cation, we remove neutral words and only make
prediction with positive/negative classes. The first
observation is that the performance of using emoti-
cons is much better than that of using seed opin-
ion words. We conjecture that this may be be-
cause new sentiment words are more frequently
co-occurring with emoticons than with these opin-
ion words. The second observation is that three-
class polarity classification is much more diffi-
cult than two-class polarity classification because
many extracted new words are nouns such as &amp;quot;A
AE(gay)&amp;quot;,&amp;quot; ;(girl)&amp;quot;, and &amp;quot;k AE(friend)&amp;quot;. Such
nouns are more difficult to classify sentiment ori-
entation.
</bodyText>
<subsectionHeader confidence="0.999945">
4.6 Application of New Sentiment Words to
Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999984176470588">
In this section, we justify whether inclusion of
new sentiment word would benefit sentiment clas-
sification. For this purpose, we randomly sampled
and annotated 4,500 Weibo posts that contain at
least one opinion word in the union of the Hownet
4 opinion lexicons and our annotated new word-
s. We apply two models for polarity classification.
The first model is a lexicon-based model (denot-
ed by Lexicon) that counts the number of positive
and negative opinion words in a post respective-
ly, and classifies a post to be positive if there are
more positive words than negative ones, and to be
negative otherwise. The second model is a SVM
model in which opinion words are used as feature,
and 5-fold cross validation is conducted.
We experiment with different settings of
Hownet lexicon resources:
</bodyText>
<listItem confidence="0.572658">
• Hownet opinion words (denoted by Hownet):
</listItem>
<bodyText confidence="0.9923185">
After removing some obviously inappropri-
ate words, the left lexicons have 627 posi-
tive opinion words and 1,038 negative opin-
ion words, respectively.
</bodyText>
<listItem confidence="0.696617">
• Compact Hownet opinion words (denoted by
</listItem>
<bodyText confidence="0.99937575">
cptHownet): we count the frequency of the
above opinion words on the training data and
remove words whose document frequency is
less than 2. This results in 138 positive words
and 125 negative words.
Then, we add into the above resources the la-
beled new polar words(denoted by NW, including
116 positive and 112 negative words) and the top
100 words produced by the algorithm (denoted by
T100), respectively. Note that the lexicon-based
model requires the sentiment orientation of each
dictionary entry 5, we thus manually label the po-
</bodyText>
<footnote confidence="0.996341333333333">
4http://www.keenage.com/html/c_index.html.
5This is not necessary for the SVM model. All words in
the top 100 words can be used as feature.
</footnote>
<page confidence="0.978527">
538
</page>
<table confidence="0.999799333333333">
|P, |=&gt;. 50 100 200 300 400 500
K=100 0.907 0.905 0.916 0.916 0.888 0.887
K=200 0.808 0.810 0.778 0.776 0.766 0.764
K=300 0.741 0.731 0.722 0.726 0.712 0.713
K=400 0.709 0.708 0.677 0.675 0.656 0.655
K=500 0.685 0.683 0.653 0.646 0.626 0.627
</table>
<tableCaption confidence="0.996307">
Table 7: Tuning the number of patterns in P, The measure setting is FNMED(w), kp = 5, kw = 10,
and the seed word set is {&amp;quot;坑爹(reverse one&apos;s expectation)&amp;quot;}.
</tableCaption>
<table confidence="0.999729333333333">
Emoticon Polarity Emoticon Polarity
positive negative
positive negative
positive negative
positive negative
positive negative
</table>
<tableCaption confidence="0.9772465">
Table 8: The ten emoticons used for polarity pre-
diction.
</tableCaption>
<table confidence="0.999688555555556">
Methods =&gt;. Majority vote PMI
Two-class polarity classification
Large_Emo 0.861 0.865
Small_Emo 0.846 0.851
Opin_Words 0.697 0.654
Three-class polarity classification
Large_Emo 0.598 0.632
Small_Emo 0.551 0.635
Opin_Words 0.449 0.486
</table>
<tableCaption confidence="0.856003">
Table 9: The accuracy of two/three-class polarity
classification.
</tableCaption>
<bodyText confidence="0.999375538461538">
larity of all top 100 words (we did NOT remove
incorrect new word). This results in 52 positive
and 34 negative words.
Results in Table 10 show that inclusion of
new words in both models improves the perfor-
mance remarkably. In the setting of the original
lexicon (Hownet), both models obtain 2-3% gains
from the inclusion of new words. Similar improve-
ment is observed in the setting of the compact lex-
icon. Note, that T100 is automatically obtained
from Algorithm 1 so that it may contain words that
are not new sentiment words, but the resource also
improves performance remarkably.
</bodyText>
<sectionHeader confidence="0.999171" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.991989333333333">
In order to extract new sentiment words from
large-scale user-generated content, this paper pro-
poses a fully unsupervised, purely data-driven, and
</bodyText>
<table confidence="0.999771714285714">
# Pos/Neg Lexicon SVM
Hownet 627/1,038 0.737 0.756
Hownet+NW 743/1,150 0.770 0.779
Hownet+T100 679/1,172 0.761 0.774
cptHownet 138/125 0.738 0.758
cptHownet+NW 254/237 0.774 0.782
cptHownet+T100 190/159 0.764 0.775
</table>
<tableCaption confidence="0.901004">
Table 10: The accuracy of polarity classfication of
</tableCaption>
<bodyText confidence="0.980009545454546">
Weibo post with/without new sentiment words. N-
W includes 116/112 positive/negative words, and
T100 contains 52/34 positive/negative words.
almost knowledge-free (except POS tags) frame-
work. We design statistical measures to quantify
the utility of a lexical pattern and to measure the
possibility of a word being a new word, respec-
tively. The method is almost free of linguistic re-
sources (except POS tags), and does not rely on
elaborated linguistic rules. We conduct extensive
experiments to reveal the influence of different sta-
tistical measures in new word finding. Compara-
tive experiments show that our proposed method
outperforms baselines remarkably. Experiments
also demonstrate that inclusion of new sentiment
words benefits sentiment classification definitely.
From linguistic perspectives, our framework
is capable to extract adjective new words because
the lexical patterns usually modify adjective word-
s. As future work, we are considering how to ex-
tract other types of new sentiment words, such as
nounal new words that can express sentiment.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997287714285714">
This work was partly supported by the fol-
lowing grants from: the National Basic Re-
search Program (973 Program) under grant No.
2012CB316301 and 2013CB329403, the National
Science Foundation of China project under grant
No. 61332007 and No. 60803075, and the Beijing
Higher Education Young Elite Teacher Project.
</bodyText>
<page confidence="0.997578">
539
</page>
<sectionHeader confidence="0.97078" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999244101851852">
Shlomo Argamon, Ido Dagan, and Yuval Krymolows-
ki. 1998. A memory-based approach to
learning shallow natural language patterns. In
Proceedings of the 17th International Conference
on Computational Linguistics - Volume 1, COL-
ING &apos;98, pages 67--73, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fan Bu, Xiaoyan Zhu, and Ming Li. 2010. Measuring
the non-compositionality of multiword expres-
sions. In Proceedings of the 23rd International
Conference on Computational Linguistics, COL-
ING&apos;10, pages 116--124, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Un-
known word extraction for chinese documents. In
Proceedings of the 19th International Conference
on Computational Linguistics - Volume 1, COL-
ING&apos;02, pages 1--7, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Aitao Chen. 2003. Chinese word segmentation us-
ing minimal linguistic knowledge. In Proceedings
of the Second SIGHAN Workshop on Chinese
Language Processing - Volume 17, SIGHAN&apos;03,
pages 148--151, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yaacov Choueka. 1988. Looking for nee-
dles in a haystack or locating interesting col-
location expressions in large textual databas-
es. In Proceeding of the RIAO&apos;88 Conference
on User-Oriented Content-Based Text and Image
Handling, pages 21--24.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lex-
icography. Comput. Linguist., 16(1): 22--29,
March.
J Ferreira da Silva and G Pereira Lopes. 1999. A local
maxima method and a fair dispersion normaliza-
tion for extracting multi-word units from corpora.
In Sixth Meeting on Mathematics of Language,
pages 369--381.
Gaël Dias, Sylvie Guilloré, and José Gabriel Pereira
Lopes. 2000. Mining textual associations in text
corpora. 6th ACM SIGKDD Work. Text Mining.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Comput. Linguist.,
19(1):61--74, March.
Zhen Hai, Kuiyu Chang, and Gao Cong. 2012.
One seed to find them all: Mining opinion fea-
tures via association. In Proceedings of the 21st
ACM International Conference on Information
and Knowledge Management, CIKM &apos;12, pages
255--264, New York, NY, USA. ACM.
John S Justeson and Slava M Katz. 1995. Technical ter-
minology: some linguistic properties and an algo-
rithm for identification in text. Natural language
engineering, 1(1):9--27.
Hongqiao Li, Chang-Ning Huang, Jianfeng Gao, and
Xiaozhong Fan. 2005. The use of svm for
chinese new word identification. In Natural
Language Processing--IJCNLP 2004, pages 723-
-732. Springer.
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings
of the ACL Student Research Workshop, ACLstu-
dent &apos;05, pages 13--18, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fuchun Peng, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese segmentation and new
word detection using conditional random field-
s. In Proceedings of the 20th International
Conference on Computational Linguistics, COL-
ING &apos;04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extrac-
tion through double propagation. Computational
linguistics, 37(1):9--27.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dic-
tionary headwords a solved problem. In Proc.
of the 6th Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001),
pages 100--108.
Richard Sproat and Thomas Emerson. 2003. The first
international chinese word segmentation bakeoff.
In Proceedings of the Second SIGHAN Workshop
on Chinese Language Processing - Volume 17,
SIGHAN &apos;03, pages 133--143, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Xu Sun, Houfeng Wang, and Wenjie Li. 2012.
Fast online training with frequency-adaptive
learning rates for chinese word segmentation
and new word detection. In Proceedings of
the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers -
Volume 1, ACL &apos;12, pages 253--262, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Beijing Thesaurus Research Center. 2003. Xinhua Xin
Ciyu Cidian. Commercial Press, Beijing.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of seman-
tic orientation from association. ACM Trans. Inf.
Syst., 21(4):315--346, October.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of the Second SIGHAN
Workshop on Chinese Language Processing -
</reference>
<page confidence="0.963858">
540
</page>
<reference confidence="0.999830869565217">
Volume 17, SIGHAN &apos;03, pages 184--187,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Wen Zhang, Taketoshi Yoshida, Xijin Tang, and Tu-
Bao Ho. 2009. Improving effectiveness of
mutual information for substantival multiword
expression extraction. Expert Systems with
Applications, 36(8):10919--10930.
Yan Zhang, Maosong Sun, and Yang Zhang. 2010.
Chinese new word detection from query logs. In
Advanced Data Mining and Applications, pages
233--243. Springer.
Yabin Zheng, Zhiyuan Liu, Maosong Sun, Liyun Ru,
and Yang Zhang. 2009. Incorporating user be-
haviors in new word detection. In Proceedings of
the 21st International Jont Conference on Artifical
Intelligence, IJCAI&apos;09, pages 2101--2106, San
Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
GuoDong Zhou. 2005. A chunking strategy towards
unknown word detection in chinese word segmen-
tation. In Natural Language Processing--IJCNLP
2005, pages 530--541. Springer.
</reference>
<page confidence="0.997852">
541
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.414160">
<title confidence="0.997685">New Word Detection for Sentiment Analysis</title>
<author confidence="0.833532">Minlie Huang</author>
<author confidence="0.833532">Borui Ye</author>
<author confidence="0.833532">Yichen Wang</author>
<author confidence="0.833532">Haiqiang Chen</author>
<author confidence="0.833532">Junjun Cheng</author>
<author confidence="0.833532">Xiaoyan</author>
<affiliation confidence="0.7824025">State Key Lab. of Intelligent Technology and Systems, National Lab. for Information and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR *Dept. of Communication Engineering, Beijing University of Posts and **China Information Technology Security Evaluation</affiliation>
<email confidence="0.751508">aihuang@tsinghua.edu.cn</email>
<abstract confidence="0.99838945">Automatic extraction of new words is an indispensable precursor to many NLP tasks such as Chinese word segmentation, named entity extraction, and sentiment analysis. This paper aims at extractsentiment words large-scale user-generated content. We propose a fully unsupervised, purely data-driven framework for this purpose. We design statistical measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word. The method is almost free of linguistic resources (except POS tags), and requires no elaborated linguistic rules. We also demonstrate how new sentiment word will benefit sentiment analysis. Experiment results demonstrate the effectiveness of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Ido Dagan</author>
<author>Yuval Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow natural language patterns.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics - Volume 1, COLING &apos;98,</booktitle>
<pages>67--73</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7212" citStr="Argamon et al. (1998)" startWordPosition="1126" endWordPosition="1129">c models; and the detected new word can be further used to improve word segmentation. Typical models include conditional random fields proposed by (Peng et al., 2004), and a joint model trained with adaptive online gradient descent based on feature frequency information (Sun et al., 2012). Another line is to treat new word detection as a separate task, usually preceded by part-of-speech tagging. The first genre of such studies is to leverage complex linguistic rules or knowledge. For example, Justeson and Katz (1995) extracted technical terminologies from documents using a regular expression. Argamon et al. (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. Chen and Ma (2002) employed morphological and statistical rules to extract Chinese new word. The second genre of the studies is to treat new word detection as a classification problem. Zhou (2005) proposed a discriminative Markov Model to detect new words by chunking one or more separated words. In (Li et al., 2005), new word detection was viewed as a binary classification problem. However, these s</context>
</contexts>
<marker>Argamon, Dagan, Krymolowski, 1998</marker>
<rawString>Shlomo Argamon, Ido Dagan, and Yuval Krymolowski. 1998. A memory-based approach to learning shallow natural language patterns. In Proceedings of the 17th International Conference on Computational Linguistics - Volume 1, COLING &apos;98, pages 67--73, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Bu</author>
<author>Xiaoyan Zhu</author>
<author>Ming Li</author>
</authors>
<title>Measuring the non-compositionality of multiword expressions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING&apos;10,</booktitle>
<pages>116--124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4405" citStr="Bu et al., 2010" startWordPosition="663" endWordPosition="666">r, existing lexical resources never have adequate and timely coverage since new words appear constantly. People thus resort to statistical methods such as Pointwise Mutual Information (Church and Hanks, 1990), Symmetrical Conditional Probability 531 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 531–541, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics (da Silva and Lopes, 1999), Mutual Expectation (Dias et al., 2000), Enhanced Mutual Information (Zhang et al., 2009), and Multi-word Expression Distance (Bu et al., 2010). two key issues. We then present the experiments in Section 4. Finally, the work is summarized in Section 5. New word English Translation Polarity QIt lovely positive 4A- tragic/tragedy negative *JJ very cool; powerful positive k)-t* reverse one&apos;s expectation negative Table 1: Examples of new sentiment word. Our central idea for new sentiment word detection is as follows: Starting from very few seed words (for example, just one seed word), we can extract lexical patterns that have strong statistical association with the seed words; the extracted lexical patterns can be further used in finding</context>
<context position="9442" citStr="Bu et al. (2010)" startWordPosition="1497" endWordPosition="1500">al Probability (SCP) (da Silva and Lopes, 1999). Among all the 84 bi-gram association measures, PMI has been reported to be the best one in Czech data (Pecina, 2005). In order to measure arbitrary ngrams, most common strategies are to separate ngram into two parts X and Y so that existing bigram methods can be used (da Silva and Lopes, 1999; Dias et al., 2000; Schone and Jurafsky, 2001). Zhang et al. (2009) proposed Enhanced Mutual Information (EMI) which measures the cohesion of n-gram by the frequency of itself and the frequency of each single word. Based on the information distance theory, Bu et al. (2010) proposed multi-word expression distance (MED) and the normalized version, and reported superior performance to EMI, SCP, and other measures. 3 Methodology 3.1 Definitions Definition 3.1 (Adverbial word). Words that are used mainly to modify a verb or an adjective, such as &amp;quot;太(too)&amp;quot;, &amp;quot;非常(very)&amp;quot;, &amp;quot;十分(very)&amp;quot;, and &amp;quot;特 别(specially)&amp;quot;. Definition 3.2 (Auxiliary word). Words that are auxiliaries, model particles, or punctuation marks. In Chinese, such words are like &amp;quot;着,了,啦,的,啊&amp;quot;, and punctuation marks include &amp;quot;,o!?;:&amp;quot; and so on. Definition 3.3 (Lexical Pattern). A lexical pattern is a triplet &lt; AD, *, A</context>
<context position="18652" citStr="Bu et al., 2010" startWordPosition="3175" endWordPosition="3178">t measure is enhanced mutual information (EMI) (Zhang et al., 2009): F EMI (w) = log2 n / N (6) �i=1 N where F is the number of posts in which a multiword expression w = w1w2 ... wn occurs, Fi is the number of posts where wi occurs, and N is the total number of posts. The key idea of EMI is to measure word pair’s dependency as the ratio of its probability of being a multi-word to its probability of not being a multi-word. The larger the value, the more possible the expression will be a multi-word expression. The second measure we take into account is normalized multi-word expression distance (Bu et al., 2010), which has been proposed to measure the non-compositionality of multi-word expressions. log|µ(w) |− log|ϕ(w)| NMED(w) = logN − (7) log |ϕ(w) | where µ(w) is the set of documents in which all single words in w = w1w2 ... wn co-occur, ϕ(w) is the set of documents in which word w occurs as a whole, and N is the total number of documents. Different from EMI, this measure is a strict distance metric, meaning that a smaller value indicates a larger possibility of being a multi-word expression. As can be seen from the formula, the key idea of this metric is to compute the ratio of the co-occurrence </context>
<context position="22146" citStr="Bu et al., 2010" startWordPosition="3787" endWordPosition="3790">, the association model (LRT) remarkably boosts the 3All the resources are available upon request. performance of new word detection, indicating LRT is a key factor for new sentiment word extraction. From linguistic perspectives, new sentiment words are commonly modified by adverbial words and thus should have close association with lexical patterns. Second, we compare different settings of our method to two baselines. The first one is enhanced mutual information (EMI) where we set F(w) = EMI(w) (Zhang et al., 2009) and the second baseline is normalized multi-word expression distance (LAMED) (Bu et al., 2010) where we set F(w) = NMED(w). The results are shown in Figure 1. As can be seen, all the proposed measures outperform the two baselines (EMI and NMED) remarkably and consistently. The setting of FLAMED produces the best performance. Adding NMED or EMI leads to remarkable improvements because of their capability of measuring non-compositionality of new words. Only using LRT can obtain a fairly good results when K is small, however, the performance drops sharply because it&apos;s unable to measure non-compositionality. Comparison between LRT + LPE (or LRT + LPE + NWP) and LRT shows that inclusion of </context>
</contexts>
<marker>Bu, Zhu, Li, 2010</marker>
<rawString>Fan Bu, Xiaoyan Zhu, and Ming Li. 2010. Measuring the non-compositionality of multiword expressions. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING&apos;10, pages 116--124, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Wei-Yun Ma</author>
</authors>
<title>Unknown word extraction for chinese documents.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7429" citStr="Chen and Ma (2002)" startWordPosition="1162" endWordPosition="1165">ent descent based on feature frequency information (Sun et al., 2012). Another line is to treat new word detection as a separate task, usually preceded by part-of-speech tagging. The first genre of such studies is to leverage complex linguistic rules or knowledge. For example, Justeson and Katz (1995) extracted technical terminologies from documents using a regular expression. Argamon et al. (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. Chen and Ma (2002) employed morphological and statistical rules to extract Chinese new word. The second genre of the studies is to treat new word detection as a classification problem. Zhou (2005) proposed a discriminative Markov Model to detect new words by chunking one or more separated words. In (Li et al., 2005), new word detection was viewed as a binary classification problem. However, these supervised models requires not only heavy engineering of linguistic features, but also expensive annotation of training data. User behavior data has recently been explored for finding new words. Zheng et al. (2009) exp</context>
</contexts>
<marker>Chen, Ma, 2002</marker>
<rawString>Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word extraction for chinese documents. In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING&apos;02, pages 1--7, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitao Chen</author>
</authors>
<title>Chinese word segmentation using minimal linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing - Volume 17, SIGHAN&apos;03,</booktitle>
<pages>148--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1950" citStr="Chen, 2003" startWordPosition="287" endWordPosition="288">ser-generated content. Users like to update and share their information on social websites with their own language styles, among which new political/social/cultural words are constantly used. However, such new words have made many natural language processing tasks more challenging. Automatic extraction of new words is indispensable to many tasks such as Chinese word segmentation, machine translation, named entity extraction, question answering, and sentiment analysis. New word detection is one of the most critical issues in Chinese word segmentation. Recent studies (Sproat and Emerson, 2003) (Chen, 2003) have shown that more than 60% of word segmentation errors result from new words. Statistics show that more than 1000 new Chinese words appear every year (Thesaurus Research Center, 2003). These words are mostly domain-specific technical terms and time-sensitive political/social /cultural terms. Most of them are not yet correctly recognized by the segmentation algorithm, and remain as out of vocabulary (OOV) words. New word detection is also important for sentiment analysis such as opinionated phrase extraction and polarity classification. A sentiment phrase with complete meaning should have a</context>
</contexts>
<marker>Chen, 2003</marker>
<rawString>Aitao Chen. 2003. Chinese word segmentation using minimal linguistic knowledge. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing - Volume 17, SIGHAN&apos;03, pages 148--151, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaacov Choueka</author>
</authors>
<title>Looking for needles in a haystack or locating interesting collocation expressions in large textual databases.</title>
<date>1988</date>
<booktitle>In Proceeding of the RIAO&apos;88 Conference on User-Oriented Content-Based Text and Image Handling,</booktitle>
<pages>21--24</pages>
<contexts>
<context position="3657" citStr="Choueka, 1988" startWordPosition="557" endWordPosition="558">words as feature in classification models will improve the performance of polarity classification, as demonstrated later in this paper. This paper aims to detect new word for sentiment analysis. We are particulary interested in extracting new sentiment word that can express opinions or sentiment, which is of high value towards sentiment analysis. New sentiment word, as exemplified in Table 1, is a sub-class of multi-word expressions which is a sequence of neighboring words &amp;quot;whose exact and unambiguous meaning or connotation cannot be derived from the meaning or connotation of its components&amp;quot; (Choueka, 1988). Such new words cannot be directly identified using grammatical rules, which poses a major challenge to automatic analysis. Moreover, existing lexical resources never have adequate and timely coverage since new words appear constantly. People thus resort to statistical methods such as Pointwise Mutual Information (Church and Hanks, 1990), Symmetrical Conditional Probability 531 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 531–541, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics (da Silva and Lopes, 1</context>
</contexts>
<marker>Choueka, 1988</marker>
<rawString>Yaacov Choueka. 1988. Looking for needles in a haystack or locating interesting collocation expressions in large textual databases. In Proceeding of the RIAO&apos;88 Conference on User-Oriented Content-Based Text and Image Handling, pages 21--24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>22--29</pages>
<contexts>
<context position="3997" citStr="Church and Hanks, 1990" startWordPosition="607" endWordPosition="611">ntiment analysis. New sentiment word, as exemplified in Table 1, is a sub-class of multi-word expressions which is a sequence of neighboring words &amp;quot;whose exact and unambiguous meaning or connotation cannot be derived from the meaning or connotation of its components&amp;quot; (Choueka, 1988). Such new words cannot be directly identified using grammatical rules, which poses a major challenge to automatic analysis. Moreover, existing lexical resources never have adequate and timely coverage since new words appear constantly. People thus resort to statistical methods such as Pointwise Mutual Information (Church and Hanks, 1990), Symmetrical Conditional Probability 531 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 531–541, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics (da Silva and Lopes, 1999), Mutual Expectation (Dias et al., 2000), Enhanced Mutual Information (Zhang et al., 2009), and Multi-word Expression Distance (Bu et al., 2010). two key issues. We then present the experiments in Section 4. Finally, the work is summarized in Section 5. New word English Translation Polarity QIt lovely positive 4A- tragic/tragedy negat</context>
<context position="8664" citStr="Church and Hanks, 1990" startWordPosition="1362" endWordPosition="1365">yping behaviors in Sogou Chinese Pinyin input method to detect new words. Zhang et al. (2010) proposed to use dynamic time warping to detect new words from query logs. However, both of the work are limited due to the public unavailability of expensive commercial resources. Statistical methods for new word detection have been extensively studied, and in some sense exhibit advantages over linguistics-based methods. In this setting, new word detection is mostly 532 known as multi-word expression extraction. To measure multi-word association, the first model is Pointwise Mutual Information (PMI) (Church and Hanks, 1990). Since then, a variety of statistical methods have been proposed to measure bi-gram association, such as Log-likelihood (Dunning, 1993) and Symmetrical Conditional Probability (SCP) (da Silva and Lopes, 1999). Among all the 84 bi-gram association measures, PMI has been reported to be the best one in Czech data (Pecina, 2005). In order to measure arbitrary ngrams, most common strategies are to separate ngram into two parts X and Y so that existing bigram methods can be used (da Silva and Lopes, 1999; Dias et al., 2000; Schone and Jurafsky, 2001). Zhang et al. (2009) proposed Enhanced Mutual In</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16(1): 22--29, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ferreira da Silva</author>
<author>G Pereira Lopes</author>
</authors>
<title>A local maxima method and a fair dispersion normalization for extracting multi-word units from corpora.</title>
<date>1999</date>
<booktitle>In Sixth Meeting on Mathematics of Language,</booktitle>
<pages>369--381</pages>
<contexts>
<context position="4261" citStr="Silva and Lopes, 1999" startWordPosition="642" endWordPosition="645">s&amp;quot; (Choueka, 1988). Such new words cannot be directly identified using grammatical rules, which poses a major challenge to automatic analysis. Moreover, existing lexical resources never have adequate and timely coverage since new words appear constantly. People thus resort to statistical methods such as Pointwise Mutual Information (Church and Hanks, 1990), Symmetrical Conditional Probability 531 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 531–541, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics (da Silva and Lopes, 1999), Mutual Expectation (Dias et al., 2000), Enhanced Mutual Information (Zhang et al., 2009), and Multi-word Expression Distance (Bu et al., 2010). two key issues. We then present the experiments in Section 4. Finally, the work is summarized in Section 5. New word English Translation Polarity QIt lovely positive 4A- tragic/tragedy negative *JJ very cool; powerful positive k)-t* reverse one&apos;s expectation negative Table 1: Examples of new sentiment word. Our central idea for new sentiment word detection is as follows: Starting from very few seed words (for example, just one seed word), we can extr</context>
<context position="8873" citStr="Silva and Lopes, 1999" startWordPosition="1394" endWordPosition="1397">to the public unavailability of expensive commercial resources. Statistical methods for new word detection have been extensively studied, and in some sense exhibit advantages over linguistics-based methods. In this setting, new word detection is mostly 532 known as multi-word expression extraction. To measure multi-word association, the first model is Pointwise Mutual Information (PMI) (Church and Hanks, 1990). Since then, a variety of statistical methods have been proposed to measure bi-gram association, such as Log-likelihood (Dunning, 1993) and Symmetrical Conditional Probability (SCP) (da Silva and Lopes, 1999). Among all the 84 bi-gram association measures, PMI has been reported to be the best one in Czech data (Pecina, 2005). In order to measure arbitrary ngrams, most common strategies are to separate ngram into two parts X and Y so that existing bigram methods can be used (da Silva and Lopes, 1999; Dias et al., 2000; Schone and Jurafsky, 2001). Zhang et al. (2009) proposed Enhanced Mutual Information (EMI) which measures the cohesion of n-gram by the frequency of itself and the frequency of each single word. Based on the information distance theory, Bu et al. (2010) proposed multi-word expression</context>
</contexts>
<marker>Silva, Lopes, 1999</marker>
<rawString>J Ferreira da Silva and G Pereira Lopes. 1999. A local maxima method and a fair dispersion normalization for extracting multi-word units from corpora. In Sixth Meeting on Mathematics of Language, pages 369--381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gaël Dias</author>
</authors>
<title>Sylvie Guilloré, and José Gabriel Pereira Lopes.</title>
<date>2000</date>
<booktitle>6th ACM SIGKDD Work. Text Mining.</booktitle>
<marker>Dias, 2000</marker>
<rawString>Gaël Dias, Sylvie Guilloré, and José Gabriel Pereira Lopes. 2000. Mining textual associations in text corpora. 6th ACM SIGKDD Work. Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="8800" citStr="Dunning, 1993" startWordPosition="1384" endWordPosition="1386">words from query logs. However, both of the work are limited due to the public unavailability of expensive commercial resources. Statistical methods for new word detection have been extensively studied, and in some sense exhibit advantages over linguistics-based methods. In this setting, new word detection is mostly 532 known as multi-word expression extraction. To measure multi-word association, the first model is Pointwise Mutual Information (PMI) (Church and Hanks, 1990). Since then, a variety of statistical methods have been proposed to measure bi-gram association, such as Log-likelihood (Dunning, 1993) and Symmetrical Conditional Probability (SCP) (da Silva and Lopes, 1999). Among all the 84 bi-gram association measures, PMI has been reported to be the best one in Czech data (Pecina, 2005). In order to measure arbitrary ngrams, most common strategies are to separate ngram into two parts X and Y so that existing bigram methods can be used (da Silva and Lopes, 1999; Dias et al., 2000; Schone and Jurafsky, 2001). Zhang et al. (2009) proposed Enhanced Mutual Information (EMI) which measures the cohesion of n-gram by the frequency of itself and the frequency of each single word. Based on the inf</context>
<context position="12598" citStr="Dunning, 1993" startWordPosition="2025" endWordPosition="2026"> however, the differences are apparent in that: firstly, we use very lightweight linguistic information (except POS tags); secondly, our major contributions are to propose statistical measures to address the following key issues: first, to measure the utility of lexical patterns; second, to measure the possibility of a candidate word being a new word. 3.3 Measuring the Utility of a Pattern The first key issue is to quantify the utility of a pattern at each iteration. This can be measured by the association of a pattern to the current word set used in the algorithm. The likelihood ratio tests (Dunning, 1993) is used for this purpose. This association model has also been used to model association between opinion target words by (Hai et 533 Algorithm 1: New word detection algorithm Input: D: a large set of POS tagged posts Ws: a set of seed words kp: the number of patterns chosen at each iteration kc: the number of patterns in the candidate pattern set kw: the number of words added at each iteration K: the number of words returned Output: A list of ranked new words W 1 Obtain all lexical patterns using regular expressions on D; 2 Count the frequency of each lexical pattern and extract words matched</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Comput. Linguist., 19(1):61--74, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhen Hai</author>
<author>Kuiyu Chang</author>
<author>Gao Cong</author>
</authors>
<title>One seed to find them all: Mining opinion features via association.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12,</booktitle>
<pages>255--264</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Hai, Chang, Cong, 2012</marker>
<rawString>Zhen Hai, Kuiyu Chang, and Gao Cong. 2012. One seed to find them all: Mining opinion features via association. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12, pages 255--264, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Justeson</author>
<author>Slava M Katz</author>
</authors>
<title>Technical terminology: some linguistic properties and an algorithm for identification in text. Natural language engineering,</title>
<date>1995</date>
<pages>1--1</pages>
<contexts>
<context position="7113" citStr="Justeson and Katz (1995)" startWordPosition="1111" endWordPosition="1114">egmentation, where new words are identified as the most probable segments inferred by the probabilistic models; and the detected new word can be further used to improve word segmentation. Typical models include conditional random fields proposed by (Peng et al., 2004), and a joint model trained with adaptive online gradient descent based on feature frequency information (Sun et al., 2012). Another line is to treat new word detection as a separate task, usually preceded by part-of-speech tagging. The first genre of such studies is to leverage complex linguistic rules or knowledge. For example, Justeson and Katz (1995) extracted technical terminologies from documents using a regular expression. Argamon et al. (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. Chen and Ma (2002) employed morphological and statistical rules to extract Chinese new word. The second genre of the studies is to treat new word detection as a classification problem. Zhou (2005) proposed a discriminative Markov Model to detect new words by chunking one or more separated words. In (L</context>
</contexts>
<marker>Justeson, Katz, 1995</marker>
<rawString>John S Justeson and Slava M Katz. 1995. Technical terminology: some linguistic properties and an algorithm for identification in text. Natural language engineering, 1(1):9--27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongqiao Li</author>
<author>Chang-Ning Huang</author>
<author>Jianfeng Gao</author>
<author>Xiaozhong Fan</author>
</authors>
<title>The use of svm for chinese new word identification.</title>
<date>2005</date>
<booktitle>In Natural Language Processing--IJCNLP</booktitle>
<pages>723--732</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7728" citStr="Li et al., 2005" startWordPosition="1216" endWordPosition="1219">) extracted technical terminologies from documents using a regular expression. Argamon et al. (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. Chen and Ma (2002) employed morphological and statistical rules to extract Chinese new word. The second genre of the studies is to treat new word detection as a classification problem. Zhou (2005) proposed a discriminative Markov Model to detect new words by chunking one or more separated words. In (Li et al., 2005), new word detection was viewed as a binary classification problem. However, these supervised models requires not only heavy engineering of linguistic features, but also expensive annotation of training data. User behavior data has recently been explored for finding new words. Zheng et al. (2009) explored user typing behaviors in Sogou Chinese Pinyin input method to detect new words. Zhang et al. (2010) proposed to use dynamic time warping to detect new words from query logs. However, both of the work are limited due to the public unavailability of expensive commercial resources. Statistical m</context>
</contexts>
<marker>Li, Huang, Gao, Fan, 2005</marker>
<rawString>Hongqiao Li, Chang-Ning Huang, Jianfeng Gao, and Xiaozhong Fan. 2005. The use of svm for chinese new word identification. In Natural Language Processing--IJCNLP 2004, pages 723--732. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
</authors>
<title>An extensive empirical study of collocation extraction methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop, ACLstudent &apos;05,</booktitle>
<pages>13--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8991" citStr="Pecina, 2005" startWordPosition="1417" endWordPosition="1418">y studied, and in some sense exhibit advantages over linguistics-based methods. In this setting, new word detection is mostly 532 known as multi-word expression extraction. To measure multi-word association, the first model is Pointwise Mutual Information (PMI) (Church and Hanks, 1990). Since then, a variety of statistical methods have been proposed to measure bi-gram association, such as Log-likelihood (Dunning, 1993) and Symmetrical Conditional Probability (SCP) (da Silva and Lopes, 1999). Among all the 84 bi-gram association measures, PMI has been reported to be the best one in Czech data (Pecina, 2005). In order to measure arbitrary ngrams, most common strategies are to separate ngram into two parts X and Y so that existing bigram methods can be used (da Silva and Lopes, 1999; Dias et al., 2000; Schone and Jurafsky, 2001). Zhang et al. (2009) proposed Enhanced Mutual Information (EMI) which measures the cohesion of n-gram by the frequency of itself and the frequency of each single word. Based on the information distance theory, Bu et al. (2010) proposed multi-word expression distance (MED) and the normalized version, and reported superior performance to EMI, SCP, and other measures. 3 Metho</context>
</contexts>
<marker>Pecina, 2005</marker>
<rawString>Pavel Pecina. 2005. An extensive empirical study of collocation extraction methods. In Proceedings of the ACL Student Research Workshop, ACLstudent &apos;05, pages 13--18, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics, COLING &apos;04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6757" citStr="Peng et al., 2004" startWordPosition="1052" endWordPosition="1055">ection. We will describe the proposed method in Section 3, including definitions, the overview of the algorithm, and the statistical measures for addressing the 2 Related Work New word detection has been usually interweaved with word segmentation, particularly in Chinese NLP. In these works, new word detection is considered as an integral part of segmentation, where new words are identified as the most probable segments inferred by the probabilistic models; and the detected new word can be further used to improve word segmentation. Typical models include conditional random fields proposed by (Peng et al., 2004), and a joint model trained with adaptive online gradient descent based on feature frequency information (Sun et al., 2012). Another line is to treat new word detection as a separate task, usually preceded by part-of-speech tagging. The first genre of such studies is to leverage complex linguistic rules or knowledge. For example, Justeson and Katz (1995) extracted technical terminologies from documents using a regular expression. Argamon et al. (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set res</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of the 20th International Conference on Computational Linguistics, COLING &apos;04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Opinion word expansion and target extraction through double propagation.</title>
<date>2011</date>
<journal>Computational linguistics,</journal>
<pages>37--1</pages>
<contexts>
<context position="11983" citStr="Qiu et al., 2011" startWordPosition="1919" endWordPosition="1922">ndidate word being a new word, and the topranked words will be added into the seed word set for the next iteration. The process can be run iteratively until a stop condition is met. Note that we do not augment the pattern set (P) at each iteration, instead, we keep a fixed small number of patterns during iteration because this strategy produces optimal results. From linguistic perspectives, new sentiment words are commonly modified by adverbial words and thus can be extracted by lexical patterns. This is the reason why the algorithm will work. Our algorithm is in spirit to double propagation (Qiu et al., 2011), however, the differences are apparent in that: firstly, we use very lightweight linguistic information (except POS tags); secondly, our major contributions are to propose statistical measures to address the following key issues: first, to measure the utility of lexical patterns; second, to measure the possibility of a candidate word being a new word. 3.3 Measuring the Utility of a Pattern The first key issue is to quantify the utility of a pattern at each iteration. This can be measured by the association of a pattern to the current word set used in the algorithm. The likelihood ratio tests </context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2011</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through double propagation. Computational linguistics, 37(1):9--27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Is knowledge-free induction of multiword unit dictionary headwords a solved problem.</title>
<date>2001</date>
<booktitle>In Proc. of the 6th Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>100--108</pages>
<contexts>
<context position="9215" citStr="Schone and Jurafsky, 2001" startWordPosition="1458" endWordPosition="1461"> first model is Pointwise Mutual Information (PMI) (Church and Hanks, 1990). Since then, a variety of statistical methods have been proposed to measure bi-gram association, such as Log-likelihood (Dunning, 1993) and Symmetrical Conditional Probability (SCP) (da Silva and Lopes, 1999). Among all the 84 bi-gram association measures, PMI has been reported to be the best one in Czech data (Pecina, 2005). In order to measure arbitrary ngrams, most common strategies are to separate ngram into two parts X and Y so that existing bigram methods can be used (da Silva and Lopes, 1999; Dias et al., 2000; Schone and Jurafsky, 2001). Zhang et al. (2009) proposed Enhanced Mutual Information (EMI) which measures the cohesion of n-gram by the frequency of itself and the frequency of each single word. Based on the information distance theory, Bu et al. (2010) proposed multi-word expression distance (MED) and the normalized version, and reported superior performance to EMI, SCP, and other measures. 3 Methodology 3.1 Definitions Definition 3.1 (Adverbial word). Words that are used mainly to modify a verb or an adjective, such as &amp;quot;太(too)&amp;quot;, &amp;quot;非常(very)&amp;quot;, &amp;quot;十分(very)&amp;quot;, and &amp;quot;特 别(specially)&amp;quot;. Definition 3.2 (Auxiliary word). Words that</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2001. Is knowledge-free induction of multiword unit dictionary headwords a solved problem. In Proc. of the 6th Conference on Empirical Methods in Natural Language Processing (EMNLP 2001), pages 100--108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The first international chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing - Volume 17, SIGHAN &apos;03,</booktitle>
<pages>133--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1937" citStr="Sproat and Emerson, 2003" startWordPosition="283" endWordPosition="286">the time, particularly in user-generated content. Users like to update and share their information on social websites with their own language styles, among which new political/social/cultural words are constantly used. However, such new words have made many natural language processing tasks more challenging. Automatic extraction of new words is indispensable to many tasks such as Chinese word segmentation, machine translation, named entity extraction, question answering, and sentiment analysis. New word detection is one of the most critical issues in Chinese word segmentation. Recent studies (Sproat and Emerson, 2003) (Chen, 2003) have shown that more than 60% of word segmentation errors result from new words. Statistics show that more than 1000 new Chinese words appear every year (Thesaurus Research Center, 2003). These words are mostly domain-specific technical terms and time-sensitive political/social /cultural terms. Most of them are not yet correctly recognized by the segmentation algorithm, and remain as out of vocabulary (OOV) words. New word detection is also important for sentiment analysis such as opinionated phrase extraction and polarity classification. A sentiment phrase with complete meaning </context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The first international chinese word segmentation bakeoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing - Volume 17, SIGHAN &apos;03, pages 133--143, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Houfeng Wang</author>
<author>Wenjie Li</author>
</authors>
<title>Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers -Volume 1, ACL &apos;12,</booktitle>
<pages>253--262</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6880" citStr="Sun et al., 2012" startWordPosition="1073" endWordPosition="1076">tistical measures for addressing the 2 Related Work New word detection has been usually interweaved with word segmentation, particularly in Chinese NLP. In these works, new word detection is considered as an integral part of segmentation, where new words are identified as the most probable segments inferred by the probabilistic models; and the detected new word can be further used to improve word segmentation. Typical models include conditional random fields proposed by (Peng et al., 2004), and a joint model trained with adaptive online gradient descent based on feature frequency information (Sun et al., 2012). Another line is to treat new word detection as a separate task, usually preceded by part-of-speech tagging. The first genre of such studies is to leverage complex linguistic rules or knowledge. For example, Justeson and Katz (1995) extracted technical terminologies from documents using a regular expression. Argamon et al. (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. Chen and Ma (2002) employed morphological and statistical rules to ex</context>
</contexts>
<marker>Sun, Wang, Li, 2012</marker>
<rawString>Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers -Volume 1, ACL &apos;12, pages 253--262, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beijing Thesaurus Research Center</author>
</authors>
<title>Xinhua Xin Ciyu Cidian.</title>
<date>2003</date>
<publisher>Commercial Press,</publisher>
<location>Beijing.</location>
<contexts>
<context position="2137" citStr="Center, 2003" startWordPosition="317" endWordPosition="318"> used. However, such new words have made many natural language processing tasks more challenging. Automatic extraction of new words is indispensable to many tasks such as Chinese word segmentation, machine translation, named entity extraction, question answering, and sentiment analysis. New word detection is one of the most critical issues in Chinese word segmentation. Recent studies (Sproat and Emerson, 2003) (Chen, 2003) have shown that more than 60% of word segmentation errors result from new words. Statistics show that more than 1000 new Chinese words appear every year (Thesaurus Research Center, 2003). These words are mostly domain-specific technical terms and time-sensitive political/social /cultural terms. Most of them are not yet correctly recognized by the segmentation algorithm, and remain as out of vocabulary (OOV) words. New word detection is also important for sentiment analysis such as opinionated phrase extraction and polarity classification. A sentiment phrase with complete meaning should have a correct boundary, however, characters in a new word may be broken up. For example, in a sentence &amp;quot; * O/ n 4� S/ adv */ v JJ/ n(artists&apos; performance is very impressive)&amp;quot; the two Chinese c</context>
</contexts>
<marker>Center, 2003</marker>
<rawString>Beijing Thesaurus Research Center. 2003. Xinhua Xin Ciyu Cidian. Commercial Press, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="26090" citStr="Turney and Littman, 2003" startWordPosition="4480" endWordPosition="4483"> in computing left pattern entropy, see Formula (4). Too small size of Pc may lead to insufficient estimation of left pattern entropy. Results in Table 7 shows that larger Pc decrease the performance, particularly when the number of words returned (K) becomes larger. Therefore, we set |Pc |= 100. 4.5 Polarity Prediction of New Sentiment Words In this section, we attempt to classifying the polarity of the annotated 323 new words. Two methods are adapted with different settings for this purpose. The first one is majority vote (MV), and the second one is pointwise mutual information, similar to (Turney and Littman, 2003). The majority vote method is formulated as below: where PW and NW are a positive and negative set of emoticons (or seed words) respectively, and #(w, wp) is the co-occurrence count of the input word w and the item wp. The polarity is judged according to this rule: if MV (w) &gt; th1, the word w is positive; if MV (w) &lt; −th1 the word negative; otherwise neutral. The threshold th1 is manually tuned. And PMI is computed as follows: where PMI(x, y) = log2( P r(x,y) P r(x)�P r(y)), and Pr(·) denotes probability. The polarity is judged according to the rule: if PMI(w) &gt; th2, w is positive; if PMI(w) &lt;</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Trans. Inf. Syst., 21(4):315--346, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua-Ping Zhang</author>
<author>Hong-Kui Yu</author>
<author>De-Yi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>Hhmm-based chinese lexical analyzer ictclas.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing -Volume 17, SIGHAN &apos;03,</booktitle>
<pages>184--187</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17743" citStr="Zhang et al., 2003" startWordPosition="3016" endWordPosition="3019">mpositionality. In such words, each single character has high probability to be a word. Thus, we design the following measure to favor this observation. p(wi) (5) 1 − p(wi) where w = w1w2 ... wn, each wi is a single character, and p(wi) is the probability of the character wi being a word, as computed as follows: all(wi) − s(wi) p(wi) = all(wi) where all(wi) is the total frequency of wi, and s(wi) is the frequency of wi being a single character word. Obviously, in order to obtain the value of s(wi), some particular Chinese word segmentation tool is required. In this work, we resort to ICTCLAS (Zhang et al., 2003), a widely used tool in the literature. 3.4.4 Non-compositionality Measures New words are usually multi-word expressions, where a variety of statistical measures have been proposed to detect multi-word expressions. Thus, such measures can be naturally incorporated into our algorithm. The first measure is enhanced mutual information (EMI) (Zhang et al., 2009): F EMI (w) = log2 n / N (6) �i=1 N where F is the number of posts in which a multiword expression w = w1w2 ... wn occurs, Fi is the number of posts where wi occurs, and N is the total number of posts. The key idea of EMI is to measure word</context>
<context position="20328" citStr="Zhang et al., 2003" startWordPosition="3462" endWordPosition="3465">n, we will conduct the following experiments: first, we will compare our method to several baselines, and perform parameter tuning with extensive experiments; second, we will classify polarity of new sentiment words using two methods; third, we will demonstrate how new sentiment words will benefit sentiment classification. 4.1 Data Preparation We crawled 237,108,977 Weibo posts from http://www.weibo.com, the largest social website in China. These posts range from January of 2011 to December of 2012. The posts were then part-ofspeech tagged using a Chinese word segmentation tool named ICTCLAS (Zhang et al., 2003). Then, we asked two annotators to label the top 5,000 frequent words that were extracted by lexical patterns as described in Algorithm 1. The annotators were requested to judge whether a candidate word is a new word, and also to judge the polarity of a new word (positive, negative, and neutral). If there is a disagreement on either of the two tasks, discussions are required to make the final decision. The annotation led to 323 new words, among which there are 116 positive words, 112 negative words, and 95 neutral words3. 4.2 Evaluation Metric As our algorithm outputs a ranked list of words, w</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun Liu. 2003. Hhmm-based chinese lexical analyzer ictclas. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing -Volume 17, SIGHAN &apos;03, pages 184--187, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Zhang</author>
<author>Taketoshi Yoshida</author>
<author>Xijin Tang</author>
<author>TuBao Ho</author>
</authors>
<title>Improving effectiveness of mutual information for substantival multiword expression extraction. Expert Systems with Applications,</title>
<date>2009</date>
<pages>36--8</pages>
<contexts>
<context position="4351" citStr="Zhang et al., 2009" startWordPosition="655" endWordPosition="658">ch poses a major challenge to automatic analysis. Moreover, existing lexical resources never have adequate and timely coverage since new words appear constantly. People thus resort to statistical methods such as Pointwise Mutual Information (Church and Hanks, 1990), Symmetrical Conditional Probability 531 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 531–541, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics (da Silva and Lopes, 1999), Mutual Expectation (Dias et al., 2000), Enhanced Mutual Information (Zhang et al., 2009), and Multi-word Expression Distance (Bu et al., 2010). two key issues. We then present the experiments in Section 4. Finally, the work is summarized in Section 5. New word English Translation Polarity QIt lovely positive 4A- tragic/tragedy negative *JJ very cool; powerful positive k)-t* reverse one&apos;s expectation negative Table 1: Examples of new sentiment word. Our central idea for new sentiment word detection is as follows: Starting from very few seed words (for example, just one seed word), we can extract lexical patterns that have strong statistical association with the seed words; the ext</context>
<context position="9236" citStr="Zhang et al. (2009)" startWordPosition="1462" endWordPosition="1465">tual Information (PMI) (Church and Hanks, 1990). Since then, a variety of statistical methods have been proposed to measure bi-gram association, such as Log-likelihood (Dunning, 1993) and Symmetrical Conditional Probability (SCP) (da Silva and Lopes, 1999). Among all the 84 bi-gram association measures, PMI has been reported to be the best one in Czech data (Pecina, 2005). In order to measure arbitrary ngrams, most common strategies are to separate ngram into two parts X and Y so that existing bigram methods can be used (da Silva and Lopes, 1999; Dias et al., 2000; Schone and Jurafsky, 2001). Zhang et al. (2009) proposed Enhanced Mutual Information (EMI) which measures the cohesion of n-gram by the frequency of itself and the frequency of each single word. Based on the information distance theory, Bu et al. (2010) proposed multi-word expression distance (MED) and the normalized version, and reported superior performance to EMI, SCP, and other measures. 3 Methodology 3.1 Definitions Definition 3.1 (Adverbial word). Words that are used mainly to modify a verb or an adjective, such as &amp;quot;太(too)&amp;quot;, &amp;quot;非常(very)&amp;quot;, &amp;quot;十分(very)&amp;quot;, and &amp;quot;特 别(specially)&amp;quot;. Definition 3.2 (Auxiliary word). Words that are auxiliaries, mod</context>
<context position="18103" citStr="Zhang et al., 2009" startWordPosition="3069" endWordPosition="3072">is the total frequency of wi, and s(wi) is the frequency of wi being a single character word. Obviously, in order to obtain the value of s(wi), some particular Chinese word segmentation tool is required. In this work, we resort to ICTCLAS (Zhang et al., 2003), a widely used tool in the literature. 3.4.4 Non-compositionality Measures New words are usually multi-word expressions, where a variety of statistical measures have been proposed to detect multi-word expressions. Thus, such measures can be naturally incorporated into our algorithm. The first measure is enhanced mutual information (EMI) (Zhang et al., 2009): F EMI (w) = log2 n / N (6) �i=1 N where F is the number of posts in which a multiword expression w = w1w2 ... wn occurs, Fi is the number of posts where wi occurs, and N is the total number of posts. The key idea of EMI is to measure word pair’s dependency as the ratio of its probability of being a multi-word to its probability of not being a multi-word. The larger the value, the more possible the expression will be a multi-word expression. The second measure we take into account is normalized multi-word expression distance (Bu et al., 2010), which has been proposed to measure the non-compos</context>
<context position="22051" citStr="Zhang et al., 2009" startWordPosition="3772" endWordPosition="3775">tio test, which measures the association of a word to the pattern set. As can be seen from Table 4, the association model (LRT) remarkably boosts the 3All the resources are available upon request. performance of new word detection, indicating LRT is a key factor for new sentiment word extraction. From linguistic perspectives, new sentiment words are commonly modified by adverbial words and thus should have close association with lexical patterns. Second, we compare different settings of our method to two baselines. The first one is enhanced mutual information (EMI) where we set F(w) = EMI(w) (Zhang et al., 2009) and the second baseline is normalized multi-word expression distance (LAMED) (Bu et al., 2010) where we set F(w) = NMED(w). The results are shown in Figure 1. As can be seen, all the proposed measures outperform the two baselines (EMI and NMED) remarkably and consistently. The setting of FLAMED produces the best performance. Adding NMED or EMI leads to remarkable improvements because of their capability of measuring non-compositionality of new words. Only using LRT can obtain a fairly good results when K is small, however, the performance drops sharply because it&apos;s unable to measure non-compo</context>
</contexts>
<marker>Zhang, Yoshida, Tang, Ho, 2009</marker>
<rawString>Wen Zhang, Taketoshi Yoshida, Xijin Tang, and TuBao Ho. 2009. Improving effectiveness of mutual information for substantival multiword expression extraction. Expert Systems with Applications, 36(8):10919--10930.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Zhang</author>
<author>Maosong Sun</author>
<author>Yang Zhang</author>
</authors>
<title>Chinese new word detection from query logs.</title>
<date>2010</date>
<booktitle>In Advanced Data Mining and Applications,</booktitle>
<pages>233--243</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="8134" citStr="Zhang et al. (2010)" startWordPosition="1281" endWordPosition="1284">genre of the studies is to treat new word detection as a classification problem. Zhou (2005) proposed a discriminative Markov Model to detect new words by chunking one or more separated words. In (Li et al., 2005), new word detection was viewed as a binary classification problem. However, these supervised models requires not only heavy engineering of linguistic features, but also expensive annotation of training data. User behavior data has recently been explored for finding new words. Zheng et al. (2009) explored user typing behaviors in Sogou Chinese Pinyin input method to detect new words. Zhang et al. (2010) proposed to use dynamic time warping to detect new words from query logs. However, both of the work are limited due to the public unavailability of expensive commercial resources. Statistical methods for new word detection have been extensively studied, and in some sense exhibit advantages over linguistics-based methods. In this setting, new word detection is mostly 532 known as multi-word expression extraction. To measure multi-word association, the first model is Pointwise Mutual Information (PMI) (Church and Hanks, 1990). Since then, a variety of statistical methods have been proposed to m</context>
</contexts>
<marker>Zhang, Sun, Zhang, 2010</marker>
<rawString>Yan Zhang, Maosong Sun, and Yang Zhang. 2010. Chinese new word detection from query logs. In Advanced Data Mining and Applications, pages 233--243. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yabin Zheng</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
<author>Liyun Ru</author>
<author>Yang Zhang</author>
</authors>
<title>Incorporating user behaviors in new word detection.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Jont Conference on Artifical Intelligence, IJCAI&apos;09,</booktitle>
<pages>2101--2106</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="8025" citStr="Zheng et al. (2009)" startWordPosition="1262" endWordPosition="1265">nts. Chen and Ma (2002) employed morphological and statistical rules to extract Chinese new word. The second genre of the studies is to treat new word detection as a classification problem. Zhou (2005) proposed a discriminative Markov Model to detect new words by chunking one or more separated words. In (Li et al., 2005), new word detection was viewed as a binary classification problem. However, these supervised models requires not only heavy engineering of linguistic features, but also expensive annotation of training data. User behavior data has recently been explored for finding new words. Zheng et al. (2009) explored user typing behaviors in Sogou Chinese Pinyin input method to detect new words. Zhang et al. (2010) proposed to use dynamic time warping to detect new words from query logs. However, both of the work are limited due to the public unavailability of expensive commercial resources. Statistical methods for new word detection have been extensively studied, and in some sense exhibit advantages over linguistics-based methods. In this setting, new word detection is mostly 532 known as multi-word expression extraction. To measure multi-word association, the first model is Pointwise Mutual Inf</context>
</contexts>
<marker>Zheng, Liu, Sun, Ru, Zhang, 2009</marker>
<rawString>Yabin Zheng, Zhiyuan Liu, Maosong Sun, Liyun Ru, and Yang Zhang. 2009. Incorporating user behaviors in new word detection. In Proceedings of the 21st International Jont Conference on Artifical Intelligence, IJCAI&apos;09, pages 2101--2106, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
</authors>
<title>A chunking strategy towards unknown word detection in chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Natural Language Processing--IJCNLP</booktitle>
<pages>530--541</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7607" citStr="Zhou (2005)" startWordPosition="1195" endWordPosition="1196">rst genre of such studies is to leverage complex linguistic rules or knowledge. For example, Justeson and Katz (1995) extracted technical terminologies from documents using a regular expression. Argamon et al. (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. Chen and Ma (2002) employed morphological and statistical rules to extract Chinese new word. The second genre of the studies is to treat new word detection as a classification problem. Zhou (2005) proposed a discriminative Markov Model to detect new words by chunking one or more separated words. In (Li et al., 2005), new word detection was viewed as a binary classification problem. However, these supervised models requires not only heavy engineering of linguistic features, but also expensive annotation of training data. User behavior data has recently been explored for finding new words. Zheng et al. (2009) explored user typing behaviors in Sogou Chinese Pinyin input method to detect new words. Zhang et al. (2010) proposed to use dynamic time warping to detect new words from query logs</context>
</contexts>
<marker>Zhou, 2005</marker>
<rawString>GuoDong Zhou. 2005. A chunking strategy towards unknown word detection in chinese word segmentation. In Natural Language Processing--IJCNLP 2005, pages 530--541. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>