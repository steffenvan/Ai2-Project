<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000645">
<title confidence="0.9994545">
Three-Dimensional Parametrization for Parsing
Morphologically Rich Languages
</title>
<author confidence="0.992584">
Reut Tsarfaty and Khalil Sima’an
</author>
<affiliation confidence="0.997739">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<address confidence="0.714587">
Plantage Muidergracht 24, 1018TV Amsterdam, The Netherlands
</address>
<email confidence="0.998884">
{rtsarfat,simaan}@science.uva.nl
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947666666667">
Current parameters of accurate unlexical-
ized parsers based on Probabilistic Context-
Free Grammars (PCFGs) form a two-
dimensional grid in which rewrite events
are conditioned on both horizontal (head-
outward) and vertical (parental) histories.
In Semitic languages, where arguments
may move around rather freely and phrase-
structures are often shallow, there are ad-
ditional morphological factors that govern
the generation process. Here we pro-
pose that agreement features percolated up
the parse-tree form a third dimension of
parametrization that is orthogonal to the pre-
vious two. This dimension differs from
mere “state-splits” as it applies to a whole
set of categories rather than to individual
ones and encodes linguistically motivated
co-occurrences between them. This paper
presents extensive experiments with exten-
sions of unlexicalized PCFGs for parsing
Modern Hebrew in which tuning the param-
eters in three dimensions gradually leads to
improved performance. Our best result in-
troduces a new, stronger, lower bound on the
performance of treebank grammars for pars-
ing Modern Hebrew, and is on a par with
current results for parsing Modern Standard
Arabic obtained by a fully lexicalized parser
trained on a much larger treebank.
</bodyText>
<sectionHeader confidence="0.877961" genericHeader="keywords">
1 Dimensions of Unlexicalized Parsing
</sectionHeader>
<bodyText confidence="0.9974475625">
Probabilistic Context Free Grammars (PCFGs) are
the formal backbone of most high-accuracy statisti-
cal parsers for English, and a variety of techniques
was developed to enhance their performance rela-
tive to the naive treebank implementation — from
unlexicalized extensions exploiting simple category
splits (Johnson, 1998; Klein and Manning, 2003)
to fully lexicalized parsers that condition events be-
low a constituent upon the head and additional lexi-
cal content (Collins, 2003; Charniak, 1997). While
it is clear that conditioning on lexical content im-
proves the grammar’s disambiguation capabilities,
Klein and Manning (2003) demonstrate that a well-
crafted unlexicalized PCFG can close the gap, to a
large extent, with current state-of-the-art lexicalized
parsers for English.
The factor that sets apart vanilla PCFGs (Char-
niak, 1996) from their unlexicalized extensions pro-
posed by, e.g., (Johnson, 1998; Klein and Manning,
2003), is the choice for statistical parametrization
that weakens the independence assumptions implicit
in the treebank grammar. Studies on accurate unlex-
icalized parsing models outline two dimensions of
parametrization. The first, proposed by (Johnson,
1998), is the annotation of parental history, and the
second encodes a head-outward generation process
(Collins, 2003). Johnson (1998) augments node la-
bels with the label of their parent, thus incorporat-
ing a dependency on the node’s grandparent. Collins
(2003) proposes to generate the head of a phrase first
and then generate its sisters using Markovian pro-
cesses, thereby exploiting head/sister-dependencies.
</bodyText>
<page confidence="0.981071">
156
</page>
<note confidence="0.923723">
Proceedings of the 10th Conference on Parsing Technologies, pages 156–167,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999572533333333">
Klein and Manning (2003) systematize the dis-
tinction between these two forms of parametrization
by drawing them on a horizontal-vertical grid: par-
ent encoding is vertical (external to the rule) whereas
head-outward generation is horizontal (internal to
the rule). By varying the value of the parame-
ters along the grid, Klein and Manning (2003) tune
their treebank grammar to achieve improved perfor-
mance. This two-dimensional parametrization has
been instrumental in devising parsing models that
improve disambiguation capabilities for English as
well as other languages, such as German (Dubey and
Keller, 2003) Czech (Collins et al., 1999) and Chi-
nese (Bikel and Chiang, 2000). However, accuracy
results for parsing languages other than English still
lag behind.1
We propose that for various languages includ-
ing the Semitic family, e.g. Modern Hebrew (MH)
and Modern Standard Arabic (MSA), a third di-
mension of parametrization is necessary for encod-
ing linguistic information relevant for breaking false
independence assumptions. In Semitic languages,
arguments may move around rather freely and the
phrase-structure of clause-level categories is often
shallow. For such languages agreement features play
a role in disambiguation at least as important as the
vertical and horizontal conditioning. We propose a
third dimension of parameterizations that encodes
morphological features such as those realizing syn-
tactic agreement. These features are percolated from
surface forms in a bottom-up fashion and express
information that is complementary to the horizon-
tal and vertical generation histories proposed before.
Such morphological information refines syntactic
categories based on their morpho-syntactic role, and
captures linguistically motivated co-occurrences and
dependencies manifested via, e.g., morpho-syntactic
agreement.
This work aims at parsing MH and explores the
empirical contribution of the three dimensions of
parameters specified above. We present extensive
experiments that gradually lead to improved perfor-
mance as we extend the degree to which the three
dimensions are exploited. Our best model uses all
three dimensions of parametrization, and our best re-
</bodyText>
<footnote confidence="0.970115666666667">
1The learning curves over increasing training data (e.g., for
German (Dubey and Keller, 2003)) show that treebank size can-
not be the sole factor to account for the inferior performance.
</footnote>
<bodyText confidence="0.999861466666667">
sult is on a par with those achieved for MSA using a
fully lexicalized parser and a much larger treebank.
The remainder of this document is organized as fol-
lows. In section 2 we review characteristic aspects
of MH (and other Semitic languages) and illustrate
the special role of morphology and dependencies
displayed by morpho-syntactic processes using the
case of syntactic definiteness in MH. In section 3 we
define our three-dimensional parametrization space.
In section 4 we spell out the method and procedure
for the empirical evaluation of one, two and three
parametrization dimensions, and in section 5 we re-
port and analyze results for different parametrization
choices. Finally, section 6 discusses related work
and in section 7 we summarize and conclude.
</bodyText>
<sectionHeader confidence="0.845572" genericHeader="introduction">
2 Dimensions of Modern Hebrew Syntax
</sectionHeader>
<bodyText confidence="0.999847666666667">
Parsing MH is in its infancy. Although a syntacti-
cally annotated corpus has been available for quite
some time (Sima’an et al., 2001), we know of only
two studies attempting to parse MH using statistical
methods (see section 6). One reason for the sparse-
ness in this field is that the adaptation of existing
models to parsing MH is technically involved yet
does not guarantee to yield comparable results as
the processes that license grammatical structures of
phrases and sentences in MH differ from those as-
sumed for English. This section outlines differences
between English and MH and discusses their reflec-
tion in the MH treebank annotation scheme. We
argue that on top of syntactic processes exploited
by current parsers there is an orthogonal morpho-
syntactic dimension which is invaluable for syntac-
tic disambiguation, and it can be effectively learned
using simple treebank grammars.
</bodyText>
<subsectionHeader confidence="0.994791">
2.1 Modern Hebrew Structure
</subsectionHeader>
<bodyText confidence="0.9929131">
Phrases and sentences in MH, as well as in Arabic
and other Semitic languages, have a relatively flexi-
ble phrase structure. Subjects, verbs and objects can
be inverted and prepositional phrases, adjuncts and
verbal modifiers can move around rather freely. The
factors that affect word-order in the language are not
exclusively syntactic and have to do with rhetorical
and pragmatic factors as well.2
2See, for instance, (Melnik, 2002) for an Information
Structure-syntactic account of verb initial sentences.
</bodyText>
<page confidence="0.981836">
157
</page>
<figure confidence="0.9987098">
(a) S
NP.FS-OBJ
N.FS
ewgh
cake.FS
</figure>
<figureCaption confidence="0.998416">
Figure 1: Word Order and Agreement Features in MH
</figureCaption>
<construct confidence="0.837729">
Phrases: Agreement on MP features reveals the subject-
predicate dependency between surface forms and their dom-
inating constituents in a variable phrase-structure (marking
M(asculine), F(eminine), S(ingular), P(lural).)
</construct>
<bodyText confidence="0.999063038461539">
It would be too strong a claim, however, to clas-
sify MH (and similar languages) as a free-word-
order language in the canonical sense. The level of
freedom in the order and number of internal con-
stituents varies between syntactic categories. Within
a verb phrase or a sentential clause, for instance,
the order of constituents obeys less strict rules than
within, e.g., a noun phrase.3 Figure 1 illustrates two
syntactic structures that express the same grammat-
ical relations yet vary in their internal order of con-
stituents. Within the noun phrase constituents, how-
ever, determiners always precede nouns.
Within the flexible phrase structure it is typically
morphological information that provides cues for the
grammatical relations between surface forms. In
figure 1, for example, it is agreement on gender
and number that reveals the subject-predicate depen-
dency between surface forms. Figure 1 also shows
that agreement features help to reveal such relations
between higher levels of constituents as well.
Determining the child constituents that contribute
each of the features is not a trivial matter either. To
illustrate the extent and the complexity of that matter
let us consider definiteness in MH, which is morpho-
logically marked (as an h prefix to the stem, glossed
here explicitly as “the-”) and behaves as a syntactic
</bodyText>
<footnote confidence="0.978559">
3See (Wintner, 2000) and (Goldberg et al., 2006) for formal
and statistical accounts (respectively) of noun phrases in MH.
</footnote>
<figureCaption confidence="0.98350075">
Figure 2: Definiteness in MH as a Phrase-Level Agreement
Feature: Agreement on definiteness helps to determine the in-
ternal structure of a higher level NP (a), and the absence thereof
helps to determine the attachment to a predicate in a verb-less
sentence (b) (marking D(efiniteness))
Figure 3: Phrase-Level Agreement Features and Head-
Dependencies in MH: The direction of percolating definiteness
in MH is distinct of that of the head (marking (head-tag))
</figureCaption>
<bodyText confidence="0.999789037037037">
property (Danon, 2001). Definite noun-phrases ex-
hibit agreement with other modifying phrases, and
such agreement helps to determine the internal struc-
ture, labels, and the correct level of attachment as
illustrated in figure 2. The agreement on definite-
ness helps to determine the internal structure of noun
phrases 2(a), and the absence thereof helps in de-
termining the attachment to predicates in verb-less
sentences, as in 2(b). Finally, definiteness may be
percolated from a different form than the one deter-
mining the gender and number of a phrase. In figure
3(a), for instance, the definiteness feature (marked
as D) percolates from ‘hmnhl’ (the-manager.MS.D)
while the gender and number are percolated from
‘sganit’ (deputy.FS). The direction of percolation
of definiteness may be distinct of that of percolat-
ing head information, as can be seem in figure 3(b).
(The direction of head-dependencies in MH typi-
cally coincides with that of percolating gender.)
To summarize, agreement features are helpful in
analyzing and disambiguating syntactic structures in
MH, not only at the lexical level, but also at higher
levels of constituency. In MH, features percolated
from different surface forms jointly determine the
features of higher-level constituents, and such fea-
tures manifest multiple dependencies, which in turn
cannot be collapsed onto a single head.
</bodyText>
<figure confidence="0.99978775">
NP.FS-OBJ
N.FS
ewgh
cake.FS
VP.MP
V.MP
aklw
ate.MP
sni
two.MP
CD.MP
NP.MP-SBJ
hildim
the-children.MP
N.MP
msw�h
dedicated.FS
sganit hmnhl
deputy.FS the-manager.MS.D
NP.FS.D
(a) S
NP.FS.D
ADJP.FS.D
NP.FS.D
PREDP.FS
(a)
sganit hmnhl
deputy.FS the-manager.MS.D
hmsw�h
the-dedicated.FS.D
NP.FS.D VP.FS NPhNNTi.FS.D VPhVi).FS
V.FS V.FS
NNT.FS
sganit
deputy.FS
N.MS.D
hmnhl
the-manager.MS.D
htpjrh
resigned.FS
NNT.FS
sganit
deputy.FS
N.MS.D
hmnhl
the-manager.MS.D
htpjrh
resigned.FS
(a) S
(b) ShVi
sni hildim
two.MP the-children.MP
(b) S
aklw
ate.MP
NP.MP-SBJ
CD.MP
N.MP
VP.MP
V.MP
</figure>
<page confidence="0.91274">
158
</page>
<subsectionHeader confidence="0.987467">
2.2 The Modern Hebrew Treebank Scheme
</subsectionHeader>
<bodyText confidence="0.999988642857143">
The annotation scheme of version 2.0 of the MH
treebank (Sima’an et al., 2001)4 aims to capture the
morphological and syntactic properties of MH just
described. This results in several aspects that dis-
tinguish the MH treebank from, e.g., the WSJ Penn
treebank annotation scheme (Marcus et al., 1994).
The MH treebank is built over word segments.
This means that the yields of the syntactic trees do
not correspond to space delimited words but rather
to morphological segments that carry distinct syn-
tactic roles, i.e., each segment corresponds to a sin-
gle POS tag. (This in turn means that prefixes
marking determiners, relativizers, prepositions and
definite articles are segmented away and appear as
leaves in a syntactic parse tree.) The POS categories
assigned to segmented words are decorated with fea-
tures such as gender, number, person and tense, and
these features are percolated higher up the tree ac-
cording to pre-defined syntactic dependencies (Kry-
molowski et al., 2007). Since agreement features
of non-terminal constituents may be contributed by
more than one child, the annotation scheme defines
multiple dependency labels that guide the percola-
tion of the different features higher up the tree. Def-
initeness in the MH treebank is treated as a segment
at the POS tags level and as a feature at the level of
non-terminals. As any other feature, it is percolated
higher up the tree according to marked dependency
labels. Table 1 lists the features and values annotated
on top of syntactic categories and table 2 describes
the dependencies according to which these features
are percolated from child constituents to their par-
ents.
In order to comply with the flexible phrase struc-
ture in MH, clausal categories (S, SBAR and FRAG
and their corresponding interrogatives SQ, SQBAR
and FRAGQ) are annotated as flat structures. Verbs
(VB tags) always attach to a VP mother, however
only non-finite VBs can accept complements un-
der the same VP parent, meaning that all inflected
verb forms are represented as unary productions
under an inflected VP. NP and PP are annotated
</bodyText>
<footnote confidence="0.7911584">
4Version 2.0 of the MH treebank is publicly available
at http://mila.cs.technion.ac.il/english/
index.html along with a complete overview of the MH
annotation scheme and illustrative examples (Krymolowski et
al., 2007).
</footnote>
<figure confidence="0.569596222222222">
Feature:Value Value Encoded
gender:Z
gender:N
gender:B
number:Y
number:R
number:B
definite
underspecified
</figure>
<tableCaption confidence="0.996228">
Table 1: Features and Values in the MH Treebank
</tableCaption>
<table confidence="0.885262">
Dependency Type Features Percolated
</table>
<tableCaption confidence="0.990856">
Table 2: Dependency Labels in the MH Treebank
</tableCaption>
<bodyText confidence="0.999993631578947">
as nested structures capturing the recursive struc-
ture of construct-state nouns, numerical expressions
and possession. An additional category, PREDP, is
added in the treebank scheme to account for sen-
tences in MH that lack a copular element, and it may
also be decorated with inflectional features agreeing
with the subject. The MH treebank scheme also fea-
tures null elements that mark traces and additional
labels that mark functional features (e.g., SBJ,OBJ)
which we strip off and ignore throughout this study.
Morphological features percolated up the tree
manifest dependencies that are marked locally yet
have a global effect. We propose to learn treebank
grammars in which the syntactic categories are aug-
mented with morphological features at all levels of
the hierarchy. This allows to learn finer-grained
categories with subtle differences in their syntactic
behavior and to capture non-independence between
certain parts of the syntactic parse-tree.
</bodyText>
<sectionHeader confidence="0.977015" genericHeader="method">
3 Refining the Parameter Space
</sectionHeader>
<bodyText confidence="0.9999115">
(Klein and Manning, 2003) argue that parent en-
coding on top of syntactic categories and RHS
markovization of CFG productions are two instances
of the same idea, namely that of encoding the gener-
ation history of a node to a varying degree. They
subsequently describe two dimensions that define
their parameters’ space. The vertical dimension (v),
capturing the history of the node’s ancestors in a top-
</bodyText>
<figure confidence="0.99945295">
DEP HEAD
all
DEP MAJOR
at least gender
number
DEP NUMBER
definiteness
DEP DEFINITE
DEP ACCUSATIVE
case
all (e.g., conjunction)
DEP MULTIPLE
masculine
feminine
both
singular
plural
both
definiteness:H
definiteness:U
</figure>
<page confidence="0.985324">
159
</page>
<bodyText confidence="0.999525176470588">
down generation process (e.g., its parent and grand-
parent), and the horizontal dimension (h), capturing
the previously generated horizontal ancestors of a
node (effectively, its sisters) in a head-outward gen-
eration process. By varying the value of h and v
along this two-dimensional grid they improve per-
formance of their induced treebank grammar.
Formally, the probability of a parse tree 7r is cal-
culated as the probability of its derivation, the se-
quential application of rewrite rules. This in turn
is calculated as the product of rules’ probabilities,
approximated by assuming independence between
them P(7r) = Ili P(ri|r1 o ... o ri−1) Pz� Ili P(ri).
The vertical dimension v can be thought of as a func-
tion IF0 selecting features from the generation his-
tory of the constituent thus restoring selected depen-
dencies:
</bodyText>
<equation confidence="0.973089">
P(ri) = P(ri|IF0(r1 o .. o ri−1))
</equation>
<bodyText confidence="0.999942">
The horizontal dimension h can be thought of as two
functions IF1, IF2 over decomposed rules, where IF1
selects hidden internal features of the parent, and
IF2 selects previously generated sisters in a head-
outward Markovian process (we retain here the as-
sumption that the head child H always matters).
</bodyText>
<equation confidence="0.992310666666667">
P(ri) = Ph(H|IF1(LHS(ri)))
11 X PC(C|IF2(RHS(ri)), H)
CERHS(rz)−H
</equation>
<bodyText confidence="0.999896785714286">
The fact that the default notion of a treebank
grammar takes v = 1 (i.e., IF0(r1 o .. o ri−1) = 0)
and h = oo (RHS cannot decompose) is, according
to Klein and Manning (2003), a historical accident.
We claim that languages with freeer word order
and richer morphology call for an additional dimen-
sion of parametrization. The additional parameter
shows to what extent morphological features en-
coded in a specialized structure back up the deriva-
tion of the tree. This dimension can be thought of
as a function IF3 selecting aspects of morphological
orthogonal analysis of the rules, where MA denotes
morphological analysis of the syntactic categories in
both LHS and RHS of the rule.
</bodyText>
<equation confidence="0.987831">
P(ri) = P(ri|IF3(MA(ri)))
</equation>
<bodyText confidence="0.861113">
The fact that in current parsers 43(MA(ri)) = 0 is,
we claim, another historical accident. Parsing En-
glish is quite remarkable in that it can be done with
</bodyText>
<figureCaption confidence="0.999713">
Figure 4: The Three-Dimensional Parametrization Space
</figureCaption>
<bodyText confidence="0.999969454545454">
impoverished morphological treatment, but for lan-
guages in which morphological processes are more
pertinent, we argue, bi-dimensional parametrization
shall not suffice.
The emerging picture is as follows. Bare-category
skeletons reside in a bi-dimensional parametrization
space (figure 3(a)) in which the vertical (figure 3(b))
and horizontal (figure 3(c)) parameter instantiations
elaborate the generation history of a non-terminal
node. Specialized structures enriched with (an in-
creasing amount of) morphological features reside
deeper along a third dimension we refer to as depth
(d). Figure 4 illustrates an instantiation of d = 1
with a single definiteness feature. Higher d values
would imply adding more (accumulating) features.
Klein and Manning (2003) view the vertical
and horizontal parametrization dimensions as im-
plementing external and internal annotation strate-
gies respectively. External parameters indicate fea-
tures of the external environment that influence the
node’s expansion possibilities, and internal parame-
ters mark aspects of hidden internal content which
influence constituents’ external distribution. We
view the third dimension of parametrization as im-
plementing a relational strategy of annotation en-
coding the way different constituents may combine
to form phrases and sentences. In a bottom up pro-
cess this annotation strategy imposes soft constraints
on a the top-down head-outward generation process.
Figure 6(a) focuses on a selected NP node high-
lighted in figure 4 and shows its expansion possibil-
ities in three dimensions. Figure 6(b) illustrates how
the depth expansion interacts with both parent anno-
</bodyText>
<page confidence="0.938719">
160
</page>
<figure confidence="0.970163">
(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension
</figure>
<figureCaption confidence="0.999973">
Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003)
</figureCaption>
<bodyText confidence="0.7704225">
tation and neighbor dependencies thereby affecting
both distributions.
</bodyText>
<subsectionHeader confidence="0.993793">
3.1 A Note on State-Splits
</subsectionHeader>
<bodyText confidence="0.999974045454546">
Recent studies (Klein and Manning, 2003; Mat-
suzaki et al., 2005; Prescher, 2005; Petrov et al.,
2006) suggest that category-splits help in enhanc-
ing the performance of treebank grammars, and a
previous study on MH (Tsarfaty, 2006) outlines spe-
cific POS-tags splits that improve MH parsing ac-
curacy. Yet, there is a major difference between
category-splits, whether manually or automatically
acquired, and the kind of state-splits that arise from
agreement features that refine phrasal categories.
While category-splits aim at each category in iso-
lation, agreement features apply to a whole set
of categories all at once, thereby capturing refine-
ment of the categories as well as linguistically mo-
tivated co-occurrences between them. Individual
category-splits are viewed as taking place in a two-
dimensional space and it is hard to analyze and em-
pirically evaluate their interaction with other annota-
tion strategies. Here we propose a principled way to
statistically model the interaction between different
linguistic processes that license grammatical struc-
tures and empirically contrast their contribution.
</bodyText>
<subsectionHeader confidence="0.998367">
3.2 A Note on Stochastic AV grammars
</subsectionHeader>
<bodyText confidence="0.999947857142857">
The practice of having morphological features or-
thogonal to a constituency structure is not a new
one and is familiar from formal theories of syntax
such as HPSG (Sag et al., 2003) and LFG (Ka-
plan and Bresnan, 1982). Here we propose to re-
frame systematic morphological decoration of syn-
tactic categories at all levels of the hierarchy as
</bodyText>
<figure confidence="0.98782">
(a) (b)
</figure>
<figureCaption confidence="0.979224">
Figure 6: The Expansion Possibilities of a Non-Terminal
Node: Expanding the NP from figure 4 in a three-dimensional
parameterization Space
</figureCaption>
<bodyText confidence="0.999878666666667">
an additional dimension of statistical estimation for
learning unlexicalized treebank PCFGs. Our pro-
posal deviates from various stochastic extensions of
such constraints-based grammatical formalisms (cf.
(Abney, 1997)) and has the advantage of elegantly
bypassing the issue of loosing probability mass to
failed derivations due to unification failures. To the
best of our knowledge, this proposal has not been
empirically explored before.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999964166666667">
Our goal is to determine the optimal strategy for
learning treebank grammars for MH and to contrast
it with bi-dimensional strategies explored for En-
glish. The methodology we use is adopted from
(Klein and Manning, 2003) and our procedure is
identical to the one described in (Johnson, 1998).
We define transformations over the treebank that ac-
cept as input specific points in the (h, v, d) space de-
picted in figure 7. We use the transformed training
sets for learning different treebank PCFGs which we
then used to parse unseen sentences, and detrans-
form the parses for the purpose of evaluation.5
</bodyText>
<footnote confidence="0.9205505">
5Previous studied on MH used different portions of the tree-
bank and its annotation scheme due to its gradual development
</footnote>
<page confidence="0.997987">
161
</page>
<bodyText confidence="0.999931244444445">
Data We use version 2.0 of the MH treebank
which consists of 6501 sentences from the daily
newspaper ‘Ha’aretz’. We employ the syntactic cat-
egories, POS categories and morphological features
annotated therein. The data set is split into 13 sec-
tions consisting of 500 sentences each. We use the
first section (section 0) as our development set and
the last section (section 12) as our test set. The re-
maining sentences (sections 1–11) are all used for
training. After removing empty sentences, sentences
with uneven bracketing and sentences that do not
match the annotation scheme6 we remain with a de-
vset of 483 sentences (average length in word seg-
ments 48), a trainset of 5241 sentences (53) and
a testset of 496 sentences (58). Since this work
is only the first step towards the development of a
broad-coverage statistical parser for MH (and other
Semitic languages) we use the development set for
parameter-tuning and error analysis and use the test
set only for confirming our best results.
Models The models we implement use one-, two-
or three-dimensional parametrization and different
instantiation of values thereof. (Due to the small
size of our data set we only use the values 10, 11
as possible instantiations.)
The v dimension is implemented using a trans-
form as in (Johnson, 1998) where v = 0 corresponds
to bare syntactic categories and v = 1 augments
node labels with the label of their parent node.
The h dimension is peculiar in that it distinguishes
PCFGs (h = oc), where RHS cannot decompose,
from their head-driven unlexicalized variety. To im-
plement h =� oc we use a PCFG transformation em-
ulating (Collins, 2003)’s first model, in which sisters
are generated conditioned on the head tag and a sim-
ple ‘distance’ function (Hageloh, 2007).7 The in-
process. As the MH treebank is approaching maturity we feel
that the time is ripe to standardize its use for MH statistical
parsing. The software we implemented will be made available
for non-commercial use upon request to the author(s) and the
feature percolation software by (Krymolowski et al., 2007) is
publicly available through the Knowledge Center for Process-
ing Hebrew. By this we hope to increase the interest in MH
within the parsing community and to facilitate the application
of more sophisticated models by cutting down on setup time.
</bodyText>
<footnote confidence="0.9858754">
6Marked as “NO MATCH” in the treebank.
7A formal overview of the transformation and its corre-
spondence to (Collins, 2003)’s models is available at (Hageloh,
2007). We use the distance function defined therein, marking
the direction and whether it is the first node to be generated.
</footnote>
<bodyText confidence="0.999847577777778">
stantiated value of h then selects the number of pre-
viously generated (non-head) sisters to be taken into
account when generating the next sister in a Marko-
vian process (F2 in our formal exposition).
The d dimension we proposed is implemented us-
ing a transformation that augments syntactic cate-
gories with morphological features percolated up the
tree. We use d = 0 to select bare syntactic cate-
gories and instantiate d = 1 with the definiteness
feature. The decision to select definiteness (rather
than, e.g., gender or number) is rather pragmatic as
its direction of percolation may be distinct of head
information and the question remains whether the
combination of such non-overlapping dependencies
is instrumental for parsing MH.
Our baseline model is a vanilla treebank PCFG
as described in (Charniak, 1996) which we locate
on the (oc, 0, 0) point of our coordinates-system.
In a first set of experiments we implement simple
PCFG extensions of the treebank trees based on se-
lected points on the (oc, v, d) plain. In a second
set of experiments we use an unlexicalized head-
driven baseline a` la (Collins, 2003) located on the
(0, 0, 0) coordinate. We transform the treebank trees
in correspondence with different points in the three-
dimensional space defined by (h, v, d). The models
we implement are marked in the coordinate-system
depicted in figure 7. The implementation details of
the transformations we use are spelled out in tables
3–4.
Procedure We implement different models that
correspond to different instantiations of h, v and d.
For each instantiation we transform the training set
and learn a PCFG using Maximum Likelihood es-
timates, and we use BitPar (Schmidt, 2004), an ef-
ficient general-purpose parser, to parse unseen sen-
tences. The input to the parser is a sequence of word
segments where each segment corresponds to a sin-
gle POS tag, possibly decorated with morphologi-
cal features. This setup assumes partial morpholog-
ical disambiguation (namely, segmentation) but cru-
cially we do not disambiguate their respective POS
categories. This setup is more appropriate for us-
ing general-purpose parsing tools and it makes our
results comparable to studies in other languages.8
</bodyText>
<footnote confidence="0.5719135">
8Our working assumption is that better performance of a
parsing model in our setup will improve performance also
</footnote>
<page confidence="0.992051">
162
</page>
<table confidence="0.69307015">
Transliterate The lexical items (leaves) in the MH treebank are written left-to-write and are encoded
in utf8. A transliteration software is used to convert the utf encoding into Latin characters and to reverse
their order, essentially allowing for standard left-to-right processing.
Correct The manual annotation resulted in unavoidable errors in the annotation scheme, such as typos
(e.g., SQBQR instead of SQBAR) wrong delimiters (e.g., “-” instead of “ ”) or wrong feature order (e.g.,
number-gender instead of gender-number). We used an automatic script to detect these error, we manually
determine their correction. Then we created an automatic script to apply all fixes (57 errors in 1% sentences).
Re-attach VB elements are attached by convention to a VP which inherits its morphological features.
9 VB instances in the treebank are mistakenly attached to an S parent without an intermediate VP level.
Our software re-attaches those VB elements to a VP parent and percolates its morphological features.
Disjoint Due to recursive processes of generating noun phrases and numerical expression (smixut)
in MH the sets of POS and syntactic categories are not disjoint. This is a major concern for PCFG parsers
that assume disjoint sets of pre- and non-terminals. The overlap between the sets also introduces additional
infinite derivations to which we loose probability mass. Our software takes care to decorate POS categories
used as non-terminal with an additional “P”, creating a new set of categories encoding partial derivations.
Lexicalize A pre-condition for applying horizontal parameterizations a` la Collins is the annotation of
heads of syntactic phrases. The treebank provided by the knowledge center does not define unique heads,
but rather, mark multiple dependencies for some categories and none for others. Our software uses rules
for choosing the syntactic head according to specified dependencies and a head table when none are specified.
Linearize In order to implement the head-outward constituents’ generation process we use software made
</table>
<tableCaption confidence="0.97745175">
available to us by (Hageloh, 2007) which converts PCFG production such as the generation of a head is followed by left and right
markovized derivation processes. We used two versions of Markovization, one which conditions only on the
head and a distance function, and another which conditions also on immediately neighboring sister(s).
Decorate Our software implements an additional general transform which selects the features that are to be
annotated on top of syntactic categories to implement various parametrization decisions. This transform can be
used for, e.g., displaying parent information, selecting morphological features, etc.
Table 3: Transforms over the MH Treebank: We clean and correct the treebank using Transliterate, Correct, Re-attach and
Disjoint, and transform the training set according to certain parametrization decisions using Lexicalize, Linearize and Decorate.
</tableCaption>
<bodyText confidence="0.9986702">
Smoothing pre-terminal rules is done explicitly by
collecting statistics on “rare word” occurrences and
providing the parser with possible open class cat-
egories and their corresponding frequency counts.
The frequency threshold defining “rare words” was
tuned empirically and set to 1. The resulting test
parses are detransformed and to skeletal constituent
structures, and are compared against the gold parses
to evaluate parsing accuracy.
Evaluation We evaluate our models using EVALB
in accordance with standard PARSEVAL evaluation
metrics. The evaluation of all models focuses on
Labeled Precision and Recall considering bare syn-
tactic categories (stripping off all morphological or
parental features and removing intermediate nodes
for linearization). We report the average F-measure
for sentences of length up to 40 and for all sentences
(F&lt;40 and FAII respectively). We report the results
within an integrated model for morphological and syntactic dis-
ambiguation in the spirit of (Tsarfaty, 2006). We conjecture
that the kind of models developed here which takes into account
morphological information is more appropriate for the morpho-
logical disambiguation task defined therein.
for two evaluation options, once including punctua-
tion marks (WP) and once excluding them (WOP).
</bodyText>
<sectionHeader confidence="0.999895" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999932055555556">
Our baseline for the first set of experiments is
a vanilla PCFG as described in (Charniak, 1996)
(without a preceding POS tagging phase and without
right branching corrections). We transform the tree-
bank trees based on various points in the (∞, v, d)
two-dimensional space to evaluate the performance
of the resulting PCFG extensions.
Table 5 reports the accuracy results for all models
on section 0 (devset) of the treebank. The accuracy
results for the vanilla PCFG are approximately 10%
lower than reported by (Charniak, 1996) for English
demonstrating that parsing MH using the currently
available treebank is a harder task. For all unlexical-
ized extensions learned from the transfromed tree-
banks, the resulting grammars show enhanced dis-
ambiguation capabilities and improved parsing ac-
curacy. We observe that the vertical dimension con-
tributes the most from both one-dimensional mod-
</bodyText>
<page confidence="0.997631">
163
</page>
<table confidence="0.8824228">
Name Params Description Transforms used
DIST h = 0 0-order Markov process Lexicalize(category), Linearize(distance)
MRK h = 1 1-order Markov process Lexicalize(category), Linearize(distance, neighbor)
PA v = 1 Parent Annotation Decorate(parent)
DEF d = 1 Definiteness feature percolation Decorate(definiteness)
</table>
<tableCaption confidence="0.992948">
Table 4: Implementing Different Parametrization Options using Transforms
</tableCaption>
<table confidence="0.999916666666667">
Implementation (h, v, d) FALL F&lt;ao FALL F&lt;ao
WP WP WOP WOP
PCFG (00,0,0) 65.17 66.63 66.17 67.7
PA (00, 0,1) 70.6 71.96 70.96 72.18
DEF (00,1,0) 67.53 68.78 68.82 70.06
PA+DEF (00,1,1) 72.63 73.89 73.01 74.11
</table>
<tableCaption confidence="0.991864">
Table 5: PCFG Two-Dimensional Extensions: Accuracy re-
sults for parsing the devest (section 0)
</tableCaption>
<bodyText confidence="0.999952871428572">
els. A qualitative error analysis reveals that parent
annotation strategy distinguishes effectively various
kinds of distributions clustered together under a sin-
gle category. For example, S categories that appear
under TOP tend to be more flat than S categories ap-
pearing under SBAR (SBAR clauses typically gen-
erate a non-finite VP node under which additional
PP modifiers can be attached).
Orthogonal morphological marking provide addi-
tional information that is indicative of the kind of
dependencies that exist between a category and its
various child constituents, and we see that the d di-
mension instantiated with definiteness not only con-
tribute more than 2% to the overall parsing accuracy
of a vanilla PCFG, but also contributes as much to
the improvement obtained from a treebank already
annotated with the vertical dimension. The contribu-
tions are thus additive providing preliminary empir-
ical support to our claim that these two dimensions
provide information that is complementary.
In our next set of experiments we evaluate the
contribution of the depth dimension to extensions of
the head-driven unlexicalized variety a` la (Collins,
2003). We set our baseline at the (0, 0, 0) coordi-
nate and evaluate models that combine one, two and
three dimensions of parametrization. Table 6 shows
the accuracy results for parsing section 0 using the
resulting models.
The first outcome of these experiments is that our
new baseline improves on the accuracy results of
a simple treebank PCFG. This result indicates that
head-dependencies which play a role in determin-
ing grammatical structures in English are also in-
strumental for parsing MH. However, the marginal
contribution of the head-driven variation is surpris-
ingly low. Next we observe that for one-dimensional
models the vertical dimension still contributes the
most to parsing accuracy. However, morphologi-
cal information represented by the depth dimension
contributes more to parsing accuracy than informa-
tion concerning immediately preceding sisters on
the horizontal dimension. This outcome is consis-
tent with our observation that the grammar of MH
puts less significance on the position of constituents
relative to one others and that morphological in-
formation is more indicative of the kind of syntac-
tic relations that appear between them. For two-
dimensional models, incorporating the depth dimen-
sion (orthogonal morphological marking) is better
than not doing so, and relying solely on horizon-
tal/vertical parameters performs slightly worse than
the vertical/depth combination. The best performing
model for two-dimensional head-driven extensions
is the one combining vertical history and morpho-
logical depth. This is again consistent with the prop-
erties of MH highlighted in section 2 — parental in-
formation gives cues about the possible expansion
on the current node, and morphological information
indicates possible interrelation between child con-
stituents that may be generated in a flexible order.
Our second set of experiments shows that a three-
dimensional annotation strategy strikes the best bal-
ance between bias and variance and achieves the best
accuracy results among all models. Different dimen-
sions provide different sorts of information which
are complementary, resulting in a model that is ca-
pable of generalizing better. The total error reduc-
tion from a plain PCFG is more than 20%, and our
best result is on a par with those achieved for other
languages (e.g., 75% for MSA).
</bodyText>
<page confidence="0.9933">
164
</page>
<table confidence="0.9999735">
Implementation Params FALL F&lt;40 FALL F&lt;40
(h, v, d) WP WP WOP WOP
DIST (0, 0, 0) 66.56 68.20 67.59 69.24
MRK (1, 0, 0) 66.69 68.14 67.93 69.37
PA (0,1, 0) 68.87 70.48 69.64 70.91
DEF (0, 0,1) 68.85 69.92 70.42 71.45
PA+MRK (1,1, 0) 69.97 71.48 70.69 71.98
MRK+DEF (1, 0,1) 69.46 70.79 71.05 72.37
PA+DEF (0,1,1) 71.15 72.34 71.98 72.91
PA+MRK+DEF (1,1,1) 72.34 73.63 73.27 74.41
</table>
<tableCaption confidence="0.991773">
Table 6: Head-Driven Three-Dimensional Extensions: Ac-
curacy results for parsing the devest (section 0)
</tableCaption>
<table confidence="0.999963833333333">
Implementation Params FALL F&lt;40 FALL F&lt;40
(h, v, d) WP WP WOP WOP
PCFG (∞,0,0) 65.08 67.31 65.82 68.22
PCFG+PA+DEF (∞,1,1) 72.26 74.46 72.42 74.52
DIST (0, 0, 0) 66.33 68.79 67.06 69.47
PA+MRK+DEF (1,1,1) 72.64 74.64 73.21 75.25
</table>
<tableCaption confidence="0.999188">
Table 7: PCFG and Head-Driven Unlexicalized Models:
</tableCaption>
<sectionHeader confidence="0.817141" genericHeader="method">
Accuracy Results for parsing the testst (section 12)
</sectionHeader>
<bodyText confidence="0.999672380952381">
Figure 8 shows the FAll(WOP) results for all
models we implemented. In general, we see that for
parsing MH higher dimensionality is better. More-
over, we see that for all points on the (v, h, 0) plain
the corresponding models on the (v, h,1) plain al-
ways perform better. We further see that the contri-
bution of the depth dimension to a parent annotated
PCFG can compensate, to a large extent on the lack
of head-dependency information. These accumula-
tive results, then, provide empirical evidence to the
importance of morphological and morpho-syntactic
processes such as definiteness for syntactic analysis
and disambiguation as argued for in section 2.
We confirm our results on the testset and report
in table 7 our results on section 12 of the treebank.
The performance has slightly increased and we ob-
tain better results for our best strategy. We retain the
high error-reduction rate and propose our best result,
75.25% for sentences of length ≤ 40, as an empiri-
cally established string baseline on the performance
of treebank grammars for MH.
</bodyText>
<sectionHeader confidence="0.99999" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999977086956522">
The MH treebank (Sima’an et al., 2001), a mor-
phologically and syntactically annotated corpus, has
been successfully used for various NLP tasks such as
morphological disambiguation, POS tagging (Bar-
Haim et al., 2007) and NP chunking (Goldberg et
al., 2006). However its use for statistical parsing has
been more scarce and less successful. The only pre-
vious studies attempting to parse MH we know of
are (Sima’an et al., 2001), applying a variation of the
DOP tree-gram model to 500 sentences, and (Tsar-
faty, 2006), using a treebank PCFG in an integrated
system for morphological and syntactic disambigua-
tion.9 The adaptation of state-of-the-art parsing
models to MH is not immediate as the flat variable
structures of phrases are hard to parse and a plen-
tiful of morphological features that would facilitate
disambiguation are not exploited by currently avail-
able parsers. Also, the MH treebank is much smaller
than the ones for, e.g., English (Marcus et al., 1994)
and Arabic (Maamouri and Bies, 2004), making it
hard to apply data-intensive methods such as the all-
subtrees approach (Bod, 1992) or full lexicalization
(Collins, 2003). Our best performing model incor-
porates three dimensions of parametrization and our
best result (75.25%) is similar to the one obtained
by the parser of (Bikel, 2004) for Modern Standard
Arabic (75%) using a fully lexicalized model and
a training corpus about three times as large as our
newest MH treebank.
This work has shown that devising an adequate
baseline for parsing MH requires more than sim-
ple category-splits and sophisticated head-driven ex-
tensions, and our results provide preliminary evi-
dence for the variation in performance of different
parametrization strategies relative to the properties
and structure of a given language. The compari-
son with parsing accuracy for MSA suggests that
parametrizing an orthogonal depth dimension may
be able to compensate, to some extent, on the lack
of sister-dependencies, lexical information, and per-
haps even the lack of annotated data, but establish-
ing empirically its contribution to parsing MSA is a
matter for further research. In the future we intend
to further investigate the significance of the depth di-
mension by extending our models to include more
morphological features, more variation in the pa-
</bodyText>
<footnote confidence="0.95123575">
9Both studies acheived between 60%–70% accuracy, how-
ever the results are not comparable to our study because of the
use of different training sets, different annotation conventions,
and different evaluation schemes.
</footnote>
<page confidence="0.995051">
165
</page>
<figureCaption confidence="0.999851">
Figure 7: All Models: Locating Unlexicalized Parsing Models Figure 8: All Results: Parsing Results for Unlexicalized Mod-
</figureCaption>
<bodyText confidence="0.7619">
in a Three-Dimensional Parametrization Space els in a Three-Dimensional Parametrization Space
rameter space, and applications to more languages.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999973368421053">
Morphologically rich languages introduce a new di-
mension into the expansion possibilities of a non-
terminal node in a syntactic parse tree. This di-
mension is orthogonal to the vertical (Collins, 2003)
and horizontal (Johnson, 1998) dimensions previ-
ously outlined by Klein and Manning (2003), and
it cannot be collapsed into any one of the previous
two. These additional dependencies exist alongside
the syntactic head dependency and are attested using
morphosyntactic phenomena such as long distance
agreement. We demonstrate using syntactic defi-
niteness in MH that incorporating morphologically
marked features as a third, orthogonal dimension
for annotating syntactic categories is invaluable for
weakening the independence assumptions implicit
in a treebank PCFG and increasing the model’s dis-
ambiguation capabilities. Using a three-dimensional
model we establish a new, stronger, lower bound on
the performance of unlexicalized parsing models for
Modern Hebrew, comparable to those achieved for
other languages (Czech, Chinese, German and Ara-
bic) with much larger corpora.
Tuning the dimensions and value of the parame-
ters for learning treebank grammars is largely an em-
pirical matter, and we do not wish to claim here that
a three-dimensional annotation strategy is the best
for any given language. Rather, we argue that for
different languages different optimal parametriza-
tion strategies may apply. MH is not a free-word-
order language in the canonical sense, and our qual-
itative analysis shows that all dimensions contribute
to the models’ disambiguation capabilities. Orthog-
onal dimensions provide complementary informa-
tion that is invaluable for the parsing process to the
extent that the relevant linguistic phenomena license
grammatical structures in the language. Our results
point out a principled way to quantitatively charac-
terizing differences between languages, thus guid-
ing the selection of parameters for the development
of annotated resources, custom parsers and cross-
linguistic robust parsing engines.
Acknowledgments We thank the Knowledge
Center for Processing Hebrew and Dalia Bojan for
providing us with the newest version of the MH
treebank. We are particularly grateful to the devel-
opment team of version 2.0, Adi Mile’a and Yuval
Krymolowsky, supervised by Yoad Winter for con-
tinued collaboration and technical support. We fur-
ther thank Felix Hageloh for allowing us to use the
software resulting from his M.Sc. thesis work. We
also like to thank Remko Scha, Jelle Zuidema, Yoav
Seginer and three anonymous reviewers for helpful
comments on the text, and Noa Tsarfaty for techni-
cal help in the graphical display. The work of the
first author is funded by the Netherlands Organiza-
tion for Scientific Research (NWO), grant number
017.001.271, for which we are grateful.
</bodyText>
<page confidence="0.99829">
166
</page>
<sectionHeader confidence="0.998343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999980524390244">
S. Abney. 1997. Stochastic Attribute-Value Grammars.
Computational Linguistics, 23 (4):597–618.
R. Bar-Haim, K. Sima’an, and Y. Winter. 2007. Part-of-
Speech Tagging of Modern Hebrew Text. Journal of
Natural Language Engineering.
D. Bikel and D. Chiang. 2000. Two Statistical Parsing
Models Applied to the Chinese Treebank. In Second
Chinese Language Processing Workshop, Hong Kong.
D. Bikel. 2004. Intricacies of Collins’ Parsing Model.
Computational Linguistics, 4(30).
R. Bod. 1992. Data Oriented Parsing. In Proceedings of
COLING.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2, pages 1031–1036.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In AAAI/IAAI,
pages 598–603.
M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann. 1999.
A Statistical Parser for Czech. In Proceedings ofACL,
College Park, Maryland.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics, 29(4).
G. Danon. 2001. Syntactic Definiteness in the Grammar
of Modern Hebrew. Linguistics, 6(39):1071–1116.
A. Dubey and F. Keller. 2003. Probabilistic Parsing for
German using Sister-Head Dependencies. In Proceed-
ings ofACL.
Y. Goldberg, M. Adler, and M. Elhadad. 2006. Noun
Phrase Chunking in Hebrew: Influence of Lexical and
Morphological Features. In Proceedings of COLING-
ACL.
F. Hageloh. 2007. Parsing using Transforms over Tree-
banks. Master’s thesis, University of Amsterdam.
M. Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24(4):613–632.
R. Kaplan and J. Bresnan. 1982. Lexical-Functional
Grammar: A formal system for grammatical represen-
tation. In J. Bresnan, editor, The Mental Representa-
tion of Grammatical Relations, Cambridge, MA. The
MIT Press.
D. Klein and C. Manning. 2003. Accurate Unlexicalized
Parsing. In Proceedings ofACL, pages 423–430.
Y. Krymolowski, Y. Adiel, N. Guthmann, S. Kenan,
A. Milea, N. Nativ, R. Tenzman, and P. Veisberg.
2007. Treebank Annotation Guide. MILA, Knowl-
edge Center for Hebrew Processing.
M. Maamouri and A. Bies. 2004. Developing an Ara-
bic Treebank: Methods, Guidelines, Procedures, and
Tools. In Proceedings of COLING.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn Treebank: Annotating Predicate-
Argument Structure.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with Latent Annotations. In Proceedings of
ACL’05.
N. Melnik. 2002. Verb-Initial Constructions in Modern
Hebrew. Ph.D. thesis, Berkeley University of Califor-
nia.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of ACL-COLING, pages
433–440, Sydney, Australia, July.
D. Prescher. 2005. Head-Driven PCFGs with Latent-
Head Statistics. In In Proceedings of the International
Workshop on Parsing Technologies.
I. A. Sag, T. Wasow, and E. M. Bender. 2003. Syntactic
Theory: A Formal Introduction. CSLI Publications,
address, second edition.
H. Schmidt. 2004. Efficient Parsing of Highly Ambigu-
ous Context-Free Grammars with Bit Vectors. In Pro-
ceedings of COLING, Geneva, Switzerland.
K. Sima’an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a Tree-Bank of Modern Hebrew Text.
In Traitment Automatique des Langues.
R. Tsarfaty. 2006. Integrated Morphological and Syntac-
tic Disambiguation for Modern Hebrew. In Proceed-
ing of SRW COLING-ACL.
S. Wintner. 2000. Definiteness in the Hebrew Noun
Phrase. Journal ofLinguistics, 36:319–363.
</reference>
<page confidence="0.99776">
167
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.7791655">Three-Dimensional Parametrization for Morphologically Rich Languages</title>
<author confidence="0.571447">Reut Tsarfaty</author>
<author confidence="0.571447">Khalil</author>
<affiliation confidence="0.9760405">Institute for Logic, Language and University of</affiliation>
<note confidence="0.517674">Plantage Muidergracht 24, 1018TV Amsterdam, The</note>
<abstract confidence="0.988969037634411">Current parameters of accurate unlexicalized parsers based on Probabilistic Context- Free Grammars (PCFGs) form a twodimensional grid in which rewrite events are conditioned on both horizontal (headoutward) and vertical (parental) histories. In Semitic languages, where arguments may move around rather freely and phrasestructures are often shallow, there are additional morphological factors that govern generation process. Here we pose that agreement features percolated up the parse-tree form a third dimension of parametrization that is orthogonal to the previous two. This dimension differs from mere “state-splits” as it applies to a whole set of categories rather than to individual ones and encodes linguistically motivated co-occurrences between them. This paper presents extensive experiments with extensions of unlexicalized PCFGs for parsing Modern Hebrew in which tuning the parameters in three dimensions gradually leads to improved performance. Our best result introduces a new, stronger, lower bound on the performance of treebank grammars for parsing Modern Hebrew, and is on a par with current results for parsing Modern Standard Arabic obtained by a fully lexicalized parser trained on a much larger treebank. 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the naive treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. The factor that sets apart vanilla PCFGs (Charniak, 1996) from their unlexicalized extensions proposed by, e.g., (Johnson, 1998; Klein and Manning, 2003), is the choice for statistical parametrization that weakens the independence assumptions implicit in the treebank grammar. Studies on accurate unlexicalized parsing models outline two dimensions of parametrization. The first, proposed by (Johnson, 1998), is the annotation of parental history, and the second encodes a head-outward generation process (Collins, 2003). Johnson (1998) augments node labels with the label of their parent, thus incorporating a dependency on the node’s grandparent. Collins (2003) proposes to generate the head of a phrase first and then generate its sisters using Markovian processes, thereby exploiting head/sister-dependencies. 156 of the 10th Conference on Parsing pages Czech Republic, June 2007. Association for Computational Linguistics Klein and Manning (2003) systematize the distinction between these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, accuracy results for parsing languages other than English still We propose that for various languages including the Semitic family, e.g. Modern Hebrew (MH) and Modern Standard Arabic (MSA), a third dimension of parametrization is necessary for encoding linguistic information relevant for breaking false independence assumptions. In Semitic languages, arguments may move around rather freely and the phrase-structure of clause-level categories is often shallow. For such languages agreement features play a role in disambiguation at least as important as the vertical and horizontal conditioning. We propose a third dimension of parameterizations that encodes morphological features such as those realizing syntactic agreement. These features are percolated from surface forms in a bottom-up fashion and express information that is complementary to the horizontal and vertical generation histories proposed before. Such morphological information refines syntactic categories based on their morpho-syntactic role, and captures linguistically motivated co-occurrences and dependencies manifested via, e.g., morpho-syntactic agreement. This work aims at parsing MH and explores the empirical contribution of the three dimensions of parameters specified above. We present extensive experiments that gradually lead to improved performance as we extend the degree to which the three dimensions are exploited. Our best model uses all dimensions of parametrization, and our best relearning curves over increasing training data (e.g., for German (Dubey and Keller, 2003)) show that treebank size cannot be the sole factor to account for the inferior performance. sult is on a par with those achieved for MSA using a fully lexicalized parser and a much larger treebank. The remainder of this document is organized as follows. In section 2 we review characteristic aspects of MH (and other Semitic languages) and illustrate the special role of morphology and dependencies displayed by morpho-syntactic processes using the case of syntactic definiteness in MH. In section 3 we define our three-dimensional parametrization space. In section 4 we spell out the method and procedure for the empirical evaluation of one, two and three parametrization dimensions, and in section 5 we report and analyze results for different parametrization choices. Finally, section 6 discusses related work and in section 7 we summarize and conclude. 2 Dimensions of Modern Hebrew Syntax Parsing MH is in its infancy. Although a syntactically annotated corpus has been available for quite some time (Sima’an et al., 2001), we know of only two studies attempting to parse MH using statistical methods (see section 6). One reason for the sparseness in this field is that the adaptation of existing models to parsing MH is technically involved yet does not guarantee to yield comparable results as the processes that license grammatical structures of phrases and sentences in MH differ from those assumed for English. This section outlines differences between English and MH and discusses their reflection in the MH treebank annotation scheme. We argue that on top of syntactic processes exploited by current parsers there is an orthogonal morphosyntactic dimension which is invaluable for syntactic disambiguation, and it can be effectively learned using simple treebank grammars. 2.1 Modern Hebrew Structure Phrases and sentences in MH, as well as in Arabic and other Semitic languages, have a relatively flexible phrase structure. Subjects, verbs and objects can be inverted and prepositional phrases, adjuncts and verbal modifiers can move around rather freely. The factors that affect word-order in the language are not exclusively syntactic and have to do with rhetorical pragmatic factors as for instance, (Melnik, 2002) for an Information Structure-syntactic account of verb initial sentences. 157 ewgh 1: Order and Agreement Features in MH on reveals the subjectpredicate dependency between surface forms and their dominating constituents in a variable phrase-structure (marking It would be too strong a claim, however, to classify MH (and similar languages) as a free-wordorder language in the canonical sense. The level of freedom in the order and number of internal constituents varies between syntactic categories. Within a verb phrase or a sentential clause, for instance, the order of constituents obeys less strict rules than e.g., a noun 1 illustrates two syntactic structures that express the same grammatical relations yet vary in their internal order of constituents. Within the noun phrase constituents, however, determiners always precede nouns. Within the flexible phrase structure it is typically morphological information that provides cues for the grammatical relations between surface forms. In figure 1, for example, it is agreement on gender and number that reveals the subject-predicate dependency between surface forms. Figure 1 also shows that agreement features help to reveal such relations between higher levels of constituents as well. Determining the child constituents that contribute each of the features is not a trivial matter either. To illustrate the extent and the complexity of that matter us consider MH, which is morphomarked (as an to the stem, glossed here explicitly as “the-”) and behaves as a syntactic (Wintner, 2000) and (Goldberg et al., 2006) for formal and statistical accounts (respectively) of noun phrases in MH. 2: in MH as a Phrase-Level Agreement on definiteness helps to determine the internal structure of a higher level NP (a), and the absence thereof helps to determine the attachment to a predicate in a verb-less (b) (marking 3: Agreement Features and Headin MH: direction of percolating definiteness MH is distinct of that of the head (marking property (Danon, 2001). Definite noun-phrases exhibit agreement with other modifying phrases, and such agreement helps to determine the internal structure, labels, and the correct level of attachment as illustrated in figure 2. The agreement on definiteness helps to determine the internal structure of noun phrases 2(a), and the absence thereof helps in determining the attachment to predicates in verb-less sentences, as in 2(b). Finally, definiteness may be percolated from a different form than the one determining the gender and number of a phrase. In figure 3(a), for instance, the definiteness feature (marked D) percolates from (the-manager.MS.D) while the gender and number are percolated from (deputy.FS). The direction of percolation of definiteness may be distinct of that of percolating head information, as can be seem in figure 3(b). (The direction of head-dependencies in MH typically coincides with that of percolating gender.) To summarize, agreement features are helpful in analyzing and disambiguating syntactic structures in MH, not only at the lexical level, but also at higher levels of constituency. In MH, features percolated from different surface forms jointly determine the features of higher-level constituents, and such features manifest multiple dependencies, which in turn cannot be collapsed onto a single head. ewgh aklw msw�h sganit hmnhl (a) sganit hmnhl hmsw�h sganit sganit aklw 158 2.2 The Modern Hebrew Treebank Scheme The annotation scheme of version 2.0 of the MH (Sima’an et al., aims to capture the morphological and syntactic properties of MH just described. This results in several aspects that distinguish the MH treebank from, e.g., the WSJ Penn treebank annotation scheme (Marcus et al., 1994). The MH treebank is built over word segments. This means that the yields of the syntactic trees do not correspond to space delimited words but rather to morphological segments that carry distinct syntactic roles, i.e., each segment corresponds to a single POS tag. (This in turn means that prefixes marking determiners, relativizers, prepositions and definite articles are segmented away and appear as leaves in a syntactic parse tree.) The POS categories assigned to segmented words are decorated with features such as gender, number, person and tense, and these features are percolated higher up the tree according to pre-defined syntactic dependencies (Krymolowski et al., 2007). Since agreement features of non-terminal constituents may be contributed by more than one child, the annotation scheme defines multiple dependency labels that guide the percolation of the different features higher up the tree. Definiteness in the MH treebank is treated as a segment at the POS tags level and as a feature at the level of non-terminals. As any other feature, it is percolated higher up the tree according to marked dependency labels. Table 1 lists the features and values annotated on top of syntactic categories and table 2 describes the dependencies according to which these features are percolated from child constituents to their parents. In order to comply with the flexible phrase structure in MH, clausal categories (S, SBAR and FRAG and their corresponding interrogatives SQ, SQBAR and FRAGQ) are annotated as flat structures. Verbs (VB tags) always attach to a VP mother, however only non-finite VBs can accept complements under the same VP parent, meaning that all inflected verb forms are represented as unary productions under an inflected VP. NP and PP are annotated 2.0 of the MH treebank is publicly available with a complete overview of the MH annotation scheme and illustrative examples (Krymolowski et al., 2007). Feature:Value Value Encoded gender:Z gender:N gender:B number:Y number:R number:B definite underspecified 1: and Values in the MH Treebank Dependency Type Features Percolated 2: Labels in the MH Treebank as nested structures capturing the recursive structure of construct-state nouns, numerical expressions and possession. An additional category, PREDP, is added in the treebank scheme to account for sentences in MH that lack a copular element, and it may also be decorated with inflectional features agreeing with the subject. The MH treebank scheme also features null elements that mark traces and additional labels that mark functional features (e.g., SBJ,OBJ) which we strip off and ignore throughout this study. Morphological features percolated up the tree manifest dependencies that are marked locally yet have a global effect. We propose to learn treebank grammars in which the syntactic categories are augmented with morphological features at all levels of the hierarchy. This allows to learn finer-grained categories with subtle differences in their syntactic behavior and to capture non-independence between certain parts of the syntactic parse-tree. 3 Refining the Parameter Space (Klein and Manning, 2003) argue that parent encoding on top of syntactic categories and RHS markovization of CFG productions are two instances of the same idea, namely that of encoding the generation history of a node to a varying degree. They subsequently describe two dimensions that define parameters’ space. The the history of the node’s ancestors in a top- DEP HEAD all DEP MAJOR at least gender number DEP NUMBER definiteness DEP DEFINITE DEP ACCUSATIVE case all (e.g., conjunction) DEP MULTIPLE masculine feminine both singular plural both definiteness:H definiteness:U 159 down generation process (e.g., its parent and grandand the capturing the previously generated horizontal ancestors of a node (effectively, its sisters) in a head-outward genprocess. By varying the value of along this two-dimensional grid they improve performance of their induced treebank grammar. the probability of a parse tree calculated as the probability of its derivation, the sequential application of rewrite rules. This in turn is calculated as the product of rules’ probabilities, approximated by assuming independence between = vertical dimension be thought of as a funcfeatures from the generation history of the constituent thus restoring selected dependencies: = horizontal dimension be thought of as two decomposed rules, where selects hidden internal features of the parent, and previously generated sisters in a headoutward Markovian process (we retain here the assumption that the head child H always matters). = 11X The fact that the default notion of a treebank takes 1 = cannot decompose) is, according to Klein and Manning (2003), a historical accident. We claim that languages with freeer word order and richer morphology call for an additional dimension of parametrization. The additional parameter shows to what extent morphological features encoded in a specialized structure back up the derivation of the tree. This dimension can be thought of a function aspects of morphological analysis of the rules, where morphological analysis of the syntactic categories in the rule. = fact that in current parsers = we claim, another historical accident. Parsing English is quite remarkable in that it can be done with 4: Three-Dimensional Parametrization Space impoverished morphological treatment, but for languages in which morphological processes are more pertinent, we argue, bi-dimensional parametrization shall not suffice. The emerging picture is as follows. Bare-category skeletons reside in a bi-dimensional parametrization space (figure 3(a)) in which the vertical (figure 3(b)) and horizontal (figure 3(c)) parameter instantiations elaborate the generation history of a non-terminal node. Specialized structures enriched with (an increasing amount of) morphological features reside along a third dimension we refer to as Figure 4 illustrates an instantiation of 1 a single definiteness feature. Higher would imply adding more (accumulating) features. and Manning (2003) view the dimensions as imstrategies respectively. External parameters indicate features of the external environment that influence the node’s expansion possibilities, and internal parameters mark aspects of hidden internal content which influence constituents’ external distribution. We view the third dimension of parametrization as ima of annotation encoding the way different constituents may combine to form phrases and sentences. In a bottom up process this annotation strategy imposes soft constraints on a the top-down head-outward generation process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how depth expansion interacts with both parent anno- 160 5: Two-Dimensional Space: horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the categories as well as linguistically motivated co-occurrences between them. Individual category-splits are viewed as taking place in a twodimensional space and it is hard to analyze and empirically evaluate their interaction with other annotation strategies. Here we propose a principled way to statistically model the interaction between different linguistic processes that license grammatical structures and empirically contrast their contribution. 3.2 A Note on Stochastic AV grammars The practice of having morphological features orthogonal to a constituency structure is not a new one and is familiar from formal theories of syntax such as HPSG (Sag et al., 2003) and LFG (Kaplan and Bresnan, 1982). Here we propose to reframe systematic morphological decoration of syntactic categories at all levels of the hierarchy as (a) (b) 6: Expansion Possibilities of a Non-Terminal the NP from figure 4 in a three-dimensional parameterization Space an additional dimension of statistical estimation for learning unlexicalized treebank PCFGs. Our proposal deviates from various stochastic extensions of such constraints-based grammatical formalisms (cf. (Abney, 1997)) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures. To the best of our knowledge, this proposal has not been empirically explored before. 4 Experimental Setup Our goal is to determine the optimal strategy for learning treebank grammars for MH and to contrast it with bi-dimensional strategies explored for English. The methodology we use is adopted from (Klein and Manning, 2003) and our procedure is identical to the one described in (Johnson, 1998). We define transformations over the treebank that acas input specific points in the v, depicted in figure 7. We use the transformed training sets for learning different treebank PCFGs which we then used to parse unseen sentences, and detransthe parses for the purpose of studied on MH used different portions of the treebank and its annotation scheme due to its gradual development 161 use version 2.0 of the MH treebank which consists of 6501 sentences from the daily newspaper ‘Ha’aretz’. We employ the syntactic categories, POS categories and morphological features annotated therein. The data set is split into 13 sections consisting of 500 sentences each. We use the first section (section 0) as our development set and the last section (section 12) as our test set. The remaining sentences (sections 1–11) are all used for training. After removing empty sentences, sentences with uneven bracketing and sentences that do not the annotation we remain with a de- 483 sentences (average length in word seg- 48), a 5241 sentences (53) and 496 sentences (58). Since this work is only the first step towards the development of a broad-coverage statistical parser for MH (and other Semitic languages) we use the development set for parameter-tuning and error analysis and use the test set only for confirming our best results. models we implement use one-, twoor three-dimensional parametrization and different instantiation of values thereof. (Due to the small of our data set we only use the values as possible instantiations.) is implemented using a transas in (Johnson, 1998) where 0 bare syntactic categories and 1 node labels with the label of their parent node. is peculiar in that it distinguishes where RHS cannot decompose, from their head-driven unlexicalized variety. To imoc use a PCFG transformation emulating (Collins, 2003)’s first model, in which sisters are generated conditioned on the head tag and a sim- ‘distance’ function (Hageloh, The inprocess. As the MH treebank is approaching maturity we feel that the time is ripe to standardize its use for MH statistical parsing. The software we implemented will be made available for non-commercial use upon request to the author(s) and the feature percolation software by (Krymolowski et al., 2007) is publicly available through the Knowledge Center for Processing Hebrew. By this we hope to increase the interest in MH within the parsing community and to facilitate the application of more sophisticated models by cutting down on setup time. as “NO MATCH” in the treebank. formal overview of the transformation and its correspondence to (Collins, 2003)’s models is available at (Hageloh, 2007). We use the distance function defined therein, marking the direction and whether it is the first node to be generated. value of selects the number of previously generated (non-head) sisters to be taken into account when generating the next sister in a Markoprocess our formal exposition). we proposed is implemented using a transformation that augments syntactic categories with morphological features percolated up the We use 0 select bare syntactic cateand instantiate 1 the definiteness feature. The decision to select definiteness (rather than, e.g., gender or number) is rather pragmatic as its direction of percolation may be distinct of head information and the question remains whether the combination of such non-overlapping dependencies is instrumental for parsing MH. Our baseline model is a vanilla treebank PCFG as described in (Charniak, 1996) which we locate the of our coordinates-system. In a first set of experiments we implement simple PCFG extensions of the treebank trees based on sepoints on the v, In a second set of experiments we use an unlexicalized headdriven baseline a` la (Collins, 2003) located on the We transform the treebank trees in correspondence with different points in the threespace defined by v, The models we implement are marked in the coordinate-system depicted in figure 7. The implementation details of the transformations we use are spelled out in tables 3–4. implement different models that to different instantiations of v For each instantiation we transform the training set and learn a PCFG using Maximum Likelihood estimates, and we use BitPar (Schmidt, 2004), an efficient general-purpose parser, to parse unseen sentences. The input to the parser is a sequence of word segments where each segment corresponds to a single POS tag, possibly decorated with morphological features. This setup assumes partial morphological disambiguation (namely, segmentation) but cruwe do their respective POS categories. This setup is more appropriate for using general-purpose parsing tools and it makes our comparable to studies in other working assumption is that better performance of a parsing model in our setup will improve performance also 162 lexical items (leaves) in the MH treebank are written left-to-write and are encoded in utf8. A transliteration software is used to convert the utf encoding into Latin characters and to reverse their order, essentially allowing for standard left-to-right processing. manual annotation resulted in unavoidable errors in the annotation scheme, such as typos (e.g., SQBQR instead of SQBAR) wrong delimiters (e.g., “-” instead of “ ”) or wrong feature order (e.g., number-gender instead of gender-number). We used an automatic script to detect these error, we manually determine their correction. Then we created an automatic script to apply all fixes (57 errors in 1% sentences). elements are attached by convention to a VP which inherits its morphological features. 9 VB instances in the treebank are mistakenly attached to an S parent without an intermediate VP level. Our software re-attaches those VB elements to a VP parent and percolates its morphological features. to recursive processes of generating noun phrases and numerical expression in MH the sets of POS and syntactic categories are not disjoint. This is a major concern for PCFG parsers that assume disjoint sets of preand non-terminals. The overlap between the sets also introduces additional infinite derivations to which we loose probability mass. Our software takes care to decorate POS categories used as non-terminal with an additional “P”, creating a new set of categories encoding partial derivations. pre-condition for applying horizontal parameterizations a` la Collins is the annotation of heads of syntactic phrases. The treebank provided by the knowledge center does not define unique heads, but rather, mark multiple dependencies for some categories and none for others. Our software uses rules for choosing the syntactic head according to specified dependencies and a head table when none are specified. order to implement the head-outward constituents’ generation process we use software made available to us by (Hageloh, 2007) which converts PCFG production such as the generation of a head is followed by left and right markovized derivation processes. We used two versions of Markovization, one which conditions only on the head and a distance function, and another which conditions also on immediately neighboring sister(s). software implements an additional general transform which selects the features that are to be annotated on top of syntactic categories to implement various parametrization decisions. This transform can be used for, e.g., displaying parent information, selecting morphological features, etc. 3: over the MH Treebank: clean and correct the treebank using Correct, Re-attach and transform the training set according to certain parametrization decisions using Linearize Smoothing pre-terminal rules is done explicitly by collecting statistics on “rare word” occurrences and providing the parser with possible open class categories and their corresponding frequency counts. The frequency threshold defining “rare words” was tuned empirically and set to 1. The resulting test parses are detransformed and to skeletal constituent structures, and are compared against the gold parses to evaluate parsing accuracy. evaluate our models using EVALB in accordance with standard PARSEVAL evaluation metrics. The evaluation of all models focuses on Labeled Precision and Recall considering bare syntactic categories (stripping off all morphological or parental features and removing intermediate nodes for linearization). We report the average F-measure for sentences of length up to 40 and for all sentences and We report the results within an integrated model for morphological and syntactic disambiguation in the spirit of (Tsarfaty, 2006). We conjecture that the kind of models developed here which takes into account morphological information is more appropriate for the morphological disambiguation task defined therein. for two evaluation options, once including punctuamarks and once excluding them 5 Results Our baseline for the first set of experiments is a vanilla PCFG as described in (Charniak, 1996) (without a preceding POS tagging phase and without right branching corrections). We transform the treetrees based on various points in the v, two-dimensional space to evaluate the performance of the resulting PCFG extensions. Table 5 reports the accuracy results for all models section 0 of the treebank. The accuracy results for the vanilla PCFG are approximately 10% lower than reported by (Charniak, 1996) for English demonstrating that parsing MH using the currently available treebank is a harder task. For all unlexicalized extensions learned from the transfromed treebanks, the resulting grammars show enhanced disambiguation capabilities and improved parsing accuracy. We observe that the vertical dimension conthe most from both one-dimensional mod- 163 Name Params Description Transforms used 0 Markov process Linearize(distance) 1 Markov process Linearize(distance, neighbor) 1 Annotation 1 feature percolation 4: Different Parametrization Options using Transforms Implementation v, WP WP WOP WOP PCFG 65.17 66.63 66.17 67.7 PA 70.6 71.96 70.96 72.18 DEF 67.53 68.78 68.82 70.06 PA+DEF 72.63 73.89 73.01 74.11 5: Two-Dimensional Extensions: refor parsing the 0) els. A qualitative error analysis reveals that parent annotation strategy distinguishes effectively various kinds of distributions clustered together under a single category. For example, S categories that appear under TOP tend to be more flat than S categories appearing under SBAR (SBAR clauses typically generate a non-finite VP node under which additional PP modifiers can be attached). Orthogonal morphological marking provide additional information that is indicative of the kind of dependencies that exist between a category and its child constituents, and we see that the diinstantiated with only contribute more than 2% to the overall parsing accuracy of a vanilla PCFG, but also contributes as much to the improvement obtained from a treebank already annotated with the vertical dimension. The contributions are thus additive providing preliminary empirical support to our claim that these two dimensions provide information that is complementary. In our next set of experiments we evaluate the contribution of the depth dimension to extensions of the head-driven unlexicalized variety a` la (Collins, We set our baseline at the coordinate and evaluate models that combine one, two and three dimensions of parametrization. Table 6 shows the accuracy results for parsing section 0 using the resulting models. The first outcome of these experiments is that our new baseline improves on the accuracy results of a simple treebank PCFG. This result indicates that head-dependencies which play a role in determining grammatical structures in English are also instrumental for parsing MH. However, the marginal contribution of the head-driven variation is surprisingly low. Next we observe that for one-dimensional models the vertical dimension still contributes the most to parsing accuracy. However, morphological information represented by the depth dimension contributes more to parsing accuracy than information concerning immediately preceding sisters on the horizontal dimension. This outcome is consistent with our observation that the grammar of MH puts less significance on the position of constituents relative to one others and that morphological information is more indicative of the kind of syntactic relations that appear between them. For twodimensional models, incorporating the depth dimension (orthogonal morphological marking) is better than not doing so, and relying solely on horizontal/vertical parameters performs slightly worse than the vertical/depth combination. The best performing model for two-dimensional head-driven extensions is the one combining vertical history and morphological depth. This is again consistent with the properties of MH highlighted in section 2 — parental information gives cues about the possible expansion on the current node, and morphological information indicates possible interrelation between child constituents that may be generated in a flexible order. Our second set of experiments shows that a threedimensional annotation strategy strikes the best balance between bias and variance and achieves the best accuracy results among all models. Different dimensions provide different sorts of information which are complementary, resulting in a model that is capable of generalizing better. The total error reduction from a plain PCFG is more than 20%, and our best result is on a par with those achieved for other languages (e.g., 75% for MSA).</abstract>
<note confidence="0.929984526315789">164 Implementation Params v, WP WP WOP WOP DIST 66.56 68.20 67.59 69.24 MRK 66.69 68.14 67.93 69.37 PA 68.87 70.48 69.64 70.91 DEF 68.85 69.92 70.42 71.45 PA+MRK 69.97 71.48 70.69 71.98 MRK+DEF 69.46 70.79 71.05 72.37 PA+DEF 71.15 72.34 71.98 72.91 PA+MRK+DEF 72.34 73.63 73.27 74.41 6: Three-Dimensional Extensions: Acresults for parsing the 0) Implementation v, WP WP WOP WOP PCFG 65.08 67.31 65.82 68.22 PCFG+PA+DEF 72.26 74.46 72.42 74.52 DIST 66.33 68.79 67.06 69.47 PA+MRK+DEF 72.64 74.64 73.21 75.25 7: and Head-Driven Unlexicalized Models: Results for parsing the 12</note>
<abstract confidence="0.99434571641791">8 shows the for all models we implemented. In general, we see that for parsing MH higher dimensionality is better. Morewe see that for all points on the h, corresponding models on the always perform better. We further see that the contribution of the depth dimension to a parent annotated PCFG can compensate, to a large extent on the lack of head-dependency information. These accumulative results, then, provide empirical evidence to the importance of morphological and morpho-syntactic processes such as definiteness for syntactic analysis and disambiguation as argued for in section 2. confirm our results on the report in table 7 our results on section 12 of the treebank. The performance has slightly increased and we obtain better results for our best strategy. We retain the high error-reduction rate and propose our best result, for sentences of length as an empirically established string baseline on the performance of treebank grammars for MH. 6 Related Work The MH treebank (Sima’an et al., 2001), a morphologically and syntactically annotated corpus, has been successfully used for various NLP tasks such as morphological disambiguation, POS tagging (Bar- Haim et al., 2007) and NP chunking (Goldberg et al., 2006). However its use for statistical parsing has been more scarce and less successful. The only previous studies attempting to parse MH we know of are (Sima’an et al., 2001), applying a variation of the DOP tree-gram model to 500 sentences, and (Tsarfaty, 2006), using a treebank PCFG in an integrated system for morphological and syntactic disambigua- The adaptation of state-of-the-art parsing models to MH is not immediate as the flat variable structures of phrases are hard to parse and a plentiful of morphological features that would facilitate disambiguation are not exploited by currently available parsers. Also, the MH treebank is much smaller than the ones for, e.g., English (Marcus et al., 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the allsubtrees approach (Bod, 1992) or full lexicalization (Collins, 2003). Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH treebank. This work has shown that devising an adequate baseline for parsing MH requires more than simple category-splits and sophisticated head-driven extensions, and our results provide preliminary evidence for the variation in performance of different parametrization strategies relative to the properties and structure of a given language. The comparison with parsing accuracy for MSA suggests that parametrizing an orthogonal depth dimension may be able to compensate, to some extent, on the lack of sister-dependencies, lexical information, and perhaps even the lack of annotated data, but establishing empirically its contribution to parsing MSA is a matter for further research. In the future we intend to further investigate the significance of the depth dimension by extending our models to include more features, more variation in the pastudies acheived between 60%–70% accuracy, however the results are not comparable to our study because of the use of different training sets, different annotation conventions, and different evaluation schemes. 165 7: Models: Unlexicalized Parsing Models 8: Results: Results for Unlexicalized Modin a Three-Dimensional Parametrization Space els in a Three-Dimensional Parametrization Space rameter space, and applications to more languages. 7 Conclusion Morphologically rich languages introduce a new dimension into the expansion possibilities of a nonterminal node in a syntactic parse tree. This dimension is orthogonal to the vertical (Collins, 2003) and horizontal (Johnson, 1998) dimensions previously outlined by Klein and Manning (2003), and it cannot be collapsed into any one of the previous two. These additional dependencies exist alongside the syntactic head dependency and are attested using morphosyntactic phenomena such as long distance agreement. We demonstrate using syntactic definiteness in MH that incorporating morphologically marked features as a third, orthogonal dimension for annotating syntactic categories is invaluable for weakening the independence assumptions implicit in a treebank PCFG and increasing the model’s disambiguation capabilities. Using a three-dimensional model we establish a new, stronger, lower bound on the performance of unlexicalized parsing models for Modern Hebrew, comparable to those achieved for other languages (Czech, Chinese, German and Arabic) with much larger corpora. Tuning the dimensions and value of the parameters for learning treebank grammars is largely an empirical matter, and we do not wish to claim here that a three-dimensional annotation strategy is the best for any given language. Rather, we argue that for different languages different optimal parametrization strategies may apply. MH is not a free-wordorder language in the canonical sense, and our qualitative analysis shows that all dimensions contribute to the models’ disambiguation capabilities. Orthogonal dimensions provide complementary information that is invaluable for the parsing process to the extent that the relevant linguistic phenomena license grammatical structures in the language. Our results point out a principled way to quantitatively characterizing differences between languages, thus guiding the selection of parameters for the development of annotated resources, custom parsers and crosslinguistic robust parsing engines. thank the Knowledge Center for Processing Hebrew and Dalia Bojan for providing us with the newest version of the MH treebank. We are particularly grateful to the development team of version 2.0, Adi Mile’a and Yuval Krymolowsky, supervised by Yoad Winter for continued collaboration and technical support. We further thank Felix Hageloh for allowing us to use the software resulting from his M.Sc. thesis work. We also like to thank Remko Scha, Jelle Zuidema, Yoav Seginer and three anonymous reviewers for helpful comments on the text, and Noa Tsarfaty for technical help in the graphical display. The work of the first author is funded by the Netherlands Organization for Scientific Research (NWO), grant number 017.001.271, for which we are grateful.</abstract>
<note confidence="0.889352259259259">166 References S. Abney. 1997. Stochastic Attribute-Value Grammars. 23 (4):597–618. R. Bar-Haim, K. Sima’an, and Y. Winter. 2007. Part-of- Tagging of Modern Hebrew Text. of Language D. Bikel and D. Chiang. 2000. Two Statistical Parsing Applied to the Chinese Treebank. In Language Processing Hong Kong. D. Bikel. 2004. Intricacies of Collins’ Parsing Model. 4(30). Bod. 1992. Data Oriented Parsing. In of E. Charniak. 1996. Tree-Bank Grammars. Vol. pages 1031–1036. E. Charniak. 1997. Statistical Parsing with a Context- Grammar and Word Statistics. In pages 598–603. M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann. 1999. Statistical Parser for Czech. In College Park, Maryland. M. Collins. 2003. Head-Driven Statistical Models for Language Parsing. Linguis- 29(4). G. Danon. 2001. Syntactic Definiteness in the Grammar Modern Hebrew. 6(39):1071–1116. A. Dubey and F. Keller. 2003. Probabilistic Parsing for</note>
<title confidence="0.937037">using Sister-Head Dependencies. In Proceed-</title>
<author confidence="0.877721">Noun</author>
<affiliation confidence="0.610526">Phrase Chunking in Hebrew: Influence of Lexical and</affiliation>
<note confidence="0.923985714285714">Features. In of COLING- F. Hageloh. 2007. Parsing using Transforms over Treebanks. Master’s thesis, University of Amsterdam. M. Johnson. 1998. PCFG Models of Linguistic Representations. 24(4):613–632. R. Kaplan and J. Bresnan. 1982. Lexical-Functional</note>
<title confidence="0.72616">Grammar: A formal system for grammatical represen-</title>
<author confidence="0.7340535">The</author>
<affiliation confidence="0.753039">MIT Press.</affiliation>
<note confidence="0.832576684210526">D. Klein and C. Manning. 2003. Accurate Unlexicalized In pages 423–430. Y. Krymolowski, Y. Adiel, N. Guthmann, S. Kenan, A. Milea, N. Nativ, R. Tenzman, and P. Veisberg. 2007. Treebank Annotation Guide. MILA, Knowledge Center for Hebrew Processing. M. Maamouri and A. Bies. 2004. Developing an Arabic Treebank: Methods, Guidelines, Procedures, and In of M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn Treebank: Annotating Predicate- Argument Structure. T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis- CFG with Latent Annotations. In of Melnik. 2002. Constructions in Modern Ph.D. thesis, Berkeley University of California. S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.</note>
<title confidence="0.722342">Learning Accurate, Compact, and Interpretable Tree</title>
<affiliation confidence="0.391865">In of pages</affiliation>
<address confidence="0.483565">433–440, Sydney, Australia, July.</address>
<note confidence="0.662408055555555">D. Prescher. 2005. Head-Driven PCFGs with Latent- Statistics. In Proceedings of the International on Parsing A. Sag, T. Wasow, and E. M. Bender. 2003. A Formal CSLI Publications, address, second edition. H. Schmidt. 2004. Efficient Parsing of Highly Ambigu- Context-Free Grammars with Bit Vectors. In Proof Geneva, Switzerland. K. Sima’an, A. Itai, Y. Winter, A. Altman, and N. Nativ. 2001. Building a Tree-Bank of Modern Hebrew Text. Automatique des R. Tsarfaty. 2006. Integrated Morphological and Syntac- Disambiguation for Modern Hebrew. In Proceedof SRW S. Wintner. 2000. Definiteness in the Hebrew Noun 36:319–363. 167</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Stochastic Attribute-Value Grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<pages>4--597</pages>
<contexts>
<context position="22183" citStr="Abney, 1997" startWordPosition="3362" endWordPosition="3363">t a new one and is familiar from formal theories of syntax such as HPSG (Sag et al., 2003) and LFG (Kaplan and Bresnan, 1982). Here we propose to reframe systematic morphological decoration of syntactic categories at all levels of the hierarchy as (a) (b) Figure 6: The Expansion Possibilities of a Non-Terminal Node: Expanding the NP from figure 4 in a three-dimensional parameterization Space an additional dimension of statistical estimation for learning unlexicalized treebank PCFGs. Our proposal deviates from various stochastic extensions of such constraints-based grammatical formalisms (cf. (Abney, 1997)) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures. To the best of our knowledge, this proposal has not been empirically explored before. 4 Experimental Setup Our goal is to determine the optimal strategy for learning treebank grammars for MH and to contrast it with bi-dimensional strategies explored for English. The methodology we use is adopted from (Klein and Manning, 2003) and our procedure is identical to the one described in (Johnson, 1998). We define transformations over the treebank that accept as input</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>S. Abney. 1997. Stochastic Attribute-Value Grammars. Computational Linguistics, 23 (4):597–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bar-Haim</author>
<author>K Sima’an</author>
<author>Y Winter</author>
</authors>
<title>Part-ofSpeech Tagging of Modern Hebrew Text.</title>
<date>2007</date>
<journal>Journal of Natural Language Engineering.</journal>
<marker>Bar-Haim, Sima’an, Winter, 2007</marker>
<rawString>R. Bar-Haim, K. Sima’an, and Y. Winter. 2007. Part-ofSpeech Tagging of Modern Hebrew Text. Journal of Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>D Chiang</author>
</authors>
<title>Two Statistical Parsing Models Applied to the Chinese Treebank.</title>
<date>2000</date>
<booktitle>In Second Chinese Language Processing Workshop,</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="3987" citStr="Bikel and Chiang, 2000" startWordPosition="574" endWordPosition="577">etween these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, accuracy results for parsing languages other than English still lag behind.1 We propose that for various languages including the Semitic family, e.g. Modern Hebrew (MH) and Modern Standard Arabic (MSA), a third dimension of parametrization is necessary for encoding linguistic information relevant for breaking false independence assumptions. In Semitic languages, arguments may move around rather freely and the phrase-structure of clause-level categories is often shallow. For such languages agreement features play a role in disambiguation at least as important as the vertical and hori</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>D. Bikel and D. Chiang. 2000. Two Statistical Parsing Models Applied to the Chinese Treebank. In Second Chinese Language Processing Workshop, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ Parsing Model. Computational Linguistics,</journal>
<volume>4</volume>
<issue>30</issue>
<contexts>
<context position="40523" citStr="Bikel, 2004" startWordPosition="6259" endWordPosition="6260"> flat variable structures of phrases are hard to parse and a plentiful of morphological features that would facilitate disambiguation are not exploited by currently available parsers. Also, the MH treebank is much smaller than the ones for, e.g., English (Marcus et al., 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the allsubtrees approach (Bod, 1992) or full lexicalization (Collins, 2003). Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH treebank. This work has shown that devising an adequate baseline for parsing MH requires more than simple category-splits and sophisticated head-driven extensions, and our results provide preliminary evidence for the variation in performance of different parametrization strategies relative to the properties and structure of a given language. The comparison with parsing accuracy for MSA suggests that parametrizing an orthogonal depth dimension may be able to compen</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>D. Bikel. 2004. Intricacies of Collins’ Parsing Model. Computational Linguistics, 4(30).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Data Oriented Parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="40317" citStr="Bod, 1992" startWordPosition="6227" endWordPosition="6228">tences, and (Tsarfaty, 2006), using a treebank PCFG in an integrated system for morphological and syntactic disambiguation.9 The adaptation of state-of-the-art parsing models to MH is not immediate as the flat variable structures of phrases are hard to parse and a plentiful of morphological features that would facilitate disambiguation are not exploited by currently available parsers. Also, the MH treebank is much smaller than the ones for, e.g., English (Marcus et al., 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the allsubtrees approach (Bod, 1992) or full lexicalization (Collins, 2003). Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH treebank. This work has shown that devising an adequate baseline for parsing MH requires more than simple category-splits and sophisticated head-driven extensions, and our results provide preliminary evidence for the variation in performance of different paramet</context>
</contexts>
<marker>Bod, 1992</marker>
<rawString>R. Bod. 1992. Data Oriented Parsing. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Tree-Bank Grammars.</title>
<date>1996</date>
<booktitle>In AAAI/IAAI,</booktitle>
<volume>2</volume>
<pages>1031--1036</pages>
<contexts>
<context position="2390" citStr="Charniak, 1996" startWordPosition="343" endWordPosition="345">ank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. The factor that sets apart vanilla PCFGs (Charniak, 1996) from their unlexicalized extensions proposed by, e.g., (Johnson, 1998; Klein and Manning, 2003), is the choice for statistical parametrization that weakens the independence assumptions implicit in the treebank grammar. Studies on accurate unlexicalized parsing models outline two dimensions of parametrization. The first, proposed by (Johnson, 1998), is the annotation of parental history, and the second encodes a head-outward generation process (Collins, 2003). Johnson (1998) augments node labels with the label of their parent, thus incorporating a dependency on the node’s grandparent. Collins </context>
<context position="26557" citStr="Charniak, 1996" startWordPosition="4085" endWordPosition="4086">. The d dimension we proposed is implemented using a transformation that augments syntactic categories with morphological features percolated up the tree. We use d = 0 to select bare syntactic categories and instantiate d = 1 with the definiteness feature. The decision to select definiteness (rather than, e.g., gender or number) is rather pragmatic as its direction of percolation may be distinct of head information and the question remains whether the combination of such non-overlapping dependencies is instrumental for parsing MH. Our baseline model is a vanilla treebank PCFG as described in (Charniak, 1996) which we locate on the (oc, 0, 0) point of our coordinates-system. In a first set of experiments we implement simple PCFG extensions of the treebank trees based on selected points on the (oc, v, d) plain. In a second set of experiments we use an unlexicalized headdriven baseline a` la (Collins, 2003) located on the (0, 0, 0) coordinate. We transform the treebank trees in correspondence with different points in the threedimensional space defined by (h, v, d). The models we implement are marked in the coordinate-system depicted in figure 7. The implementation details of the transformations we u</context>
<context position="32399" citStr="Charniak, 1996" startWordPosition="4976" endWordPosition="4977">ge F-measure for sentences of length up to 40 and for all sentences (F&lt;40 and FAII respectively). We report the results within an integrated model for morphological and syntactic disambiguation in the spirit of (Tsarfaty, 2006). We conjecture that the kind of models developed here which takes into account morphological information is more appropriate for the morphological disambiguation task defined therein. for two evaluation options, once including punctuation marks (WP) and once excluding them (WOP). 5 Results Our baseline for the first set of experiments is a vanilla PCFG as described in (Charniak, 1996) (without a preceding POS tagging phase and without right branching corrections). We transform the treebank trees based on various points in the (∞, v, d) two-dimensional space to evaluate the performance of the resulting PCFG extensions. Table 5 reports the accuracy results for all models on section 0 (devset) of the treebank. The accuracy results for the vanilla PCFG are approximately 10% lower than reported by (Charniak, 1996) for English demonstrating that parsing MH using the currently available treebank is a harder task. For all unlexicalized extensions learned from the transfromed treeb</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>E. Charniak. 1996. Tree-Bank Grammars. In AAAI/IAAI, Vol. 2, pages 1031–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Parsing with a ContextFree Grammar and Word Statistics. In</title>
<date>1997</date>
<booktitle>AAAI/IAAI,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="2048" citStr="Charniak, 1997" startWordPosition="293" endWordPosition="294">d Arabic obtained by a fully lexicalized parser trained on a much larger treebank. 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the naive treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. The factor that sets apart vanilla PCFGs (Charniak, 1996) from their unlexicalized extensions proposed by, e.g., (Johnson, 1998; Klein and Manning, 2003), is the choice for statistical parametrization that weakens the independence assumptions implicit in the treebank grammar. Studies on accurate unlexicalized pars</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical Parsing with a ContextFree Grammar and Word Statistics. In AAAI/IAAI, pages 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Hajic</author>
<author>L Ramshaw</author>
<author>C Tillmann</author>
</authors>
<title>A Statistical Parser for Czech.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>College Park, Maryland.</location>
<contexts>
<context position="3950" citStr="Collins et al., 1999" startWordPosition="567" endWordPosition="570">2003) systematize the distinction between these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, accuracy results for parsing languages other than English still lag behind.1 We propose that for various languages including the Semitic family, e.g. Modern Hebrew (MH) and Modern Standard Arabic (MSA), a third dimension of parametrization is necessary for encoding linguistic information relevant for breaking false independence assumptions. In Semitic languages, arguments may move around rather freely and the phrase-structure of clause-level categories is often shallow. For such languages agreement features play a role in disambiguation at least </context>
</contexts>
<marker>Collins, Hajic, Ramshaw, Tillmann, 1999</marker>
<rawString>M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann. 1999. A Statistical Parser for Czech. In Proceedings ofACL, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="2031" citStr="Collins, 2003" startWordPosition="291" endWordPosition="292"> Modern Standard Arabic obtained by a fully lexicalized parser trained on a much larger treebank. 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the naive treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. The factor that sets apart vanilla PCFGs (Charniak, 1996) from their unlexicalized extensions proposed by, e.g., (Johnson, 1998; Klein and Manning, 2003), is the choice for statistical parametrization that weakens the independence assumptions implicit in the treebank grammar. Studies on accurate u</context>
<context position="24782" citStr="Collins, 2003" startWordPosition="3797" endWordPosition="3798">mplement use one-, twoor three-dimensional parametrization and different instantiation of values thereof. (Due to the small size of our data set we only use the values 10, 11 as possible instantiations.) The v dimension is implemented using a transform as in (Johnson, 1998) where v = 0 corresponds to bare syntactic categories and v = 1 augments node labels with the label of their parent node. The h dimension is peculiar in that it distinguishes PCFGs (h = oc), where RHS cannot decompose, from their head-driven unlexicalized variety. To implement h =� oc we use a PCFG transformation emulating (Collins, 2003)’s first model, in which sisters are generated conditioned on the head tag and a simple ‘distance’ function (Hageloh, 2007).7 The inprocess. As the MH treebank is approaching maturity we feel that the time is ripe to standardize its use for MH statistical parsing. The software we implemented will be made available for non-commercial use upon request to the author(s) and the feature percolation software by (Krymolowski et al., 2007) is publicly available through the Knowledge Center for Processing Hebrew. By this we hope to increase the interest in MH within the parsing community and to facilit</context>
<context position="26859" citStr="Collins, 2003" startWordPosition="4140" endWordPosition="4141">an, e.g., gender or number) is rather pragmatic as its direction of percolation may be distinct of head information and the question remains whether the combination of such non-overlapping dependencies is instrumental for parsing MH. Our baseline model is a vanilla treebank PCFG as described in (Charniak, 1996) which we locate on the (oc, 0, 0) point of our coordinates-system. In a first set of experiments we implement simple PCFG extensions of the treebank trees based on selected points on the (oc, v, d) plain. In a second set of experiments we use an unlexicalized headdriven baseline a` la (Collins, 2003) located on the (0, 0, 0) coordinate. We transform the treebank trees in correspondence with different points in the threedimensional space defined by (h, v, d). The models we implement are marked in the coordinate-system depicted in figure 7. The implementation details of the transformations we use are spelled out in tables 3–4. Procedure We implement different models that correspond to different instantiations of h, v and d. For each instantiation we transform the training set and learn a PCFG using Maximum Likelihood estimates, and we use BitPar (Schmidt, 2004), an efficient general-purpose</context>
<context position="35033" citStr="Collins, 2003" startWordPosition="5375" endWordPosition="5376">s various child constituents, and we see that the d dimension instantiated with definiteness not only contribute more than 2% to the overall parsing accuracy of a vanilla PCFG, but also contributes as much to the improvement obtained from a treebank already annotated with the vertical dimension. The contributions are thus additive providing preliminary empirical support to our claim that these two dimensions provide information that is complementary. In our next set of experiments we evaluate the contribution of the depth dimension to extensions of the head-driven unlexicalized variety a` la (Collins, 2003). We set our baseline at the (0, 0, 0) coordinate and evaluate models that combine one, two and three dimensions of parametrization. Table 6 shows the accuracy results for parsing section 0 using the resulting models. The first outcome of these experiments is that our new baseline improves on the accuracy results of a simple treebank PCFG. This result indicates that head-dependencies which play a role in determining grammatical structures in English are also instrumental for parsing MH. However, the marginal contribution of the head-driven variation is surprisingly low. Next we observe that fo</context>
<context position="40356" citStr="Collins, 2003" startWordPosition="6232" endWordPosition="6233"> a treebank PCFG in an integrated system for morphological and syntactic disambiguation.9 The adaptation of state-of-the-art parsing models to MH is not immediate as the flat variable structures of phrases are hard to parse and a plentiful of morphological features that would facilitate disambiguation are not exploited by currently available parsers. Also, the MH treebank is much smaller than the ones for, e.g., English (Marcus et al., 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the allsubtrees approach (Bod, 1992) or full lexicalization (Collins, 2003). Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH treebank. This work has shown that devising an adequate baseline for parsing MH requires more than simple category-splits and sophisticated head-driven extensions, and our results provide preliminary evidence for the variation in performance of different parametrization strategies relative to the pro</context>
<context position="42214" citStr="Collins, 2003" startWordPosition="6516" endWordPosition="6517">able to our study because of the use of different training sets, different annotation conventions, and different evaluation schemes. 165 Figure 7: All Models: Locating Unlexicalized Parsing Models Figure 8: All Results: Parsing Results for Unlexicalized Modin a Three-Dimensional Parametrization Space els in a Three-Dimensional Parametrization Space rameter space, and applications to more languages. 7 Conclusion Morphologically rich languages introduce a new dimension into the expansion possibilities of a nonterminal node in a syntactic parse tree. This dimension is orthogonal to the vertical (Collins, 2003) and horizontal (Johnson, 1998) dimensions previously outlined by Klein and Manning (2003), and it cannot be collapsed into any one of the previous two. These additional dependencies exist alongside the syntactic head dependency and are attested using morphosyntactic phenomena such as long distance agreement. We demonstrate using syntactic definiteness in MH that incorporating morphologically marked features as a third, orthogonal dimension for annotating syntactic categories is invaluable for weakening the independence assumptions implicit in a treebank PCFG and increasing the model’s disambi</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics, 29(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Danon</author>
</authors>
<date>2001</date>
<booktitle>Syntactic Definiteness in the Grammar of Modern Hebrew. Linguistics,</booktitle>
<pages>6--39</pages>
<contexts>
<context position="10144" citStr="Danon, 2001" startWordPosition="1519" endWordPosition="1520">syntactic 3See (Wintner, 2000) and (Goldberg et al., 2006) for formal and statistical accounts (respectively) of noun phrases in MH. Figure 2: Definiteness in MH as a Phrase-Level Agreement Feature: Agreement on definiteness helps to determine the internal structure of a higher level NP (a), and the absence thereof helps to determine the attachment to a predicate in a verb-less sentence (b) (marking D(efiniteness)) Figure 3: Phrase-Level Agreement Features and HeadDependencies in MH: The direction of percolating definiteness in MH is distinct of that of the head (marking (head-tag)) property (Danon, 2001). Definite noun-phrases exhibit agreement with other modifying phrases, and such agreement helps to determine the internal structure, labels, and the correct level of attachment as illustrated in figure 2. The agreement on definiteness helps to determine the internal structure of noun phrases 2(a), and the absence thereof helps in determining the attachment to predicates in verb-less sentences, as in 2(b). Finally, definiteness may be percolated from a different form than the one determining the gender and number of a phrase. In figure 3(a), for instance, the definiteness feature (marked as D)</context>
</contexts>
<marker>Danon, 2001</marker>
<rawString>G. Danon. 2001. Syntactic Definiteness in the Grammar of Modern Hebrew. Linguistics, 6(39):1071–1116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
<author>F Keller</author>
</authors>
<title>Probabilistic Parsing for German using Sister-Head Dependencies.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="3921" citStr="Dubey and Keller, 2003" startWordPosition="562" endWordPosition="565">Linguistics Klein and Manning (2003) systematize the distinction between these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, accuracy results for parsing languages other than English still lag behind.1 We propose that for various languages including the Semitic family, e.g. Modern Hebrew (MH) and Modern Standard Arabic (MSA), a third dimension of parametrization is necessary for encoding linguistic information relevant for breaking false independence assumptions. In Semitic languages, arguments may move around rather freely and the phrase-structure of clause-level categories is often shallow. For such languages agreement features play a rol</context>
<context position="5583" citStr="Dubey and Keller, 2003" startWordPosition="800" endWordPosition="803">ines syntactic categories based on their morpho-syntactic role, and captures linguistically motivated co-occurrences and dependencies manifested via, e.g., morpho-syntactic agreement. This work aims at parsing MH and explores the empirical contribution of the three dimensions of parameters specified above. We present extensive experiments that gradually lead to improved performance as we extend the degree to which the three dimensions are exploited. Our best model uses all three dimensions of parametrization, and our best re1The learning curves over increasing training data (e.g., for German (Dubey and Keller, 2003)) show that treebank size cannot be the sole factor to account for the inferior performance. sult is on a par with those achieved for MSA using a fully lexicalized parser and a much larger treebank. The remainder of this document is organized as follows. In section 2 we review characteristic aspects of MH (and other Semitic languages) and illustrate the special role of morphology and dependencies displayed by morpho-syntactic processes using the case of syntactic definiteness in MH. In section 3 we define our three-dimensional parametrization space. In section 4 we spell out the method and pro</context>
</contexts>
<marker>Dubey, Keller, 2003</marker>
<rawString>A. Dubey and F. Keller. 2003. Probabilistic Parsing for German using Sister-Head Dependencies. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Adler</author>
<author>M Elhadad</author>
</authors>
<title>Noun Phrase Chunking in Hebrew: Influence of Lexical and Morphological Features.</title>
<date>2006</date>
<booktitle>In Proceedings of COLINGACL.</booktitle>
<contexts>
<context position="9590" citStr="Goldberg et al., 2006" startWordPosition="1432" endWordPosition="1435">n figure 1, for example, it is agreement on gender and number that reveals the subject-predicate dependency between surface forms. Figure 1 also shows that agreement features help to reveal such relations between higher levels of constituents as well. Determining the child constituents that contribute each of the features is not a trivial matter either. To illustrate the extent and the complexity of that matter let us consider definiteness in MH, which is morphologically marked (as an h prefix to the stem, glossed here explicitly as “the-”) and behaves as a syntactic 3See (Wintner, 2000) and (Goldberg et al., 2006) for formal and statistical accounts (respectively) of noun phrases in MH. Figure 2: Definiteness in MH as a Phrase-Level Agreement Feature: Agreement on definiteness helps to determine the internal structure of a higher level NP (a), and the absence thereof helps to determine the attachment to a predicate in a verb-less sentence (b) (marking D(efiniteness)) Figure 3: Phrase-Level Agreement Features and HeadDependencies in MH: The direction of percolating definiteness in MH is distinct of that of the head (marking (head-tag)) property (Danon, 2001). Definite noun-phrases exhibit agreement with</context>
<context position="39477" citStr="Goldberg et al., 2006" startWordPosition="6087" endWordPosition="6090">report in table 7 our results on section 12 of the treebank. The performance has slightly increased and we obtain better results for our best strategy. We retain the high error-reduction rate and propose our best result, 75.25% for sentences of length ≤ 40, as an empirically established string baseline on the performance of treebank grammars for MH. 6 Related Work The MH treebank (Sima’an et al., 2001), a morphologically and syntactically annotated corpus, has been successfully used for various NLP tasks such as morphological disambiguation, POS tagging (BarHaim et al., 2007) and NP chunking (Goldberg et al., 2006). However its use for statistical parsing has been more scarce and less successful. The only previous studies attempting to parse MH we know of are (Sima’an et al., 2001), applying a variation of the DOP tree-gram model to 500 sentences, and (Tsarfaty, 2006), using a treebank PCFG in an integrated system for morphological and syntactic disambiguation.9 The adaptation of state-of-the-art parsing models to MH is not immediate as the flat variable structures of phrases are hard to parse and a plentiful of morphological features that would facilitate disambiguation are not exploited by currently a</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2006</marker>
<rawString>Y. Goldberg, M. Adler, and M. Elhadad. 2006. Noun Phrase Chunking in Hebrew: Influence of Lexical and Morphological Features. In Proceedings of COLINGACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Hageloh</author>
</authors>
<title>Parsing using Transforms over Treebanks. Master’s thesis,</title>
<date>2007</date>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="24905" citStr="Hageloh, 2007" startWordPosition="3817" endWordPosition="3818"> size of our data set we only use the values 10, 11 as possible instantiations.) The v dimension is implemented using a transform as in (Johnson, 1998) where v = 0 corresponds to bare syntactic categories and v = 1 augments node labels with the label of their parent node. The h dimension is peculiar in that it distinguishes PCFGs (h = oc), where RHS cannot decompose, from their head-driven unlexicalized variety. To implement h =� oc we use a PCFG transformation emulating (Collins, 2003)’s first model, in which sisters are generated conditioned on the head tag and a simple ‘distance’ function (Hageloh, 2007).7 The inprocess. As the MH treebank is approaching maturity we feel that the time is ripe to standardize its use for MH statistical parsing. The software we implemented will be made available for non-commercial use upon request to the author(s) and the feature percolation software by (Krymolowski et al., 2007) is publicly available through the Knowledge Center for Processing Hebrew. By this we hope to increase the interest in MH within the parsing community and to facilitate the application of more sophisticated models by cutting down on setup time. 6Marked as “NO MATCH” in the treebank. 7A f</context>
<context position="30153" citStr="Hageloh, 2007" startWordPosition="4649" endWordPosition="4650"> a new set of categories encoding partial derivations. Lexicalize A pre-condition for applying horizontal parameterizations a` la Collins is the annotation of heads of syntactic phrases. The treebank provided by the knowledge center does not define unique heads, but rather, mark multiple dependencies for some categories and none for others. Our software uses rules for choosing the syntactic head according to specified dependencies and a head table when none are specified. Linearize In order to implement the head-outward constituents’ generation process we use software made available to us by (Hageloh, 2007) which converts PCFG production such as the generation of a head is followed by left and right markovized derivation processes. We used two versions of Markovization, one which conditions only on the head and a distance function, and another which conditions also on immediately neighboring sister(s). Decorate Our software implements an additional general transform which selects the features that are to be annotated on top of syntactic categories to implement various parametrization decisions. This transform can be used for, e.g., displaying parent information, selecting morphological features,</context>
</contexts>
<marker>Hageloh, 2007</marker>
<rawString>F. Hageloh. 2007. Parsing using Transforms over Treebanks. Master’s thesis, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG Models of Linguistic Tree Representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="1874" citStr="Johnson, 1998" startWordPosition="266" endWordPosition="267">t introduces a new, stronger, lower bound on the performance of treebank grammars for parsing Modern Hebrew, and is on a par with current results for parsing Modern Standard Arabic obtained by a fully lexicalized parser trained on a much larger treebank. 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the naive treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. The factor that sets apart vanilla PCFGs (Charniak, 1996) from their unlexicalized extensions proposed by, e.g., (Johnson, 1998; Klein and Ma</context>
<context position="22717" citStr="Johnson, 1998" startWordPosition="3447" endWordPosition="3448">c extensions of such constraints-based grammatical formalisms (cf. (Abney, 1997)) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures. To the best of our knowledge, this proposal has not been empirically explored before. 4 Experimental Setup Our goal is to determine the optimal strategy for learning treebank grammars for MH and to contrast it with bi-dimensional strategies explored for English. The methodology we use is adopted from (Klein and Manning, 2003) and our procedure is identical to the one described in (Johnson, 1998). We define transformations over the treebank that accept as input specific points in the (h, v, d) space depicted in figure 7. We use the transformed training sets for learning different treebank PCFGs which we then used to parse unseen sentences, and detransform the parses for the purpose of evaluation.5 5Previous studied on MH used different portions of the treebank and its annotation scheme due to its gradual development 161 Data We use version 2.0 of the MH treebank which consists of 6501 sentences from the daily newspaper ‘Ha’aretz’. We employ the syntactic categories, POS categories and</context>
<context position="24442" citStr="Johnson, 1998" startWordPosition="3737" endWordPosition="3738"> sentences (53) and a testset of 496 sentences (58). Since this work is only the first step towards the development of a broad-coverage statistical parser for MH (and other Semitic languages) we use the development set for parameter-tuning and error analysis and use the test set only for confirming our best results. Models The models we implement use one-, twoor three-dimensional parametrization and different instantiation of values thereof. (Due to the small size of our data set we only use the values 10, 11 as possible instantiations.) The v dimension is implemented using a transform as in (Johnson, 1998) where v = 0 corresponds to bare syntactic categories and v = 1 augments node labels with the label of their parent node. The h dimension is peculiar in that it distinguishes PCFGs (h = oc), where RHS cannot decompose, from their head-driven unlexicalized variety. To implement h =� oc we use a PCFG transformation emulating (Collins, 2003)’s first model, in which sisters are generated conditioned on the head tag and a simple ‘distance’ function (Hageloh, 2007).7 The inprocess. As the MH treebank is approaching maturity we feel that the time is ripe to standardize its use for MH statistical pars</context>
<context position="42245" citStr="Johnson, 1998" startWordPosition="6520" endWordPosition="6521">e use of different training sets, different annotation conventions, and different evaluation schemes. 165 Figure 7: All Models: Locating Unlexicalized Parsing Models Figure 8: All Results: Parsing Results for Unlexicalized Modin a Three-Dimensional Parametrization Space els in a Three-Dimensional Parametrization Space rameter space, and applications to more languages. 7 Conclusion Morphologically rich languages introduce a new dimension into the expansion possibilities of a nonterminal node in a syntactic parse tree. This dimension is orthogonal to the vertical (Collins, 2003) and horizontal (Johnson, 1998) dimensions previously outlined by Klein and Manning (2003), and it cannot be collapsed into any one of the previous two. These additional dependencies exist alongside the syntactic head dependency and are attested using morphosyntactic phenomena such as long distance agreement. We demonstrate using syntactic definiteness in MH that incorporating morphologically marked features as a third, orthogonal dimension for annotating syntactic categories is invaluable for weakening the independence assumptions implicit in a treebank PCFG and increasing the model’s disambiguation capabilities. Using a t</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG Models of Linguistic Tree Representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>Lexical-Functional Grammar: A formal system for grammatical representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<editor>In J. Bresnan, editor,</editor>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21696" citStr="Kaplan and Bresnan, 1982" startWordPosition="3290" endWordPosition="3294">dividual category-splits are viewed as taking place in a twodimensional space and it is hard to analyze and empirically evaluate their interaction with other annotation strategies. Here we propose a principled way to statistically model the interaction between different linguistic processes that license grammatical structures and empirically contrast their contribution. 3.2 A Note on Stochastic AV grammars The practice of having morphological features orthogonal to a constituency structure is not a new one and is familiar from formal theories of syntax such as HPSG (Sag et al., 2003) and LFG (Kaplan and Bresnan, 1982). Here we propose to reframe systematic morphological decoration of syntactic categories at all levels of the hierarchy as (a) (b) Figure 6: The Expansion Possibilities of a Non-Terminal Node: Expanding the NP from figure 4 in a three-dimensional parameterization Space an additional dimension of statistical estimation for learning unlexicalized treebank PCFGs. Our proposal deviates from various stochastic extensions of such constraints-based grammatical formalisms (cf. (Abney, 1997)) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>R. Kaplan and J. Bresnan. 1982. Lexical-Functional Grammar: A formal system for grammatical representation. In J. Bresnan, editor, The Mental Representation of Grammatical Relations, Cambridge, MA. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="1900" citStr="Klein and Manning, 2003" startWordPosition="268" endWordPosition="271">new, stronger, lower bound on the performance of treebank grammars for parsing Modern Hebrew, and is on a par with current results for parsing Modern Standard Arabic obtained by a fully lexicalized parser trained on a much larger treebank. 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the naive treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. The factor that sets apart vanilla PCFGs (Charniak, 1996) from their unlexicalized extensions proposed by, e.g., (Johnson, 1998; Klein and Manning, 2003), is the choic</context>
<context position="3334" citStr="Klein and Manning (2003)" startWordPosition="475" endWordPosition="478">t, proposed by (Johnson, 1998), is the annotation of parental history, and the second encodes a head-outward generation process (Collins, 2003). Johnson (1998) augments node labels with the label of their parent, thus incorporating a dependency on the node’s grandparent. Collins (2003) proposes to generate the head of a phrase first and then generate its sisters using Markovian processes, thereby exploiting head/sister-dependencies. 156 Proceedings of the 10th Conference on Parsing Technologies, pages 156–167, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics Klein and Manning (2003) systematize the distinction between these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Colli</context>
<context position="15655" citStr="Klein and Manning, 2003" startWordPosition="2357" endWordPosition="2360">s and additional labels that mark functional features (e.g., SBJ,OBJ) which we strip off and ignore throughout this study. Morphological features percolated up the tree manifest dependencies that are marked locally yet have a global effect. We propose to learn treebank grammars in which the syntactic categories are augmented with morphological features at all levels of the hierarchy. This allows to learn finer-grained categories with subtle differences in their syntactic behavior and to capture non-independence between certain parts of the syntactic parse-tree. 3 Refining the Parameter Space (Klein and Manning, 2003) argue that parent encoding on top of syntactic categories and RHS markovization of CFG productions are two instances of the same idea, namely that of encoding the generation history of a node to a varying degree. They subsequently describe two dimensions that define their parameters’ space. The vertical dimension (v), capturing the history of the node’s ancestors in a topDEP HEAD all DEP MAJOR at least gender number DEP NUMBER definiteness DEP DEFINITE DEP ACCUSATIVE case all (e.g., conjunction) DEP MULTIPLE masculine feminine both singular plural both definiteness:H definiteness:U 159 down g</context>
<context position="17648" citStr="Klein and Manning (2003)" startWordPosition="2684" endWordPosition="2687">nstituent thus restoring selected dependencies: P(ri) = P(ri|IF0(r1 o .. o ri−1)) The horizontal dimension h can be thought of as two functions IF1, IF2 over decomposed rules, where IF1 selects hidden internal features of the parent, and IF2 selects previously generated sisters in a headoutward Markovian process (we retain here the assumption that the head child H always matters). P(ri) = Ph(H|IF1(LHS(ri))) 11 X PC(C|IF2(RHS(ri)), H) CERHS(rz)−H The fact that the default notion of a treebank grammar takes v = 1 (i.e., IF0(r1 o .. o ri−1) = 0) and h = oo (RHS cannot decompose) is, according to Klein and Manning (2003), a historical accident. We claim that languages with freeer word order and richer morphology call for an additional dimension of parametrization. The additional parameter shows to what extent morphological features encoded in a specialized structure back up the derivation of the tree. This dimension can be thought of as a function IF3 selecting aspects of morphological orthogonal analysis of the rules, where MA denotes morphological analysis of the syntactic categories in both LHS and RHS of the rule. P(ri) = P(ri|IF3(MA(ri))) The fact that in current parsers 43(MA(ri)) = 0 is, we claim, anot</context>
<context position="19152" citStr="Klein and Manning (2003)" startWordPosition="2909" endWordPosition="2912">ametrization shall not suffice. The emerging picture is as follows. Bare-category skeletons reside in a bi-dimensional parametrization space (figure 3(a)) in which the vertical (figure 3(b)) and horizontal (figure 3(c)) parameter instantiations elaborate the generation history of a non-terminal node. Specialized structures enriched with (an increasing amount of) morphological features reside deeper along a third dimension we refer to as depth (d). Figure 4 illustrates an instantiation of d = 1 with a single definiteness feature. Higher d values would imply adding more (accumulating) features. Klein and Manning (2003) view the vertical and horizontal parametrization dimensions as implementing external and internal annotation strategies respectively. External parameters indicate features of the external environment that influence the node’s expansion possibilities, and internal parameters mark aspects of hidden internal content which influence constituents’ external distribution. We view the third dimension of parametrization as implementing a relational strategy of annotation encoding the way different constituents may combine to form phrases and sentences. In a bottom up process this annotation strategy i</context>
<context position="20375" citStr="Klein and Manning, 2003" startWordPosition="3085" endWordPosition="3088">mposes soft constraints on a the top-down head-outward generation process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how the depth expansion interacts with both parent anno160 (a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing </context>
<context position="22646" citStr="Klein and Manning, 2003" startWordPosition="3433" endWordPosition="3436">arning unlexicalized treebank PCFGs. Our proposal deviates from various stochastic extensions of such constraints-based grammatical formalisms (cf. (Abney, 1997)) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures. To the best of our knowledge, this proposal has not been empirically explored before. 4 Experimental Setup Our goal is to determine the optimal strategy for learning treebank grammars for MH and to contrast it with bi-dimensional strategies explored for English. The methodology we use is adopted from (Klein and Manning, 2003) and our procedure is identical to the one described in (Johnson, 1998). We define transformations over the treebank that accept as input specific points in the (h, v, d) space depicted in figure 7. We use the transformed training sets for learning different treebank PCFGs which we then used to parse unseen sentences, and detransform the parses for the purpose of evaluation.5 5Previous studied on MH used different portions of the treebank and its annotation scheme due to its gradual development 161 Data We use version 2.0 of the MH treebank which consists of 6501 sentences from the daily newsp</context>
<context position="42304" citStr="Klein and Manning (2003)" startWordPosition="6527" endWordPosition="6530">ation conventions, and different evaluation schemes. 165 Figure 7: All Models: Locating Unlexicalized Parsing Models Figure 8: All Results: Parsing Results for Unlexicalized Modin a Three-Dimensional Parametrization Space els in a Three-Dimensional Parametrization Space rameter space, and applications to more languages. 7 Conclusion Morphologically rich languages introduce a new dimension into the expansion possibilities of a nonterminal node in a syntactic parse tree. This dimension is orthogonal to the vertical (Collins, 2003) and horizontal (Johnson, 1998) dimensions previously outlined by Klein and Manning (2003), and it cannot be collapsed into any one of the previous two. These additional dependencies exist alongside the syntactic head dependency and are attested using morphosyntactic phenomena such as long distance agreement. We demonstrate using syntactic definiteness in MH that incorporating morphologically marked features as a third, orthogonal dimension for annotating syntactic categories is invaluable for weakening the independence assumptions implicit in a treebank PCFG and increasing the model’s disambiguation capabilities. Using a three-dimensional model we establish a new, stronger, lower </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings ofACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Krymolowski</author>
<author>Y Adiel</author>
<author>N Guthmann</author>
<author>S Kenan</author>
<author>A Milea</author>
<author>N Nativ</author>
<author>R Tenzman</author>
<author>P Veisberg</author>
</authors>
<title>Treebank Annotation Guide. MILA, Knowledge Center for Hebrew Processing.</title>
<date>2007</date>
<contexts>
<context position="13086" citStr="Krymolowski et al., 2007" startWordPosition="1961" endWordPosition="1965"> that the yields of the syntactic trees do not correspond to space delimited words but rather to morphological segments that carry distinct syntactic roles, i.e., each segment corresponds to a single POS tag. (This in turn means that prefixes marking determiners, relativizers, prepositions and definite articles are segmented away and appear as leaves in a syntactic parse tree.) The POS categories assigned to segmented words are decorated with features such as gender, number, person and tense, and these features are percolated higher up the tree according to pre-defined syntactic dependencies (Krymolowski et al., 2007). Since agreement features of non-terminal constituents may be contributed by more than one child, the annotation scheme defines multiple dependency labels that guide the percolation of the different features higher up the tree. Definiteness in the MH treebank is treated as a segment at the POS tags level and as a feature at the level of non-terminals. As any other feature, it is percolated higher up the tree according to marked dependency labels. Table 1 lists the features and values annotated on top of syntactic categories and table 2 describes the dependencies according to which these featu</context>
<context position="14402" citStr="Krymolowski et al., 2007" startWordPosition="2172" endWordPosition="2175">exible phrase structure in MH, clausal categories (S, SBAR and FRAG and their corresponding interrogatives SQ, SQBAR and FRAGQ) are annotated as flat structures. Verbs (VB tags) always attach to a VP mother, however only non-finite VBs can accept complements under the same VP parent, meaning that all inflected verb forms are represented as unary productions under an inflected VP. NP and PP are annotated 4Version 2.0 of the MH treebank is publicly available at http://mila.cs.technion.ac.il/english/ index.html along with a complete overview of the MH annotation scheme and illustrative examples (Krymolowski et al., 2007). Feature:Value Value Encoded gender:Z gender:N gender:B number:Y number:R number:B definite underspecified Table 1: Features and Values in the MH Treebank Dependency Type Features Percolated Table 2: Dependency Labels in the MH Treebank as nested structures capturing the recursive structure of construct-state nouns, numerical expressions and possession. An additional category, PREDP, is added in the treebank scheme to account for sentences in MH that lack a copular element, and it may also be decorated with inflectional features agreeing with the subject. The MH treebank scheme also features </context>
<context position="25217" citStr="Krymolowski et al., 2007" startWordPosition="3866" endWordPosition="3869">in that it distinguishes PCFGs (h = oc), where RHS cannot decompose, from their head-driven unlexicalized variety. To implement h =� oc we use a PCFG transformation emulating (Collins, 2003)’s first model, in which sisters are generated conditioned on the head tag and a simple ‘distance’ function (Hageloh, 2007).7 The inprocess. As the MH treebank is approaching maturity we feel that the time is ripe to standardize its use for MH statistical parsing. The software we implemented will be made available for non-commercial use upon request to the author(s) and the feature percolation software by (Krymolowski et al., 2007) is publicly available through the Knowledge Center for Processing Hebrew. By this we hope to increase the interest in MH within the parsing community and to facilitate the application of more sophisticated models by cutting down on setup time. 6Marked as “NO MATCH” in the treebank. 7A formal overview of the transformation and its correspondence to (Collins, 2003)’s models is available at (Hageloh, 2007). We use the distance function defined therein, marking the direction and whether it is the first node to be generated. stantiated value of h then selects the number of previously generated (no</context>
</contexts>
<marker>Krymolowski, Adiel, Guthmann, Kenan, Milea, Nativ, Tenzman, Veisberg, 2007</marker>
<rawString>Y. Krymolowski, Y. Adiel, N. Guthmann, S. Kenan, A. Milea, N. Nativ, R. Tenzman, and P. Veisberg. 2007. Treebank Annotation Guide. MILA, Knowledge Center for Hebrew Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
</authors>
<title>Developing an Arabic Treebank: Methods, Guidelines, Procedures, and Tools.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="40224" citStr="Maamouri and Bies, 2004" startWordPosition="6210" endWordPosition="6213"> parse MH we know of are (Sima’an et al., 2001), applying a variation of the DOP tree-gram model to 500 sentences, and (Tsarfaty, 2006), using a treebank PCFG in an integrated system for morphological and syntactic disambiguation.9 The adaptation of state-of-the-art parsing models to MH is not immediate as the flat variable structures of phrases are hard to parse and a plentiful of morphological features that would facilitate disambiguation are not exploited by currently available parsers. Also, the MH treebank is much smaller than the ones for, e.g., English (Marcus et al., 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the allsubtrees approach (Bod, 1992) or full lexicalization (Collins, 2003). Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH treebank. This work has shown that devising an adequate baseline for parsing MH requires more than simple category-splits and sophisticated head-driven extensions, and o</context>
</contexts>
<marker>Maamouri, Bies, 2004</marker>
<rawString>M. Maamouri and A. Bies. 2004. Developing an Arabic Treebank: Methods, Guidelines, Procedures, and Tools. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>M Marcinkiewicz</author>
<author>R MacIntyre</author>
<author>A Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating PredicateArgument Structure.</title>
<date>1994</date>
<contexts>
<context position="12404" citStr="Marcus et al., 1994" startWordPosition="1853" endWordPosition="1856"> NPhNNTi.FS.D VPhVi).FS V.FS V.FS NNT.FS sganit deputy.FS N.MS.D hmnhl the-manager.MS.D htpjrh resigned.FS NNT.FS sganit deputy.FS N.MS.D hmnhl the-manager.MS.D htpjrh resigned.FS (a) S (b) ShVi sni hildim two.MP the-children.MP (b) S aklw ate.MP NP.MP-SBJ CD.MP N.MP VP.MP V.MP 158 2.2 The Modern Hebrew Treebank Scheme The annotation scheme of version 2.0 of the MH treebank (Sima’an et al., 2001)4 aims to capture the morphological and syntactic properties of MH just described. This results in several aspects that distinguish the MH treebank from, e.g., the WSJ Penn treebank annotation scheme (Marcus et al., 1994). The MH treebank is built over word segments. This means that the yields of the syntactic trees do not correspond to space delimited words but rather to morphological segments that carry distinct syntactic roles, i.e., each segment corresponds to a single POS tag. (This in turn means that prefixes marking determiners, relativizers, prepositions and definite articles are segmented away and appear as leaves in a syntactic parse tree.) The POS categories assigned to segmented words are decorated with features such as gender, number, person and tense, and these features are percolated higher up t</context>
<context position="40187" citStr="Marcus et al., 1994" startWordPosition="6204" endWordPosition="6207">ly previous studies attempting to parse MH we know of are (Sima’an et al., 2001), applying a variation of the DOP tree-gram model to 500 sentences, and (Tsarfaty, 2006), using a treebank PCFG in an integrated system for morphological and syntactic disambiguation.9 The adaptation of state-of-the-art parsing models to MH is not immediate as the flat variable structures of phrases are hard to parse and a plentiful of morphological features that would facilitate disambiguation are not exploited by currently available parsers. Also, the MH treebank is much smaller than the ones for, e.g., English (Marcus et al., 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the allsubtrees approach (Bod, 1992) or full lexicalization (Collins, 2003). Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH treebank. This work has shown that devising an adequate baseline for parsing MH requires more than simple category-splits and sophis</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn Treebank: Annotating PredicateArgument Structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with Latent Annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05.</booktitle>
<contexts>
<context position="20399" citStr="Matsuzaki et al., 2005" startWordPosition="3089" endWordPosition="3093">n a the top-down head-outward generation process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how the depth expansion interacts with both parent anno160 (a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the catego</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with Latent Annotations. In Proceedings of ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Melnik</author>
</authors>
<title>Verb-Initial Constructions in Modern Hebrew.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Berkeley University of California.</institution>
<contexts>
<context position="7831" citStr="Melnik, 2002" startWordPosition="1162" endWordPosition="1163">thogonal morphosyntactic dimension which is invaluable for syntactic disambiguation, and it can be effectively learned using simple treebank grammars. 2.1 Modern Hebrew Structure Phrases and sentences in MH, as well as in Arabic and other Semitic languages, have a relatively flexible phrase structure. Subjects, verbs and objects can be inverted and prepositional phrases, adjuncts and verbal modifiers can move around rather freely. The factors that affect word-order in the language are not exclusively syntactic and have to do with rhetorical and pragmatic factors as well.2 2See, for instance, (Melnik, 2002) for an Information Structure-syntactic account of verb initial sentences. 157 (a) S NP.FS-OBJ N.FS ewgh cake.FS Figure 1: Word Order and Agreement Features in MH Phrases: Agreement on MP features reveals the subjectpredicate dependency between surface forms and their dominating constituents in a variable phrase-structure (marking M(asculine), F(eminine), S(ingular), P(lural).) It would be too strong a claim, however, to classify MH (and similar languages) as a free-wordorder language in the canonical sense. The level of freedom in the order and number of internal constituents varies between s</context>
</contexts>
<marker>Melnik, 2002</marker>
<rawString>N. Melnik. 2002. Verb-Initial Constructions in Modern Hebrew. Ph.D. thesis, Berkeley University of California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-COLING,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="20437" citStr="Petrov et al., 2006" startWordPosition="3096" endWordPosition="3099"> process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how the depth expansion interacts with both parent anno160 (a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the categories as well as linguistically motivat</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of ACL-COLING, pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Prescher</author>
</authors>
<title>Head-Driven PCFGs with LatentHead Statistics. In</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="20415" citStr="Prescher, 2005" startWordPosition="3094" endWordPosition="3095">tward generation process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how the depth expansion interacts with both parent anno160 (a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the categories as well as </context>
</contexts>
<marker>Prescher, 2005</marker>
<rawString>D. Prescher. 2005. Head-Driven PCFGs with LatentHead Statistics. In In Proceedings of the International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Sag</author>
<author>T Wasow</author>
<author>E M Bender</author>
</authors>
<title>Syntactic Theory: A Formal Introduction.</title>
<date>2003</date>
<publisher>CSLI Publications,</publisher>
<note>address, second edition.</note>
<contexts>
<context position="21661" citStr="Sag et al., 2003" startWordPosition="3284" endWordPosition="3287">ccurrences between them. Individual category-splits are viewed as taking place in a twodimensional space and it is hard to analyze and empirically evaluate their interaction with other annotation strategies. Here we propose a principled way to statistically model the interaction between different linguistic processes that license grammatical structures and empirically contrast their contribution. 3.2 A Note on Stochastic AV grammars The practice of having morphological features orthogonal to a constituency structure is not a new one and is familiar from formal theories of syntax such as HPSG (Sag et al., 2003) and LFG (Kaplan and Bresnan, 1982). Here we propose to reframe systematic morphological decoration of syntactic categories at all levels of the hierarchy as (a) (b) Figure 6: The Expansion Possibilities of a Non-Terminal Node: Expanding the NP from figure 4 in a three-dimensional parameterization Space an additional dimension of statistical estimation for learning unlexicalized treebank PCFGs. Our proposal deviates from various stochastic extensions of such constraints-based grammatical formalisms (cf. (Abney, 1997)) and has the advantage of elegantly bypassing the issue of loosing probabilit</context>
</contexts>
<marker>Sag, Wasow, Bender, 2003</marker>
<rawString>I. A. Sag, T. Wasow, and E. M. Bender. 2003. Syntactic Theory: A Formal Introduction. CSLI Publications, address, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmidt</author>
</authors>
<title>Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="27429" citStr="Schmidt, 2004" startWordPosition="4232" endWordPosition="4233">ed headdriven baseline a` la (Collins, 2003) located on the (0, 0, 0) coordinate. We transform the treebank trees in correspondence with different points in the threedimensional space defined by (h, v, d). The models we implement are marked in the coordinate-system depicted in figure 7. The implementation details of the transformations we use are spelled out in tables 3–4. Procedure We implement different models that correspond to different instantiations of h, v and d. For each instantiation we transform the training set and learn a PCFG using Maximum Likelihood estimates, and we use BitPar (Schmidt, 2004), an efficient general-purpose parser, to parse unseen sentences. The input to the parser is a sequence of word segments where each segment corresponds to a single POS tag, possibly decorated with morphological features. This setup assumes partial morphological disambiguation (namely, segmentation) but crucially we do not disambiguate their respective POS categories. This setup is more appropriate for using general-purpose parsing tools and it makes our results comparable to studies in other languages.8 8Our working assumption is that better performance of a parsing model in our setup will imp</context>
</contexts>
<marker>Schmidt, 2004</marker>
<rawString>H. Schmidt. 2004. Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors. In Proceedings of COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima’an</author>
<author>A Itai</author>
<author>Y Winter</author>
<author>A Altman</author>
<author>N Nativ</author>
</authors>
<title>Building a Tree-Bank of Modern Hebrew Text. In Traitment Automatique des Langues.</title>
<date>2001</date>
<marker>Sima’an, Itai, Winter, Altman, Nativ, 2001</marker>
<rawString>K. Sima’an, A. Itai, Y. Winter, A. Altman, and N. Nativ. 2001. Building a Tree-Bank of Modern Hebrew Text. In Traitment Automatique des Langues.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tsarfaty</author>
</authors>
<title>Integrated Morphological and Syntactic Disambiguation for Modern Hebrew.</title>
<date>2006</date>
<booktitle>In Proceeding of SRW COLING-ACL.</booktitle>
<contexts>
<context position="20566" citStr="Tsarfaty, 2006" startWordPosition="3118" endWordPosition="3119"> Figure 6(b) illustrates how the depth expansion interacts with both parent anno160 (a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the categories as well as linguistically motivated co-occurrences between them. Individual category-splits are viewed as taking place in a twodimensional space and it is hard to</context>
<context position="32011" citStr="Tsarfaty, 2006" startWordPosition="4916" endWordPosition="4917">gainst the gold parses to evaluate parsing accuracy. Evaluation We evaluate our models using EVALB in accordance with standard PARSEVAL evaluation metrics. The evaluation of all models focuses on Labeled Precision and Recall considering bare syntactic categories (stripping off all morphological or parental features and removing intermediate nodes for linearization). We report the average F-measure for sentences of length up to 40 and for all sentences (F&lt;40 and FAII respectively). We report the results within an integrated model for morphological and syntactic disambiguation in the spirit of (Tsarfaty, 2006). We conjecture that the kind of models developed here which takes into account morphological information is more appropriate for the morphological disambiguation task defined therein. for two evaluation options, once including punctuation marks (WP) and once excluding them (WOP). 5 Results Our baseline for the first set of experiments is a vanilla PCFG as described in (Charniak, 1996) (without a preceding POS tagging phase and without right branching corrections). We transform the treebank trees based on various points in the (∞, v, d) two-dimensional space to evaluate the performance of the </context>
<context position="39735" citStr="Tsarfaty, 2006" startWordPosition="6133" endWordPosition="6135">mpirically established string baseline on the performance of treebank grammars for MH. 6 Related Work The MH treebank (Sima’an et al., 2001), a morphologically and syntactically annotated corpus, has been successfully used for various NLP tasks such as morphological disambiguation, POS tagging (BarHaim et al., 2007) and NP chunking (Goldberg et al., 2006). However its use for statistical parsing has been more scarce and less successful. The only previous studies attempting to parse MH we know of are (Sima’an et al., 2001), applying a variation of the DOP tree-gram model to 500 sentences, and (Tsarfaty, 2006), using a treebank PCFG in an integrated system for morphological and syntactic disambiguation.9 The adaptation of state-of-the-art parsing models to MH is not immediate as the flat variable structures of phrases are hard to parse and a plentiful of morphological features that would facilitate disambiguation are not exploited by currently available parsers. Also, the MH treebank is much smaller than the ones for, e.g., English (Marcus et al., 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the allsubtrees approach (Bod, 1992) or full lexicaliz</context>
</contexts>
<marker>Tsarfaty, 2006</marker>
<rawString>R. Tsarfaty. 2006. Integrated Morphological and Syntactic Disambiguation for Modern Hebrew. In Proceeding of SRW COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wintner</author>
</authors>
<title>Definiteness in the Hebrew Noun Phrase.</title>
<date>2000</date>
<journal>Journal ofLinguistics,</journal>
<pages>36--319</pages>
<contexts>
<context position="9562" citStr="Wintner, 2000" startWordPosition="1429" endWordPosition="1430">een surface forms. In figure 1, for example, it is agreement on gender and number that reveals the subject-predicate dependency between surface forms. Figure 1 also shows that agreement features help to reveal such relations between higher levels of constituents as well. Determining the child constituents that contribute each of the features is not a trivial matter either. To illustrate the extent and the complexity of that matter let us consider definiteness in MH, which is morphologically marked (as an h prefix to the stem, glossed here explicitly as “the-”) and behaves as a syntactic 3See (Wintner, 2000) and (Goldberg et al., 2006) for formal and statistical accounts (respectively) of noun phrases in MH. Figure 2: Definiteness in MH as a Phrase-Level Agreement Feature: Agreement on definiteness helps to determine the internal structure of a higher level NP (a), and the absence thereof helps to determine the attachment to a predicate in a verb-less sentence (b) (marking D(efiniteness)) Figure 3: Phrase-Level Agreement Features and HeadDependencies in MH: The direction of percolating definiteness in MH is distinct of that of the head (marking (head-tag)) property (Danon, 2001). Definite noun-ph</context>
</contexts>
<marker>Wintner, 2000</marker>
<rawString>S. Wintner. 2000. Definiteness in the Hebrew Noun Phrase. Journal ofLinguistics, 36:319–363.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>