<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000346">
<title confidence="0.989314">
Using Syntactic Information to Identify Plagiarism
</title>
<author confidence="0.98142">
¨Ozlem Uzuner, Boris Katz, and Thade Nahnsen
</author>
<affiliation confidence="0.961115">
Massachusetts Institute of Technology
Computer Science and Artificial Intelligence Laboratory
</affiliation>
<address confidence="0.91655">
Cambridge, MA 02139
</address>
<email confidence="0.999545">
ozlem,boris,tnahnsen@csail.mit.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951592592593">
Using keyword overlaps to identify pla-
giarism can result in many false negatives
and positives: substitution of synonyms
for each other reduces the similarity be-
tween works, making it difficult to rec-
ognize plagiarism; overlap in ambiguous
keywords can falsely inflate the similar-
ity of works that are in fact different in
content. Plagiarism detection based on
verbatim similarity of works can be ren-
dered ineffective when works are para-
phrased even in superficial and immate-
rial ways. Considering linguistic informa-
tion related to creative aspects of writing
can improve identification of plagiarism
by adding a crucial dimension to evalu-
ation of similarity: documents that share
linguistic elements in addition to content
are more likely to be copied from each
other. In this paper, we present a set of
low-level syntactic structures that capture
creative aspects of writing and show that
information about linguistic similarities
of works improves recognition of plagia-
rism (over tfidf-weighted keywords alone)
when combined with similarity measure-
ments based on tfidf-weighted keywords.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955857142857">
To plagiarize is “to steal and pass off (the ideas
or words of another) as one’s own; [to] use (an-
other’s production) without crediting the source; [or]
to commit literary theft [by] presenting as new and
original an idea or product derived from an exist-
ing source”.&apos; Plagiarism is frequently encountered
in academic settings. According to turnitin.com, a
2001 survey of 4500 high school students revealed
that “15% [of students] had submitted a paper ob-
tained in large part from a term paper mill or web-
site”. Increased rate of plagiarism hurts quality of
education received by students; facilitating recog-
nition of plagiarism can help teachers control this
damage.
To facilitate recognition of plagiarism, in the re-
cent years many commercial and academic prod-
ucts have been developed. Most of these approaches
identify verbatim plagiarism and can fail when
works are paraphrased. To recognize plagiarism
in paraphrased works, we need to capture similar-
ities that go beyond keywords and verbatim over-
laps. Two works that exhibit similarity both in their
conceptual content (as indicated by keywords) and
in their expression of this content should be consid-
ered more similar than two works that are similar
only in content. In this context, content refers to
the story or the information; expression refers to the
linguistic choices of authors used in presenting the
content, i.e., creative elements of writing, such as
whether authors tend toward passive or active voice,
whether they prefer complex sentences with embed-
ded clauses or simple sentences with independent
clauses, as well as combinations of such choices.
Linguistic information can be a source of power
for measuring similarity between works based on
</bodyText>
<footnote confidence="0.9998615">
1www.webster.com
2www.turnitin.com
</footnote>
<page confidence="0.979998">
37
</page>
<note confidence="0.972044">
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 37–44, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.999648588235294">
their expression of content. In this paper, we use lin-
guistic information related to the creative aspects of
writing to improve recognition of paraphrased doc-
uments as a first step towards plagiarism detection.
To identify a set of features that relate to the linguis-
tic choices of authors, we rely on different syntactic
expressions of the same content. After identifying
the relevant features (which we call syntactic ele-
ments of expression), we rely on patterns in the use
of these features to recognize paraphrases of works.
In the absence of real-life plagiarism data, in this
paper, we use a corpus of parallel translations of
novels as surrogate for plagiarism data. Transla-
tions of titles, i.e., original works, into English by
different people provide us with books that are para-
phrases of the same content. We use these para-
phrases to automatically identify:
</bodyText>
<listItem confidence="0.972571333333333">
1. Titles even when they are paraphrased, and
2. Pairs of book chapters that are paraphrases of
each other.
</listItem>
<bodyText confidence="0.9999648">
Our first experiment shows that syntactic elements
of expression outperform all baselines in recogniz-
ing titles even when they are paraphrased, provid-
ing a way of recognizing copies of works based on
the similarities in their expression of content. Our
second experiment shows that similarity measure-
ments based on the combination of tfidf-weighted
keywords and syntactic elements of expression out-
perform the weighted keywords in recognizing pairs
of book chapters that are paraphrases of each other.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999985119047619">
We define expression as “the linguistic choices of
authors in presenting a particular content” (Uzuner,
2005; Uzuner and Katz, 2005). Linguistic similarity
between works has been studied in the text classifi-
cation literature for identifying the style of an author.
However, it is important to differentiate expression
from style. Style refers to the linguistic elements
that, independently of content, persist over the works
of an author and has been widely studied in author-
ship attribution. Expression involves the linguistic
elements that relate to how an author phrases par-
ticular content and can be used to identify potential
copyright infringement or plagiarism. Similarities
in the expression of similar content in two differ-
ent works signal potential copying. We hypothesize
that syntax plays a role in capturing expression of
content. Our approach to recognizing paraphrased
works is based on phrase structure of sentences in
general, and structure of verb phrases in particular.
Most approaches to similarity detection use com-
putationally cheap but linguistically less informed
features (Peng and Hengartner, 2002; Sichel, 1974;
Williams, 1975) such as keywords, function words,
word lengths, and sentence lengths; approaches that
include deeper linguistic information, such as syn-
tactic information, usually incur significant compu-
tational costs (Uzuner et al., 2004). Our approach
identifies useful linguistic information without in-
curring the computational cost of full text pars-
ing; it uses context-free grammars to perform high-
level syntactic analysis of part-of-speech tagged
text (Brill, 1992). It turns out that such a level of
analysis is sufficient to capture syntactic informa-
tion related to creative aspects of writing; this in
turn helps improve recognition of paraphrased doc-
uments. The results presented here show that ex-
traction of useful linguistic information for text clas-
sification purposes does not have to be computa-
tionally prohibitively expensive, and that despite the
tradeoff between the accuracy of features and com-
putational efficiency, we can extract linguistically-
informed features without full parsing.
</bodyText>
<sectionHeader confidence="0.976739" genericHeader="method">
3 Identifying Creative Aspects of Writing
</sectionHeader>
<bodyText confidence="0.954214833333333">
In this paper, we first identify linguistic elements
of expression and then study patterns in the use of
these elements to recognize a work even when it is
paraphrased. Translated literary works provide ex-
amples of linguistic elements that differ in expres-
sion but convey similar content. These works pro-
vide insight into the linguistic elements that capture
expression. For example, consider the following se-
mantically equivalent excerpts from three different
translations of Madame Bovary by Gustave Flaubert.
Excerpt 1: “Now Emma would often take it into
her head to write him during the day. Through her
window she would signal to Justin, and he would
whip off his apron and fly to la huchette. And when
Rodolphe arrived in response to her summons, it
was to hear that she was miserable, that her husband
was odious, that her life was a torment.” (Trans-
lated by Unknown1.)
</bodyText>
<page confidence="0.995736">
38
</page>
<bodyText confidence="0.978309">
Excerpt 2: “Often, even in the middle of the day,
Emma suddenly wrote to him, then from the win-
dow made a sign to Justin, who, taking his apron
off, quickly ran to la huchette. Rodolphe would
come; she had sent for him to tell him that she was
bored, that her husband was odious, her life fright-
ful.” (Translated by Aveling.)
Excerpt 3: “Often, in the middle of the day, Emma
would take up a pen and write to him. Then she
would beckon across to Justin, who would off with
his apron in an instant and fly away with the letter
to la huchette. And Rodolphe would come. She
wanted to tell him that life was a burden to her, that
she could not endure her husband and that things
were unbearable.” (Translated by Unknown2.)
Inspired by syntactic differences displayed in
such parallel translations, we identified a novel set
of syntactic features that relate to how people con-
vey content.
</bodyText>
<subsectionHeader confidence="0.997705">
3.1 Syntactic Elements of Expression
</subsectionHeader>
<bodyText confidence="0.999857076923077">
We hypothesize that given particular content, au-
thors choose from a set of semantically equivalent
syntactic constructs to express this content. To para-
phrase a work without changing content, people
try to interchange semantically equivalent syntactic
constructs; patterns in the use of various syntactic
constructs can be sufficient to indicate copying.
Our observations of the particular expressive
choices of authors in a corpus of parallel translations
led us to define syntactic elements of expression in
terms of sentence-initial and -final phrase structures,
semantic classes and argument structures of verb
phrases, and syntactic classes of verb phrases.
</bodyText>
<subsectionHeader confidence="0.75466">
3.1.1 Sentence-initial and -final phrase
structures
</subsectionHeader>
<bodyText confidence="0.999521">
The order of phrases in a sentence can shift the
emphasis of a sentence, can attract attention to par-
ticular pieces of information and can be used as an
expressive tool.
</bodyText>
<listItem confidence="0.968703555555556">
1 (a) Martha can finally put some money in the bank.
(b) Martha can put some money in the bank, finally.
(c) Finally, Martha can put some money in the bank.
2 (a) Martha put some money in the bank on Friday.
(b) On Friday, Martha put some money in the bank.
(c) Some money is what Martha put in the bank on Fri-
day.
(d) In the bank is where Martha put some money on
Friday.
</listItem>
<bodyText confidence="0.999909916666667">
The result of such expressive changes affect the
distributions of various phrase types in sentence-
initial and -final positions; studying these distribu-
tions can help us capture some elements of expres-
sion. Despite its inability to detect the structural
changes that do not affect the sentence-initial and
-final phrase types, this approach captures some of
the phrase-level expressive differences between se-
mantically equivalent content; it also captures dif-
ferent sentential structures, including question con-
structs, imperatives, and coordinating and subordi-
nating conjuncts.
</bodyText>
<subsectionHeader confidence="0.782523">
3.1.2 Semantic Classes of Verbs
</subsectionHeader>
<bodyText confidence="0.999843529411765">
Levin (1993) observed that verbs that exhibit sim-
ilar syntactic behavior are also related semantically.
Based on this observation, she sorted 3024 verbs
into 49 high-level semantic classes. Verbs of “send-
ing and carrying”, such as convey, deliver,
move, roll, bring, carry, shuttle, and
wire, for example, are collected under this seman-
tic class and can be further broken down into five
semantically coherent lower-level classes which in-
clude “drive verbs”, “carry verbs”, “bring and take
verbs”, “slide verbs”, and “send verbs”. Each of
these lower-level classes represents a group of verbs
that have similarities both in semantics and in syn-
tactic behavior, i.e., they can grammatically un-
dergo similar syntactic alternations. For example,
“send verbs” can be seen in the following alterna-
tions (Levin, 1993):
</bodyText>
<listItem confidence="0.989921166666667">
1. Base Form
• Nora sent the book to Peter.
• NP + V + NP + PP.
2. Dative Alternation
• Nora sent Peter the book.
• NP+V+NP+NP.
</listItem>
<bodyText confidence="0.998884625">
Semantics of verbs in general, and Levin’s verb
classes in particular, have previously been used for
evaluating content and genre similarity (Hatzivas-
siloglou et al., 1999). In addition, similar seman-
tic classes of verbs were used in natural language
processing applications: START was the first nat-
ural language question answering system to use
such verb classes (Katz and Levin, 1988). We use
</bodyText>
<page confidence="0.99716">
39
</page>
<bodyText confidence="0.999789642857143">
Levin’s semantic verb classes to describe the ex-
pression of an author in a particular work. We as-
sume that semantically similar verbs are often used
in semantically similar syntactic alternations; we
describe part of an author’s expression in a par-
ticular work in terms of the semantic classes of
verbs she uses and the particular argument struc-
tures, e.g., NP + V + NP + PP, she prefers for them.
As many verbs belong to multiple semantic classes,
to capture the dominant semantic verb classes in
each document we credit all semantic classes of all
observed verbs. We extract the argument structures
from part of speech tagged text, using context-free
grammars (Uzuner, 2005).
</bodyText>
<subsectionHeader confidence="0.436339">
3.1.3 Syntactic Classes of Verbs
</subsectionHeader>
<bodyText confidence="0.999619764705883">
Levin’s verb classes include exclusively “non-
embedding verbs”, i.e., verbs that do not take
clausal arguments, and need to be supplemented by
classes of “embedding verbs” that do take such argu-
ments. Alexander and Kunz (1964) identified syn-
tactic classes of embedding verbs, collected a com-
prehensive set of verbs for each class, and described
the identified verb classes with formulae written in
terms of phrasal and clausal elements, such as verb
phrase heads (Vh), participial phrases (Partcp.), in-
finitive phrases (Inf.), indicative clauses (IS), and
subjunctives (Subjunct.). We used 29 of the more
frequent embedding verb classes and identified their
distributions in different works. Examples of these
verb classes are shown in Table 1. Further examples
can be found in (Uzuner, 2005; Uzuner and Katz,
2005).
</bodyText>
<table confidence="0.999043666666667">
Syntactic Formula Example
NP + Vh + NP + from The belt kept him from dying.
+ Partcp.
NP + Vh + that + IS He admitted that he was guilty.
NP + Vh + that I request that she go alone.
+ Subjunct.
NP + Vh + to + Inf. My father wanted to travel.
NP + Vh + wh + IS He asked if they were alone.
NP + pass. + Partcp. He was seen stealing.
</table>
<tableCaption confidence="0.987134">
Table 1: Sample syntactic formulae and examples of
embedding verb classes.
</tableCaption>
<bodyText confidence="0.99941725">
We study the syntax of embedding verbs by iden-
tifying their syntactic class and the structure of
their observed embedded arguments. After identi-
fying syntactic and semantic characteristics of verb
phrases, we combine these features to create fur-
ther elements of expression, e.g., syntactic classes
of embedding verbs and the classes of semantic non-
embedding verbs they co-occur with.
</bodyText>
<sectionHeader confidence="0.998227" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999806">
We tested sentence-initial and -final phrase struc-
tures, semantic and syntactic classes of verbs, and
structure of verb arguments, i.e., syntactic elements
of expression, in paraphrase recognition and in pla-
giarism detection in two ways:
</bodyText>
<listItem confidence="0.975823">
• Recognizing titles even when they are para-
phrased, and
• Recognizing pairs of book chapters that are
paraphrases of each other.
</listItem>
<bodyText confidence="0.999562">
For our experiments, we split books into chapters,
extracted all relevant features from each chapter, and
normalized them by the length of the chapter.
</bodyText>
<subsectionHeader confidence="0.999175">
4.1 Recognizing Titles
</subsectionHeader>
<bodyText confidence="0.999976434782609">
Frequently, people paraphrase parts of rather than
complete works. For example, they may paraphrase
chapters or paragraphs from a work rather than the
whole work. We tested the effectiveness of our
features on recognizing paraphrased components of
works by focusing on chapter-level excerpts (smaller
components than chapters have very sparse vectors
given our sentence-level features and will be the
foci of future research) and using boosted decision
trees (Witten and Frank, 2000).
Our goal was to recognize chapters from the ti-
tles in our corpus even when some titles were para-
phrased into multiple books; in this context, titles
are original works and paraphrased books are trans-
lations of these titles. For this, we assumed the ex-
istence of one legitimate book from each title. We
used this book to train a model that captured the syn-
tactic elements of expression used in this title. We
used the remaining paraphrases of the title (i.e., the
remaining books paraphrasing the title) as the test
set—these paraphrases are considered to be plagia-
rized copies and should be identified as such given
the model for the title.
</bodyText>
<page confidence="0.996784">
40
</page>
<sectionHeader confidence="0.474749" genericHeader="method">
4.1.1 Data
</sectionHeader>
<bodyText confidence="0.999501074074074">
Real life plagiarism data is difficult to obtain.
However, English translations of foreign titles ex-
ist and can be obtained relatively easily. Titles that
have been translated on different occasions by dif-
ferent translators and that have multiple translations
provide us with examples of books that paraphrase
the same content and serve as our surrogate for pla-
giarism data.
To evaluate syntactic elements of expression on
recognizing paraphrased chapters from titles, we
compared the performance of these features with
tfidf-weighted keywords on a 45-way classifica-
tion task. The corpus used for this experiment
included 49 books from 45 titles. Of the 45 ti-
tles, 3 were paraphrased into a total of 7 books
(3 books paraphrased the title Madame Bovary, 2
books paraphrased 20000 Leagues, and 2 books
paraphrased The Kreutzer Sonata). The remaining
titles included works from J. Austen (1775-1817),
C. Dickens (1812-1870), F. Dostoyevski (1821-
1881), A. Doyle (1859-1887), G. Eliot (1819-
1880), G. Flaubert (1821-1880), T. Hardy (1840-
1928), V. Hugo (1802-1885), W. Irving (1789-
1859), J. London (1876-1916), W. M. Thack-
eray (1811-1863), L. Tolstoy (1828-1910), I. Tur-
genev (1818-1883), M. Twain (1835-1910), and
J. Verne (1828-1905).
</bodyText>
<subsubsectionHeader confidence="0.592589">
4.1.2 Baseline Features
</subsubsectionHeader>
<bodyText confidence="0.999616547619048">
The task described in this section focuses on rec-
ognizing paraphrases of works based on the way
they are written. Given the focus of authorship attri-
bution literature on “the way people write”, to eval-
uate the syntactic elements of expression on recog-
nizing paraphrased chapters of a work, we compared
these features against features frequently used in au-
thorship attribution as well as features used in con-
tent recognition.
Tfidf-weighted Keywords: Keywords, i.e., con-
tent words, are frequently used in content-based text
classification and constitute one of our baselines.
Function Words: In studies of authorship at-
tribution, many researchers have taken advantage
of the differences in the way authors use function
words (Mosteller and Wallace, 1963; Peng and Hen-
gartner, 2002). In our studies, we used a set of 506
function words (Uzuner, 2005).
Distributions of Word Lengths and Sentence
Lengths: Distributions of word lengths and sen-
tence lengths have been used in the literature for
authorship attribution (Mendenhall, 1887; Williams,
1975; Holmes, 1994). We include these features
in our sets of baselines along with information
about means and standard deviations of sentence
lengths (Holmes, 1994).
Baseline Linguistic Features: Sets of surface,
syntactic, and semantic features have been found to
be useful for authorship attribution and have been
adopted here as baseline features. These features
included: the number of words and the number of
sentences in the document; type–token ratio; aver-
age and standard deviation of the lengths of words
(in characters) and of the lengths of sentences (in
words) in the document; frequencies of declara-
tive sentences, interrogatives, imperatives, and frag-
mental sentences; frequencies of active voice sen-
tences, be-passives and get-passives; frequencies of
’s-genitives, of-genitives and of phrases that lack
genitives; frequency of overt negations, e.g., “not”,
“no”, etc.; and frequency of uncertainty markers,
e.g., “could”, “possibly”, etc.
</bodyText>
<subsectionHeader confidence="0.821402">
4.1.3 Experiment
</subsectionHeader>
<bodyText confidence="0.999822125">
To recognize chapters from the titles in our corpus
even when some titles were paraphrased into mul-
tiple books, we randomly selected 40–50 chapters
from each title. We used 60% of the selected chap-
ters from each title for training and the remaining
40% for testing. For paraphrased titles, we selected
training chapters from one of the paraphrases and
testing chapters from the remaining paraphrases. We
repeated this experiment three times; at each round,
a different paraphrase was chosen for training and
the rest were used for testing.
Our results show that, on average, syntactic ele-
ments of expression accurately recognized compo-
nents of titles 73% of the time and significantly out-
performed all baselines3 (see middle column in Ta-
ble 2).4
</bodyText>
<footnote confidence="0.927596285714286">
3The tfidf-weighted keywords used in this experiment do not
include proper nouns. These words are unique to each title and
can be easily replaced without changing content or expression
in order to trick a plagiarism detection system that would rely
on proper nouns.
4For the corpora used in this paper, a difference of 4% or
more is statistically significant with α = 0.05.
</footnote>
<page confidence="0.995921">
41
</page>
<table confidence="0.999932181818182">
Feature Set Avg. Avg.
accuracy accuracy
(complete (para-
corpus) phrases)
only
Syntactic elements of expression 73% 95%
Function words 53% 34%
Tfidf-weighted keywords 47% 38%
Baseline linguistic 40% 67%
Dist. of word length 18% 54%
Dist. of sentence length 12% 17%
</table>
<tableCaption confidence="0.872471">
Table 2: Classification results (on the test set) for
recognizing titles in the corpus even when some ti-
tles are paraphrased (middle column) and classifi-
cation results only on the paraphrased titles (right
column). In either case, random chance would rec-
ognize a paraphrased title 2% of the time.
</tableCaption>
<bodyText confidence="0.9966655">
The right column in Table 2 shows that the syntac-
tic elements of expression accurately recognized on
average 95% of the chapters taken from paraphrased
titles. This finding implies that some of our elements
of expression are common to books that are derived
from the same title. This commonality could be due
to the similarity of their content or due to the under-
lying expression of the original author.
</bodyText>
<subsectionHeader confidence="0.767849">
4.2 Recognizing Pairs of Paraphrased
Chapters
</subsectionHeader>
<bodyText confidence="0.9999494">
Experiments in Section 4.1 show that we can use
syntactic elements of expression to recognize titles
and their components based on the way they are
written even when some works are paraphrased. In
this section, our goal is to identify pairs of chapters
that paraphrase the same content, i.e., chapter 1 of
translation 1 of Madame Bovary and chapter 1 of
translation 2 of Madame Bovary. For this evalua-
tion, we used a similar approach to that presented by
Nahnsen et al. (2005).
</bodyText>
<sectionHeader confidence="0.39699" genericHeader="method">
4.2.1 Data
</sectionHeader>
<bodyText confidence="0.9999752">
Our data for this experiment included 47 chap-
ters from each of two translations of 20000 Leagues
under the Sea (Verne), 35 chapters from each of 3
translations of Madame Bovary (Flaubert), 28 chap-
ters from each of two translations of The Kreutzer
Sonata (Tolstoy), and 365 chapters from each of 2
translations of War and Peace (Tolstoy). Pairing
up the chapters from these titles provided us with
more than 1,000,000 chapter pairs, of which approx-
imately 1080 were paraphrases of each other.5
</bodyText>
<sectionHeader confidence="0.386905" genericHeader="evaluation">
4.2.2 Experiment
</sectionHeader>
<bodyText confidence="0.999649882352941">
For experiments on finding pairwise matches, we
used similarity of vectors of tfidf-weighted key-
words;6 and the multiplicative combination of the
similarity of vectors of tfidf-weighed keywords of
works with the similarity of vectors of syntactic ele-
ments of expression of these works. We used cosine
to evaluate the similarity of the vectors of works. We
omitted the remaining baseline features from this
experiment—they are features that are common to
majority of the chapters from each book, they do
not relate to the task of finding pairs of chapters that
could be paraphrases of each other.
We ranked all chapter pairs in the corpus based
on their similarity. From this ranked list, we iden-
tified the top n most similar pairs and predicted that
they are paraphrases of each other. We evaluated our
methods with precision, recall, and f-measure.7
</bodyText>
<figureCaption confidence="0.992112">
Figure 1: Precision.
</figureCaption>
<bodyText confidence="0.971986714285714">
Figures 1, 2, and 3 show that syntactic elements
of expression improve the performance of tfidf-
weighted keywords in recognizing pairs of para-
phrased chapters significantly in terms of precision,
recall, and f-measure for all n; in all of these figures,
the blue line marked syn tfidf represents the per-
formance of tfidf-weighted keywords enhanced with
</bodyText>
<footnote confidence="0.764226875">
5Note that this number double-counts the paraphrased pairs;
however, this fact is immaterial for our discussion.
6In this experiment, proper nouns are included in the
weighted keywords.
7The ground truth marks only the same chapter from two
different translations of the same title as similar, i.e., chapter x
of translation 1 of Madame Bovary and chapter y of translation
2 of Madame Bovary are similar only when x = y.
</footnote>
<page confidence="0.998264">
42
</page>
<figureCaption confidence="0.996044">
Figure 2: Recall.
</figureCaption>
<bodyText confidence="0.99949875">
syntactic elements of expression. More specifically,
the peak f-measure for tfidf-weighted keywords is
approximately 0.77 without contribution from syn-
tactic elements of expression. Adding information
about similarity of syntactic features to cosine sim-
ilarity of tfidf-weighted keywords boosts peak f-
measure value to approximately 0.82.8 Although
the f-measure of both representations degrade when
n &gt; 1100, this degradation is an artifact of the eval-
uation metric: the corpus includes only 1080 similar
pairs, at n &gt; 1100, recall is very close to 1, and
therefore increasing n hurts overall performance.
</bodyText>
<figureCaption confidence="0.811632">
Figure 3: F-measure.
</figureCaption>
<sectionHeader confidence="0.999016" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9994102">
Plagiarism is a problem at all levels of education.
Increased availability of digital versions of works
makes it easier to plagiarize others’ work and the
large volumes of information available on the web
makes it difficult to identify cases of plagiarism.
</bodyText>
<footnote confidence="0.953258">
8The difference is statistically significant at α = 0.05.
</footnote>
<bodyText confidence="0.999965647058824">
To identify plagiarism even when works are para-
phrased, we propose studying the use of particular
syntactic constructs as well as keywords in docu-
ments.
This paper shows that syntactic information can
help recognize works based on the way they are
written. Syntactic elements of expression that fo-
cus on the changes in the phrase structure of works
help identify paraphrased components of a title. The
same features help improve identification of pairs
of chapters that are paraphrases of each other, de-
spite the content these chapters share with the rest
of the chapters taken from the same title. The re-
sults presented in this paper are based on experi-
ments that use translated novels as surrogate for pla-
giarism data. Our future work will extend our study
to real life plagiarism data.
</bodyText>
<sectionHeader confidence="0.999669" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99997">
The authors would like to thank Sue Felshin for her
insightful comments. This work is supported in part
by the Advanced Research and Development Activ-
ity as part of the AQUAINT research program.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975815">
D. Alexander and W. J. Kunz. 1964. Some classes of
verbs in English. In Linguistics Research Project. In-
diana University, June.
E. Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the 3rd Conference on Applied
Natural Language Processing.
V. Hatzivassiloglou, J. Klavans, and E. Eskin. 1999. De-
tecting similarity by applying learning over indicators.
In Proceedings of the 37th Annual Meeting of the ACL.
D. I. Holmes. 1994. Authorship attribution. Computers
and the Humanities, 28.
B. Katz and B. Levin. 1988. Exploiting lexical reg-
ularities in designing natural language systems. In
Proceedings of the 12th Int’l Conference on Compu-
tational Linguistics (COLING ’88).
B. Levin. 1993. English Verb Classes and Alternations.
A Preliminary Investigation. University of Chicago
Press.
T. C. Mendenhall. 1887. Characteristic curves of com-
position. Science, 11.
</reference>
<page confidence="0.995554">
43
</page>
<reference confidence="0.999339733333333">
F. Mosteller and D. L. Wallace. 1963. Inference in an au-
thorship problem. Journal of the American Statistical
Association, 58(302).
T. Nahnsen, ¨O. Uzuner, and B. Katz. 2005. Lexical
chains and sliding locality windows in content-based
text similarity detection. CSAIL Memo, AIM-2005-
017.
R. D. Peng and H. Hengartner. 2002. Quantitative analy-
sis of literary styles. The American Statistician, 56(3).
H. S. Sichel. 1974. On a distribution representing
sentence-length in written prose. Journal of the Royal
Statistical Society (A), 137.
¨O. Uzuner and B. Katz. 2005. Capturing expression us-
ing linguistic information. In Proceedings of the 20th
National Conference on Artificial Intelligence (AAAI-
05).
¨O. Uzuner, R. Davis, and B. Katz. 2004. Using em-
pirical methods for evaluating expression and content
similarity. In Proceedings of the 37th Hawaiian Inter-
national Conference on System Sciences (HICSS-37).
IEEE Computer Society.
¨O. Uzuner. 2005. Identifying Expression Fingerprints
Using Linguistic Information. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
C. B. Williams. 1975. Mendenhall’s studies of word-
length distribution in the works of Shakespeare and
Bacon. Biometrika, 62(1).
I. H. Witten and E. Frank. 2000. Data Mining: Practical
Machine Learning Tools with Java Implementations.
Morgan Kaufmann, San Francisco.
</reference>
<page confidence="0.999294">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.418886">
<title confidence="0.998388">Using Syntactic Information to Identify Plagiarism</title>
<author confidence="0.451692">Boris Katz Uzuner</author>
<author confidence="0.451692">Thade</author>
<affiliation confidence="0.951767">Massachusetts Institute of Computer Science and Artificial Intelligence</affiliation>
<address confidence="0.990524">Cambridge, MA</address>
<email confidence="0.999459">ozlem,boris,tnahnsen@csail.mit.edu</email>
<abstract confidence="0.997816642857143">Using keyword overlaps to identify plagiarism can result in many false negatives and positives: substitution of synonyms for each other reduces the similarity between works, making it difficult to recognize plagiarism; overlap in ambiguous keywords can falsely inflate the similarity of works that are in fact different in content. Plagiarism detection based on verbatim similarity of works can be rendered ineffective when works are paraphrased even in superficial and immaterial ways. Considering linguistic information related to creative aspects of writing can improve identification of plagiarism by adding a crucial dimension to evaluation of similarity: documents that share linguistic elements in addition to content are more likely to be copied from each other. In this paper, we present a set of low-level syntactic structures that capture creative aspects of writing and show that information about linguistic similarities of works improves recognition of plagiarism (over tfidf-weighted keywords alone) when combined with similarity measurements based on tfidf-weighted keywords.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Alexander</author>
<author>W J Kunz</author>
</authors>
<title>Some classes of verbs in English.</title>
<date>1964</date>
<booktitle>In Linguistics Research</booktitle>
<institution>Project. Indiana University,</institution>
<contexts>
<context position="12903" citStr="Alexander and Kunz (1964)" startWordPosition="2039" endWordPosition="2042"> and the particular argument structures, e.g., NP + V + NP + PP, she prefers for them. As many verbs belong to multiple semantic classes, to capture the dominant semantic verb classes in each document we credit all semantic classes of all observed verbs. We extract the argument structures from part of speech tagged text, using context-free grammars (Uzuner, 2005). 3.1.3 Syntactic Classes of Verbs Levin’s verb classes include exclusively “nonembedding verbs”, i.e., verbs that do not take clausal arguments, and need to be supplemented by classes of “embedding verbs” that do take such arguments. Alexander and Kunz (1964) identified syntactic classes of embedding verbs, collected a comprehensive set of verbs for each class, and described the identified verb classes with formulae written in terms of phrasal and clausal elements, such as verb phrase heads (Vh), participial phrases (Partcp.), infinitive phrases (Inf.), indicative clauses (IS), and subjunctives (Subjunct.). We used 29 of the more frequent embedding verb classes and identified their distributions in different works. Examples of these verb classes are shown in Table 1. Further examples can be found in (Uzuner, 2005; Uzuner and Katz, 2005). Syntactic</context>
</contexts>
<marker>Alexander, Kunz, 1964</marker>
<rawString>D. Alexander and W. J. Kunz. 1964. Some classes of verbs in English. In Linguistics Research Project. Indiana University, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the 3rd Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="6384" citStr="Brill, 1992" startWordPosition="969" endWordPosition="970">oaches to similarity detection use computationally cheap but linguistically less informed features (Peng and Hengartner, 2002; Sichel, 1974; Williams, 1975) such as keywords, function words, word lengths, and sentence lengths; approaches that include deeper linguistic information, such as syntactic information, usually incur significant computational costs (Uzuner et al., 2004). Our approach identifies useful linguistic information without incurring the computational cost of full text parsing; it uses context-free grammars to perform highlevel syntactic analysis of part-of-speech tagged text (Brill, 1992). It turns out that such a level of analysis is sufficient to capture syntactic information related to creative aspects of writing; this in turn helps improve recognition of paraphrased documents. The results presented here show that extraction of useful linguistic information for text classification purposes does not have to be computationally prohibitively expensive, and that despite the tradeoff between the accuracy of features and computational efficiency, we can extract linguisticallyinformed features without full parsing. 3 Identifying Creative Aspects of Writing In this paper, we first </context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill. 1992. A simple rule-based part of speech tagger. In Proceedings of the 3rd Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>J Klavans</author>
<author>E Eskin</author>
</authors>
<title>Detecting similarity by applying learning over indicators.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="11741" citStr="Hatzivassiloglou et al., 1999" startWordPosition="1846" endWordPosition="1850">d take verbs”, “slide verbs”, and “send verbs”. Each of these lower-level classes represents a group of verbs that have similarities both in semantics and in syntactic behavior, i.e., they can grammatically undergo similar syntactic alternations. For example, “send verbs” can be seen in the following alternations (Levin, 1993): 1. Base Form • Nora sent the book to Peter. • NP + V + NP + PP. 2. Dative Alternation • Nora sent Peter the book. • NP+V+NP+NP. Semantics of verbs in general, and Levin’s verb classes in particular, have previously been used for evaluating content and genre similarity (Hatzivassiloglou et al., 1999). In addition, similar semantic classes of verbs were used in natural language processing applications: START was the first natural language question answering system to use such verb classes (Katz and Levin, 1988). We use 39 Levin’s semantic verb classes to describe the expression of an author in a particular work. We assume that semantically similar verbs are often used in semantically similar syntactic alternations; we describe part of an author’s expression in a particular work in terms of the semantic classes of verbs she uses and the particular argument structures, e.g., NP + V + NP + PP</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Eskin, 1999</marker>
<rawString>V. Hatzivassiloglou, J. Klavans, and E. Eskin. 1999. Detecting similarity by applying learning over indicators. In Proceedings of the 37th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D I Holmes</author>
</authors>
<date>1994</date>
<booktitle>Authorship attribution. Computers and the Humanities,</booktitle>
<pages>28</pages>
<contexts>
<context position="18305" citStr="Holmes, 1994" startWordPosition="2912" endWordPosition="2913">: Keywords, i.e., content words, are frequently used in content-based text classification and constitute one of our baselines. Function Words: In studies of authorship attribution, many researchers have taken advantage of the differences in the way authors use function words (Mosteller and Wallace, 1963; Peng and Hengartner, 2002). In our studies, we used a set of 506 function words (Uzuner, 2005). Distributions of Word Lengths and Sentence Lengths: Distributions of word lengths and sentence lengths have been used in the literature for authorship attribution (Mendenhall, 1887; Williams, 1975; Holmes, 1994). We include these features in our sets of baselines along with information about means and standard deviations of sentence lengths (Holmes, 1994). Baseline Linguistic Features: Sets of surface, syntactic, and semantic features have been found to be useful for authorship attribution and have been adopted here as baseline features. These features included: the number of words and the number of sentences in the document; type–token ratio; average and standard deviation of the lengths of words (in characters) and of the lengths of sentences (in words) in the document; frequencies of declarative s</context>
</contexts>
<marker>Holmes, 1994</marker>
<rawString>D. I. Holmes. 1994. Authorship attribution. Computers and the Humanities, 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Katz</author>
<author>B Levin</author>
</authors>
<title>Exploiting lexical regularities in designing natural language systems.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th Int’l Conference on Computational Linguistics (COLING ’88).</booktitle>
<contexts>
<context position="11955" citStr="Katz and Levin, 1988" startWordPosition="1881" endWordPosition="1884">syntactic alternations. For example, “send verbs” can be seen in the following alternations (Levin, 1993): 1. Base Form • Nora sent the book to Peter. • NP + V + NP + PP. 2. Dative Alternation • Nora sent Peter the book. • NP+V+NP+NP. Semantics of verbs in general, and Levin’s verb classes in particular, have previously been used for evaluating content and genre similarity (Hatzivassiloglou et al., 1999). In addition, similar semantic classes of verbs were used in natural language processing applications: START was the first natural language question answering system to use such verb classes (Katz and Levin, 1988). We use 39 Levin’s semantic verb classes to describe the expression of an author in a particular work. We assume that semantically similar verbs are often used in semantically similar syntactic alternations; we describe part of an author’s expression in a particular work in terms of the semantic classes of verbs she uses and the particular argument structures, e.g., NP + V + NP + PP, she prefers for them. As many verbs belong to multiple semantic classes, to capture the dominant semantic verb classes in each document we credit all semantic classes of all observed verbs. We extract the argumen</context>
</contexts>
<marker>Katz, Levin, 1988</marker>
<rawString>B. Katz and B. Levin. 1988. Exploiting lexical regularities in designing natural language systems. In Proceedings of the 12th Int’l Conference on Computational Linguistics (COLING ’88).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations. A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="10641" citStr="Levin (1993)" startWordPosition="1670" endWordPosition="1671"> of such expressive changes affect the distributions of various phrase types in sentenceinitial and -final positions; studying these distributions can help us capture some elements of expression. Despite its inability to detect the structural changes that do not affect the sentence-initial and -final phrase types, this approach captures some of the phrase-level expressive differences between semantically equivalent content; it also captures different sentential structures, including question constructs, imperatives, and coordinating and subordinating conjuncts. 3.1.2 Semantic Classes of Verbs Levin (1993) observed that verbs that exhibit similar syntactic behavior are also related semantically. Based on this observation, she sorted 3024 verbs into 49 high-level semantic classes. Verbs of “sending and carrying”, such as convey, deliver, move, roll, bring, carry, shuttle, and wire, for example, are collected under this semantic class and can be further broken down into five semantically coherent lower-level classes which include “drive verbs”, “carry verbs”, “bring and take verbs”, “slide verbs”, and “send verbs”. Each of these lower-level classes represents a group of verbs that have similariti</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English Verb Classes and Alternations. A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="false">
<title>Characteristic curves of composition.</title>
<journal>Science,</journal>
<volume>11</volume>
<marker></marker>
<rawString>T. C. Mendenhall. 1887. Characteristic curves of composition. Science, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mosteller</author>
<author>D L Wallace</author>
</authors>
<title>Inference in an authorship problem.</title>
<date>1963</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>58</volume>
<issue>302</issue>
<contexts>
<context position="17996" citStr="Mosteller and Wallace, 1963" startWordPosition="2862" endWordPosition="2865">ocus of authorship attribution literature on “the way people write”, to evaluate the syntactic elements of expression on recognizing paraphrased chapters of a work, we compared these features against features frequently used in authorship attribution as well as features used in content recognition. Tfidf-weighted Keywords: Keywords, i.e., content words, are frequently used in content-based text classification and constitute one of our baselines. Function Words: In studies of authorship attribution, many researchers have taken advantage of the differences in the way authors use function words (Mosteller and Wallace, 1963; Peng and Hengartner, 2002). In our studies, we used a set of 506 function words (Uzuner, 2005). Distributions of Word Lengths and Sentence Lengths: Distributions of word lengths and sentence lengths have been used in the literature for authorship attribution (Mendenhall, 1887; Williams, 1975; Holmes, 1994). We include these features in our sets of baselines along with information about means and standard deviations of sentence lengths (Holmes, 1994). Baseline Linguistic Features: Sets of surface, syntactic, and semantic features have been found to be useful for authorship attribution and hav</context>
</contexts>
<marker>Mosteller, Wallace, 1963</marker>
<rawString>F. Mosteller and D. L. Wallace. 1963. Inference in an authorship problem. Journal of the American Statistical Association, 58(302).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nahnsen</author>
<author>¨O Uzuner</author>
<author>B Katz</author>
</authors>
<title>Lexical chains and sliding locality windows in content-based text similarity detection.</title>
<date>2005</date>
<journal>CSAIL Memo,</journal>
<pages>2005--017</pages>
<contexts>
<context position="21868" citStr="Nahnsen et al. (2005)" startWordPosition="3482" endWordPosition="3485">due to the similarity of their content or due to the underlying expression of the original author. 4.2 Recognizing Pairs of Paraphrased Chapters Experiments in Section 4.1 show that we can use syntactic elements of expression to recognize titles and their components based on the way they are written even when some works are paraphrased. In this section, our goal is to identify pairs of chapters that paraphrase the same content, i.e., chapter 1 of translation 1 of Madame Bovary and chapter 1 of translation 2 of Madame Bovary. For this evaluation, we used a similar approach to that presented by Nahnsen et al. (2005). 4.2.1 Data Our data for this experiment included 47 chapters from each of two translations of 20000 Leagues under the Sea (Verne), 35 chapters from each of 3 translations of Madame Bovary (Flaubert), 28 chapters from each of two translations of The Kreutzer Sonata (Tolstoy), and 365 chapters from each of 2 translations of War and Peace (Tolstoy). Pairing up the chapters from these titles provided us with more than 1,000,000 chapter pairs, of which approximately 1080 were paraphrases of each other.5 4.2.2 Experiment For experiments on finding pairwise matches, we used similarity of vectors of</context>
</contexts>
<marker>Nahnsen, Uzuner, Katz, 2005</marker>
<rawString>T. Nahnsen, ¨O. Uzuner, and B. Katz. 2005. Lexical chains and sliding locality windows in content-based text similarity detection. CSAIL Memo, AIM-2005-017.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Peng</author>
<author>H Hengartner</author>
</authors>
<title>Quantitative analysis of literary styles.</title>
<date>2002</date>
<journal>The American Statistician,</journal>
<volume>56</volume>
<issue>3</issue>
<contexts>
<context position="5897" citStr="Peng and Hengartner, 2002" startWordPosition="899" endWordPosition="902">ssion involves the linguistic elements that relate to how an author phrases particular content and can be used to identify potential copyright infringement or plagiarism. Similarities in the expression of similar content in two different works signal potential copying. We hypothesize that syntax plays a role in capturing expression of content. Our approach to recognizing paraphrased works is based on phrase structure of sentences in general, and structure of verb phrases in particular. Most approaches to similarity detection use computationally cheap but linguistically less informed features (Peng and Hengartner, 2002; Sichel, 1974; Williams, 1975) such as keywords, function words, word lengths, and sentence lengths; approaches that include deeper linguistic information, such as syntactic information, usually incur significant computational costs (Uzuner et al., 2004). Our approach identifies useful linguistic information without incurring the computational cost of full text parsing; it uses context-free grammars to perform highlevel syntactic analysis of part-of-speech tagged text (Brill, 1992). It turns out that such a level of analysis is sufficient to capture syntactic information related to creative a</context>
<context position="18024" citStr="Peng and Hengartner, 2002" startWordPosition="2866" endWordPosition="2870">n literature on “the way people write”, to evaluate the syntactic elements of expression on recognizing paraphrased chapters of a work, we compared these features against features frequently used in authorship attribution as well as features used in content recognition. Tfidf-weighted Keywords: Keywords, i.e., content words, are frequently used in content-based text classification and constitute one of our baselines. Function Words: In studies of authorship attribution, many researchers have taken advantage of the differences in the way authors use function words (Mosteller and Wallace, 1963; Peng and Hengartner, 2002). In our studies, we used a set of 506 function words (Uzuner, 2005). Distributions of Word Lengths and Sentence Lengths: Distributions of word lengths and sentence lengths have been used in the literature for authorship attribution (Mendenhall, 1887; Williams, 1975; Holmes, 1994). We include these features in our sets of baselines along with information about means and standard deviations of sentence lengths (Holmes, 1994). Baseline Linguistic Features: Sets of surface, syntactic, and semantic features have been found to be useful for authorship attribution and have been adopted here as basel</context>
</contexts>
<marker>Peng, Hengartner, 2002</marker>
<rawString>R. D. Peng and H. Hengartner. 2002. Quantitative analysis of literary styles. The American Statistician, 56(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Sichel</author>
</authors>
<title>On a distribution representing sentence-length in written prose.</title>
<date>1974</date>
<journal>Journal of the Royal Statistical Society (A),</journal>
<pages>137</pages>
<contexts>
<context position="5911" citStr="Sichel, 1974" startWordPosition="903" endWordPosition="904">ic elements that relate to how an author phrases particular content and can be used to identify potential copyright infringement or plagiarism. Similarities in the expression of similar content in two different works signal potential copying. We hypothesize that syntax plays a role in capturing expression of content. Our approach to recognizing paraphrased works is based on phrase structure of sentences in general, and structure of verb phrases in particular. Most approaches to similarity detection use computationally cheap but linguistically less informed features (Peng and Hengartner, 2002; Sichel, 1974; Williams, 1975) such as keywords, function words, word lengths, and sentence lengths; approaches that include deeper linguistic information, such as syntactic information, usually incur significant computational costs (Uzuner et al., 2004). Our approach identifies useful linguistic information without incurring the computational cost of full text parsing; it uses context-free grammars to perform highlevel syntactic analysis of part-of-speech tagged text (Brill, 1992). It turns out that such a level of analysis is sufficient to capture syntactic information related to creative aspects of writ</context>
</contexts>
<marker>Sichel, 1974</marker>
<rawString>H. S. Sichel. 1974. On a distribution representing sentence-length in written prose. Journal of the Royal Statistical Society (A), 137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨O Uzuner</author>
<author>B Katz</author>
</authors>
<title>Capturing expression using linguistic information.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI05).</booktitle>
<contexts>
<context position="4906" citStr="Uzuner and Katz, 2005" startWordPosition="751" endWordPosition="754">that syntactic elements of expression outperform all baselines in recognizing titles even when they are paraphrased, providing a way of recognizing copies of works based on the similarities in their expression of content. Our second experiment shows that similarity measurements based on the combination of tfidf-weighted keywords and syntactic elements of expression outperform the weighted keywords in recognizing pairs of book chapters that are paraphrases of each other. 2 Related Work We define expression as “the linguistic choices of authors in presenting a particular content” (Uzuner, 2005; Uzuner and Katz, 2005). Linguistic similarity between works has been studied in the text classification literature for identifying the style of an author. However, it is important to differentiate expression from style. Style refers to the linguistic elements that, independently of content, persist over the works of an author and has been widely studied in authorship attribution. Expression involves the linguistic elements that relate to how an author phrases particular content and can be used to identify potential copyright infringement or plagiarism. Similarities in the expression of similar content in two differ</context>
<context position="13492" citStr="Uzuner and Katz, 2005" startWordPosition="2130" endWordPosition="2133">uments. Alexander and Kunz (1964) identified syntactic classes of embedding verbs, collected a comprehensive set of verbs for each class, and described the identified verb classes with formulae written in terms of phrasal and clausal elements, such as verb phrase heads (Vh), participial phrases (Partcp.), infinitive phrases (Inf.), indicative clauses (IS), and subjunctives (Subjunct.). We used 29 of the more frequent embedding verb classes and identified their distributions in different works. Examples of these verb classes are shown in Table 1. Further examples can be found in (Uzuner, 2005; Uzuner and Katz, 2005). Syntactic Formula Example NP + Vh + NP + from The belt kept him from dying. + Partcp. NP + Vh + that + IS He admitted that he was guilty. NP + Vh + that I request that she go alone. + Subjunct. NP + Vh + to + Inf. My father wanted to travel. NP + Vh + wh + IS He asked if they were alone. NP + pass. + Partcp. He was seen stealing. Table 1: Sample syntactic formulae and examples of embedding verb classes. We study the syntax of embedding verbs by identifying their syntactic class and the structure of their observed embedded arguments. After identifying syntactic and semantic characteristics of</context>
</contexts>
<marker>Uzuner, Katz, 2005</marker>
<rawString>¨O. Uzuner and B. Katz. 2005. Capturing expression using linguistic information. In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨O Uzuner</author>
<author>R Davis</author>
<author>B Katz</author>
</authors>
<title>Using empirical methods for evaluating expression and content similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the 37th Hawaiian International Conference on System Sciences (HICSS-37).</booktitle>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="6152" citStr="Uzuner et al., 2004" startWordPosition="934" endWordPosition="937"> copying. We hypothesize that syntax plays a role in capturing expression of content. Our approach to recognizing paraphrased works is based on phrase structure of sentences in general, and structure of verb phrases in particular. Most approaches to similarity detection use computationally cheap but linguistically less informed features (Peng and Hengartner, 2002; Sichel, 1974; Williams, 1975) such as keywords, function words, word lengths, and sentence lengths; approaches that include deeper linguistic information, such as syntactic information, usually incur significant computational costs (Uzuner et al., 2004). Our approach identifies useful linguistic information without incurring the computational cost of full text parsing; it uses context-free grammars to perform highlevel syntactic analysis of part-of-speech tagged text (Brill, 1992). It turns out that such a level of analysis is sufficient to capture syntactic information related to creative aspects of writing; this in turn helps improve recognition of paraphrased documents. The results presented here show that extraction of useful linguistic information for text classification purposes does not have to be computationally prohibitively expensi</context>
</contexts>
<marker>Uzuner, Davis, Katz, 2004</marker>
<rawString>¨O. Uzuner, R. Davis, and B. Katz. 2004. Using empirical methods for evaluating expression and content similarity. In Proceedings of the 37th Hawaiian International Conference on System Sciences (HICSS-37). IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨O Uzuner</author>
</authors>
<title>Identifying Expression Fingerprints Using Linguistic Information.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="4882" citStr="Uzuner, 2005" startWordPosition="749" endWordPosition="750">eriment shows that syntactic elements of expression outperform all baselines in recognizing titles even when they are paraphrased, providing a way of recognizing copies of works based on the similarities in their expression of content. Our second experiment shows that similarity measurements based on the combination of tfidf-weighted keywords and syntactic elements of expression outperform the weighted keywords in recognizing pairs of book chapters that are paraphrases of each other. 2 Related Work We define expression as “the linguistic choices of authors in presenting a particular content” (Uzuner, 2005; Uzuner and Katz, 2005). Linguistic similarity between works has been studied in the text classification literature for identifying the style of an author. However, it is important to differentiate expression from style. Style refers to the linguistic elements that, independently of content, persist over the works of an author and has been widely studied in authorship attribution. Expression involves the linguistic elements that relate to how an author phrases particular content and can be used to identify potential copyright infringement or plagiarism. Similarities in the expression of simil</context>
<context position="12643" citStr="Uzuner, 2005" startWordPosition="2000" endWordPosition="2001">author in a particular work. We assume that semantically similar verbs are often used in semantically similar syntactic alternations; we describe part of an author’s expression in a particular work in terms of the semantic classes of verbs she uses and the particular argument structures, e.g., NP + V + NP + PP, she prefers for them. As many verbs belong to multiple semantic classes, to capture the dominant semantic verb classes in each document we credit all semantic classes of all observed verbs. We extract the argument structures from part of speech tagged text, using context-free grammars (Uzuner, 2005). 3.1.3 Syntactic Classes of Verbs Levin’s verb classes include exclusively “nonembedding verbs”, i.e., verbs that do not take clausal arguments, and need to be supplemented by classes of “embedding verbs” that do take such arguments. Alexander and Kunz (1964) identified syntactic classes of embedding verbs, collected a comprehensive set of verbs for each class, and described the identified verb classes with formulae written in terms of phrasal and clausal elements, such as verb phrase heads (Vh), participial phrases (Partcp.), infinitive phrases (Inf.), indicative clauses (IS), and subjunctiv</context>
<context position="18092" citStr="Uzuner, 2005" startWordPosition="2882" endWordPosition="2883">ression on recognizing paraphrased chapters of a work, we compared these features against features frequently used in authorship attribution as well as features used in content recognition. Tfidf-weighted Keywords: Keywords, i.e., content words, are frequently used in content-based text classification and constitute one of our baselines. Function Words: In studies of authorship attribution, many researchers have taken advantage of the differences in the way authors use function words (Mosteller and Wallace, 1963; Peng and Hengartner, 2002). In our studies, we used a set of 506 function words (Uzuner, 2005). Distributions of Word Lengths and Sentence Lengths: Distributions of word lengths and sentence lengths have been used in the literature for authorship attribution (Mendenhall, 1887; Williams, 1975; Holmes, 1994). We include these features in our sets of baselines along with information about means and standard deviations of sentence lengths (Holmes, 1994). Baseline Linguistic Features: Sets of surface, syntactic, and semantic features have been found to be useful for authorship attribution and have been adopted here as baseline features. These features included: the number of words and the n</context>
</contexts>
<marker>Uzuner, 2005</marker>
<rawString>¨O. Uzuner. 2005. Identifying Expression Fingerprints Using Linguistic Information. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C B Williams</author>
</authors>
<title>Mendenhall’s studies of wordlength distribution in the works of Shakespeare and Bacon.</title>
<date>1975</date>
<journal>Biometrika,</journal>
<volume>62</volume>
<issue>1</issue>
<contexts>
<context position="5928" citStr="Williams, 1975" startWordPosition="905" endWordPosition="906">at relate to how an author phrases particular content and can be used to identify potential copyright infringement or plagiarism. Similarities in the expression of similar content in two different works signal potential copying. We hypothesize that syntax plays a role in capturing expression of content. Our approach to recognizing paraphrased works is based on phrase structure of sentences in general, and structure of verb phrases in particular. Most approaches to similarity detection use computationally cheap but linguistically less informed features (Peng and Hengartner, 2002; Sichel, 1974; Williams, 1975) such as keywords, function words, word lengths, and sentence lengths; approaches that include deeper linguistic information, such as syntactic information, usually incur significant computational costs (Uzuner et al., 2004). Our approach identifies useful linguistic information without incurring the computational cost of full text parsing; it uses context-free grammars to perform highlevel syntactic analysis of part-of-speech tagged text (Brill, 1992). It turns out that such a level of analysis is sufficient to capture syntactic information related to creative aspects of writing; this in turn</context>
<context position="18290" citStr="Williams, 1975" startWordPosition="2910" endWordPosition="2911">eighted Keywords: Keywords, i.e., content words, are frequently used in content-based text classification and constitute one of our baselines. Function Words: In studies of authorship attribution, many researchers have taken advantage of the differences in the way authors use function words (Mosteller and Wallace, 1963; Peng and Hengartner, 2002). In our studies, we used a set of 506 function words (Uzuner, 2005). Distributions of Word Lengths and Sentence Lengths: Distributions of word lengths and sentence lengths have been used in the literature for authorship attribution (Mendenhall, 1887; Williams, 1975; Holmes, 1994). We include these features in our sets of baselines along with information about means and standard deviations of sentence lengths (Holmes, 1994). Baseline Linguistic Features: Sets of surface, syntactic, and semantic features have been found to be useful for authorship attribution and have been adopted here as baseline features. These features included: the number of words and the number of sentences in the document; type–token ratio; average and standard deviation of the lengths of words (in characters) and of the lengths of sentences (in words) in the document; frequencies o</context>
</contexts>
<marker>Williams, 1975</marker>
<rawString>C. B. Williams. 1975. Mendenhall’s studies of wordlength distribution in the works of Shakespeare and Bacon. Biometrika, 62(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools with Java Implementations.</title>
<date>2000</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="15324" citStr="Witten and Frank, 2000" startWordPosition="2436" endWordPosition="2439"> into chapters, extracted all relevant features from each chapter, and normalized them by the length of the chapter. 4.1 Recognizing Titles Frequently, people paraphrase parts of rather than complete works. For example, they may paraphrase chapters or paragraphs from a work rather than the whole work. We tested the effectiveness of our features on recognizing paraphrased components of works by focusing on chapter-level excerpts (smaller components than chapters have very sparse vectors given our sentence-level features and will be the foci of future research) and using boosted decision trees (Witten and Frank, 2000). Our goal was to recognize chapters from the titles in our corpus even when some titles were paraphrased into multiple books; in this context, titles are original works and paraphrased books are translations of these titles. For this, we assumed the existence of one legitimate book from each title. We used this book to train a model that captured the syntactic elements of expression used in this title. We used the remaining paraphrases of the title (i.e., the remaining books paraphrasing the title) as the test set—these paraphrases are considered to be plagiarized copies and should be identif</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>I. H. Witten and E. Frank. 2000. Data Mining: Practical Machine Learning Tools with Java Implementations. Morgan Kaufmann, San Francisco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>