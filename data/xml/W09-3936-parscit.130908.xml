<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.993708">
Turn-Yielding Cues in Task-Oriented Dialogue
</title>
<author confidence="0.998417">
Agustin Gravano
</author>
<affiliation confidence="0.9967515">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.912603">
New York, NY, USA
</address>
<email confidence="0.999305">
agus@cs.columbia.edu
</email>
<author confidence="0.995346">
Julia Hirschberg
</author>
<affiliation confidence="0.996746">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.912704">
New York, NY, USA
</address>
<email confidence="0.999547">
julia@cs.columbia.edu
</email>
<sectionHeader confidence="0.993915" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942555555556">
We examine a number of objective, au-
tomatically computable TURN-YIELDING
CUES — distinct prosodic, acoustic and
syntactic events in a speaker’s speech that
tend to precede a smooth turn exchange —
in the Columbia Games Corpus, a large
corpus of task-oriented dialogues. We
show that the likelihood of occurrence of
a turn-taking attempt from the interlocu-
tor increases linearly with the number of
cues conjointly displayed by the speaker.
Our results are important for improving
the coordination of speaking turns in in-
teractive voice-response systems, so that
systems can correctly estimate when the
user is willing to yield the conversational
floor, and so that they can produce their
own turn-yielding cues appropriately.
</bodyText>
<sectionHeader confidence="0.987413" genericHeader="categories and subject descriptors">
1 Introduction and Previous Research
</sectionHeader>
<bodyText confidence="0.999967407407407">
Users of state-of-the-art interactive voice response
(IVR) systems often find interactions with these
systems to be unsatisfactory. Part of this reac-
tion is due to deficiencies in speech recognition
and synthesis technologies, but some can also be
traced to coordination problems in the exchange
of speaking turns between system and user (Ward
et al., 2005; Raux et al., 2006). Users are not sure
when the system is ready to end its turn, and sys-
tems are not sure when users are ready to relin-
quish theirs. Currently, the standard method for
determining when a user is willing to yield the
conversational floor is to wait for a silence longer
than a prespecified threshold, typically ranging
from 0.5 to 1 second (Ferrer et al., 2003). How-
ever, this strategy is rarely used by humans, who
rely instead on cues from sources such as syntax,
acoustics and prosody to anticipate turn transitions
(Yngve, 1970). If such TURN-YIELDING CUES
could be modeled and incorporated in IVR sys-
tems, it should be possible to make faster, more
accurate turn-taking decisions, thus leading to a
more fluent interaction. Additionally, a better un-
derstanding of the mechanics of turn-taking could
be used to vary the speech output of IVR systems
to (i) produce turn-yielding cues when the sys-
tem is finished speaking and the user is expected
to speak next, and (ii) avoid producing such cues
when the system has more things to say. In this
paper we examine the existence of turn-yielding
cues in a large corpus of task-oriented dialogues
in Standard American English (SAE).
The question of what types of cues humans ex-
ploit for engaging in synchronized conversation
has been addressed by several studies. Duncan
(1972, inter alia) conjectures that speakers dis-
play complex signals at turn endings, composed
of one or more discrete turn-yielding cues, such
as the completion of a grammatical clause, or any
phrase-final intonation other than a plateau. Dun-
can also hypothesizes that the likelihood of a turn-
taking attempt by the listener increases linearly
with the number of such cues conjointly displayed
by the speaker. Subsequent studies have investi-
gated some of these hypotheses (Ford and Thomp-
son, 1996; Wennerstrom and Siegel, 2003). More
recent studies have investigated how to improve
IVR system’s the turn-taking decisions by incor-
porating some of the features found to correlate
with turn endings (Ferrer et al., 2003; Atterer et
al., 2008; Raux and Eskenazi, 2008). All of these
models are shown to improve over silence-based
techniques for predicting turn endings, motivating
further research. In this paper we present results
</bodyText>
<note confidence="0.709917">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 253–261,
</note>
<affiliation confidence="0.662768">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999204">
253
</page>
<bodyText confidence="0.9991365">
of a large, corpus-based study of turn-yielding
cues in the Columbia Games Corpus which veri-
fies some of Duncan’s hypotheses and adds addi-
tional cues to turn-taking behavior.
</bodyText>
<sectionHeader confidence="0.897962" genericHeader="method">
2 Materials and Method
</sectionHeader>
<bodyText confidence="0.997327636363636">
The materials for our study are taken from the
Columbia Games Corpus (Gravano, 2009), a col-
lection of 12 spontaneous task-oriented dyadic
conversations elicited from 13 native speakers of
SAE. In each session, two subjects were paid to
play a series of computer games requiring verbal
communication to achieve joint goals of identify-
ing and moving images on the screen, while seated
in a soundproof booth divided by a curtain to en-
sure that all communication was verbal. The sub-
jects’ speech was not restricted in any way, and
the games were not timed. The corpus contains
9 hours of dialogue, which were orthographically
transcribed; words were time-aligned to the source
by hand. Around 5.4 hours have also been into-
nationally transcribed using the ToBI framework
(Beckman and Hirschberg, 1994).
We automatically extracted a number of acous-
tic features from the corpus using the Praat toolkit
(Boersma and Weenink, 2001), including pitch,
intensity and voice quality features. Pitch slopes
were computed by fitting least-squares linear re-
gression models to the Fo track extracted from
given portions of the signal. Part-of-speech
(POS) tags were labeled automatically using Rat-
naparkhi’s maxent tagger trained on a subset of the
Switchboard corpus in lower-case with all punctu-
ation removed, to simulate spoken language tran-
scripts. All speaker normalizations were calcu-
lated using z-scores: z = (x − µVQ, where x
is a raw measurement, and µ and Q are the mean
and standard deviation for a speaker.
For our turn-taking studies, we define an
INTER-PAUSAL UNIT (IPU) as a maximal se-
quence of words surrounded by silence longer than
50 ms.1 A TURN then is defined as a maximal se-
quence of IPUs from one speaker, such that be-
tween any two adjacent IPUs there is no speech
from the interlocutor. Boundaries of IPUs and
turns are computed automatically from the time-
aligned transcriptions. Two trained annotators
classified each turn transition in the corpus using a
labeling scheme adapted from Beattie (1982) that
identifies, inter alia, SMOOTH SWITCHES — tran-
</bodyText>
<page confidence="0.53074">
150 ms was identified empirically to avoid stopgaps.
</page>
<bodyText confidence="0.99994488">
sitions from speaker A to speaker B such that (i)
A manages to complete her utterance, and (ii) no
overlapping speech occurs between the two con-
versational turns. Additionally, all continuations
from one IPU to the next within the same turn
were labeled automatically as HOLD transitions.
The complete labeling scheme is shown in the Ap-
pendix.
Our general approach consists in contrasting
IPUs immediately preceding smooth switches
(S) with IPUs immediately preceding holds (H).
(Note that in this paper we consider only non-
overlapping exchanges.) We hypothesize that
turn-yielding cues are more likely to occur before
S than before H. It is important to emphasize the
optionality of all turn-taking phenomena and de-
cisions: For H, turn-yielding cues — whatever
their nature — may still be present; and for S, they
may sometimes be absent. However, we hypothe-
size that their likelihood of occurrence should be
much higher before S. Finally, note that we do
not make claims regarding whether speakers con-
sciously produce turn-yielding cues, or whether
listeners consciously perceive and/or use them to
aid their turn-taking decisions.
</bodyText>
<sectionHeader confidence="0.98342" genericHeader="method">
3 Individual Turn-Yielding Cues
</sectionHeader>
<bodyText confidence="0.999752428571429">
Figures 1 and 2 show the speaker-normalized
mean of a number of objective, automatically
computed variables for IPUs preceding S and H.
In all cases, one-way ANOVA and Kruskal-Wallis
tests reveal significant differences (at p &lt; 0.001)
between the two groups. We we discuss these re-
sults in detail below.
</bodyText>
<subsectionHeader confidence="0.992399">
3.1 Intonation
</subsectionHeader>
<bodyText confidence="0.999790857142857">
The literature contains frequent mention of the
propensity of speaking turns to end in any into-
nation contour other than a plateau (a sustained
pitch level, neither rising nor falling). We first
analyze the categorical prosodic labels in the por-
tion of the Columbia Games Corpus annotated us-
ing the ToBI annotations. We tabulate the phrase
</bodyText>
<table confidence="0.995837375">
S H
H-H% 484 22.1% 513 9.1%
[!]H-L% 289 13.2% 1680 29.9%
L-H% 309 14.1% 646 11.5%
L-L% 1032 47.2% 1387 24.7%
No boundary tone 16 0.7% 1261 22.4%
Other 56 2.6% 136 2.4%
Total 2186 100% 5623 100%
</table>
<tableCaption confidence="0.999909">
Table 1: ToBI phrase accents and boundary tones.
</tableCaption>
<page confidence="0.998524">
254
</page>
<figureCaption confidence="0.999744">
Figure 1: Individual turn-yielding cues: intonation, speaking rate and IPU duration.
</figureCaption>
<bodyText confidence="0.999977906976744">
accent and boundary tone labels assigned to the
end of each IPU, and compare their distribution
for the S and H turn exchange types, as shown
in Table 1. A chi-square test indicates that there
is a significant departure from a random distribu-
tion (x2 =1102.5, df =5, pPz�0). Only 13.2% of
all IPUs immediately preceding a smooth switch
(S) — where turn-yielding cues are most likely
present — end in a plateau ([!]H-L%); most of the
remaining ones end in either a falling pitch (L-L%)
or a high rise (H-H%). For IPUs preceding a hold
(H) the counts approximate a uniform distribution,
with the plateau contours being the most common,
supporting the hypothesis that this contour func-
tions as a TURN-HOLDING CUE (that is, a cue that
typically prevents turn-taking attempts from the
listener). The high counts for the falling contour
preceding a hold (24.7%) may be explained by the
fact that, as discussed above, taking the turn is
optional for the listener, who may choose not to
act despite hearing some turn-yielding cues. It is
not entirely clear what the role is of the low-rising
contour (L-H%), as it occurs in similar proportions
before S and before H. Finally, we note that the ab-
sence of a boundary tone works as a strong indi-
cation that the speaker has not finished speaking,
since nearly all (98%) IPUs without a boundary
tone precede a hold transition.
Next, we examine four objective acoustic ap-
proximations of this perceptual feature: the ab-
solute value of the speaker-normalized F0 slope,
both raw and stylized, computed over the final 200
and 300 ms of each IPU. The case of a plateau
corresponds to a value of F0 slope close to zero;
the other case, of either a rising or a falling pitch,
corresponds to a high absolute value of F0 slope.
As shown in Figure 1, we find that the final slope
before S is significantly higher than before H in
all four cases. These findings provide additional
support to the hypothesis that turns tend to end in
falling and high-rising final intonations, and pro-
vide automatically identifiable indicators of this
turn-yielding cue.
</bodyText>
<subsectionHeader confidence="0.999899">
3.2 Speaking rate
</subsectionHeader>
<bodyText confidence="0.999916466666667">
Duncan (1972) hypothesizes a “drawl on the fi-
nal syllable or on the stressed syllable of a termi-
nal clause” [p. 287] as a turn-yielding cue, which
would probably correspond to a noticeable de-
crease in speaking rate. We examine this hypothe-
sis in our corpus using two common definitions of
speaking rate: syllables per second and phonemes
per second. Syllable and phoneme counts were
estimated from dictionary lookup, and word dura-
tions were extracted from the manual orthographic
alignments. Figure 1 shows that both measures,
computed over either the whole IPU or its final
word, are significantly higher before S than be-
fore H, which indicates an increase in speaking
rate before turn boundaries rather than Duncan’s
hypothesized drawl.
Furthermore, the speaking rate is, in both cases
(before S and before H), significantly slower on
the final word than over the whole IPU, a finding
that is in line with phonological theories that pre-
dict a segmental lengthening near prosodic phrase
boundaries (Wightman et al., 1992). This finding
may indeed correspond to the drawl or lengthen-
ing described by Duncan before turn boundaries.
However, it seems to be the case — at least for
our corpus — that the final lengthening tends to
occur at all phrase final positions, not just at turn
endings. In fact, our results indicate that the fi-
nal lengthening is more prominent in turn-medial
IPUs than in turn-final ones.
</bodyText>
<page confidence="0.997189">
255
</page>
<figureCaption confidence="0.997869">
Figure 2: Individual turn-yielding cues: intensity, pitch and voice quality.
</figureCaption>
<subsectionHeader confidence="0.996043">
3.3 IPU duration and acoustic cues
</subsectionHeader>
<bodyText confidence="0.999973095238095">
In the Columbia Games Corpus, we find that turn-
final IPUs tend to be significantly longer than turn-
medial ones, both when measured in seconds and
in number of words (Figure 1). This suggests that
IPU duration could function as a turn-yielding cue,
supporting similar findings in perceptual experi-
ments by Cutler and Pearson (1986).
We also find that IPUs followed by S have a
mean intensity significantly lower than those fol-
lowed by H (computed over the IPU-final 500 and
1000 ms, see Figure 2). Also, the differences in-
crease when moving towards the end of the IPU.
This suggests that speakers tend to lower their
voices when approaching potential turn bound-
aries, whereas they reach turn-internal pauses with
a higher intensity.
Phonological theories conjecture a declination
in the pitch level, which tends to decrease grad-
ually within utterances, and across utterances
within the same discourse segment, as a conse-
quence of a gradual compression of the pitch range
(Pierrehumbert and Hirschberg, 1990). For con-
versational turns, then, we would expect to find
that speakers tend to lower their pitch level as they
reach potential turn boundaries. This hypothesis
is verified by the dialogues in our corpus, where
we find that IPUs preceding S have a significantly
lower mean pitch than those preceding H (Figure
2). In consequence, pitch level may also work as a
turn-yielding cue.
Next we examine three acoustic features asso-
ciated with the perception of voice quality: jit-
ter, shimmer and noise-to-harmonics ratio (NHR)
(Bhuta et al., 2004), computed over the IPU-final
500 and 1000 ms (Figure 2). We compute jit-
ter and shimmer only over voiced frames for im-
proved robustness. For all three features, the mean
value for IPUs preceding S is significantly higher
than for IPUs preceding H, with the difference in-
creasing towards the end of the IPU. Therefore,
voice quality seems to play a clear role as a turn-
yielding cue.
</bodyText>
<subsectionHeader confidence="0.988135">
3.4 Lexical cues
</subsectionHeader>
<bodyText confidence="0.991833484848485">
Stereotyped expressions such as you know or I
think have been proposed in the literature as lex-
ical turn-yielding cues. However, in the Games
Corpus we find that none of the most frequent
IPU-final unigrams and bigrams, both preceding
S and H, correspond to such expressions (see Ta-
ble A.1 in the Appendix). Instead, such unigrams
and bigrams are specific to the computer games
in which the subjects participated. For example,
the game objects tended to be spontaneously de-
scribed by subjects from top to bottom and from
left to right, as shown in the following excerpt
(pauses are indicated with #):
A: I have a blue lion on top # with a lemon
in the bottom left # and a yellow crescent
moon in- # i- # in the bottom right
B: oh okay [...]
In consequence, bigrams such as lower right and
bottom right are common before S, while on top
or bottom left are common before H. These are all
task-specific lexical constructions and do not con-
stitute stereotyped expressions in the traditional
sense.
Also very common among the most frequent
IPU-final expressions are AFFIRMATIVE CUE
WORDS — heavily overloaded words, such as
okay or yeah, that are used both to initiate and
to end discourse segments, among other functions
(Gravano et al., 2007). The occurrence of these
words does not constitute a turn-yielding or turn-
holding cue per se; rather, additional contextual,
acoustic and prosodic information is needed to dis-
ambiguate their meaning.
</bodyText>
<page confidence="0.989954">
256
</page>
<bodyText confidence="0.9999892">
While we do not find clear examples of lexical
turn-yielding cues in our task-oriented corpus, we
do find two lexical turn-holding cues: word frag-
ments (e.g., incompl-) and filled pauses (e.g., uh,
um). Of the 8123 IPUs preceding H, 6.7% end
in a word fragment, and 9.4% in a filled pause.
By constrast, only 0.3% of the 3246 IPUs preced-
ing S end in a word fragment, and 1% in a filled
pause. These differences suggest that, after either
a word fragment or a filled pause, the speaker is
much more likely to intend to continue holding
the floor. This notion of disfluencies functioning
as a turn-taking cue has been studied by Goodwin
(1981), who shows that they maybe used to secure
the listener’s attention at turn beginnings.
</bodyText>
<subsectionHeader confidence="0.864844">
3.5 Textual completion
</subsectionHeader>
<bodyText confidence="0.983530333333333">
Several authors (Duncan, 1972; Ford and Thomp-
son, 1996; Wennerstrom and Siegel, 2003) claim
that some form of syntactic or semantic comple-
tion, independent of intonation and interactional
import, functions as a turn-yielding cue. Although
some call this syntactic completion, since all au-
thors acknowledge the need for semantic and dis-
course information in judging it, we choose the
more neutral term TEXTUAL COMPLETION for
this phenomenon. We annotated a portion of
our corpus with respect to textual completion and
trained a machine learning (ML) classifier to auto-
matically label the whole corpus. From these an-
notations we then examined how textual comple-
tion labels relate to turn-taking categories in the
corpus.
3.5.1. Manual labeling: In conversation, lis-
teners judge textual completion incrementally and
without access to later material. To simulate these
conditions in the labeling task, annotators were
asked to judge the textual completion of a turn up
to a target pause from the written transcript alone,
without listening to the speech. They were al-
lowed to read the transcript of the full previous
turn by the other speaker (if any), but they were
not given access to anything after the target pause.
These are two sample tokens:
A: the lion’s left paw our front
B: yeah and it’s th- right so the
A: and then a tea kettle and then the wine
B: okay well I have the big shoe and the wine
We selected 400 tokens at random from the Games
Corpus; the target pauses were also chosen at ran-
dom. Three annotators labeled each token inde-
pendently as either complete or incomplete ac-
cording to these guidelines: Determine whether
you believe what speaker B has said up to this
point could constitute a complete response to what
speaker A has said in the previous turn/segment.
Note: If there are no words by A, then B is begin-
ning a new task, such as describing a card or the
location of an object. To avoid biasing the results,
annotators were not given the turn-taking labels of
the tokens. Inter-annotator reliability is measured
by Fleiss’ n at 0.814, which corresponds to the
‘almost perfect’ agreement category. The mean
pairwise agreement between the three subjects is
90.8%. For the cases in which there is disagree-
ment between the three annotators, we adopt the
MAJORITY LABEL as our gold standard; that is,
the label chosen by two annotators.
</bodyText>
<subsectionHeader confidence="0.334282">
3.5.2. Automatic classification: Next, we
</subsectionHeader>
<bodyText confidence="0.99984571875">
trained a ML model using the 400 manually anno-
tated tokens as training data to automatically clas-
sify all IPUs in the corpus as either complete or in-
complete. For each IPU we extracted a number of
lexical and syntactic features from the current turn
up to the IPU itself: lexical identity of the IPU-
final word (w); POS tags and simplified POS tags
(N, V, Adj, Adv, Other) of w and of the IPU-final
bigram; number of words in the IPU; a binary flag
indicating if w is a word fragment; size and type of
the biggest (bp) and smallest (sp) phrase that end
in w; binary flags indicating if each of bp and sp is
a major phrase (NP, VP, PP, ADJP, ADVP); binary
flags indicating if w is the head of each of bp and
sp. We chose these features in order to capture as
much lexical and syntactic information as possible
from the transcripts. The syntactic features were
computed using two different parsers: the Collins
statistical parser (Collins, 2003) and CASS, a par-
tial parser especially designed for use with noisy
text (Abney, 1996). We experimented with the
learners listed in Table 2, using the implementa-
tions provided in the WEKA ML toolkit (Witten
and Frank, 2000). Table 2 shows the accuracy of
the majority-class baseline and of each classifier,
using 10-fold cross validation on the 400 train-
ing data points, and the mean pairwise agreement
by the three human labelers. The linear-kernel
support-vector-machine (SVM) classifier achieves
the highest accuracy, significantly outperforming
the baseline, and approaching the mean agreement
of human labelers.
</bodyText>
<page confidence="0.993645">
257
</page>
<table confidence="0.999561125">
Classifier Accuracy
Majority-class (‘complete’) 55.2%
C4.5 (decision trees) 55.2%
Ripper (propositional rules) 68.2%
Bayesian networks 75.7%
SVM, RBF kernel (c = 1, a = 10−12) 78.2%
SVM, linear kernel (c = 1, a = 10−12) 80.0%
Human labelers (mean agreement) 90.8%
</table>
<tableCaption confidence="0.997236">
Table 2: Textual completion: ML results.
</tableCaption>
<bodyText confidence="0.9622830625">
3.5.3. Results: First we examine the tokens that
were manually labeled by the human annotators.
Of the 100 tokens followed by S, 91 were labeled
textually complete, a significantly higher propor-
tion than the 42% followed by H that were labeled
complete (x2=51.7, df =1, pP0). Next, we used
our highest performing classifier, the linear-kernel
SVM, to automatically label all IPUs in the cor-
pus. Of the 3246 IPUs preceding S, 2649 (81.6%)
were labeled textually complete, and about half of
all IPUs preceding H (4272/8123, or 52.6%) were
labeled complete. The difference is also signifi-
cant (x2 = 818.7, df = 1, p Pz� 0). These results
suggest that textual completion as defined above
constitutes a necessary, but not sufficient, turn-
yielding cue.
</bodyText>
<sectionHeader confidence="0.884589" genericHeader="method">
4 Combining Turn-Yielding Cues
</sectionHeader>
<bodyText confidence="0.999859083333334">
So far, we have shown strong evidence supporting
the existence of individual acoustic, prosodic and
textual turn-yielding cues. Now we shift our atten-
tion to the manner in which they combine together
to form more complex turn-yielding signals. For
each individual cue type, we choose two or three
features shown to correlate strongly with smooth
switches, as shown in Table 3 (e.g., the speaking
rate cue is represented by two automatic features:
syllables and phonemes per second over the whole
IPU). We consider a cue c to be PRESENT on IPU
u if, for any feature f modeling c, the value of f
on u is closer to fS than to fH, where fS and fH
are the mean values of f across all IPUs preced-
ing S and H, respectively. Otherwise, we say c is
ABSENT on u. Also, we automatically annotate all
IPUs in the corpus for textual completion using the
linear-kernel SVM classifier described in Section
3.5. IPUs classified as complete are considered to
bear the textual completion turn-yielding cue.
We first analyze the frequency of occurrence
of conjoined individual turn-yielding cues. Ta-
ble 4 shows the top frequencies of complex turn-
yielding cues for IPUs immediately before smooth
</bodyText>
<table confidence="0.999651214285714">
Individual cues Automatic features
Intonation Abs(F0 slope) over IPU-final 200 ms
Abs(F0 slope) over IPU-final 300 ms
Speaking rate Syllables per second over whole IPU
Phonemes per second over whole IPU
Intensity level Mean intensity over IPU-final 500 ms
Mean intensity over IPU-final 1000 ms
Pitch level Mean pitch over IPU-final 500 ms
Mean pitch over IPU-final 1000 ms
IPU duration IPU duration in ms
Number of words in IPU
Voice quality Jitter over IPU-final 500 ms
Shimmer over IPU-final 500 ms
NHR over IPU-final500 ms
</table>
<tableCaption confidence="0.928719">
Table 3: Features used to estimate the presence of
individual turn-yielding cues.
</tableCaption>
<figureCaption confidence="0.9445308">
switches (S) and holds (H). The most frequent
cases before S correspond to all, or almost all, cues
present at once. For IPUs preceding a hold (H),
the opposite is true: those with no cues, or with
just one or two, represent the most frequent cases.
</figureCaption>
<table confidence="0.997083615384615">
S H
Cues Count Cues Count
1234567 267 ...4... 392
.234567 226 247
7
1234.67 138 223
.234.67 109 ...4..7 218
.23..67 98 ...45 178
..34567 94 .2....7 166
123..67 93 1234.67 163
.2.4567 73 .2..5.7 157
... ...
Total 3246 Total 8123
</table>
<tableCaption confidence="0.862058">
Table 4: Top frequencies of complex turn-yielding
cues for IPUs preceding S and H. A digit indicates
</tableCaption>
<listItem confidence="0.95759725">
the presence of a specific cue; a dot, its absence.
1: Intonation; 2: Speaking rate; 3: Intensity level;
4: Pitch level; 5: IPU duration; 6: Voice quality;
7: Textual completion.
</listItem>
<bodyText confidence="0.945746266666667">
Table 5 shows the same results, now grouping
together all IPUs with the same number of cues,
independently of the cue types. Again, we observe
that larger proportions of IPUs preceding S present
more conjoined cues than IPUs preceding H.
Next we look at how the likelihood of a turn-
taking attempt varies with respect to the number
of individual cues displayed by the speaker, a rela-
tion hypothesized to be linear by Duncan (1972).
Figure 3 shows the proportion of IPUs with 0-7
cues present that are followed by a turn-taking at-
tempt from the interlocutor.2 The dashed line cor-
2 The proportion of turn-taking attempts is computed for
each cue count as the number of S and PI divided by the num-
ber of S, PI, H and BC, according to our labeling scheme.
</bodyText>
<page confidence="0.989029">
258
</page>
<table confidence="0.9996511">
Cue count S H
0 4 0.1% 223 2.7%
1 52 1.6% 970 11.9%
2 241 7.4% 1552 19.1%
3 518 16.0% 1829 22.5%
4 740 22.8% 1666 20.5%
5 830 25.6% 1142 14.1%
6 594 18.3% 611 7.5%
7 267 8.2% 130 1.6%
Total 3246 100% 8123 100%
</table>
<tableCaption confidence="0.911780666666667">
Table 5: Distribution of the number of turn-
yielding cues displayed in IPUs preceding smooth
switches (S) and hold transitions (H).
</tableCaption>
<figureCaption confidence="0.961233333333333">
Figure 3: Percentage of turn-taking attempts from
the listener (either S or PI) following IPUs con-
taining 0-7 turn-yielding cues.
</figureCaption>
<bodyText confidence="0.999130384615385">
responds to a linear model fitted to the data (Pear-
son’s correlation test: r2 = 0.969), and the contin-
uous line, to a quadratic model (r2 = 0.995). The
high correlation coefficient of the linear model
supports Duncan’s hypothesis, that the likelihood
of a turn-taking attempt by the interlocutor in-
creases linearly with the number of individual cues
displayed by the speaker. However, an ANOVA test
reveals that the quadratic model fits the data sig-
nificantly better than the linear model (F(1, 5) =
23.01; p = 0.005), even though the curvature of
the quadratic model is only moderate, as can be
observed in the figure.
</bodyText>
<sectionHeader confidence="0.981014" genericHeader="method">
5 Speaker Variation
</sectionHeader>
<bodyText confidence="0.999926111111111">
To investigate possible speaker dependence in our
turn-yielding cues, we examine evidence for each
cue for each of our thirteen speakers. Table 6
summarizes this data. For each speaker, a check
(.\/) indicates that there is significant evidence of
the speaker producing the corresponding individ-
ual turn-yielding cue (at p &lt; 0.05, using the same
statistical tests described in the previous sections).
Five speakers show evidence of all seven cues,
</bodyText>
<table confidence="0.9996482">
Speaker 101 102 103 104 105 106 107 108 109 110 111 112 113
Intonation ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./
Spk. rate ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./
Intensity ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./
Pitch ./ ./ ./ ./ ./ ./ ./
Completion ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./
Voice quality ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./
IPU duration ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./ ./
LM r2 .92 .93 .82 .88 .97 .96 .95 .95 .97 .91 .95 .97 .89
QM r2 .98 .95 .95 .92 .98 .98 .96 .95 .99 .94 .98 .99 .90
</table>
<tableCaption confidence="0.9087235">
Table 6: Summary of results for each individual
speaker.
</tableCaption>
<bodyText confidence="0.998433466666667">
while the remaining eight speakers show either
five or six cues. Pitch level is the least reliable
cue, present only for seven subjects. Notably, the
cues related to speaking rate, textual completion,
voice quality, and IPU duration are present for all
thirteen speakers.
The two bottom rows in Table 6 show the cor-
relation coefficients (r2) of linear and quadratic
regressions performed on the data from each
speaker. In all cases, the coefficients are very high.
The fit of the quadratic model is significantly bet-
ter for six speakers (shown in bold typeface); for
the remaining seven speakers, both models pro-
vide statistically indistinguishable explanations of
the data.
</bodyText>
<sectionHeader confidence="0.998527" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999947521739131">
We have examined seven turn-yielding cues —
i.e., seven measurable events that take place with
a significantly higher frequency on IPUs preced-
ing smooth turn switches than on IPUs preceding
hold transitions. These events may be summarized
as follows: (i) a falling or high-rising intonation at
the end of the IPU; (ii) an increased speaking rate;
(iii) a lower intensity level; (iv) a lower pitch level;
(v) a longer IPU duration; (vi) a higher value of
three voice quality features: jitter, shimmer, and
NHR; and (vii) a point of textual completion. We
have also shown that, when several turn-yielding
cues occur simultaneously, the likelihood of a sub-
sequent turn-taking attempt by the interlocutor in-
creases in an almost linear fashion.
We propose that these findings can be used to
improve some turn-taking decisions of state-of-
the-art IVR systems. For example, if a system
wishes to yield the floor to a user, it should in-
clude in its output as many of the described cues
as possible. Conversely, when the user is speak-
ing, the system may detect appropriate moments
to take the turn by estimating the presence of turn-
</bodyText>
<page confidence="0.995153">
259
</page>
<bodyText confidence="0.999928533333333">
yielding cues at every silence. If the number of de-
tected cues is high enough, then the system should
take the turn; otherwise, it should remain silent.
Two assumptions of our study are that turn-
yielding cues are binary and all contribute equally
to the overall “count”. In future research we
will explore alternative methods of combining and
weighting the different features — by means of
multiple linear regression, for example — in or-
der to experiment with more sophisticated models
of turn-yielding behavior. We also plan to exam-
ine new turn-yielding cues, paying special atten-
tion to additional voice quality features, given the
promising results obtained for jitter, shimmer and
noise-to-harmonics ratio.
</bodyText>
<sectionHeader confidence="0.999082" genericHeader="method">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999939833333333">
This work was funded in part by NSF IIS-
0307905. We thank Stefan Benus, Enrique Hen-
estroza, Elisa Sneed and Gregory Ward, for valu-
able discussion and for their help in collecting and
labeling the data, and the anonymous reviewers for
helpful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998186" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.998945220779221">
S. Abney. 1996. Partial parsing via finite-state cas-
cades. Journal of Natural Language Engineering,
2(4):337–344.
M. Atterer, T. Baumann, and D. Schlangen. 2008. To-
wards incremental end-of-utterance detection in di-
alogue systems. In Proceedings of Coling, Manch-
ester, UK.
G. W. Beattie. 1982. Turn-taking and interruption
in political interviews: Margaret Thatcher and Jim
Callaghan compared and contrasted. Semiotica,
39(1/2):93–114.
M. E. Beckman and J. Hirschberg. 1994. The ToBI
annotation conventions. Ohio State University.
T. Bhuta, L. Patrick, and J. D. Garnett. 2004. Per-
ceptual evaluation of voice quality and its correla-
tion with acoustic measurements. Journal of Voice,
18(3):299–304.
P. Boersma and D. Weenink. 2001. Praat: Doing pho-
netics by computer. http://www.praat.org.
M. J. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589–637.
E. A. Cutler and M. Pearson. 1986. On the analysis of
prosodic turn-taking cues. In C. Johns-Lewis, Ed.,
Intonation in Discourse, pp. 139–156. College-Hill.
S. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23(2):283–292.
L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A
prosody-based approach to end-of-utterance detec-
tion that does not require speech recognition. In
Proceedings of ICASSP.
C. E. Ford and S. A. Thompson. 1996. Interactional
units in conversation: Syntactic, intonational and
pragmatic resources for the management of turns. In
E. Ochs, E. A. Schegloff, and S. A. Thompson, Eds.,
Interaction and Grammar, pp. 134–184. Cambridge
University Press.
C. Goodwin. 1981. Conversational Organization:
Interaction between Speakers and Hearers. Aca-
demic Press.
A. Gravano, S. Benus, J. Hirschberg, S. Mitchell, and
I. Vovsha. 2007. Classification of discourse func-
tions of affirmative words in spoken dialogue. In
Proceedings of Interspeech.
A. Gravano. 2009. Turn-Taking and Affirmative Cue
Words in Task-Oriented Dialogue. Ph.D. thesis,
Columbia University, New York.
J. Pierrehumbert and J. Hirschberg. 1990. The mean-
ing of intonational contours in the interpretation of
discourse. In P. Cohen, J. Morgan, and M. Pol-
lack, Eds., Intentions in Communication, pp. 271–
311. MIT Pr.
A. Raux and M. Eskenazi. 2008. Optimizing endpoint-
ing thresholds using dialogue features in a spoken
dialogue system. In Proceedings of SIGdial.
A. Raux, D. Bohus, B. Langner, A. W. Black, and
M. Eskenazi. 2006. Doing research on a deployed
spoken dialogue system: One year of Let’s Go! ex-
perience. In Proceedings of Interspeech.
N. G. Ward, A. G. Rivera, K. Ward, and D. G. Novick.
2005. Root causes of lost time and user stress in
a simple dialog system. In Proceedings of Inter-
speech.
A. Wennerstrom and A. F. Siegel. 2003. Keeping the
floor in multiparty conversations: Intonation, syn-
tax, and pause. Discourse Processes, 36(2):77–107.
C. W. Wightman, S. Shattuck-Hufnagel, M. Ostendorf,
and P. J. Price. 1992. Segmental durations in the
vicinity of prosodic phrase boundaries. The Journal
of the Acoustical Society of America, 91:1707.
I. H. Witten and E. Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
V. H. Yngve. 1970. On getting a word in edgewise.
Sixth Regional Meeting of the Chicago Linguistic
Society, 6:657–677.
</reference>
<page confidence="0.992258">
260
</page>
<figure confidence="0.936404638888889">
For each turn by speaker S2, where S1 is the other speaker, label S2’s turn as follows:
Is S2’s utterance in response to S1’s utterance and indicates only
“I’m still here / I hear you and please continue”?(1)
no
✟_✟ ✟
✟✙
Simultaneous speech present?
❍❍❍❍❥
yes
Simultaneous speech present?
✟ ❍❍❍❍❥ ✟ ❍❍❍❍❥
yes ✟ ✟ no yes ✟ ✟ no
✟✙ ✟✙
✟
yes ✟ ✟
✟✙
❍❍❍❍❥
no
Backchannel
with overlap
(BC O)
S2 is successful?
Smooth
switch (S)
Pause interruption
(PI)
Backchannel
(BC)
✟ ✟
yes✙
Butting-in
(BI)
✟ ✟
yes✙
Overlap Interruption
(O) (I)
</figure>
<figureCaption confidence="0.99864">
Figure A.1: Turn-taking labeling scheme.
</figureCaption>
<sectionHeader confidence="0.747821" genericHeader="method">
Appendix: Turn-Taking Labeling Scheme
</sectionHeader>
<bodyText confidence="0.930797454545454">
We adopt a slightly modified version of Beat-
tie’s (1982) labeling scheme, depicted in Fig-
ure A.1. We incorporate backchannels (excluded
from Beattie’s study) by adding the decision
marked (1) at the root of the decision tree, for
which we use the annotations described in Gra-
vano et al. (2007). For the decision marked (2), we
use Beattie’s informal definition of utterance com-
pleteness: “Completeness [is] judged intuitively,
taking into account the intonation, syntax, and
meaning of the utterance” [p. 100]. All continu-
ations from one IPU to the next within the same
turn are labeled automatically H, for ‘hold’. Also,
we identify three special cases that do not corre-
spond to actual turn exchanges:
Task beginnings: Turns beginning a new game
task are labeled X1.
Continuations after BC or BC O: If a turn t is a
continuation after a backchannel b from the other
speaker, it is labeled X2 O if t and b overlap, or
X2 if not.
Simultaneous starts: Fry (1975) reports that hu-
mans require at least 210 ms to react verbally to a
verbal stimulus.3 Thus, if two turns begin within
210 ms of each other, they are most probably con-
nected to preceding events than to one another. In
Figure A.2, A1, A2 and B1 represent turns from
speakers A and B. Most likely, A2 is simply a
continuation from A1, and B1 occurs in response
3D. B. Fry. 1975. Simple reaction-times to speech and
non-speech stimuli. Cortex, 11(4):355-60.
to A1. Thus, B1 is labeled with respect to A1 (not
A2), and A2 is labeled X3.
</bodyText>
<equation confidence="0.552412">
A1 X A2
B1
</equation>
<figureCaption confidence="0.979648">
Figure A.2: Simultaneous start (|y −x |&lt; 210ms).
</figureCaption>
<table confidence="0.950861821428572">
S Count H Count
okay 241 okay 402
yeah 167 on top 172
lower right 85 um 136
bottom right 74 the top 117
the right 59 of the 67
hand corner 52 blue lion 57
lower left 43 bottom left 56
the iron 37 with the 54
the onion 33 the um 54
bottom left 31 yeah 53
the ruler 30 the left 48
mm-hm 30 and 48
right 28 lower left 46
right corner 27 uh 45
the bottom 26 oh 45
the left 24 and a 45
crescent moon 23 alright 44
the lemon 22 okay um 43
the moon 20 the uh 42
tennis racket 20 the right 41
blue lion 19 the bottom 39
the whale 18 I have 39
the crescent 18 yellow lion 37
the middle 17 the middle 37
of it 17 I’ve got 34
... ...
Total 3246 Total 8123
</table>
<figure confidence="0.974709625">
S1’s utterance
complete?(2)
✟ ✟ ❍❍❍❍❥
no
S1’s utterance
complete?(2)
✟ ✟ ❍❍❍❍❥
no
</figure>
<tableCaption confidence="0.550156">
Table A.1: 25 most frequent final bigrams preced-
ing smooth turn switches (S) and hold transitions
(H). (See Section 3.4.)
</tableCaption>
<page confidence="0.996769">
261
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.226367">
<title confidence="0.999845">Turn-Yielding Cues in Task-Oriented Dialogue</title>
<author confidence="0.992784">Agustin</author>
<affiliation confidence="0.999944">Department of Computer</affiliation>
<address confidence="0.875677">Columbia</address>
<author confidence="0.562529">New York</author>
<author confidence="0.562529">NY</author>
<email confidence="0.99629">agus@cs.columbia.edu</email>
<author confidence="0.983534">Julia</author>
<affiliation confidence="0.999928">Department of Computer</affiliation>
<address confidence="0.7069185">Columbia New York, NY,</address>
<email confidence="0.999783">julia@cs.columbia.edu</email>
<abstract confidence="0.999390210526316">We examine a number of objective, aucomputable distinct prosodic, acoustic and syntactic events in a speaker’s speech that tend to precede a smooth turn exchange — in the Columbia Games Corpus, a large corpus of task-oriented dialogues. We show that the likelihood of occurrence of a turn-taking attempt from the interlocutor increases linearly with the number of cues conjointly displayed by the speaker. Our results are important for improving the coordination of speaking turns in interactive voice-response systems, so that systems can correctly estimate when the user is willing to yield the conversational floor, and so that they can produce their own turn-yielding cues appropriately.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Partial parsing via finite-state cascades.</title>
<date>1996</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="19534" citStr="Abney, 1996" startWordPosition="3242" endWordPosition="3243"> words in the IPU; a binary flag indicating if w is a word fragment; size and type of the biggest (bp) and smallest (sp) phrase that end in w; binary flags indicating if each of bp and sp is a major phrase (NP, VP, PP, ADJP, ADVP); binary flags indicating if w is the head of each of bp and sp. We chose these features in order to capture as much lexical and syntactic information as possible from the transcripts. The syntactic features were computed using two different parsers: the Collins statistical parser (Collins, 2003) and CASS, a partial parser especially designed for use with noisy text (Abney, 1996). We experimented with the learners listed in Table 2, using the implementations provided in the WEKA ML toolkit (Witten and Frank, 2000). Table 2 shows the accuracy of the majority-class baseline and of each classifier, using 10-fold cross validation on the 400 training data points, and the mean pairwise agreement by the three human labelers. The linear-kernel support-vector-machine (SVM) classifier achieves the highest accuracy, significantly outperforming the baseline, and approaching the mean agreement of human labelers. 257 Classifier Accuracy Majority-class (‘complete’) 55.2% C4.5 (decis</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>S. Abney. 1996. Partial parsing via finite-state cascades. Journal of Natural Language Engineering, 2(4):337–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Atterer</author>
<author>T Baumann</author>
<author>D Schlangen</author>
</authors>
<title>Towards incremental end-of-utterance detection in dialogue systems.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="3460" citStr="Atterer et al., 2008" startWordPosition="544" endWordPosition="547"> turn-yielding cues, such as the completion of a grammatical clause, or any phrase-final intonation other than a plateau. Duncan also hypothesizes that the likelihood of a turntaking attempt by the listener increases linearly with the number of such cues conjointly displayed by the speaker. Subsequent studies have investigated some of these hypotheses (Ford and Thompson, 1996; Wennerstrom and Siegel, 2003). More recent studies have investigated how to improve IVR system’s the turn-taking decisions by incorporating some of the features found to correlate with turn endings (Ferrer et al., 2003; Atterer et al., 2008; Raux and Eskenazi, 2008). All of these models are shown to improve over silence-based techniques for predicting turn endings, motivating further research. In this paper we present results Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 253–261, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 253 of a large, corpus-based study of turn-yielding cues in the Columbia Games Corpus which verifies some of Duncan’s hypotheses and adds additional cues to turn-taking behavior. 2 Materi</context>
</contexts>
<marker>Atterer, Baumann, Schlangen, 2008</marker>
<rawString>M. Atterer, T. Baumann, and D. Schlangen. 2008. Towards incremental end-of-utterance detection in dialogue systems. In Proceedings of Coling, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Beattie</author>
</authors>
<title>Turn-taking and interruption in political interviews: Margaret Thatcher and Jim Callaghan compared and contrasted.</title>
<date>1982</date>
<booktitle>Semiotica,</booktitle>
<pages>39--1</pages>
<contexts>
<context position="6081" citStr="Beattie (1982)" startWordPosition="966" endWordPosition="967">: z = (x − µVQ, where x is a raw measurement, and µ and Q are the mean and standard deviation for a speaker. For our turn-taking studies, we define an INTER-PAUSAL UNIT (IPU) as a maximal sequence of words surrounded by silence longer than 50 ms.1 A TURN then is defined as a maximal sequence of IPUs from one speaker, such that between any two adjacent IPUs there is no speech from the interlocutor. Boundaries of IPUs and turns are computed automatically from the timealigned transcriptions. Two trained annotators classified each turn transition in the corpus using a labeling scheme adapted from Beattie (1982) that identifies, inter alia, SMOOTH SWITCHES — tran150 ms was identified empirically to avoid stopgaps. sitions from speaker A to speaker B such that (i) A manages to complete her utterance, and (ii) no overlapping speech occurs between the two conversational turns. Additionally, all continuations from one IPU to the next within the same turn were labeled automatically as HOLD transitions. The complete labeling scheme is shown in the Appendix. Our general approach consists in contrasting IPUs immediately preceding smooth switches (S) with IPUs immediately preceding holds (H). (Note that in th</context>
</contexts>
<marker>Beattie, 1982</marker>
<rawString>G. W. Beattie. 1982. Turn-taking and interruption in political interviews: Margaret Thatcher and Jim Callaghan compared and contrasted. Semiotica, 39(1/2):93–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Beckman</author>
<author>J Hirschberg</author>
</authors>
<title>The ToBI annotation conventions.</title>
<date>1994</date>
<institution>Ohio State University.</institution>
<contexts>
<context position="4871" citStr="Beckman and Hirschberg, 1994" startWordPosition="766" endWordPosition="769">3 native speakers of SAE. In each session, two subjects were paid to play a series of computer games requiring verbal communication to achieve joint goals of identifying and moving images on the screen, while seated in a soundproof booth divided by a curtain to ensure that all communication was verbal. The subjects’ speech was not restricted in any way, and the games were not timed. The corpus contains 9 hours of dialogue, which were orthographically transcribed; words were time-aligned to the source by hand. Around 5.4 hours have also been intonationally transcribed using the ToBI framework (Beckman and Hirschberg, 1994). We automatically extracted a number of acoustic features from the corpus using the Praat toolkit (Boersma and Weenink, 2001), including pitch, intensity and voice quality features. Pitch slopes were computed by fitting least-squares linear regression models to the Fo track extracted from given portions of the signal. Part-of-speech (POS) tags were labeled automatically using Ratnaparkhi’s maxent tagger trained on a subset of the Switchboard corpus in lower-case with all punctuation removed, to simulate spoken language transcripts. All speaker normalizations were calculated using z-scores: z </context>
</contexts>
<marker>Beckman, Hirschberg, 1994</marker>
<rawString>M. E. Beckman and J. Hirschberg. 1994. The ToBI annotation conventions. Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bhuta</author>
<author>L Patrick</author>
<author>J D Garnett</author>
</authors>
<title>Perceptual evaluation of voice quality and its correlation with acoustic measurements.</title>
<date>2004</date>
<journal>Journal of Voice,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="13512" citStr="Bhuta et al., 2004" startWordPosition="2199" endWordPosition="2202">f a gradual compression of the pitch range (Pierrehumbert and Hirschberg, 1990). For conversational turns, then, we would expect to find that speakers tend to lower their pitch level as they reach potential turn boundaries. This hypothesis is verified by the dialogues in our corpus, where we find that IPUs preceding S have a significantly lower mean pitch than those preceding H (Figure 2). In consequence, pitch level may also work as a turn-yielding cue. Next we examine three acoustic features associated with the perception of voice quality: jitter, shimmer and noise-to-harmonics ratio (NHR) (Bhuta et al., 2004), computed over the IPU-final 500 and 1000 ms (Figure 2). We compute jitter and shimmer only over voiced frames for improved robustness. For all three features, the mean value for IPUs preceding S is significantly higher than for IPUs preceding H, with the difference increasing towards the end of the IPU. Therefore, voice quality seems to play a clear role as a turnyielding cue. 3.4 Lexical cues Stereotyped expressions such as you know or I think have been proposed in the literature as lexical turn-yielding cues. However, in the Games Corpus we find that none of the most frequent IPU-final uni</context>
</contexts>
<marker>Bhuta, Patrick, Garnett, 2004</marker>
<rawString>T. Bhuta, L. Patrick, and J. D. Garnett. 2004. Perceptual evaluation of voice quality and its correlation with acoustic measurements. Journal of Voice, 18(3):299–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boersma</author>
<author>D Weenink</author>
</authors>
<title>Praat: Doing phonetics by computer.</title>
<date>2001</date>
<note>http://www.praat.org.</note>
<contexts>
<context position="4997" citStr="Boersma and Weenink, 2001" startWordPosition="786" endWordPosition="789">on to achieve joint goals of identifying and moving images on the screen, while seated in a soundproof booth divided by a curtain to ensure that all communication was verbal. The subjects’ speech was not restricted in any way, and the games were not timed. The corpus contains 9 hours of dialogue, which were orthographically transcribed; words were time-aligned to the source by hand. Around 5.4 hours have also been intonationally transcribed using the ToBI framework (Beckman and Hirschberg, 1994). We automatically extracted a number of acoustic features from the corpus using the Praat toolkit (Boersma and Weenink, 2001), including pitch, intensity and voice quality features. Pitch slopes were computed by fitting least-squares linear regression models to the Fo track extracted from given portions of the signal. Part-of-speech (POS) tags were labeled automatically using Ratnaparkhi’s maxent tagger trained on a subset of the Switchboard corpus in lower-case with all punctuation removed, to simulate spoken language transcripts. All speaker normalizations were calculated using z-scores: z = (x − µVQ, where x is a raw measurement, and µ and Q are the mean and standard deviation for a speaker. For our turn-taking s</context>
</contexts>
<marker>Boersma, Weenink, 2001</marker>
<rawString>P. Boersma and D. Weenink. 2001. Praat: Doing phonetics by computer. http://www.praat.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="19449" citStr="Collins, 2003" startWordPosition="3227" endWordPosition="3228">simplified POS tags (N, V, Adj, Adv, Other) of w and of the IPU-final bigram; number of words in the IPU; a binary flag indicating if w is a word fragment; size and type of the biggest (bp) and smallest (sp) phrase that end in w; binary flags indicating if each of bp and sp is a major phrase (NP, VP, PP, ADJP, ADVP); binary flags indicating if w is the head of each of bp and sp. We chose these features in order to capture as much lexical and syntactic information as possible from the transcripts. The syntactic features were computed using two different parsers: the Collins statistical parser (Collins, 2003) and CASS, a partial parser especially designed for use with noisy text (Abney, 1996). We experimented with the learners listed in Table 2, using the implementations provided in the WEKA ML toolkit (Witten and Frank, 2000). Table 2 shows the accuracy of the majority-class baseline and of each classifier, using 10-fold cross validation on the 400 training data points, and the mean pairwise agreement by the three human labelers. The linear-kernel support-vector-machine (SVM) classifier achieves the highest accuracy, significantly outperforming the baseline, and approaching the mean agreement of </context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. J. Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Cutler</author>
<author>M Pearson</author>
</authors>
<title>On the analysis of prosodic turn-taking cues.</title>
<date>1986</date>
<booktitle>In C. Johns-Lewis, Ed., Intonation in Discourse,</booktitle>
<pages>139--156</pages>
<publisher>College-Hill.</publisher>
<contexts>
<context position="12296" citStr="Cutler and Pearson (1986)" startWordPosition="2001" endWordPosition="2004">r at all phrase final positions, not just at turn endings. In fact, our results indicate that the final lengthening is more prominent in turn-medial IPUs than in turn-final ones. 255 Figure 2: Individual turn-yielding cues: intensity, pitch and voice quality. 3.3 IPU duration and acoustic cues In the Columbia Games Corpus, we find that turnfinal IPUs tend to be significantly longer than turnmedial ones, both when measured in seconds and in number of words (Figure 1). This suggests that IPU duration could function as a turn-yielding cue, supporting similar findings in perceptual experiments by Cutler and Pearson (1986). We also find that IPUs followed by S have a mean intensity significantly lower than those followed by H (computed over the IPU-final 500 and 1000 ms, see Figure 2). Also, the differences increase when moving towards the end of the IPU. This suggests that speakers tend to lower their voices when approaching potential turn boundaries, whereas they reach turn-internal pauses with a higher intensity. Phonological theories conjecture a declination in the pitch level, which tends to decrease gradually within utterances, and across utterances within the same discourse segment, as a consequence of a</context>
</contexts>
<marker>Cutler, Pearson, 1986</marker>
<rawString>E. A. Cutler and M. Pearson. 1986. On the analysis of prosodic turn-taking cues. In C. Johns-Lewis, Ed., Intonation in Discourse, pp. 139–156. College-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Duncan</author>
</authors>
<title>Some signals and rules for taking speaking turns in conversations.</title>
<date>1972</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="2727" citStr="Duncan (1972" startWordPosition="430" endWordPosition="431">ent interaction. Additionally, a better understanding of the mechanics of turn-taking could be used to vary the speech output of IVR systems to (i) produce turn-yielding cues when the system is finished speaking and the user is expected to speak next, and (ii) avoid producing such cues when the system has more things to say. In this paper we examine the existence of turn-yielding cues in a large corpus of task-oriented dialogues in Standard American English (SAE). The question of what types of cues humans exploit for engaging in synchronized conversation has been addressed by several studies. Duncan (1972, inter alia) conjectures that speakers display complex signals at turn endings, composed of one or more discrete turn-yielding cues, such as the completion of a grammatical clause, or any phrase-final intonation other than a plateau. Duncan also hypothesizes that the likelihood of a turntaking attempt by the listener increases linearly with the number of such cues conjointly displayed by the speaker. Subsequent studies have investigated some of these hypotheses (Ford and Thompson, 1996; Wennerstrom and Siegel, 2003). More recent studies have investigated how to improve IVR system’s the turn-t</context>
<context position="10452" citStr="Duncan (1972)" startWordPosition="1698" endWordPosition="1699">slope, both raw and stylized, computed over the final 200 and 300 ms of each IPU. The case of a plateau corresponds to a value of F0 slope close to zero; the other case, of either a rising or a falling pitch, corresponds to a high absolute value of F0 slope. As shown in Figure 1, we find that the final slope before S is significantly higher than before H in all four cases. These findings provide additional support to the hypothesis that turns tend to end in falling and high-rising final intonations, and provide automatically identifiable indicators of this turn-yielding cue. 3.2 Speaking rate Duncan (1972) hypothesizes a “drawl on the final syllable or on the stressed syllable of a terminal clause” [p. 287] as a turn-yielding cue, which would probably correspond to a noticeable decrease in speaking rate. We examine this hypothesis in our corpus using two common definitions of speaking rate: syllables per second and phonemes per second. Syllable and phoneme counts were estimated from dictionary lookup, and word durations were extracted from the manual orthographic alignments. Figure 1 shows that both measures, computed over either the whole IPU or its final word, are significantly higher before </context>
<context position="16136" citStr="Duncan, 1972" startWordPosition="2655" endWordPosition="2656">and filled pauses (e.g., uh, um). Of the 8123 IPUs preceding H, 6.7% end in a word fragment, and 9.4% in a filled pause. By constrast, only 0.3% of the 3246 IPUs preceding S end in a word fragment, and 1% in a filled pause. These differences suggest that, after either a word fragment or a filled pause, the speaker is much more likely to intend to continue holding the floor. This notion of disfluencies functioning as a turn-taking cue has been studied by Goodwin (1981), who shows that they maybe used to secure the listener’s attention at turn beginnings. 3.5 Textual completion Several authors (Duncan, 1972; Ford and Thompson, 1996; Wennerstrom and Siegel, 2003) claim that some form of syntactic or semantic completion, independent of intonation and interactional import, functions as a turn-yielding cue. Although some call this syntactic completion, since all authors acknowledge the need for semantic and discourse information in judging it, we choose the more neutral term TEXTUAL COMPLETION for this phenomenon. We annotated a portion of our corpus with respect to textual completion and trained a machine learning (ML) classifier to automatically label the whole corpus. From these annotations we th</context>
<context position="24123" citStr="Duncan (1972)" startWordPosition="4007" endWordPosition="4008">ates the presence of a specific cue; a dot, its absence. 1: Intonation; 2: Speaking rate; 3: Intensity level; 4: Pitch level; 5: IPU duration; 6: Voice quality; 7: Textual completion. Table 5 shows the same results, now grouping together all IPUs with the same number of cues, independently of the cue types. Again, we observe that larger proportions of IPUs preceding S present more conjoined cues than IPUs preceding H. Next we look at how the likelihood of a turntaking attempt varies with respect to the number of individual cues displayed by the speaker, a relation hypothesized to be linear by Duncan (1972). Figure 3 shows the proportion of IPUs with 0-7 cues present that are followed by a turn-taking attempt from the interlocutor.2 The dashed line cor2 The proportion of turn-taking attempts is computed for each cue count as the number of S and PI divided by the number of S, PI, H and BC, according to our labeling scheme. 258 Cue count S H 0 4 0.1% 223 2.7% 1 52 1.6% 970 11.9% 2 241 7.4% 1552 19.1% 3 518 16.0% 1829 22.5% 4 740 22.8% 1666 20.5% 5 830 25.6% 1142 14.1% 6 594 18.3% 611 7.5% 7 267 8.2% 130 1.6% Total 3246 100% 8123 100% Table 5: Distribution of the number of turnyielding cues display</context>
</contexts>
<marker>Duncan, 1972</marker>
<rawString>S. Duncan. 1972. Some signals and rules for taking speaking turns in conversations. Journal of Personality and Social Psychology, 23(2):283–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ferrer</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>A prosody-based approach to end-of-utterance detection that does not require speech recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="1764" citStr="Ferrer et al., 2003" startWordPosition="269" endWordPosition="272">e unsatisfactory. Part of this reaction is due to deficiencies in speech recognition and synthesis technologies, but some can also be traced to coordination problems in the exchange of speaking turns between system and user (Ward et al., 2005; Raux et al., 2006). Users are not sure when the system is ready to end its turn, and systems are not sure when users are ready to relinquish theirs. Currently, the standard method for determining when a user is willing to yield the conversational floor is to wait for a silence longer than a prespecified threshold, typically ranging from 0.5 to 1 second (Ferrer et al., 2003). However, this strategy is rarely used by humans, who rely instead on cues from sources such as syntax, acoustics and prosody to anticipate turn transitions (Yngve, 1970). If such TURN-YIELDING CUES could be modeled and incorporated in IVR systems, it should be possible to make faster, more accurate turn-taking decisions, thus leading to a more fluent interaction. Additionally, a better understanding of the mechanics of turn-taking could be used to vary the speech output of IVR systems to (i) produce turn-yielding cues when the system is finished speaking and the user is expected to speak nex</context>
<context position="3438" citStr="Ferrer et al., 2003" startWordPosition="540" endWordPosition="543"> one or more discrete turn-yielding cues, such as the completion of a grammatical clause, or any phrase-final intonation other than a plateau. Duncan also hypothesizes that the likelihood of a turntaking attempt by the listener increases linearly with the number of such cues conjointly displayed by the speaker. Subsequent studies have investigated some of these hypotheses (Ford and Thompson, 1996; Wennerstrom and Siegel, 2003). More recent studies have investigated how to improve IVR system’s the turn-taking decisions by incorporating some of the features found to correlate with turn endings (Ferrer et al., 2003; Atterer et al., 2008; Raux and Eskenazi, 2008). All of these models are shown to improve over silence-based techniques for predicting turn endings, motivating further research. In this paper we present results Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 253–261, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 253 of a large, corpus-based study of turn-yielding cues in the Columbia Games Corpus which verifies some of Duncan’s hypotheses and adds additional cues to turn-tak</context>
</contexts>
<marker>Ferrer, Shriberg, Stolcke, 2003</marker>
<rawString>L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A prosody-based approach to end-of-utterance detection that does not require speech recognition. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Ford</author>
<author>S A Thompson</author>
</authors>
<title>Interactional units in conversation: Syntactic, intonational and pragmatic resources for the management of turns. In</title>
<date>1996</date>
<pages>134--184</pages>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3218" citStr="Ford and Thompson, 1996" startWordPosition="505" endWordPosition="509">n of what types of cues humans exploit for engaging in synchronized conversation has been addressed by several studies. Duncan (1972, inter alia) conjectures that speakers display complex signals at turn endings, composed of one or more discrete turn-yielding cues, such as the completion of a grammatical clause, or any phrase-final intonation other than a plateau. Duncan also hypothesizes that the likelihood of a turntaking attempt by the listener increases linearly with the number of such cues conjointly displayed by the speaker. Subsequent studies have investigated some of these hypotheses (Ford and Thompson, 1996; Wennerstrom and Siegel, 2003). More recent studies have investigated how to improve IVR system’s the turn-taking decisions by incorporating some of the features found to correlate with turn endings (Ferrer et al., 2003; Atterer et al., 2008; Raux and Eskenazi, 2008). All of these models are shown to improve over silence-based techniques for predicting turn endings, motivating further research. In this paper we present results Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 253–261, Queen Mary University of London, September </context>
<context position="16161" citStr="Ford and Thompson, 1996" startWordPosition="2657" endWordPosition="2661">ses (e.g., uh, um). Of the 8123 IPUs preceding H, 6.7% end in a word fragment, and 9.4% in a filled pause. By constrast, only 0.3% of the 3246 IPUs preceding S end in a word fragment, and 1% in a filled pause. These differences suggest that, after either a word fragment or a filled pause, the speaker is much more likely to intend to continue holding the floor. This notion of disfluencies functioning as a turn-taking cue has been studied by Goodwin (1981), who shows that they maybe used to secure the listener’s attention at turn beginnings. 3.5 Textual completion Several authors (Duncan, 1972; Ford and Thompson, 1996; Wennerstrom and Siegel, 2003) claim that some form of syntactic or semantic completion, independent of intonation and interactional import, functions as a turn-yielding cue. Although some call this syntactic completion, since all authors acknowledge the need for semantic and discourse information in judging it, we choose the more neutral term TEXTUAL COMPLETION for this phenomenon. We annotated a portion of our corpus with respect to textual completion and trained a machine learning (ML) classifier to automatically label the whole corpus. From these annotations we then examined how textual c</context>
</contexts>
<marker>Ford, Thompson, 1996</marker>
<rawString>C. E. Ford and S. A. Thompson. 1996. Interactional units in conversation: Syntactic, intonational and pragmatic resources for the management of turns. In E. Ochs, E. A. Schegloff, and S. A. Thompson, Eds., Interaction and Grammar, pp. 134–184. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Goodwin</author>
</authors>
<title>Conversational Organization: Interaction between Speakers and Hearers.</title>
<date>1981</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="15996" citStr="Goodwin (1981)" startWordPosition="2634" endWordPosition="2635">examples of lexical turn-yielding cues in our task-oriented corpus, we do find two lexical turn-holding cues: word fragments (e.g., incompl-) and filled pauses (e.g., uh, um). Of the 8123 IPUs preceding H, 6.7% end in a word fragment, and 9.4% in a filled pause. By constrast, only 0.3% of the 3246 IPUs preceding S end in a word fragment, and 1% in a filled pause. These differences suggest that, after either a word fragment or a filled pause, the speaker is much more likely to intend to continue holding the floor. This notion of disfluencies functioning as a turn-taking cue has been studied by Goodwin (1981), who shows that they maybe used to secure the listener’s attention at turn beginnings. 3.5 Textual completion Several authors (Duncan, 1972; Ford and Thompson, 1996; Wennerstrom and Siegel, 2003) claim that some form of syntactic or semantic completion, independent of intonation and interactional import, functions as a turn-yielding cue. Although some call this syntactic completion, since all authors acknowledge the need for semantic and discourse information in judging it, we choose the more neutral term TEXTUAL COMPLETION for this phenomenon. We annotated a portion of our corpus with respec</context>
</contexts>
<marker>Goodwin, 1981</marker>
<rawString>C. Goodwin. 1981. Conversational Organization: Interaction between Speakers and Hearers. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gravano</author>
<author>S Benus</author>
<author>J Hirschberg</author>
<author>S Mitchell</author>
<author>I Vovsha</author>
</authors>
<title>Classification of discourse functions of affirmative words in spoken dialogue.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="15150" citStr="Gravano et al., 2007" startWordPosition="2487" endWordPosition="2490">lion on top # with a lemon in the bottom left # and a yellow crescent moon in- # i- # in the bottom right B: oh okay [...] In consequence, bigrams such as lower right and bottom right are common before S, while on top or bottom left are common before H. These are all task-specific lexical constructions and do not constitute stereotyped expressions in the traditional sense. Also very common among the most frequent IPU-final expressions are AFFIRMATIVE CUE WORDS — heavily overloaded words, such as okay or yeah, that are used both to initiate and to end discourse segments, among other functions (Gravano et al., 2007). The occurrence of these words does not constitute a turn-yielding or turnholding cue per se; rather, additional contextual, acoustic and prosodic information is needed to disambiguate their meaning. 256 While we do not find clear examples of lexical turn-yielding cues in our task-oriented corpus, we do find two lexical turn-holding cues: word fragments (e.g., incompl-) and filled pauses (e.g., uh, um). Of the 8123 IPUs preceding H, 6.7% end in a word fragment, and 9.4% in a filled pause. By constrast, only 0.3% of the 3246 IPUs preceding S end in a word fragment, and 1% in a filled pause. Th</context>
</contexts>
<marker>Gravano, Benus, Hirschberg, Mitchell, Vovsha, 2007</marker>
<rawString>A. Gravano, S. Benus, J. Hirschberg, S. Mitchell, and I. Vovsha. 2007. Classification of discourse functions of affirmative words in spoken dialogue. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gravano</author>
</authors>
<title>Turn-Taking and Affirmative Cue Words in Task-Oriented Dialogue.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University,</institution>
<location>New York.</location>
<contexts>
<context position="4159" citStr="Gravano, 2009" startWordPosition="652" endWordPosition="653">ed techniques for predicting turn endings, motivating further research. In this paper we present results Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 253–261, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 253 of a large, corpus-based study of turn-yielding cues in the Columbia Games Corpus which verifies some of Duncan’s hypotheses and adds additional cues to turn-taking behavior. 2 Materials and Method The materials for our study are taken from the Columbia Games Corpus (Gravano, 2009), a collection of 12 spontaneous task-oriented dyadic conversations elicited from 13 native speakers of SAE. In each session, two subjects were paid to play a series of computer games requiring verbal communication to achieve joint goals of identifying and moving images on the screen, while seated in a soundproof booth divided by a curtain to ensure that all communication was verbal. The subjects’ speech was not restricted in any way, and the games were not timed. The corpus contains 9 hours of dialogue, which were orthographically transcribed; words were time-aligned to the source by hand. Ar</context>
</contexts>
<marker>Gravano, 2009</marker>
<rawString>A. Gravano. 2009. Turn-Taking and Affirmative Cue Words in Task-Oriented Dialogue. Ph.D. thesis, Columbia University, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pierrehumbert</author>
<author>J Hirschberg</author>
</authors>
<title>The meaning of intonational contours in the interpretation of discourse. In</title>
<date>1990</date>
<booktitle>Eds., Intentions in Communication,</booktitle>
<pages>271--311</pages>
<publisher>MIT Pr.</publisher>
<contexts>
<context position="12972" citStr="Pierrehumbert and Hirschberg, 1990" startWordPosition="2110" endWordPosition="2113">ve a mean intensity significantly lower than those followed by H (computed over the IPU-final 500 and 1000 ms, see Figure 2). Also, the differences increase when moving towards the end of the IPU. This suggests that speakers tend to lower their voices when approaching potential turn boundaries, whereas they reach turn-internal pauses with a higher intensity. Phonological theories conjecture a declination in the pitch level, which tends to decrease gradually within utterances, and across utterances within the same discourse segment, as a consequence of a gradual compression of the pitch range (Pierrehumbert and Hirschberg, 1990). For conversational turns, then, we would expect to find that speakers tend to lower their pitch level as they reach potential turn boundaries. This hypothesis is verified by the dialogues in our corpus, where we find that IPUs preceding S have a significantly lower mean pitch than those preceding H (Figure 2). In consequence, pitch level may also work as a turn-yielding cue. Next we examine three acoustic features associated with the perception of voice quality: jitter, shimmer and noise-to-harmonics ratio (NHR) (Bhuta et al., 2004), computed over the IPU-final 500 and 1000 ms (Figure 2). We</context>
</contexts>
<marker>Pierrehumbert, Hirschberg, 1990</marker>
<rawString>J. Pierrehumbert and J. Hirschberg. 1990. The meaning of intonational contours in the interpretation of discourse. In P. Cohen, J. Morgan, and M. Pollack, Eds., Intentions in Communication, pp. 271– 311. MIT Pr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>M Eskenazi</author>
</authors>
<title>Optimizing endpointing thresholds using dialogue features in a spoken dialogue system.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGdial.</booktitle>
<contexts>
<context position="3486" citStr="Raux and Eskenazi, 2008" startWordPosition="548" endWordPosition="551">uch as the completion of a grammatical clause, or any phrase-final intonation other than a plateau. Duncan also hypothesizes that the likelihood of a turntaking attempt by the listener increases linearly with the number of such cues conjointly displayed by the speaker. Subsequent studies have investigated some of these hypotheses (Ford and Thompson, 1996; Wennerstrom and Siegel, 2003). More recent studies have investigated how to improve IVR system’s the turn-taking decisions by incorporating some of the features found to correlate with turn endings (Ferrer et al., 2003; Atterer et al., 2008; Raux and Eskenazi, 2008). All of these models are shown to improve over silence-based techniques for predicting turn endings, motivating further research. In this paper we present results Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 253–261, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 253 of a large, corpus-based study of turn-yielding cues in the Columbia Games Corpus which verifies some of Duncan’s hypotheses and adds additional cues to turn-taking behavior. 2 Materials and Method The materia</context>
</contexts>
<marker>Raux, Eskenazi, 2008</marker>
<rawString>A. Raux and M. Eskenazi. 2008. Optimizing endpointing thresholds using dialogue features in a spoken dialogue system. In Proceedings of SIGdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>D Bohus</author>
<author>B Langner</author>
<author>A W Black</author>
<author>M Eskenazi</author>
</authors>
<title>Doing research on a deployed spoken dialogue system: One year of Let’s Go! experience.</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="1406" citStr="Raux et al., 2006" startWordPosition="204" endWordPosition="207">eractive voice-response systems, so that systems can correctly estimate when the user is willing to yield the conversational floor, and so that they can produce their own turn-yielding cues appropriately. 1 Introduction and Previous Research Users of state-of-the-art interactive voice response (IVR) systems often find interactions with these systems to be unsatisfactory. Part of this reaction is due to deficiencies in speech recognition and synthesis technologies, but some can also be traced to coordination problems in the exchange of speaking turns between system and user (Ward et al., 2005; Raux et al., 2006). Users are not sure when the system is ready to end its turn, and systems are not sure when users are ready to relinquish theirs. Currently, the standard method for determining when a user is willing to yield the conversational floor is to wait for a silence longer than a prespecified threshold, typically ranging from 0.5 to 1 second (Ferrer et al., 2003). However, this strategy is rarely used by humans, who rely instead on cues from sources such as syntax, acoustics and prosody to anticipate turn transitions (Yngve, 1970). If such TURN-YIELDING CUES could be modeled and incorporated in IVR s</context>
</contexts>
<marker>Raux, Bohus, Langner, Black, Eskenazi, 2006</marker>
<rawString>A. Raux, D. Bohus, B. Langner, A. W. Black, and M. Eskenazi. 2006. Doing research on a deployed spoken dialogue system: One year of Let’s Go! experience. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N G Ward</author>
<author>A G Rivera</author>
<author>K Ward</author>
<author>D G Novick</author>
</authors>
<title>Root causes of lost time and user stress in a simple dialog system.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="1386" citStr="Ward et al., 2005" startWordPosition="200" endWordPosition="203">eaking turns in interactive voice-response systems, so that systems can correctly estimate when the user is willing to yield the conversational floor, and so that they can produce their own turn-yielding cues appropriately. 1 Introduction and Previous Research Users of state-of-the-art interactive voice response (IVR) systems often find interactions with these systems to be unsatisfactory. Part of this reaction is due to deficiencies in speech recognition and synthesis technologies, but some can also be traced to coordination problems in the exchange of speaking turns between system and user (Ward et al., 2005; Raux et al., 2006). Users are not sure when the system is ready to end its turn, and systems are not sure when users are ready to relinquish theirs. Currently, the standard method for determining when a user is willing to yield the conversational floor is to wait for a silence longer than a prespecified threshold, typically ranging from 0.5 to 1 second (Ferrer et al., 2003). However, this strategy is rarely used by humans, who rely instead on cues from sources such as syntax, acoustics and prosody to anticipate turn transitions (Yngve, 1970). If such TURN-YIELDING CUES could be modeled and i</context>
</contexts>
<marker>Ward, Rivera, Ward, Novick, 2005</marker>
<rawString>N. G. Ward, A. G. Rivera, K. Ward, and D. G. Novick. 2005. Root causes of lost time and user stress in a simple dialog system. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Wennerstrom</author>
<author>A F Siegel</author>
</authors>
<title>Keeping the floor in multiparty conversations: Intonation, syntax, and pause.</title>
<date>2003</date>
<booktitle>Discourse Processes,</booktitle>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="3249" citStr="Wennerstrom and Siegel, 2003" startWordPosition="510" endWordPosition="513">umans exploit for engaging in synchronized conversation has been addressed by several studies. Duncan (1972, inter alia) conjectures that speakers display complex signals at turn endings, composed of one or more discrete turn-yielding cues, such as the completion of a grammatical clause, or any phrase-final intonation other than a plateau. Duncan also hypothesizes that the likelihood of a turntaking attempt by the listener increases linearly with the number of such cues conjointly displayed by the speaker. Subsequent studies have investigated some of these hypotheses (Ford and Thompson, 1996; Wennerstrom and Siegel, 2003). More recent studies have investigated how to improve IVR system’s the turn-taking decisions by incorporating some of the features found to correlate with turn endings (Ferrer et al., 2003; Atterer et al., 2008; Raux and Eskenazi, 2008). All of these models are shown to improve over silence-based techniques for predicting turn endings, motivating further research. In this paper we present results Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 253–261, Queen Mary University of London, September 2009. c�2009 Association for Co</context>
<context position="16192" citStr="Wennerstrom and Siegel, 2003" startWordPosition="2662" endWordPosition="2665">e 8123 IPUs preceding H, 6.7% end in a word fragment, and 9.4% in a filled pause. By constrast, only 0.3% of the 3246 IPUs preceding S end in a word fragment, and 1% in a filled pause. These differences suggest that, after either a word fragment or a filled pause, the speaker is much more likely to intend to continue holding the floor. This notion of disfluencies functioning as a turn-taking cue has been studied by Goodwin (1981), who shows that they maybe used to secure the listener’s attention at turn beginnings. 3.5 Textual completion Several authors (Duncan, 1972; Ford and Thompson, 1996; Wennerstrom and Siegel, 2003) claim that some form of syntactic or semantic completion, independent of intonation and interactional import, functions as a turn-yielding cue. Although some call this syntactic completion, since all authors acknowledge the need for semantic and discourse information in judging it, we choose the more neutral term TEXTUAL COMPLETION for this phenomenon. We annotated a portion of our corpus with respect to textual completion and trained a machine learning (ML) classifier to automatically label the whole corpus. From these annotations we then examined how textual completion labels relate to turn</context>
</contexts>
<marker>Wennerstrom, Siegel, 2003</marker>
<rawString>A. Wennerstrom and A. F. Siegel. 2003. Keeping the floor in multiparty conversations: Intonation, syntax, and pause. Discourse Processes, 36(2):77–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Wightman</author>
<author>S Shattuck-Hufnagel</author>
<author>M Ostendorf</author>
<author>P J Price</author>
</authors>
<title>Segmental durations in the vicinity of prosodic phrase boundaries.</title>
<date>1992</date>
<journal>The Journal of the Acoustical Society of America,</journal>
<pages>91--1707</pages>
<contexts>
<context position="11461" citStr="Wightman et al., 1992" startWordPosition="1861" endWordPosition="1864">ctionary lookup, and word durations were extracted from the manual orthographic alignments. Figure 1 shows that both measures, computed over either the whole IPU or its final word, are significantly higher before S than before H, which indicates an increase in speaking rate before turn boundaries rather than Duncan’s hypothesized drawl. Furthermore, the speaking rate is, in both cases (before S and before H), significantly slower on the final word than over the whole IPU, a finding that is in line with phonological theories that predict a segmental lengthening near prosodic phrase boundaries (Wightman et al., 1992). This finding may indeed correspond to the drawl or lengthening described by Duncan before turn boundaries. However, it seems to be the case — at least for our corpus — that the final lengthening tends to occur at all phrase final positions, not just at turn endings. In fact, our results indicate that the final lengthening is more prominent in turn-medial IPUs than in turn-final ones. 255 Figure 2: Individual turn-yielding cues: intensity, pitch and voice quality. 3.3 IPU duration and acoustic cues In the Columbia Games Corpus, we find that turnfinal IPUs tend to be significantly longer than </context>
</contexts>
<marker>Wightman, Shattuck-Hufnagel, Ostendorf, Price, 1992</marker>
<rawString>C. W. Wightman, S. Shattuck-Hufnagel, M. Ostendorf, and P. J. Price. 1992. Segmental durations in the vicinity of prosodic phrase boundaries. The Journal of the Acoustical Society of America, 91:1707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<date>2000</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="19671" citStr="Witten and Frank, 2000" startWordPosition="3263" endWordPosition="3266">e that end in w; binary flags indicating if each of bp and sp is a major phrase (NP, VP, PP, ADJP, ADVP); binary flags indicating if w is the head of each of bp and sp. We chose these features in order to capture as much lexical and syntactic information as possible from the transcripts. The syntactic features were computed using two different parsers: the Collins statistical parser (Collins, 2003) and CASS, a partial parser especially designed for use with noisy text (Abney, 1996). We experimented with the learners listed in Table 2, using the implementations provided in the WEKA ML toolkit (Witten and Frank, 2000). Table 2 shows the accuracy of the majority-class baseline and of each classifier, using 10-fold cross validation on the 400 training data points, and the mean pairwise agreement by the three human labelers. The linear-kernel support-vector-machine (SVM) classifier achieves the highest accuracy, significantly outperforming the baseline, and approaching the mean agreement of human labelers. 257 Classifier Accuracy Majority-class (‘complete’) 55.2% C4.5 (decision trees) 55.2% Ripper (propositional rules) 68.2% Bayesian networks 75.7% SVM, RBF kernel (c = 1, a = 10−12) 78.2% SVM, linear kernel (</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>I. H. Witten and E. Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V H Yngve</author>
</authors>
<title>On getting a word in edgewise.</title>
<date>1970</date>
<booktitle>Sixth Regional Meeting of the Chicago Linguistic Society,</booktitle>
<pages>6--657</pages>
<contexts>
<context position="1935" citStr="Yngve, 1970" startWordPosition="299" endWordPosition="300">ge of speaking turns between system and user (Ward et al., 2005; Raux et al., 2006). Users are not sure when the system is ready to end its turn, and systems are not sure when users are ready to relinquish theirs. Currently, the standard method for determining when a user is willing to yield the conversational floor is to wait for a silence longer than a prespecified threshold, typically ranging from 0.5 to 1 second (Ferrer et al., 2003). However, this strategy is rarely used by humans, who rely instead on cues from sources such as syntax, acoustics and prosody to anticipate turn transitions (Yngve, 1970). If such TURN-YIELDING CUES could be modeled and incorporated in IVR systems, it should be possible to make faster, more accurate turn-taking decisions, thus leading to a more fluent interaction. Additionally, a better understanding of the mechanics of turn-taking could be used to vary the speech output of IVR systems to (i) produce turn-yielding cues when the system is finished speaking and the user is expected to speak next, and (ii) avoid producing such cues when the system has more things to say. In this paper we examine the existence of turn-yielding cues in a large corpus of task-orient</context>
</contexts>
<marker>Yngve, 1970</marker>
<rawString>V. H. Yngve. 1970. On getting a word in edgewise. Sixth Regional Meeting of the Chicago Linguistic Society, 6:657–677.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>