<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000184">
<title confidence="0.8532665">
Annotating Dialogue Acts to Construct Dialogue Systems for Consulting
Kiyonori Ohtake Teruhisa Misu Chiori Hori Hideki Kashioka Satoshi Nakamura
</title>
<note confidence="0.671500666666667">
MASTAR Project, National Institute of Information and Communications Technology
Hikaridai, Keihanna Science City, JAPAN
kiyonori.ohtake (at) nict.go.jp
</note>
<sectionHeader confidence="0.976122" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948666666667">
This paper introduces a new corpus of con-
sulting dialogues, which is designed for
training a dialogue manager that can han-
dle consulting dialogues through sponta-
neous interactions from the tagged dia-
logue corpus. We have collected 130 h
of consulting dialogues in the tourist guid-
ance domain. This paper outlines our tax-
onomy of dialogue act annotation that can
describe two aspects of an utterances: the
communicative function (speech act), and
the semantic content of the utterance. We
provide an overview of the Kyoto tour
guide dialogue corpus and a preliminary
analysis using the dialogue act tags.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989671875">
This paper introduces a new dialogue corpus for
consulting in the tourist guidance domain. The
corpus consists of speech, transcripts, speech act
tags, morphological analysis results, dependency
analysis results, and semantic content tags. In this
paper, we describe the current status of a dialogue
corpus that is being developed by our research
group, focusing on two types of tags: speech act
tags and semantic content tags. These speech act
and semantic content tags were designed to ex-
press the dialogue act of each utterance.
Many studies have focused on developing spo-
ken dialogue systems. Their typical task do-
mains included the retrieval of information from
databases or making reservations, such as airline
information e.g., DARPA Communicator (Walker
et al., 2001) and train information e.g., ARISE
(Bouwman et al., 1999) and MASK (Lamel et al.,
2002). Most studies assumed a definite and con-
sistent user objective, and the dialogue strategy
was usually designed to minimize the cost of in-
formation access. Other target tasks include tutor-
ing and trouble-shooting dialogues (Boye, 2007).
In such tasks, dialogue scenarios or agendas are
usually described using a (dynamic) tree structure,
and the objective is to satisfy all requirements.
In this paper, we introduce our corpus, which is
being developed as part of a project to construct
consulting dialogue systems, that helps the user in
making a decision. So far, several projects have
been organized to construct speech corpora such
as CSJ (Maekawa et al., 2000) for Japanese. The
size of CSJ is very large, and a great part of the
corpus consists of monologues. Although, CSJ
includes some dialogues, the size of dialogues is
not enough to construct a dialogue system via re-
cent statistical techniques. In addition, relatively
to consulting dialogues, the existing large dialogue
corpora covered very clear tasks in limited do-
mains.
However, consulting is a frequently used and
very natural form of human interaction. We of-
ten consult with a sales clerk while shopping or
with staff at a concierge desk in a hotel. Such dia-
logues usually form part of a series of information
retrieval dialogues that have been investigated in
many previous studies. They also contains various
exchanges, such as clarifications and explanations.
The user may explain his/her preferences vaguely
by listing examples. The server would then sense
the user’s preferences from his/her utterances, pro-
vide some information, and then request a deci-
sion.
It is almost impossible to handcraft a scenario
that can handle such spontaneous consulting dia-
logues; thus, the dialogue strategy should be boot-
strapped from a dialogue corpus. If an extensive
dialogue corpus is available, we can model the
dialogue using machine learning techniques such
as partially observable Markov decision processes
(POMDPs) (Thomson et al., 2008). Hori et al.
(2008) have also proposed an efficient approach to
organize a dialogue system using weighted finite-
state transducers (WFSTs); the system obtains the
</bodyText>
<page confidence="0.994">
32
</page>
<note confidence="0.975637">
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32–39,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<tableCaption confidence="0.977127">
Table 2: Overview of Kyoto tour guide dialogue
corpus
</tableCaption>
<table confidence="0.983073714285714">
dialogue type F2F WOZ TEL
# of dialogues 114 80 62
# of guides 3 2 2
avg. # of utterance 365.4 165.2 324.5
/ dialogue (guide) 301.7 112.9 373.5
avg. # of utterance
/ dialogue (tourist)
</table>
<bodyText confidence="0.99995935">
structure of the transducers and the weight for
each state transitions from an annotated corpus.
Thus, the corpus must be sufficiently rich in in-
formation to describe the consulting dialogue to
construct the statistical dialogue manager via such
techniques.
In addition, a detailed description would be
preferable when developing modules that focus
on spoken language understanding and generation
modules. In this study, we adopt dialogue acts
(DAs) (Bunt, 2000; Shriberg et al., 2004; Banga-
lore et al., 2006; Rodriguez et al., 2007; Levin et
al., 2002) for this information and annotate DAs in
the corpus.
In this paper, we describe the design of the Ky-
oto tour guide dialogue corpus in Section 2. Our
design of the DA annotation is described in Sec-
tion 3. Sections 4 and 5 respectively describe two
types of the tag sets, namely, the speech act tag
and the semantic content tag.
</bodyText>
<sectionHeader confidence="0.889101" genericHeader="method">
2 Kyoto Tour Guide Dialogue Corpus
</sectionHeader>
<bodyText confidence="0.999985029411765">
We are currently developing a dialogue corpus
based on tourist guidance for Kyoto City as the tar-
get domain. Thus far, we have collected itinerary
planning dialogues in Japanese, in which users
plan a one-day visit to Kyoto City. There are
three types of dialogues in the corpus: face-to-
face (F2F), Wizard of OZ (WOZ), and telephonic
(TEL) dialogues. The corpus consists of 114 face-
to-face dialogues, 80 dialogues using the WOZ
system, and 62 dialogues obtained from telephone
conversations with the interface of the WOZ sys-
tem.
The overview of these three types of dialogues
is shown in Table 2. Each dialogue lasts for almost
30 min. Most of all the dialogues have been man-
ually transcribed. Table 2 also shows the average
number of utterances per a dialogue.
Each face-to-face dialogue involved a profes-
sional tour guide and a tourist. Three guides, one
male and two females, were employed to collect
the dialogues. All three guides were involved in
almost the same number of dialogues. The guides
used maps, guidebooks, and a PC connected to the
internet.
In the WOZ dialogues, two female guides were
employed. Each of them was participated in 40
dialogues. The WOZ system consists of two in-
ternet browsers, speech synthesis program, and
an integration program for the collaborative work.
Collaboration was required because in addition to
the guide, operators were employed to operate the
WOZ system and support the guide. Each of the
guide and operators used own computer connected
each other, and they collaboratively operate the
WOZ system to serve a user (tourist).
In the telephone dialogues, two female guides
who are the same for the WOZ dialogues were
employed. In these dialogues, we used the WOZ
system, but we did not need the speech synthesis
program. The guide and a tourist shared the same
interface in different rooms, and they could talk to
each other through the hands-free headset.
Dialogues to plan a one-day visit consist of sev-
eral conversations for choosing places to visit. The
conversations usually included sequences of re-
quests from the users and provision of information
by the guides as well as consultation in the form of
explanation and evaluation. It should be noted that
in this study, enabling the user to access informa-
tion is not an objective in itself, unlike information
kiosk systems such as those developed in (Lamel
et al., 2002) or (Thomson et al., 2008). The objec-
tive is similar to the problem-solving dialogue of
the study by Ferguson and Allen (1998), in other
words, accessing information is just an aspect of
consulting dialogues.
An example of dialogue via face-to-face com-
munication is shown in Table 1. This dialogue is
a part of a consultation to decide on a sightseeing
spot to visit. The user asks about the location of a
spot, and the guide answers it. Then, the user pro-
vides a follow-up by evaluating the answer. The
task is challenging because there are many utter-
ances that affect the flow of the dialogue during a
consultation. The utterances are listed in the order
of their start times with the utterance ids (UID).
From the column “Time” in the table, it is easy to
see that there are many overlaps.
</bodyText>
<page confidence="0.999468">
33
</page>
<tableCaption confidence="0.999832">
Table 1: Example dialogue from the Kyoto tour guide dialogue corpus
</tableCaption>
<table confidence="0.844817875">
UID Time (ms) Speaker Transcript Speech act tag** Semantic content tag
Ato (And,) null
56 76669–78819 User Ohara ga (Ohara is) (activity),location
WH–Question Where
dono henni (where) (activity),(demonstrative),interr
narimasuka (I’d like to know) (activity),predicate
State Acknowledgment→64 (consulting),(activity),predicate
65 89889–90759 User Iidesune (that would be nice.) Evaluation→64
</table>
<tableCaption confidence="0.195469">
* Tags are concatenated using a delimiter ’ ’ and omitting null values.
The number following the ’→’ symbol denotes the target utterance of the function.
</tableCaption>
<figure confidence="0.991456">
kono (here) (demonstrative),kosoa
57 80788–81358 Guide State Answer→56
hendesune (is around) (demonstrative),noun
58 81358–81841 Guide Ohara ha (Ohara) State Inversion location
Chotto (a bit) (transp),(cost),(distance),adverb-phrase
59 81386–82736 User State Evaluation→57
hanaresugitemasune (is too far) (transp),(cost),(distance),predicate
60 83116–83316 Guide A (Yeah,) Pause Grabber null
Kore demo (it)
null
61 83136–85023 User
ichinichi dewa (in a day) Y/N–Question (activity),(planning),duration
doudeshou (Do you think I can do) (activity),(planning),(demonstrative),interr
62 83386–84396 Guide Soudesune (right.) State Acknowledgment→59 null
Ichinichi (One day)
(activity),(planning),(entity),day-window
63 85206–87076 Guide
areba (is) (activity),(planning),predicate
State AffirmativeAnswer→61
jubuN (enough) (consulting),(activity),adverb-phrase
ikemasu (to enjoy it.) (consulting),(activity),action
Oharamo (Ohara is)
(activity),location
64 88392–90072 Guide
sugoku (very) State Opinion (recommendation),(activity),adverb-phrase
kireidesuyo (a beautiful spot) (recommendation),(activity),predicate
</figure>
<sectionHeader confidence="0.8483595" genericHeader="method">
3 Annotation of Communicative
Function and Semantic Content in DA
</sectionHeader>
<bodyText confidence="0.9999636">
We annotate DAs in the corpus in order to de-
scribe a user’s intention and a system’s (or the tour
guide’s) action. Recently, several studies have ad-
dressed multilevel annotation of dialogues (Levin
et al., 2002; Bangalore et al., 2006; Rodriguez et
al., 2007); in our study, we focus on the two as-
pects of a DA indicated by Bunt (2000). One is the
communicative function that corresponds to how
the content should be used in order to update the
context, and the other is a semantic content that
corresponds to what the act is about. We consider
both of them important information to handle the
consulting dialogue. We designed two different
tag sets to annotate DAs in the corpus. The speech
act tag is used to capture the communicative func-
tions of an utterance using domain-independent
multiple function layers. The semantic content tag
is used to describe the semantic contents of an ut-
terance using domain-specific hierarchical seman-
tic classes.
</bodyText>
<sectionHeader confidence="0.978609" genericHeader="method">
4 Speech Act Tags
</sectionHeader>
<bodyText confidence="0.99989325">
In this section, we introduce the speech act (SA)
tag set that describes communicative functions of
utterances. As the base units for tag annotation,
we adopt clauses that are detected by applying
the clause boundary annotation program (Kash-
ioka and Maruyama, 2004) to the transcript of the
dialogue. Thus, in the following discussions, “ut-
terance” denotes a clause.
</bodyText>
<subsectionHeader confidence="0.998477">
4.1 Tag Specifications
</subsectionHeader>
<bodyText confidence="0.999859137931035">
There are two major policies in SA annotation.
One is to select exactly one label from the tag set
(e.g., the AMI corpus&apos;). The other is to annotate
with as many labels as required. MRDA (Shriberg
et al., 2004) and DIT++ (Bunt, 2000) are defined
on the basis of the second policy. We believe that
utterances are generally multifunctional and this
multifunctionality is an important aspect for man-
aging consulting dialogues through spontaneous
interactions. Therefore, we have adopted the latter
policy.
By extending the MRDA tag set and DIT++, we
defined our speech act tag set that consists of six
layers to describe six groups of function: Gen-
eral, Response, Check, Constrain, ActionDiscus-
sion, and Others. A list of the tag sets (excluding
the Others layer is shown in Table 3. The General
layer has two sublayers under the labels, Pause
and WH-Question, respectively. The two sublay-
ers are used to elaborate on the two labels, respec-
tively. A tag of the General layer must be labeled
to an utterance, but the other layer’s tags are op-
tional, in other words, layers other than the Gen-
eral layer can take null values when there is no tag
which is appropriate to the utterance. In the practi-
cal annotation, the most appropriate tag is selected
from each layer, without taking into account any
of the other layers.
The descriptions of the layers are as follows:
</bodyText>
<footnote confidence="0.6430735">
General: It is used to represent the basic form
lhttp://corpus.amiproject.org
</footnote>
<page confidence="0.996688">
34
</page>
<tableCaption confidence="0.999229">
Table 3: List of speech act tags and their occurrence in the experiment
</tableCaption>
<table confidence="0.999504166666667">
Tag Percentage(%) Tag Percentage(%) Tag Percentage(%) Tag Percentage(%)
User Guide User Guide User Guide User Guide
(General) (Response) (ActionDiscussion) (Constrain)
Statement 45.25 44.53 Acknowledgment 19.13 5.45 Opinion 0.52 2.12 Reason 0.64 2.52
Pause 12.99 15.05 Accept 4.68 6.25 Wish 1.23 0.05 Condition 0.61 3.09
Backchannel 26.05 9.09 PartialAccept 0.02 0.10 Request 0.22 0.19 Elaboration 0.28 4.00
Y/N-Question 3.61 2.19 AffirmativeAnswer 0.08 0.20 Suggestion 0.16 1.12 Evaluation 1.35 2.01
WH-Question 1.13 0.40 Reject 0.25 0.11 Commitment 1.15 0.29 (Check)
Open-Question 0.32 0.32 PartialReject 0.04 0.03 RepetitionRequest 0.07 0.03
OR–after-Y/N 0.05 0.02 NegativeAnswer 0.10 0.10 UnderstandingCheck 0.19 0.20
OR-Question 0.05 0.03 Answer 1.16 2.57 DoubleCheck 0.36 0.15
Statement== 9.91 27.79 ApprovalRequest 2.01 1.07
</table>
<bodyText confidence="0.999521775">
of the unit. Most of the tags in this layer
are used to describe forward-looking func-
tions. The tags are classified into three large
groups: “Question,” “Fragment,” and “State-
ment.” “Statement==” denotes the continua-
tion of the utterance.
Response: It is used to label responses directed
to a specific previous utterance made by the
addressee.
Check: It is used to label confirmations that are
along a certain expected response.
Constrain: It is used to label utterances that re-
strict or complement the target of the utter-
ance.
ActionDiscussion: It is used to label utterances
that pertain to a future action.
Others: It is used to describe various functions of
the utterance, e.g., Greeting, SelfTalk, Wel-
come, Apology, etc.
In the General layer, there are two sublayers:— (1)
the Pause sublayer that consists of Hold, Grabber,
Holder, and Releaser and (2) the WH sublayer that
labels the WH-Question type.
It should be noted that this taxonomy is in-
tended to be used for training spoken dialogue sys-
tems. Consequently, it contains detailed descrip-
tions to elaborate on the decision-making process.
For example, checks are classified into four cat-
egories because they should be treated in various
ways in a dialogue system. UnderstandingCheck
is often used to describe clarifications; thus, it
should be taken into account when creating a di-
alogue scenario. In contrast, RepetitionRequest,
which is used to request that the missed portions
of the previous utterance be repeated, is not con-
cerned with the overall dialogue flow.
An example of an annotation is shown in Table
1. Since the Response and Constrain layers are not
necessarily directed to the immediately preceding
utterance, the target utterance ID is specified.
</bodyText>
<subsectionHeader confidence="0.920499">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999911571428571">
We performed a preliminary annotation of the
speech act tags in the corpus. Thirty dialogues
(900 min, 23,169 utterances) were annotated by
three labellers. When annotating the dialogues, we
took into account textual information, audio infor-
mation, and contextual information The result was
cross-checked by another labeller.
</bodyText>
<subsectionHeader confidence="0.70472">
4.2.1 Distributional Statistics
</subsectionHeader>
<bodyText confidence="0.999947066666666">
The frequencies of the tags, expressed as a per-
centages, are shown in Table 3. In the General
layer, nearly half of the utterances were Statement.
This bias is acceptable because 66% of the utter-
ances had tag(s) of other layers.
The percentages of tags in the Constrain layer
are relatively higher than those of tags in the other
layers. They are also higher than the percentages
of the corresponding tags of MRDA (Shriberg
et al., 2004) and SWBD-DAMSL(Jurafsky et al.,
1997).
These statistics characterize the consulting dia-
logue of sightseeing planning, where explanations
and evaluations play an important role during the
decision process.
</bodyText>
<subsectionHeader confidence="0.479603">
4.2.2 Reliability
</subsectionHeader>
<bodyText confidence="0.999984916666667">
We investigated the reliability of the annotation.
Another two dialogues (2,087 utterances) were an-
notated by three labelers and the agreement among
them was examined. These results are listed in Ta-
ble 4. The agreement ratio is the average of all the
combinations of the three individual agreements.
In the same way, we also computed the average
Kappa statistic, which is often used to measure the
agreement by considering the chance rate.
A high concordance rate was obtained for the
General layer. When the specific layers and sub-
layers are taken into account, Kappa statistic was
</bodyText>
<page confidence="0.999661">
35
</page>
<tableCaption confidence="0.998867">
Table 4: Agreement among labellers
</tableCaption>
<bodyText confidence="0.7246016">
General layer All layers
Agreement ratio 86.7% 74.2%
Kappa statistic 0.74 0.68
0.68, which is considered a good result for this
type of task. (cf. (Shriberg et al., 2004) etc.)
</bodyText>
<subsectionHeader confidence="0.996298">
4.2.3 Analysis of Occurrence Tendency
during Progress of Episode
</subsectionHeader>
<bodyText confidence="0.999984894736842">
We then investigated the tendencies of tag occur-
rence through a dialogue to clarify how consult-
ing is conducted in the corpus. We annotated the
boundaries of episodes that determined the spots
to visit in order to carefully investigate the struc-
ture of the decision-making processes. In our cor-
pus, users were asked to write down their itinerary
for a practical one day tour. Thus, the beginning
and ending of an episode can be determined on the
basis of this itinerary.
As a result, we found 192 episodes. We selected
122 episodes that had more than 50 utterances,
and analyzed the tendency of tag occurrence. The
episodes were divided into five segments so that
each segment had an equal number of utterances.
The tendency of tag occurrence is shown in Figure
1. The relative occurrence rate denotes the number
of times the tags appeared in each segment divided
by the total number of occurrences throughout the
dialogues. We found three patterns in the tendency
of occurrence. The tags corresponding to the first
pattern frequently appear in the early part of an
episode; this typically applies to Open-Question,
WH-Question, and Wish. The tags of the sec-
ond pattern frequently appear in the later part, this
typically applies to Evaluation, Commitment, and
Opinion. The tags of the third pattern appear uni-
formly over an episode, e.g., Y/N-Question, Ac-
cept, and Elaboration. These statistics characterize
the dialogue flow of sightseeing planning, where
the guide and the user first clarify the latter’s in-
terests (Open, WH-Questions), list and evaluate
candidates (Evaluation), and then the user makes
a decision (Commitment).
This progression indicates that a session (or di-
alogue phase) management is required within an
episode to manage the consulting dialogue, al-
though the test-set perplexity2, which was calcu-
</bodyText>
<footnote confidence="0.844497">
2The perplexity was calculated by 10-fold cross validation
of the 30 dialogues.
</footnote>
<figureCaption confidence="0.7823134">
Figure 1: Progress of episodes vs. occurrence of
speech act tags
lated by a 3-gram language model trained with the
SA tags, was not high (4.25 using the general layer
and 14.75 using all layers).
</figureCaption>
<sectionHeader confidence="0.988455" genericHeader="method">
5 Semantic Content Tags
</sectionHeader>
<bodyText confidence="0.99890575">
The semantic content tag set was designed to cap-
ture the contents of an utterance. Some might con-
sider semantic representations by HPSG (Pollard
and Sag, 1994) or LFG (Dalrymple et al., 1994)
for an utterance. Such frameworks require knowl-
edge of grammar and experiences to describe the
meaning of an utterance. In addition, the utter-
ances in a dialogue are often fragmentary, which
makes the description more difficult.
We focused on the predicate-argument structure
that is based on dependency relations. Annotating
dependency relations is more intuitive and is easier
than annotating the syntax structure; moreover, a
dependency parser is more robust for fragmentary
expressions than syntax parsers.
We introduced semantic classes to represent the
semantic contents of an utterance. Semantic class
labels are applied to each unit of the predicate-
argument structure. The task that identifies the
semantic classes is very similar to named entity
recognition, because the classes of the named en-
tities can be equated to the semantic classes that
are used to express semantic content. However,
both nouns and predicates are very important for
capturing the semantic contents of an utterance.
For example, “10 a.m.” might denote the current
time in the context of planning, or it might signify
the opening time of a sightseeing spot. Thus, we
represent the semantic contents on the basis of the
predicate-argument structure. Each predicate and
argument is assigned a semantic category.
For example, the sentence “I would like to see
</bodyText>
<figure confidence="0.997203588235294">
Ratio of occurenca
0.6
0.5
0.4
0.3
0.2
0.1
0
1 2 3 4 5
Episode segment
Open—Question
Wish
Y/N—Question
Acknowledgment
Elaboration
Evaluation
Commitment
</figure>
<page confidence="0.794483">
36
</page>
<figureCaption confidence="0.956155">
Figure 2: Example of annotation with semantic
content tags
Figure 3: A part of the semantic category hierar-
chy
</figureCaption>
<bodyText confidence="0.999866642857143">
Kinkakuji temple.” is annotated as shown in Fig-
ure 2. In this figure, the semantic content tag
preference.action indicates that the predicate por-
tion expresses the speaker’s preference for the
speaker’s action, while the semantic content tag
preference.spot.name indicates the name of the
spot as the object of the speaker’s preference.
Although we do not define semantic the role
(e.g., object (Kinakuji temple) and subject (I))
of each argument item in this case, we can use
conventional semantic role labeling techniques
(Gildea and Jurafsky, 2002) to estimate them.
Therefore, we do not annotate such semantic role
labels in the corpus.
</bodyText>
<subsectionHeader confidence="0.999048">
5.1 Tag Specifications
</subsectionHeader>
<bodyText confidence="0.999983055555556">
We defined hierarchical semantic classes to anno-
tate the semantic content tags. There are 33 la-
bels (classes) at the top hierarchical level. The la-
bels are, for example, activity, event, meal, spot,
transportation, cost, consulting, and location, as
shown in Figure 3. There two kinds of labels,
nodes and leaves. A node must have at least one
child, a node or a leaf. A leaf has no children. The
number of kinds for nodes is 47, and the number
of kinds for leaves is 47. The labels of leaves are
very similar to the labels for named entity recog-
nition. For example, there are “year, date, time,
organizer, name, and so on.” in the labels of the
leaves.
One of the characteristics of the semantic struc-
ture is that the lower level structures are shared by
many upper nodes. Thus, the lower level structure
can be used in any other domains or target tasks.
</bodyText>
<subsectionHeader confidence="0.999821">
5.2 Annotation of semantic contents tags
</subsectionHeader>
<bodyText confidence="0.99994821875">
The annotation of semantic contents tags is per-
formed by the following four steps. First, an ut-
terance is analyzed by a morphological analyzer,
ChaSen3. Second, the morphemes are chunked
into dependency unit (bunsetsu). Third, depen-
dency analysis is performed using a Japanese de-
pendency parser, CaboCha4. Finally, we annotate
the semantic content tags for each bunsetsu unit by
using our annotation tool. An example of an an-
notation is shown in Table 1. Each row in column
“Transcript” denotes the divided bunsetsu units.
The annotation tool interface is shown in Figure
4. In the left side of this figure, the dialogue files
and each utterance of the dialogue information are
displayed. The dependency structure of an utter-
ance is displayed in the upper part of the figure.
The morphological analysis results and chunk in-
formation are displayed in the lower part of the
figure.
At present, the annotations of semantic con-
tent tags are being carried out for 10 dialogues.
Approximately 22,000 paths, including paths that
will not be used, exist if the layered structure is
fully expanded. In the 10 dialogues, 1,380 tags (or
paths) are used.
In addition, not only to annotate semantic con-
tent tags, but to correct the morphological analyze
results and dependency analyzed results are being
carried out. If we complete the annotation, we will
also obtain these correctly tagged data of Kyoto
tour guide corpus. These corpora can be used to
develop analyzers such as morphological analyz-
</bodyText>
<footnote confidence="0.9991605">
3http://sourceforge.jp/projects/chasen-legacy/
4http://chasen.org/˜taku/software/cabocha/
</footnote>
<figure confidence="0.99714684">
automatically analyzed
predicate argument structure
would like to see ( I Kinkakuji temple )
predicate arguments
manually annotated
given sentence
I would like to see Kinkakuji temple
annotation result
would like to see ( I Kinkakuji temple )
preference.action preference.spot.name
(preference) (recommendation) (decision) (consulting)
(distance)
(cost)
(spot)
action
(money)
(restaurant)
(view)
entity predicate
architecture nature
(schedule)
(activity)
name type
action predicate
object
</figure>
<page confidence="0.866618">
37
</page>
<figureCaption confidence="0.999481">
Figure 4: Annotation tool interface for annotating semantic content tags
</figureCaption>
<bodyText confidence="0.999743">
ers and dependency analyzers via machine learn-
ing techniques or to adapt analyzers for this do-
main.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999919695652174">
In this paper, we have introduced our spoken di-
alogue corpus for developing consulting dialogue
systems. We designed a dialogue act annotation
scheme that describes two aspects of a DA: speech
act and semantic content. The speech act tag set
was designed by extending the MRDA tag set.
The design of the semantic content tag set is al-
most complete. If we complete the annotation, we
will obtain speech act tags and semantic content
tags, as well as manual transcripts, morphologi-
cal analysis results, dependency analysis results,
and dialogue episodes. As a preliminary analysis,
we have evaluated the SA tag set in terms of the
agreement between labellers and investigated the
patterns of tag occurrences.
In the next step, we will construct automatic
taggers for speech act and semantic content tags
by using the annotated corpora and machine learn-
ing techniques. Our future work also includes a
condensation or selection of dialogue acts that di-
rectly affect the dialogue flow in order to construct
a consulting dialogue system using the DA tags as
an input.
</bodyText>
<sectionHeader confidence="0.999099" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999615258064516">
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2006. Learning the structure of
task-driven human-human dialogs. In Proceedings
of COLING/ACL, pages 201–208.
Gies Bouwman, Janienke Sturm, and Louis Boves.
1999. Incorporating Confidence Measures in the
Dutch Train Timetable Information System Devel-
oped in the ARISE Project. In Proc. ICASSP.
Johan Boye. 2007. Dialogue Management for Auto-
matic Troubleshooting and Other Problem-solving
Applications. In Proc. of 8th SIGdial Workshop on
Discourse and Dialogue, pages 247–255.
Harry Bunt. 2000. Dialogue pragmatics and context
specification. In Harry Bunt and William Black,
editors, Abduction, Belief and Context in Dialogue,
pages 81–150. John Benjamins.
Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell
III, and Anni e Zaenen, editors. 1994. Formal Is-
sues in Lexical-Functional Grammar. CSLI Publi-
cations.
George Ferguson and James F. Allen. 1998. TRIPS:
An intelligent integrated problem-solving assistant.
In Proc. Fifteenth National Conference on Artificial
Intelligence, pages 567–573.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Chiori Hori, Kiyonori Ohtake, Teruhisa Misu, Hideki
Kashioka, and Satoshi Nakamura. 2008. Dialog
Management using Weighted Finite-state Transduc-
ers. In Proc. Interspeech, pages 211–214.
</reference>
<page confidence="0.985382">
38
</page>
<reference confidence="0.999688456521739">
Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function annotation coders manual, draft
13. Technical report, University of Colorado at
Boulder &amp; SRI International.
Hideki Kashioka and Takehiko Maruyama. 2004. Seg-
mentation of Semantic Unit in Japanese Monologue.
In Proc. ICSLT-O-COCOSDA.
Lori F. Lamel, Samir Bennacef, Jean-Luc Gauvain,
H. Dartigues, and J. N. Temem. 2002. User eval-
uation of the MASK kiosk. Speech Communication,
38(1):131–139.
Lori Levin, Donna Gates, Dorcas Wallace, Kay Peter-
son, Along Lavie, Fabio Pianesi, Emanuele Pianta,
Roldano Cattoni, and Nadia Mana. 2002. Balancing
expressiveness and simplicity in an interlingua for
task based dialogue. In Proceedings of ACL 2002
workshop on Speech-to-speech Translation: Algo-
rithms and Systems.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus
of Japanese. In Proceedings of the Second Interna-
tional Conference of Language Resources and Eval-
uation (LREC2000), pages 947–952.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press.
Kepa Joseba Rodriguez, Stefanie Dipper, Michael
G¨otze, Massimo Poesio, Giuseppe Riccardi, Chris-
tian Raymond, and Joanna Rabiega-Wisniewska.
2007. Standoff Coordination for Multi-Tool Anno-
tation in a Dialogue Corpus. In Proc. Linguistic An-
notation Workshop, pages 148–155.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI Meet-
ing Recorder Dialog Act (MRDA) Corpus. In Proc.
5th SIGdial Workshop on Discourse and Dialogue,
pages 97–100.
Blaise Thomson, Jost Schatzmann, and Steve Young.
2008. Bayesian update of dialogue state for robust
dialogue systems. In Proceedings of ICASSP ’08.
Marilyn A. Walker, Rebecca Passonneau, and Julie E.
Boland. 2001. Quantitative and Qualitative Eval-
uation of DARPA Communicator Spoken Dialogue
Systems. In Proc. of 39th Annual Meeting of the
ACL, pages 515–522.
</reference>
<page confidence="0.999531">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.588297">
<title confidence="0.999976">Annotating Dialogue Acts to Construct Dialogue Systems for Consulting</title>
<author confidence="0.828383">Kiyonori Ohtake Teruhisa Misu Chiori Hori Hideki Kashioka Satoshi</author>
<affiliation confidence="0.723711">MASTAR Project, National Institute of Information and Communications Hikaridai, Keihanna Science City,</affiliation>
<email confidence="0.903968">kiyonori.ohtake(at)nict.go.jp</email>
<abstract confidence="0.999755625">This paper introduces a new corpus of consulting dialogues, which is designed for training a dialogue manager that can handle consulting dialogues through spontaneous interactions from the tagged dialogue corpus. We have collected 130 h of consulting dialogues in the tourist guidance domain. This paper outlines our taxonomy of dialogue act annotation that can describe two aspects of an utterances: the communicative function (speech act), and the semantic content of the utterance. We provide an overview of the Kyoto tour guide dialogue corpus and a preliminary analysis using the dialogue act tags.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Giuseppe Di Fabbrizio</author>
<author>Amanda Stent</author>
</authors>
<title>Learning the structure of task-driven human-human dialogs.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>201--208</pages>
<marker>Bangalore, Di Fabbrizio, Stent, 2006</marker>
<rawString>Srinivas Bangalore, Giuseppe Di Fabbrizio, and Amanda Stent. 2006. Learning the structure of task-driven human-human dialogs. In Proceedings of COLING/ACL, pages 201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gies Bouwman</author>
<author>Janienke Sturm</author>
<author>Louis Boves</author>
</authors>
<date>1999</date>
<booktitle>Incorporating Confidence Measures in the Dutch Train Timetable Information System Developed in the ARISE Project. In Proc. ICASSP.</booktitle>
<contexts>
<context position="1757" citStr="Bouwman et al., 1999" startWordPosition="262" endWordPosition="265">, and semantic content tags. In this paper, we describe the current status of a dialogue corpus that is being developed by our research group, focusing on two types of tags: speech act tags and semantic content tags. These speech act and semantic content tags were designed to express the dialogue act of each utterance. Many studies have focused on developing spoken dialogue systems. Their typical task domains included the retrieval of information from databases or making reservations, such as airline information e.g., DARPA Communicator (Walker et al., 2001) and train information e.g., ARISE (Bouwman et al., 1999) and MASK (Lamel et al., 2002). Most studies assumed a definite and consistent user objective, and the dialogue strategy was usually designed to minimize the cost of information access. Other target tasks include tutoring and trouble-shooting dialogues (Boye, 2007). In such tasks, dialogue scenarios or agendas are usually described using a (dynamic) tree structure, and the objective is to satisfy all requirements. In this paper, we introduce our corpus, which is being developed as part of a project to construct consulting dialogue systems, that helps the user in making a decision. So far, seve</context>
</contexts>
<marker>Bouwman, Sturm, Boves, 1999</marker>
<rawString>Gies Bouwman, Janienke Sturm, and Louis Boves. 1999. Incorporating Confidence Measures in the Dutch Train Timetable Information System Developed in the ARISE Project. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Boye</author>
</authors>
<title>Dialogue Management for Automatic Troubleshooting and Other Problem-solving Applications.</title>
<date>2007</date>
<booktitle>In Proc. of 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>247--255</pages>
<contexts>
<context position="2022" citStr="Boye, 2007" startWordPosition="306" endWordPosition="307">o express the dialogue act of each utterance. Many studies have focused on developing spoken dialogue systems. Their typical task domains included the retrieval of information from databases or making reservations, such as airline information e.g., DARPA Communicator (Walker et al., 2001) and train information e.g., ARISE (Bouwman et al., 1999) and MASK (Lamel et al., 2002). Most studies assumed a definite and consistent user objective, and the dialogue strategy was usually designed to minimize the cost of information access. Other target tasks include tutoring and trouble-shooting dialogues (Boye, 2007). In such tasks, dialogue scenarios or agendas are usually described using a (dynamic) tree structure, and the objective is to satisfy all requirements. In this paper, we introduce our corpus, which is being developed as part of a project to construct consulting dialogue systems, that helps the user in making a decision. So far, several projects have been organized to construct speech corpora such as CSJ (Maekawa et al., 2000) for Japanese. The size of CSJ is very large, and a great part of the corpus consists of monologues. Although, CSJ includes some dialogues, the size of dialogues is not e</context>
</contexts>
<marker>Boye, 2007</marker>
<rawString>Johan Boye. 2007. Dialogue Management for Automatic Troubleshooting and Other Problem-solving Applications. In Proc. of 8th SIGdial Workshop on Discourse and Dialogue, pages 247–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Bunt</author>
</authors>
<title>Dialogue pragmatics and context specification.</title>
<date>2000</date>
<booktitle>Abduction, Belief and Context in Dialogue,</booktitle>
<pages>81--150</pages>
<editor>In Harry Bunt and William Black, editors,</editor>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="4804" citStr="Bunt, 2000" startWordPosition="750" endWordPosition="751">gues 114 80 62 # of guides 3 2 2 avg. # of utterance 365.4 165.2 324.5 / dialogue (guide) 301.7 112.9 373.5 avg. # of utterance / dialogue (tourist) structure of the transducers and the weight for each state transitions from an annotated corpus. Thus, the corpus must be sufficiently rich in information to describe the consulting dialogue to construct the statistical dialogue manager via such techniques. In addition, a detailed description would be preferable when developing modules that focus on spoken language understanding and generation modules. In this study, we adopt dialogue acts (DAs) (Bunt, 2000; Shriberg et al., 2004; Bangalore et al., 2006; Rodriguez et al., 2007; Levin et al., 2002) for this information and annotate DAs in the corpus. In this paper, we describe the design of the Kyoto tour guide dialogue corpus in Section 2. Our design of the DA annotation is described in Section 3. Sections 4 and 5 respectively describe two types of the tag sets, namely, the speech act tag and the semantic content tag. 2 Kyoto Tour Guide Dialogue Corpus We are currently developing a dialogue corpus based on tourist guidance for Kyoto City as the target domain. Thus far, we have collected itinerar</context>
<context position="10552" citStr="Bunt (2000)" startWordPosition="1637" endWordPosition="1638">ty),action Oharamo (Ohara is) (activity),location 64 88392–90072 Guide sugoku (very) State Opinion (recommendation),(activity),adverb-phrase kireidesuyo (a beautiful spot) (recommendation),(activity),predicate 3 Annotation of Communicative Function and Semantic Content in DA We annotate DAs in the corpus in order to describe a user’s intention and a system’s (or the tour guide’s) action. Recently, several studies have addressed multilevel annotation of dialogues (Levin et al., 2002; Bangalore et al., 2006; Rodriguez et al., 2007); in our study, we focus on the two aspects of a DA indicated by Bunt (2000). One is the communicative function that corresponds to how the content should be used in order to update the context, and the other is a semantic content that corresponds to what the act is about. We consider both of them important information to handle the consulting dialogue. We designed two different tag sets to annotate DAs in the corpus. The speech act tag is used to capture the communicative functions of an utterance using domain-independent multiple function layers. The semantic content tag is used to describe the semantic contents of an utterance using domain-specific hierarchical sem</context>
<context position="11808" citStr="Bunt, 2000" startWordPosition="1845" endWordPosition="1846">ion, we introduce the speech act (SA) tag set that describes communicative functions of utterances. As the base units for tag annotation, we adopt clauses that are detected by applying the clause boundary annotation program (Kashioka and Maruyama, 2004) to the transcript of the dialogue. Thus, in the following discussions, “utterance” denotes a clause. 4.1 Tag Specifications There are two major policies in SA annotation. One is to select exactly one label from the tag set (e.g., the AMI corpus&apos;). The other is to annotate with as many labels as required. MRDA (Shriberg et al., 2004) and DIT++ (Bunt, 2000) are defined on the basis of the second policy. We believe that utterances are generally multifunctional and this multifunctionality is an important aspect for managing consulting dialogues through spontaneous interactions. Therefore, we have adopted the latter policy. By extending the MRDA tag set and DIT++, we defined our speech act tag set that consists of six layers to describe six groups of function: General, Response, Check, Constrain, ActionDiscussion, and Others. A list of the tag sets (excluding the Others layer is shown in Table 3. The General layer has two sublayers under the labels</context>
</contexts>
<marker>Bunt, 2000</marker>
<rawString>Harry Bunt. 2000. Dialogue pragmatics and context specification. In Harry Bunt and William Black, editors, Abduction, Belief and Context in Dialogue, pages 81–150. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
<author>T John</author>
</authors>
<title>Formal Issues in Lexical-Functional Grammar.</title>
<date>1994</date>
<booktitle>III, and Anni e Zaenen,</booktitle>
<editor>Maxwell</editor>
<publisher>CSLI Publications.</publisher>
<location>editors.</location>
<contexts>
<context position="19855" citStr="Dalrymple et al., 1994" startWordPosition="3124" endWordPosition="3127">hase) management is required within an episode to manage the consulting dialogue, although the test-set perplexity2, which was calcu2The perplexity was calculated by 10-fold cross validation of the 30 dialogues. Figure 1: Progress of episodes vs. occurrence of speech act tags lated by a 3-gram language model trained with the SA tags, was not high (4.25 using the general layer and 14.75 using all layers). 5 Semantic Content Tags The semantic content tag set was designed to capture the contents of an utterance. Some might consider semantic representations by HPSG (Pollard and Sag, 1994) or LFG (Dalrymple et al., 1994) for an utterance. Such frameworks require knowledge of grammar and experiences to describe the meaning of an utterance. In addition, the utterances in a dialogue are often fragmentary, which makes the description more difficult. We focused on the predicate-argument structure that is based on dependency relations. Annotating dependency relations is more intuitive and is easier than annotating the syntax structure; moreover, a dependency parser is more robust for fragmentary expressions than syntax parsers. We introduced semantic classes to represent the semantic contents of an utterance. Seman</context>
</contexts>
<marker>Dalrymple, Kaplan, John, 1994</marker>
<rawString>Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Anni e Zaenen, editors. 1994. Formal Issues in Lexical-Functional Grammar. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Ferguson</author>
<author>James F Allen</author>
</authors>
<title>TRIPS: An intelligent integrated problem-solving assistant.</title>
<date>1998</date>
<booktitle>In Proc. Fifteenth National Conference on Artificial Intelligence,</booktitle>
<pages>567--573</pages>
<contexts>
<context position="7758" citStr="Ferguson and Allen (1998)" startWordPosition="1251" endWordPosition="1254">rough the hands-free headset. Dialogues to plan a one-day visit consist of several conversations for choosing places to visit. The conversations usually included sequences of requests from the users and provision of information by the guides as well as consultation in the form of explanation and evaluation. It should be noted that in this study, enabling the user to access information is not an objective in itself, unlike information kiosk systems such as those developed in (Lamel et al., 2002) or (Thomson et al., 2008). The objective is similar to the problem-solving dialogue of the study by Ferguson and Allen (1998), in other words, accessing information is just an aspect of consulting dialogues. An example of dialogue via face-to-face communication is shown in Table 1. This dialogue is a part of a consultation to decide on a sightseeing spot to visit. The user asks about the location of a spot, and the guide answers it. Then, the user provides a follow-up by evaluating the answer. The task is challenging because there are many utterances that affect the flow of the dialogue during a consultation. The utterances are listed in the order of their start times with the utterance ids (UID). From the column “T</context>
</contexts>
<marker>Ferguson, Allen, 1998</marker>
<rawString>George Ferguson and James F. Allen. 1998. TRIPS: An intelligent integrated problem-solving assistant. In Proc. Fifteenth National Conference on Artificial Intelligence, pages 567–573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="22012" citStr="Gildea and Jurafsky, 2002" startWordPosition="3461" endWordPosition="3464"> semantic content tags Figure 3: A part of the semantic category hierarchy Kinkakuji temple.” is annotated as shown in Figure 2. In this figure, the semantic content tag preference.action indicates that the predicate portion expresses the speaker’s preference for the speaker’s action, while the semantic content tag preference.spot.name indicates the name of the spot as the object of the speaker’s preference. Although we do not define semantic the role (e.g., object (Kinakuji temple) and subject (I)) of each argument item in this case, we can use conventional semantic role labeling techniques (Gildea and Jurafsky, 2002) to estimate them. Therefore, we do not annotate such semantic role labels in the corpus. 5.1 Tag Specifications We defined hierarchical semantic classes to annotate the semantic content tags. There are 33 labels (classes) at the top hierarchical level. The labels are, for example, activity, event, meal, spot, transportation, cost, consulting, and location, as shown in Figure 3. There two kinds of labels, nodes and leaves. A node must have at least one child, a node or a leaf. A leaf has no children. The number of kinds for nodes is 47, and the number of kinds for leaves is 47. The labels of l</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiori Hori</author>
<author>Kiyonori Ohtake</author>
<author>Teruhisa Misu</author>
<author>Hideki Kashioka</author>
<author>Satoshi Nakamura</author>
</authors>
<title>Dialog Management using Weighted Finite-state Transducers. In</title>
<date>2008</date>
<booktitle>Proc. Interspeech,</booktitle>
<pages>211--214</pages>
<contexts>
<context position="3810" citStr="Hori et al. (2008)" startWordPosition="589" endWordPosition="592">ications and explanations. The user may explain his/her preferences vaguely by listing examples. The server would then sense the user’s preferences from his/her utterances, provide some information, and then request a decision. It is almost impossible to handcraft a scenario that can handle such spontaneous consulting dialogues; thus, the dialogue strategy should be bootstrapped from a dialogue corpus. If an extensive dialogue corpus is available, we can model the dialogue using machine learning techniques such as partially observable Markov decision processes (POMDPs) (Thomson et al., 2008). Hori et al. (2008) have also proposed an efficient approach to organize a dialogue system using weighted finitestate transducers (WFSTs); the system obtains the 32 Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32–39, Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Table 2: Overview of Kyoto tour guide dialogue corpus dialogue type F2F WOZ TEL # of dialogues 114 80 62 # of guides 3 2 2 avg. # of utterance 365.4 165.2 324.5 / dialogue (guide) 301.7 112.9 373.5 avg. # of utterance / dialogue (tourist) structure of the transducers and the weight for each state transiti</context>
</contexts>
<marker>Hori, Ohtake, Misu, Kashioka, Nakamura, 2008</marker>
<rawString>Chiori Hori, Kiyonori Ohtake, Teruhisa Misu, Hideki Kashioka, and Satoshi Nakamura. 2008. Dialog Management using Weighted Finite-state Transducers. In Proc. Interspeech, pages 211–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Debra Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL shallowdiscourse-function annotation coders manual, draft 13.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>University of Colorado at Boulder &amp; SRI International.</institution>
<contexts>
<context position="16495" citStr="Jurafsky et al., 1997" startWordPosition="2581" endWordPosition="2584">xtual information, audio information, and contextual information The result was cross-checked by another labeller. 4.2.1 Distributional Statistics The frequencies of the tags, expressed as a percentages, are shown in Table 3. In the General layer, nearly half of the utterances were Statement. This bias is acceptable because 66% of the utterances had tag(s) of other layers. The percentages of tags in the Constrain layer are relatively higher than those of tags in the other layers. They are also higher than the percentages of the corresponding tags of MRDA (Shriberg et al., 2004) and SWBD-DAMSL(Jurafsky et al., 1997). These statistics characterize the consulting dialogue of sightseeing planning, where explanations and evaluations play an important role during the decision process. 4.2.2 Reliability We investigated the reliability of the annotation. Another two dialogues (2,087 utterances) were annotated by three labelers and the agreement among them was examined. These results are listed in Table 4. The agreement ratio is the average of all the combinations of the three individual agreements. In the same way, we also computed the average Kappa statistic, which is often used to measure the agreement by con</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca. 1997. Switchboard SWBD-DAMSL shallowdiscourse-function annotation coders manual, draft 13. Technical report, University of Colorado at Boulder &amp; SRI International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kashioka</author>
<author>Takehiko Maruyama</author>
</authors>
<title>Segmentation of Semantic Unit in Japanese Monologue.</title>
<date>2004</date>
<booktitle>In Proc. ICSLT-O-COCOSDA.</booktitle>
<contexts>
<context position="11450" citStr="Kashioka and Maruyama, 2004" startWordPosition="1780" endWordPosition="1784">dialogue. We designed two different tag sets to annotate DAs in the corpus. The speech act tag is used to capture the communicative functions of an utterance using domain-independent multiple function layers. The semantic content tag is used to describe the semantic contents of an utterance using domain-specific hierarchical semantic classes. 4 Speech Act Tags In this section, we introduce the speech act (SA) tag set that describes communicative functions of utterances. As the base units for tag annotation, we adopt clauses that are detected by applying the clause boundary annotation program (Kashioka and Maruyama, 2004) to the transcript of the dialogue. Thus, in the following discussions, “utterance” denotes a clause. 4.1 Tag Specifications There are two major policies in SA annotation. One is to select exactly one label from the tag set (e.g., the AMI corpus&apos;). The other is to annotate with as many labels as required. MRDA (Shriberg et al., 2004) and DIT++ (Bunt, 2000) are defined on the basis of the second policy. We believe that utterances are generally multifunctional and this multifunctionality is an important aspect for managing consulting dialogues through spontaneous interactions. Therefore, we have</context>
</contexts>
<marker>Kashioka, Maruyama, 2004</marker>
<rawString>Hideki Kashioka and Takehiko Maruyama. 2004. Segmentation of Semantic Unit in Japanese Monologue. In Proc. ICSLT-O-COCOSDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori F Lamel</author>
<author>Samir Bennacef</author>
<author>Jean-Luc Gauvain</author>
<author>H Dartigues</author>
<author>J N Temem</author>
</authors>
<date>2002</date>
<journal>User evaluation of the MASK kiosk. Speech Communication,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="1787" citStr="Lamel et al., 2002" startWordPosition="268" endWordPosition="271">this paper, we describe the current status of a dialogue corpus that is being developed by our research group, focusing on two types of tags: speech act tags and semantic content tags. These speech act and semantic content tags were designed to express the dialogue act of each utterance. Many studies have focused on developing spoken dialogue systems. Their typical task domains included the retrieval of information from databases or making reservations, such as airline information e.g., DARPA Communicator (Walker et al., 2001) and train information e.g., ARISE (Bouwman et al., 1999) and MASK (Lamel et al., 2002). Most studies assumed a definite and consistent user objective, and the dialogue strategy was usually designed to minimize the cost of information access. Other target tasks include tutoring and trouble-shooting dialogues (Boye, 2007). In such tasks, dialogue scenarios or agendas are usually described using a (dynamic) tree structure, and the objective is to satisfy all requirements. In this paper, we introduce our corpus, which is being developed as part of a project to construct consulting dialogue systems, that helps the user in making a decision. So far, several projects have been organiz</context>
<context position="7632" citStr="Lamel et al., 2002" startWordPosition="1229" endWordPosition="1232">esis program. The guide and a tourist shared the same interface in different rooms, and they could talk to each other through the hands-free headset. Dialogues to plan a one-day visit consist of several conversations for choosing places to visit. The conversations usually included sequences of requests from the users and provision of information by the guides as well as consultation in the form of explanation and evaluation. It should be noted that in this study, enabling the user to access information is not an objective in itself, unlike information kiosk systems such as those developed in (Lamel et al., 2002) or (Thomson et al., 2008). The objective is similar to the problem-solving dialogue of the study by Ferguson and Allen (1998), in other words, accessing information is just an aspect of consulting dialogues. An example of dialogue via face-to-face communication is shown in Table 1. This dialogue is a part of a consultation to decide on a sightseeing spot to visit. The user asks about the location of a spot, and the guide answers it. Then, the user provides a follow-up by evaluating the answer. The task is challenging because there are many utterances that affect the flow of the dialogue durin</context>
</contexts>
<marker>Lamel, Bennacef, Gauvain, Dartigues, Temem, 2002</marker>
<rawString>Lori F. Lamel, Samir Bennacef, Jean-Luc Gauvain, H. Dartigues, and J. N. Temem. 2002. User evaluation of the MASK kiosk. Speech Communication, 38(1):131–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori Levin</author>
<author>Donna Gates</author>
<author>Dorcas Wallace</author>
<author>Kay Peterson</author>
<author>Along Lavie</author>
<author>Fabio Pianesi</author>
<author>Emanuele Pianta</author>
<author>Roldano Cattoni</author>
<author>Nadia Mana</author>
</authors>
<title>Balancing expressiveness and simplicity in an interlingua for task based dialogue.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="4896" citStr="Levin et al., 2002" startWordPosition="765" endWordPosition="768">guide) 301.7 112.9 373.5 avg. # of utterance / dialogue (tourist) structure of the transducers and the weight for each state transitions from an annotated corpus. Thus, the corpus must be sufficiently rich in information to describe the consulting dialogue to construct the statistical dialogue manager via such techniques. In addition, a detailed description would be preferable when developing modules that focus on spoken language understanding and generation modules. In this study, we adopt dialogue acts (DAs) (Bunt, 2000; Shriberg et al., 2004; Bangalore et al., 2006; Rodriguez et al., 2007; Levin et al., 2002) for this information and annotate DAs in the corpus. In this paper, we describe the design of the Kyoto tour guide dialogue corpus in Section 2. Our design of the DA annotation is described in Section 3. Sections 4 and 5 respectively describe two types of the tag sets, namely, the speech act tag and the semantic content tag. 2 Kyoto Tour Guide Dialogue Corpus We are currently developing a dialogue corpus based on tourist guidance for Kyoto City as the target domain. Thus far, we have collected itinerary planning dialogues in Japanese, in which users plan a one-day visit to Kyoto City. There a</context>
<context position="10427" citStr="Levin et al., 2002" startWordPosition="1610" endWordPosition="1613">redicate State AffirmativeAnswer→61 jubuN (enough) (consulting),(activity),adverb-phrase ikemasu (to enjoy it.) (consulting),(activity),action Oharamo (Ohara is) (activity),location 64 88392–90072 Guide sugoku (very) State Opinion (recommendation),(activity),adverb-phrase kireidesuyo (a beautiful spot) (recommendation),(activity),predicate 3 Annotation of Communicative Function and Semantic Content in DA We annotate DAs in the corpus in order to describe a user’s intention and a system’s (or the tour guide’s) action. Recently, several studies have addressed multilevel annotation of dialogues (Levin et al., 2002; Bangalore et al., 2006; Rodriguez et al., 2007); in our study, we focus on the two aspects of a DA indicated by Bunt (2000). One is the communicative function that corresponds to how the content should be used in order to update the context, and the other is a semantic content that corresponds to what the act is about. We consider both of them important information to handle the consulting dialogue. We designed two different tag sets to annotate DAs in the corpus. The speech act tag is used to capture the communicative functions of an utterance using domain-independent multiple function laye</context>
</contexts>
<marker>Levin, Gates, Wallace, Peterson, Lavie, Pianesi, Pianta, Cattoni, Mana, 2002</marker>
<rawString>Lori Levin, Donna Gates, Dorcas Wallace, Kay Peterson, Along Lavie, Fabio Pianesi, Emanuele Pianta, Roldano Cattoni, and Nadia Mana. 2002. Balancing expressiveness and simplicity in an interlingua for task based dialogue. In Proceedings of ACL 2002 workshop on Speech-to-speech Translation: Algorithms and Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
<author>Hanae Koiso</author>
<author>Sadaoki Furui</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Spontaneous speech corpus of Japanese.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference of Language Resources and Evaluation (LREC2000),</booktitle>
<pages>947--952</pages>
<contexts>
<context position="2452" citStr="Maekawa et al., 2000" startWordPosition="374" endWordPosition="377">tent user objective, and the dialogue strategy was usually designed to minimize the cost of information access. Other target tasks include tutoring and trouble-shooting dialogues (Boye, 2007). In such tasks, dialogue scenarios or agendas are usually described using a (dynamic) tree structure, and the objective is to satisfy all requirements. In this paper, we introduce our corpus, which is being developed as part of a project to construct consulting dialogue systems, that helps the user in making a decision. So far, several projects have been organized to construct speech corpora such as CSJ (Maekawa et al., 2000) for Japanese. The size of CSJ is very large, and a great part of the corpus consists of monologues. Although, CSJ includes some dialogues, the size of dialogues is not enough to construct a dialogue system via recent statistical techniques. In addition, relatively to consulting dialogues, the existing large dialogue corpora covered very clear tasks in limited domains. However, consulting is a frequently used and very natural form of human interaction. We often consult with a sales clerk while shopping or with staff at a concierge desk in a hotel. Such dialogues usually form part of a series o</context>
</contexts>
<marker>Maekawa, Koiso, Furui, Isahara, 2000</marker>
<rawString>Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Isahara. 2000. Spontaneous speech corpus of Japanese. In Proceedings of the Second International Conference of Language Resources and Evaluation (LREC2000), pages 947–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="19823" citStr="Pollard and Sag, 1994" startWordPosition="3118" endWordPosition="3121">s that a session (or dialogue phase) management is required within an episode to manage the consulting dialogue, although the test-set perplexity2, which was calcu2The perplexity was calculated by 10-fold cross validation of the 30 dialogues. Figure 1: Progress of episodes vs. occurrence of speech act tags lated by a 3-gram language model trained with the SA tags, was not high (4.25 using the general layer and 14.75 using all layers). 5 Semantic Content Tags The semantic content tag set was designed to capture the contents of an utterance. Some might consider semantic representations by HPSG (Pollard and Sag, 1994) or LFG (Dalrymple et al., 1994) for an utterance. Such frameworks require knowledge of grammar and experiences to describe the meaning of an utterance. In addition, the utterances in a dialogue are often fragmentary, which makes the description more difficult. We focused on the predicate-argument structure that is based on dependency relations. Annotating dependency relations is more intuitive and is easier than annotating the syntax structure; moreover, a dependency parser is more robust for fragmentary expressions than syntax parsers. We introduced semantic classes to represent the semantic</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kepa Joseba Rodriguez</author>
<author>Stefanie Dipper</author>
<author>Michael G¨otze</author>
<author>Massimo Poesio</author>
<author>Giuseppe Riccardi</author>
<author>Christian Raymond</author>
<author>Joanna Rabiega-Wisniewska</author>
</authors>
<title>Standoff Coordination for Multi-Tool Annotation in a Dialogue Corpus. In</title>
<date>2007</date>
<booktitle>Proc. Linguistic Annotation Workshop,</booktitle>
<pages>148--155</pages>
<marker>Rodriguez, Dipper, G¨otze, Poesio, Riccardi, Raymond, Rabiega-Wisniewska, 2007</marker>
<rawString>Kepa Joseba Rodriguez, Stefanie Dipper, Michael G¨otze, Massimo Poesio, Giuseppe Riccardi, Christian Raymond, and Joanna Rabiega-Wisniewska. 2007. Standoff Coordination for Multi-Tool Annotation in a Dialogue Corpus. In Proc. Linguistic Annotation Workshop, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Raj Dhillon</author>
<author>Sonali Bhagat</author>
<author>Jeremy Ang</author>
<author>Hannah Carvey</author>
</authors>
<title>The ICSI Meeting Recorder Dialog Act (MRDA) Corpus.</title>
<date>2004</date>
<booktitle>In Proc. 5th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>97--100</pages>
<contexts>
<context position="4827" citStr="Shriberg et al., 2004" startWordPosition="752" endWordPosition="755">62 # of guides 3 2 2 avg. # of utterance 365.4 165.2 324.5 / dialogue (guide) 301.7 112.9 373.5 avg. # of utterance / dialogue (tourist) structure of the transducers and the weight for each state transitions from an annotated corpus. Thus, the corpus must be sufficiently rich in information to describe the consulting dialogue to construct the statistical dialogue manager via such techniques. In addition, a detailed description would be preferable when developing modules that focus on spoken language understanding and generation modules. In this study, we adopt dialogue acts (DAs) (Bunt, 2000; Shriberg et al., 2004; Bangalore et al., 2006; Rodriguez et al., 2007; Levin et al., 2002) for this information and annotate DAs in the corpus. In this paper, we describe the design of the Kyoto tour guide dialogue corpus in Section 2. Our design of the DA annotation is described in Section 3. Sections 4 and 5 respectively describe two types of the tag sets, namely, the speech act tag and the semantic content tag. 2 Kyoto Tour Guide Dialogue Corpus We are currently developing a dialogue corpus based on tourist guidance for Kyoto City as the target domain. Thus far, we have collected itinerary planning dialogues in</context>
<context position="11785" citStr="Shriberg et al., 2004" startWordPosition="1839" endWordPosition="1842">es. 4 Speech Act Tags In this section, we introduce the speech act (SA) tag set that describes communicative functions of utterances. As the base units for tag annotation, we adopt clauses that are detected by applying the clause boundary annotation program (Kashioka and Maruyama, 2004) to the transcript of the dialogue. Thus, in the following discussions, “utterance” denotes a clause. 4.1 Tag Specifications There are two major policies in SA annotation. One is to select exactly one label from the tag set (e.g., the AMI corpus&apos;). The other is to annotate with as many labels as required. MRDA (Shriberg et al., 2004) and DIT++ (Bunt, 2000) are defined on the basis of the second policy. We believe that utterances are generally multifunctional and this multifunctionality is an important aspect for managing consulting dialogues through spontaneous interactions. Therefore, we have adopted the latter policy. By extending the MRDA tag set and DIT++, we defined our speech act tag set that consists of six layers to describe six groups of function: General, Response, Check, Constrain, ActionDiscussion, and Others. A list of the tag sets (excluding the Others layer is shown in Table 3. The General layer has two sub</context>
<context position="16457" citStr="Shriberg et al., 2004" startWordPosition="2576" endWordPosition="2579">the dialogues, we took into account textual information, audio information, and contextual information The result was cross-checked by another labeller. 4.2.1 Distributional Statistics The frequencies of the tags, expressed as a percentages, are shown in Table 3. In the General layer, nearly half of the utterances were Statement. This bias is acceptable because 66% of the utterances had tag(s) of other layers. The percentages of tags in the Constrain layer are relatively higher than those of tags in the other layers. They are also higher than the percentages of the corresponding tags of MRDA (Shriberg et al., 2004) and SWBD-DAMSL(Jurafsky et al., 1997). These statistics characterize the consulting dialogue of sightseeing planning, where explanations and evaluations play an important role during the decision process. 4.2.2 Reliability We investigated the reliability of the annotation. Another two dialogues (2,087 utterances) were annotated by three labelers and the agreement among them was examined. These results are listed in Table 4. The agreement ratio is the average of all the combinations of the three individual agreements. In the same way, we also computed the average Kappa statistic, which is ofte</context>
</contexts>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy Ang, and Hannah Carvey. 2004. The ICSI Meeting Recorder Dialog Act (MRDA) Corpus. In Proc. 5th SIGdial Workshop on Discourse and Dialogue, pages 97–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaise Thomson</author>
<author>Jost Schatzmann</author>
<author>Steve Young</author>
</authors>
<title>Bayesian update of dialogue state for robust dialogue systems.</title>
<date>2008</date>
<booktitle>In Proceedings of ICASSP ’08.</booktitle>
<contexts>
<context position="3790" citStr="Thomson et al., 2008" startWordPosition="585" endWordPosition="588">changes, such as clarifications and explanations. The user may explain his/her preferences vaguely by listing examples. The server would then sense the user’s preferences from his/her utterances, provide some information, and then request a decision. It is almost impossible to handcraft a scenario that can handle such spontaneous consulting dialogues; thus, the dialogue strategy should be bootstrapped from a dialogue corpus. If an extensive dialogue corpus is available, we can model the dialogue using machine learning techniques such as partially observable Markov decision processes (POMDPs) (Thomson et al., 2008). Hori et al. (2008) have also proposed an efficient approach to organize a dialogue system using weighted finitestate transducers (WFSTs); the system obtains the 32 Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32–39, Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Table 2: Overview of Kyoto tour guide dialogue corpus dialogue type F2F WOZ TEL # of dialogues 114 80 62 # of guides 3 2 2 avg. # of utterance 365.4 165.2 324.5 / dialogue (guide) 301.7 112.9 373.5 avg. # of utterance / dialogue (tourist) structure of the transducers and the weight for</context>
<context position="7658" citStr="Thomson et al., 2008" startWordPosition="1234" endWordPosition="1237">and a tourist shared the same interface in different rooms, and they could talk to each other through the hands-free headset. Dialogues to plan a one-day visit consist of several conversations for choosing places to visit. The conversations usually included sequences of requests from the users and provision of information by the guides as well as consultation in the form of explanation and evaluation. It should be noted that in this study, enabling the user to access information is not an objective in itself, unlike information kiosk systems such as those developed in (Lamel et al., 2002) or (Thomson et al., 2008). The objective is similar to the problem-solving dialogue of the study by Ferguson and Allen (1998), in other words, accessing information is just an aspect of consulting dialogues. An example of dialogue via face-to-face communication is shown in Table 1. This dialogue is a part of a consultation to decide on a sightseeing spot to visit. The user asks about the location of a spot, and the guide answers it. Then, the user provides a follow-up by evaluating the answer. The task is challenging because there are many utterances that affect the flow of the dialogue during a consultation. The utte</context>
</contexts>
<marker>Thomson, Schatzmann, Young, 2008</marker>
<rawString>Blaise Thomson, Jost Schatzmann, and Steve Young. 2008. Bayesian update of dialogue state for robust dialogue systems. In Proceedings of ICASSP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Rebecca Passonneau</author>
<author>Julie E Boland</author>
</authors>
<title>Quantitative and Qualitative Evaluation of DARPA Communicator Spoken Dialogue Systems.</title>
<date>2001</date>
<booktitle>In Proc. of 39th Annual Meeting of the ACL,</booktitle>
<pages>515--522</pages>
<contexts>
<context position="1700" citStr="Walker et al., 2001" startWordPosition="253" endWordPosition="256">phological analysis results, dependency analysis results, and semantic content tags. In this paper, we describe the current status of a dialogue corpus that is being developed by our research group, focusing on two types of tags: speech act tags and semantic content tags. These speech act and semantic content tags were designed to express the dialogue act of each utterance. Many studies have focused on developing spoken dialogue systems. Their typical task domains included the retrieval of information from databases or making reservations, such as airline information e.g., DARPA Communicator (Walker et al., 2001) and train information e.g., ARISE (Bouwman et al., 1999) and MASK (Lamel et al., 2002). Most studies assumed a definite and consistent user objective, and the dialogue strategy was usually designed to minimize the cost of information access. Other target tasks include tutoring and trouble-shooting dialogues (Boye, 2007). In such tasks, dialogue scenarios or agendas are usually described using a (dynamic) tree structure, and the objective is to satisfy all requirements. In this paper, we introduce our corpus, which is being developed as part of a project to construct consulting dialogue system</context>
</contexts>
<marker>Walker, Passonneau, Boland, 2001</marker>
<rawString>Marilyn A. Walker, Rebecca Passonneau, and Julie E. Boland. 2001. Quantitative and Qualitative Evaluation of DARPA Communicator Spoken Dialogue Systems. In Proc. of 39th Annual Meeting of the ACL, pages 515–522.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>