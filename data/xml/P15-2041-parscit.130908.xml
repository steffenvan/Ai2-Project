<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013041">
<title confidence="0.995048">
CCG Supertagging with a Recurrent Neural Network
</title>
<author confidence="0.998533">
Wenduan Xu Michael Auli* Stephen Clark
</author>
<affiliation confidence="0.9863625">
University of Cambridge Facebook AI Research University of Cambridge
Computer Laboratory michaelauli@fb.com Computer Laboratory
</affiliation>
<email confidence="0.987002">
wx217@cam.ac.uk sc609@cam.ac.uk
</email>
<sectionHeader confidence="0.993576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998695">
Recent work on supertagging using a feed-
forward neural network achieved signifi-
cant improvements for CCG supertagging
and parsing (Lewis and Steedman, 2014).
However, their architecture is limited to
considering local contexts and does not
naturally model sequences of arbitrary
length. In this paper, we show how di-
rectly capturing sequence information us-
ing a recurrent neural network leads to fur-
ther accuracy improvements for both su-
pertagging (up to 1.9%) and parsing (up
to 1% F1), on CCGBank, Wikipedia and
biomedical text.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998078375">
Combinatory Categorial Grammar (CCG; Steed-
man, 2000) is a highly lexicalized formalism;
the standard parsing model of Clark and Curran
(2007) uses over 400 lexical categories (or su-
pertags), compared to about 50 POS tags for typ-
ical CFG parsers. This makes accurate disam-
biguation of lexical types much more challenging.
However, the assignment of lexical categories can
still be solved reasonably well by treating it as
a sequence tagging problem, often referred to as
supertagging (Bangalore and Joshi, 1999). Clark
and Curran (2004) show that high tagging accu-
racy can be achieved by leaving some ambiguity to
the parser to resolve, but with enough of a reduc-
tion in the number of tags assigned to each word
so that parsing efficiency is greatly increased.
In addition to improving parsing efficiency, su-
pertagging also has a large impact on parsing ac-
curacy (Curran et al., 2006; Kummerfeld et al.,
2010), since the derivation space of the parser
is determined by the supertagger, at both train-
*All work was completed before the author joined Face-
book.
ing and test time. Clark and Curran (2007) en-
hanced supertagging using a so-called adaptive
strategy, such that additional categories are sup-
plied to the parser only if a spanning analysis can-
not be found. This strategy is used in the de
facto C&amp;C parser (Curran et al., 2007), and the
two-stage CCG parsing pipeline (supertagging and
parsing) continues to be the choice for most re-
cent CCG parsers (Zhang and Clark, 2011; Auli
and Lopez, 2011; Xu et al., 2014).
Despite the effectiveness of supertagging, the
most widely used model for this task (Clark and
Curran, 2007) has a number of drawbacks. First,
it relies too heavily on POS tags, which leads
to lower accuracy on out-of-domain data (Rimell
and Clark, 2008). Second, due to the sparse, in-
dicator feature sets mainly based on raw words
and POS tags, it shows pronounced performance
degradation in the presence of rare and unseen
words (Rimell and Clark, 2008; Lewis and Steed-
man, 2014). And third, in order to reduce com-
putational requirements and feature sparsity, each
tagging decision is made without considering any
potentially useful contextual information beyond a
local context window.
Lewis and Steedman (2014) introduced a feed-
forward neural network to supertagging, and ad-
dressed the first two problems mentioned above.
However, their attempt to tackle the third prob-
lem by pairing a conditional random field with
their feed-forward tagger provided little accuracy
improvement and vastly increased computational
complexity, incurring a large efficiency penalty.
We introduce a recurrent neural network-based
(RNN) supertagging model to tackle all the above
problems, with an emphasis on the third one.
RNNs are powerful models for sequential data,
which can potentially capture long-term depen-
dencies, based on an unbounded history of pre-
vious words (§2); similar to Lewis and Steedman
(2014) we only use distributed word representa-
</bodyText>
<page confidence="0.838426">
250
</page>
<bodyText confidence="0.83045825">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 250–255,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
tions (§2.2). Our model is highly accurate, and by
integrating it with the C&amp;C parser as its adaptive
supertagger, we obtain substantial accuracy im-
provements, outperforming the feed-forward setup
on both supertagging and parsing.
</bodyText>
<subsectionHeader confidence="0.849719">
2 Supertagging with a RNN
2.1 Model
</subsectionHeader>
<bodyText confidence="0.999982333333334">
We use an Elman recurrent neural network (El-
man, 1990) which consists of an input layer xt,
a hidden state (layer) ht with a recurrent connec-
tion to the previous hidden state ht−1 and an out-
put layer yt. The input layer is a vector represent-
ing the surrounding context of the current word
at position t, whose supertag is being predicted.1
The hidden state ht−1 keeps a representation of all
context history up to the current word. The cur-
rent hidden state ht is computed using the current
input xt and hidden state ht−1 from the previous
position. The output layer represents probability
scores of all possible supertags, with the size of
the output layer being equal to the size of the lexi-
cal category set.
The parameterization of the network consists
of three matrices which are learned during super-
vised training. Matrix U contains weights be-
tween the input and hidden layers, V contains
weights between the hidden and output layers, and
W contains weights between the previous hidden
state and the current hidden state. The following
recurrence2 is used to compute the activations of
the hidden state at word position t:
</bodyText>
<equation confidence="0.997847">
ht = f(xtU + ht−1W), (1)
</equation>
<bodyText confidence="0.9899125">
where f is a non-linear activation function; here
we use the sigmoid function f(z) = 1
</bodyText>
<equation confidence="0.739352666666667">
1+e−z . The
output activations are calculated as:
yt = g(htV), (2)
</equation>
<bodyText confidence="0.9476105">
where g is the softmax activation function g(zi) =
�ezi
j ezj that squeezes raw output activations into a
probability distribution.
</bodyText>
<subsectionHeader confidence="0.999268">
2.2 Word Embeddings
</subsectionHeader>
<bodyText confidence="0.962811">
Our RNN supertagger only uses continuous vec-
tor representations for features and each feature
</bodyText>
<footnote confidence="0.98666625">
1This is different from some RNN models (e.g., Mikolov
et al. (2010)) where the input is a one-hot vector.
2We assume the input to any layer is a row vector unless
otherwise stated.
</footnote>
<bodyText confidence="0.998733083333333">
type has an associated look-up table, which maps
a feature to its distributed representation. In to-
tal, three feature types are used. The first type is
word embeddings: given a sentence of N words,
(w1, w2, ... , wN), the embedding feature of wt
(for 1 &lt; t &lt; N) is obtained by projecting it onto
a n-dimensional vector space through the look-up
table Lw E R|w|xn, where |w |is the size of the vo-
cabulary. Algebraically, the projection operation
is a simple vector-matrix product where a one-hot
vector bj E R1x|w |(with zeros everywhere except
at the jth position) is multiplied with Lw:
</bodyText>
<equation confidence="0.519825">
ewt = bjLw E R1xn, (3)
</equation>
<bodyText confidence="0.9996886875">
where j is the look-up index for wt.
In addition, as in Lewis and Steedman (2014),
for every word we also include its 2-character suf-
fix and capitalization as features. Two more look-
up tables are used for these features. Ls E R|s|xm
is the look-up table for suffix embeddings, where
|s |is the suffix vocabulary size. Lc E R2xm
is the look-up table for the capitalization embed-
dings. Lc contains only two embeddings, repre-
senting whether or not a given word is capitalized.
We extract features from a context window sur-
rounding the current word to make a tagging de-
cision. Concretely, with a context window of size
k, Lk/2] words either side of the target word are
included. For a word wt, its continuous feature
representation is:
</bodyText>
<equation confidence="0.907884">
fwt = [ewt; swt; cwt], (4)
</equation>
<bodyText confidence="0.999974666666667">
where ewt E R1xn, swt E R1xm and cwt E
R1xm are the output vectors from the three differ-
ent look-up tables, and [ewt; swt; cwt] denotes the
concatenation of three vectors and hence fwt E
R1x(n+2m). At word position t, the input layer of
the network xt is:
</bodyText>
<equation confidence="0.998189">
xt = [fwt−Lk/2J; ... fwt; ...; fwt+Lk/2J], (5)
</equation>
<bodyText confidence="0.9999703">
where xt E R1xk(n+2m) and the right-hand side is
the concatenation of all feature representations in
a size k context window.
We use pre-trained word embeddings
from Turian et al. (2010) to initialize look-
up table Lw, and we apply a set of word
pre-processing techniques at both training and
test time to reduce sparsity. All words are first
lower-cased, and all numbers are collapsed into
a single digit ‘0’. If a lower-cased hyphenated
</bodyText>
<page confidence="0.984038">
251
</page>
<bodyText confidence="0.9999210625">
word does not have an entry in the pre-trained
word embeddings, we attempt to back-off to the
substring after the last hyphen. For compound
words and numbers delimited by “\/”, we attempt
to back-off to the substring after the delimiter.
After pre-processing, the Turian embeddings have
a coverage of 94.25% on the training data; for
out-of-vocabulary words, three separate randomly
initialized embeddings are used for lower-case
alphanumeric words, upper-case alphanumeric
words, and non-alphanumeric symbols. For
padding at the start and end of a sentence, the “un-
known” entry from the pre-trained embeddings is
used. Look-up tables L3 and L, are also randomly
initialized, and all look-up tables are modified
during supervised training using backpropagation.
</bodyText>
<sectionHeader confidence="0.999556" genericHeader="introduction">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99945706060606">
Datasets and Baseline. We follow the standard
splits of CCGBank (Hockenmaier and Steedman,
2007) for all experiments using sections 2-21 for
training, section 00 for development and section
23 as in-domain test set. The Wikipedia corpus
from Honnibal et al. (2009) and the Bioinfer cor-
pus (Pyysalo et al., 2007) are used as two out-
of-domain test sets. We compare supertagging
accuracy with the MaxEnt C&amp;C supertagger and
the neural network tagger of Lewis and Steed-
man (2014) (henceforth NN), and we also evaluate
parsing accuracy using these three supertaggers as
a front-end to the C&amp;C parser. We use the same
425 supertag set used in both C&amp;C and NN.
Hyperparameters and Training. For L,,,, we
use the scaled 50-dimensional Turian embeddings
(n = 50 for L,,,) as initialization. We have ex-
perimented during development with using 100-
dimensional embeddings and found no improve-
ments in the resulting model. Out-of-vocabulary
embedding values in L,,, and all embedding values
in L3 and L, are initialized with a uniform distri-
bution in the interval [−2.0,2.0]. The embedding
dimension size m of L3 and L, is set to 5. Other
parameters of the network {U, V, W} are initial-
ized with values drawn uniformly from the inter-
val [−2.0,2.0], and are then scaled by their corre-
sponding input vector size. We experimented with
context window sizes of 3, 5, 7, 9 and 11 during
development and found a window size of 7 gives
the best performing model on the dev set. We use
a fixed learning rate of 0.0025 and a hidden state
size of 200.
</bodyText>
<table confidence="0.999442833333333">
Model Accuracy Time
C&amp;C (gold POS) 92.60 -
C&amp;C (auto POS) 91.50 0.57
NN 91.10 21.00
RNN 92.63 -
RNN+dropout 93.07 2.02
</table>
<tableCaption confidence="0.882291">
Table 1: 1-best tagging accuracy and speed com-
parison on CCGBank Section 00 with a single
CPU core (1,913 sentences), tagging time in secs.
</tableCaption>
<bodyText confidence="0.986114066666666">
To train the model, we optimize cross-entropy
loss with stochastic gradient descent using mini-
batched backpropagation through time (BPTT;
Rumelhart et al., 1988; Mikolov, 2012); the mini-
batch size for BPTT, again tuned on the dev set, is
set to 9.
Embedding Dropout Regularization. Without
any regularization, we found cross-entropy error
on the dev set started to increase while the error on
the training set was continuously driven to a very
small value (Fig. 1a). With the suspicion of over-
fitting, we experimented with l1 and l2 regulariza-
tion and learning rate decay but none of these tech-
niques gave any noticeable improvements for our
model. Following Legrand and Collobert (2014),
we instead implemented word embedding dropout
as a regularization for all the look-up tables, since
the capacity of our tagging model mainly comes
from the look-up tables, as in their system. We
observed more stable learning and better general-
ization of the trained model with dropout. Similar
to other forms of droput (Srivastava et al., 2014),
we randomly drop units and their connections to
other units at training time. Concretely, we apply
a binary dropout mask to xt, with a dropout rate
of 0.25, and at test time no mask is applied, but
the input to the network, xt, at each word position
is scaled by 0.75. We experimented during devel-
opment with different dropout rates, but found the
above choice to be optimal in our setting.
</bodyText>
<subsectionHeader confidence="0.999751">
3.1 Supertagging Results
</subsectionHeader>
<bodyText confidence="0.986767">
We use the RNN model which gives the high-
est 1-best supertagging accuracy on the dev set
as the final model for all experiments. Without
any form of regularization, the best model was ob-
tained at the 20th epoch, and it took 35 epochs for
the dropout model to peak (Fig. 1b). We use the
dropout model for all experiments and, unlike the
C&amp;C supertagger, no tag dictionaries are used.
Table 1 shows 1-best supertagging accuracies
on the dev set. The accuracy of the C&amp;C supertag-
</bodyText>
<page confidence="0.964451">
252
</page>
<figure confidence="0.999666078431373">
cross-entropy error
1-best accuracy
multi-tagging accuracy
0 5 10 15 20 25 30 35 40 45 50
epochs
(a) learning curve on dev set
0 5 10 15 20 25 30 35 40 45 50
epochs
(b) 1-best accuracy on dev set
0 2 4 6 8 10 12 14 16
ambiguity level
(c) multi-tagging accuracy on dev set
16
15
14
13
12
11
10
9
8
7
6
5
RNN + dropout
RNN
0.94
0.93
0.92
0.91
0.9
0.89
0.88
0.87
0.86
0.85
RNN + dropout
RNN
0.995
0.985
0.975
0.965
0.99
0.98
0.97
0.96
1
RNN + dropout
RNN
NN
C&amp;C
</figure>
<figureCaption confidence="0.99164575">
Figure 1: Learning curve and 1-best tagging accuracy of the RNN model on CCGBank Section 00. Plot
(c) shows ambiguity vs. multi-tagging accuracy for all supertaggers (auto POS).
Figure 2: Multi-tagging accuracy for all supertagging models on CCGBank Section 23, Wikipedia and
Bio-GENIA data (auto POS).
</figureCaption>
<table confidence="0.99528">
RNN NN C&amp;C (auto pos) C&amp;C (gold pos)
β WORD SENT amb. WORD SENT amb. WORD SENT amb. WORD SENT amb.
0.075 97.33 66.07 1.27 96.83 61.27 1.34 96.34 60.27 1.27 97.34 67.43 1.27
0.030 98.12 74.39 1.46 97.81 70.83 1.58 97.05 65.50 1.43 97.92 72.87 1.43
0.010 98.71 81.70 1.84 98.54 79.25 2.06 97.63 70.52 1.72 98.37 77.73 1.72
0.005 99.01 84.79 2.22 98.84 83.38 2.55 97.86 72.24 1.98 98.52 79.25 1.98
0.001 99.41 90.54 3.90 99.29 89.07 4.72 98.25 80.24 3.57 99.17 87.19 3.00
</table>
<tableCaption confidence="0.99407">
Table 2: Multi-tagging accuracy and ambiguity comparison (supertags/word) at the default C&amp;C β levels
on CCGBank Section 00.
</tableCaption>
<figure confidence="0.992035545454546">
0 10 20 30 40 50 60 70 80 90
ambiguity level
(a) CCBBank Section 23
0 20 40 60 80 100 120 140 160 180
ambiguity level
(b) wikipedia
0 20 40 60 80 100 120 140
ambiguity level
(c) bio-GENIA
multi-tagging accuracy
0.99
0.98
0.97
0.96
0.95
0.94
1
RNN + dropout
NN
C&amp;C
multi-tagging accuracy
0.99
0.98
0.97
0.96
0.95
0.94
0.93
1
multi-tagging accuracy
0.995
0.985
0.975
0.965
0.99
0.98
0.97
1
RNN + dropout
NN
C&amp;C
RNN + dropout
NN
C&amp;C
</figure>
<table confidence="0.9416138">
Model Section 23 Wiki Bio
C&amp;C (gold POS) 93.32 88.80 91.85
C&amp;C (auto POS) 92.02 88.80 89.08
NN 91.57 89.00 88.16
RNN 93.00 90.00 88.27
</table>
<tableCaption confidence="0.99414">
Table 3: 1-best tagging accuracy compari-
</tableCaption>
<bodyText confidence="0.9988806">
son on CCGBank Section 23 (2,407 sentences),
Wikipedia (200 sentences) and Bio-GENIA (1,000
sentences).
ger drops about 1% with automatically assigned
POS tags, while our RNN model gives higher ac-
curacy (+0.47%) than the C&amp;C supertagger with
gold POS tags. All timing values are obtained on
a single Intel i7-4790k core, and all implementa-
tions are in C++ except NN which is implemented
using Torch and Java, and therefore we believe the
efficiency of NN could be vastly improved with an
implementation with a lower-level language.
Table 2 compares different supertagging mod-
els for multi-tagging accuracy at the default β
levels used by the C&amp;C parser on the dev set.
The β parameter determines the average number
of supertags assigned to each word (ambiguity)
by a supertagger when integrated with the parser;
categories whose probabilities are not within β
times the probability of the 1-best category are
pruned. At the first β level (0.075), the three su-
pertagging models give very close ambiguity lev-
els, but our RNN model clearly outperforms NN
and C&amp;C (auto POS) in both word (WORD) and
sentence (SENT) level accuracies, giving similar
word-level accuracy as C&amp;C (gold POS). For other
β levels (except β = 0.001), the RNN model gives
comparable ambiguity levels to the C&amp;C model
which uses a tagdict, while being much more ac-
curate than both the other two models.
</bodyText>
<page confidence="0.996463">
253
</page>
<table confidence="0.999623727272727">
LP LR LF SENT CAT cov.
C&amp;C (normal) 85.18 82.53 83.83 31.42 92.39 100
C&amp;C (hybrid) 86.07 82.77 84.39 32.62 92.57 100
C&amp;C (normal + RNN) 86.74 84.58 85.65 34.13 93.60 100
C&amp;C (hybrid + RNN) 87.73 84.83 86.25 34.97 93.84 100
C&amp;C (normal) 85.18 84.32 84.75 31.73 92.83 99.01 (C&amp;C cov)
C&amp;C (hybrid) 86.07 84.49 85.28 32.93 93.02 99.06 (C&amp;C cov)
C&amp;C (normal + RNN) 86.81 86.01 86.41 34.37 93.80 99.01 (C&amp;C cov)
C&amp;C (hybrid + RNN) 87.77 86.25 87.00 35.20 94.04 99.06 (C&amp;C cov)
C&amp;C (normal + RNN) 86.74 86.15 86.45 34.33 93.81 99.42
C&amp;C (hybrid + RNN) 87.73 86.41 87.06 35.17 94.05 99.42
</table>
<tableCaption confidence="0.998389">
Table 4: Parsing development results on CCGBank Section 00 (auto POS).
</tableCaption>
<table confidence="0.999787875">
CCGBank Section 23 LP Wikipedia cov. LP Bioinfer cov.
LP LR LF cov. LR LF LR LF
C&amp;C 86.24 84.85 85.54 99.42 81.58 80.08 80.83 99.50 77.78 76.07 76.91 95.40
C&amp;C (+ NN) 86.71 85.56 86.13 99.92 82.65 81.36 82.00 100 79.77 78.62 79.19 97.40
C&amp;C (+ RNN) 87.68 86.47 87.07 99.96 83.22 81.78 82.49 100 80.10 78.21 79.14 97.80
C&amp;C 86.24 84.17 85.19 100 81.58 79.48 80.52 100 77.78 71.44 74.47 100
C&amp;C (+ NN) 86.71 85.40 86.05 100 - - - - 79.77 75.35 77.50 100
C&amp;C (+ RNN) 87.68 86.41 87.04 100 - - - - 80.10 75.52 77.74 100
</table>
<tableCaption confidence="0.998784">
Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%
</tableCaption>
<bodyText confidence="0.998207357142857">
coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN
both have 100% coverage on the Wikipedia data.
Fig. 1c compares multi-tagging accuracies of all
the models on the dev set. For all models, the same
Q levels are used (ranging from 0.075 to 10−4,
and all C&amp;C default values are included). The
RNN model consistently outperforms other mod-
els across different ambiguity levels.
Table 3 shows 1-best accuracies of all models
on the test data sets (Bio-GENIA gold-standard
CCG lexical category data from Rimell and Clark
(2008) are used, since no gold categories are avail-
able in the Bioinfer data). With gold-standard POS
tags, the C&amp;C model outperforms both the NN and
RNN models on CCGBank and Bio-GENIA; with
auto POS, the accuracy of the C&amp;C model drops
significantly, due to its high reliance on POS tags.
Fig. 2 shows multi-tagging accuracies on all
test data (using Q levels ranging from 0.075 to
10−6, and all C&amp;C default values are included).
On CCGBank, the RNN model has a clear accu-
racy advantage, while on the other two data sets,
the accuracies given by the NN model are closer
to the RNN model at some ambiguity levels, rep-
resenting these data sets are still more challenging
than CCGBank. However, both the NN and RNN
models are more robust than the C&amp;C model on the
two out-of-domain data sets.
</bodyText>
<subsectionHeader confidence="0.999697">
3.2 Parsing Results
</subsectionHeader>
<bodyText confidence="0.999989388888889">
We integrate our supertagging model into the C&amp;C
parser, at both training and test time, using all de-
fault parser settings; C&amp;C hybrid model is used for
CCGBank and Wikipedia; the normal-form model
is used for the Bioinfer data, in line with Lewis and
Steedman (2014) and Rimell and Clark (2008).
Parsing development results are shown in Table 4;
for out-of-domain data sets, no separate develop-
ment experiments were done. Final results are
shown in Table 5, and we substantially improve
parsing accuracies on CCGBank and Wikipedia.
The accuracy of our model on CCGBank repre-
sents a F1 score improvement of 1.53%/1.85%
over the C&amp;C baseline, which is comparable to
the best known accuracy reported in Auli and
Lopez (2011). However, our RNN-supertagging-
based model is conceptually much simpler, with
no change to the parsing model required at all.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999744">
We presented a RNN-based model for CCG su-
pertagging, which brings significant accuracy im-
provements for supertagging and parsing, on both
in- and out-of-domain data sets. Our supertagger
is fast and well-suited for large scale processing.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999844">
The first author acknowledges the Carnegie Trust
for the Universities of Scotland and the Cambridge
Trusts for providing funding. SC is supported
by ERC Starting Grant DisCoTex (306920) and
EPSRC grant EP/I037512/1. We also thank the
anonymous reviewers for their helpful comments.
</bodyText>
<page confidence="0.997234">
254
</page>
<sectionHeader confidence="0.989795" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99979936">
Michael Auli and Adam Lopez. 2011. Training a
log-linear parser with loss functions via softmax-
margin. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 333–343. Association for Computational Lin-
guistics.
Srinivas Bangalore and Aravind K Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational linguistics, 25(2):237–265.
Stephen Clark and James R Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of the 20th international confer-
ence on Computational Linguistics, page 282. Asso-
ciation for Computational Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
James R Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 697–704. Association for
Computational Linguistics.
James R Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with C&amp;C
and Boxer. In Proceedings of the 45th Annual Meet-
ing of the ACL on Interactive Poster and Demonstra-
tion Sessions, pages 33–36. Association for Compu-
tational Linguistics.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Matthew Honnibal, Joel Nothman, and James R Cur-
ran. 2009. Evaluating a statistical CCG parser on
wikipedia. In Proceedings of the 2009 Workshop on
The People’s Web Meets NLP: Collaboratively Con-
structed Semantic Resources, pages 38–41. Associ-
ation for Computational Linguistics.
Jonathan K Kummerfeld, Jessika Roesner, Tim Daw-
born, James Haggerty, James R Curran, and Stephen
Clark. 2010. Faster parsing by supertagger adap-
tation. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 345–355. Association for Computational
Linguistics.
Jo¨el Legrand and Ronan Collobert. 2014. Joint RNN-
based greedy parsing and word composition. arXiv
preprint arXiv:1412.7028.
Mike Lewis and Mark Steedman. 2014. Improved
CCG parsing with semi-supervised supertagging.
Transactions of the Association for Computational
Linguistics, 2:327–338.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.
Tom´aˇs Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio
Salakoski. 2007. Bioinfer: a corpus for information
extraction in the biomedical domain. BMC bioinfor-
matics, 8(1):50.
Laura Rimell and Stephen Clark. 2008. Adapt-
ing a lexicalized-grammar parser to contrasting do-
mains. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 475–484. Association for Computational Lin-
guistics.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1988. Learning representations by back-
propagating errors. Cognitive modeling, 5.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, Mass.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
Wenduan Xu, Stephen Clark, and Yue Zhang. 2014.
Shift-reduce CCG parsing with a dependency
model. In Proceedings of the 2014 ACL Conference.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proc. ACL 2011, pages 683–692,
Portland, OR.
</reference>
<page confidence="0.998407">
255
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.585382">
<title confidence="0.999133">CCG Supertagging with a Recurrent Neural Network</title>
<author confidence="0.999362">Wenduan Xu Michael Auli Stephen Clark</author>
<affiliation confidence="0.9998135">University of Cambridge Facebook AI Research University of Cambridge Laboratory Laboratory</affiliation>
<email confidence="0.617289">wx217@cam.ac.uksc609@cam.ac.uk</email>
<abstract confidence="0.9961056">Recent work on supertagging using a feedforward neural network achieved signifiimprovements for and parsing (Lewis and Steedman, 2014). However, their architecture is limited to considering local contexts and does not naturally model sequences of arbitrary length. In this paper, we show how directly capturing sequence information using a recurrent neural network leads to further accuracy improvements for both supertagging (up to 1.9%) and parsing (up to 1% F1), on CCGBank, Wikipedia and biomedical text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>Training a log-linear parser with loss functions via softmaxmargin.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>333--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2303" citStr="Auli and Lopez, 2011" startWordPosition="363" endWordPosition="366">06; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. ing and test time. Clark and Curran (2007) enhanced supertagging using a so-called adaptive strategy, such that additional categories are supplied to the parser only if a spanning analysis cannot be found. This strategy is used in the de facto C&amp;C parser (Curran et al., 2007), and the two-stage CCG parsing pipeline (supertagging and parsing) continues to be the choice for most recent CCG parsers (Zhang and Clark, 2011; Auli and Lopez, 2011; Xu et al., 2014). Despite the effectiveness of supertagging, the most widely used model for this task (Clark and Curran, 2007) has a number of drawbacks. First, it relies too heavily on POS tags, which leads to lower accuracy on out-of-domain data (Rimell and Clark, 2008). Second, due to the sparse, indicator feature sets mainly based on raw words and POS tags, it shows pronounced performance degradation in the presence of rare and unseen words (Rimell and Clark, 2008; Lewis and Steedman, 2014). And third, in order to reduce computational requirements and feature sparsity, each tagging decis</context>
<context position="19352" citStr="Auli and Lopez (2011)" startWordPosition="3291" endWordPosition="3294">fault parser settings; C&amp;C hybrid model is used for CCGBank and Wikipedia; the normal-form model is used for the Bioinfer data, in line with Lewis and Steedman (2014) and Rimell and Clark (2008). Parsing development results are shown in Table 4; for out-of-domain data sets, no separate development experiments were done. Final results are shown in Table 5, and we substantially improve parsing accuracies on CCGBank and Wikipedia. The accuracy of our model on CCGBank represents a F1 score improvement of 1.53%/1.85% over the C&amp;C baseline, which is comparable to the best known accuracy reported in Auli and Lopez (2011). However, our RNN-supertaggingbased model is conceptually much simpler, with no change to the parsing model required at all. 4 Conclusion We presented a RNN-based model for CCG supertagging, which brings significant accuracy improvements for supertagging and parsing, on both in- and out-of-domain data sets. Our supertagger is fast and well-suited for large scale processing. Acknowledgements The first author acknowledges the Carnegie Trust for the Universities of Scotland and the Cambridge Trusts for providing funding. SC is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/</context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011. Training a log-linear parser with loss functions via softmaxmargin. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 333–343. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational linguistics,</journal>
<pages>25--2</pages>
<contexts>
<context position="1313" citStr="Bangalore and Joshi, 1999" startWordPosition="191" endWordPosition="194">both supertagging (up to 1.9%) and parsing (up to 1% F1), on CCGBank, Wikipedia and biomedical text. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a highly lexicalized formalism; the standard parsing model of Clark and Curran (2007) uses over 400 lexical categories (or supertags), compared to about 50 POS tags for typical CFG parsers. This makes accurate disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. ing and test time. Clark and Curran (2007) enhanced s</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K Joshi. 1999. Supertagging: An approach to almost parsing. Computational linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>282</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1338" citStr="Clark and Curran (2004)" startWordPosition="195" endWordPosition="198">%) and parsing (up to 1% F1), on CCGBank, Wikipedia and biomedical text. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a highly lexicalized formalism; the standard parsing model of Clark and Curran (2007) uses over 400 lexical categories (or supertags), compared to about 50 POS tags for typical CFG parsers. This makes accurate disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. ing and test time. Clark and Curran (2007) enhanced supertagging using a so-ca</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of the 20th international conference on Computational Linguistics, page 282. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="944" citStr="Clark and Curran (2007)" startWordPosition="132" endWordPosition="135">chieved significant improvements for CCG supertagging and parsing (Lewis and Steedman, 2014). However, their architecture is limited to considering local contexts and does not naturally model sequences of arbitrary length. In this paper, we show how directly capturing sequence information using a recurrent neural network leads to further accuracy improvements for both supertagging (up to 1.9%) and parsing (up to 1% F1), on CCGBank, Wikipedia and biomedical text. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a highly lexicalized formalism; the standard parsing model of Clark and Curran (2007) uses over 400 lexical categories (or supertags), compared to about 50 POS tags for typical CFG parsers. This makes accurate disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is </context>
<context position="2431" citStr="Clark and Curran, 2007" startWordPosition="384" endWordPosition="387">rk was completed before the author joined Facebook. ing and test time. Clark and Curran (2007) enhanced supertagging using a so-called adaptive strategy, such that additional categories are supplied to the parser only if a spanning analysis cannot be found. This strategy is used in the de facto C&amp;C parser (Curran et al., 2007), and the two-stage CCG parsing pipeline (supertagging and parsing) continues to be the choice for most recent CCG parsers (Zhang and Clark, 2011; Auli and Lopez, 2011; Xu et al., 2014). Despite the effectiveness of supertagging, the most widely used model for this task (Clark and Curran, 2007) has a number of drawbacks. First, it relies too heavily on POS tags, which leads to lower accuracy on out-of-domain data (Rimell and Clark, 2008). Second, due to the sparse, indicator feature sets mainly based on raw words and POS tags, it shows pronounced performance degradation in the presence of rare and unseen words (Rimell and Clark, 2008; Lewis and Steedman, 2014). And third, in order to reduce computational requirements and feature sparsity, each tagging decision is made without considering any potentially useful contextual information beyond a local context window. Lewis and Steedman </context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>David Vadas</author>
</authors>
<title>Multi-tagging for lexicalized-grammar parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>697--704</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1685" citStr="Curran et al., 2006" startWordPosition="257" endWordPosition="260">te disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. ing and test time. Clark and Curran (2007) enhanced supertagging using a so-called adaptive strategy, such that additional categories are supplied to the parser only if a spanning analysis cannot be found. This strategy is used in the de facto C&amp;C parser (Curran et al., 2007), and the two-stage CCG parsing pipeline (supertagging and parsing) continues to be the choice for most recent CCG parsers (Zhang and Clark, 2011; Au</context>
</contexts>
<marker>Curran, Clark, Vadas, 2006</marker>
<rawString>James R Curran, Stephen Clark, and David Vadas. 2006. Multi-tagging for lexicalized-grammar parsing. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 697–704. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically motivated large-scale NLP with C&amp;C and Boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>33--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2136" citStr="Curran et al., 2007" startWordPosition="335" endWordPosition="338"> that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. ing and test time. Clark and Curran (2007) enhanced supertagging using a so-called adaptive strategy, such that additional categories are supplied to the parser only if a spanning analysis cannot be found. This strategy is used in the de facto C&amp;C parser (Curran et al., 2007), and the two-stage CCG parsing pipeline (supertagging and parsing) continues to be the choice for most recent CCG parsers (Zhang and Clark, 2011; Auli and Lopez, 2011; Xu et al., 2014). Despite the effectiveness of supertagging, the most widely used model for this task (Clark and Curran, 2007) has a number of drawbacks. First, it relies too heavily on POS tags, which leads to lower accuracy on out-of-domain data (Rimell and Clark, 2008). Second, due to the sparse, indicator feature sets mainly based on raw words and POS tags, it shows pronounced performance degradation in the presence of rare</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James R Curran, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale NLP with C&amp;C and Boxer. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 33–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="4362" citStr="Elman, 1990" startWordPosition="678" endWordPosition="680"> representa250 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 250–255, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tions (§2.2). Our model is highly accurate, and by integrating it with the C&amp;C parser as its adaptive supertagger, we obtain substantial accuracy improvements, outperforming the feed-forward setup on both supertagging and parsing. 2 Supertagging with a RNN 2.1 Model We use an Elman recurrent neural network (Elman, 1990) which consists of an input layer xt, a hidden state (layer) ht with a recurrent connection to the previous hidden state ht−1 and an output layer yt. The input layer is a vector representing the surrounding context of the current word at position t, whose supertag is being predicted.1 The hidden state ht−1 keeps a representation of all context history up to the current word. The current hidden state ht is computed using the current input xt and hidden state ht−1 from the previous position. The output layer represents probability scores of all possible supertags, with the size of the output lay</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="9029" citStr="Hockenmaier and Steedman, 2007" startWordPosition="1473" endWordPosition="1476">er pre-processing, the Turian embeddings have a coverage of 94.25% on the training data; for out-of-vocabulary words, three separate randomly initialized embeddings are used for lower-case alphanumeric words, upper-case alphanumeric words, and non-alphanumeric symbols. For padding at the start and end of a sentence, the “unknown” entry from the pre-trained embeddings is used. Look-up tables L3 and L, are also randomly initialized, and all look-up tables are modified during supervised training using backpropagation. 3 Experiments Datasets and Baseline. We follow the standard splits of CCGBank (Hockenmaier and Steedman, 2007) for all experiments using sections 2-21 for training, section 00 for development and section 23 as in-domain test set. The Wikipedia corpus from Honnibal et al. (2009) and the Bioinfer corpus (Pyysalo et al., 2007) are used as two outof-domain test sets. We compare supertagging accuracy with the MaxEnt C&amp;C supertagger and the neural network tagger of Lewis and Steedman (2014) (henceforth NN), and we also evaluate parsing accuracy using these three supertaggers as a front-end to the C&amp;C parser. We use the same 425 supertag set used in both C&amp;C and NN. Hyperparameters and Training. For L,,,, we</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
<author>Joel Nothman</author>
<author>James R Curran</author>
</authors>
<title>Evaluating a statistical CCG parser on wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources,</booktitle>
<pages>38--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9197" citStr="Honnibal et al. (2009)" startWordPosition="1500" endWordPosition="1503"> lower-case alphanumeric words, upper-case alphanumeric words, and non-alphanumeric symbols. For padding at the start and end of a sentence, the “unknown” entry from the pre-trained embeddings is used. Look-up tables L3 and L, are also randomly initialized, and all look-up tables are modified during supervised training using backpropagation. 3 Experiments Datasets and Baseline. We follow the standard splits of CCGBank (Hockenmaier and Steedman, 2007) for all experiments using sections 2-21 for training, section 00 for development and section 23 as in-domain test set. The Wikipedia corpus from Honnibal et al. (2009) and the Bioinfer corpus (Pyysalo et al., 2007) are used as two outof-domain test sets. We compare supertagging accuracy with the MaxEnt C&amp;C supertagger and the neural network tagger of Lewis and Steedman (2014) (henceforth NN), and we also evaluate parsing accuracy using these three supertaggers as a front-end to the C&amp;C parser. We use the same 425 supertag set used in both C&amp;C and NN. Hyperparameters and Training. For L,,,, we use the scaled 50-dimensional Turian embeddings (n = 50 for L,,,) as initialization. We have experimented during development with using 100- dimensional embeddings and</context>
</contexts>
<marker>Honnibal, Nothman, Curran, 2009</marker>
<rawString>Matthew Honnibal, Joel Nothman, and James R Curran. 2009. Evaluating a statistical CCG parser on wikipedia. In Proceedings of the 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources, pages 38–41. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>Jessika Roesner</author>
<author>Tim Dawborn</author>
<author>James Haggerty</author>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Faster parsing by supertagger adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>345--355</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1711" citStr="Kummerfeld et al., 2010" startWordPosition="261" endWordPosition="264">lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. ing and test time. Clark and Curran (2007) enhanced supertagging using a so-called adaptive strategy, such that additional categories are supplied to the parser only if a spanning analysis cannot be found. This strategy is used in the de facto C&amp;C parser (Curran et al., 2007), and the two-stage CCG parsing pipeline (supertagging and parsing) continues to be the choice for most recent CCG parsers (Zhang and Clark, 2011; Auli and Lopez, 2011; Xu et </context>
</contexts>
<marker>Kummerfeld, Roesner, Dawborn, Haggerty, Curran, Clark, 2010</marker>
<rawString>Jonathan K Kummerfeld, Jessika Roesner, Tim Dawborn, James Haggerty, James R Curran, and Stephen Clark. 2010. Faster parsing by supertagger adaptation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345–355. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jo¨el Legrand</author>
<author>Ronan Collobert</author>
</authors>
<title>Joint RNNbased greedy parsing and word composition. arXiv preprint arXiv:1412.7028.</title>
<date>2014</date>
<marker>Jo¨el Legrand, Collobert, 2014</marker>
<rawString>Jo¨el Legrand and Ronan Collobert. 2014. Joint RNNbased greedy parsing and word composition. arXiv preprint arXiv:1412.7028.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Improved CCG parsing with semi-supervised supertagging.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--327</pages>
<contexts>
<context position="2804" citStr="Lewis and Steedman, 2014" startWordPosition="447" endWordPosition="451">upertagging and parsing) continues to be the choice for most recent CCG parsers (Zhang and Clark, 2011; Auli and Lopez, 2011; Xu et al., 2014). Despite the effectiveness of supertagging, the most widely used model for this task (Clark and Curran, 2007) has a number of drawbacks. First, it relies too heavily on POS tags, which leads to lower accuracy on out-of-domain data (Rimell and Clark, 2008). Second, due to the sparse, indicator feature sets mainly based on raw words and POS tags, it shows pronounced performance degradation in the presence of rare and unseen words (Rimell and Clark, 2008; Lewis and Steedman, 2014). And third, in order to reduce computational requirements and feature sparsity, each tagging decision is made without considering any potentially useful contextual information beyond a local context window. Lewis and Steedman (2014) introduced a feedforward neural network to supertagging, and addressed the first two problems mentioned above. However, their attempt to tackle the third problem by pairing a conditional random field with their feed-forward tagger provided little accuracy improvement and vastly increased computational complexity, incurring a large efficiency penalty. We introduce </context>
<context position="6734" citStr="Lewis and Steedman (2014)" startWordPosition="1092" endWordPosition="1095">istributed representation. In total, three feature types are used. The first type is word embeddings: given a sentence of N words, (w1, w2, ... , wN), the embedding feature of wt (for 1 &lt; t &lt; N) is obtained by projecting it onto a n-dimensional vector space through the look-up table Lw E R|w|xn, where |w |is the size of the vocabulary. Algebraically, the projection operation is a simple vector-matrix product where a one-hot vector bj E R1x|w |(with zeros everywhere except at the jth position) is multiplied with Lw: ewt = bjLw E R1xn, (3) where j is the look-up index for wt. In addition, as in Lewis and Steedman (2014), for every word we also include its 2-character suffix and capitalization as features. Two more lookup tables are used for these features. Ls E R|s|xm is the look-up table for suffix embeddings, where |s |is the suffix vocabulary size. Lc E R2xm is the look-up table for the capitalization embeddings. Lc contains only two embeddings, representing whether or not a given word is capitalized. We extract features from a context window surrounding the current word to make a tagging decision. Concretely, with a context window of size k, Lk/2] words either side of the target word are included. For a </context>
<context position="9408" citStr="Lewis and Steedman (2014)" startWordPosition="1536" endWordPosition="1540">p tables L3 and L, are also randomly initialized, and all look-up tables are modified during supervised training using backpropagation. 3 Experiments Datasets and Baseline. We follow the standard splits of CCGBank (Hockenmaier and Steedman, 2007) for all experiments using sections 2-21 for training, section 00 for development and section 23 as in-domain test set. The Wikipedia corpus from Honnibal et al. (2009) and the Bioinfer corpus (Pyysalo et al., 2007) are used as two outof-domain test sets. We compare supertagging accuracy with the MaxEnt C&amp;C supertagger and the neural network tagger of Lewis and Steedman (2014) (henceforth NN), and we also evaluate parsing accuracy using these three supertaggers as a front-end to the C&amp;C parser. We use the same 425 supertag set used in both C&amp;C and NN. Hyperparameters and Training. For L,,,, we use the scaled 50-dimensional Turian embeddings (n = 50 for L,,,) as initialization. We have experimented during development with using 100- dimensional embeddings and found no improvements in the resulting model. Out-of-vocabulary embedding values in L,,, and all embedding values in L3 and L, are initialized with a uniform distribution in the interval [−2.0,2.0]. The embeddi</context>
<context position="18897" citStr="Lewis and Steedman (2014)" startWordPosition="3217" endWordPosition="3220">del has a clear accuracy advantage, while on the other two data sets, the accuracies given by the NN model are closer to the RNN model at some ambiguity levels, representing these data sets are still more challenging than CCGBank. However, both the NN and RNN models are more robust than the C&amp;C model on the two out-of-domain data sets. 3.2 Parsing Results We integrate our supertagging model into the C&amp;C parser, at both training and test time, using all default parser settings; C&amp;C hybrid model is used for CCGBank and Wikipedia; the normal-form model is used for the Bioinfer data, in line with Lewis and Steedman (2014) and Rimell and Clark (2008). Parsing development results are shown in Table 4; for out-of-domain data sets, no separate development experiments were done. Final results are shown in Table 5, and we substantially improve parsing accuracies on CCGBank and Wikipedia. The accuracy of our model on CCGBank represents a F1 score improvement of 1.53%/1.85% over the C&amp;C baseline, which is comparable to the best known accuracy reported in Auli and Lopez (2011). However, our RNN-supertaggingbased model is conceptually much simpler, with no change to the parsing model required at all. 4 Conclusion We pre</context>
</contexts>
<marker>Lewis, Steedman, 2014</marker>
<rawString>Mike Lewis and Mark Steedman. 2014. Improved CCG parsing with semi-supervised supertagging. Transactions of the Association for Computational Linguistics, 2:327–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan,</location>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
</authors>
<title>Statistical Language Models Based on Neural Networks.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="10897" citStr="Mikolov, 2012" startWordPosition="1795" endWordPosition="1796">7, 9 and 11 during development and found a window size of 7 gives the best performing model on the dev set. We use a fixed learning rate of 0.0025 and a hidden state size of 200. Model Accuracy Time C&amp;C (gold POS) 92.60 - C&amp;C (auto POS) 91.50 0.57 NN 91.10 21.00 RNN 92.63 - RNN+dropout 93.07 2.02 Table 1: 1-best tagging accuracy and speed comparison on CCGBank Section 00 with a single CPU core (1,913 sentences), tagging time in secs. To train the model, we optimize cross-entropy loss with stochastic gradient descent using minibatched backpropagation through time (BPTT; Rumelhart et al., 1988; Mikolov, 2012); the minibatch size for BPTT, again tuned on the dev set, is set to 9. Embedding Dropout Regularization. Without any regularization, we found cross-entropy error on the dev set started to increase while the error on the training set was continuously driven to a very small value (Fig. 1a). With the suspicion of overfitting, we experimented with l1 and l2 regularization and learning rate decay but none of these techniques gave any noticeable improvements for our model. Following Legrand and Collobert (2014), we instead implemented word embedding dropout as a regularization for all the look-up t</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom´aˇs Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Juho Heimonen</author>
<author>Jari Bj¨orne</author>
<author>Jorma Boberg</author>
<author>Jouni J¨arvinen</author>
<author>Tapio Salakoski</author>
</authors>
<title>Bioinfer: a corpus for information extraction in the biomedical domain.</title>
<date>2007</date>
<journal>BMC bioinformatics,</journal>
<volume>8</volume>
<issue>1</issue>
<marker>Pyysalo, Ginter, Heimonen, Bj¨orne, Boberg, J¨arvinen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio Salakoski. 2007. Bioinfer: a corpus for information extraction in the biomedical domain. BMC bioinformatics, 8(1):50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a lexicalized-grammar parser to contrasting domains.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>475--484</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2577" citStr="Rimell and Clark, 2008" startWordPosition="409" endWordPosition="412">rategy, such that additional categories are supplied to the parser only if a spanning analysis cannot be found. This strategy is used in the de facto C&amp;C parser (Curran et al., 2007), and the two-stage CCG parsing pipeline (supertagging and parsing) continues to be the choice for most recent CCG parsers (Zhang and Clark, 2011; Auli and Lopez, 2011; Xu et al., 2014). Despite the effectiveness of supertagging, the most widely used model for this task (Clark and Curran, 2007) has a number of drawbacks. First, it relies too heavily on POS tags, which leads to lower accuracy on out-of-domain data (Rimell and Clark, 2008). Second, due to the sparse, indicator feature sets mainly based on raw words and POS tags, it shows pronounced performance degradation in the presence of rare and unseen words (Rimell and Clark, 2008; Lewis and Steedman, 2014). And third, in order to reduce computational requirements and feature sparsity, each tagging decision is made without considering any potentially useful contextual information beyond a local context window. Lewis and Steedman (2014) introduced a feedforward neural network to supertagging, and addressed the first two problems mentioned above. However, their attempt to ta</context>
<context position="17824" citStr="Rimell and Clark (2008)" startWordPosition="3028" endWordPosition="3031">omains (auto POS). We evaluate on all sentences (100% coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN both have 100% coverage on the Wikipedia data. Fig. 1c compares multi-tagging accuracies of all the models on the dev set. For all models, the same Q levels are used (ranging from 0.075 to 10−4, and all C&amp;C default values are included). The RNN model consistently outperforms other models across different ambiguity levels. Table 3 shows 1-best accuracies of all models on the test data sets (Bio-GENIA gold-standard CCG lexical category data from Rimell and Clark (2008) are used, since no gold categories are available in the Bioinfer data). With gold-standard POS tags, the C&amp;C model outperforms both the NN and RNN models on CCGBank and Bio-GENIA; with auto POS, the accuracy of the C&amp;C model drops significantly, due to its high reliance on POS tags. Fig. 2 shows multi-tagging accuracies on all test data (using Q levels ranging from 0.075 to 10−6, and all C&amp;C default values are included). On CCGBank, the RNN model has a clear accuracy advantage, while on the other two data sets, the accuracies given by the NN model are closer to the RNN model at some ambiguity</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008. Adapting a lexicalized-grammar parser to contrasting domains. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 475–484. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1988</date>
<booktitle>Cognitive modeling,</booktitle>
<pages>5</pages>
<contexts>
<context position="10881" citStr="Rumelhart et al., 1988" startWordPosition="1791" endWordPosition="1794">t window sizes of 3, 5, 7, 9 and 11 during development and found a window size of 7 gives the best performing model on the dev set. We use a fixed learning rate of 0.0025 and a hidden state size of 200. Model Accuracy Time C&amp;C (gold POS) 92.60 - C&amp;C (auto POS) 91.50 0.57 NN 91.10 21.00 RNN 92.63 - RNN+dropout 93.07 2.02 Table 1: 1-best tagging accuracy and speed comparison on CCGBank Section 00 with a single CPU core (1,913 sentences), tagging time in secs. To train the model, we optimize cross-entropy loss with stochastic gradient descent using minibatched backpropagation through time (BPTT; Rumelhart et al., 1988; Mikolov, 2012); the minibatch size for BPTT, again tuned on the dev set, is set to 9. Embedding Dropout Regularization. Without any regularization, we found cross-entropy error on the dev set started to increase while the error on the training set was continuously driven to a very small value (Fig. 1a). With the suspicion of overfitting, we experimented with l1 and l2 regularization and learning rate decay but none of these techniques gave any noticeable improvements for our model. Following Legrand and Collobert (2014), we instead implemented word embedding dropout as a regularization for a</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1988</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1988. Learning representations by backpropagating errors. Cognitive modeling, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="855" citStr="Steedman, 2000" startWordPosition="119" endWordPosition="121">m.ac.uk Abstract Recent work on supertagging using a feedforward neural network achieved significant improvements for CCG supertagging and parsing (Lewis and Steedman, 2014). However, their architecture is limited to considering local contexts and does not naturally model sequences of arbitrary length. In this paper, we show how directly capturing sequence information using a recurrent neural network leads to further accuracy improvements for both supertagging (up to 1.9%) and parsing (up to 1% F1), on CCGBank, Wikipedia and biomedical text. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a highly lexicalized formalism; the standard parsing model of Clark and Curran (2007) uses over 400 lexical categories (or supertags), compared to about 50 POS tags for typical CFG parsers. This makes accurate disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough </context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7901" citStr="Turian et al. (2010)" startWordPosition="1299" endWordPosition="1302">s either side of the target word are included. For a word wt, its continuous feature representation is: fwt = [ewt; swt; cwt], (4) where ewt E R1xn, swt E R1xm and cwt E R1xm are the output vectors from the three different look-up tables, and [ewt; swt; cwt] denotes the concatenation of three vectors and hence fwt E R1x(n+2m). At word position t, the input layer of the network xt is: xt = [fwt−Lk/2J; ... fwt; ...; fwt+Lk/2J], (5) where xt E R1xk(n+2m) and the right-hand side is the concatenation of all feature representations in a size k context window. We use pre-trained word embeddings from Turian et al. (2010) to initialize lookup table Lw, and we apply a set of word pre-processing techniques at both training and test time to reduce sparsity. All words are first lower-cased, and all numbers are collapsed into a single digit ‘0’. If a lower-cased hyphenated 251 word does not have an entry in the pre-trained word embeddings, we attempt to back-off to the substring after the last hyphen. For compound words and numbers delimited by “\/”, we attempt to back-off to the substring after the delimiter. After pre-processing, the Turian embeddings have a coverage of 94.25% on the training data; for out-of-voc</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenduan Xu</author>
<author>Stephen Clark</author>
<author>Yue Zhang</author>
</authors>
<title>Shift-reduce CCG parsing with a dependency model.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 ACL Conference.</booktitle>
<contexts>
<context position="2321" citStr="Xu et al., 2014" startWordPosition="367" endWordPosition="370"> 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. ing and test time. Clark and Curran (2007) enhanced supertagging using a so-called adaptive strategy, such that additional categories are supplied to the parser only if a spanning analysis cannot be found. This strategy is used in the de facto C&amp;C parser (Curran et al., 2007), and the two-stage CCG parsing pipeline (supertagging and parsing) continues to be the choice for most recent CCG parsers (Zhang and Clark, 2011; Auli and Lopez, 2011; Xu et al., 2014). Despite the effectiveness of supertagging, the most widely used model for this task (Clark and Curran, 2007) has a number of drawbacks. First, it relies too heavily on POS tags, which leads to lower accuracy on out-of-domain data (Rimell and Clark, 2008). Second, due to the sparse, indicator feature sets mainly based on raw words and POS tags, it shows pronounced performance degradation in the presence of rare and unseen words (Rimell and Clark, 2008; Lewis and Steedman, 2014). And third, in order to reduce computational requirements and feature sparsity, each tagging decision is made withou</context>
</contexts>
<marker>Xu, Clark, Zhang, 2014</marker>
<rawString>Wenduan Xu, Stephen Clark, and Yue Zhang. 2014. Shift-reduce CCG parsing with a dependency model. In Proceedings of the 2014 ACL Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Shift-reduce CCG parsing.</title>
<date>2011</date>
<booktitle>In Proc. ACL 2011,</booktitle>
<pages>683--692</pages>
<location>Portland, OR.</location>
<contexts>
<context position="2281" citStr="Zhang and Clark, 2011" startWordPosition="359" endWordPosition="362">racy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. ing and test time. Clark and Curran (2007) enhanced supertagging using a so-called adaptive strategy, such that additional categories are supplied to the parser only if a spanning analysis cannot be found. This strategy is used in the de facto C&amp;C parser (Curran et al., 2007), and the two-stage CCG parsing pipeline (supertagging and parsing) continues to be the choice for most recent CCG parsers (Zhang and Clark, 2011; Auli and Lopez, 2011; Xu et al., 2014). Despite the effectiveness of supertagging, the most widely used model for this task (Clark and Curran, 2007) has a number of drawbacks. First, it relies too heavily on POS tags, which leads to lower accuracy on out-of-domain data (Rimell and Clark, 2008). Second, due to the sparse, indicator feature sets mainly based on raw words and POS tags, it shows pronounced performance degradation in the presence of rare and unseen words (Rimell and Clark, 2008; Lewis and Steedman, 2014). And third, in order to reduce computational requirements and feature sparsi</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Shift-reduce CCG parsing. In Proc. ACL 2011, pages 683–692, Portland, OR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>