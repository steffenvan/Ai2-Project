<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.9983275">
Sparse, Contextually Informed Models for Irony Detection:
Exploiting User Communities, Entities and Sentiment
</title>
<author confidence="0.999427">
Byron C. Wallace Do Kook Choe and Eugene Charniak
</author>
<affiliation confidence="0.998644">
University of Texas at Austin Brown University
</affiliation>
<email confidence="0.995575">
byron.wallace@utexas.edu {dc65, ec}@cs.brown.edu
</email>
<sectionHeader confidence="0.993823" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966296296296">
Automatically detecting verbal irony
(roughly, sarcasm) in online content is
important for many practical applications
(e.g., sentiment detection), but it is dif-
ficult. Previous approaches have relied
predominantly on signal gleaned from
word counts and grammatical cues. But
such approaches fail to exploit the context
in which comments are embedded. We
thus propose a novel strategy for verbal
irony classification that exploits contex-
tual features, specifically by combining
noun phrases and sentiment extracted
from comments with the forum type
(e.g., conservative or liberal) to which
they were posted. We show that this
approach improves verbal irony classifica-
tion performance. Furthermore, because
this method generates a very large feature
space (and we expect predictive contextual
features to be strong but few), we propose
a mixed regularization strategy that places
a sparsity-inducing E1 penalty on the
contextual feature weights on top of the E2
penalty applied to all model coefficients.
This increases model sparsity and reduces
the variance of model performance.
</bodyText>
<sectionHeader confidence="0.990708" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.978358">
Automated verbal irony detection is a challenging
problem.1 But recognizing when an author has in-
tended a statement ironically is practically impor-
tant for many text classification tasks (e.g., senti-
ment detection).
Previous models for irony detection (Tsur et
al., 2010; Lukin and Walker, 2013; Riloff et al.,
</bodyText>
<footnote confidence="0.99188725">
1In this paper we will be a bit cavalier in using the terms
‘verbal irony’ and ‘sarcasm’ interchangeably. We recognize
that the latter is a special type of the former, the definition of
which is difficult to pin down precisely.
</footnote>
<figureCaption confidence="0.999108">
Figure 1: A reddit comment illustrating contextualizing fea-
</figureCaption>
<bodyText confidence="0.9304135">
tures that we propose leveraging to improve classification.
Here the highlighted entities (external the comment text it-
self) provide contextual signals indicating that the shown
comment was intended ironically. As we shall see, Oba-
macare is in general a strong indicator of irony when present
in posts to the conservative subreddit, but less so in posts to
the progressive subreddit.
2013) have relied predominantly on features in-
trinsic to the texts to be classified. By contrast,
here we propose exploiting contextualizing infor-
mation, which is often available for web-based
classification tasks. More specifically, we exploit
signal gleaned from the conversational threads to
which comments belong. Our approach capital-
izes on the intuition that members of different user
communities are likely to be sarcastic about dif-
ferent things. As a proxy for user community,
we leverage knowledge of the specific forums to
which comments were posted. For example, one
may surmise that the statement ‘I really am proud
of Obama’ is likely to have been intended iron-
ically if it was posted to a forum frequented by
political conservatives. But if this same utterance
were posted to a liberal-leaning forum, it is more
likely to have been intended in earnest. This sort
of information is often directly or indirectly avail-
able on social media, but previous models have not
capitalized on it. This is problematic; recent work
has shown that humans require such contextualiz-
ing information to infer ironic intent (Wallace et
Guys who the fuck cares?! Leave him alone, there are real problems
like bridge-gate scandal with Chris Cristie
</bodyText>
<page confidence="0.950663">
1035
</page>
<note confidence="0.979956">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1035–1044,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.989460066666667">
al., 2014).
As a concrete example, we consider the task
of identifying verbal irony in comments posted to
reddit (http://www.reddit.com), a social-
news website. Users post content (e.g., links to
news stories) to reddit, which are then voted on by
the community. Users may also discuss this con-
tent on the website; these are the comments that
we will work with here. Reddit comprises many
subreddits, which are user communities centered
around specific topics of interest. In this work
we consider comments posted to two pairs of po-
larized user communities, or subreddits: (1) pro-
gressive and conservative subreddits (comprising
individuals on the left and right of the US polit-
ical spectrum, respectively), and (2) atheism and
Christianity subreddits.
Our aim is to develop a model that can recog-
nize verbal irony in comments posted to such fo-
rums, e.g., automatically discern that the user who
posted the comment shown in Figure 1 intended
his or her comment ironically. To this end, we pro-
pose a strategy that capitalizes on available con-
textualizing information, such as interactions be-
tween the user community (subreddit) that com-
ments were posted to, extracted entities (here we
use noun phrases, or NNPs) and inferred senti-
ment.
The contributions of this work are summarized
as follows.
</bodyText>
<listItem confidence="0.930460909090909">
• We demonstrate that contextual information,
such as inferred user-community (in this
case, the subreddit) can be crossed with ex-
tracted entities and sentiment to improve de-
tection of verbal irony. This improves perfor-
mance over baseline models (including those
that exploit inferred sentiment, but not con-
text).
• We introduce a novel composite regular-
ization strategy that applies a sparsifying
E1 penalty to the contextual/sentiment/entity
</listItem>
<bodyText confidence="0.99672525">
feature weights in addition to the standard
squared E2 penalty to all feature weights.
This induces more compact, interpretable
models that exhibit lower variance.
While discerning ironic comments on reddit
is our immediate task, the proposed approach is
generally applicable to a wide-range of subjec-
tive, web-based text classification tasks. Indeed,
this approach would be useful for any scenario in
which we expect different groups of individuals
producing content to tend to discuss different en-
tities in a way that correlates with the target cate-
gorization. The key is in identifying an available
proxy for user groupings (here we rely on the sub-
reddits to which a comment was posted). Such
information is often available (or can be derived)
for comments posted to different mediums on the
web: for example on Twitter we know who a user
follows; and on YouTube we know the channels to
which videos belong.
</bodyText>
<sectionHeader confidence="0.989016" genericHeader="method">
2 Exploiting context
</sectionHeader>
<subsectionHeader confidence="0.997978">
2.1 Communities and sentiment
</subsectionHeader>
<bodyText confidence="0.999958178571429">
As discussed above, a shortcoming with existing
models for detecting sarcasm/verbal irony on the
web is their failure to capitalize on contextualiz-
ing information. But such information is critical
to discerning irony. A large body of work on the
use and interpretation of verbal irony supports this
supposition (Grice, 1975; Clark and Gerrig, 1984;
Wallace, 2013; Wallace et al., 2014). Individu-
als will be more likely, in general, to use sarcasm
when discussing specific entities. Which entities
will depend in part on the community to which
the individual belongs. As a proxy for user com-
munity, here we leverage the subreddits to which
comments were posted.
Sentiment may also play an important role. In
general, verbal irony is almost always used to con-
vey negative views via ostensibly positive utter-
ances (Sperber and Wilson, 1981). And recent
work (Riloff et al., 2013) has exploited features
based on sentiment to improve irony detection.
To summarize: when assuming an ironic voice
we expect that individuals will convey ostensibly
positive sentiment about entities, and that these en-
tities will depend on the type of individual in ques-
tion. We propose capitalizing on such informa-
tion by introducing features that encode subred-
dits, sentiment and noun phrases (NNPs), as we
describe next.
</bodyText>
<subsectionHeader confidence="0.972292">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.999210714285714">
We leverage the feature sets enumerated in Ta-
ble 1. Subreddits are observed variables. Noun
phrase (NNP) extraction and sentiment inference
are performed automatically via state of the art
NLP tools. In particular, we use the Stanford Sen-
timent Analysis tool (Socher et al., 2013) to infer
sentiment. To extract NNPs we use the Stanford
</bodyText>
<page confidence="0.930676">
1036
</page>
<bodyText confidence="0.517459">
Feature
</bodyText>
<subsectionHeader confidence="0.357948">
Description
</subsectionHeader>
<bodyText confidence="0.999418833333333">
The inferred sentiment (nega-
tive/neutral or positive) for a given
comment.
the subreddit (e.g., progressive or con-
servative; atheism or Christianity) to
which a comment was posted.
Noun phrases (e.g., proper nouns) ex-
tracted from comment texts.
Noun phrases extracted from comment
texts and the thread to which they be-
long (for example, ‘Obamacare’ from
the title in Figure 1).
</bodyText>
<sectionHeader confidence="0.897056" genericHeader="method">
3 Enforcing sparsity
</sectionHeader>
<subsectionHeader confidence="0.992951">
3.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.998710833333333">
In this work we consider linear models with bi-
nary outputs (y E {−1,+1}). We will assume
we have access to a training dataset comprising n
instances, x = {x1, ..., xn} and associated labels
y = {y1, ..., yn}. We then aim to find a weight-
vector w that optimizes the following objective.
</bodyText>
<figure confidence="0.93858875">
Sentiment
Subreddit
NNP
NNP+
</figure>
<tableCaption confidence="0.964121">
Table 1: Feature types that we exploit. We view the (ob- argmin n G(sign{w · xi}, yi) + αR(w) (1)
</tableCaption>
<bodyText confidence="0.972827229166667">
served) subreddit as a proxy for user type. We combine this w i=1
with sentiment and extracted noun phrases (NNPs) to im-
prove classifier performance.
Part of Speech tagger (Toutanova et al., 2003). We
then introduce ‘bag-of-NNP’ features and features
that indicate whether the sentiment inferred for a
given sentence was positive or not.
Additionally, we introduce ‘interaction’ fea-
tures that capture combinations of these. For ex-
ample, a feature that indicates whether a given
sentence mentions Obamacare (which will be one
of many NNPs automatically extracted) and was
posted in the conservative subreddit. This is an
example of a two-way interaction. We also exper-
iment with three-way interactions, crossing senti-
ment with NNPs and subreddits. An example is a
feature that indicates if a sentence was: inferred
to be positive and mentions Obamacare (NNP)
and was part of a comment made in the conserva-
tive subreddit. Finally, we experiment with adding
NNPs extracted from the comment thread in addi-
tion to the comment text.
These are rich features that capture signal not
directly available from the sentences themselves.
Features that encode subreddits crossed with ex-
tracted NNP’s, in particular, offer a chance to ex-
plicitly account for differences in how the ironic
device is used by individuals in different com-
munities. However, this has the downside of in-
troducing a large number of irrelevant terms into
the model: we expect, a priori, that many enti-
ties will not correlate with the use of verbal irony.
We would therefore expect this strategy to exhibit
high variance in terms of predictive performance,
and we later confirm this empirically. Ideally, a
model would perform feature selection during pa-
rameter estimation, thus dropping irrelevant inter-
action terms. We next introduce a composite E1/E2
regularization strategy toward this end.
Where G is a loss function, R(w) is a regulariza-
tion term and α is a parameter expressing the rel-
ative emphasis placed on achieving minimum em-
pirical loss versus producing a simple model (i.e.,
a weight vector with small weights). Typically one
searches for a good α using the available train-
ing data. For G, we will use the log-loss in this
work, though other loss functions may be used in
its place.
</bodyText>
<subsectionHeader confidence="0.999602">
3.2 Sparsity via Regularization
</subsectionHeader>
<bodyText confidence="0.999844">
Concerning R, one popular regularization func-
tion is the squared E2 norm:
</bodyText>
<equation confidence="0.7679875">
I:
w2 (2)
j
j
</equation>
<bodyText confidence="0.9999764">
This is the norm used in the standard Support Vec-
tor Machine (SVM) formulation, for example, and
has been shown empirically to work well for text
classification (Joachims, 1998). An alternative is
to use the E1 norm:
</bodyText>
<equation confidence="0.843793666666667">
I:
|wj |(3)
j
</equation>
<bodyText confidence="0.9999755">
Which has the advantage of inducing sparse mod-
els: i.e., using the E1 norm as a penalty tends to
drive feature weights to 0.
Returning to the present task of detecting ver-
bal irony in comments, it seems reasonable to as-
sume that there will be a relatively small set of
entities that correlate with sarcasm. But because
we are introducing ‘interaction’ features that enu-
merate the cross-product of subreddits and entities
(and, in some cases, sentiment), we have a large
feature-space. This space includes features that
correspond to NNPs extracted from, and sentiment
inferred for, the sentence itself: we will denote the
indices for these by Z. Other interaction features
</bodyText>
<page confidence="0.97692">
1037
</page>
<bodyText confidence="0.999966666666667">
correspond to entities extracted from the threads
associated with comments: we denote the corre-
sponding set of indices by T . We expect only a
fraction of the features comprising both Z and T
to have non-zero weights (i.e., to signal ironic in-
tent).
This scenario is prone to the undesirable prop-
erty of high-variance, and hence calls for stronger
regularization. But in general replacing the
squared E2 norm with an E1 penalty (over all
weights) hampers classification performance (in-
deed, as we later report, this strategy performs
very poorly here). Therefore, in our scenario we
would like to place a sparsifying E1 regularizer
over the contextual (interaction) features while
still leveraging the squared E2-norm penalty for the
standard bag-of-words (BoW) features.2 We thus
propose the following composite penalty:
</bodyText>
<equation confidence="0.965933">
X X
w2 j +
j kEZ
</equation>
<bodyText confidence="0.992773272727273">
The idea is that this will drive many of the weights
associated with the contextual features to zero,
which is desirable in light of the intuition that a
relatively small number of entities will likely in-
dicate sarcasm. At the same time, this composite
penalty applies only the squared E2 norm to the
standard BoW features, given the comparatively
strong predictive performance realized with this
strategy.
Putting this together, we modify the original ob-
jective (Equation 1) as follows:
</bodyText>
<equation confidence="0.991110666666667">
G(sign{w · xil, yi)+
Xw2j + α1 X|wk|+α2 |wl |(5)
kEZ lET
</equation>
<bodyText confidence="0.964809076923077">
Where we have placed separate α scalars on the re-
spective penalty terms. Note that this is similar to
the elastic net (Zou and Hastie, 2005) joint regu-
larization and variable selection strategy. The dis-
tinction here is that we only apply the E1 penalty
to (i.e., perform feature selection for) the subset
of ‘interaction’ feature weights, which is in con-
trast to the elastic net, which imposes the compos-
ite penalty to all feature weights. One can view
this as using the regularizer to encourage a spar-
sity pattern specific to the task at hand.
2Note that we apply both 21 and 22 penalties to the fea-
tures in I and T.
</bodyText>
<subsectionHeader confidence="0.90315">
3.3 Inference
</subsectionHeader>
<bodyText confidence="0.999987">
We fit this model via Stochastic Gradient Descent
(SGD).3 During each update, we impose both the
squared E2 and E1 penalties; the latter is applied
only to the contextual/interaction features in Z and
T . For the E1 penalty, we adopt the cumulative
truncated gradient method proposed by Tsuruoka
et al. (2009).
</bodyText>
<sectionHeader confidence="0.998573" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.883722">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999979942857143">
For our development dataset, we used a subset of
the reddit irony corpus (Wallace et al., 2014) com-
prising annotated comments from the progressive
and conservative subreddits. We also report re-
sults from experiments performed using a sepa-
rate, held-out portion of this data, which we did
not use during model refinement. Furthermore, we
later present results on comments from the athe-
ism and Christianity subreddits (we did not use
this data during model development, either).
The development dataset includes 1,825 anno-
tated comments (876 and 949 from the progressive
and conservative subreddits, respectively). These
comprise 5,625 sentences in total, each of which
was independently labeled by three annotators as
having been intended ironically or not. For addi-
tional details on the annotation process, see (Wal-
lace et al., 2014). For simplicity, we consider a
sentence to be ‘ironic’ (y = 1) when at least two
of the three annotators designated it as such, and
‘unironic’ (y = −1) otherwise. Using this crite-
ria, 286 (5%) of the labeled sentences are labeled
‘ironic’.
The test portion of the political dataset com-
prises 996 annotated comments (409 progressive
and 587 conservative comments), totalling 2,884
sentences. Using the same criteria as above – at
least 2/3 annotators labeling a given sentence as
‘ironic’ – we have 154 ‘ironic’ sentences (again
about 5%).
The ‘religion’ dataset (comments from athe-
ism and Christianity) contains 1,682 labeled com-
ments comprising 5615 sentences (2,966 and
2,649 from the atheism and Christian subreddits,
respectively); 313 (-6%) were deemed ‘ironic’.
</bodyText>
<footnote confidence="0.943118">
3We have implemented this within the sklearn package
(Pedregosa et al., 2011).
</footnote>
<equation confidence="0.938794166666667">
|wk|+ X |wl |(4)
lET
argmin Xn
W i=1
Xα0
j
</equation>
<page confidence="0.950629">
1038
</page>
<subsectionHeader confidence="0.855441">
4.2 Experimental Details
</subsectionHeader>
<bodyText confidence="0.999974111111111">
We recorded results from 500 independently per-
formed experiments on random train (80%)/test
(20%) splits of the data. These splits were per-
formed at the comment (rather than sentence)
level, so as not to test on sentences belonging to
comments encountered in the training set. We
measured performance, however, at the sentence
level (often only a single sentence in a given com-
ment will have been labeled as ‘ironic’).
Our baseline approach is a standard squared-E2
regularized log-loss linear model (fit via SGD) that
leverages uni- and bi-grams and features indicat-
ing grammatical cues, such as exclamation points
and emoticons. We also experiment with a model
that includes inferred sentiment indicators, but not
context. We performed standard English stop-
wording, and we used Term Frequency Inverse-
Document Frequency (TF-IDF) feature weighting.
For the gradient descent procedure, we used a de-
caying learning rate (specifically, t1 , where t is the
update count). We performed a coarse grid search
to find values for α that maximize F 1 on the train-
ing datasets. We took five full passes over the
training data before terminating descent.
We report paired recalls and precisions, as ob-
served on each random train/test split of the data.
The former is defined as T P
</bodyText>
<equation confidence="0.7788565">
T P +F Nand the latter
as T P
</equation>
<bodyText confidence="0.9899635">
T P +FP , where TP denotes the true positive
count, FN the number of false negatives and FP
the false positive count. We report these sepa-
rately - rather than collapsing into F1 - because
it is not clear that one would value recall and pre-
cision equally for irony detection, and because this
allows us to tease out how the models differ in per-
formance. Notably, for example, sentiment and
context features both improve recall, but the lat-
ter does so without harming precision.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.985847">
5.1 Results on the Development Corpus
</subsectionHeader>
<bodyText confidence="0.9618582">
Figure 2 and Table 2 summarize the performance
of the different approaches over 500 indepen-
dently performed train/test splits of the political
development corpus. For reference, a random
chance strategy (which predicts ‘ironic’ with prob-
ability equal to the observed prevalence) achieves
a median recall of 0.048 and a median precision of
0.047.
Figure 2 shows histograms of the observed ab-
solute differences between the baseline linear clas-
</bodyText>
<figureCaption confidence="0.993674">
Figure 4: Empirical distributions (violin plots) of non-zero
feature counts in the NNP × subreddit model (rows 3 and 4
in Figure 3) using standard 22-norm (left) and the proposed
f1f2-norm (right) regularization approaches on the athe-
ism/Christianity data over 500 independent train/test splits.
The composite norm achieves much greater sparsity, result-
ing in lower variance. This sparsity also (arguably) provides
greater interpretability; one can inspect contextual features
with non-zero weights.
</figureCaption>
<bodyText confidence="0.9999911">
sifier and the proposed augmentations. Adding
the proposed features (which capitalize on senti-
ment and NNP-mentions on specific subreddits)
increases absolute median recall by 3.4 percent-
age points (a relative gain of ∼12%). And this is
achieved without sacrificing precision (in contrast
to exploiting only sentiment). Furthermore, as we
can see in Figures 2 and 3, the proposed regular-
ization strategy shrinks the variance of the classi-
fier. This variance reduction is achieved through
greater model sparsity, as can be seen in Figure
4, which improves interpretability. We note that
leveraging only an E1 regularization penalty (with
the full feature-set) results in very poor perfor-
mance (median recall and precision of 0.05 and
0.09, respectively). Similarly, the elastic-net strat-
egy (Zou and Hastie, 2005) (in which we do not
specify which features to apply the E1 penalty to),
here achieves a median recall of 0.11 and a median
precision of 0.07.
</bodyText>
<subsectionHeader confidence="0.981831">
5.2 Results on the Held-out (Test) Corpus
</subsectionHeader>
<bodyText confidence="0.999885916666667">
Table 4 reports results on the held-out political test
dataset, achieved after training the models on the
entirety of the development corpus. To account
for the variance inherent to inference via SGD, we
performed 100 runs of the SGD procedure and re-
port median results from these runs. These results
mostly agree with those reported for the develop-
ment corpus: the proposed strategy improves me-
dian recall on the held-out corpus by nearly 4.0
percentage points, at a median cost of about 1
point in precision. By contrast, sentiment alone
provides a 2% absolute improvement in recall at
</bodyText>
<page confidence="0.99395">
1039
</page>
<table confidence="0.977410777777778">
mean; median (25th, 75th) mean; median (25th, 75th)
baseline (BoW) 0.288; 0.283 (0.231, 0.333) 0.129; 0.124 (0.103, 0.149)
A recall A precision
(overall) sent. +0.036; +0.037 (+0.015, +0.063) -0.008; -0.007 (-0.018, +0.003)
NNP +0.021; +0.018 (+0.000, +0.036) -0.008; -0.008 (-0.016, -0.001)
NNP x subreddit +0.013; +0.016 (+0.000, +0.031) -0.002; -0.003 (-0.009, +0.004)
NNP x subreddit (`1 `2) +0.010; +0.000 (+0.000, +0.021) -0.002; -0.002 (-0.007, +0.004)
NNP+ x sent. x subreddit+ sent. +0.036; +0.038 (+0.000, +0.065) -0.000; -0.001 (-0.012, +0.011)
NNP+ x sent. x subreddit+ sent. (`1 `2) +0.035; +0.034 (+0.000, +0.062) +0.001; +0.000 (-0.011, +0.011)
</table>
<tableCaption confidence="0.786681166666667">
Table 2: Summary results over 500 random train/test splits of the development dataset. The top row reports mean and median
baseline (BoW) recall and precision and lower and upper (25th and 75th) percentiles. We report pairwise differences w.r.t. this
baseline in terms of recall and precision for each strategy. Exploiting NNP features and subreddits improves recall with little
to not cost in precision. Capitalizing on sentiment alone improves recall but at a greater cost in precision. The proposed `1`2
regularization strategy achieves comparable performance with fewer features, and shrinks the variance over different train/test
splits (as can bee seen in Figure 2).
</tableCaption>
<table confidence="0.998724777777778">
mean; median (25th, 75th) mean; median (25th, 75th)
baseline (BoW) 0.281; 0.268 (0.222, 0.327) 0.189; 0.187 (0.144, 0.230)
A recall A precision
(overall) sent. +0.001; +0.000 (-0.011, +0.015) -0.014; -0.012 (-0.023, -0.002)
NNP +0.018; +0.018 (+0.000, +0.039) -0.009; -0.010 (-0.021, +0.001)
NNP x subreddit +0.024; +0.025 (+0.000, +0.046) +0.002; +0.001 (-0.011, +0.013)
NNP x subreddit (`1 `2) +0.013; +0.015 (+0.000, +0.033) +0.002; +0.002 (-0.009, +0.011)
NNP+ x sent. x subreddit+ sent. +0.023; +0.024 (+0.000, +0.046) +0.001; +0.001 (-0.012, +0.013)
NNP+ x sent. x subreddit+ sent. (`1 `2) +0.014; +0.015 (+0.000, +0.036) -0.008; -0.008 (-0.021, +0.004)
</table>
<tableCaption confidence="0.667799666666667">
Table 3: Results on the atheism and Christianity subreddits. In general sentiment does not help on this dataset (see row 1). But
the NNP and subreddit features again consistently improve recall without hurting precision. And, as above, `1`2 regularization
shrinks variance (see Figures 2 and 3).
</tableCaption>
<figureCaption confidence="0.995359428571429">
Figure 2: Results from 500 independent train/test splits of the development subset of our political data. Shown are histograms
with smoothed kernel density estimates of differences in recall and precision between the baseline bag-of-words based approach
and each feature space/method (one per row). The solid black line at 0 indicates no difference; solid and dotted blue lines
demarcate means and medians, respectively. Features are as in Table 1. The × symbol denotes interactions; + indicates
addition. The proposed contextual features substantially improve recall, with little to no loss in precision. Moreover, in general,
the `1`2 regularization approach reduces variance. (We note that in constructing histograms we have excluded a handful of
points – never more than 1% – where the difference exceeded 0.15).
</figureCaption>
<table confidence="0.998059333333333">
median recall (std. dev.) median precision (std. dev.)
baseline 0.331 (0.146) 0.148 (0.022)
(overall) sent. 0.351 (0.054) 0.125 (0.003)
NNP 0.364 (0.119) 0.135 (0.021)
NNP x subreddit 0.357 (0.108) 0.143 (0.020)
NNP+ x sent. x subreddit 0.344 (0.116) 0.142 (0.019)
NNP+ x sent. x subreddit (`1 `2) 0.325 (0.052) 0.141 (0.008)
NNP+ x sent. x subreddit+ sent. 0.377 (0.104) 0.141 (0.014)
NNP+ x sent. x subreddit+ sent. (`1 `2) 0.370 (0.056) 0.140 (0.008)
</table>
<tableCaption confidence="0.98547">
Table 4: Results on the held-out political dataset, using the entire development corpus as a training set. Abbreviations are as
</tableCaption>
<footnote confidence="0.68924">
described in the caption for Figure 2. Due to the variance inherent to the stochastic gradient descent procedure, we repeat
the experiment 100 times and report the median performance and standard deviations (of different SGD runs). Results are
consistent with those reported for the development corpus.
</footnote>
<page confidence="0.985695">
1040
</page>
<figureCaption confidence="0.9658745">
Figure 3: Results from 500 independent train/test splits of the development subset of the religion corpus). The description is
the same as for Figure 2.
</figureCaption>
<bodyText confidence="0.527138">
the expense of more than 2 points in precision.
</bodyText>
<subsectionHeader confidence="0.96637">
5.3 Results on the religion dataset
</subsectionHeader>
<bodyText confidence="0.9999712">
To assess the general applicability of the proposed
approach, we also evaluate the method on com-
ments from a separate pair of polarized communi-
ties: atheism and Christianity, as described in Sec-
tion 4.1. This dataset was not used during model
development. We follow the experimental setup
described in Section 4.2.
In this case, capitalizing on the NNP × subred-
dit features produces a mean 2.3% absolute gain in
recall (median: 2.4%) over the baseline approach,
with a (very) slight gain in precision. The i1 E2
approach achieves a lower expected gain in recall
(median: 1.5%), but again shrinks the variance
w.r.t. model performance (see Figure 3). More-
over, as we show in Figure 4, this is achieved with
a much more compact (sparser) model. We note
that for the religion data, inferred sentiment fea-
tures do not seem to improve performance, in con-
trast to the results on the political subreddits. At
present, we are not sure why this is the case.
These results demonstrate that introducing fea-
tures that encode entities and user communities
(NNPs × subreddit) improve recall for irony de-
tection in comments addressing relatively diverse
topics (politics and religion).
</bodyText>
<subsectionHeader confidence="0.992126">
5.4 Predictive features
</subsectionHeader>
<bodyText confidence="0.9994135">
We report the interaction features that are the best
predictors of verbal irony in the respective subred-
</bodyText>
<table confidence="0.9656535">
progressive conservative
feature weight feature weight
freedom 0.102 (0.048) racist 0.148 (0.043)
god 0.085 (0.045) news 0.100 (0.044)
christmas 0.081 (0.046) way 0.078 (0.044)
jesus 0.060 (0.038) obamacare 0.068 (0.041)
kenya 0.052 (0.035) white 0.059 (0.037)
brave 0.043 (0.035) let 0.058 (0.038)
bravo 0.041 (0.035) course 0.046 (0.033)
know 0.038 (0.030) huh 0.044 (0.036)
dennis 0.038 (0.029) education 0.043 (0.032)
ronald 0.036 (0.030) president 0.039 (0.031)
</table>
<tableCaption confidence="0.927809333333333">
Table 5: Average weights (and standard deviations calculated
across samples) for top 10 NNP × subreddit features from
the progressive and conservative subreddits.
</tableCaption>
<bodyText confidence="0.998972904761905">
dits (for both polar community pairs). Specifically,
we estimated the weights for every interaction fea-
ture using the entire training dataset, and repeated
this process 100 times to account for variation due
to the SGD procedure.
Table 5 displays the top 10 NNP × subreddit
features for the political subreddits, with respect to
the mean magnitude of the weights associated with
them. We report these means and the standard de-
viations calculated across the 100 runs. This table
implies, for example, that mentions of ‘freedom’
and ‘kenya’ indicate irony in the progressive sub-
reddit; while mentions of ‘obamacare’ and ‘pres-
ident’ (for example) in the conservative subreddit
tend to imply irony.
Table 6 reports analagous results for the religion
subreddits. Here we can see, e.g., that ‘god’ is a
good predictor of irony in the atheism subreddit,
and ‘professor’ is in the Christianity subreddit.
We also report the top ranking ‘three-way’ in-
teraction features that cross NNP’s extracted from
</bodyText>
<page confidence="0.944931">
1041
</page>
<table confidence="0.999023583333333">
atheism Christianity
feature weight feature weight
right 0.353 (0.014) professor 0.297 (0.013)
god 0.324 (0.013) let 0.084 (0.014)
women 0.214 (0.013) peter 0.080 (0.019)
christ 0.160 (0.014) geez 0.054 (0.016)
news 0.146 (0.013) evil 0.054 (0.015)
trust 0.139 (0.013) killing 0.053 (0.015)
shit 0.132 (0.015) liberal 0.049 (0.014)
believe 0.123 (0.013) antichrist 0.049 (0.014)
great 0.121 (0.016) rock 0.047 (0.014)
ftfy 0.108 (0.016) pedophilia 0.046 (0.014)
</table>
<tableCaption confidence="0.987316">
Table 6: Top 10 NNP × subreddit features from the atheism
and Christianity subreddits.
</tableCaption>
<table confidence="0.975492833333333">
progressive conservative
feature weight feature weight
american (+) 0.045 (0.023) mr (+) 0.041 (0.021)
yay (+) 0.042 (0.022) cruz (+) 0.040 (0.021)
ollie (+) 0.036 (0.019) king (+) 0.036 (0.019)
north (+) 0.036 (0.019) onion (+) 0.035 (0.018)
fuck (+) 0.034 (0.018) russia (+) 0.034 (0.018)
washington (+) 0.034 (0.018) oprah (+) 0.030 (0.016)
times* (+) 0.034 (0.018) science (+) 0.027 (0.015)
world (+) 0.030 (0.016) math (+) 0.027 (0.015)
magic (+) 0.024 (0.013) america (+) 0.026 (0.014)
where (+) 0.024 (0.013) ben (+) 0.020 (0.011)
</table>
<tableCaption confidence="0.98317425">
Table 7: Average weights for top 10 NNP × subreddit ×
sentiment features. The parenthetical ‘+’ indicates that the
inferred sentiment was positive. In general, (ostensibly) pos-
itive sentiment indicates irony.
</tableCaption>
<bodyText confidence="0.999752894736842">
sentences with subreddits and the inferred senti-
ment for the political corpus (Table 7). This would
imply, e.g., that if a sentence in the progressive
subreddit conveys an ostensibly positive sentiment
about the political commentator ‘Ollie’,4 then this
sentence is likely to have been intended ironically.
Some of these may seem counter-intuitive, such
as ostensibly positive sentiment regarding ‘Cruz’
(as in the conservative senator Ted Cruz) in the
conservative subreddit. On inspection of the com-
ments, it would seem Ted Cruz does not find
general support even in this community. Exam-
ple comments include: “Stay classy Ted Cruz”
and “Great idea on the talkathon Cruz”. The
‘mr’ and ‘king’ terms are almost exclusively ref-
erences to Obama in the conservative subreddit.
In any case, because these are three-way interac-
tion terms, they are all relatively rare: therefore we
would caution against over interpretation here.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.991539894736842">
The task of automated irony detection has recently
received a great deal of attention from the NLP and
ML communities (Tepperman et al., 2006; Davi-
dov et al., 2010; Carvalho et al., 2009; Burfoot and
Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´a˜nez
et al., 2011; Filatova, 2012; Reyes et al., 2012;
Lukin and Walker, 2013; Riloff et al., 2013). This
work has mostly focussed on exploiting token-
4‘Ollie’ is a conservative political commentator.
based indicators of verbal irony. For example, it
is clear that gratuitous punctuation (e.g. “oh re-
ally??!!!”) signals irony (Carvalho et al., 2009).
Davidov et al. (2010) proposed a semi-
supervised approach in which they look for sen-
tence templates indicative of irony. Elsewhere,
Riloff et al. (2013) proposed a method that ex-
ploits apparently contrasting sentiment in the same
utterance to detect irony. While innovative, these
approaches still rely on features intrinsic to com-
ments; i.e., they do not attempt to capitalize on
contextualizing features external to the comment
text. This means that there will necessarily be cer-
tain (subtle) ironies that escape detection by such
approaches. For example, without any additional
information about the speaker, it would be impos-
sible to deduce whether the comment “Obamacare
is a great program” is intended sarcastically.
Other related recent work has shown the
promise of sparse models, both for prediction and
interpretation (Eisenstein et al., 2011a; Eisenstein
et al., 2011b; Yogatama and Smith, 2014a). Yo-
gatama (2014a; 2014b), e.g., has leveraged the
group lasso approach to impose ‘structured’ spar-
sity on feature weights. Our work here may simi-
larly be viewed as assuming a specific sparsity pat-
tern (specifically that feature weights for ‘interac-
tion features’ will be sparse) and expressing this
via regularization.
</bodyText>
<sectionHeader confidence="0.997274" genericHeader="conclusions">
7 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999938736842105">
We have shown that we can leverage contextual-
izing information to improve identification of ver-
bal irony in online comments. This is in contrast
to previous models, which have relied predomi-
nantly on features that are intrinsic to the texts
to be classified. We exploited features that indi-
cate user communities crossed with sentiment and
extracted noun phrases. This led to consistently
improved recall with little to no cost in precision.
We also proposed a novel composite regulariza-
tion strategy that imposes a sparsifying Ei penalty
on the interaction features, as we expect most of
these to be irrelevant. This reduced performance
variance.
Future work will include expanding the corpus
and experimenting with datasets outside of the po-
litical domain. We also plan to evaluate this strat-
egy on data from different online sources, e.g.,
Twitter or YouTube.
</bodyText>
<page confidence="0.991399">
1042
</page>
<sectionHeader confidence="0.928084" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9925038">
This work was supported by ARO grant W911NF-
14-1-0442.
A Reyes, P Rosso, and T Veale. 2012. A multidimen-
sional approach for detecting irony in twitter. LREC,
pages 1–30.
</bodyText>
<sectionHeader confidence="0.98987" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994378554455446">
C Burfoot and T Baldwin. 2009. Automatic satire de-
tection: are you having a laugh? In ACL-IJCNLP,
pages 161–164. ACL.
P Carvalho, L Sarmento, MJ Silva, and E de Oliveira.
2009. Clues for detecting irony in user-generated
contents: oh...!! it’s so easy;-). In CIKM workshop
on Topic-sentiment analysis for mass opinion, pages
53–56. ACM.
HH Clark and RJ Gerrig. 1984. On the pretense the-
ory of irony. Journal of Experimental Psychology,
113:121–126.
D Davidov, O Tsur, and A Rappoport. 2010. Semi-
supervised recognition of sarcastic sentences in twit-
ter and amazon. Conference on Natural Language
Learning (CoNLL), page 107.
J Eisenstein, A Ahmed, and EP Xing. 2011a. Sparse
additive generative models of text. In International
Conference on Machine Learning (ICML).
J Eisenstein, NA Smith, and EP Xing. 2011b. Dis-
covering sociolinguistic associations with structured
sparsity. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL), pages 1365–1374.
E Filatova. 2012. Irony and sarcasm: Corpus gener-
ation and analysis using crowdsourcing. In LREC,
volume 12, pages 392–398.
R Gonz´alez-Ib´a˜nez, S Muresan, and N Wacholder.
2011. Identifying sarcasm in twitter: a closer look.
In ACL, volume 2, pages 581–586. Citeseer.
HP Grice. 1975. Logic and conversation. 1975, pages
41–58.
T Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. Springer.
S Lukin and M Walker. 2013. Really? well. ap-
parently bootstrapping improves the performance of
sarcasm and nastiness classifiers for online dialogue.
NAACL, pages 30–40.
F Pedregosa, G Varoquaux, A Gramfort, V Michel,
B Thirion, O Grisel, M Blondel, P Prettenhofer,
R Weiss, V Dubourg, J Vanderplas, A Passos,
D Cournapeau, M Brucher, M Perrot, and E Duch-
esnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research,
12:2825–2830.
E Riloff, A Qadir, P Surve, LD Silva, N Gilbert, and
R Huang. 2013. Sarcasm as contrast between a pos-
itive sentiment and negative situation. In EMNLP,
pages 704–714.
R Socher, A Perelygin, JY Wu, J Chuang, CD Man-
ning, AY Ng, and C Potts. 2013. Recursive deep
models for semantic compositionality over a senti-
ment treebank. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642. Citeseer.
D Sperber and D Wilson. 1981. Irony and the use-
mention distinction. 1981.
J Tepperman, D Traum, and S Narayanan. 2006.
”Yeah Right”: Sarcasm Recognition for Spoken Di-
alogue Systems.
K Toutanova, D Klein, CD Manning, and Y Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of the
2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 173–
180. Association for Computational Linguistics.
O Tsur, D Davidov, and A Rappoport. 2010. ICWSM-
a great catchy name: Semi-supervised recognition
of sarcastic sentences in online product reviews. In
AAAI Conference on Weblogs and Social Media.
Y Tsuruoka, J Tsujii, and S Ananiadou. 2009.
Stochastic gradient descent training for l1-
regularized log-linear models with cumulative
penalty. In Proceedings of the Joint Conference of
the Annual Meeting of the ACL and the International
Joint Conference on Natural Language Processing
of the AFNLP, pages 477–485. Association for
Computational Linguistics.
BC Wallace, DK Choe, L Kertz, and E Charniak. 2014.
Humans require context to infer ironic intent (so
computers probably do, too). Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 512–516.
BC Wallace. 2013. Computational irony: A survey
and new perspectives. Artificial Intelligence Review,
pages 1–17.
D Yogatama and NA Smith. 2014a. Linguistic struc-
tured sparsity in text categorization. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 786–796.
D Yogatama and NA Smith. 2014b. Making the most
of bag of words: Sentence regularization with alter-
nating direction method of multipliers. In Proceed-
ings of The 31st International Conference on Ma-
chine Learning, pages 656–664.
1043
H Zou and T Hastie. 2005. Regularization and variable
selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodol-
ogy), 67(2):301–320.
</reference>
<page confidence="0.993561">
1044
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.529990">
<title confidence="0.9997915">Sparse, Contextually Informed Models for Irony Detection: Exploiting User Communities, Entities and Sentiment</title>
<author confidence="0.994601">C Wallace Do Kook Choe Charniak</author>
<affiliation confidence="0.999889">University of Texas at Austin Brown University</affiliation>
<abstract confidence="0.967016642857143">Automatically detecting verbal irony (roughly, sarcasm) in online content is important for many practical applications (e.g., sentiment detection), but it is difficult. Previous approaches have relied predominantly on signal gleaned from word counts and grammatical cues. But approaches fail to exploit the in which comments are embedded. We thus propose a novel strategy for verbal irony classification that exploits contextual features, specifically by combining noun phrases and sentiment extracted from comments with the forum type (e.g., conservative or liberal) to which they were posted. We show that this approach improves verbal irony classification performance. Furthermore, because this method generates a very large feature space (and we expect predictive contextual features to be strong but few), we propose a mixed regularization strategy that places sparsity-inducing on the feature weights on top of the penalty applied to all model coefficients. This increases model sparsity and reduces the variance of model performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Burfoot</author>
<author>T Baldwin</author>
</authors>
<title>Automatic satire detection: are you having a laugh? In</title>
<date>2009</date>
<booktitle>ACL-IJCNLP,</booktitle>
<pages>161--164</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="30435" citStr="Burfoot and Baldwin, 2009" startWordPosition="4839" endWordPosition="4842">uz does not find general support even in this community. Example comments include: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´a˜nez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentimen</context>
</contexts>
<marker>Burfoot, Baldwin, 2009</marker>
<rawString>C Burfoot and T Baldwin. 2009. Automatic satire detection: are you having a laugh? In ACL-IJCNLP, pages 161–164. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Carvalho</author>
<author>L Sarmento</author>
<author>MJ Silva</author>
<author>E de Oliveira</author>
</authors>
<title>Clues for detecting irony in user-generated contents: oh...!! it’s so easy;-).</title>
<date>2009</date>
<booktitle>In CIKM workshop on Topic-sentiment analysis for mass opinion,</booktitle>
<pages>53--56</pages>
<publisher>ACM.</publisher>
<marker>Carvalho, Sarmento, Silva, de Oliveira, 2009</marker>
<rawString>P Carvalho, L Sarmento, MJ Silva, and E de Oliveira. 2009. Clues for detecting irony in user-generated contents: oh...!! it’s so easy;-). In CIKM workshop on Topic-sentiment analysis for mass opinion, pages 53–56. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>HH Clark</author>
<author>RJ Gerrig</author>
</authors>
<title>On the pretense theory of irony.</title>
<date>1984</date>
<journal>Journal of Experimental Psychology,</journal>
<pages>113--121</pages>
<contexts>
<context position="6913" citStr="Clark and Gerrig, 1984" startWordPosition="1069" endWordPosition="1072">d). Such information is often available (or can be derived) for comments posted to different mediums on the web: for example on Twitter we know who a user follows; and on YouTube we know the channels to which videos belong. 2 Exploiting context 2.1 Communities and sentiment As discussed above, a shortcoming with existing models for detecting sarcasm/verbal irony on the web is their failure to capitalize on contextualizing information. But such information is critical to discerning irony. A large body of work on the use and interpretation of verbal irony supports this supposition (Grice, 1975; Clark and Gerrig, 1984; Wallace, 2013; Wallace et al., 2014). Individuals will be more likely, in general, to use sarcasm when discussing specific entities. Which entities will depend in part on the community to which the individual belongs. As a proxy for user community, here we leverage the subreddits to which comments were posted. Sentiment may also play an important role. In general, verbal irony is almost always used to convey negative views via ostensibly positive utterances (Sperber and Wilson, 1981). And recent work (Riloff et al., 2013) has exploited features based on sentiment to improve irony detection. </context>
</contexts>
<marker>Clark, Gerrig, 1984</marker>
<rawString>HH Clark and RJ Gerrig. 1984. On the pretense theory of irony. Journal of Experimental Psychology, 113:121–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>O Tsur</author>
<author>A Rappoport</author>
</authors>
<title>Semisupervised recognition of sarcastic sentences</title>
<date>2010</date>
<booktitle>in twitter and amazon. Conference on Natural Language Learning (CoNLL),</booktitle>
<pages>107</pages>
<contexts>
<context position="30385" citStr="Davidov et al., 2010" startWordPosition="4830" endWordPosition="4834">pection of the comments, it would seem Ted Cruz does not find general support even in this community. Example comments include: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´a˜nez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a me</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>D Davidov, O Tsur, and A Rappoport. 2010. Semisupervised recognition of sarcastic sentences in twitter and amazon. Conference on Natural Language Learning (CoNLL), page 107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>A Ahmed</author>
<author>EP Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="31671" citStr="Eisenstein et al., 2011" startWordPosition="5030" endWordPosition="5033"> utterance to detect irony. While innovative, these approaches still rely on features intrinsic to comments; i.e., they do not attempt to capitalize on contextualizing features external to the comment text. This means that there will necessarily be certain (subtle) ironies that escape detection by such approaches. For example, without any additional information about the speaker, it would be impossible to deduce whether the comment “Obamacare is a great program” is intended sarcastically. Other related recent work has shown the promise of sparse models, both for prediction and interpretation (Eisenstein et al., 2011a; Eisenstein et al., 2011b; Yogatama and Smith, 2014a). Yogatama (2014a; 2014b), e.g., has leveraged the group lasso approach to impose ‘structured’ sparsity on feature weights. Our work here may similarly be viewed as assuming a specific sparsity pattern (specifically that feature weights for ‘interaction features’ will be sparse) and expressing this via regularization. 7 Conclusions and Future Directions We have shown that we can leverage contextualizing information to improve identification of verbal irony in online comments. This is in contrast to previous models, which have relied predom</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>J Eisenstein, A Ahmed, and EP Xing. 2011a. Sparse additive generative models of text. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>NA Smith</author>
<author>EP Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1365--1374</pages>
<contexts>
<context position="31671" citStr="Eisenstein et al., 2011" startWordPosition="5030" endWordPosition="5033"> utterance to detect irony. While innovative, these approaches still rely on features intrinsic to comments; i.e., they do not attempt to capitalize on contextualizing features external to the comment text. This means that there will necessarily be certain (subtle) ironies that escape detection by such approaches. For example, without any additional information about the speaker, it would be impossible to deduce whether the comment “Obamacare is a great program” is intended sarcastically. Other related recent work has shown the promise of sparse models, both for prediction and interpretation (Eisenstein et al., 2011a; Eisenstein et al., 2011b; Yogatama and Smith, 2014a). Yogatama (2014a; 2014b), e.g., has leveraged the group lasso approach to impose ‘structured’ sparsity on feature weights. Our work here may similarly be viewed as assuming a specific sparsity pattern (specifically that feature weights for ‘interaction features’ will be sparse) and expressing this via regularization. 7 Conclusions and Future Directions We have shown that we can leverage contextualizing information to improve identification of verbal irony in online comments. This is in contrast to previous models, which have relied predom</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>J Eisenstein, NA Smith, and EP Xing. 2011b. Discovering sociolinguistic associations with structured sparsity. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1365–1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filatova</author>
</authors>
<title>Irony and sarcasm: Corpus generation and analysis using crowdsourcing.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<volume>12</volume>
<pages>392--398</pages>
<contexts>
<context position="30503" citStr="Filatova, 2012" startWordPosition="4851" endWordPosition="4852">de: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´a˜nez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentiment in the same utterance to detect irony. While innovative, these app</context>
</contexts>
<marker>Filatova, 2012</marker>
<rawString>E Filatova. 2012. Irony and sarcasm: Corpus generation and analysis using crowdsourcing. In LREC, volume 12, pages 392–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gonz´alez-Ib´a˜nez</author>
<author>S Muresan</author>
<author>N Wacholder</author>
</authors>
<title>Identifying sarcasm in twitter: a closer look.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<volume>2</volume>
<pages>581--586</pages>
<publisher>Citeseer.</publisher>
<marker>Gonz´alez-Ib´a˜nez, Muresan, Wacholder, 2011</marker>
<rawString>R Gonz´alez-Ib´a˜nez, S Muresan, and N Wacholder. 2011. Identifying sarcasm in twitter: a closer look. In ACL, volume 2, pages 581–586. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>HP Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<pages>41--58</pages>
<contexts>
<context position="6889" citStr="Grice, 1975" startWordPosition="1067" endWordPosition="1068">ent was posted). Such information is often available (or can be derived) for comments posted to different mediums on the web: for example on Twitter we know who a user follows; and on YouTube we know the channels to which videos belong. 2 Exploiting context 2.1 Communities and sentiment As discussed above, a shortcoming with existing models for detecting sarcasm/verbal irony on the web is their failure to capitalize on contextualizing information. But such information is critical to discerning irony. A large body of work on the use and interpretation of verbal irony supports this supposition (Grice, 1975; Clark and Gerrig, 1984; Wallace, 2013; Wallace et al., 2014). Individuals will be more likely, in general, to use sarcasm when discussing specific entities. Which entities will depend in part on the community to which the individual belongs. As a proxy for user community, here we leverage the subreddits to which comments were posted. Sentiment may also play an important role. In general, verbal irony is almost always used to convey negative views via ostensibly positive utterances (Sperber and Wilson, 1981). And recent work (Riloff et al., 2013) has exploited features based on sentiment to i</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>HP Grice. 1975. Logic and conversation. 1975, pages 41–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<publisher>Springer.</publisher>
<contexts>
<context position="11623" citStr="Joachims, 1998" startWordPosition="1847" endWordPosition="1848">ssing the relative emphasis placed on achieving minimum empirical loss versus producing a simple model (i.e., a weight vector with small weights). Typically one searches for a good α using the available training data. For G, we will use the log-loss in this work, though other loss functions may be used in its place. 3.2 Sparsity via Regularization Concerning R, one popular regularization function is the squared E2 norm: I: w2 (2) j j This is the norm used in the standard Support Vector Machine (SVM) formulation, for example, and has been shown empirically to work well for text classification (Joachims, 1998). An alternative is to use the E1 norm: I: |wj |(3) j Which has the advantage of inducing sparse models: i.e., using the E1 norm as a penalty tends to drive feature weights to 0. Returning to the present task of detecting verbal irony in comments, it seems reasonable to assume that there will be a relatively small set of entities that correlate with sarcasm. But because we are introducing ‘interaction’ features that enumerate the cross-product of subreddits and entities (and, in some cases, sentiment), we have a large feature-space. This space includes features that correspond to NNPs extracte</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lukin</author>
<author>M Walker</author>
</authors>
<title>Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue.</title>
<date>2013</date>
<journal>NAACL,</journal>
<pages>30--40</pages>
<contexts>
<context position="1670" citStr="Lukin and Walker, 2013" startWordPosition="237" endWordPosition="240">ntextual features to be strong but few), we propose a mixed regularization strategy that places a sparsity-inducing E1 penalty on the contextual feature weights on top of the E2 penalty applied to all model coefficients. This increases model sparsity and reduces the variance of model performance. 1 Introduction and Motivation Automated verbal irony detection is a challenging problem.1 But recognizing when an author has intended a statement ironically is practically important for many text classification tasks (e.g., sentiment detection). Previous models for irony detection (Tsur et al., 2010; Lukin and Walker, 2013; Riloff et al., 1In this paper we will be a bit cavalier in using the terms ‘verbal irony’ and ‘sarcasm’ interchangeably. We recognize that the latter is a special type of the former, the definition of which is difficult to pin down precisely. Figure 1: A reddit comment illustrating contextualizing features that we propose leveraging to improve classification. Here the highlighted entities (external the comment text itself) provide contextual signals indicating that the shown comment was intended ironically. As we shall see, Obamacare is in general a strong indicator of irony when present in </context>
<context position="30547" citStr="Lukin and Walker, 2013" startWordPosition="4857" endWordPosition="4860">t idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´a˜nez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentiment in the same utterance to detect irony. While innovative, these approaches still rely on features intrinsic to </context>
</contexts>
<marker>Lukin, Walker, 2013</marker>
<rawString>S Lukin and M Walker. 2013. Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue. NAACL, pages 30–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="16395" citStr="Pedregosa et al., 2011" startWordPosition="2632" endWordPosition="2635">labeled ‘ironic’. The test portion of the political dataset comprises 996 annotated comments (409 progressive and 587 conservative comments), totalling 2,884 sentences. Using the same criteria as above – at least 2/3 annotators labeling a given sentence as ‘ironic’ – we have 154 ‘ironic’ sentences (again about 5%). The ‘religion’ dataset (comments from atheism and Christianity) contains 1,682 labeled comments comprising 5615 sentences (2,966 and 2,649 from the atheism and Christian subreddits, respectively); 313 (-6%) were deemed ‘ironic’. 3We have implemented this within the sklearn package (Pedregosa et al., 2011). |wk|+ X |wl |(4) lET argmin Xn W i=1 Xα0 j 1038 4.2 Experimental Details We recorded results from 500 independently performed experiments on random train (80%)/test (20%) splits of the data. These splits were performed at the comment (rather than sentence) level, so as not to test on sentences belonging to comments encountered in the training set. We measured performance, however, at the sentence level (often only a single sentence in a given comment will have been labeled as ‘ironic’). Our baseline approach is a standard squared-E2 regularized log-loss linear model (fit via SGD) that levera</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, and E Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>A Qadir</author>
<author>P Surve</author>
<author>LD Silva</author>
<author>N Gilbert</author>
<author>R Huang</author>
</authors>
<title>Sarcasm as contrast between a positive sentiment and negative situation.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>704--714</pages>
<contexts>
<context position="7442" citStr="Riloff et al., 2013" startWordPosition="1157" endWordPosition="1160">terpretation of verbal irony supports this supposition (Grice, 1975; Clark and Gerrig, 1984; Wallace, 2013; Wallace et al., 2014). Individuals will be more likely, in general, to use sarcasm when discussing specific entities. Which entities will depend in part on the community to which the individual belongs. As a proxy for user community, here we leverage the subreddits to which comments were posted. Sentiment may also play an important role. In general, verbal irony is almost always used to convey negative views via ostensibly positive utterances (Sperber and Wilson, 1981). And recent work (Riloff et al., 2013) has exploited features based on sentiment to improve irony detection. To summarize: when assuming an ironic voice we expect that individuals will convey ostensibly positive sentiment about entities, and that these entities will depend on the type of individual in question. We propose capitalizing on such information by introducing features that encode subreddits, sentiment and noun phrases (NNPs), as we describe next. 2.2 Features We leverage the feature sets enumerated in Table 1. Subreddits are observed variables. Noun phrase (NNP) extraction and sentiment inference are performed automatica</context>
<context position="30569" citStr="Riloff et al., 2013" startWordPosition="4861" endWordPosition="4864">Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´a˜nez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentiment in the same utterance to detect irony. While innovative, these approaches still rely on features intrinsic to comments; i.e., they d</context>
</contexts>
<marker>Riloff, Qadir, Surve, Silva, Gilbert, Huang, 2013</marker>
<rawString>E Riloff, A Qadir, P Surve, LD Silva, N Gilbert, and R Huang. 2013. Sarcasm as contrast between a positive sentiment and negative situation. In EMNLP, pages 704–714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>A Perelygin</author>
<author>JY Wu</author>
<author>J Chuang</author>
<author>CD Manning</author>
<author>AY Ng</author>
<author>C Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="8158" citStr="Socher et al., 2013" startWordPosition="1270" endWordPosition="1273">ng an ironic voice we expect that individuals will convey ostensibly positive sentiment about entities, and that these entities will depend on the type of individual in question. We propose capitalizing on such information by introducing features that encode subreddits, sentiment and noun phrases (NNPs), as we describe next. 2.2 Features We leverage the feature sets enumerated in Table 1. Subreddits are observed variables. Noun phrase (NNP) extraction and sentiment inference are performed automatically via state of the art NLP tools. In particular, we use the Stanford Sentiment Analysis tool (Socher et al., 2013) to infer sentiment. To extract NNPs we use the Stanford 1036 Feature Description The inferred sentiment (negative/neutral or positive) for a given comment. the subreddit (e.g., progressive or conservative; atheism or Christianity) to which a comment was posted. Noun phrases (e.g., proper nouns) extracted from comment texts. Noun phrases extracted from comment texts and the thread to which they belong (for example, ‘Obamacare’ from the title in Figure 1). 3 Enforcing sparsity 3.1 Preliminaries In this work we consider linear models with binary outputs (y E {−1,+1}). We will assume we have acce</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>R Socher, A Perelygin, JY Wu, J Chuang, CD Manning, AY Ng, and C Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sperber</author>
<author>D Wilson</author>
</authors>
<title>Irony and the usemention distinction.</title>
<date>1981</date>
<contexts>
<context position="7403" citStr="Sperber and Wilson, 1981" startWordPosition="1150" endWordPosition="1153">rony. A large body of work on the use and interpretation of verbal irony supports this supposition (Grice, 1975; Clark and Gerrig, 1984; Wallace, 2013; Wallace et al., 2014). Individuals will be more likely, in general, to use sarcasm when discussing specific entities. Which entities will depend in part on the community to which the individual belongs. As a proxy for user community, here we leverage the subreddits to which comments were posted. Sentiment may also play an important role. In general, verbal irony is almost always used to convey negative views via ostensibly positive utterances (Sperber and Wilson, 1981). And recent work (Riloff et al., 2013) has exploited features based on sentiment to improve irony detection. To summarize: when assuming an ironic voice we expect that individuals will convey ostensibly positive sentiment about entities, and that these entities will depend on the type of individual in question. We propose capitalizing on such information by introducing features that encode subreddits, sentiment and noun phrases (NNPs), as we describe next. 2.2 Features We leverage the feature sets enumerated in Table 1. Subreddits are observed variables. Noun phrase (NNP) extraction and senti</context>
</contexts>
<marker>Sperber, Wilson, 1981</marker>
<rawString>D Sperber and D Wilson. 1981. Irony and the usemention distinction. 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tepperman</author>
<author>D Traum</author>
<author>S Narayanan</author>
</authors>
<title>Yeah Right”: Sarcasm Recognition for Spoken Dialogue Systems.</title>
<date>2006</date>
<contexts>
<context position="30363" citStr="Tepperman et al., 2006" startWordPosition="4826" endWordPosition="4829">vative subreddit. On inspection of the comments, it would seem Ted Cruz does not find general support even in this community. Example comments include: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´a˜nez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al</context>
</contexts>
<marker>Tepperman, Traum, Narayanan, 2006</marker>
<rawString>J Tepperman, D Traum, and S Narayanan. 2006. ”Yeah Right”: Sarcasm Recognition for Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>CD Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9266" citStr="Toutanova et al., 2003" startWordPosition="1459" endWordPosition="1462">Preliminaries In this work we consider linear models with binary outputs (y E {−1,+1}). We will assume we have access to a training dataset comprising n instances, x = {x1, ..., xn} and associated labels y = {y1, ..., yn}. We then aim to find a weightvector w that optimizes the following objective. Sentiment Subreddit NNP NNP+ Table 1: Feature types that we exploit. We view the (ob- argmin n G(sign{w · xi}, yi) + αR(w) (1) served) subreddit as a proxy for user type. We combine this w i=1 with sentiment and extracted noun phrases (NNPs) to improve classifier performance. Part of Speech tagger (Toutanova et al., 2003). We then introduce ‘bag-of-NNP’ features and features that indicate whether the sentiment inferred for a given sentence was positive or not. Additionally, we introduce ‘interaction’ features that capture combinations of these. For example, a feature that indicates whether a given sentence mentions Obamacare (which will be one of many NNPs automatically extracted) and was posted in the conservative subreddit. This is an example of a two-way interaction. We also experiment with three-way interactions, crossing sentiment with NNPs and subreddits. An example is a feature that indicates if a sente</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K Toutanova, D Klein, CD Manning, and Y Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 173– 180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Tsur</author>
<author>D Davidov</author>
<author>A Rappoport</author>
</authors>
<title>ICWSMa great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews.</title>
<date>2010</date>
<booktitle>In AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="1646" citStr="Tsur et al., 2010" startWordPosition="233" endWordPosition="236">xpect predictive contextual features to be strong but few), we propose a mixed regularization strategy that places a sparsity-inducing E1 penalty on the contextual feature weights on top of the E2 penalty applied to all model coefficients. This increases model sparsity and reduces the variance of model performance. 1 Introduction and Motivation Automated verbal irony detection is a challenging problem.1 But recognizing when an author has intended a statement ironically is practically important for many text classification tasks (e.g., sentiment detection). Previous models for irony detection (Tsur et al., 2010; Lukin and Walker, 2013; Riloff et al., 1In this paper we will be a bit cavalier in using the terms ‘verbal irony’ and ‘sarcasm’ interchangeably. We recognize that the latter is a special type of the former, the definition of which is difficult to pin down precisely. Figure 1: A reddit comment illustrating contextualizing features that we propose leveraging to improve classification. Here the highlighted entities (external the comment text itself) provide contextual signals indicating that the shown comment was intended ironically. As we shall see, Obamacare is in general a strong indicator o</context>
<context position="30454" citStr="Tsur et al., 2010" startWordPosition="4843" endWordPosition="4846">pport even in this community. Example comments include: “Stay classy Ted Cruz” and “Great idea on the talkathon Cruz”. The ‘mr’ and ‘king’ terms are almost exclusively references to Obama in the conservative subreddit. In any case, because these are three-way interaction terms, they are all relatively rare: therefore we would caution against over interpretation here. 6 Related Work The task of automated irony detection has recently received a great deal of attention from the NLP and ML communities (Tepperman et al., 2006; Davidov et al., 2010; Carvalho et al., 2009; Burfoot and Baldwin, 2009; Tsur et al., 2010; Gonz´alez-Ib´a˜nez et al., 2011; Filatova, 2012; Reyes et al., 2012; Lukin and Walker, 2013; Riloff et al., 2013). This work has mostly focussed on exploiting token4‘Ollie’ is a conservative political commentator. based indicators of verbal irony. For example, it is clear that gratuitous punctuation (e.g. “oh really??!!!”) signals irony (Carvalho et al., 2009). Davidov et al. (2010) proposed a semisupervised approach in which they look for sentence templates indicative of irony. Elsewhere, Riloff et al. (2013) proposed a method that exploits apparently contrasting sentiment in the same utter</context>
</contexts>
<marker>Tsur, Davidov, Rappoport, 2010</marker>
<rawString>O Tsur, D Davidov, and A Rappoport. 2010. ICWSMa great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews. In AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
<author>S Ananiadou</author>
</authors>
<title>Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Annual Meeting of the ACL and the International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>477--485</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14680" citStr="Tsuruoka et al. (2009)" startWordPosition="2363" endWordPosition="2366">n’ feature weights, which is in contrast to the elastic net, which imposes the composite penalty to all feature weights. One can view this as using the regularizer to encourage a sparsity pattern specific to the task at hand. 2Note that we apply both 21 and 22 penalties to the features in I and T. 3.3 Inference We fit this model via Stochastic Gradient Descent (SGD).3 During each update, we impose both the squared E2 and E1 penalties; the latter is applied only to the contextual/interaction features in Z and T . For the E1 penalty, we adopt the cumulative truncated gradient method proposed by Tsuruoka et al. (2009). 4 Experimental Setup 4.1 Datasets For our development dataset, we used a subset of the reddit irony corpus (Wallace et al., 2014) comprising annotated comments from the progressive and conservative subreddits. We also report results from experiments performed using a separate, held-out portion of this data, which we did not use during model refinement. Furthermore, we later present results on comments from the atheism and Christianity subreddits (we did not use this data during model development, either). The development dataset includes 1,825 annotated comments (876 and 949 from the progres</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Y Tsuruoka, J Tsujii, and S Ananiadou. 2009. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In Proceedings of the Joint Conference of the Annual Meeting of the ACL and the International Joint Conference on Natural Language Processing of the AFNLP, pages 477–485. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BC Wallace</author>
<author>DK Choe</author>
<author>L Kertz</author>
<author>E Charniak</author>
</authors>
<title>Humans require context to infer ironic intent (so computers probably do, too).</title>
<date>2014</date>
<booktitle>Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>512--516</pages>
<contexts>
<context position="6951" citStr="Wallace et al., 2014" startWordPosition="1075" endWordPosition="1078"> (or can be derived) for comments posted to different mediums on the web: for example on Twitter we know who a user follows; and on YouTube we know the channels to which videos belong. 2 Exploiting context 2.1 Communities and sentiment As discussed above, a shortcoming with existing models for detecting sarcasm/verbal irony on the web is their failure to capitalize on contextualizing information. But such information is critical to discerning irony. A large body of work on the use and interpretation of verbal irony supports this supposition (Grice, 1975; Clark and Gerrig, 1984; Wallace, 2013; Wallace et al., 2014). Individuals will be more likely, in general, to use sarcasm when discussing specific entities. Which entities will depend in part on the community to which the individual belongs. As a proxy for user community, here we leverage the subreddits to which comments were posted. Sentiment may also play an important role. In general, verbal irony is almost always used to convey negative views via ostensibly positive utterances (Sperber and Wilson, 1981). And recent work (Riloff et al., 2013) has exploited features based on sentiment to improve irony detection. To summarize: when assuming an ironic </context>
<context position="14811" citStr="Wallace et al., 2014" startWordPosition="2385" endWordPosition="2388">w this as using the regularizer to encourage a sparsity pattern specific to the task at hand. 2Note that we apply both 21 and 22 penalties to the features in I and T. 3.3 Inference We fit this model via Stochastic Gradient Descent (SGD).3 During each update, we impose both the squared E2 and E1 penalties; the latter is applied only to the contextual/interaction features in Z and T . For the E1 penalty, we adopt the cumulative truncated gradient method proposed by Tsuruoka et al. (2009). 4 Experimental Setup 4.1 Datasets For our development dataset, we used a subset of the reddit irony corpus (Wallace et al., 2014) comprising annotated comments from the progressive and conservative subreddits. We also report results from experiments performed using a separate, held-out portion of this data, which we did not use during model refinement. Furthermore, we later present results on comments from the atheism and Christianity subreddits (we did not use this data during model development, either). The development dataset includes 1,825 annotated comments (876 and 949 from the progressive and conservative subreddits, respectively). These comprise 5,625 sentences in total, each of which was independently labeled b</context>
</contexts>
<marker>Wallace, Choe, Kertz, Charniak, 2014</marker>
<rawString>BC Wallace, DK Choe, L Kertz, and E Charniak. 2014. Humans require context to infer ironic intent (so computers probably do, too). Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 512–516.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BC Wallace</author>
</authors>
<title>Computational irony: A survey and new perspectives.</title>
<date>2013</date>
<journal>Artificial Intelligence Review,</journal>
<pages>1--17</pages>
<contexts>
<context position="6928" citStr="Wallace, 2013" startWordPosition="1073" endWordPosition="1074">often available (or can be derived) for comments posted to different mediums on the web: for example on Twitter we know who a user follows; and on YouTube we know the channels to which videos belong. 2 Exploiting context 2.1 Communities and sentiment As discussed above, a shortcoming with existing models for detecting sarcasm/verbal irony on the web is their failure to capitalize on contextualizing information. But such information is critical to discerning irony. A large body of work on the use and interpretation of verbal irony supports this supposition (Grice, 1975; Clark and Gerrig, 1984; Wallace, 2013; Wallace et al., 2014). Individuals will be more likely, in general, to use sarcasm when discussing specific entities. Which entities will depend in part on the community to which the individual belongs. As a proxy for user community, here we leverage the subreddits to which comments were posted. Sentiment may also play an important role. In general, verbal irony is almost always used to convey negative views via ostensibly positive utterances (Sperber and Wilson, 1981). And recent work (Riloff et al., 2013) has exploited features based on sentiment to improve irony detection. To summarize: w</context>
</contexts>
<marker>Wallace, 2013</marker>
<rawString>BC Wallace. 2013. Computational irony: A survey and new perspectives. Artificial Intelligence Review, pages 1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yogatama</author>
<author>NA Smith</author>
</authors>
<title>Linguistic structured sparsity in text categorization.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>786--796</pages>
<contexts>
<context position="31724" citStr="Yogatama and Smith, 2014" startWordPosition="5038" endWordPosition="5041">approaches still rely on features intrinsic to comments; i.e., they do not attempt to capitalize on contextualizing features external to the comment text. This means that there will necessarily be certain (subtle) ironies that escape detection by such approaches. For example, without any additional information about the speaker, it would be impossible to deduce whether the comment “Obamacare is a great program” is intended sarcastically. Other related recent work has shown the promise of sparse models, both for prediction and interpretation (Eisenstein et al., 2011a; Eisenstein et al., 2011b; Yogatama and Smith, 2014a). Yogatama (2014a; 2014b), e.g., has leveraged the group lasso approach to impose ‘structured’ sparsity on feature weights. Our work here may similarly be viewed as assuming a specific sparsity pattern (specifically that feature weights for ‘interaction features’ will be sparse) and expressing this via regularization. 7 Conclusions and Future Directions We have shown that we can leverage contextualizing information to improve identification of verbal irony in online comments. This is in contrast to previous models, which have relied predominantly on features that are intrinsic to the texts t</context>
</contexts>
<marker>Yogatama, Smith, 2014</marker>
<rawString>D Yogatama and NA Smith. 2014a. Linguistic structured sparsity in text categorization. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 786–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yogatama</author>
<author>NA Smith</author>
</authors>
<title>Making the most of bag of words: Sentence regularization with alternating direction method of multipliers.</title>
<date>2014</date>
<booktitle>In Proceedings of The 31st International Conference on Machine Learning,</booktitle>
<pages>656--664</pages>
<contexts>
<context position="31724" citStr="Yogatama and Smith, 2014" startWordPosition="5038" endWordPosition="5041">approaches still rely on features intrinsic to comments; i.e., they do not attempt to capitalize on contextualizing features external to the comment text. This means that there will necessarily be certain (subtle) ironies that escape detection by such approaches. For example, without any additional information about the speaker, it would be impossible to deduce whether the comment “Obamacare is a great program” is intended sarcastically. Other related recent work has shown the promise of sparse models, both for prediction and interpretation (Eisenstein et al., 2011a; Eisenstein et al., 2011b; Yogatama and Smith, 2014a). Yogatama (2014a; 2014b), e.g., has leveraged the group lasso approach to impose ‘structured’ sparsity on feature weights. Our work here may similarly be viewed as assuming a specific sparsity pattern (specifically that feature weights for ‘interaction features’ will be sparse) and expressing this via regularization. 7 Conclusions and Future Directions We have shown that we can leverage contextualizing information to improve identification of verbal irony in online comments. This is in contrast to previous models, which have relied predominantly on features that are intrinsic to the texts t</context>
</contexts>
<marker>Yogatama, Smith, 2014</marker>
<rawString>D Yogatama and NA Smith. 2014b. Making the most of bag of words: Sentence regularization with alternating direction method of multipliers. In Proceedings of The 31st International Conference on Machine Learning, pages 656–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zou</author>
<author>T Hastie</author>
</authors>
<title>Regularization and variable selection via the elastic net.</title>
<date>2005</date>
<journal>Journal of the Royal Statistical Society: Series B (Statistical Methodology),</journal>
<volume>67</volume>
<issue>2</issue>
<contexts>
<context position="13879" citStr="Zou and Hastie, 2005" startWordPosition="2222" endWordPosition="2225">with the contextual features to zero, which is desirable in light of the intuition that a relatively small number of entities will likely indicate sarcasm. At the same time, this composite penalty applies only the squared E2 norm to the standard BoW features, given the comparatively strong predictive performance realized with this strategy. Putting this together, we modify the original objective (Equation 1) as follows: G(sign{w · xil, yi)+ Xw2j + α1 X|wk|+α2 |wl |(5) kEZ lET Where we have placed separate α scalars on the respective penalty terms. Note that this is similar to the elastic net (Zou and Hastie, 2005) joint regularization and variable selection strategy. The distinction here is that we only apply the E1 penalty to (i.e., perform feature selection for) the subset of ‘interaction’ feature weights, which is in contrast to the elastic net, which imposes the composite penalty to all feature weights. One can view this as using the regularizer to encourage a sparsity pattern specific to the task at hand. 2Note that we apply both 21 and 22 penalties to the features in I and T. 3.3 Inference We fit this model via Stochastic Gradient Descent (SGD).3 During each update, we impose both the squared E2 </context>
<context position="20048" citStr="Zou and Hastie, 2005" startWordPosition="3221" endWordPosition="3224">points (a relative gain of ∼12%). And this is achieved without sacrificing precision (in contrast to exploiting only sentiment). Furthermore, as we can see in Figures 2 and 3, the proposed regularization strategy shrinks the variance of the classifier. This variance reduction is achieved through greater model sparsity, as can be seen in Figure 4, which improves interpretability. We note that leveraging only an E1 regularization penalty (with the full feature-set) results in very poor performance (median recall and precision of 0.05 and 0.09, respectively). Similarly, the elastic-net strategy (Zou and Hastie, 2005) (in which we do not specify which features to apply the E1 penalty to), here achieves a median recall of 0.11 and a median precision of 0.07. 5.2 Results on the Held-out (Test) Corpus Table 4 reports results on the held-out political test dataset, achieved after training the models on the entirety of the development corpus. To account for the variance inherent to inference via SGD, we performed 100 runs of the SGD procedure and report median results from these runs. These results mostly agree with those reported for the development corpus: the proposed strategy improves median recall on the h</context>
</contexts>
<marker>Zou, Hastie, 2005</marker>
<rawString>H Zou and T Hastie. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>