<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997146">
Pattern-Based Context-Free Grammars for Machine Translation
</title>
<author confidence="0.9965">
Koichi Takeda
</author>
<affiliation confidence="0.997869">
Tokyo Research Laboratory, IBM Research
</affiliation>
<address confidence="0.9331685">
1623-14 Shimotsuruma, Yamato, Kanagawa 242, Japan
Phone: 81-462-73-4569, 81-462-73-7413 (FAX)
</address>
<email confidence="0.967984">
takedatrl.vnet.ibm.com
</email>
<sectionHeader confidence="0.998349" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938615384615">
This paper proposes the use of &amp;quot;pattern-
based&amp;quot; context-free grammars as a basis
for building machine translation (MT) sys-
tems, which are now being adopted as per-
sonal tools by a broad range of users in
the cyberspace society. We discuss ma-
jor requirements for such tools, including
easy customization for diverse domains,
the efficiency of the translation algorithm,
and scalability (incremental improvement
in translation quality through user interac-
tion), and describe how our approach meets
these requirements.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999455130434783">
With the explosive growth of the World-Wide Web
(WWW) as information source, it has become rou-
tine for Internet users to access textual data written
in foreign languages. In Japan, for example, a dozen
or so inexpensive MT tools have recently been put
on the market to help PC users understand English
text in WWW home pages. The MT techniques em-
ployed in the tools, however, are fairly conventional.
For reasons of affordability, their designers appear
to have made no attempt to tackle the well-known
problems in MT, such as how to ensure the learnabil-
ity of correct translations and facilitate customiza-
tion. As a result, users are forced to see the same
kinds of translation errors over and over again, ex-
cept they in cases where they involve merely adding
a missing word or compound to a user dictionary, or
specifying one of several word-to-word translations
as a correct choice.
There are several alternative approaches that
might eventually liberate us from this limitation on
the usability of MT systems:
Unification-based grammar for-
malisms and lexical-semantics formalisms (see LFG
(Kaplan and Bresnan, 1982), HPSG (Pollard and
Sag, 1987), and Generative Lexicon (Pustejovsky,
1991), for example) have been proposed to facili-
tate computationally precise description of natural-
language syntax and semantics. It is possible that,
with the descriptive power of these grammars and
lexicons, individual usages of words and phrases may
be defined specifically enough to give correct trans-
lations. Practical implementation of MT systems
based on these formalisms, on the other hand, would
not be possible without much more efficient parsing
and disambiguation algorithms for these formalisms
and a method for building a lexicon that is easy even
for novices to use.
Corpus-based or example-based MT (Sato and
Nagao, 1990; Sumita and Iida, 1991) and statisti-
cal MT (Brown et al., 1993) systems provide the
easiest customizability, since users have only to sup-
ply a collection of source and target sentence pairs
(a bilingual corpus). Two open questions, however,
have yet to be satisfactorily answered before we can
confidently build commercial MT systems based on
these approaches:
</bodyText>
<listItem confidence="0.997837625">
• Can the system be used for various domains
without showing severe degradation of transla-
tion accuracy?
• What is the minimum number of examples (or
training data) required to achieve reasonable
MT quality for a new domain?
TAG-based MT (Abeille, Schabes, and Joshi,
1990)1 and pattern-based translation (Maruyama,
1993) share many important properties for successful
implementation in practical MT systems, namely:
• The existence of a polynomial-time parsing al-
gorithm
• A capability for describing a larger domain of
locality (Schabes, Abeille, and Joshi, 1988)
• Synchronization (Shieber and Schabes, 1990) of
the source and target language structures
</listItem>
<note confidence="0.360383">
Readers should note, however, that the pars-
</note>
<footnote confidence="0.89953425">
1See LTAG (Schabes, Abeille, and Josh, 1988) (Lex-
icalized TAG) and STAG (Shieber and Schabes, 1990)
(Synchronized TAG) for each member of the TAG (Tree
Adjoining Grammar) family.
</footnote>
<page confidence="0.99756">
144
</page>
<bodyText confidence="0.9998723125">
ing algorithm for TAGs has 0(107/6)2 worst case
time complexity (Vijay-Shanker, 1987), and that
the &amp;quot;patterns&amp;quot; in Maruyama&apos;s approach are merely
context-free grammar (CFG) rules. Thus, it has
been a challenge to find a framework in which we
can enjoy both a grammar formalism with better
descriptive power than CFG and more efficient pars-
ing/generation algorithms than those of TAGs.3
In this paper, we will show that there exists a
class of &amp;quot;pattern-based&amp;quot; grammars that is weakly
equivalent to CFG (thus allowing the CFG parsing
algorithms to be used for our grammars), but that
it facilitates description of the domain of locality.
Furthermore, we will show that our framework can
be extended to incorporate example-based MT and
a powerful learning mechanism.
</bodyText>
<sectionHeader confidence="0.9432055" genericHeader="method">
2 Pattern-Based Context-Free
Grammars
</sectionHeader>
<bodyText confidence="0.9912962">
Pattern-based context-free grammars (PCFG) con-
sists of a set of translation patterns. A pattern is a
pair of CFG rules, and zero or more syntactic head
and link constraints for nonterminal symbols. For
example, the English-French translation pattern&apos;
</bodyText>
<equation confidence="0.9086065">
NP:1 miss:V:2 NP:3 -- S:2
S:2 4-- NP:3 manquer:V:2 a NP:1
</equation>
<bodyText confidence="0.730507428571429">
essentially describes a synchronized5 pair consisting
of a left-hand-side English CFG rule (called a source
rule)
NP V NP —+ S
and a French CFG rule (called a target rule)
S 4- NP V a NP
accompanied by the following constraints.
</bodyText>
<listItem confidence="0.945694555555556">
1. Head constraints: The nonterminal symbol V
in the source rule must have the verb miss as a
syntactic head. The symbol V in the target rule
must have the verb man quer as a syntactic head.
The head of symbol S in the source (target) rule
is identical to the head of symbol V in the source
(target) rule as they are co-indexed.
2. Link constraints: Nonterminal symbols in
source and target CFG rules are linked if they
</listItem>
<footnote confidence="0.993513333333333">
2Where IGI stands for the size of grammar G, and n
is the length of an input string.
3Lexicalized CFG, or Tree Insertion Grammar (TIG)
(Schabes and Waters, 1995), has been recently intro-
duced to achieve such efficiency and lexicalization.
4and its inflectional variants — we will discuss inflec-
tions and agreement issues later.
5The meaning of the word &amp;quot;synchronized&amp;quot; here is ex-
actly the same as in STAG (Shieber and Schabes, 1990).
See also bilingual signs (Tsujii and Fujita, 1991) for a
discussion of the importance of combining the appropri-
ate domain of locality and synchronization.
</footnote>
<bodyText confidence="0.997040666666667">
are given the same index &amp;quot;:i&amp;quot;. Linked nonter-
minal must be derived from a sequence of syn-
chronized pairs. Thus, the first NP (NP:1) in
the source rule corresponds to the second NP
(NP:1) in the target rule, the Vs in both rules
correspond to each other, and the second NP
(NP:3) in the source rule corresponds to the first
NP (NP:3) in the target rule.
The source and target rules are called CFG skele-
ton of the pattern. The notion of a syntactic head
is similar to that used in unification grammars, al-
though the heads in our patterns are simply encoded
as character strings rather than as complex feature
structures. A head is typically introduced&apos; in preter-
minal rules such as
</bodyText>
<equation confidence="0.522179">
leave —4 V V 4— partir
</equation>
<bodyText confidence="0.997626666666667">
where two verbs, &amp;quot;leave&amp;quot; and &amp;quot;partir,&amp;quot; are associated
with the heads of the nonterminal symbol V. This is
equivalently expressed as
</bodyText>
<equation confidence="0.841581">
leave:1 —4 V:1 V:1 4-- partir:1
</equation>
<bodyText confidence="0.9985482">
which is physically implemented as an entry of an
English-French lexicon.
A set T of translation patterns is said to accept
an input s if there is a derivation sequence Q for s
using the source CFG skeletons of T, and every head
constraint associated with the CFG skeletons in Q is
satisfied. Similarly, T is said to translate s if there
is a synchronized derivation sequence Q for s such
that T accepts s, and every head and link constraint
associated with the source and target CFG skeletons
in Q is satisfied. The derivation Q then produces a
translation t as the resulting sequence of terminal
symbols included in the target CFG skeletons in Q.
Translation of an input string s essentially consists
of the following three steps:
</bodyText>
<listItem confidence="0.9976605">
1. Parsing s by using the source CFG skeletons
2. Propagating link constraints from source to tar-
get CFG skeletons to build a target CFG deriva-
tion sequence
3. Generating t from the target CFG derivation
sequence
</listItem>
<bodyText confidence="0.911303">
The third step is a trivial procedure when the target
CFG derivation is obtained.
</bodyText>
<construct confidence="0.998403">
Theorem 1 Let T be a PCFG. Then, there exists
a CFG GT such that for two languages L(T) and
L(GT) accepted by T and GT, respectively, L(T) =
L(GT) holds. That is, T accepts a sentence s iff GT
accepts s.
</construct>
<footnote confidence="0.7876975">
Proof: We can construct a CFG GT as follows:
1. GT has the same set of terminal symbols as T.
</footnote>
<bodyText confidence="0.7580655">
&apos;A nonterminal symbol X in a source or target CFG
rule X --÷ X1 • • • Xk can only be constrained to have one
of the heads in the RHS X1 • • • X. Thus, monotonicity
of head constraints holds throughout the parsing process.
</bodyText>
<page confidence="0.997011">
145
</page>
<table confidence="0.9780816">
2. For each nonterminal symbol X in T, GT in- Two CFGs G and H define the range of CFL L(T).
cludes a set of nonterminal symbols {Xy, iw is These two CFGs can be used to measure the &amp;quot;de-
either a terminal symbol in T or a special sym- fault&amp;quot; translation quality, since idioms and colloca-
bol e}. tional phrases are typically translated by patterns
3. For each preterminal rule with head constraints.
X:i wi:1 w2:2 wk:k (1 &lt; &lt; k), Theorem 2 Let a CFG G be a set of source CFG
GT includear skeletons in T. Then, L(T) C L(G) is undecidable.
Xwi w1 w2 wk (1 &lt;i &lt; k). Proof: The decision problem, L(T) C L(G), of
If X is not co-indexed with any of w, GT in- two CFLs such that L(T) C L(G) is solvable if
cludes L(T) = L(G) is solvable. This includes a known un-
X, wi w2 • • • Wk • decidable problem, L(T) = E*?, since we can choose
4. For each source CFG rule with head constraints a grammar U with L(U) = E*, nullify the entire set
(hi, h2, hk) and indexes (ii, i2, of rules in U by defining T to be a vacuous set {S:1
Y:ij hk:Xk:ik (1 &lt; j &lt; a:Sb:1, Sb:1 ---÷ b:Su:1} U U (Su and S are start
k), symbols in U and T, respectively), and, finally, let
GT includes T further include an arbitrary CFG F. L(G) = E*
Yhj Xhi Xh2 Xhk. is obvious, since G has {S Sb, Sb Su} U U.
If Y is not co-indexed with any of its children, Now, we have L(G) L(T) if L(F) E*.
we have
Y, Xhi Xh2 Xhk.
</table>
<bodyText confidence="0.990325896103897">
If Xi has no head constraint in the above rule,
GT includes a set of (N + 1) rules, where Xhj
above is replaced with Xw for every terminal
symbol w and X, (Yhi will also be replaced if
it is co-indexed with Xi) .8
Now, L(T) C L(GT) is obvious, since GT can simu-
late the derivation sequence in T with corresponding
rules in GT. L(GT) C L(T) can be proven, with
mathematical induction, from the fact that every
valid derivation sequence of GT satisfies head con-
straints of corresponding rules in T.
Proposition 1 Let a CFG G be a set of source CFG
skeletons in T. Then, L(T) C L(G).
Since a valid derivation sequence in T is always a
valid derivation sequence in G, the proof is immedi-
ate. Similarly, we have
Proposition 2 Let a CFG H be a subset of source
CFG skeletons in T such tha i a source CFG skeleton
k is in H iff k has no head constraints associated with
it. Then, L(H) C L(T).
0
Theorem 2 shows that the syntactic coverage of
T is, in general, only computable by T itself, even
though T is merely a CFL. This may pose a serious
problem when a grammar writer wishes to know if
there is a specific expression that is only acceptable
by using at least one pattern with head constraints,
for which the answer is &amp;quot;no&amp;quot; iff L(G) = L(T). One
way to trivialize this problem is to let T include a
pattern with a pair of pure CFG rules for every pat-
tern with head constraints, which guarantees that
L(H) = L(T) = L(G). In this case, we know that
the coverage of &amp;quot;default&amp;quot; patterns is always identi-
cal to L(T).
Although our &amp;quot;patterns&amp;quot; have no more theoreti-
cal descriptive power than CFG, they can provide
considerably better descriptions of the domain of lo-
cality than ordinary CFG rules. For example,
be:V:1 year:NP:2 old --+ VP:1
VP:1 avoir:V:1 an:NP:2
can handle such NP pairs as &amp;quot;one year&amp;quot; and &amp;quot;un an,&amp;quot;
and &amp;quot;more than two years&amp;quot; and &amp;quot;plus que deux ans,&amp;quot;
which would have to be covered by a large number
of plain CFG rules. TAGs, on the other hand, are
known to be &amp;quot;mildly context-sensitive&amp;quot; grammars,
and they can capture a broader range of syntactic
dependencies, such as cross-serial dependencies. The
computational complexity of parsing for TAGs, how-
ever, is 0(iGin6), which is far greater than that of
CFG parsing. Moreover, defining a new STAG rule
is not as easy for the users as just adding an entry
into a dictionary, because each STAG rule has to be
specified as a pair of tree structures. Our patterns,
on the other hand, concentrate on specifying linear
ordering of source and target constituents, and can
be written by the users as easily as9
7Head constraints are trivially satisfied or violated in
preterminal rules. Hence, we assume, without loss of
generality, that no head constraint is given in pretermi-
nal rules. We also assume that &amp;quot;X —&gt; w&amp;quot; implies &amp;quot;X:1
w:1&amp;quot;.
&apos;Therefore, a single rule in T can be mapped to as
many as (N -I-1)k rules in GT, where N is the number of
terminal symbols in T. GT could be exponentially larger
than T.
&apos;By sacrificing linguistic accuracy for the description
of syntactic structures.
146
to leave * = de quitter *
to be year:* old = d&apos;avoir an:*
Here, the wildcard &amp;quot;*&amp;quot; stands for an NP by default.
The preposition &amp;quot;to&amp;quot; and &amp;quot;de&amp;quot; are used to specify
that the patterns are for VP pairs, and &amp;quot;to be&amp;quot; is
used to show that the phrase is the BE-verb and its
complement. A wildcard can be constrained with a
head, as in &amp;quot;house:*&amp;quot; and &amp;quot;maison:*&amp;quot;. The internal
representations of these patterns are as follows:
</bodyText>
<equation confidence="0.99781425">
leave:V:1 NP:2 VP:1
VP:1 4-- quitter:V:1 NP:2
be:V:1 year:NP:2 old VP:1
VP:1 avoir:V:1 an:NP:2
</equation>
<bodyText confidence="0.999980428571429">
These patterns can be associated with an explicit
nonterminal symbol such as &amp;quot;V:*&amp;quot; or &amp;quot;ADJP:*&amp;quot; in
addition to head constraints (e.g., &amp;quot;leave:V:*&amp;quot;). By
defining a few such notations, these patterns can
be successfully converted into the formal represen-
tations defined in this section. Many of the diver-
gences (Dorr, 1993) in source and target language
expressions are fairly collocational, and can be ap-
propriately handled by using our patterns. Note
the simplicity that results from using a notation in
which users only have to specify the surface ordering
of words and phrases. More powerful grammar for-
malisms would generally require either a structural
description or complex feature structures.
</bodyText>
<sectionHeader confidence="0.974888" genericHeader="method">
3 The Translation Algorithm
</sectionHeader>
<bodyText confidence="0.999986125">
The parsing algorithm for translation patterns can
be any of known CFG parsing algorithms includ-
ing CKY and Earley algorithmsl° At this stage,
head and link constraints are ignored. It is easy
to show that the number of target charts for a sin-
gle source chart increases exponentially if we build
target charts simultaneously with source charts. For
example, the two patterns
</bodyText>
<equation confidence="0.8826785">
A:1 B:2 B:2 B:2 4-- A:1 B:2, and
A:1 B:2 B:2 A:1 4- B:2 A:1
</equation>
<bodyText confidence="0.999873">
will generate the following 2n synchronized pairs of
charts for the sequence of (n+1) nonterminal sym-
bols AAA...AB, for which no effective packing of
the target charts is possible.
</bodyText>
<equation confidence="0.948559333333333">
(A (A ... (A B))) with (A (A ... (A B)))
(A (A ... (A B))) with ((A ... (A B)) A)
(A (A ... (A B))) with (((B A) A) ... A)
</equation>
<bodyText confidence="0.997892611111111">
Our strategy is thus to find a candidate set of
source charts in polynomial time. We therefore
apply heuristic measurements to identify the most
promising patterns for generating translations. In
&apos;Our prototype implementation was based on the
Earley algorithm, since this does not require lexicaliza-
tion of CFG rules.
this sense, the entire translation algorithm is not
guaranteed to run in polynomial time. Practically, a
timeout mechanism and a process for recovery from
unsuccessful translation (e.g., applying the idea of
fitted parse (Jensen and Heidorn, 1983) to target
CFG rules) should be incorporated into the transla-
tion algorithm.
Some restrictions on patterns must be imposed
to avoid infinitely many ambiguities and arbitrarily
long translations. The following patterns are there-
fore not allowed:
</bodyText>
<listItem confidence="0.986412">
1. A —÷ X Y B
2. A X Y 4— .. • B . • •Ck
</listItem>
<bodyText confidence="0.896571388888889">
if there is a cycle of synchronized derivation such
that
and
B (or CI. ...B ...Ck) --+ Y B,
where A, B, X, and Y are nonterminal symbols with
or without head and link constraints, and C&apos;s are
either terminal or nonterminal symbols.
The basic strategy for choosing a candidate
derivation sequence from ambiguous parses is as
follows.&apos; A simplified view of the Earley algorithm
(Earley, 1970) consists of three major components,
predict(i), complete(i), and scan(i), which are called
at each position i = 0,1, ..., n in an input string I =
si Predict(i) returns a set of currently ap-
plicable CFG rules at position i. Complete(i) com-
bines inactive charts ending at i with active charts
that look for the inactive charts at position i to pro-
duce a new collection of active and inactive charts.
Scan(i) tries to combine inactive charts with the
symbol 5i+ at position i. Complete(n) gives the
set of possible parses for the input I.
Now, for every inactive chart associated with a
nonterminal symbol X for a span of (i,j) (1 &lt; j &lt;
n), there exists a set P of patterns with the source
CFG skeleton, ... X. We can define the fol-
lowing ordering of patterns in P; this gives patterns
with which we can use head and link constraints for
building target charts and translations. These can-
didate patterns can be arranged and associated with
the chart in the complete() procedure.
1. Prefer a pattern p with a source CFG skeleton
X —+ Xl • • • Xk over any other pattern q with
the same source CFG skeleton X --+ Xl• • • Xk,
such that p has a head constraint h:Xi if q has
h:Xi (i = 1,...,k). The pattern p is said to
be more specific than q. For example, p =
</bodyText>
<footnote confidence="0.7517992">
liThis strategy is similar to that of transfer-driven MT
(TDMT) (Furuse and Iida, 1994). TDMT, however, is
based on a combination of declarative/procedural knowl-
edge sources for MT, and no clear computational prop-
erties have been investigated.
</footnote>
<page confidence="0.982707">
147
</page>
<bodyText confidence="0.643442">
&amp;quot;leave:V:1 house:NP VP:1&amp;quot; is preferred to
q = &amp;quot;leave:V:1 NP --4 VP:1&amp;quot;.
</bodyText>
<listItem confidence="0.945270923076923">
2. Prefer a pattern p with a source CFG skeleton
to any pattern q that has fewer terminal sym-
bols in the source CFG skeleton than p. For
example, prefer &amp;quot;take:V:1 a walk&amp;quot; to &amp;quot;take:V:1
NP&amp;quot; if these patterns give the VP charts with
the same span.
3. Prefer a pattern p which does not violate any
head constraint over those which violate a head
constraint.
4. Prefer the shortest derivation sequence for each
input substring. A pattern for a larger domain
of locality tends to give a shorter derivation se-
quence.
</listItem>
<bodyText confidence="0.999775">
These preferences can be expressed as numeric
values (cost) for patterns.&apos; Thus, our strategy fa-
vors lexicalized (or head constrained) and colloca-
tional patterns, which is exactly what we are go-
ing to achieve with pattern-based MT. Selection of
patterns in the derivation sequence accompanies the
construction of a target chart. Link constraints are
propagated from source to target derivation trees.
This is basically a bottom-up procedure.
Since the number M of distinct pairs (X,w), for a
nonterminal symbol X and a subsequence w of input
string s, is bounded by Kn2, we can compute the m-
best choice of pattern candidates for every inactive
chart in time O(ITIKn3) as claimed by Maruyama
(Maruyama, 1993), and Schabes and Waters (Sch-
abes and Waters, 1995). Here, K is the number of
distinct nonterminal symbols in T, and n is the size
of the input string. Note that the head constraints
associated with the source CFG rules can be incor-
porated in the parsing algorithm, since the number
of triples (X,w,h), where h is a head of X, is bounded
by Kn3. We can modify the predict(), completes,
and scan() procedures to run in 0(ITIKO) while
checking the source head constraints. Construction
of the target charts, if possible, on the basis of the m
best candidate patterns for each source chart takes
0(Kn2rn) time. Here, 171 can be larger than 2 if we
generate every possible translation.
The reader should note critical differences between
lexicalized grammar rules (in the sense of LTAG and
TIG) and translation patterns when they are used
for MT.
Firstly, a pattern is not necessarily lexicalized. An
economical way of organizing translation patterns
is to include non-lexicalized patterns as &amp;quot;default&amp;quot;
translation rules.
12A similar preference can be defined for the tar-
get part of each pattern, but we found many counter-
examples, where the number of nonterminal symbols
shows no specificity of the patterns, in the target part
of English-to-Japanese translation patterns. Therefore,
only the head constraint violation in the target part is
accounted for in our prototype.
Secondly, lexicalization might increase the size of
STAG grammars (in particular, compositional gram-
mar rules such as ADJP NP --4 NP) considerably
when a large number of phrasal variations (adjec-
tives, verbs in present participle form, various nu-
meric expressions, and so on) multiplied by the num-
ber of their translations, are associated with the
ADJP part. The notion of structure sharing (Vijay-
Shanker and Schabes, 1992) may have to be ex-
tended from lexical to phrasal structures, as well as
from monolingual to bilingual structures.
Thirdly, a translation pattern can omit the tree
structure of a collocation, and leave it as just a se-
quence of terminal symbols. The simplicity of this
helps users to add patterns easily, although precise
description of syntactic dependencies is lost.
</bodyText>
<sectionHeader confidence="0.99496" genericHeader="method">
4 Features and Agreements
</sectionHeader>
<bodyText confidence="0.999518590909091">
Translation patterns can be enhanced with unifica-
tion and feature structures to give patterns addi-
tional power for describing gender, number, agree-
ment, and so on. Since the descriptive power of
unification-based grammars is considerably greater
than that of CFG (Berwick, 1982), feature struc-
tures have to be restricted to maintain the efficiency
of parsing and generation algorithms. Shieber and
Schabes briefly discuss the issue (Shieber and Sch-
abes, 1990). We can also extend translation patterns
as follows:
Each nonterminal node in a pattern can be
associated with a fixed-length vector of bi-
nary features.
This will enable us to specify such syntactic de-
pendencies as agreement and subcategorization in
patterns. Unification of binary features, however,
is much simpler: unification of a feature-value pair
succeeds only when the pair is either (0,0) or (1,1).
Since the feature vector has a fixed length, unifica-
tion of two feature vectors is performed in a constant
time. For example, the patterns13
</bodyText>
<equation confidence="0.99944825">
V:1:+TRANS NP:2 —4 VP:1 VP:1 4-
V:1:+TRANS NP:2
V:1:-FINTRANS —4 VP:1 VP:1 4—
V:1:+INTRANS
</equation>
<bodyText confidence="0.999901">
are unifiable with transitive and intransitive verbs,
respectively. We can also distinguish local and head
features, as postulated in HPSG. Simplified version
of verb sub categorization is then encoded as
</bodyText>
<equation confidence="0.833595">
VP:1:+TRANS-OBJ NP:2 --4 VP:1:+0BJ
VP:1:+0BJ VP:1:+TRANS-OBJ NP:2
</equation>
<bodyText confidence="0.9461754">
where &amp;quot;-OBJ&amp;quot; is a local feature for head VPs in
LHSs, while &amp;quot;+OBJ&amp;quot; is a local feature for VPs in
&amp;quot;Again, these patterns can be mapped to a weakly
equivalent set of CFG rules. See GPSG (Gazdar, Pul-
lum, and Sag, 1985) for more details.
</bodyText>
<page confidence="0.996804">
148
</page>
<bodyText confidence="0.9986472">
the RHSs. Unification of a local feature with +OBJ
succeeds since it is not bound.
Agreement on subjects (nominative NPs) and
finite-form verbs (VPs, excluding the BE verb) is
disjunctively specified as
</bodyText>
<equation confidence="0.984780833333333">
NP:1:+NOMI+3RD+SG VP:2:+FIN+3SG
NP:1:+NOMI+3RD+PL VP:2:+FIN-3SG
NP:1:+NOMI-3RD VP:2:+FIN-3SG
NP:1:+NOMI VP:2:+FIN+PAST
which is collectively expressed as
NP:1:*AGRS VP:2:*AGRV
</equation>
<bodyText confidence="0.999879130434783">
Here, *AGRS and *AGRV are a pair of aggregate
unification specifiers that succeeds only when one
of the above combinations of the feature values is
unifiable.
Another way to extend our grammar formalism is
to associate weights with patterns. It is then possi-
ble to rank the matching patterns according to a lin-
ear ordering of the weights rather than the pairwise
partial ordering of patterns described in the previ-
ous section. In our prototype system, each pattern
has its original weight, and according to the prefer-
ence measurement described in the previous section,
a penalty is added to the weight to give the effective
weight of the pattern in a particular context. Pat-
terns with the least weight are to be chosen as the
most preferred patterns.
Numeric weights for patterns are extremely use-
ful as means of assigning higher priorities uniformly
to user-defined patterns. Statistical training of pat-
terns can also be incorporated to calculate such
weights systematically (Fujisaki et al., 1989).
Figure 1 shows a sample translation of the input
&amp;quot;He knows me well,&amp;quot; using the following patterns.
</bodyText>
<equation confidence="0.999304">
NP:1:*AGRS VP:1:*AGRS S:1
S:1 4- NP:1:*AGRS VP:1:*AGRS ... (a)
VP:1 ADVP:2 VP:1
VP:1 4- VP:1 ADVP:2 (b)
know:VP:1:-FOBJ well VP:1
VP:1 4- connaitre:VP:1:+0BJ bien (c)
V:1 NP:2 --+ VP:1:-FOBJ
VP:1:+0BJ 4- V:1 NP:2:-PRO (d)
V:1 NP:2
VP:1:+0BJ NP:2:-I-PRO V:1 ... (e)
</equation>
<bodyText confidence="0.9996095">
To simplify the example, let us assume that we
have the following preterminal rules:
</bodyText>
<footnote confidence="0.991732375">
he NPH-PRO+NOMI+3RD+SG
NP:+PRO+NOMI+3RD+SG il (f)
me NP:d-PRO+CAUS+SG-3RD
NP:-FPRO+CAUS+SG-3RD 4- me ... (g)
knows -4 V:+FIN+3SG
sait (h)
knows -4 V:+FIN+3SG
V:-FFIN+3SG 4- connait (i)
</footnote>
<note confidence="0.899277">
Input: He knows me well
Phase 1: Source Analysis
</note>
<equation confidence="0.75320295">
[0 1] He ---&gt; (f) NP
(active arc CO 1] (a) NP.VP)
[1 2] knows ---&gt; (h) V, (i) V
(active arcs [1 2] (d) V.NP,
[1 2] (e) V.NP)
[2 3] me ---&gt; (g) NP
(inactive arcs Cl 3] (d) V NP,
Cl 3] (e) V NP)
[1. 3] knows me ---&gt; (d), (e) VP
(inactive arc CO 3] (a) NP VP,
active arcs Cl 3] (b) VP.well,
[1 3] (c) VP.ADVP)
Co 3] He knows me ---&gt; (a) S
[3 4] well ---&gt; (j) ADVP, (k) ADVP
(inactive arcs Cl 4] (b) VP ADVP,
(c) VP ADVP)
[1 4] knows me well ---&gt; (b), (c) VP
(inactive arc CO 4] (a) NP VP)
CO 4] He knows me well ---&gt; (a) S
Phase 2: Constraint Checking
[0 1] He ---&gt; (f) ip
[1 2] knows ---&gt; (i) V, (j) V
[2 3] me ---&gt; (g) NP
Cl 3] knows me ---&gt; (e) VP
(pattern (d) fails)
CO 3] He knows me ---&gt; (a) S
[3 4] well ---&gt; (i) ADVP, (j) ADVP
Ci 4] knows me well ---&gt; (b), (c) VP
(preference ordering (c), (b))
CO 4] He knows me well ---&gt; (a) S
Phase 3: Target Generation
CO 4] He knows me well ---&gt; (a) S
CO 1] He ---&gt; ii
Cl 4] knows me well ---&gt; (c) VP
well ---&gt; bien
Cl 3] knows me ---&gt; (e) VP
[1 2] knows ---&gt; connait
(h) violates a head constraint
[2 3] me ---&gt; me
Translation: il me connait bien
</equation>
<figureCaption confidence="0.751340666666667">
Figure 1: Sample Translation
well --4 ADVP ADVP 4- bien ..•
well ADVP ADVP 4- beaucoup (k)
</figureCaption>
<bodyText confidence="0.999869875">
In the above example, the Earley-based algorithm
with source CFG rules is used in Phase 1. In Phase
2, head and link constraints are examined, and unifi-
cation of feature structures is performed by using the
charts obtained in Phase 1. Candidate patterns are
ordered by their weights and preferences. Finally,
in Phase 3, the target charts are built to generate
translations based on the selected patterns.
</bodyText>
<sectionHeader confidence="0.949362" genericHeader="method">
5 Integration of Bilingual Corpora
</sectionHeader>
<bodyText confidence="0.999342666666667">
Integration of translation patterns with translation
examples, or bilingual corpora, is the most impor-
tant extension of our framework. There is no dis-
</bodyText>
<page confidence="0.99793">
149
</page>
<bodyText confidence="0.999765333333333">
crete line between patterns and bilingual corpora.
Rather, we can view them together as a uniform
set of translation pairs with varying degrees of lex-
icalization. Sentence pairs in the corpora, however,
should not be just added as patterns, since they are
often redundant, and such additions contribute to
neither acquisition nor refinement of non-sentential
patterns.
Therefore, we have been testing the integration
method with the following steps. Let T be a set of
translation patterns, B be a bilingual corpus, and
(s,t) be a pair of source and target sentences.
</bodyText>
<listItem confidence="0.961652">
1. [Correct Translation] If T can translate s into
t, do nothing.
2. [Competitive Situation] If T can translate s
</listItem>
<bodyText confidence="0.981876583333333">
into t&apos; (t t&apos;), do the following:
(a) [Lexicalization] If there is a paired deriva-
tion sequence Q of (s,t) in T, create a new
pattern p&apos; for a pattern p used in Q such
that every nonterminal symbol X in p with
no head constraint is associated with h:X
in q, where the head h is instantiated in X
of p. Add p&apos; to T if it is not already there.
Repeat the addition of such patterns, and
assign low weights to them until the refined
sequence Q becomes the most likely trans-
lation of s. For example, add
</bodyText>
<equation confidence="0.940961">
considerably:ADVP:2 --+ VP:1
VP:1 4- laisser:VP:1:-I-OBJ con-
</equation>
<bodyText confidence="0.961315333333333">
siderablement:ADVP:2
if the existing VP ADVP pattern does not
give a correct translation.
(b) [Addition of New Patterns] If there is
no such paired derivation sequence, add
specific patterns, if possible, for idioms and
collocations that are missing in T, or add
the pair (s,t) to T as a translation pattern.
For example, add
</bodyText>
<equation confidence="0.983083">
leave:VP:1:4-OBJ behind VP:1
VP:1 4- laisser:VP:1:-I-OBJ
</equation>
<bodyText confidence="0.881447923076923">
if the phrase &amp;quot;leave it behind&amp;quot; is not cor-
rectly translated.
3. [Translation Failure] If T cannot translate s
at all, add the pair (s,t) to T as a translation
pattern.
The grammar acquisition scheme described above
has not yet been automated, but has been manually
simulated for a set of 770 English-Japanese simple
sentence pairs designed for use in MT system eval-
uation, which is available from JEIDA (the Japan
Electronic Industry Development Association) ((the
Japan Electronic Industry Development Associa-
tion), 1995), including:
</bodyText>
<footnote confidence="0.710368">
#100: Any question will be welcomed.
#200: He kept calm in the face of great
</footnote>
<bodyText confidence="0.9707275">
danger.
#300: He is what is called &amp;quot;the man in the
news&amp;quot;.
#400: Japan registered a trade deficit of
$101 million, reflecting the country&apos;s eco-
nomic sluggishness, according to govern-
ment figures.
#500: I also went to the beach 2 weeks
earlier.
At an early stage of grammar acquisition, [Addition
of New Patterns] was primarily used to enrich
the set T of patterns, and many sentences were un-
ambiguously and correctly translated. At a later
stage, however, JEIDA sentences usually gave sev-
eral translations, and [Lexicalization] with care-
ful assignment of weights was the most critical task.
Although these sentences are intended to test a sys-
tem&apos;s ability to translate one basic linguistic phe-
nomenon in each simple sentence, the result was
strong evidence for our claim. Over 90% of JEIDA
sentences were correctly translated. Among the fail-
ures were:
#95: I see some stamps on the desk .
#171: He is for the suggestion, but I&apos;m
against it.
#244: She made him an excellent wife.
#660: He painted the walls and the floor
white.
Some (prepositional and sentential) attachment am-
biguities needs to be resolved on the basis of seman-
tic information, and scoping of coordinated struc-
tures would have to be determined by using not only
collocational patterns but also some measures of bal-
ance and similarities among constituents.
</bodyText>
<sectionHeader confidence="0.998266" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999713125">
Some assumptions about patterns should be re-
examined when we extend the definition of patterns.
The notion of head constraints may have to be ex-
tended into one of a set membership constraint if we
need to handle coordinated structures (Kaplan and
Maxwell III, 1988). Some light-verb phrases cannot
be correctly translated without &amp;quot;exchanging&amp;quot; several
feature values between the verb and its object. A
similar problem has been found in be-verb phrases.
Grammar acquisition and corpus integration are
fundamental issues, but automation of these pro-
cesses (Watanabe, 1993) is still not complete. Devel-
opment of an efficient translation algorithm, not just
an efficient parsing algorithm, will make a significant
contribution to research on synchronized grammars,
including STAGs and our PCFGs.
</bodyText>
<sectionHeader confidence="0.998579" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.980255">
Hideo Watanabe designed and implemented a pro-
totype MT system for pattern-based CFGs, while
Shiho Ogino developed a Japanese generator of the
</bodyText>
<page confidence="0.992879">
150
</page>
<bodyText confidence="0.999972142857143">
prototype. Their technical discussions and sugges-
tions greatly helped me shape the idea of pattern-
based CFGs. I would also like to thank Taijiro
Tsutsumi, Masayuki Morohashi, Hiroshi Nomiyama,
Tetsuya Nasukawa, and Naohiko Uramoto for their
valuable comments. Michael McDonald, as usual,
helped me write the final version.
</bodyText>
<sectionHeader confidence="0.997942" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999763368932039">
Abeille, A., Y. Schabes, and A. K. Joshi. 1990.
&amp;quot;Using Lexicalized Tags for Machine Translation&amp;quot;.
In Proc. of the 13th International Conference on
Computational Linguistics, volume 3, pages 1-6,
Aug.
Berwick, R. C. 1982. &amp;quot;Computational Complex-
ity and Lexical-Functional Grammar&amp;quot;. American
Journal of Computational Linguistics, pages 97-
109, July-Dec.
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. &amp;quot;The Mathematics of
Statistical Machine Translation: Parametric Es-
timation&amp;quot;. Computational Linguistics, 19(2):263—
311, June.
Dorr, B. J. 1993. &amp;quot;Machine Translation: A View
from the Lexicon&amp;quot;. The MIT Press, Cambridge,
Mass.
Earley, J. 1970. &amp;quot;An Efficient Context-free Pars-
ing Algorithm&amp;quot;. Communications of the ACM,
6(8):94-102, February.
Fujisaki, T., F. Jelinek, J. Cocke, E. Black, and
T. Nishino. 1989. &amp;quot;A Probabilistic Parsing
Method for Sentence Disambiguation&amp;quot;. In Proc.
of the International Workshop on Parsing Tech-
nologies, pages 85-94, Pittsburgh, Aug.
Furuse, 0. and H. Iida. 1994. &amp;quot;Cooperation be-
tween Transfer and Analysis in Example-Based
Framework&amp;quot;. In Proc. of the 15th International
Conference on Computational Linguistics, pages
645-651, Aug.
Gazdar, G., G. K. Pullum, and I. A. Sag. 1985.
&amp;quot;Generalized Phrase Structure Grammar&amp;quot;. Har-
vard University Press, Cambridge, Mass.
Jensen, K. and G. E. Heidorn. 1983. &amp;quot;The Fit-
ted Parse: 100% Parsing Capability in a Syntactic
Grammar of English&amp;quot;. In Proc. of the 1st Confer-
ence on Applied NLP, pages 93-98.
Kaplan, R. and J. Bresnan. 1982. &amp;quot;Lexical-
Functional Grammar: A Formal System for
Generalized Grammatical Representation&amp;quot;. In
J. Bresnan, editor, &amp;quot;Mental Representation of
Grammatical Relations&amp;quot;. MIT Press, Cambridge,
Mass., pages 173-281.
Kaplan, R. M. and J. T. Maxwell III. 1988.
&amp;quot;Constituent Coordination in Lexical-Functional
Grammar&amp;quot;. In Proc. of the 12th International
Conference on Computational Linguistics, pages
303-305, Aug.
Maruyama, H. 1993. &amp;quot;Pattern-Based Translation:
Context-Free Transducer and Its Applications to
Practical NLP&amp;quot;. In Proc. of Natural Language Pa-
cific Rim Symposium (NLPRS&apos; 93), pages 232-
237, Dec.
Pollard, C. and I. A. Sag. 1987. &amp;quot;An Information-
Based Syntax and Semantics, Vol.1 Fundamen-
tals&amp;quot;. CSLI Lecture Notes, Number 13.
Pustejovsky, J. 1991. &amp;quot;The Generative Lexi-
con&amp;quot;. Computational Linguistics, 17(4):409-441,
December.
Sato, S. and M. Nagao. 1990. &amp;quot;Toward Memory-
based Translation&amp;quot;. In Proc. of the 13th Interna-
tional Conference on Computational Linguistics,
pages 247-252, Helsinki, Aug.
Schabes, Y., A. Abeille, and A. K. Joshi. 1988.
&amp;quot;Parsing Algorithm with &apos;lexicalized&apos; grammars:
Application to tree adjoining grammars&amp;quot;. In Proc.
of the 12th International Conference on Compu-
tational Linguistics, pages 578-583, Aug.
Schabes, Y. and R. C. Waters. 1995. &amp;quot;Tree In-
sertion Grammar: A Cubic-Time, Parsable For-
malism that Lexicalizes Context-Free Grammar
without Changing the Trees Produced&amp;quot;. Compu-
tational Linguistics, 21(4):479-513, Dec.
Shieber, S. M. and Y. Schabes. 1990. &amp;quot;Synchronous
Tree-Adjoining Grammars&amp;quot;. In Proc. of the 13th
International Conference on Computational Lin-
guistics, pages 253-258, August.
Sumita, E. and H. Iida. 1991. &amp;quot;Experiments and
Prospects of Example-Based Machine Transla-
tion&amp;quot;. In Proc. of the 29th Annual Meeting of the
Association for Computational Linguistics, pages
185-192, Berkeley, June.
JEIDA (the Japan Electronic Industry Develop-
ment Association). 1995. &amp;quot;Evaluation Standards
for Machine Translation Systems (in Japanese)&amp;quot;.
95-COMP-17, Tokyo.
Tsujii, J. and K. Fujita. 1991. &amp;quot;Lexical Transfer
based on Bilingual Signs&amp;quot;. In Proc. of the 5th
European ACL Conference.
Vijay-Shanker, K. 1987. &amp;quot;A Study of Tree Ad-
joining Grammars&amp;quot;. Ph.D. thesis, Department of
Computer and Information Science, University of
Pennsylvania.
Vijay-Shanker, K. and Y. Schabes. 1992. &amp;quot;Struc-
ture Sharing in Lexicalized Tree-Adjoining Gram-
mars&amp;quot;. In Proc. of the 14th International Con-
ference on Computational Linguistics, pages 205-
211, Aug.
Watanabe, H. 1993. &amp;quot;A Method for Extract-
ing Translation Patterns from Translation Exam-
ples&amp;quot;. In Proc. of 5th Intl. Conf. on Theoretical
and Methodological Issues in Machine Translation
of Natural Languages, pages 292-301, July.
</reference>
<page confidence="0.998342">
151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978016">
<title confidence="0.999922">Pattern-Based Context-Free Grammars for Machine Translation</title>
<author confidence="0.999399">Koichi Takeda</author>
<affiliation confidence="0.997239">Research Laboratory, Research</affiliation>
<address confidence="0.996608">1623-14 Shimotsuruma, Yamato, Kanagawa 242, Japan</address>
<phone confidence="0.992467">81-462-73-4569, 81-462-73-7413 (FAX)</phone>
<email confidence="0.999748">takedatrl.vnet.ibm.com</email>
<abstract confidence="0.999404928571429">This paper proposes the use of &amp;quot;patternbased&amp;quot; context-free grammars as a basis for building machine translation (MT) systems, which are now being adopted as personal tools by a broad range of users in the cyberspace society. We discuss major requirements for such tools, including easy customization for diverse domains, the efficiency of the translation algorithm, and scalability (incremental improvement in translation quality through user interaction), and describe how our approach meets these requirements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abeille</author>
<author>Y Schabes</author>
<author>A K Joshi</author>
</authors>
<title>Using Lexicalized Tags for Machine Translation&amp;quot;.</title>
<date>1990</date>
<booktitle>In Proc. of the 13th International Conference on Computational Linguistics,</booktitle>
<volume>3</volume>
<pages>1--6</pages>
<contexts>
<context position="3226" citStr="Abeille, Schabes, and Joshi, 1990" startWordPosition="492" endWordPosition="496"> Sumita and Iida, 1991) and statistical MT (Brown et al., 1993) systems provide the easiest customizability, since users have only to supply a collection of source and target sentence pairs (a bilingual corpus). Two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial MT systems based on these approaches: • Can the system be used for various domains without showing severe degradation of translation accuracy? • What is the minimum number of examples (or training data) required to achieve reasonable MT quality for a new domain? TAG-based MT (Abeille, Schabes, and Joshi, 1990)1 and pattern-based translation (Maruyama, 1993) share many important properties for successful implementation in practical MT systems, namely: • The existence of a polynomial-time parsing algorithm • A capability for describing a larger domain of locality (Schabes, Abeille, and Joshi, 1988) • Synchronization (Shieber and Schabes, 1990) of the source and target language structures Readers should note, however, that the pars1See LTAG (Schabes, Abeille, and Josh, 1988) (Lexicalized TAG) and STAG (Shieber and Schabes, 1990) (Synchronized TAG) for each member of the TAG (Tree Adjoining Grammar) f</context>
</contexts>
<marker>Abeille, Schabes, Joshi, 1990</marker>
<rawString>Abeille, A., Y. Schabes, and A. K. Joshi. 1990. &amp;quot;Using Lexicalized Tags for Machine Translation&amp;quot;. In Proc. of the 13th International Conference on Computational Linguistics, volume 3, pages 1-6, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Berwick</author>
</authors>
<title>Computational Complexity and Lexical-Functional Grammar&amp;quot;.</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics,</journal>
<pages>97--109</pages>
<contexts>
<context position="21479" citStr="Berwick, 1982" startWordPosition="3709" endWordPosition="3710">es, as well as from monolingual to bilingual structures. Thirdly, a translation pattern can omit the tree structure of a collocation, and leave it as just a sequence of terminal symbols. The simplicity of this helps users to add patterns easily, although precise description of syntactic dependencies is lost. 4 Features and Agreements Translation patterns can be enhanced with unification and feature structures to give patterns additional power for describing gender, number, agreement, and so on. Since the descriptive power of unification-based grammars is considerably greater than that of CFG (Berwick, 1982), feature structures have to be restricted to maintain the efficiency of parsing and generation algorithms. Shieber and Schabes briefly discuss the issue (Shieber and Schabes, 1990). We can also extend translation patterns as follows: Each nonterminal node in a pattern can be associated with a fixed-length vector of binary features. This will enable us to specify such syntactic dependencies as agreement and subcategorization in patterns. Unification of binary features, however, is much simpler: unification of a feature-value pair succeeds only when the pair is either (0,0) or (1,1). Since the </context>
</contexts>
<marker>Berwick, 1982</marker>
<rawString>Berwick, R. C. 1982. &amp;quot;Computational Complexity and Lexical-Functional Grammar&amp;quot;. American Journal of Computational Linguistics, pages 97-109, July-Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parametric Estimation&amp;quot;.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="2656" citStr="Brown et al., 1993" startWordPosition="403" endWordPosition="406">ription of naturallanguage syntax and semantics. It is possible that, with the descriptive power of these grammars and lexicons, individual usages of words and phrases may be defined specifically enough to give correct translations. Practical implementation of MT systems based on these formalisms, on the other hand, would not be possible without much more efficient parsing and disambiguation algorithms for these formalisms and a method for building a lexicon that is easy even for novices to use. Corpus-based or example-based MT (Sato and Nagao, 1990; Sumita and Iida, 1991) and statistical MT (Brown et al., 1993) systems provide the easiest customizability, since users have only to supply a collection of source and target sentence pairs (a bilingual corpus). Two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial MT systems based on these approaches: • Can the system be used for various domains without showing severe degradation of translation accuracy? • What is the minimum number of examples (or training data) required to achieve reasonable MT quality for a new domain? TAG-based MT (Abeille, Schabes, and Joshi, 1990)1 and pattern-based translati</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. &amp;quot;The Mathematics of Statistical Machine Translation: Parametric Estimation&amp;quot;. Computational Linguistics, 19(2):263— 311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Dorr</author>
</authors>
<title>Machine Translation: A View from the Lexicon&amp;quot;.</title>
<date>1993</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="13836" citStr="Dorr, 1993" startWordPosition="2408" endWordPosition="2409">o show that the phrase is the BE-verb and its complement. A wildcard can be constrained with a head, as in &amp;quot;house:*&amp;quot; and &amp;quot;maison:*&amp;quot;. The internal representations of these patterns are as follows: leave:V:1 NP:2 VP:1 VP:1 4-- quitter:V:1 NP:2 be:V:1 year:NP:2 old VP:1 VP:1 avoir:V:1 an:NP:2 These patterns can be associated with an explicit nonterminal symbol such as &amp;quot;V:*&amp;quot; or &amp;quot;ADJP:*&amp;quot; in addition to head constraints (e.g., &amp;quot;leave:V:*&amp;quot;). By defining a few such notations, these patterns can be successfully converted into the formal representations defined in this section. Many of the divergences (Dorr, 1993) in source and target language expressions are fairly collocational, and can be appropriately handled by using our patterns. Note the simplicity that results from using a notation in which users only have to specify the surface ordering of words and phrases. More powerful grammar formalisms would generally require either a structural description or complex feature structures. 3 The Translation Algorithm The parsing algorithm for translation patterns can be any of known CFG parsing algorithms including CKY and Earley algorithmsl° At this stage, head and link constraints are ignored. It is easy </context>
</contexts>
<marker>Dorr, 1993</marker>
<rawString>Dorr, B. J. 1993. &amp;quot;Machine Translation: A View from the Lexicon&amp;quot;. The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-free Parsing Algorithm&amp;quot;.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>6--8</pages>
<contexts>
<context position="16223" citStr="Earley, 1970" startWordPosition="2815" endWordPosition="2816">on algorithm. Some restrictions on patterns must be imposed to avoid infinitely many ambiguities and arbitrarily long translations. The following patterns are therefore not allowed: 1. A —÷ X Y B 2. A X Y 4— .. • B . • •Ck if there is a cycle of synchronized derivation such that and B (or CI. ...B ...Ck) --+ Y B, where A, B, X, and Y are nonterminal symbols with or without head and link constraints, and C&apos;s are either terminal or nonterminal symbols. The basic strategy for choosing a candidate derivation sequence from ambiguous parses is as follows.&apos; A simplified view of the Earley algorithm (Earley, 1970) consists of three major components, predict(i), complete(i), and scan(i), which are called at each position i = 0,1, ..., n in an input string I = si Predict(i) returns a set of currently applicable CFG rules at position i. Complete(i) combines inactive charts ending at i with active charts that look for the inactive charts at position i to produce a new collection of active and inactive charts. Scan(i) tries to combine inactive charts with the symbol 5i+ at position i. Complete(n) gives the set of possible parses for the input I. Now, for every inactive chart associated with a nonterminal sy</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J. 1970. &amp;quot;An Efficient Context-free Parsing Algorithm&amp;quot;. Communications of the ACM, 6(8):94-102, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fujisaki</author>
<author>F Jelinek</author>
<author>J Cocke</author>
<author>E Black</author>
<author>T Nishino</author>
</authors>
<title>A Probabilistic Parsing Method for Sentence Disambiguation&amp;quot;.</title>
<date>1989</date>
<booktitle>In Proc. of the International Workshop on Parsing Technologies,</booktitle>
<pages>85--94</pages>
<location>Pittsburgh,</location>
<contexts>
<context position="24188" citStr="Fujisaki et al., 1989" startWordPosition="4131" endWordPosition="4134">ng of patterns described in the previous section. In our prototype system, each pattern has its original weight, and according to the preference measurement described in the previous section, a penalty is added to the weight to give the effective weight of the pattern in a particular context. Patterns with the least weight are to be chosen as the most preferred patterns. Numeric weights for patterns are extremely useful as means of assigning higher priorities uniformly to user-defined patterns. Statistical training of patterns can also be incorporated to calculate such weights systematically (Fujisaki et al., 1989). Figure 1 shows a sample translation of the input &amp;quot;He knows me well,&amp;quot; using the following patterns. NP:1:*AGRS VP:1:*AGRS S:1 S:1 4- NP:1:*AGRS VP:1:*AGRS ... (a) VP:1 ADVP:2 VP:1 VP:1 4- VP:1 ADVP:2 (b) know:VP:1:-FOBJ well VP:1 VP:1 4- connaitre:VP:1:+0BJ bien (c) V:1 NP:2 --+ VP:1:-FOBJ VP:1:+0BJ 4- V:1 NP:2:-PRO (d) V:1 NP:2 VP:1:+0BJ NP:2:-I-PRO V:1 ... (e) To simplify the example, let us assume that we have the following preterminal rules: he NPH-PRO+NOMI+3RD+SG NP:+PRO+NOMI+3RD+SG il (f) me NP:d-PRO+CAUS+SG-3RD NP:-FPRO+CAUS+SG-3RD 4- me ... (g) knows -4 V:+FIN+3SG sait (h) knows -4 V:</context>
</contexts>
<marker>Fujisaki, Jelinek, Cocke, Black, Nishino, 1989</marker>
<rawString>Fujisaki, T., F. Jelinek, J. Cocke, E. Black, and T. Nishino. 1989. &amp;quot;A Probabilistic Parsing Method for Sentence Disambiguation&amp;quot;. In Proc. of the International Workshop on Parsing Technologies, pages 85-94, Pittsburgh, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Iida</author>
</authors>
<title>Cooperation between Transfer and Analysis in Example-Based Framework&amp;quot;.</title>
<date>1994</date>
<booktitle>In Proc. of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>645--651</pages>
<contexts>
<context position="17566" citStr="Iida, 1994" startWordPosition="3068" endWordPosition="3069">owing ordering of patterns in P; this gives patterns with which we can use head and link constraints for building target charts and translations. These candidate patterns can be arranged and associated with the chart in the complete() procedure. 1. Prefer a pattern p with a source CFG skeleton X —+ Xl • • • Xk over any other pattern q with the same source CFG skeleton X --+ Xl• • • Xk, such that p has a head constraint h:Xi if q has h:Xi (i = 1,...,k). The pattern p is said to be more specific than q. For example, p = liThis strategy is similar to that of transfer-driven MT (TDMT) (Furuse and Iida, 1994). TDMT, however, is based on a combination of declarative/procedural knowledge sources for MT, and no clear computational properties have been investigated. 147 &amp;quot;leave:V:1 house:NP VP:1&amp;quot; is preferred to q = &amp;quot;leave:V:1 NP --4 VP:1&amp;quot;. 2. Prefer a pattern p with a source CFG skeleton to any pattern q that has fewer terminal symbols in the source CFG skeleton than p. For example, prefer &amp;quot;take:V:1 a walk&amp;quot; to &amp;quot;take:V:1 NP&amp;quot; if these patterns give the VP charts with the same span. 3. Prefer a pattern p which does not violate any head constraint over those which violate a head constraint. 4. Prefer the </context>
</contexts>
<marker>Iida, 1994</marker>
<rawString>Furuse, 0. and H. Iida. 1994. &amp;quot;Cooperation between Transfer and Analysis in Example-Based Framework&amp;quot;. In Proc. of the 15th International Conference on Computational Linguistics, pages 645-651, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>G K Pullum</author>
<author>I A Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar&amp;quot;.</title>
<date>1985</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="22786" citStr="Gazdar, Pullum, and Sag, 1985" startWordPosition="3914" endWordPosition="3919">rmed in a constant time. For example, the patterns13 V:1:+TRANS NP:2 —4 VP:1 VP:1 4- V:1:+TRANS NP:2 V:1:-FINTRANS —4 VP:1 VP:1 4— V:1:+INTRANS are unifiable with transitive and intransitive verbs, respectively. We can also distinguish local and head features, as postulated in HPSG. Simplified version of verb sub categorization is then encoded as VP:1:+TRANS-OBJ NP:2 --4 VP:1:+0BJ VP:1:+0BJ VP:1:+TRANS-OBJ NP:2 where &amp;quot;-OBJ&amp;quot; is a local feature for head VPs in LHSs, while &amp;quot;+OBJ&amp;quot; is a local feature for VPs in &amp;quot;Again, these patterns can be mapped to a weakly equivalent set of CFG rules. See GPSG (Gazdar, Pullum, and Sag, 1985) for more details. 148 the RHSs. Unification of a local feature with +OBJ succeeds since it is not bound. Agreement on subjects (nominative NPs) and finite-form verbs (VPs, excluding the BE verb) is disjunctively specified as NP:1:+NOMI+3RD+SG VP:2:+FIN+3SG NP:1:+NOMI+3RD+PL VP:2:+FIN-3SG NP:1:+NOMI-3RD VP:2:+FIN-3SG NP:1:+NOMI VP:2:+FIN+PAST which is collectively expressed as NP:1:*AGRS VP:2:*AGRV Here, *AGRS and *AGRV are a pair of aggregate unification specifiers that succeeds only when one of the above combinations of the feature values is unifiable. Another way to extend our grammar form</context>
</contexts>
<marker>Gazdar, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., G. K. Pullum, and I. A. Sag. 1985. &amp;quot;Generalized Phrase Structure Grammar&amp;quot;. Harvard University Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
<author>G E Heidorn</author>
</authors>
<title>The Fitted Parse: 100% Parsing Capability in a Syntactic Grammar of English&amp;quot;.</title>
<date>1983</date>
<booktitle>In Proc. of the 1st Conference on Applied NLP,</booktitle>
<pages>93--98</pages>
<contexts>
<context position="15547" citStr="Jensen and Heidorn, 1983" startWordPosition="2693" endWordPosition="2696"> ((A ... (A B)) A) (A (A ... (A B))) with (((B A) A) ... A) Our strategy is thus to find a candidate set of source charts in polynomial time. We therefore apply heuristic measurements to identify the most promising patterns for generating translations. In &apos;Our prototype implementation was based on the Earley algorithm, since this does not require lexicalization of CFG rules. this sense, the entire translation algorithm is not guaranteed to run in polynomial time. Practically, a timeout mechanism and a process for recovery from unsuccessful translation (e.g., applying the idea of fitted parse (Jensen and Heidorn, 1983) to target CFG rules) should be incorporated into the translation algorithm. Some restrictions on patterns must be imposed to avoid infinitely many ambiguities and arbitrarily long translations. The following patterns are therefore not allowed: 1. A —÷ X Y B 2. A X Y 4— .. • B . • •Ck if there is a cycle of synchronized derivation such that and B (or CI. ...B ...Ck) --+ Y B, where A, B, X, and Y are nonterminal symbols with or without head and link constraints, and C&apos;s are either terminal or nonterminal symbols. The basic strategy for choosing a candidate derivation sequence from ambiguous par</context>
</contexts>
<marker>Jensen, Heidorn, 1983</marker>
<rawString>Jensen, K. and G. E. Heidorn. 1983. &amp;quot;The Fitted Parse: 100% Parsing Capability in a Syntactic Grammar of English&amp;quot;. In Proc. of the 1st Conference on Applied NLP, pages 93-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>LexicalFunctional Grammar: A Formal System for Generalized Grammatical Representation&amp;quot;.</title>
<date>1982</date>
<pages>173--281</pages>
<editor>In J. Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.,</location>
<contexts>
<context position="1887" citStr="Kaplan and Bresnan, 1982" startWordPosition="284" endWordPosition="287">ll-known problems in MT, such as how to ensure the learnability of correct translations and facilitate customization. As a result, users are forced to see the same kinds of translation errors over and over again, except they in cases where they involve merely adding a missing word or compound to a user dictionary, or specifying one of several word-to-word translations as a correct choice. There are several alternative approaches that might eventually liberate us from this limitation on the usability of MT systems: Unification-based grammar formalisms and lexical-semantics formalisms (see LFG (Kaplan and Bresnan, 1982), HPSG (Pollard and Sag, 1987), and Generative Lexicon (Pustejovsky, 1991), for example) have been proposed to facilitate computationally precise description of naturallanguage syntax and semantics. It is possible that, with the descriptive power of these grammars and lexicons, individual usages of words and phrases may be defined specifically enough to give correct translations. Practical implementation of MT systems based on these formalisms, on the other hand, would not be possible without much more efficient parsing and disambiguation algorithms for these formalisms and a method for buildi</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Kaplan, R. and J. Bresnan. 1982. &amp;quot;LexicalFunctional Grammar: A Formal System for Generalized Grammatical Representation&amp;quot;. In J. Bresnan, editor, &amp;quot;Mental Representation of Grammatical Relations&amp;quot;. MIT Press, Cambridge, Mass., pages 173-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>J T Maxwell</author>
</authors>
<title>Constituent Coordination in Lexical-Functional Grammar&amp;quot;.</title>
<date>1988</date>
<booktitle>In Proc. of the 12th International Conference on Computational Linguistics,</booktitle>
<pages>303--305</pages>
<marker>Kaplan, Maxwell, 1988</marker>
<rawString>Kaplan, R. M. and J. T. Maxwell III. 1988. &amp;quot;Constituent Coordination in Lexical-Functional Grammar&amp;quot;. In Proc. of the 12th International Conference on Computational Linguistics, pages 303-305, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Maruyama</author>
</authors>
<title>Pattern-Based Translation: Context-Free Transducer and Its Applications to Practical NLP&amp;quot;.</title>
<date>1993</date>
<booktitle>In Proc. of Natural Language Pacific Rim Symposium (NLPRS&apos; 93),</booktitle>
<pages>232--237</pages>
<contexts>
<context position="3275" citStr="Maruyama, 1993" startWordPosition="500" endWordPosition="501">tems provide the easiest customizability, since users have only to supply a collection of source and target sentence pairs (a bilingual corpus). Two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial MT systems based on these approaches: • Can the system be used for various domains without showing severe degradation of translation accuracy? • What is the minimum number of examples (or training data) required to achieve reasonable MT quality for a new domain? TAG-based MT (Abeille, Schabes, and Joshi, 1990)1 and pattern-based translation (Maruyama, 1993) share many important properties for successful implementation in practical MT systems, namely: • The existence of a polynomial-time parsing algorithm • A capability for describing a larger domain of locality (Schabes, Abeille, and Joshi, 1988) • Synchronization (Shieber and Schabes, 1990) of the source and target language structures Readers should note, however, that the pars1See LTAG (Schabes, Abeille, and Josh, 1988) (Lexicalized TAG) and STAG (Shieber and Schabes, 1990) (Synchronized TAG) for each member of the TAG (Tree Adjoining Grammar) family. 144 ing algorithm for TAGs has 0(107/6)2 w</context>
<context position="19015" citStr="Maruyama, 1993" startWordPosition="3311" endWordPosition="3312">y favors lexicalized (or head constrained) and collocational patterns, which is exactly what we are going to achieve with pattern-based MT. Selection of patterns in the derivation sequence accompanies the construction of a target chart. Link constraints are propagated from source to target derivation trees. This is basically a bottom-up procedure. Since the number M of distinct pairs (X,w), for a nonterminal symbol X and a subsequence w of input string s, is bounded by Kn2, we can compute the mbest choice of pattern candidates for every inactive chart in time O(ITIKn3) as claimed by Maruyama (Maruyama, 1993), and Schabes and Waters (Schabes and Waters, 1995). Here, K is the number of distinct nonterminal symbols in T, and n is the size of the input string. Note that the head constraints associated with the source CFG rules can be incorporated in the parsing algorithm, since the number of triples (X,w,h), where h is a head of X, is bounded by Kn3. We can modify the predict(), completes, and scan() procedures to run in 0(ITIKO) while checking the source head constraints. Construction of the target charts, if possible, on the basis of the m best candidate patterns for each source chart takes 0(Kn2rn</context>
</contexts>
<marker>Maruyama, 1993</marker>
<rawString>Maruyama, H. 1993. &amp;quot;Pattern-Based Translation: Context-Free Transducer and Its Applications to Practical NLP&amp;quot;. In Proc. of Natural Language Pacific Rim Symposium (NLPRS&apos; 93), pages 232-237, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>An InformationBased Syntax and Semantics, Vol.1 Fundamentals&amp;quot;.</title>
<date>1987</date>
<journal>CSLI Lecture Notes, Number</journal>
<volume>13</volume>
<contexts>
<context position="1917" citStr="Pollard and Sag, 1987" startWordPosition="289" endWordPosition="292">how to ensure the learnability of correct translations and facilitate customization. As a result, users are forced to see the same kinds of translation errors over and over again, except they in cases where they involve merely adding a missing word or compound to a user dictionary, or specifying one of several word-to-word translations as a correct choice. There are several alternative approaches that might eventually liberate us from this limitation on the usability of MT systems: Unification-based grammar formalisms and lexical-semantics formalisms (see LFG (Kaplan and Bresnan, 1982), HPSG (Pollard and Sag, 1987), and Generative Lexicon (Pustejovsky, 1991), for example) have been proposed to facilitate computationally precise description of naturallanguage syntax and semantics. It is possible that, with the descriptive power of these grammars and lexicons, individual usages of words and phrases may be defined specifically enough to give correct translations. Practical implementation of MT systems based on these formalisms, on the other hand, would not be possible without much more efficient parsing and disambiguation algorithms for these formalisms and a method for building a lexicon that is easy even</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, C. and I. A. Sag. 1987. &amp;quot;An InformationBased Syntax and Semantics, Vol.1 Fundamentals&amp;quot;. CSLI Lecture Notes, Number 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The Generative Lexicon&amp;quot;.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--4</pages>
<contexts>
<context position="1961" citStr="Pustejovsky, 1991" startWordPosition="296" endWordPosition="297">ations and facilitate customization. As a result, users are forced to see the same kinds of translation errors over and over again, except they in cases where they involve merely adding a missing word or compound to a user dictionary, or specifying one of several word-to-word translations as a correct choice. There are several alternative approaches that might eventually liberate us from this limitation on the usability of MT systems: Unification-based grammar formalisms and lexical-semantics formalisms (see LFG (Kaplan and Bresnan, 1982), HPSG (Pollard and Sag, 1987), and Generative Lexicon (Pustejovsky, 1991), for example) have been proposed to facilitate computationally precise description of naturallanguage syntax and semantics. It is possible that, with the descriptive power of these grammars and lexicons, individual usages of words and phrases may be defined specifically enough to give correct translations. Practical implementation of MT systems based on these formalisms, on the other hand, would not be possible without much more efficient parsing and disambiguation algorithms for these formalisms and a method for building a lexicon that is easy even for novices to use. Corpus-based or example</context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>Pustejovsky, J. 1991. &amp;quot;The Generative Lexicon&amp;quot;. Computational Linguistics, 17(4):409-441, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
<author>M Nagao</author>
</authors>
<title>Toward Memorybased Translation&amp;quot;.</title>
<date>1990</date>
<booktitle>In Proc. of the 13th International Conference on Computational Linguistics,</booktitle>
<pages>247--252</pages>
<location>Helsinki,</location>
<contexts>
<context position="2592" citStr="Sato and Nagao, 1990" startWordPosition="391" endWordPosition="394">le) have been proposed to facilitate computationally precise description of naturallanguage syntax and semantics. It is possible that, with the descriptive power of these grammars and lexicons, individual usages of words and phrases may be defined specifically enough to give correct translations. Practical implementation of MT systems based on these formalisms, on the other hand, would not be possible without much more efficient parsing and disambiguation algorithms for these formalisms and a method for building a lexicon that is easy even for novices to use. Corpus-based or example-based MT (Sato and Nagao, 1990; Sumita and Iida, 1991) and statistical MT (Brown et al., 1993) systems provide the easiest customizability, since users have only to supply a collection of source and target sentence pairs (a bilingual corpus). Two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial MT systems based on these approaches: • Can the system be used for various domains without showing severe degradation of translation accuracy? • What is the minimum number of examples (or training data) required to achieve reasonable MT quality for a new domain? TAG-based MT </context>
</contexts>
<marker>Sato, Nagao, 1990</marker>
<rawString>Sato, S. and M. Nagao. 1990. &amp;quot;Toward Memorybased Translation&amp;quot;. In Proc. of the 13th International Conference on Computational Linguistics, pages 247-252, Helsinki, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>A Abeille</author>
<author>A K Joshi</author>
</authors>
<title>Parsing Algorithm with &apos;lexicalized&apos; grammars: Application to tree adjoining grammars&amp;quot;.</title>
<date>1988</date>
<booktitle>In Proc. of the 12th International Conference on Computational Linguistics,</booktitle>
<pages>578--583</pages>
<contexts>
<context position="3518" citStr="Schabes, Abeille, and Joshi, 1988" startWordPosition="533" endWordPosition="537">n confidently build commercial MT systems based on these approaches: • Can the system be used for various domains without showing severe degradation of translation accuracy? • What is the minimum number of examples (or training data) required to achieve reasonable MT quality for a new domain? TAG-based MT (Abeille, Schabes, and Joshi, 1990)1 and pattern-based translation (Maruyama, 1993) share many important properties for successful implementation in practical MT systems, namely: • The existence of a polynomial-time parsing algorithm • A capability for describing a larger domain of locality (Schabes, Abeille, and Joshi, 1988) • Synchronization (Shieber and Schabes, 1990) of the source and target language structures Readers should note, however, that the pars1See LTAG (Schabes, Abeille, and Josh, 1988) (Lexicalized TAG) and STAG (Shieber and Schabes, 1990) (Synchronized TAG) for each member of the TAG (Tree Adjoining Grammar) family. 144 ing algorithm for TAGs has 0(107/6)2 worst case time complexity (Vijay-Shanker, 1987), and that the &amp;quot;patterns&amp;quot; in Maruyama&apos;s approach are merely context-free grammar (CFG) rules. Thus, it has been a challenge to find a framework in which we can enjoy both a grammar formalism with </context>
</contexts>
<marker>Schabes, Abeille, Joshi, 1988</marker>
<rawString>Schabes, Y., A. Abeille, and A. K. Joshi. 1988. &amp;quot;Parsing Algorithm with &apos;lexicalized&apos; grammars: Application to tree adjoining grammars&amp;quot;. In Proc. of the 12th International Conference on Computational Linguistics, pages 578-583, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>R C Waters</author>
</authors>
<title>Tree Insertion Grammar: A Cubic-Time, Parsable Formalism that Lexicalizes Context-Free Grammar without Changing the Trees Produced&amp;quot;.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="5757" citStr="Schabes and Waters, 1995" startWordPosition="906" endWordPosition="909">P accompanied by the following constraints. 1. Head constraints: The nonterminal symbol V in the source rule must have the verb miss as a syntactic head. The symbol V in the target rule must have the verb man quer as a syntactic head. The head of symbol S in the source (target) rule is identical to the head of symbol V in the source (target) rule as they are co-indexed. 2. Link constraints: Nonterminal symbols in source and target CFG rules are linked if they 2Where IGI stands for the size of grammar G, and n is the length of an input string. 3Lexicalized CFG, or Tree Insertion Grammar (TIG) (Schabes and Waters, 1995), has been recently introduced to achieve such efficiency and lexicalization. 4and its inflectional variants — we will discuss inflections and agreement issues later. 5The meaning of the word &amp;quot;synchronized&amp;quot; here is exactly the same as in STAG (Shieber and Schabes, 1990). See also bilingual signs (Tsujii and Fujita, 1991) for a discussion of the importance of combining the appropriate domain of locality and synchronization. are given the same index &amp;quot;:i&amp;quot;. Linked nonterminal must be derived from a sequence of synchronized pairs. Thus, the first NP (NP:1) in the source rule corresponds to the seco</context>
<context position="19066" citStr="Schabes and Waters, 1995" startWordPosition="3317" endWordPosition="3321">) and collocational patterns, which is exactly what we are going to achieve with pattern-based MT. Selection of patterns in the derivation sequence accompanies the construction of a target chart. Link constraints are propagated from source to target derivation trees. This is basically a bottom-up procedure. Since the number M of distinct pairs (X,w), for a nonterminal symbol X and a subsequence w of input string s, is bounded by Kn2, we can compute the mbest choice of pattern candidates for every inactive chart in time O(ITIKn3) as claimed by Maruyama (Maruyama, 1993), and Schabes and Waters (Schabes and Waters, 1995). Here, K is the number of distinct nonterminal symbols in T, and n is the size of the input string. Note that the head constraints associated with the source CFG rules can be incorporated in the parsing algorithm, since the number of triples (X,w,h), where h is a head of X, is bounded by Kn3. We can modify the predict(), completes, and scan() procedures to run in 0(ITIKO) while checking the source head constraints. Construction of the target charts, if possible, on the basis of the m best candidate patterns for each source chart takes 0(Kn2rn) time. Here, 171 can be larger than 2 if we genera</context>
</contexts>
<marker>Schabes, Waters, 1995</marker>
<rawString>Schabes, Y. and R. C. Waters. 1995. &amp;quot;Tree Insertion Grammar: A Cubic-Time, Parsable Formalism that Lexicalizes Context-Free Grammar without Changing the Trees Produced&amp;quot;. Computational Linguistics, 21(4):479-513, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
<author>Y Schabes</author>
</authors>
<title>Synchronous Tree-Adjoining Grammars&amp;quot;.</title>
<date>1990</date>
<booktitle>In Proc. of the 13th International Conference on Computational Linguistics,</booktitle>
<pages>253--258</pages>
<contexts>
<context position="3565" citStr="Shieber and Schabes, 1990" startWordPosition="540" endWordPosition="543">se approaches: • Can the system be used for various domains without showing severe degradation of translation accuracy? • What is the minimum number of examples (or training data) required to achieve reasonable MT quality for a new domain? TAG-based MT (Abeille, Schabes, and Joshi, 1990)1 and pattern-based translation (Maruyama, 1993) share many important properties for successful implementation in practical MT systems, namely: • The existence of a polynomial-time parsing algorithm • A capability for describing a larger domain of locality (Schabes, Abeille, and Joshi, 1988) • Synchronization (Shieber and Schabes, 1990) of the source and target language structures Readers should note, however, that the pars1See LTAG (Schabes, Abeille, and Josh, 1988) (Lexicalized TAG) and STAG (Shieber and Schabes, 1990) (Synchronized TAG) for each member of the TAG (Tree Adjoining Grammar) family. 144 ing algorithm for TAGs has 0(107/6)2 worst case time complexity (Vijay-Shanker, 1987), and that the &amp;quot;patterns&amp;quot; in Maruyama&apos;s approach are merely context-free grammar (CFG) rules. Thus, it has been a challenge to find a framework in which we can enjoy both a grammar formalism with better descriptive power than CFG and more effi</context>
<context position="6027" citStr="Shieber and Schabes, 1990" startWordPosition="950" endWordPosition="953"> (target) rule is identical to the head of symbol V in the source (target) rule as they are co-indexed. 2. Link constraints: Nonterminal symbols in source and target CFG rules are linked if they 2Where IGI stands for the size of grammar G, and n is the length of an input string. 3Lexicalized CFG, or Tree Insertion Grammar (TIG) (Schabes and Waters, 1995), has been recently introduced to achieve such efficiency and lexicalization. 4and its inflectional variants — we will discuss inflections and agreement issues later. 5The meaning of the word &amp;quot;synchronized&amp;quot; here is exactly the same as in STAG (Shieber and Schabes, 1990). See also bilingual signs (Tsujii and Fujita, 1991) for a discussion of the importance of combining the appropriate domain of locality and synchronization. are given the same index &amp;quot;:i&amp;quot;. Linked nonterminal must be derived from a sequence of synchronized pairs. Thus, the first NP (NP:1) in the source rule corresponds to the second NP (NP:1) in the target rule, the Vs in both rules correspond to each other, and the second NP (NP:3) in the source rule corresponds to the first NP (NP:3) in the target rule. The source and target rules are called CFG skeleton of the pattern. The notion of a syntact</context>
<context position="21660" citStr="Shieber and Schabes, 1990" startWordPosition="3734" endWordPosition="3738">terminal symbols. The simplicity of this helps users to add patterns easily, although precise description of syntactic dependencies is lost. 4 Features and Agreements Translation patterns can be enhanced with unification and feature structures to give patterns additional power for describing gender, number, agreement, and so on. Since the descriptive power of unification-based grammars is considerably greater than that of CFG (Berwick, 1982), feature structures have to be restricted to maintain the efficiency of parsing and generation algorithms. Shieber and Schabes briefly discuss the issue (Shieber and Schabes, 1990). We can also extend translation patterns as follows: Each nonterminal node in a pattern can be associated with a fixed-length vector of binary features. This will enable us to specify such syntactic dependencies as agreement and subcategorization in patterns. Unification of binary features, however, is much simpler: unification of a feature-value pair succeeds only when the pair is either (0,0) or (1,1). Since the feature vector has a fixed length, unification of two feature vectors is performed in a constant time. For example, the patterns13 V:1:+TRANS NP:2 —4 VP:1 VP:1 4- V:1:+TRANS NP:2 V:</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Shieber, S. M. and Y. Schabes. 1990. &amp;quot;Synchronous Tree-Adjoining Grammars&amp;quot;. In Proc. of the 13th International Conference on Computational Linguistics, pages 253-258, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>H Iida</author>
</authors>
<title>Experiments and Prospects of Example-Based Machine Translation&amp;quot;.</title>
<date>1991</date>
<booktitle>In Proc. of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>185--192</pages>
<location>Berkeley,</location>
<contexts>
<context position="2616" citStr="Sumita and Iida, 1991" startWordPosition="395" endWordPosition="398"> to facilitate computationally precise description of naturallanguage syntax and semantics. It is possible that, with the descriptive power of these grammars and lexicons, individual usages of words and phrases may be defined specifically enough to give correct translations. Practical implementation of MT systems based on these formalisms, on the other hand, would not be possible without much more efficient parsing and disambiguation algorithms for these formalisms and a method for building a lexicon that is easy even for novices to use. Corpus-based or example-based MT (Sato and Nagao, 1990; Sumita and Iida, 1991) and statistical MT (Brown et al., 1993) systems provide the easiest customizability, since users have only to supply a collection of source and target sentence pairs (a bilingual corpus). Two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial MT systems based on these approaches: • Can the system be used for various domains without showing severe degradation of translation accuracy? • What is the minimum number of examples (or training data) required to achieve reasonable MT quality for a new domain? TAG-based MT (Abeille, Schabes, and J</context>
</contexts>
<marker>Sumita, Iida, 1991</marker>
<rawString>Sumita, E. and H. Iida. 1991. &amp;quot;Experiments and Prospects of Example-Based Machine Translation&amp;quot;. In Proc. of the 29th Annual Meeting of the Association for Computational Linguistics, pages 185-192, Berkeley, June.</rawString>
</citation>
<citation valid="true">
<title>JEIDA (the Japan Electronic Industry Development Association).</title>
<date>1995</date>
<tech>95-COMP-17,</tech>
<location>Tokyo.</location>
<marker>1995</marker>
<rawString>JEIDA (the Japan Electronic Industry Development Association). 1995. &amp;quot;Evaluation Standards for Machine Translation Systems (in Japanese)&amp;quot;. 95-COMP-17, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tsujii</author>
<author>K Fujita</author>
</authors>
<title>Lexical Transfer based on Bilingual Signs&amp;quot;.</title>
<date>1991</date>
<booktitle>In Proc. of the 5th European ACL Conference.</booktitle>
<contexts>
<context position="6079" citStr="Tsujii and Fujita, 1991" startWordPosition="958" endWordPosition="961"> the source (target) rule as they are co-indexed. 2. Link constraints: Nonterminal symbols in source and target CFG rules are linked if they 2Where IGI stands for the size of grammar G, and n is the length of an input string. 3Lexicalized CFG, or Tree Insertion Grammar (TIG) (Schabes and Waters, 1995), has been recently introduced to achieve such efficiency and lexicalization. 4and its inflectional variants — we will discuss inflections and agreement issues later. 5The meaning of the word &amp;quot;synchronized&amp;quot; here is exactly the same as in STAG (Shieber and Schabes, 1990). See also bilingual signs (Tsujii and Fujita, 1991) for a discussion of the importance of combining the appropriate domain of locality and synchronization. are given the same index &amp;quot;:i&amp;quot;. Linked nonterminal must be derived from a sequence of synchronized pairs. Thus, the first NP (NP:1) in the source rule corresponds to the second NP (NP:1) in the target rule, the Vs in both rules correspond to each other, and the second NP (NP:3) in the source rule corresponds to the first NP (NP:3) in the target rule. The source and target rules are called CFG skeleton of the pattern. The notion of a syntactic head is similar to that used in unification gramm</context>
</contexts>
<marker>Tsujii, Fujita, 1991</marker>
<rawString>Tsujii, J. and K. Fujita. 1991. &amp;quot;Lexical Transfer based on Bilingual Signs&amp;quot;. In Proc. of the 5th European ACL Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
</authors>
<title>A Study of Tree Adjoining Grammars&amp;quot;.</title>
<date>1987</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="3922" citStr="Vijay-Shanker, 1987" startWordPosition="597" endWordPosition="598">rties for successful implementation in practical MT systems, namely: • The existence of a polynomial-time parsing algorithm • A capability for describing a larger domain of locality (Schabes, Abeille, and Joshi, 1988) • Synchronization (Shieber and Schabes, 1990) of the source and target language structures Readers should note, however, that the pars1See LTAG (Schabes, Abeille, and Josh, 1988) (Lexicalized TAG) and STAG (Shieber and Schabes, 1990) (Synchronized TAG) for each member of the TAG (Tree Adjoining Grammar) family. 144 ing algorithm for TAGs has 0(107/6)2 worst case time complexity (Vijay-Shanker, 1987), and that the &amp;quot;patterns&amp;quot; in Maruyama&apos;s approach are merely context-free grammar (CFG) rules. Thus, it has been a challenge to find a framework in which we can enjoy both a grammar formalism with better descriptive power than CFG and more efficient parsing/generation algorithms than those of TAGs.3 In this paper, we will show that there exists a class of &amp;quot;pattern-based&amp;quot; grammars that is weakly equivalent to CFG (thus allowing the CFG parsing algorithms to be used for our grammars), but that it facilitates description of the domain of locality. Furthermore, we will show that our framework can b</context>
</contexts>
<marker>Vijay-Shanker, 1987</marker>
<rawString>Vijay-Shanker, K. 1987. &amp;quot;A Study of Tree Adjoining Grammars&amp;quot;. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>Y Schabes</author>
</authors>
<title>Structure Sharing in Lexicalized Tree-Adjoining Grammars&amp;quot;.</title>
<date>1992</date>
<booktitle>In Proc. of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>205--211</pages>
<marker>Vijay-Shanker, Schabes, 1992</marker>
<rawString>Vijay-Shanker, K. and Y. Schabes. 1992. &amp;quot;Structure Sharing in Lexicalized Tree-Adjoining Grammars&amp;quot;. In Proc. of the 14th International Conference on Computational Linguistics, pages 205-211, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Watanabe</author>
</authors>
<title>A Method for Extracting Translation Patterns from Translation Examples&amp;quot;.</title>
<date>1993</date>
<booktitle>In Proc. of 5th Intl. Conf. on Theoretical and Methodological Issues in Machine Translation of Natural Languages,</booktitle>
<pages>292--301</pages>
<contexts>
<context position="30806" citStr="Watanabe, 1993" startWordPosition="5294" endWordPosition="5295">g constituents. 6 Conclusions and Future Work Some assumptions about patterns should be reexamined when we extend the definition of patterns. The notion of head constraints may have to be extended into one of a set membership constraint if we need to handle coordinated structures (Kaplan and Maxwell III, 1988). Some light-verb phrases cannot be correctly translated without &amp;quot;exchanging&amp;quot; several feature values between the verb and its object. A similar problem has been found in be-verb phrases. Grammar acquisition and corpus integration are fundamental issues, but automation of these processes (Watanabe, 1993) is still not complete. Development of an efficient translation algorithm, not just an efficient parsing algorithm, will make a significant contribution to research on synchronized grammars, including STAGs and our PCFGs. Acknowledgments Hideo Watanabe designed and implemented a prototype MT system for pattern-based CFGs, while Shiho Ogino developed a Japanese generator of the 150 prototype. Their technical discussions and suggestions greatly helped me shape the idea of patternbased CFGs. I would also like to thank Taijiro Tsutsumi, Masayuki Morohashi, Hiroshi Nomiyama, Tetsuya Nasukawa, and N</context>
</contexts>
<marker>Watanabe, 1993</marker>
<rawString>Watanabe, H. 1993. &amp;quot;A Method for Extracting Translation Patterns from Translation Examples&amp;quot;. In Proc. of 5th Intl. Conf. on Theoretical and Methodological Issues in Machine Translation of Natural Languages, pages 292-301, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>