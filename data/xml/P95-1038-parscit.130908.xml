<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.462329">
<title confidence="0.994574">
Evaluation of Semantic Clusters
</title>
<author confidence="0.992968">
Rajeev Agarwal
</author>
<affiliation confidence="0.996548">
Mississippi State University
</affiliation>
<address confidence="0.8145325">
Mississippi State, MS 39762
USA
</address>
<email confidence="0.917299">
rajeev©cs.msstate.edu
</email>
<sectionHeader confidence="0.991101" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876615384615">
Semantic clusters of a domain form an
important feature that can be useful for
performing syntactic and semantic disam-
biguation. Several attempts have been
made to extract the semantic clusters of a
domain by probabilistic or taxonomic tech-
niques. However, not much progress has
been made in evaluating the obtained se-
mantic clusters. This paper focuses on an
evaluation mechanism that can be used to
evaluate semantic clusters produced by a
system against those provided by human
experts.
</bodyText>
<sectionHeader confidence="0.998133" genericHeader="keywords">
1 Introduction&apos;
</sectionHeader>
<bodyText confidence="0.999957916666667">
Most natural language processing (NLP) systems are
designed to work on certain specific domains and
porting them to other domains is often a very time-
consuming and human-intensive process. As the
need for applying NLP systems to more and var-
ied domains grows, it becomes increasingly impor-
tant that some techniques be used to make these
systems more portable. Several researchers (Lang
and Hirschman, 1988; Rau et al., 1989; Pustejovsky,
1992; Grishrnan and Sterling, 1993; Basili et al.,
1994), either directly or indirectly, have addressed
issues that assist in making it easier to move an
NLP system from one domain to another. One of
the reasons for the lack of portability is the need for
domain-specific semantic features that such systems
often use for lexical, syntactic, and semantic disam-
biguation. One such feature is the knowledge of the
semantic clusters in a domain.
Since semantic classes are often domain-specific,
their automatic acquisition is not trivial. Such
classes can be derived either by distributional means
or from existing taxonomies, knowledge bases, dic-
tionaries, thesauruses, and so on. A prime exam-
ple of the latter is WordNet which has been used to
</bodyText>
<footnote confidence="0.9151005">
1The author is currently at Texas Instruments and all
inquiries should be addressed to rajeevOcsc.ti.com.
</footnote>
<bodyText confidence="0.998537714285714">
provide such semantic classes (Resnik, 1993; Basili
et al., 1994) to assist in text understanding. Our
efforts to obtain such semantic clusters with limited
human intervention have been described elsewhere
(Agarwal, 1995). This paper concentrates on the
aspect of evaluating the obtained clusters against
classes provided by human experts.
</bodyText>
<sectionHeader confidence="0.951325" genericHeader="introduction">
2 The Need
</sectionHeader>
<bodyText confidence="0.999982617647059">
Although there has been a lot of work done in ex-
tracting semantic classes of a given domain, rela-
tively little attention has been paid to the task of
evaluating the generated classes. In the absence of
an evaluation scheme, the only way to decide if the
semantic classes produced by a system are &amp;quot;reason-
able&amp;quot; or not is by having an expert analyze them by
inspection. Such informal evaluations make it very
difficult to compare one set of classes against an-
other and are also not very reliable estimates of the
quality of a set of classes. It is clear that a formal
evaluation scheme would be of great help.
Hatzivassiloglou and McKeown (1993) cluster ad-
jectives into partitions and present an interest-
ing evaluation to compare the generated adjective
classes against those provided by an expert. Their
evaluation scheme bases the comparison between
two classes on the presence or absence of pairs of
words in them. Their approach involves filling in a
YES—NO contingency table based on whether a pair
of words (adjectives, in their case) is classified in the
same class by the human expert and by the system.
This method works very well for partitions. How-
ever, if it is used to evaluate sets of classes where the
classes may be potentially overlapping, their tech-
nique yields a weaker measure since the same word
pair could possibly be present in more than one class.
An ideal scheme used to evaluate semantic classes
should be able to handle overlapping classes (as op-
posed to partitions) as well as hierarchies. The tech-
nique proposed by Hatzivassiloglou and McKeown
does not do a good job of evaluating either of these.
In this paper, we present an evaluation methodology
which makes it possible to properly evaluate over-
</bodyText>
<page confidence="0.998777">
284
</page>
<tableCaption confidence="0.998994">
Table 1: Two Example Classes
</tableCaption>
<bodyText confidence="0.983629333333333">
lapping classes. Our scheme is also capable of in-
corporating hierarchies provided by an expert into
the evaluation, but still lacks the ability to compare
hierarchies against hierarchies.
In the discussion that follows, the word &amp;quot;cluster-
ing&amp;quot; is used to refer to the set of classes that may
be either provided by an expert or generated by the
system, and the word &amp;quot;class&amp;quot; is used to refer to a
single class in the clustering.
</bodyText>
<sectionHeader confidence="0.982566" genericHeader="method">
3 Evaluation Approach
</sectionHeader>
<bodyText confidence="0.999370344827586">
As mentioned above, we intend to be able to com-
pare a clustering generated by a system against one
provided by an ex-pert. Since a word can occur in
more than one class, it is important to find some
kind of mapping between the classes generated by
the system and the classes given by the expert. Such
a mapping tells us which class in the system&apos;s clus-
tering maps to which one in the expert&apos;s clustering,
and an overall comparison of the clusterings is based
on the comparison of the mutually mapping classes.
Before we delve deeper into the evaluation pro-
cess, we must decide on some measure of &amp;quot;closeness&amp;quot;
between a pair of classes. We have adopted the
F-measure (Hatzivassiloglou and McKeown, 1993;
Chincor, 1992). In our computation of the F-
measure, we construct a contingency table based
on the presence or absence of individual elements
in the two classes being compared, as opposed to
basing it on pairs of words. For example, suppose
that Class A is generated by the system and Class B
is provided by an expert (as shown in Table 1). The
contingency table obtained for this pair of classes is
shown in Table 2.
The three main steps in the evaluation process are
the acquisition of &amp;quot;correct&amp;quot; classes from domain ex-
perts, mapping the experts&apos; clustering to that gener-
ated by the system, and generating an overall mea-
sure that represents the system&apos;s performance when
compared against the expert.
</bodyText>
<tableCaption confidence="0.98822">
Table 2: Contingency Table for Classes A and B
</tableCaption>
<subsectionHeader confidence="0.998221">
3.1 Knowledge Acquisition from Experts
</subsectionHeader>
<bodyText confidence="0.998962333333334">
The objective of this step is to get human experts to
undertake the same task that the system performs,
i.e., classifying a set of words into several potentially
overlapping classes. The classes produced by a sys-
tem are later compared to these &amp;quot;correct&amp;quot; classifica-
tions provided by the expert.
</bodyText>
<subsectionHeader confidence="0.999533">
3.2 Mapping Algorithm
</subsectionHeader>
<bodyText confidence="0.994598289473684">
In order to determine pairwise mappings between
the clustering generated by the system and one pro-
vided by an expert, a table of F-measures is con-
structed, with a row for each class generated by the
system, and a column for every class provided by the
expert. Note that since the expert actually provides
a hierarchy, there is one column corresponding to
every individual class and subclass provided by the
expert. This allows the system&apos;s classes to map to
a class at any level in the expert&apos;s hierarchy. This
table gives an estimate of how well each class gen-
erated by the system maps to the ones provided by
the expert.
The algorithm used to compute the actual map-
pings from the F-measure table is briefly described
here. In each row of the table, mark the cell with the
highest F-measure as a potential mapping. In gen-
eral, conflicts arise when more than one class gener-
ated by the system maps to a given class provided
by the expert. In other words, whenever a column
in the table has more than one cell marked as a po-
tential mapping, a conflict is said to exist. To re-
solve a conflict, one of the system classes must be
re-mapped. The heuristic used here is that the class
for which such a re-mapping results in minimal loss
of F-measure is the one that must be re-mapped.
Several such conflicts may exist, and re-mapping
may lead to further conflicts. The mapping algo-
rithm iteratively searches for conflicts and resolves
them till no more conflicts exist. Note also that a
system class may map to an expert class only if the
F-measure between them exceeds a certain threshold
value. This ensures that a certain degree of similar-
ity must exist between two classes for them to map
to each other. We have used a threshold value of
0.20. This value is obtained purely by observations
made on the F-measures between different pairs of
classes with varying degrees of similarity.
</bodyText>
<figure confidence="0.99967209375">
Class A
(System)
cat
dog
stomach
pig
cow
hair
cattle
goat
Class B
(Expert)
horse
cow
cat
pig
lamb
dog
sheep
mare
cattle
swine
goat
Expert
YES
NO
System - YES
System - NO
6
2
5
0
</figure>
<page confidence="0.996473">
285
</page>
<tableCaption confidence="0.997478">
Table 3: Noun Clustering Results
</tableCaption>
<table confidence="0.997165333333333">
Expert A 75.38 29.09 0.42
Expert B 77.08 25.23 0.38
Expert C 73.85 37.88 0.50
</table>
<subsectionHeader confidence="0.99976">
3.3 Computation of the Overall F-measure
</subsectionHeader>
<bodyText confidence="0.9999565">
Once the mappings have been determined between
the clusterings of the system and the expert, the next
step is to compute the F-measure between the two
clusterings. Rather than populating separate con-
tingency tables for every pair of classes, construct
a single contingency table. For every pairwise map-
ping found for the classes in these two clusterings,
populate the YES-YES, YES-NO, and NO-YES cells
of the contingency table appropriately (see Table 2).
Once all the mapped classes have been incorporated
into this contingency table, add every element of all
unmapped classes generated by the system to the
YES-NO cell and every element of all unmapped
classes provided by the expert to the NO-YES cell
of this table. Once all classes in the two clusterings
have been accounted for, calculate the precision, re-
call, and F-measure as explained in (Hatzivassiloglou
and McKeown, 1993).
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999965461538462">
In one of our experiments, the 400 most frequent
nouns in the Merck Veterinary Manual were clus-
tered. Three experts were used to evaluate the gen-
erated noun clusters. Some examples of the classes
that were generated by the system for the veteri-
nary medicine domain are PROBLEM, TREAT-
MENT, ORGAN, DIET, ANIMAL, MEASURE-
MENT, PROCESS, and so on. The results obtained
by comparing these noun classes to the clusterings
provided by three different experts are shown in Ta-
ble 3. We have also experimented with the use of
WordNet to improve the classes obtained by a dis-
tributional technique. Some initial experiments have
shown that WordNet consistently improves the F-
measures for these noun classes by about 0.05 on an
average. Details of these experiments can be found
in (Agarwal, 1995).
It is our belief that the evaluation scheme pre-
sented in this paper is useful for comparing different
clusterings produced by the same system or those
produced by different systems against one provided
by an expert. The resulting precision, recall, and
F-measure should not be treated as a kind of &amp;quot;gold
standard&amp;quot; to represent the quality of these classes
in some absolute sense. It has been our experience
that, as semantic clustering is a highly subjective
task, evaluating a given clustering against different
experts may yield numbers that vary considerably.
However, when different clusterings generated by a
system are compared against the same expert (or
the same set of experts), such relative comparisons
are useful.
The evaluation scheme presented here still suffers
from one major limitation — it is not capable of
evaluating a hierarchy generated by a system against
one provided by an expert. Such evaluations get
complicated because of the restriction of one-to-one
mapping. More work definitely needs to be done in
this area.
</bodyText>
<sectionHeader confidence="0.998598" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997871146341464">
Rajeev Agarwal. 1995. Semantic feature extraction
from technical texts with limited human interven-
tion. Ph.D. thesis, Mississippi State University,
May.
Roberto Basili, Maria Pazienza, and Paola Velardi.
1994. The noisy channel and the braying donkey.
In Proceedings of the ACL Balancing Act Work-
shop, pages 21-28, Las Cruces, New Mexico, July.
Nancy Chincor. 1992. MUC-4 evaluation metrics.
In Proceedings of the Fourth Message Understand-
ing Conference (MUC-4).
Ralph Grishman and John Sterling. 1993. Smooth-
ing of automatically generated selectional con-
straints. In Proceedings of the ARPA Workshop
on Human Language Technology. Morgan Kauf-
mann Publishers, Inc., March.
Vasileios Hatzivassiloglou and Kathleen R. McKe-
own. 1993. Towards the automatic identifica-
tion of adjectival scales: Clustering adjectives ac-
cording to meaning. In Proceedings of the 31st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 172-82.
Francois-Michel Lang and Lynette Hirschman. 1988.
Improved portability and parsing through interac-
tive acquisition of semantic information. In Pro-
ceedings of the Second Conference on Applied Nat-
ural Language Processing, pages 49-57, February.
James Pustejovsky. 1992. The acquisition of lex-
ical semantic knowledge from large corpora. In
Proceedings of the Speech and Natural Language
Workshop, pages 243-48, Harriman, N.Y., Febru-
ary.
Lisa Rau, Paul Jacobs, and Uri Zernik. 1989. In-
formation extraction and text summarization us-
ing linguistic knowledge acquisition. Information
Processing and Management, 25(4):419-28.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania, Decem-
ber. (Institute for Research in Cognitive Science
report IRCS-93-42).
</reference>
<figure confidence="0.9942996">
System
Expert
F-measure
Precision
Recall
</figure>
<page confidence="0.975896">
286
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.890190">
<title confidence="0.999727">Evaluation of Semantic Clusters</title>
<author confidence="0.990789">Rajeev Agarwal</author>
<affiliation confidence="0.999958">Mississippi State University</affiliation>
<address confidence="0.998434">Mississippi State, MS 39762 USA</address>
<email confidence="0.999862">rajeev©cs.msstate.edu</email>
<abstract confidence="0.992924571428572">Semantic clusters of a domain form an important feature that can be useful for performing syntactic and semantic disambiguation. Several attempts have been made to extract the semantic clusters of a domain by probabilistic or taxonomic techniques. However, not much progress has been made in evaluating the obtained semantic clusters. This paper focuses on an evaluation mechanism that can be used to evaluate semantic clusters produced by a system against those provided by human experts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rajeev Agarwal</author>
</authors>
<title>Semantic feature extraction from technical texts with limited human intervention.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Mississippi State University,</institution>
<contexts>
<context position="2154" citStr="Agarwal, 1995" startWordPosition="331" endWordPosition="332"> classes are often domain-specific, their automatic acquisition is not trivial. Such classes can be derived either by distributional means or from existing taxonomies, knowledge bases, dictionaries, thesauruses, and so on. A prime example of the latter is WordNet which has been used to 1The author is currently at Texas Instruments and all inquiries should be addressed to rajeevOcsc.ti.com. provide such semantic classes (Resnik, 1993; Basili et al., 1994) to assist in text understanding. Our efforts to obtain such semantic clusters with limited human intervention have been described elsewhere (Agarwal, 1995). This paper concentrates on the aspect of evaluating the obtained clusters against classes provided by human experts. 2 The Need Although there has been a lot of work done in extracting semantic classes of a given domain, relatively little attention has been paid to the task of evaluating the generated classes. In the absence of an evaluation scheme, the only way to decide if the semantic classes produced by a system are &amp;quot;reasonable&amp;quot; or not is by having an expert analyze them by inspection. Such informal evaluations make it very difficult to compare one set of classes against another and are </context>
<context position="10203" citStr="Agarwal, 1995" startWordPosition="1720" endWordPosition="1721">me examples of the classes that were generated by the system for the veterinary medicine domain are PROBLEM, TREATMENT, ORGAN, DIET, ANIMAL, MEASUREMENT, PROCESS, and so on. The results obtained by comparing these noun classes to the clusterings provided by three different experts are shown in Table 3. We have also experimented with the use of WordNet to improve the classes obtained by a distributional technique. Some initial experiments have shown that WordNet consistently improves the Fmeasures for these noun classes by about 0.05 on an average. Details of these experiments can be found in (Agarwal, 1995). It is our belief that the evaluation scheme presented in this paper is useful for comparing different clusterings produced by the same system or those produced by different systems against one provided by an expert. The resulting precision, recall, and F-measure should not be treated as a kind of &amp;quot;gold standard&amp;quot; to represent the quality of these classes in some absolute sense. It has been our experience that, as semantic clustering is a highly subjective task, evaluating a given clustering against different experts may yield numbers that vary considerably. However, when different clusterings</context>
</contexts>
<marker>Agarwal, 1995</marker>
<rawString>Rajeev Agarwal. 1995. Semantic feature extraction from technical texts with limited human intervention. Ph.D. thesis, Mississippi State University, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Maria Pazienza</author>
<author>Paola Velardi</author>
</authors>
<title>The noisy channel and the braying donkey.</title>
<date>1994</date>
<booktitle>In Proceedings of the ACL Balancing Act Workshop,</booktitle>
<pages>21--28</pages>
<location>Las Cruces, New Mexico,</location>
<contexts>
<context position="1139" citStr="Basili et al., 1994" startWordPosition="171" endWordPosition="174"> that can be used to evaluate semantic clusters produced by a system against those provided by human experts. 1 Introduction&apos; Most natural language processing (NLP) systems are designed to work on certain specific domains and porting them to other domains is often a very timeconsuming and human-intensive process. As the need for applying NLP systems to more and varied domains grows, it becomes increasingly important that some techniques be used to make these systems more portable. Several researchers (Lang and Hirschman, 1988; Rau et al., 1989; Pustejovsky, 1992; Grishrnan and Sterling, 1993; Basili et al., 1994), either directly or indirectly, have addressed issues that assist in making it easier to move an NLP system from one domain to another. One of the reasons for the lack of portability is the need for domain-specific semantic features that such systems often use for lexical, syntactic, and semantic disambiguation. One such feature is the knowledge of the semantic clusters in a domain. Since semantic classes are often domain-specific, their automatic acquisition is not trivial. Such classes can be derived either by distributional means or from existing taxonomies, knowledge bases, dictionaries, </context>
</contexts>
<marker>Basili, Pazienza, Velardi, 1994</marker>
<rawString>Roberto Basili, Maria Pazienza, and Paola Velardi. 1994. The noisy channel and the braying donkey. In Proceedings of the ACL Balancing Act Workshop, pages 21-28, Las Cruces, New Mexico, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chincor</author>
</authors>
<title>MUC-4 evaluation metrics.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth Message Understanding Conference (MUC-4).</booktitle>
<contexts>
<context position="5205" citStr="Chincor, 1992" startWordPosition="857" endWordPosition="858">vided by an ex-pert. Since a word can occur in more than one class, it is important to find some kind of mapping between the classes generated by the system and the classes given by the expert. Such a mapping tells us which class in the system&apos;s clustering maps to which one in the expert&apos;s clustering, and an overall comparison of the clusterings is based on the comparison of the mutually mapping classes. Before we delve deeper into the evaluation process, we must decide on some measure of &amp;quot;closeness&amp;quot; between a pair of classes. We have adopted the F-measure (Hatzivassiloglou and McKeown, 1993; Chincor, 1992). In our computation of the Fmeasure, we construct a contingency table based on the presence or absence of individual elements in the two classes being compared, as opposed to basing it on pairs of words. For example, suppose that Class A is generated by the system and Class B is provided by an expert (as shown in Table 1). The contingency table obtained for this pair of classes is shown in Table 2. The three main steps in the evaluation process are the acquisition of &amp;quot;correct&amp;quot; classes from domain experts, mapping the experts&apos; clustering to that generated by the system, and generating an overa</context>
</contexts>
<marker>Chincor, 1992</marker>
<rawString>Nancy Chincor. 1992. MUC-4 evaluation metrics. In Proceedings of the Fourth Message Understanding Conference (MUC-4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>John Sterling</author>
</authors>
<title>Smoothing of automatically generated selectional constraints.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology.</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.,</publisher>
<marker>Grishman, Sterling, 1993</marker>
<rawString>Ralph Grishman and John Sterling. 1993. Smoothing of automatically generated selectional constraints. In Proceedings of the ARPA Workshop on Human Language Technology. Morgan Kaufmann Publishers, Inc., March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>172--82</pages>
<contexts>
<context position="2926" citStr="Hatzivassiloglou and McKeown (1993)" startWordPosition="464" endWordPosition="467">h there has been a lot of work done in extracting semantic classes of a given domain, relatively little attention has been paid to the task of evaluating the generated classes. In the absence of an evaluation scheme, the only way to decide if the semantic classes produced by a system are &amp;quot;reasonable&amp;quot; or not is by having an expert analyze them by inspection. Such informal evaluations make it very difficult to compare one set of classes against another and are also not very reliable estimates of the quality of a set of classes. It is clear that a formal evaluation scheme would be of great help. Hatzivassiloglou and McKeown (1993) cluster adjectives into partitions and present an interesting evaluation to compare the generated adjective classes against those provided by an expert. Their evaluation scheme bases the comparison between two classes on the presence or absence of pairs of words in them. Their approach involves filling in a YES—NO contingency table based on whether a pair of words (adjectives, in their case) is classified in the same class by the human expert and by the system. This method works very well for partitions. However, if it is used to evaluate sets of classes where the classes may be potentially o</context>
<context position="5189" citStr="Hatzivassiloglou and McKeown, 1993" startWordPosition="853" endWordPosition="856">enerated by a system against one provided by an ex-pert. Since a word can occur in more than one class, it is important to find some kind of mapping between the classes generated by the system and the classes given by the expert. Such a mapping tells us which class in the system&apos;s clustering maps to which one in the expert&apos;s clustering, and an overall comparison of the clusterings is based on the comparison of the mutually mapping classes. Before we delve deeper into the evaluation process, we must decide on some measure of &amp;quot;closeness&amp;quot; between a pair of classes. We have adopted the F-measure (Hatzivassiloglou and McKeown, 1993; Chincor, 1992). In our computation of the Fmeasure, we construct a contingency table based on the presence or absence of individual elements in the two classes being compared, as opposed to basing it on pairs of words. For example, suppose that Class A is generated by the system and Class B is provided by an expert (as shown in Table 1). The contingency table obtained for this pair of classes is shown in Table 2. The three main steps in the evaluation process are the acquisition of &amp;quot;correct&amp;quot; classes from domain experts, mapping the experts&apos; clustering to that generated by the system, and gen</context>
<context position="9393" citStr="Hatzivassiloglou and McKeown, 1993" startWordPosition="1582" endWordPosition="1585">ct a single contingency table. For every pairwise mapping found for the classes in these two clusterings, populate the YES-YES, YES-NO, and NO-YES cells of the contingency table appropriately (see Table 2). Once all the mapped classes have been incorporated into this contingency table, add every element of all unmapped classes generated by the system to the YES-NO cell and every element of all unmapped classes provided by the expert to the NO-YES cell of this table. Once all classes in the two clusterings have been accounted for, calculate the precision, recall, and F-measure as explained in (Hatzivassiloglou and McKeown, 1993). 4 Results and Discussion In one of our experiments, the 400 most frequent nouns in the Merck Veterinary Manual were clustered. Three experts were used to evaluate the generated noun clusters. Some examples of the classes that were generated by the system for the veterinary medicine domain are PROBLEM, TREATMENT, ORGAN, DIET, ANIMAL, MEASUREMENT, PROCESS, and so on. The results obtained by comparing these noun classes to the clusterings provided by three different experts are shown in Table 3. We have also experimented with the use of WordNet to improve the classes obtained by a distributiona</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1993</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1993. Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 172-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois-Michel Lang</author>
<author>Lynette Hirschman</author>
</authors>
<title>Improved portability and parsing through interactive acquisition of semantic information.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="1050" citStr="Lang and Hirschman, 1988" startWordPosition="157" endWordPosition="160">e in evaluating the obtained semantic clusters. This paper focuses on an evaluation mechanism that can be used to evaluate semantic clusters produced by a system against those provided by human experts. 1 Introduction&apos; Most natural language processing (NLP) systems are designed to work on certain specific domains and porting them to other domains is often a very timeconsuming and human-intensive process. As the need for applying NLP systems to more and varied domains grows, it becomes increasingly important that some techniques be used to make these systems more portable. Several researchers (Lang and Hirschman, 1988; Rau et al., 1989; Pustejovsky, 1992; Grishrnan and Sterling, 1993; Basili et al., 1994), either directly or indirectly, have addressed issues that assist in making it easier to move an NLP system from one domain to another. One of the reasons for the lack of portability is the need for domain-specific semantic features that such systems often use for lexical, syntactic, and semantic disambiguation. One such feature is the knowledge of the semantic clusters in a domain. Since semantic classes are often domain-specific, their automatic acquisition is not trivial. Such classes can be derived ei</context>
</contexts>
<marker>Lang, Hirschman, 1988</marker>
<rawString>Francois-Michel Lang and Lynette Hirschman. 1988. Improved portability and parsing through interactive acquisition of semantic information. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 49-57, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The acquisition of lexical semantic knowledge from large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>243--48</pages>
<location>Harriman, N.Y.,</location>
<contexts>
<context position="1087" citStr="Pustejovsky, 1992" startWordPosition="165" endWordPosition="166">rs. This paper focuses on an evaluation mechanism that can be used to evaluate semantic clusters produced by a system against those provided by human experts. 1 Introduction&apos; Most natural language processing (NLP) systems are designed to work on certain specific domains and porting them to other domains is often a very timeconsuming and human-intensive process. As the need for applying NLP systems to more and varied domains grows, it becomes increasingly important that some techniques be used to make these systems more portable. Several researchers (Lang and Hirschman, 1988; Rau et al., 1989; Pustejovsky, 1992; Grishrnan and Sterling, 1993; Basili et al., 1994), either directly or indirectly, have addressed issues that assist in making it easier to move an NLP system from one domain to another. One of the reasons for the lack of portability is the need for domain-specific semantic features that such systems often use for lexical, syntactic, and semantic disambiguation. One such feature is the knowledge of the semantic clusters in a domain. Since semantic classes are often domain-specific, their automatic acquisition is not trivial. Such classes can be derived either by distributional means or from </context>
</contexts>
<marker>Pustejovsky, 1992</marker>
<rawString>James Pustejovsky. 1992. The acquisition of lexical semantic knowledge from large corpora. In Proceedings of the Speech and Natural Language Workshop, pages 243-48, Harriman, N.Y., February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Rau</author>
<author>Paul Jacobs</author>
<author>Uri Zernik</author>
</authors>
<title>Information extraction and text summarization using linguistic knowledge acquisition.</title>
<date>1989</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>25--4</pages>
<contexts>
<context position="1068" citStr="Rau et al., 1989" startWordPosition="161" endWordPosition="164">ed semantic clusters. This paper focuses on an evaluation mechanism that can be used to evaluate semantic clusters produced by a system against those provided by human experts. 1 Introduction&apos; Most natural language processing (NLP) systems are designed to work on certain specific domains and porting them to other domains is often a very timeconsuming and human-intensive process. As the need for applying NLP systems to more and varied domains grows, it becomes increasingly important that some techniques be used to make these systems more portable. Several researchers (Lang and Hirschman, 1988; Rau et al., 1989; Pustejovsky, 1992; Grishrnan and Sterling, 1993; Basili et al., 1994), either directly or indirectly, have addressed issues that assist in making it easier to move an NLP system from one domain to another. One of the reasons for the lack of portability is the need for domain-specific semantic features that such systems often use for lexical, syntactic, and semantic disambiguation. One such feature is the knowledge of the semantic clusters in a domain. Since semantic classes are often domain-specific, their automatic acquisition is not trivial. Such classes can be derived either by distributi</context>
</contexts>
<marker>Rau, Jacobs, Zernik, 1989</marker>
<rawString>Lisa Rau, Paul Jacobs, and Uri Zernik. 1989. Information extraction and text summarization using linguistic knowledge acquisition. Information Processing and Management, 25(4):419-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania, December. (Institute</institution>
<note>for Research in Cognitive Science report IRCS-93-42).</note>
<contexts>
<context position="1976" citStr="Resnik, 1993" startWordPosition="305" endWordPosition="306">features that such systems often use for lexical, syntactic, and semantic disambiguation. One such feature is the knowledge of the semantic clusters in a domain. Since semantic classes are often domain-specific, their automatic acquisition is not trivial. Such classes can be derived either by distributional means or from existing taxonomies, knowledge bases, dictionaries, thesauruses, and so on. A prime example of the latter is WordNet which has been used to 1The author is currently at Texas Instruments and all inquiries should be addressed to rajeevOcsc.ti.com. provide such semantic classes (Resnik, 1993; Basili et al., 1994) to assist in text understanding. Our efforts to obtain such semantic clusters with limited human intervention have been described elsewhere (Agarwal, 1995). This paper concentrates on the aspect of evaluating the obtained clusters against classes provided by human experts. 2 The Need Although there has been a lot of work done in extracting semantic classes of a given domain, relatively little attention has been paid to the task of evaluating the generated classes. In the absence of an evaluation scheme, the only way to decide if the semantic classes produced by a system </context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania, December. (Institute for Research in Cognitive Science report IRCS-93-42).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>