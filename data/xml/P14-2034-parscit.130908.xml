<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000180">
<title confidence="0.979052">
Word Segmentation of Informal Arabic with Domain Adaptation
</title>
<author confidence="0.998353">
Will Monroe, Spence Green, and Christopher D. Manning
</author>
<affiliation confidence="0.995891">
Computer Science Department, Stanford University
</affiliation>
<email confidence="0.993009">
{wmonroe4,spenceg,manning}@stanford.edu
</email>
<sectionHeader confidence="0.993761" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956842105263">
Segmentation of clitics has been shown to
improve accuracy on a variety of Arabic
NLP tasks. However, state-of-the-art Ara-
bic word segmenters are either limited to
formal Modern Standard Arabic, perform-
ing poorly on Arabic text featuring dialectal
vocabulary and grammar, or rely on lin-
guistic knowledge that is hand-tuned for
each dialect. We extend an existing MSA
segmenter with a simple domain adapta-
tion technique and new features in order
to segment informal and dialectal Arabic
text. Experiments show that our system
outperforms existing systems on newswire,
broadcast news and Egyptian dialect, im-
proving segmentation F1 score on a recently
released Egyptian Arabic corpus to 95.1%,
compared to 90.8% for another segmenter
designed specifically for Egyptian Arabic.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998819">
Segmentation of words, clitics, and affixes is essen-
tial for a number of natural language processing
(NLP) applications, including machine translation,
parsing, and speech recognition (Chang et al., 2008;
Tsarfaty, 2006; Kurimo et al., 2006). Segmentation
is a common practice in Arabic NLP due to the lan-
guage&apos;s morphological richness. Specifically, clitic
separation has been shown to improve performance
on Arabic parsing (Green and Manning, 2010) and
Arabic-English machine translation (Habash and
Sadat, 2006). However, the variety of Arabic di-
alects presents challenges in Arabic NLP. Dialectal
Arabic contains non-standard orthography, vocab-
ulary, morphology, and syntax. Tools that depend
on corpora or grammatical properties that only con-
sider formal Modern Standard Arabic (MSA) do
not perform well when confronted with these differ-
ences. The creation of annotated corpora in dialec-
tal Arabic (Maamouri et al., 2006) has promoted
the development of new systems that support di-
alectal Arabic, but these systems tend to be tailored
to specific dialects and require separate efforts for
Egyptian Arabic, Levantine Arabic, Maghrebi Ara-
bic, etc.
We present a single clitic segmentation model
that is accurate on both MSA and informal Arabic.
The model is an extension of the character-level
conditional random field (CRF) model of Green
and DeNero (2012). Our work goes beyond theirs
in three aspects. First, we handle two Arabic ortho-
graphic normalization rules that commonly require
rewriting of tokens after segmentation. Second,
we add new features that improve segmentation ac-
curacy. Third, we show that dialectal data can be
handled in the framework of domain adaptation.
Specifically, we show that even simple feature space
augmentation (Daumé, 2007) yields significant im-
provements in task accuracy.
We compare our work to the original Green and
DeNero model and two other Arabic segmenta-
tion systems: the MADA+TOKAN toolkit v. 3.1
(Habash et al., 2009) and its Egyptian dialect vari-
ant, MADA-ARZ v. 0.4 (Habash et al., 2013). We
demonstrate that our system achieves better perfor-
mance across the board, beating all three systems
on MSA newswire, informal broadcast news, and
Egyptian dialect. Our segmenter achieves a 95.1%
F1 segmentation score evaluated against a gold stan-
dard on Egyptian dialect data, compared to 90.8%
for MADA-ARZ and 92.9% for Green and DeN-
ero. In addition, our model decodes input an order
of magnitude faster than either version of MADA.
Like the Green and DeNero system, but unlike
MADA and MADA-ARZ, our system does not rely
on a morphological analyzer, and can be applied
directly to any dialect for which segmented training
data is available. The source code is available in
the latest public release of the Stanford Word Seg-
menter (http://nlp.stanford.edu/software/
segmenter.shtml).
</bodyText>
<page confidence="0.982398">
206
</page>
<note confidence="0.3777245">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 206–211,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.940184" genericHeader="method">
2 Arabic Word Segmentation Model
</sectionHeader>
<bodyText confidence="0.999935571428571">
A CRF model (Lafferty et al., 2001) defines a distri-
bution p(Y|X; θ), where X = {x1, ... , xN} is the
observed input sequence and Y = {y1, ... , yN} is
the sequence of labels we seek to predict. Green
and DeNero use a linear-chain model with X as
the sequence of input characters, and Y* chosen
according to the decision rule
</bodyText>
<equation confidence="0.997979333333333">
N
Y* = arg max θTφ(X, yi,..., yi−3, i) .
Y i=1
</equation>
<bodyText confidence="0.999294666666667">
where φ is the feature map defined in Section 2.1.
Their model classifies each yi as one of I (contin-
uation of a segment), O (whitespace outside any
segment), B (beginning of a segment), or F (pre-
grouped foreign characters).
Our segmenter expands this label space in order
to handle two Arabic-specific orthographic rules.
In our model, each yi can take on one of the six
values {I, O, B, F, REwAL, REwTA}:
</bodyText>
<listItem confidence="0.9881863">
• REwAL indicates that the current character,
which is always the Arabic letter j l, starts a
new segment and should additionally be trans-
formed into the definite article JI al- when
segmented. This type of transformation occurs
after the prefix J li- “to”.
• REwTA indicates that the current character,
which is always the Arabic letter &amp;quot; t, is a
continuation but should be transformed into
the letter S h when segmented. Arabic orthog-
</listItem>
<bodyText confidence="0.977279">
raphy rules restrict the occurrence of S h to
the word-final position, writing it instead as
u t whenever it is followed by a suffix.
</bodyText>
<subsectionHeader confidence="0.871078">
2.1 Features
</subsectionHeader>
<bodyText confidence="0.995584666666667">
The model of Green and DeNero is a third-order
(i.e., 4-gram) Markov CRF, employing the follow-
ing indicator features:
</bodyText>
<listItem confidence="0.962215461538461">
• a five-character window around the current
character: for each −2 &lt; δ &lt; 2 and 1 &lt; i &lt;
N, the triple (xi+S, δ, yi)
• n-grams consisting of the current character
and up to three preceding characters: for
each 2 &lt; n &lt; 4 and n &lt; i &lt; N,
the character-sequence/label-sequence pair
(xi−n+1 ... xi, yi−n+1 ... yi)
• whether the current character is punctuation
• whether the current character is a digit
• the Unicode block of the current character
• the Unicode character class of the current char-
acter
</listItem>
<bodyText confidence="0.993336666666667">
In addition to these, we include two other types of
features motivated by specific errors the original
system made on Egyptian dialect development data:
</bodyText>
<listItem confidence="0.929193">
• Word length and position within a word: for
each 1 &lt; i &lt; N, the pairs (`, yi), (a, yi), and
(b, yi), where `, a, and b are the total length
of the word containing xi, the number of char-
acters after xi in the word, and the number of
characters before xi in the word, respectively.
Some incorrect segmentations produced by
the original system could be ruled out with the
knowledge of these statistics.
• First and last two characters of the current
</listItem>
<bodyText confidence="0.986904714285714">
word, separately influencing the first two
labels and the last two labels: for each
word consisting of characters xs ... xt, the tu-
ples (xsxs+1, xt−1xt, ysys+1, “begin”) and
(xsxs+1, xt−1xt, yt−1yt, “end”). This set of
features addresses a particular dialectal Arabic
construction, the negation La ma- + [verb] +
�: -sh, which requires a matching prefix and
suffix to be segmented simultaneously. This
feature set also allows the model to take into ac-
count other interactions between the beginning
and end of a word, particularly those involving
the definite article JI al-.
A notable property of this feature set is that it re-
mains highly dialect-agnostic, even though our ad-
ditional features were chosen in response to errors
made on text in Egyptian dialect. In particular,
it does not depend on the existence of a dialect-
specific lexicon or morphological analyzer. As a
result, we expect this model to perform similarly
well when applied to other Arabic dialects.
</bodyText>
<subsectionHeader confidence="0.999154">
2.2 Domain adaptation
</subsectionHeader>
<bodyText confidence="0.999969428571429">
In this work, we train our model to segment Arabic
text drawn from three domains: newswire, which
consists of formal text in MSA; broadcast news,
which contains scripted, formal MSA as well as
extemporaneous dialogue in a mix of MSA and di-
alect; and discussion forum posts written primarily
in Egyptian dialect.
</bodyText>
<page confidence="0.992975">
207
</page>
<table confidence="0.999000333333333">
Model Training Data ATB F1 (%) ARZ TEDEval (%)
BN ATB BN ARZ
GD ATB 97.60 94.87 79.92 98.22 96.81 87.30
GD +BN+ARZ 97.28 96.37 92.90 98.05 97.45 95.01
+Rew ATB 97.55 94.95 79.95 98.72 97.45 87.54
+Rew +BN 97.58 96.60 82.94 98.75 98.18 89.43
+Rew +BN+ARZ 97.30 96.09 92.64 98.59 97.91 95.03
+Rew+DA +BN+ARZ 97.71 96.57 93.87 98.79 98.14 95.86
+Rew+DA+Feat +BN+ARZ 98.36 97.35 95.06 99.14 98.57 96.67
</table>
<tableCaption confidence="0.999072">
Table 1: Development set results. GD is the model of Green and DeNero (2012). Rew is support for
</tableCaption>
<bodyText confidence="0.9969665">
orthographic rewrites with the REwAL and REwTA labels. The fifth row shows the strongest baseline,
which is the GD+Rew model trained on the concatenated training sets from all three treebanks. DA is
domain adaptation via feature space augmentation. Feat adds the additional feature templates described
in section 2.1. ATB is the newswire ATB; BN is the Broadcast News treebank; ARZ is the Egyptian
treebank. Best results (bold) are statistically significant (p &lt; 0.001) relative to the strongest baseline.
The approach to domain adaptation we use is
that of feature space augmentation (Daumé, 2007).
Each indicator feature from the model described
in Section 2.1 is replaced by N + 1 features in
the augmented model, where N is the number of
domains from which the data is drawn (here, N =
3). These N + 1 features consist of the original
feature and N “domain-specific” features, one for
each of the N domains, each of which is active only
when both the original feature is present and the
current text comes from its assigned domain.
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999912285714286">
We train and evaluate on three corpora: parts 1–3 of
the newswire Arabic Treebank (ATB),1 the Broad-
cast News Arabic Treebank (BN),2 and parts 1–8
of the BOLT Phase 1 Egyptian Arabic Treebank
(ARZ).3 These correspond respectively to the do-
mains in section 2.2. We target the segmentation
scheme used by these corpora (leaving morphologi-
cal affixes and the definite article attached). For the
ATB, we use the same split as Chiang et al. (2006).
For each of the other two corpora, we split the data
into 80% training, 10% development, and 10% test
in chronological order by document.4 We train the
Green and DeNero model and our improvements
using L-BFGS with L2 regularization.
</bodyText>
<footnote confidence="0.978552666666667">
1LDC2010T13, LDC2011T09, LDC2010T08
2LDC2012T07
3LDC2012E{93,98,89,99,107,125}, LDC2013E{12,21}
4These splits are publicly available at
http://nlp.stanford.edu/software/parser-
arabic-data-splits.shtml.
</footnote>
<subsectionHeader confidence="0.984696">
3.1 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.9994808">
We use two evaluation metrics in our experiments.
The first is an F1 precision-recall measure, ignoring
orthographic rewrites. F1 scores provide a more
informative assessment of performance than word-
level or character-level accuracy scores, as over 80%
of tokens in the development sets consist of only
one segment, with an average of one segmentation
every 4.7 tokens (or one every 20.4 characters).
The second metric we use is the TEDEval met-
ric (Tsarfaty et al., 2012). TEDEval was devel-
oped to evaluate joint segmentation and parsing5
in Hebrew, which requires a greater variety of or-
thographic rewrites than those possible in Arabic.
Its edit distance-based scoring algorithm is robust
enough to handle the rewrites produced by both
MADA and our segmenter.
We measure the statistical significance of differ-
ences in these metrics with an approximate ran-
domization test (Yeh, 2000; Padó, 2006), with
R = 10,000 samples.
</bodyText>
<subsectionHeader confidence="0.903341">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.9999385">
Table 1 contains results on the development set
for the model of Green and DeNero and our im-
provements. Using domain adaptation alone helps
performance on two of the three datasets (with a sta-
tistically insignificant decrease on broadcast news),
and that our additional features further improve
</bodyText>
<footnote confidence="0.759649">
5In order to evaluate segmentation in isolation, we convert
each segmented sentence from both the model output and
the gold standard to a flat tree with all segments descending
directly from the root.
</footnote>
<page confidence="0.992369">
208
</page>
<table confidence="0.9916392">
F1 (%) TEDEval (%)
ATB BN ARZ ATB BN ARZ
MADA 97.36 94.54 78.35 97.62 96.96 86.78
MADA-ARZ 92.83 91.89 90.76 91.26 91.10 90.39
GD+Rew+DA+Feat 98.30 97.17 95.13 99.10 98.42 96.75
</table>
<tableCaption confidence="0.9623965">
Table 2: Test set results. Our final model (last row) is trained on all available data (ATB+BN+ARZ). Best
results (bold) are statistically significant (p &lt; 0.001) relative to each MADA version.
</tableCaption>
<table confidence="0.9996215">
ATB BN ARZ
MADA 705.6 + 5.1 472.0 + 0.8 767.8 + 1.9
MADA-ARZ 784.7 + 1.6 492.1 + 4.2 779.0 + 2.7
GD+Rew+DA+Feat 90.0 + 1.0 59.5 + 0.3 72.7 + 0.2
</table>
<tableCaption confidence="0.7482205">
Table 3: Wallclock time (in seconds) for MADA, MADA-ARZ, and our model for decoding each of
the three development datasets. Means and standard deviations were computed for 10 independent runs.
MADA and MADA-ARZ are single-threaded. Our segmenter supports multithreaded execution, but the
times reported here are for single-threaded runs.
</tableCaption>
<bodyText confidence="0.9828721">
segmentation on all datasets. Table 2 shows the
segmentation scores our model achieves when eval-
uated on the three test sets, as well as the results for
MADA and MADA-ARZ. Our segmenter achieves
higher scores than MADA and MADA-ARZ on all
datasets under both evaluation metrics. In addi-
tion, our segmenter is faster than MADA. Table 3
compares the running times of the three systems.
Our segmenter achieves a 7x or more speedup over
MADA and MADA-ARZ on all datasets.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="method">
4 Error Analysis
</sectionHeader>
<bodyText confidence="0.9999976">
We sampled 100 errors randomly from all errors
made by our final model (trained on all three
datasets with domain adaptation and additional fea-
tures) on the ARZ development set; see Table 4.
These errors fall into three general categories:
</bodyText>
<listItem confidence="0.995053">
• typographical errors and annotation inconsis-
tencies in the gold data;
• errors that can be fixed with a fuller analysis
of just the problematic token, and therefore
represent a deficiency in the feature set; and
• errors that would require additional context or
sophisticated semantic awareness to fix.
</listItem>
<subsectionHeader confidence="0.9756845">
4.1 Typographical errors and annotation
inconsistencies
</subsectionHeader>
<bodyText confidence="0.9675561">
Of the 100 errors we sampled, 33 are due to typo-
graphical errors or inconsistencies in the gold data.
We classify 7 as typos and 26 as annotation incon-
sistencies, although the distinction between the two
is murky: typos are intentionally preserved in the
treebank data, but segmentation of typos varies de-
pending on how well they can be reconciled with
standard Arabic orthography. Four of the seven
typos are the result of a missing space, such as:
�
</bodyText>
<listItem confidence="0.976471">
• �AJJAK.Qê‚, yashar-bi-&apos;l-laydla “staysawakeat-
night” (QîD„j yashar +K. bi- + JLb@ al-laydla)
• �à�@A�J:ÊÔ« milatnd-an “madeus” ( �IÊÔ«
,amilat + A J� -nd + �)�@ wn)
</listItem>
<bodyText confidence="0.971961">
The first example is segmented in the Egyptian tree-
bank but is left unsegmented by our system; the
second is left as a single token in the treebank but is
split into the above three segments by our system.
Of the annotation inconsistencies that do not in-
volve typographical errors, a handful are segmen-
tation mistakes; however, in the majority of these
cases, the annotator chose not to segment a word
for justifiable but arbitrary reasons. In particular, a
few colloquial “filler” expressions are sometimes
not segmented, despite being compound Arabic
words that are segmented elsewhere in the data.
These include A�J;.P rabbind “[our] Lord” (oath);
AÓY:« andamd “when”/“while”; and 1/2J�Êg� khalla-
�
</bodyText>
<footnote confidence="0.683205333333333">
k “keep”/“stay”. Also, tokens containing foreign
words are sometimes not segmented, despite car-
rying Arabic affixes. An example of this isQ~~‚Óð
</footnote>
<page confidence="0.994288">
209
</page>
<table confidence="0.931010181818182">
Category # of errors
Abnormal gold data 33
Typographical error 7
Annotation inconsistency 26
Need full-token features 36
Need more context 31
Bð wl¯a 5
A J� -n¯a: verb/pron 7
ù~ -y: nisba/pron 4
�
other 15
</table>
<tableCaption confidence="0.9818835">
Table 4: Counts of error categories (out of 100
randomly sampled ARZ development set errors).
</tableCaption>
<bodyText confidence="0.8660315">
wamistur “and Mister [English]”, which could be
segmented as ð wa- + Qi‚Ó mistur.
</bodyText>
<subsectionHeader confidence="0.969418">
4.2 Features too local
</subsectionHeader>
<bodyText confidence="0.9997338">
In 36 of the 100 sampled errors, we conjecture that
the presence of the error indicates a shortcoming
of the feature set, resulting in segmentations that
make sense locally but are not plausible given the
full token. Two examples of these are:
</bodyText>
<listItem confidence="0.913699">
• é�®K
</listItem>
<bodyText confidence="0.774453526315789">
~
Q¢ ¯ð wafit.ar¯ıqah “and in the way” seg-
mented as ð wa- + aa�Q¢s fit.ar¯ıqah (correct
analysis is ð wa- + _i fi- + kkQ£ t.ar¯ıqah).
Q¢s ft.r “break”/“breakfast” is a common Ara-
bic root, but the presence of †~ q should indi-
cate thatQ¢s ft.r is not the root in this case.
•ÑêÒîE
Bð wal¯ayuhimmhum “and it’s not im-
portant to them” segmented as ð wa- + J
li- + �ÑîE
A� -ayuhimm + Ñê~ -hum (correct
analysis is ð wa- + B l¯a + (;E� yuhimm +
Ñê~ -hum). The 4-character window éK
B l¯ayh
occurs commonly with a segment boundary
after the È l, but the segment (;E
A� -ayuhimm
is not a well-formed Arabic word.
</bodyText>
<subsectionHeader confidence="0.9933615">
4.3 Context-sensitive segmentations and
multiple word senses
</subsectionHeader>
<bodyText confidence="0.941887551724138">
In the remaining 31 of 100 errors, external context
is needed. In many of these, it is not clear how to
address the error without sophisticated semantic
reasoning about the surrounding sentence.
One token accounts for five of these errors: Bð
wl¯a, which in Egyptian dialect can be analyzed as
�
ð wa- + B l¯a “and [do/does] not” or as Bð wall¯a
“or”. In a few cases, either is syntactically correct,
and the meaning must be inferred from context.
Two other ambiguities are a frequent cause of
error and seem to require sophisticated disambigua-
tion. The first is A J� -n¯a, which is both a first person
plural object pronoun and a first person plural past
tense ending. The former is segmented, while the
latter is not. An example of this is the pair A�JÒÊ«
,ilmun¯a “our knowledge” (ÕÎ« ,ilmu + A J� -n¯a) ver-
sus A�JÒÊ« ,alimn¯a “we knew” (one segment). The
other is ù
~ -y, which is both a first person singular
possessive pronoun and the nisba adjective ending
(which turns a noun into an adjective meaning “of
or related to”); only the former is segmented. One
example of this distinction that appeared in the de-
velopment set is the pair ú�«ñ �“ñÓ mawd. ¯u,¯ı “my
topic” (¨ñ�“ñÓ mawd. ¯u,+ ù
~ -y) versus ~ú
«ñ “ñÓ
mawd. ¯u,¯ıy “topical”, “objective”.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9995165625">
In this paper we demonstrate substantial gains on
Arabic clitic segmentation for both formal and
dialectal text using a single model with dialect-
independent features and a simple domain adap-
tation strategy. We present a new Arabic segmenter
which performs better than tools employing sophis-
ticated linguistic analysis, while also giving im-
pressive speed improvements. We evaluated our
segmenter on broadcast news and Egyptian Arabic
due to the current availability of annotated data in
these domains. However, as data for other Arabic di-
alects and genres becomes available, we expect that
the model’s simplicity and the domain adaptation
method we use will allow the system to be applied
to these dialects with minimal effort and without a
loss of performance in the original domains.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999669333333333">
We thank the three anonymous reviewers, and
Reut Tsarfaty for valuable correspondence regard-
ing TEDEval. The second author is supported
by a National Science Foundation Graduate Re-
search Fellowship. This work was supported by
the Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Transla-
tion (BOLT) program through IBM. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the view of DARPA
or the US government.
</bodyText>
<page confidence="0.997273">
210
</page>
<sectionHeader confidence="0.995791" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99972578">
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In WMT.
David Chiang, Mona T. Diab, Nizar Habash, Owen
Rambow, and Safiullah Shareef. 2006. Parsing Ara-
bic dialects. In EACL.
Hal Daumé, III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Spence Green and John DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In ACL.
Spence Green and Christopher D. Manning. 2010. Bet-
ter Arabic parsing: Baselines, evaluations, and anal-
ysis. In COLING.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In NAACL, Short Papers.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In MEDAR.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological anal-
ysis and disambiguation for dialectal Arabic. In HLT-
NAACL.
Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Si-
ivola, Teemu Hirsimäki, Janne Pylkkönen, Tanel
Alumäe, and Murat Saraclar. 2006. Unlimited
vocabulary speech recognition for agglutinative lan-
guages. In HLT-NAACL.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML.
Mohamed Maamouri, Ann Bies, Tim Buckwalter,
Mona Diab, Nizar Habash, Owen Rambow, and
Dalila Tabessi. 2006. Developing and using a pilot
dialectal Arabic treebank. In LREC.
Sebastian Padó, 2006. User’s guide to sigf:
Signi�cance testing by approximate randomisa-
tion. http://www.nlpado.de/~sebastian/
software/sigf.shtml.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Joint evaluation of morphological segmenta-
tion and syntactic parsing. In ACL, Short Papers.
Reut Tsarfaty. 2006. Integrated morphological and
syntactic disambiguation for Modern Hebrew. In
COLING-ACL.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In COLING.
</reference>
<page confidence="0.998788">
211
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.804537">
<title confidence="0.993406">Word Segmentation of Informal Arabic with Domain Adaptation</title>
<author confidence="0.929861">Spence Green Monroe</author>
<author confidence="0.929861">D</author>
<affiliation confidence="0.977994">Computer Science Department, Stanford</affiliation>
<email confidence="0.993657">wmonroe4@stanford.edu</email>
<email confidence="0.993657">spenceg@stanford.edu</email>
<email confidence="0.993657">manning@stanford.edu</email>
<abstract confidence="0.9901202">Segmentation of clitics has been shown to improve accuracy on a variety of Arabic NLP tasks. However, state-of-the-art Arabic word segmenters are either limited to formal Modern Standard Arabic, performing poorly on Arabic text featuring dialectal vocabulary and grammar, or rely on linguistic knowledge that is hand-tuned for each dialect. We extend an existing MSA segmenter with a simple domain adaptation technique and new features in order to segment informal and dialectal Arabic text. Experiments show that our system outperforms existing systems on newswire, broadcast news and Egyptian dialect, imsegmentation on a recently released Egyptian Arabic corpus to 95.1%, compared to 90.8% for another segmenter designed specifically for Egyptian Arabic.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="1205" citStr="Chang et al., 2008" startWordPosition="168" endWordPosition="171">r with a simple domain adaptation technique and new features in order to segment informal and dialectal Arabic text. Experiments show that our system outperforms existing systems on newswire, broadcast news and Egyptian dialect, improving segmentation F1 score on a recently released Egyptian Arabic corpus to 95.1%, compared to 90.8% for another segmenter designed specifically for Egyptian Arabic. 1 Introduction Segmentation of words, clitics, and affixes is essential for a number of natural language processing (NLP) applications, including machine translation, parsing, and speech recognition (Chang et al., 2008; Tsarfaty, 2006; Kurimo et al., 2006). Segmentation is a common practice in Arabic NLP due to the language&apos;s morphological richness. Specifically, clitic separation has been shown to improve performance on Arabic parsing (Green and Manning, 2010) and Arabic-English machine translation (Habash and Sadat, 2006). However, the variety of Arabic dialects presents challenges in Arabic NLP. Dialectal Arabic contains non-standard orthography, vocabulary, morphology, and syntax. Tools that depend on corpora or grammatical properties that only consider formal Modern Standard Arabic (MSA) do not perform</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Mona T Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Safiullah Shareef</author>
</authors>
<title>Parsing Arabic dialects.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="9953" citStr="Chiang et al. (2006)" startWordPosition="1636" endWordPosition="1639">features, one for each of the N domains, each of which is active only when both the original feature is present and the current text comes from its assigned domain. 3 Experiments We train and evaluate on three corpora: parts 1–3 of the newswire Arabic Treebank (ATB),1 the Broadcast News Arabic Treebank (BN),2 and parts 1–8 of the BOLT Phase 1 Egyptian Arabic Treebank (ARZ).3 These correspond respectively to the domains in section 2.2. We target the segmentation scheme used by these corpora (leaving morphological affixes and the definite article attached). For the ATB, we use the same split as Chiang et al. (2006). For each of the other two corpora, we split the data into 80% training, 10% development, and 10% test in chronological order by document.4 We train the Green and DeNero model and our improvements using L-BFGS with L2 regularization. 1LDC2010T13, LDC2011T09, LDC2010T08 2LDC2012T07 3LDC2012E{93,98,89,99,107,125}, LDC2013E{12,21} 4These splits are publicly available at http://nlp.stanford.edu/software/parserarabic-data-splits.shtml. 3.1 Evaluation metrics We use two evaluation metrics in our experiments. The first is an F1 precision-recall measure, ignoring orthographic rewrites. F1 scores prov</context>
</contexts>
<marker>Chiang, Diab, Habash, Rambow, Shareef, 2006</marker>
<rawString>David Chiang, Mona T. Diab, Nizar Habash, Owen Rambow, and Safiullah Shareef. 2006. Parsing Arabic dialects. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daumé</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2764" citStr="Daumé, 2007" startWordPosition="407" endWordPosition="408"> We present a single clitic segmentation model that is accurate on both MSA and informal Arabic. The model is an extension of the character-level conditional random field (CRF) model of Green and DeNero (2012). Our work goes beyond theirs in three aspects. First, we handle two Arabic orthographic normalization rules that commonly require rewriting of tokens after segmentation. Second, we add new features that improve segmentation accuracy. Third, we show that dialectal data can be handled in the framework of domain adaptation. Specifically, we show that even simple feature space augmentation (Daumé, 2007) yields significant improvements in task accuracy. We compare our work to the original Green and DeNero model and two other Arabic segmentation systems: the MADA+TOKAN toolkit v. 3.1 (Habash et al., 2009) and its Egyptian dialect variant, MADA-ARZ v. 0.4 (Habash et al., 2013). We demonstrate that our system achieves better performance across the board, beating all three systems on MSA newswire, informal broadcast news, and Egyptian dialect. Our segmenter achieves a 95.1% F1 segmentation score evaluated against a gold standard on Egyptian dialect data, compared to 90.8% for MADA-ARZ and 92.9% f</context>
<context position="9060" citStr="Daumé, 2007" startWordPosition="1480" endWordPosition="1481">s support for orthographic rewrites with the REwAL and REwTA labels. The fifth row shows the strongest baseline, which is the GD+Rew model trained on the concatenated training sets from all three treebanks. DA is domain adaptation via feature space augmentation. Feat adds the additional feature templates described in section 2.1. ATB is the newswire ATB; BN is the Broadcast News treebank; ARZ is the Egyptian treebank. Best results (bold) are statistically significant (p &lt; 0.001) relative to the strongest baseline. The approach to domain adaptation we use is that of feature space augmentation (Daumé, 2007). Each indicator feature from the model described in Section 2.1 is replaced by N + 1 features in the augmented model, where N is the number of domains from which the data is drawn (here, N = 3). These N + 1 features consist of the original feature and N “domain-specific” features, one for each of the N domains, each of which is active only when both the original feature is present and the current text comes from its assigned domain. 3 Experiments We train and evaluate on three corpora: parts 1–3 of the newswire Arabic Treebank (ATB),1 the Broadcast News Arabic Treebank (BN),2 and parts 1–8 of</context>
</contexts>
<marker>Daumé, 2007</marker>
<rawString>Hal Daumé, III. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>John DeNero</author>
</authors>
<title>A class-based agreement model for generating accurately inflected translations.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2361" citStr="Green and DeNero (2012)" startWordPosition="344" endWordPosition="347">at only consider formal Modern Standard Arabic (MSA) do not perform well when confronted with these differences. The creation of annotated corpora in dialectal Arabic (Maamouri et al., 2006) has promoted the development of new systems that support dialectal Arabic, but these systems tend to be tailored to specific dialects and require separate efforts for Egyptian Arabic, Levantine Arabic, Maghrebi Arabic, etc. We present a single clitic segmentation model that is accurate on both MSA and informal Arabic. The model is an extension of the character-level conditional random field (CRF) model of Green and DeNero (2012). Our work goes beyond theirs in three aspects. First, we handle two Arabic orthographic normalization rules that commonly require rewriting of tokens after segmentation. Second, we add new features that improve segmentation accuracy. Third, we show that dialectal data can be handled in the framework of domain adaptation. Specifically, we show that even simple feature space augmentation (Daumé, 2007) yields significant improvements in task accuracy. We compare our work to the original Green and DeNero model and two other Arabic segmentation systems: the MADA+TOKAN toolkit v. 3.1 (Habash et al.</context>
<context position="8441" citStr="Green and DeNero (2012)" startWordPosition="1381" endWordPosition="1384">d, formal MSA as well as extemporaneous dialogue in a mix of MSA and dialect; and discussion forum posts written primarily in Egyptian dialect. 207 Model Training Data ATB F1 (%) ARZ TEDEval (%) BN ATB BN ARZ GD ATB 97.60 94.87 79.92 98.22 96.81 87.30 GD +BN+ARZ 97.28 96.37 92.90 98.05 97.45 95.01 +Rew ATB 97.55 94.95 79.95 98.72 97.45 87.54 +Rew +BN 97.58 96.60 82.94 98.75 98.18 89.43 +Rew +BN+ARZ 97.30 96.09 92.64 98.59 97.91 95.03 +Rew+DA +BN+ARZ 97.71 96.57 93.87 98.79 98.14 95.86 +Rew+DA+Feat +BN+ARZ 98.36 97.35 95.06 99.14 98.57 96.67 Table 1: Development set results. GD is the model of Green and DeNero (2012). Rew is support for orthographic rewrites with the REwAL and REwTA labels. The fifth row shows the strongest baseline, which is the GD+Rew model trained on the concatenated training sets from all three treebanks. DA is domain adaptation via feature space augmentation. Feat adds the additional feature templates described in section 2.1. ATB is the newswire ATB; BN is the Broadcast News treebank; ARZ is the Egyptian treebank. Best results (bold) are statistically significant (p &lt; 0.001) relative to the strongest baseline. The approach to domain adaptation we use is that of feature space augment</context>
</contexts>
<marker>Green, DeNero, 2012</marker>
<rawString>Spence Green and John DeNero. 2012. A class-based agreement model for generating accurately inflected translations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Christopher D Manning</author>
</authors>
<title>Better Arabic parsing: Baselines, evaluations, and analysis.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="1452" citStr="Green and Manning, 2010" startWordPosition="205" endWordPosition="208">gmentation F1 score on a recently released Egyptian Arabic corpus to 95.1%, compared to 90.8% for another segmenter designed specifically for Egyptian Arabic. 1 Introduction Segmentation of words, clitics, and affixes is essential for a number of natural language processing (NLP) applications, including machine translation, parsing, and speech recognition (Chang et al., 2008; Tsarfaty, 2006; Kurimo et al., 2006). Segmentation is a common practice in Arabic NLP due to the language&apos;s morphological richness. Specifically, clitic separation has been shown to improve performance on Arabic parsing (Green and Manning, 2010) and Arabic-English machine translation (Habash and Sadat, 2006). However, the variety of Arabic dialects presents challenges in Arabic NLP. Dialectal Arabic contains non-standard orthography, vocabulary, morphology, and syntax. Tools that depend on corpora or grammatical properties that only consider formal Modern Standard Arabic (MSA) do not perform well when confronted with these differences. The creation of annotated corpora in dialectal Arabic (Maamouri et al., 2006) has promoted the development of new systems that support dialectal Arabic, but these systems tend to be tailored to specifi</context>
</contexts>
<marker>Green, Manning, 2010</marker>
<rawString>Spence Green and Christopher D. Manning. 2010. Better Arabic parsing: Baselines, evaluations, and analysis. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Fatiha Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In NAACL, Short Papers.</booktitle>
<contexts>
<context position="1516" citStr="Habash and Sadat, 2006" startWordPosition="213" endWordPosition="216"> to 95.1%, compared to 90.8% for another segmenter designed specifically for Egyptian Arabic. 1 Introduction Segmentation of words, clitics, and affixes is essential for a number of natural language processing (NLP) applications, including machine translation, parsing, and speech recognition (Chang et al., 2008; Tsarfaty, 2006; Kurimo et al., 2006). Segmentation is a common practice in Arabic NLP due to the language&apos;s morphological richness. Specifically, clitic separation has been shown to improve performance on Arabic parsing (Green and Manning, 2010) and Arabic-English machine translation (Habash and Sadat, 2006). However, the variety of Arabic dialects presents challenges in Arabic NLP. Dialectal Arabic contains non-standard orthography, vocabulary, morphology, and syntax. Tools that depend on corpora or grammatical properties that only consider formal Modern Standard Arabic (MSA) do not perform well when confronted with these differences. The creation of annotated corpora in dialectal Arabic (Maamouri et al., 2006) has promoted the development of new systems that support dialectal Arabic, but these systems tend to be tailored to specific dialects and require separate efforts for Egyptian Arabic, Lev</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In NAACL, Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization.</title>
<date>2009</date>
<booktitle>In MEDAR.</booktitle>
<contexts>
<context position="2968" citStr="Habash et al., 2009" startWordPosition="439" endWordPosition="442">DeNero (2012). Our work goes beyond theirs in three aspects. First, we handle two Arabic orthographic normalization rules that commonly require rewriting of tokens after segmentation. Second, we add new features that improve segmentation accuracy. Third, we show that dialectal data can be handled in the framework of domain adaptation. Specifically, we show that even simple feature space augmentation (Daumé, 2007) yields significant improvements in task accuracy. We compare our work to the original Green and DeNero model and two other Arabic segmentation systems: the MADA+TOKAN toolkit v. 3.1 (Habash et al., 2009) and its Egyptian dialect variant, MADA-ARZ v. 0.4 (Habash et al., 2013). We demonstrate that our system achieves better performance across the board, beating all three systems on MSA newswire, informal broadcast news, and Egyptian dialect. Our segmenter achieves a 95.1% F1 segmentation score evaluated against a gold standard on Egyptian dialect data, compared to 90.8% for MADA-ARZ and 92.9% for Green and DeNero. In addition, our model decodes input an order of magnitude faster than either version of MADA. Like the Green and DeNero system, but unlike MADA and MADA-ARZ, our system does not rely</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2009. MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization. In MEDAR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ryan Roth</author>
<author>Owen Rambow</author>
<author>Ramy Eskander</author>
<author>Nadi Tomeh</author>
</authors>
<title>Morphological analysis and disambiguation for dialectal Arabic. In HLTNAACL.</title>
<date>2013</date>
<contexts>
<context position="3040" citStr="Habash et al., 2013" startWordPosition="452" endWordPosition="455">andle two Arabic orthographic normalization rules that commonly require rewriting of tokens after segmentation. Second, we add new features that improve segmentation accuracy. Third, we show that dialectal data can be handled in the framework of domain adaptation. Specifically, we show that even simple feature space augmentation (Daumé, 2007) yields significant improvements in task accuracy. We compare our work to the original Green and DeNero model and two other Arabic segmentation systems: the MADA+TOKAN toolkit v. 3.1 (Habash et al., 2009) and its Egyptian dialect variant, MADA-ARZ v. 0.4 (Habash et al., 2013). We demonstrate that our system achieves better performance across the board, beating all three systems on MSA newswire, informal broadcast news, and Egyptian dialect. Our segmenter achieves a 95.1% F1 segmentation score evaluated against a gold standard on Egyptian dialect data, compared to 90.8% for MADA-ARZ and 92.9% for Green and DeNero. In addition, our model decodes input an order of magnitude faster than either version of MADA. Like the Green and DeNero system, but unlike MADA and MADA-ARZ, our system does not rely on a morphological analyzer, and can be applied directly to any dialect</context>
</contexts>
<marker>Habash, Roth, Rambow, Eskander, Tomeh, 2013</marker>
<rawString>Nizar Habash, Ryan Roth, Owen Rambow, Ramy Eskander, and Nadi Tomeh. 2013. Morphological analysis and disambiguation for dialectal Arabic. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
</authors>
<title>Antti Puurula, Ebru Arisoy, Vesa Siivola, Teemu Hirsimäki, Janne Pylkkönen, Tanel Alumäe, and Murat Saraclar.</title>
<date>2006</date>
<marker>Kurimo, 2006</marker>
<rawString>Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Siivola, Teemu Hirsimäki, Janne Pylkkönen, Tanel Alumäe, and Murat Saraclar. 2006. Unlimited vocabulary speech recognition for agglutinative languages. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="4114" citStr="Lafferty et al., 2001" startWordPosition="616" endWordPosition="619">en and DeNero system, but unlike MADA and MADA-ARZ, our system does not rely on a morphological analyzer, and can be applied directly to any dialect for which segmented training data is available. The source code is available in the latest public release of the Stanford Word Segmenter (http://nlp.stanford.edu/software/ segmenter.shtml). 206 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 206–211, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2 Arabic Word Segmentation Model A CRF model (Lafferty et al., 2001) defines a distribution p(Y|X; θ), where X = {x1, ... , xN} is the observed input sequence and Y = {y1, ... , yN} is the sequence of labels we seek to predict. Green and DeNero use a linear-chain model with X as the sequence of input characters, and Y* chosen according to the decision rule N Y* = arg max θTφ(X, yi,..., yi−3, i) . Y i=1 where φ is the feature map defined in Section 2.1. Their model classifies each yi as one of I (continuation of a segment), O (whitespace outside any segment), B (beginning of a segment), or F (pregrouped foreign characters). Our segmenter expands this label spac</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
<author>Mona Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Dalila Tabessi</author>
</authors>
<title>Developing and using a pilot dialectal Arabic treebank.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="1928" citStr="Maamouri et al., 2006" startWordPosition="275" endWordPosition="278">nguage&apos;s morphological richness. Specifically, clitic separation has been shown to improve performance on Arabic parsing (Green and Manning, 2010) and Arabic-English machine translation (Habash and Sadat, 2006). However, the variety of Arabic dialects presents challenges in Arabic NLP. Dialectal Arabic contains non-standard orthography, vocabulary, morphology, and syntax. Tools that depend on corpora or grammatical properties that only consider formal Modern Standard Arabic (MSA) do not perform well when confronted with these differences. The creation of annotated corpora in dialectal Arabic (Maamouri et al., 2006) has promoted the development of new systems that support dialectal Arabic, but these systems tend to be tailored to specific dialects and require separate efforts for Egyptian Arabic, Levantine Arabic, Maghrebi Arabic, etc. We present a single clitic segmentation model that is accurate on both MSA and informal Arabic. The model is an extension of the character-level conditional random field (CRF) model of Green and DeNero (2012). Our work goes beyond theirs in three aspects. First, we handle two Arabic orthographic normalization rules that commonly require rewriting of tokens after segmentati</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Diab, Habash, Rambow, Tabessi, 2006</marker>
<rawString>Mohamed Maamouri, Ann Bies, Tim Buckwalter, Mona Diab, Nizar Habash, Owen Rambow, and Dalila Tabessi. 2006. Developing and using a pilot dialectal Arabic treebank. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Padó</author>
</authors>
<title>User’s guide to sigf: Signi�cance testing by approximate randomisation.</title>
<date>2006</date>
<note>http://www.nlpado.de/~sebastian/ software/sigf.shtml.</note>
<contexts>
<context position="11308" citStr="Padó, 2006" startWordPosition="1836" endWordPosition="1837">s consist of only one segment, with an average of one segmentation every 4.7 tokens (or one every 20.4 characters). The second metric we use is the TEDEval metric (Tsarfaty et al., 2012). TEDEval was developed to evaluate joint segmentation and parsing5 in Hebrew, which requires a greater variety of orthographic rewrites than those possible in Arabic. Its edit distance-based scoring algorithm is robust enough to handle the rewrites produced by both MADA and our segmenter. We measure the statistical significance of differences in these metrics with an approximate randomization test (Yeh, 2000; Padó, 2006), with R = 10,000 samples. 3.2 Results Table 1 contains results on the development set for the model of Green and DeNero and our improvements. Using domain adaptation alone helps performance on two of the three datasets (with a statistically insignificant decrease on broadcast news), and that our additional features further improve 5In order to evaluate segmentation in isolation, we convert each segmented sentence from both the model output and the gold standard to a flat tree with all segments descending directly from the root. 208 F1 (%) TEDEval (%) ATB BN ARZ ATB BN ARZ MADA 97.36 94.54 78.</context>
</contexts>
<marker>Padó, 2006</marker>
<rawString>Sebastian Padó, 2006. User’s guide to sigf: Signi�cance testing by approximate randomisation. http://www.nlpado.de/~sebastian/ software/sigf.shtml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Joakim Nivre</author>
<author>Evelina Andersson</author>
</authors>
<title>Joint evaluation of morphological segmentation and syntactic parsing.</title>
<date>2012</date>
<booktitle>In ACL, Short Papers.</booktitle>
<contexts>
<context position="10883" citStr="Tsarfaty et al., 2012" startWordPosition="1768" endWordPosition="1771">,125}, LDC2013E{12,21} 4These splits are publicly available at http://nlp.stanford.edu/software/parserarabic-data-splits.shtml. 3.1 Evaluation metrics We use two evaluation metrics in our experiments. The first is an F1 precision-recall measure, ignoring orthographic rewrites. F1 scores provide a more informative assessment of performance than wordlevel or character-level accuracy scores, as over 80% of tokens in the development sets consist of only one segment, with an average of one segmentation every 4.7 tokens (or one every 20.4 characters). The second metric we use is the TEDEval metric (Tsarfaty et al., 2012). TEDEval was developed to evaluate joint segmentation and parsing5 in Hebrew, which requires a greater variety of orthographic rewrites than those possible in Arabic. Its edit distance-based scoring algorithm is robust enough to handle the rewrites produced by both MADA and our segmenter. We measure the statistical significance of differences in these metrics with an approximate randomization test (Yeh, 2000; Padó, 2006), with R = 10,000 samples. 3.2 Results Table 1 contains results on the development set for the model of Green and DeNero and our improvements. Using domain adaptation alone he</context>
</contexts>
<marker>Tsarfaty, Nivre, Andersson, 2012</marker>
<rawString>Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2012. Joint evaluation of morphological segmentation and syntactic parsing. In ACL, Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
</authors>
<title>Integrated morphological and syntactic disambiguation for Modern Hebrew.</title>
<date>2006</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="1221" citStr="Tsarfaty, 2006" startWordPosition="172" endWordPosition="173">in adaptation technique and new features in order to segment informal and dialectal Arabic text. Experiments show that our system outperforms existing systems on newswire, broadcast news and Egyptian dialect, improving segmentation F1 score on a recently released Egyptian Arabic corpus to 95.1%, compared to 90.8% for another segmenter designed specifically for Egyptian Arabic. 1 Introduction Segmentation of words, clitics, and affixes is essential for a number of natural language processing (NLP) applications, including machine translation, parsing, and speech recognition (Chang et al., 2008; Tsarfaty, 2006; Kurimo et al., 2006). Segmentation is a common practice in Arabic NLP due to the language&apos;s morphological richness. Specifically, clitic separation has been shown to improve performance on Arabic parsing (Green and Manning, 2010) and Arabic-English machine translation (Habash and Sadat, 2006). However, the variety of Arabic dialects presents challenges in Arabic NLP. Dialectal Arabic contains non-standard orthography, vocabulary, morphology, and syntax. Tools that depend on corpora or grammatical properties that only consider formal Modern Standard Arabic (MSA) do not perform well when confr</context>
</contexts>
<marker>Tsarfaty, 2006</marker>
<rawString>Reut Tsarfaty. 2006. Integrated morphological and syntactic disambiguation for Modern Hebrew. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="11295" citStr="Yeh, 2000" startWordPosition="1834" endWordPosition="1835">lopment sets consist of only one segment, with an average of one segmentation every 4.7 tokens (or one every 20.4 characters). The second metric we use is the TEDEval metric (Tsarfaty et al., 2012). TEDEval was developed to evaluate joint segmentation and parsing5 in Hebrew, which requires a greater variety of orthographic rewrites than those possible in Arabic. Its edit distance-based scoring algorithm is robust enough to handle the rewrites produced by both MADA and our segmenter. We measure the statistical significance of differences in these metrics with an approximate randomization test (Yeh, 2000; Padó, 2006), with R = 10,000 samples. 3.2 Results Table 1 contains results on the development set for the model of Green and DeNero and our improvements. Using domain adaptation alone helps performance on two of the three datasets (with a statistically insignificant decrease on broadcast news), and that our additional features further improve 5In order to evaluate segmentation in isolation, we convert each segmented sentence from both the model output and the gold standard to a flat tree with all segments descending directly from the root. 208 F1 (%) TEDEval (%) ATB BN ARZ ATB BN ARZ MADA 97</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>