<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004403">
<title confidence="0.98466">
The Rhythm of Lexical Stress in Prose
</title>
<author confidence="0.997368">
Doug Beeferman
</author>
<affiliation confidence="0.895287">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.932814">
dougb+Ocs.cmu.edu
</email>
<sectionHeader confidence="0.992131" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99978225">
&amp;quot;Prose rhythm&amp;quot; is a widely observed but
scarcely quantified phenomenon. We de-
scribe an information-theoretic model for
measuring the regularity of lexical stress in
English texts, and use it in combination
with trigram language models to demon-
strate a relationship between the probabil-
ity of word sequences in English and the
amount of rhythm present in them. We
find that the stream of lexical stress in text
from the Wall Street Journal has an en-
tropy rate of less than 0.75 bits per sylla-
ble for common sentences. We observe that
the average number of syllables per word
is greater for rarer word sequences, and to
normalize for this effect we run control ex-
periments to show that the choice of word
order contributes significantly to stress reg-
ularity, and increasingly with lexical prob-
ability.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999596909090909">
Rhythm inheres in creative output, asserting itself as
the meter in music, the iambs and trochees of poetry,
and the uniformity in distances between objects in
art and architecture. More subtly there is widely be-
lieved to be rhythm in English prose, reflecting the
arrangement of words, whether deliberate or sub-
conscious, to enhance the perceived acoustic signal
or reduce the burden of remembrance for the reader
or author.
In this paper we describe an information-theoretic
model based on lexical stress that substantiates this
common perception and relates stress regularity in
written speech (which we shall equate with the in-
tuitive notion of &amp;quot;rhythm&amp;quot;) to the probability of the
text itself. By computing the stress entropy rate for
both a set of Wall Street Journal sentences and a ver-
sion of the corpus with randomized intra-sentential
word order, we also find that word order contributes
significantly to rhythm, particularly within highly
probable sentences. We regard this as a first step in
quantifying the extent to which metrical properties
influence syntactic choice in writing.
</bodyText>
<subsectionHeader confidence="0.954247">
1.1 Basics
</subsectionHeader>
<bodyText confidence="0.99967795">
In speech production, syllables are emitted as pulses
of sound synchronized with movements of the mus-
culature in the rib cage. Degrees of stress arise from
variations in the amount of energy expended by the
speaker to contract these muscles, and from other
factors such as intonation. Perceptually stress is
more abstractly defined, and it is often associated
with &amp;quot;peaks of prominence&amp;quot; in some representation
of the acoustic input signal (Ochsner, 1989).
Stress as a lexical property, the primary concern
of this paper, is a function that maps a word to a
sequence of discrete levels of physical stress, approx-
imating the relative emphasis given each syllable
when the word is pronounced. Phonologists distin-
guish between three levels of lexical stress in English:
primary, secondary, and what we shall call weak
for lack of a better substitute for unstressed. For
the purposes of this paper we shall regard stresses
as symbols fused serially in time by the writer or
speaker, with words acting as building blocks of pre-
defined stress sequences that may be arranged arbi-
trarily but never broken apart.
The culminative property of stress states that ev-
ery content word has exactly one primary-stressed
syllable, and that whatever syllables remain are sub-
ordinate to it. Monosyllabic function words such as
the and of usually receive weak stress, while content
words get one strong stress and possibly many sec-
ondary and weak stresses.
It has been widely observed that strong and weak
tend to alternate at &amp;quot;rhythmically ideal disyllabic
distances&amp;quot; (Kager, 1989a). &amp;quot;Ideal&amp;quot; here is a complex
function involving production, perception, and many
unknowns. Our concern is not to pinpoint this ideal,
nor to answer precisely why it is sought by speakers
and writers, but to gauge to what extent it is sought.
We seek to investigate, for example, whether the
avoidance of primary stress clash, the placement of
two or more strongly stressed syllables in succession,
influences syntactic choice. In the Wall Street Jour-
</bodyText>
<page confidence="0.99637">
302
</page>
<bodyText confidence="0.999631347826087">
nal corpus we find such sentences as &amp;quot;The fol-low-
ing is-sues re-cent-ly were filed with the Se-cur-
i-ties and Ex-change Com-mis-sion&amp;quot;. The phrase
&amp;quot;recently were filed&amp;quot; can be syntactically permuted
as &amp;quot;were filed recently&amp;quot;, but this clashes filed with
the first syllable of recently. The chosen sentence
avoids consecutive primary stresses. Kager postu-
lates with a decidedly information theoretic under-
tone that the resulting binary alternation is &amp;quot;simply
the maximal degree of rhythmic organization com-
patible with the requirement that adjacent stresses
are to be avoided.&amp;quot; (Kager, 1989a)
Certainly we are not proposing that a hard deci-
sion based only on metrical properties of the output
is made to resolve syntactic choice ambiguity, in the
case above or in general. Clearly semantic empha-
sis has its say in the decision. But it is our belief
that rhythm makes a nontrivial contribution, and
that the tools of statistics and information theory
will help us to estimate it formally. Words are the
building blocks. How much do their selection (dic-
tion) and their arrangement (syntax) act to enhance
rhythm?
</bodyText>
<subsectionHeader confidence="0.994925">
1.2 Past models and quantifications
</subsectionHeader>
<bodyText confidence="0.995610637362637">
Lexical stress is a well-studied subject at the intra-
word level. Rules governing how to map a word&apos;s
orthographic or phonetic transcription to a sequence
of stress values have been searched for and studied
from rules-based, statistical, and connectionist per-
spectives.
Word-external stress regularity has been denied
this level of attention. Patterns in phrases and
compound words have been studied by Halle (Halle
and Vergnaud, 1987) and others, who observe and
reformulate such phenomena as the emphasis of
the penultimate constituent in a compound noun
(National Center for Supercomputing Applications,
for example.) Treatment of lexical stress across
word boundaries is scarce in the literature, however.
Though prose rhythm inquiry is more than a hun-
dred years old (Ochsner, 1989), it has largely been
dismissed by the linguistic community as irrelevant
to formal models, as a mere curiosity for literary
analysis. This is partly because formal methods of
inquiry have failed to present a compelling case for
the existence of regularity (Harding, 1976).
Past attempts to quantify prose rhythm may be
classified as perception-oriented or signal-oriented.
In both cases the studies have typically focussed on
regularities in the distance between peaks of promi-
nence, or interstress intervals, either perceived by
a human subject or measured in the signal. The
former class of experiments relies on the subjective
segmentation of utterances by a necessarily limited
number of participantsâ€”subjects tapping out the
rhythms they perceive in a waveform on a recording
device, for example (Kager, 1989b). To say nothing
of the psychoacoustic biases this methodology intro-
duces, it relies on too little data for anything but a
sterile set of means and variances.
Signal analysis, too, has not yet been applied to
very large speech corpora for the purpose of inves-
tigating prose rhythm, though the technology now
exists to lend efficiency to such studies. The ex-
periments have been of smaller scope and geared
toward detecting isochrony, regularity in absolute
time. Jassem et a/.(Jassem, Hill, and Witten, 1984)
use statistical techniques such as regression to ana-
lyze the duration of what they term rhythm units.
Jassem postulates that speech is composed of extra-
syllable narrow rhythm units with roughly fixed du-
ration independent of the number of syllable con-
stituents, surrounded by variable-length anacruses.
Abercrombie (Abercrombie, 1967) views speech as
composed of metrical feet of variable length that be-
gin with and are conceptually highlighted by a single
stressed syllable.
Many experiments lead to the common conclu-
sion that English is stress-timed, that there is some
regularity in the absolute duration between strong
stress events. In contrast to postulated syllable-
timed languages like French in which we find exactly
the inverse effect, speakers of English tend to expand
and to contract syllable streams so that the dura-
tion between bounding primary stresses matches the
other intervals in the utterance. It is unpleasant
for production and perception alike, however, when
too many weak-stressed syllables are forced into
such an interval, or when this amount of &amp;quot;padding&amp;quot;
varies wildly from one interval to the next. Prose
rhythm analysts so far have not considered the syl-
lable stream independent from syllabic, phonemic,
or interstress duration. In particular they haven&apos;t
measured the regularity of the purely lexical stream.
They have instead continually re-answered questions
concerning isochrony.
Given that speech can be divided into interstress
units of roughly equal duration, we believe the more
interesting question is whether a speaker or writer
modifies his diction and syntax to fit a regular num-
ber of syllables into each unit. This question can
only be answered by a lexical approach, an approach
that pleasingly lends itself to efficient experimenta-
tion with very large amounts of data.
2 Stress entropy rate
We regard every syllable as having either strong or
weak stress, and we employ a purely lexical, con-
text independent mapping, a pronunciation dictio-
nary 1, to tell us which syllables in a word receive
which level of stress. We base our experiments on
a binary-valued symbol set Ei = {W, S} and on a
ternary-valued symbol set E2 = S, Ph where
&apos;W&apos; indicates weak stress, &apos;S&apos; indicates strong stress,
We use the 116,000-entry CMU Pronouncing Dictio-
nary version 0.4 for all experiments in this paper.
</bodyText>
<page confidence="0.998881">
303
</page>
<figureCaption confidence="0.9735035">
Figure 2: A 5-gram model viewed as a first-order
Markov chain
</figureCaption>
<bodyText confidence="0.9994895">
and &apos;P&apos; indicates a pause. Abstractly the dictionary
maps words to sequences of symbols from {primary,
secondary, unstressed}, which we interpret by down-
sampling to our binary systemâ€”primary stress is
strong, non-stress is weak, and secondary stress (&apos;2&apos;)
we allow to be either weak or strong depending on
the experiment we are conducting.
We represent a sentence as the concatenation of
the stress sequences of its constituent words, with
-&apos;P&apos; symbols (for the E2 experiments) breaking the
stream where natural pauses occur.
Traditional approaches to lexical language mod-
eling provide insight on our analogous problem, in
which the input is a stream of syllables rather than
words and the values are drawn from a vocabu-
lary E of stress levels. We wish to create a model
that yields approximate values for probabilities of
the form p(sktso, sl, â€¢ â€¢ â€¢ , sk-i), where si E E is the
stress symbol at syllable i in the text. A model with
separate parameters for each history is prohibitively
large, as the number of possible histories grows ex-
ponentially with the length of the input; and for
the same reason it is impossible to train on limited
data. Consequently we partition the history space
into equivalence classes, and the stochastic n-gram
approach that has served lexical language modeling
so well treats two histories as equivalent if they end
in the same n â€” 1 symbols.
As Figure 2 demonstrates, an n-gram model is
simply a stationary Markov chain of order k = 71 -
1, or equivalently a first-order Markov chain whose
states are labeled with tuples from Ek
To gauge the regularity and compressibility of the
training data we can calculate the entropy rate of the
stochastic process as approximated by our model, an
upper bound on the expected number of bits needed
to encode each symbol in the best possible encod-
ing. Techniques for computing the entropy rate of
a stationary Markov chain are well known in infor-
mation theory (Cover and Thomas, 1991). If {Xi}
is a Markov chain with stationary distribution p
and transition matrix F, then its entropy rate is
</bodyText>
<equation confidence="0.59135">
H (X) = â€”Ei pipii log pii .
</equation>
<bodyText confidence="0.999377736842105">
The probabilities in P can be trained by ac-
cumulating, for each (si , s2, â€¢ . , sk) E E&apos;, the
k-gram count in C(si, s2, , sk) in the training
data, and normalizing by the (k â€” 1)-gram count
C(si,s2, â€¢ â€¢ â€¢
The stationary distribution p satisfies pP = p,
or equivalently pk Ei PjPj,k (Parzen, 1962). In
general finding p for a large state space requires an
eigenvector computation, but in the special case of
an n-gram model it can be shown that the value in p
corresponding to the state (si, s2, ,s,) is simply
the k-gram frequency C(si, S2,. sk)/N, where N
is the number of symbols in the data.2 We therefore
can compute the entropy rate of a stress sequence
in time linear in both the amount of data and the
size of the state space. This efficiency will enable us
to experiment with values of n as large as seven; for
larger values the amount of training data, not time,
is the limiting factor.
</bodyText>
<sectionHeader confidence="0.998013" genericHeader="introduction">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999938428571429">
The training procedure entails simply counting the
number of occurrences of each n-gram for the train-
ing data and computing the stress entropy rate by
the method described. As we treat each sentence as
an independent event, no cross-sentence n-grams are
kept: only those that fit between sentence bound-
aries are counted.
</bodyText>
<subsectionHeader confidence="0.999164">
3.1 The meaning of stress entropy rate
</subsectionHeader>
<bodyText confidence="0.999985772727273">
We regard these experiments as computing the en-
tropy rate of a Markov chain, estimated from train-
ing data, that approximately models the emission of
symbols from a random source. The entropy rate
bounds how compressible the training sequence is,
and not precisely how predictable unseen sequences
from the same source would be. To measure the effi-
cacy of these models in prediction it would be neces-
sary to divide the corpus, train a model on one sub-
set, and measure the entropy rate of the other with
respect to the trained model. Compression can take
place off-line, after the entire training set is read,
while prediction cannot &amp;quot;cheat&amp;quot; in this manner.
But we claim that our results predict how effective
prediction would be, for the small state space in our
Markov model and the huge amount of training data
translate to very good state coverage. In language
modeling, unseen words and unseen n-grams are a
serious problem, and are typically combatted with
smoothing techniques such as the backoff model and
the discounting formula offered by Good and Tur-
ing. In our case, unseen &amp;quot;words&amp;quot; never occur, for
</bodyText>
<footnote confidence="0.667801666666667">
2This ignores edge effects, for Es c(si,32,... ,sh) =
N â€” k +1, but this discrepancy is negligible when N is
very large.
</footnote>
<page confidence="0.994848">
304
</page>
<figure confidence="0.96139075">
Lis ten to me close ly en deav or to ex plain
S W S W S W S
what sep ar ates a char la tan from a Char le magne
S W 2 W S W W 2
</figure>
<figureCaption confidence="0.8425915">
Figure 1: A song lyric exemplifies a highly regular stress stream (from the musical Pippin by Stephen
Schwartz.)
</figureCaption>
<bodyText confidence="0.999421857142857">
the tiniest of realistic training sets will cover the bi-
nary or ternary vocabulary. Coverage of the n-gram
set is complete for our prose training texts for n as
high as eight; nor do singleton states (counts that
occur only once), which are the bases of Turing&apos;s es-
timate of the frequency of untrained states in new
data, occur until n = 7.
</bodyText>
<subsectionHeader confidence="0.999362">
3.2 Lexicalizing stress
</subsectionHeader>
<bodyText confidence="0.999995454545454">
Lexical stress is the &amp;quot;backbone of speech rhythm&amp;quot;
and the primary tool for its analysis. (Baum, 1952)
While the precise acoustical prominences of sylla-
bles within an utterance are subject to certain word-
external hierarchical constraints observed by Halle
(Halle and Vergnaud, 1987) and others, lexical stress
is a local property. The stress patterns of individ-
ual words within a phrase or sentence are generally
context independent.
One source of error in our method is the ambiguity
for words with multiple phonetic transcriptions that
differ in stress assignment. Highly accurate tech-
niques for part-of-speech labeling could be used for
stress pattern disambiguation when the ambiguity
is purely lexical, but often the choice, in both pro-
duction and perception, is dialectal. It would be
straightforward to divide among all alternatives the
count for each n-gram that includes a word with
multiple stress patterns, but in the absence of reli-
able frequency information to weight each pattern
we chose simply to use the pronunciation listed first
in the dictionary, which is judged by the lexicogra-
pher to be the most popular. Very little accuracy
is lost in making this assumption. Of the 115,966
words in the dictionary, 4635 have more than one
pronunciation; of these, 1269 have more than one
distinct stress pattern; of these, 525 have different
primary stress placements. This smallest class has a
few common words (such as &amp;quot;refuse&amp;quot; used as a noun
and as a verb), but most either occur infrequently in
text (obscure proper nouns, for example), or have a
primary pronunciation that is overwhelmingly more
common than the rest.
</bodyText>
<sectionHeader confidence="0.998248" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999972132075472">
The efficiency of the n-gram training procedure al-
lowed us to exploit a wealth of dataâ€”over 60 mil-
lion syllablesâ€”from 38 million words of Wall Street
Journal text. We discarded sentences not completely
covered by the pronunciation dictionary, leaving 36.1
million words and 60.7 million syllables for experi-
mentation.
Our first experiments used the binary E1 alpha-
bet. The maximum entropy rate possible for this
process is one bit per syllable, and given the unigram
distribution of stress values in the data (55.2% are
primary), an upper bound of slightly over 0.99 bits
can be computed. Examining the 4-gram frequencies
for the entire corpus (Figure 3a) sharpens this sub-
stantially, yielding an entropy rate estimate of 0.846
bits per syllable. Most frequent among the 4-grams
are the patterns WSWS and SWSW, consistent with
the principle of binary alternation mentioned in sec-
tion 1.
The 4-gram estimate matches quite closely with
the estimate of 0.852 bits that can be derived from
the distribution of word stress patterns excerpted
in Figure 3b. But both measures overestimate the
entropy rate by ignoring longer-range dependencies
that become evident when we use larger values of n.
For n = 6 we obtain a rate of 0.795 bits per syllable
over the entire corpus.
Since we had several thousand times more data
than is needed to make reliable estimates of stress
entropy rate for values of n less than 7, it was prac-
tical to subdivide the corpus according to some cri-
terion, and calculate the stress entropy rate for each
subset as well as for the whole. We chose to divide at
the sentence level and to partition the 1.59 million
sentences in the data based on a likelihood measure
suitable for testing the hypothesis from section 1.
A lexical trigram backoff-smoothed language
model was trained on separate data to estimate the
language perplexity of each sentence in the corpus.
Sentence perplexity PP(S) is the inverse of sentence
probability normalized for length, 1/P(S)11-1 , where
P(S) is the probability of the sentence according to
the language model and ISI is its word count. This
measure gauges the average &amp;quot;surprise&amp;quot; after reveal-
ing each word in the sentence as judged by the tri-
gram model. The question of whether more probable
word sequences are also more rhythmic can be ap-
proximated by asking whether sentences with lower
perplexity have lower stress entropy rate.
Each sentence in the corpus was assigned to one
of one hundred bins according to its perplexityâ€”
sentences with perplexity between 0 and 10 were as-
signed to the first bin; between 10 and 20, the sec-
</bodyText>
<page confidence="0.997756">
305
</page>
<table confidence="0.7805364">
WWWW: 0.78% WSWW: 6.91% SWWW: 2.96% SSWW: 3.94%
WWWS: 2.94% WSWS: 11.00% SWWS: 7.807. SSWS: 8.59%
WWSW: 6.977. WSSW: 6.16% SWSW: 11.21% SSSW: 6.25%
WWSS: 3.71% WSSS: 6.067, SWSS: 8.48% SSSS: 6.27%
S 45.87%
SW 18.94%
W 9.54%
SW 5.74%
WS 5.14%
WSW 4.54%
</table>
<figure confidence="0.9994335">
(a)
(b)
</figure>
<figureCaption confidence="0.9984715">
Figure 3: (a) The corpus frequencies of all binary stress 4-grams (based on 60.7 million syllables), with
secondary stress mapped to &amp;quot;weak&amp;quot; (W). (b) The corpus frequencies of the top six lexical stress patterns.
</figureCaption>
<figure confidence="0.990778259259259">
3e406 Wall Street Joumal baiting symbds (sylables), by petplesity bin
Wall Shad JOL6616 sentences -4--
100 200 300 400 500 603 700 800 900 1000
1-999m991m0a4
200
300
100
1000
800
900
700
Wall Street Journal syllables per nod, by perplexity bin
Wall Street Journal sentences -e-
400 500 800
1-999ualle P96319*
1.66-
1.64
1.62
1.6
tuf
tuo
1.78
1.76
1.74
1.72
1.7
k1.
</figure>
<figureCaption confidence="0.8506534">
Figure 4: The amount of training data, in syllables,
in each perplexity bin. The bin at perplexity level pp
contains all sentences in the corpus with perplexity
no less than pp and no greater than pp Â± 10. The
smallest count (at bin 990) is 50662.
</figureCaption>
<bodyText confidence="0.823085045454545">
ond; and so on. Sentences with perplexity greater
than 1000, which numbered roughly 106 thousand
out of 1.59 million, were discarded from all exper-
iments, as 10-unit bins at that level captured too
little data for statistical significance. A histogram
showing the amount of training data (in syllables)
per perplexity bin is given in Figure 4.
It is crucial to detect and understand potential
sources of bias in the methodology so far. It is clear
that the perplexity bins are well trained, but not yet
that they are comparable with each other. Figure 5
shows the average number of syllables per word in
sentences that appear in each bin. That this func-
tion is roughly increasing agrees with our intuition
that sequences with longer words are rarer. But it
biases our perplexity bins at the extremes. Early
bins, with sequences that have a small syllable rate
per word (1.57 in the 0 bin, for example), are pre-
disposed to a lower stress entropy rate since primary
stresses, which occur roughly once per word, are
more frequent. Later bins are also likely to be prej-
udiced in that direction, for the inverse reason: The
</bodyText>
<figureCaption confidence="0.968613">
Figure 5: The average number of syllables per word
for each perplexity bin.
</figureCaption>
<bodyText confidence="0.991889423076923">
increasing frequency of multisyllabic words makes
it more and more fashionable to transit to a weak-
stressed syllable following a primary stress, sharpen-
ing the probability distribution and decreasing en-
tropy.
This is verified when we run the stress entropy
rate computation for each bin. The results for ii-
gram models of orders 3 through 7, for the case
in which secondary lexical stress is mapped to the
&amp;quot;weak&amp;quot; level, are shown in Figure 6.
All of the rates calculated are substantially less
than a bit, but this only reflects the stress regu-
larity inherent in the vocabulary and in word se-
lection, and says nothing about word arrangement.
The atomic elements in the text stream, the words,
contribute regularity independently. To determine
how much is contributed by the way they are glued
together, we need to remove the bias of word choice.
For this reason we settled on a model size, n = 6,
and performed a variety of experiments with both
the original corpus and with a control set that con-
tained exactly the same bins with exactly the same
sentences, but mixed up. Each sentence in the
control set was permuted with a pseudorandom se-
quence of swaps based on an insensitive function of
the original; that is to say, identical sentences in the
</bodyText>
<page confidence="0.995889">
306
</page>
<note confidence="0.923462">
Well Street Journal BINARY stress entropy rates, by perplexity On
</note>
<footnote confidence="0.9630088">
3-gram model es-
4-gram model
5-gram model 43.-
6-gram model -4-
7-gram model -4--
</footnote>
<figure confidence="0.988145">
0.85
0.8 irc.Â§Etraf.ttosnilltigizefom
0.75
</figure>
<figureCaption confidence="0.982368">
Figure 6: n-gram stress entropy rates for E1, weak
secondary stress
</figureCaption>
<bodyText confidence="0.999584052631579">
corpus were shuffled the same way and sentences
differing by only one word were shuffled similarly.
This allowed us to keep steady the effects of mul-
tiple copies of the same sentence in the same per-
plexity bin. More importantly, these tests hold ev-
erything constantâ€”diction, syllable count, syllable
rate per wordâ€”except for syntax, the arrangement
of the chosen words within the sentence. Compar-
ing the unrandomized results with this control ex-
periment allows us, therefore, to factor out every-
thing but word order. In particular, subtracting the
stress entropy rates of the original sentences from
the rates of the randomized sentences gives us a fig-
ure, relative entropy, that estimates how many bits
we save by knowing the proper word order given the
word choice. The results for these tests for weak
and strong secondary stress are shown in Figures 7
and 8, including the difference curves between the
randomized-word and original entropy rates.
The consistently positive difference function
demonstrates that there is some extra stress regu-
larity to be had with proper word order, about a
hundredth of a bit on average. The difference is
small indeed, but its consistency over hundreds of
well-trained data points puts the observation on sta-
tistically solid ground.
The negative slopes of the difference curves sug-
gests a more interesting conclusion: As sentence per-
plexity increases, the gap in stress entropy rate be-
tween syntactic sentences and randomly permuted
sentences narrows. Restated inversely, using entropy
rates for randomly permuted sentences as a baseline,
sentences with higher sequence probability are rela-
tively more rhythmical in the sense of our definition
from section 1.
To supplement the E1 binary vocabulary tests we
ran the same experiments with E2 = {0, 1, P}, in-
troducing a pause symbol to examine how stress be-
haves near phrase boundaries. Commas, dashes,
semicolons, colons, ellipses, and all sentence-
terminating punctuation in the text, which were re-
moved in the E1 tests, were mapped to a single pause
symbol for E2. Pauses in the text arise not only
from semantic constraints but also from physiologi-
cal limitations. These include the &amp;quot;breath groups&amp;quot;
of syllables that influence both vocalized and writ-
ten production. (Ochsner, 1989). The results for
these experiments are shown in Figures 9 and 10.
Expectedly, adding the symbol increases the confu-
sion and hence the entropy, but the rates remain less
than a bit. The maximum possible rate for a ternary
sequence is log2 3 1.58.
The experiments in this section were repeated
with a larger perplexity interval that partitioned
the corpus into 20 bins, each covering 50 units of
perplexity. The resulting curves mirrored the finer-
grain curves presented here.
</bodyText>
<sectionHeader confidence="0.932803" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999964413793104">
We have quantified lexical stress regularity, mea-
sured it in a large sample of written English prose,
and shown there to be a significant contribution from
word order that increases with lexical perplexity.
This contribution was measured by comparing the
entropy rate of lexical stress in natural sentences
with randomly permuted versions of the same. Ran-
domizing the word order in this way yields a fairly
crude baseline, as it produces asyntactic sequences
in which, for example, single-syllable function words
can unnaturally clash. To correct for this we modi-
fied the randomization algorithm to permute only
open-class words and to fix in place determiners,
particles, pronouns, and other closed-class words.
We found the entropy rates to be consistently mid-
way between the fully randomized and unrandom-
ized values. But even this constrained randomiza-
tion is weaker than what we&apos;d like. Ideally we should
factor out semantics as well as word choice, compar-
ing each sentence in the corpus with its grammatical
variations. While this is a difficult experiment to do
automatically, we&apos;re hoping to approximate it using
a natural language generation system based on link
grammar under development by the author.
Also, we&apos;re currently testing other data sources
such as the Switchboard corpus of telephone speech
(Godfrey, Holliman, and McDaniel, 1992) to mea-
sure the effects of rhythm in more spontaneous and
grammatically relaxed texts.
</bodyText>
<sectionHeader confidence="0.995871" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.996536">
Comments from John Lafferty, Georg Niklfeld, and
Frank Dellaert contributed greatly to this paper.
The work was supported in part by an ARPA
AASERT award, number DAAH04-95-1-0475.
</bodyText>
<figure confidence="0.99480775">
0.95
0.9
100 200 300 400 500 600 700 800 900 1000
Language perplexity
</figure>
<page confidence="0.688195">
307
</page>
<figureCaption confidence="0.998654">
Figure 7: 6-gram stress entropy rates and difference curve for E1, weak secondary stress
</figureCaption>
<figure confidence="0.9995534">
Wall Street Journal BINARY stress entropy rates, by perplexity tin; secondary stress mapped to VVEAK Wal Street Journal BINARY stress entropy rate differences, by perplexity bin; secondary stress mapped to WEAK
0.81 0.025
0.8
0.02
0.79
100 200 300 400 500 600 700 800 900 1000
Language Make*
100 200 300 403 500 600 700 800 900
Language Perla eaVY
Wag Street Journal sentences -5â€”
Randomized Wall Street Journal sentences -4â€”
1000
0.015
0.01
8
0.005
WSJ randomized minus nonrandomized
0.78
0.77
0.78
0.75
0.74
Stress entropy rate
Well Street Journal BINARY stress entropy rates, by perpleidty bin; secondary stress mapped to STRONG
0.79
Wall Street Journal sentences
Randomized Wall Street Journal sentences
0,78 0554050
...7k *
100 203 300 400 500 600 700 800 900
Language perplexity
Wall Street Journal BINARY VMS entropy rate differences, by perplcorty bin; secondary stress mapped to STRONG
0.024
0.022
0.02
0.018
0.016
a, 0.014
0.012
0.01
0.008 -
0.006
0.004
0
A
0,77 -7
6
0.75
0.74
0.73
0.72 f
071 I
0
1000
WSJ randomized minus nonrandomized
1000
800 900
I 1
100 209 300 400 500 600 700
Language perplexity
</figure>
<figureCaption confidence="0.999888">
Figure 8: 6-gram entropy rates and difference curve for El, strong secondary stress
</figureCaption>
<page confidence="0.985272">
308
</page>
<figureCaption confidence="0.99991">
Figure 9: 6-gram entropy rates and difference curve for E2, weak secondary stress
Figure 10: 6-gram entropy rates and difference curve for E2, strong secondary stress
</figureCaption>
<figure confidence="0.999376070175439">
700
300 400 500 600
Language perplexity
100
300
200
400 500 600 700 800 900 1000
Language perplexity
Wall Street Journal TERNARY stress entropy rates, by perplexity bin; secondary stress mapped to STRONG Well Street Journal TERNARY stress entropy rate differences, by perplexity bin; secondary stress mapped o WEAK
0.97 0.05
Wall Street Journal sentences -.â€”
Randomized Wall Street Journal sentences
0.045
ass
Stress entropy rate
0.94
0.93
0.92
0.04
0.035
St
0.03
0.025
0.02
Off
0.015
0.01
200
800 900
1000
0.9
0
0.96
0.91
0.005
WSJ randomized minus nonrandomized -a--
0 87
0
800
700
1000
800 900
200 300
900 1000
400 500 6013
Language perplexity
100 200 300 400 500 603 700
baNuege PelPlaKilY
Wall Street Journal TERNARY stress entropy rates, by pepledy bin; secondary stress mapped to STRONG
0.94 irs ,
sie**
4&apos;s
Wan Street Journal TERNARY stress entropy rate differences, by perplexity bin; secondary stress mapped to STRONG
0.05
WSJ mndornized minus nonrandomized -4-
Well Street Journal sentences -*â€”
Randomized Wall Street Journal sentences
</figure>
<sectionHeader confidence="0.916901" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997857931034483">
Abercrombie, D. 1967. Elements of general phonet-
ics. Edinburgh University Press.
Baum, P. F. 1952. The Other Harmony of Prose.
Duke University Press. â€¢
Cover, T. M. and J. A. Thomas. 1991. Elements of
information theory. John Wiley &amp; Sons, Inc.
Godfrey, J., E. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for re-
search development. In Proc. ICASSP-92, pages
1-517-520.
Halle, M. and J. Vergnaud. 1987. An essay on
stress. The MIT Press.
Harding, D. W. 1976. Words into rhythm: English
speech rhythm in verse and prose. Cambridge Uni-
versity Press.
Jassem, W., D. R. Hill, and I. H. Witten. 1984.
Isochrony in English speech: its statistical valid-
ity and linguistic relevance. In D. Gibbon and
H. Richter, editors, Intonation, rhythm, and ac-
cent: Studies in Discourse Phonology. Walter de
Gruyter, pages 203-225.
Kager, R. 1989a. A metrical theory of stress and
destressing in English and Dutch. Foris Publica-
tions.
Kager, R. 1989b. The rhythm of English prose.
Foris Publications.
Ochsner, R. S. 1989. Rhythm and writing. The
Whitson Publishing Company.
Parzen, E. 1962. Stochastic processes. Holden-Day.
</reference>
<page confidence="0.999273">
309
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.821880">
<title confidence="0.999467">The Rhythm of Lexical Stress in Prose</title>
<author confidence="0.999927">Doug Beeferman</author>
<affiliation confidence="0.9997985">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.998961">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999853">dougb+Ocs.cmu.edu</email>
<abstract confidence="0.991172476190476">Prose rhythm&amp;quot; is a widely observed but scarcely quantified phenomenon. We describe an information-theoretic model for measuring the regularity of lexical stress in English texts, and use it in combination with trigram language models to demonstrate a relationship between the probability of word sequences in English and the amount of rhythm present in them. We find that the stream of lexical stress in text from the Wall Street Journal has an entropy rate of less than 0.75 bits per syllable for common sentences. We observe that the average number of syllables per word is greater for rarer word sequences, and to normalize for this effect we run control experiments to show that the choice of word order contributes significantly to stress regularity, and increasingly with lexical probability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Abercrombie</author>
</authors>
<title>Elements of general phonetics.</title>
<date>1967</date>
<publisher>Edinburgh University Press.</publisher>
<contexts>
<context position="7679" citStr="Abercrombie, 1967" startWordPosition="1216" endWordPosition="1217">ech corpora for the purpose of investigating prose rhythm, though the technology now exists to lend efficiency to such studies. The experiments have been of smaller scope and geared toward detecting isochrony, regularity in absolute time. Jassem et a/.(Jassem, Hill, and Witten, 1984) use statistical techniques such as regression to analyze the duration of what they term rhythm units. Jassem postulates that speech is composed of extrasyllable narrow rhythm units with roughly fixed duration independent of the number of syllable constituents, surrounded by variable-length anacruses. Abercrombie (Abercrombie, 1967) views speech as composed of metrical feet of variable length that begin with and are conceptually highlighted by a single stressed syllable. Many experiments lead to the common conclusion that English is stress-timed, that there is some regularity in the absolute duration between strong stress events. In contrast to postulated syllabletimed languages like French in which we find exactly the inverse effect, speakers of English tend to expand and to contract syllable streams so that the duration between bounding primary stresses matches the other intervals in the utterance. It is unpleasant for</context>
</contexts>
<marker>Abercrombie, 1967</marker>
<rawString>Abercrombie, D. 1967. Elements of general phonetics. Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Baum</author>
</authors>
<title>The Other Harmony of Prose.</title>
<date>1952</date>
<publisher>Duke University Press. â€¢</publisher>
<contexts>
<context position="15049" citStr="Baum, 1952" startWordPosition="2497" endWordPosition="2498">a Char le magne S W 2 W S W W 2 Figure 1: A song lyric exemplifies a highly regular stress stream (from the musical Pippin by Stephen Schwartz.) the tiniest of realistic training sets will cover the binary or ternary vocabulary. Coverage of the n-gram set is complete for our prose training texts for n as high as eight; nor do singleton states (counts that occur only once), which are the bases of Turing&apos;s estimate of the frequency of untrained states in new data, occur until n = 7. 3.2 Lexicalizing stress Lexical stress is the &amp;quot;backbone of speech rhythm&amp;quot; and the primary tool for its analysis. (Baum, 1952) While the precise acoustical prominences of syllables within an utterance are subject to certain wordexternal hierarchical constraints observed by Halle (Halle and Vergnaud, 1987) and others, lexical stress is a local property. The stress patterns of individual words within a phrase or sentence are generally context independent. One source of error in our method is the ambiguity for words with multiple phonetic transcriptions that differ in stress assignment. Highly accurate techniques for part-of-speech labeling could be used for stress pattern disambiguation when the ambiguity is purely lex</context>
</contexts>
<marker>Baum, 1952</marker>
<rawString>Baum, P. F. 1952. The Other Harmony of Prose. Duke University Press. â€¢</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of information theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="11701" citStr="Cover and Thomas, 1991" startWordPosition="1882" endWordPosition="1885">ent if they end in the same n â€” 1 symbols. As Figure 2 demonstrates, an n-gram model is simply a stationary Markov chain of order k = 71 - 1, or equivalently a first-order Markov chain whose states are labeled with tuples from Ek To gauge the regularity and compressibility of the training data we can calculate the entropy rate of the stochastic process as approximated by our model, an upper bound on the expected number of bits needed to encode each symbol in the best possible encoding. Techniques for computing the entropy rate of a stationary Markov chain are well known in information theory (Cover and Thomas, 1991). If {Xi} is a Markov chain with stationary distribution p and transition matrix F, then its entropy rate is H (X) = â€”Ei pipii log pii . The probabilities in P can be trained by accumulating, for each (si , s2, â€¢ . , sk) E E&apos;, the k-gram count in C(si, s2, , sk) in the training data, and normalizing by the (k â€” 1)-gram count C(si,s2, â€¢ â€¢ â€¢ The stationary distribution p satisfies pP = p, or equivalently pk Ei PjPj,k (Parzen, 1962). In general finding p for a large state space requires an eigenvector computation, but in the special case of an n-gram model it can be shown that the value in p corr</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T. M. and J. A. Thomas. 1991. Elements of information theory. John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research development.</title>
<date>1992</date>
<booktitle>In Proc. ICASSP-92,</booktitle>
<pages>1--517</pages>
<contexts>
<context position="27094" citStr="Godfrey, Holliman, and McDaniel, 1992" startWordPosition="4490" endWordPosition="4494">We found the entropy rates to be consistently midway between the fully randomized and unrandomized values. But even this constrained randomization is weaker than what we&apos;d like. Ideally we should factor out semantics as well as word choice, comparing each sentence in the corpus with its grammatical variations. While this is a difficult experiment to do automatically, we&apos;re hoping to approximate it using a natural language generation system based on link grammar under development by the author. Also, we&apos;re currently testing other data sources such as the Switchboard corpus of telephone speech (Godfrey, Holliman, and McDaniel, 1992) to measure the effects of rhythm in more spontaneous and grammatically relaxed texts. 6 Acknowledgments Comments from John Lafferty, Georg Niklfeld, and Frank Dellaert contributed greatly to this paper. The work was supported in part by an ARPA AASERT award, number DAAH04-95-1-0475. 0.95 0.9 100 200 300 400 500 600 700 800 900 1000 Language perplexity 307 Figure 7: 6-gram stress entropy rates and difference curve for E1, weak secondary stress Wall Street Journal BINARY stress entropy rates, by perplexity tin; secondary stress mapped to VVEAK Wal Street Journal BINARY stress entropy rate diff</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>Godfrey, J., E. Holliman, and J. McDaniel. 1992. Switchboard: Telephone speech corpus for research development. In Proc. ICASSP-92, pages 1-517-520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Halle</author>
<author>J Vergnaud</author>
</authors>
<title>An essay on stress.</title>
<date>1987</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5671" citStr="Halle and Vergnaud, 1987" startWordPosition="905" endWordPosition="908">elp us to estimate it formally. Words are the building blocks. How much do their selection (diction) and their arrangement (syntax) act to enhance rhythm? 1.2 Past models and quantifications Lexical stress is a well-studied subject at the intraword level. Rules governing how to map a word&apos;s orthographic or phonetic transcription to a sequence of stress values have been searched for and studied from rules-based, statistical, and connectionist perspectives. Word-external stress regularity has been denied this level of attention. Patterns in phrases and compound words have been studied by Halle (Halle and Vergnaud, 1987) and others, who observe and reformulate such phenomena as the emphasis of the penultimate constituent in a compound noun (National Center for Supercomputing Applications, for example.) Treatment of lexical stress across word boundaries is scarce in the literature, however. Though prose rhythm inquiry is more than a hundred years old (Ochsner, 1989), it has largely been dismissed by the linguistic community as irrelevant to formal models, as a mere curiosity for literary analysis. This is partly because formal methods of inquiry have failed to present a compelling case for the existence of reg</context>
<context position="15229" citStr="Halle and Vergnaud, 1987" startWordPosition="2521" endWordPosition="2524">tic training sets will cover the binary or ternary vocabulary. Coverage of the n-gram set is complete for our prose training texts for n as high as eight; nor do singleton states (counts that occur only once), which are the bases of Turing&apos;s estimate of the frequency of untrained states in new data, occur until n = 7. 3.2 Lexicalizing stress Lexical stress is the &amp;quot;backbone of speech rhythm&amp;quot; and the primary tool for its analysis. (Baum, 1952) While the precise acoustical prominences of syllables within an utterance are subject to certain wordexternal hierarchical constraints observed by Halle (Halle and Vergnaud, 1987) and others, lexical stress is a local property. The stress patterns of individual words within a phrase or sentence are generally context independent. One source of error in our method is the ambiguity for words with multiple phonetic transcriptions that differ in stress assignment. Highly accurate techniques for part-of-speech labeling could be used for stress pattern disambiguation when the ambiguity is purely lexical, but often the choice, in both production and perception, is dialectal. It would be straightforward to divide among all alternatives the count for each n-gram that includes a </context>
</contexts>
<marker>Halle, Vergnaud, 1987</marker>
<rawString>Halle, M. and J. Vergnaud. 1987. An essay on stress. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Harding</author>
</authors>
<title>Words into rhythm: English speech rhythm in verse and prose.</title>
<date>1976</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6294" citStr="Harding, 1976" startWordPosition="1003" endWordPosition="1004">rs, who observe and reformulate such phenomena as the emphasis of the penultimate constituent in a compound noun (National Center for Supercomputing Applications, for example.) Treatment of lexical stress across word boundaries is scarce in the literature, however. Though prose rhythm inquiry is more than a hundred years old (Ochsner, 1989), it has largely been dismissed by the linguistic community as irrelevant to formal models, as a mere curiosity for literary analysis. This is partly because formal methods of inquiry have failed to present a compelling case for the existence of regularity (Harding, 1976). Past attempts to quantify prose rhythm may be classified as perception-oriented or signal-oriented. In both cases the studies have typically focussed on regularities in the distance between peaks of prominence, or interstress intervals, either perceived by a human subject or measured in the signal. The former class of experiments relies on the subjective segmentation of utterances by a necessarily limited number of participantsâ€”subjects tapping out the rhythms they perceive in a waveform on a recording device, for example (Kager, 1989b). To say nothing of the psychoacoustic biases this metho</context>
</contexts>
<marker>Harding, 1976</marker>
<rawString>Harding, D. W. 1976. Words into rhythm: English speech rhythm in verse and prose. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Jassem</author>
<author>D R Hill</author>
<author>I H Witten</author>
</authors>
<title>Isochrony in English speech: its statistical validity and linguistic relevance.</title>
<date>1984</date>
<booktitle>Intonation, rhythm, and accent: Studies in Discourse Phonology. Walter de Gruyter,</booktitle>
<pages>203--225</pages>
<editor>In D. Gibbon and H. Richter, editors,</editor>
<contexts>
<context position="7344" citStr="Jassem, Hill, and Witten, 1984" startWordPosition="1164" endWordPosition="1168"> participantsâ€”subjects tapping out the rhythms they perceive in a waveform on a recording device, for example (Kager, 1989b). To say nothing of the psychoacoustic biases this methodology introduces, it relies on too little data for anything but a sterile set of means and variances. Signal analysis, too, has not yet been applied to very large speech corpora for the purpose of investigating prose rhythm, though the technology now exists to lend efficiency to such studies. The experiments have been of smaller scope and geared toward detecting isochrony, regularity in absolute time. Jassem et a/.(Jassem, Hill, and Witten, 1984) use statistical techniques such as regression to analyze the duration of what they term rhythm units. Jassem postulates that speech is composed of extrasyllable narrow rhythm units with roughly fixed duration independent of the number of syllable constituents, surrounded by variable-length anacruses. Abercrombie (Abercrombie, 1967) views speech as composed of metrical feet of variable length that begin with and are conceptually highlighted by a single stressed syllable. Many experiments lead to the common conclusion that English is stress-timed, that there is some regularity in the absolute </context>
</contexts>
<marker>Jassem, Hill, Witten, 1984</marker>
<rawString>Jassem, W., D. R. Hill, and I. H. Witten. 1984. Isochrony in English speech: its statistical validity and linguistic relevance. In D. Gibbon and H. Richter, editors, Intonation, rhythm, and accent: Studies in Discourse Phonology. Walter de Gruyter, pages 203-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kager</author>
</authors>
<title>A metrical theory of stress and destressing in English and Dutch.</title>
<date>1989</date>
<publisher>Foris Publications.</publisher>
<contexts>
<context position="3639" citStr="Kager, 1989" startWordPosition="585" endWordPosition="586">ime by the writer or speaker, with words acting as building blocks of predefined stress sequences that may be arranged arbitrarily but never broken apart. The culminative property of stress states that every content word has exactly one primary-stressed syllable, and that whatever syllables remain are subordinate to it. Monosyllabic function words such as the and of usually receive weak stress, while content words get one strong stress and possibly many secondary and weak stresses. It has been widely observed that strong and weak tend to alternate at &amp;quot;rhythmically ideal disyllabic distances&amp;quot; (Kager, 1989a). &amp;quot;Ideal&amp;quot; here is a complex function involving production, perception, and many unknowns. Our concern is not to pinpoint this ideal, nor to answer precisely why it is sought by speakers and writers, but to gauge to what extent it is sought. We seek to investigate, for example, whether the avoidance of primary stress clash, the placement of two or more strongly stressed syllables in succession, influences syntactic choice. In the Wall Street Jour302 nal corpus we find such sentences as &amp;quot;The fol-lowing is-sues re-cent-ly were filed with the Se-curi-ties and Ex-change Com-mis-sion&amp;quot;. The phrase </context>
<context position="6836" citStr="Kager, 1989" startWordPosition="1084" endWordPosition="1085">ent a compelling case for the existence of regularity (Harding, 1976). Past attempts to quantify prose rhythm may be classified as perception-oriented or signal-oriented. In both cases the studies have typically focussed on regularities in the distance between peaks of prominence, or interstress intervals, either perceived by a human subject or measured in the signal. The former class of experiments relies on the subjective segmentation of utterances by a necessarily limited number of participantsâ€”subjects tapping out the rhythms they perceive in a waveform on a recording device, for example (Kager, 1989b). To say nothing of the psychoacoustic biases this methodology introduces, it relies on too little data for anything but a sterile set of means and variances. Signal analysis, too, has not yet been applied to very large speech corpora for the purpose of investigating prose rhythm, though the technology now exists to lend efficiency to such studies. The experiments have been of smaller scope and geared toward detecting isochrony, regularity in absolute time. Jassem et a/.(Jassem, Hill, and Witten, 1984) use statistical techniques such as regression to analyze the duration of what they term rh</context>
</contexts>
<marker>Kager, 1989</marker>
<rawString>Kager, R. 1989a. A metrical theory of stress and destressing in English and Dutch. Foris Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kager</author>
</authors>
<title>The rhythm of English prose.</title>
<date>1989</date>
<publisher>Foris Publications.</publisher>
<contexts>
<context position="3639" citStr="Kager, 1989" startWordPosition="585" endWordPosition="586">ime by the writer or speaker, with words acting as building blocks of predefined stress sequences that may be arranged arbitrarily but never broken apart. The culminative property of stress states that every content word has exactly one primary-stressed syllable, and that whatever syllables remain are subordinate to it. Monosyllabic function words such as the and of usually receive weak stress, while content words get one strong stress and possibly many secondary and weak stresses. It has been widely observed that strong and weak tend to alternate at &amp;quot;rhythmically ideal disyllabic distances&amp;quot; (Kager, 1989a). &amp;quot;Ideal&amp;quot; here is a complex function involving production, perception, and many unknowns. Our concern is not to pinpoint this ideal, nor to answer precisely why it is sought by speakers and writers, but to gauge to what extent it is sought. We seek to investigate, for example, whether the avoidance of primary stress clash, the placement of two or more strongly stressed syllables in succession, influences syntactic choice. In the Wall Street Jour302 nal corpus we find such sentences as &amp;quot;The fol-lowing is-sues re-cent-ly were filed with the Se-curi-ties and Ex-change Com-mis-sion&amp;quot;. The phrase </context>
<context position="6836" citStr="Kager, 1989" startWordPosition="1084" endWordPosition="1085">ent a compelling case for the existence of regularity (Harding, 1976). Past attempts to quantify prose rhythm may be classified as perception-oriented or signal-oriented. In both cases the studies have typically focussed on regularities in the distance between peaks of prominence, or interstress intervals, either perceived by a human subject or measured in the signal. The former class of experiments relies on the subjective segmentation of utterances by a necessarily limited number of participantsâ€”subjects tapping out the rhythms they perceive in a waveform on a recording device, for example (Kager, 1989b). To say nothing of the psychoacoustic biases this methodology introduces, it relies on too little data for anything but a sterile set of means and variances. Signal analysis, too, has not yet been applied to very large speech corpora for the purpose of investigating prose rhythm, though the technology now exists to lend efficiency to such studies. The experiments have been of smaller scope and geared toward detecting isochrony, regularity in absolute time. Jassem et a/.(Jassem, Hill, and Witten, 1984) use statistical techniques such as regression to analyze the duration of what they term rh</context>
</contexts>
<marker>Kager, 1989</marker>
<rawString>Kager, R. 1989b. The rhythm of English prose. Foris Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Ochsner</author>
</authors>
<title>Rhythm and writing.</title>
<date>1989</date>
<publisher>The Whitson Publishing Company.</publisher>
<contexts>
<context position="2534" citStr="Ochsner, 1989" startWordPosition="404" endWordPosition="405">hly probable sentences. We regard this as a first step in quantifying the extent to which metrical properties influence syntactic choice in writing. 1.1 Basics In speech production, syllables are emitted as pulses of sound synchronized with movements of the musculature in the rib cage. Degrees of stress arise from variations in the amount of energy expended by the speaker to contract these muscles, and from other factors such as intonation. Perceptually stress is more abstractly defined, and it is often associated with &amp;quot;peaks of prominence&amp;quot; in some representation of the acoustic input signal (Ochsner, 1989). Stress as a lexical property, the primary concern of this paper, is a function that maps a word to a sequence of discrete levels of physical stress, approximating the relative emphasis given each syllable when the word is pronounced. Phonologists distinguish between three levels of lexical stress in English: primary, secondary, and what we shall call weak for lack of a better substitute for unstressed. For the purposes of this paper we shall regard stresses as symbols fused serially in time by the writer or speaker, with words acting as building blocks of predefined stress sequences that may</context>
<context position="6022" citStr="Ochsner, 1989" startWordPosition="960" endWordPosition="961">have been searched for and studied from rules-based, statistical, and connectionist perspectives. Word-external stress regularity has been denied this level of attention. Patterns in phrases and compound words have been studied by Halle (Halle and Vergnaud, 1987) and others, who observe and reformulate such phenomena as the emphasis of the penultimate constituent in a compound noun (National Center for Supercomputing Applications, for example.) Treatment of lexical stress across word boundaries is scarce in the literature, however. Though prose rhythm inquiry is more than a hundred years old (Ochsner, 1989), it has largely been dismissed by the linguistic community as irrelevant to formal models, as a mere curiosity for literary analysis. This is partly because formal methods of inquiry have failed to present a compelling case for the existence of regularity (Harding, 1976). Past attempts to quantify prose rhythm may be classified as perception-oriented or signal-oriented. In both cases the studies have typically focussed on regularities in the distance between peaks of prominence, or interstress intervals, either perceived by a human subject or measured in the signal. The former class of experi</context>
<context position="25239" citStr="Ochsner, 1989" startWordPosition="4198" endWordPosition="4199">e sense of our definition from section 1. To supplement the E1 binary vocabulary tests we ran the same experiments with E2 = {0, 1, P}, introducing a pause symbol to examine how stress behaves near phrase boundaries. Commas, dashes, semicolons, colons, ellipses, and all sentenceterminating punctuation in the text, which were removed in the E1 tests, were mapped to a single pause symbol for E2. Pauses in the text arise not only from semantic constraints but also from physiological limitations. These include the &amp;quot;breath groups&amp;quot; of syllables that influence both vocalized and written production. (Ochsner, 1989). The results for these experiments are shown in Figures 9 and 10. Expectedly, adding the symbol increases the confusion and hence the entropy, but the rates remain less than a bit. The maximum possible rate for a ternary sequence is log2 3 1.58. The experiments in this section were repeated with a larger perplexity interval that partitioned the corpus into 20 bins, each covering 50 units of perplexity. The resulting curves mirrored the finergrain curves presented here. 5 Conclusions and future work We have quantified lexical stress regularity, measured it in a large sample of written English </context>
</contexts>
<marker>Ochsner, 1989</marker>
<rawString>Ochsner, R. S. 1989. Rhythm and writing. The Whitson Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Parzen</author>
</authors>
<title>Stochastic processes.</title>
<date>1962</date>
<publisher>Holden-Day.</publisher>
<contexts>
<context position="12134" citStr="Parzen, 1962" startWordPosition="1971" endWordPosition="1972">code each symbol in the best possible encoding. Techniques for computing the entropy rate of a stationary Markov chain are well known in information theory (Cover and Thomas, 1991). If {Xi} is a Markov chain with stationary distribution p and transition matrix F, then its entropy rate is H (X) = â€”Ei pipii log pii . The probabilities in P can be trained by accumulating, for each (si , s2, â€¢ . , sk) E E&apos;, the k-gram count in C(si, s2, , sk) in the training data, and normalizing by the (k â€” 1)-gram count C(si,s2, â€¢ â€¢ â€¢ The stationary distribution p satisfies pP = p, or equivalently pk Ei PjPj,k (Parzen, 1962). In general finding p for a large state space requires an eigenvector computation, but in the special case of an n-gram model it can be shown that the value in p corresponding to the state (si, s2, ,s,) is simply the k-gram frequency C(si, S2,. sk)/N, where N is the number of symbols in the data.2 We therefore can compute the entropy rate of a stress sequence in time linear in both the amount of data and the size of the state space. This efficiency will enable us to experiment with values of n as large as seven; for larger values the amount of training data, not time, is the limiting factor. </context>
</contexts>
<marker>Parzen, 1962</marker>
<rawString>Parzen, E. 1962. Stochastic processes. Holden-Day.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>