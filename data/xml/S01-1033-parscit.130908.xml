<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000670">
<title confidence="0.9420325">
Japanese word sense disambiguation using the simple Bayes and
support vector machine methods
</title>
<author confidence="0.956725">
Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto,
Qing Ma, and Hitoshi Isahara
</author>
<affiliation confidence="0.834526">
Communications Research Laboratory
</affiliation>
<address confidence="0.752593">
2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
</address>
<sectionHeader confidence="0.959465" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933076923077">
We submitted four systems to the Japanese
dictionary-based lexical-sample task of
SENSEVAL-2. They were i) the support
vector machine method ii) the simple Bayes
method, iii) a method combining the two, and
iv) a method combining two kinds of each. The
combined methods obtained the best precision
among the submitted systems. After the
contest, we tuned the parameter used in the
simple Bayes method, and it obtained higher
precision. An explanation of these systems
used in Japanese word sense disambiguation
was provided.
</bodyText>
<sectionHeader confidence="0.997909" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977125">
We participated in the Japanese dictionary-
based lexical-sample task of the SENSEVAL-2
contest. We used machine learning approaches
and submitted four systems. After the con-
test, we tuned the parameter used in the simple
Bayes method and carried out additional exper-
iments. In this paper, we explain the systems
and their experimental results.
</bodyText>
<sectionHeader confidence="0.994642" genericHeader="method">
2 Task Descriptions
</sectionHeader>
<bodyText confidence="0.9997939">
The test data included 10,000 instances for eval-
uation. The RWC corpus (Shirai et al., 2001)
was given as the training data. It was made
from 3000 articles published in the Mainichi
Newspaper. The nouns, verbs, and adjectives
(the total number of which was about 150,000)
were assigned sense tags defined on the basis
of the Iwanami dictionary. The purpose of this
task was to estimate the sense of a word by us-
ing its context.
</bodyText>
<sectionHeader confidence="0.997699" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999891545454545">
Because the word sense assigned to each word
is dependent on the word itself, estimations
were conducted using machine learning meth-
ods for each word. That is, we constructed as
many learning machines as there were individ-
ual words.
We used the simple Bayes and support vec-
tor machine methods as the machine learning
method.1 In this section, we explain each of the
machine learning methods and then explain the
method combining several of them.
</bodyText>
<subsectionHeader confidence="0.997601">
3.1 Simple Bayes Method
</subsectionHeader>
<bodyText confidence="0.999704714285714">
This method estimates probability based on the
Bayes theory. The category (i.e., the sense tag)
with the highest probability is judged to be the
desired one. This is a basic approach to the
disambiguation of word sense. The probability
of category a appearing in context b is defined
as:
</bodyText>
<equation confidence="0.9996872">
p(alb) = p(a)
gbla)
p(b)
P(a) 11/5Cfila),
p(b)
</equation>
<bodyText confidence="0.999946428571428">
where context b is a set of features f3(E F,1 &lt;
j &lt; k) that is defined in advance. p(b) is the
probability of context b, which is not calculated
because it is a constant and is not dependent
on category a. 23(a) and 23( fila) are the prob-
abilities estimated by using the training data
and indicate the probability of the occurrence
of category a in the examples of the training
data and the probability of feature L occur-
ring, given category a, respectively. When we
use the maximum likelihood estimation to cal-
culate /3( fi I a), which often has a value of 0 and
is therefore difficult to estimate the desired cat-
egory, smoothing process is used. We used this
</bodyText>
<footnote confidence="0.999412">
1We made preliminary experiments using various
methods: the simple Bayes, the decision list, the max-
imum entropy, and the support vector machine. The
results showed that the simple Bayes and support vector
machine methods were better than the other two (Mu-
rata et al., 2001). We used these two methods in the
contest.
</footnote>
<page confidence="0.997013">
135
</page>
<figureCaption confidence="0.9277835">
Figure 1: Maximizing the margin
equation for smoothing:
</figureCaption>
<equation confidence="0.970700333333333">
f req( f,, a) + * f req(a)
13(.ii la) = (3)
f req(a) c * f req(a)
</equation>
<bodyText confidence="0.999972">
where f req(fi, a) is the number of events that
have the feature fi and whose category is a and
f req(a) is the number of events whose category
is a. € is a constant set by experimentation. In
this study, we used 0.01 and 0.0001 as €.2
</bodyText>
<subsectionHeader confidence="0.999839">
3.2 Support Vector Machine Method
</subsectionHeader>
<bodyText confidence="0.999884638888889">
In this method, data consisting of two categories
is classified by using a hyperplane to divide a
space. When the two categories are, for exam-
ple, positive and negative, enlarging the margin
between the positive and negative examples in
the training data (see Figure 13) reduces the
possibility of incorrectly choosing categories in
test data. The hyperplane that maximizes the
margin is thus determined, and classification is
carried out using that hyperplane. Although
the basics of this method are the same as those
described above, in the extended versions of
the method, the region between the margins
through the training data can include a small
number of examples, and the linearity of the
hyperplane can be changed to a non-linearity
by using kernel functions. The classification in
the extended versions is equivalent to the classi-
fication using the following function (Equation
(4)), and the two categories can be classified on
the basis of whether the value output by the
function is positive or negative (Cristianini and
Shawe-Taylor, 2000; Kudoh, 2000):
21n the SENSEVAL-2 contest, we used 0.01 as e. After
the contest, we tested several values (0.1 to 0.00000001)
as c. We confirmed that e = 0.0001 produced the best
results using 10-fold cross validation in the training data.
31n the figure, the white and black circles indicate
positive and negative examples, respectively. The solid
line indicates the hyperplane that divides the space, and
the broken lines indicate the planes that mark the mar-
gins.
where x is the context (a set of features) of
an input example, xi indicates the context of
a training datum, y, (i = I, ..., /, y, E {1,-1})
indicates its category, and the function sgn is
</bodyText>
<equation confidence="0.9967195">
sgn(x) -= 1 (x &gt; 0), (5)
—1 (otherwise).
</equation>
<bodyText confidence="0.7256635">
Each a, (i = 1, 2...) is fixed as the value of a,
that maximizes the value of L(a) in Equation
(6) under the conditions set by Equations (7)
and (8).
</bodyText>
<equation confidence="0.999780666666667">
ozictiyiyiK(xi,xj) (6)
,j=1
0&lt;c&lt;C (i = 1, ..., /)
</equation>
<bodyText confidence="0.99187225">
Function K is called a kernel function and var-
ious functions are used as kernel functions. We
have used the following polynomial function ex-
clusively.
</bodyText>
<equation confidence="0.981118">
K(x, y) = (x • y + 1)d (9)
</equation>
<bodyText confidence="0.967101764705882">
C and d are constants set by experimentation.
For all of the experiments reported in this pa-
per, C was fixed as 1 and d was fixed as 2.
A set of xi that satisfies a, &gt; 0 is called a
support vector (SV44. The summation portion
of Equation (4) was calculated using only the
examples that were support vectors.
Support vector machine methods are capable
of handling data consisting of two categories. In
general, data consisting of more than two cate-
gories is handled by using the pair-wise method
(Kudoh and Matsumoto, 2000).
In this method, for data consisting of N cat-
egories, pairs of two different categories (N(N-
1)/2 pairs) are constructed. The better cate-
In 1, the circles in the broken lines indicate
support vectors.
</bodyText>
<figure confidence="0.8363611875">
Small Margin
•
0 t
0
`1(
0 e?,
Large Margin
*
=- sgn (1 cxi yiK(xi, x) + b (4)
= maxi,yi bi + mini bi
2
j=1
— —
2
L(a)
i=1
</figure>
<page confidence="0.984686">
136
</page>
<bodyText confidence="0.9998759">
gory is determined by using a 2-category clas-
sifier (in this paper, a support vector machine5
was used as the 2-category classifier), and the
correct category is finally determined by &amp;quot;yd.-
ing&amp;quot; on the N(N-1)/2 pairs that result from
analysis using the 2-category classifier.
The support vector machine method is, in
fact, performed by combining the support vec-
tor machine and pair-wise methods described
above.
</bodyText>
<subsectionHeader confidence="0.983683">
3.3 Combined Method
</subsectionHeader>
<bodyText confidence="0.98837625">
Our combined method changed the used
machine-learning method for each word. The
used method for each word was the best one
for the word in the 10-fold cross validation6 on
the training data among the given methods for
combination.
We used the following three kinds of combi-
nations.
</bodyText>
<listItem confidence="0.70313975">
• Combined method 1
a combination of the simple Bayes and support
vector machine methods
• Combined method 2
</listItem>
<bodyText confidence="0.981305">
a combination of two kinds of the simple Bayes
method and two kinds of the support vector
machine method
(Here, &amp;quot;the two kinds&amp;quot; indicate an instance
where all features were used and where the syn-
tactic feature alone were not).7
</bodyText>
<listItem confidence="0.95043">
• Combined method 3
</listItem>
<bodyText confidence="0.9606345">
a combination of two kinds of the simple Bayes
method
(Here, &amp;quot;the two kinds&amp;quot; indicate instance where
c = 0.0001 and another where c = 0.01).
</bodyText>
<sectionHeader confidence="0.911284" genericHeader="method">
4 Features (information used in
classification)
</sectionHeader>
<bodyText confidence="0.997526">
In this paper, the following are defined as fea-
tures.
</bodyText>
<listItem confidence="0.9329525">
• Features based on strings
— strings in the analyzed morpheme
— strings of 1 to 3-grams just before the an-
alyzed morpheme
</listItem>
<footnote confidence="0.906293125">
5We used Kudoh&apos;s TinySVM software (Kudoh, 2000)
as the support vector machine.
6In the 10-fold cross validation, we first divide the
training data into ten parts. The answers of the in-
stances in each part are estimated by using the instances
in the remaining nine parts as the training data. We then
use all the results in the ten parts for evaluation.
7We used a case where the syntactic feature alone
</footnote>
<bodyText confidence="0.945849616666667">
was not used because it obtained a higher precision than
when all the features had been used in our preliminary
experiments.
— strings of 1 to 3-grams just after the ana-
lyzed morpheme
• Features based on the morphological in-
formation given by the RWC tags
— the part of speech (POS), the minor POS,
and the more minor POS of the analyzed
morpheme 8
— the previous morpheme, its 5-digit cate-
gory number, its 3-digit category number,
its POS, its minor POS, and its more mi-
nor POS9
— the next morpheme, its 5-digit category
number, its 3-digit category number, its
POS, its minor POS, and its more minor
POS
• Features based on the morphological in-
formation given by JUMAN
The corpus was analyzed using the Japanese
morphological analyzer, JUMAN (Kurohashi
and Nagao, 1998), and the results were used
as features.
— the POS, the minor POS, and the more
minor POS of the analyzed morpheme,
which were determined from the results of
JUMAN.
— the previous morpheme, its 5-digit cate-
gory number, its 3-digit category number,
its POS, its minor POS, and its more mi-
nor POS
— the next morpheme, its 5-digit category
number, its 3-digit category number, its
POS, its minor POS, and its more minor
POS
• Features based on syntactic information
The corpus was analyzed using the Japanese
syntactic analyzer KNP (Kurohashi, 1998), and
the results were used as features.
— the bunsetsu,1° including the analyzed
morpheme information on whether or not
8The POS, the minor POS, and the more minor POS
of a morpheme are the items in the third, fourth, and
fifth fields of the RWC corpus, respectively.
9A Japanese thesaurus, the Bunrui Goi Hyou dictio-
nary (NLRI, 1964), was used to determine the category
number of each morpheme. This thesaurus is of the &apos;is-
a&apos; hierarchical type, in which each word has a category
number, which is a 10-digit number that indicates seven
levels of an &apos;is-a&apos; hierarchy. The top five levels are ex-
pressed by the first five digits, the sixth level is expressed
by the next two digits, and the final level is expressed by
the final three digits.
m Bunsetsu is a Japanese grammatical term. A bun-
setsu is similar to a phrase in English, but is a slightly
smaller component. Eki-de &amp;quot;at the station&amp;quot; is a bun-
setsu, and sono, which corresponds to &amp;quot;the&amp;quot; or &amp;quot;its,&amp;quot; is
also a bunsetsu. A bunsetsu is, roughly, a unit of items
that refers to entities.
</bodyText>
<page confidence="0.998639">
137
</page>
<tableCaption confidence="0.999141">
Table 1: Experimental results
</tableCaption>
<table confidence="0.991073666666667">
Method Precision
Baseline method 0.726
Support vector machine (CRL1) 0.783
Simple Bayes method, e = 0.01 (CRL2) 0.778
Simple Bayes method, E = 0.0001 0.790
Combined method 1 (CRL3) 0.786
Combined method 2 (CRL4) 0.786
Combined method 3 0.793
The best method in the contest 0.786
</table>
<bodyText confidence="0.9986091875">
the bunsetsu was a noun phrase, the POS
of the bunsetsu&apos;s particle, the minor POS
of the particle, and the more minor POS
of the particle
the main word that the bunsetsu modifies,
including the analyzed morpheme and its
5-digit category number, 3-digit category
number, POS, minor POS, and more mi-
nor PUS
the main words of the modifiers of the
bunsetsu including the analyzed mor-
pheme and their 5-digit category numbers,
3-digit category numbers, POSs, minor
POSs, and more minor POSs (In this case,
the information on the particle, such as ga
or o, was used as well).
</bodyText>
<listItem confidence="0.7547885">
• Features of all words co-occurring in the
same sentence
</listItem>
<bodyText confidence="0.987891142857143">
The corpus was analyzed using the Japanese
morphological analyzer JUMAN (Kurohashi
and Nagao, 1998), and lists of the results were
used as features.
— each morphology in the same sentence, its
5-digit category number, and its 3-digit
category number
• Features of the UDC code in a document
In the RWC corpus, each document has a uni-
versal decimal code (UDC), indicating its cat-
egory.
— the first digit, the first two-digits, and the
first three-digits of the UDC in the docu-
ment
</bodyText>
<sectionHeader confidence="0.999811" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999940894736842">
We submitted the four systems (CRL1 to
CRL4), the support vector machine method,
the simple Bayes method (€ = 0.01), Combined
method 1, and Combined method 2. After the
contest, we carried out the experiments using
the simple Bayes (€ = 0.0001) and Combined
method 3. Their experimental results are shown
in Table 1. &amp;quot;Baseline method&amp;quot; selected the cate-
gory that most frequently occurred in the train-
ing data as the answer. &amp;quot;The best method in
the contest&amp;quot; was the best among all the sys-
tems submitted to the contest, which was CRL4
(0.786483). The precisions shown in the table
are the mixed-grained scores calculated by soft-
ware &amp;quot;scorer2&amp;quot; , which was given by the com-
mittees of SENSEVAL- 2. (In our systems, all
the instances were attempted, so the recall rate
was equal to its precision rate.)
We found the following items from the results.
</bodyText>
<listItem confidence="0.8572212">
• All the methods produced higher precision than
the baseline method.
• Among the four submitted systems (CRL1 to
CRL4), Combined method 2 was the best.
• The simple Bayes method using E = 0.0001
</listItem>
<reference confidence="0.6746165">
and Combined method 3 (the combination of
the two simple Bayes methods) obtained higher
precision. This indicates that the simple Bayes
method was effective.
</reference>
<sectionHeader confidence="0.995237" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999987555555555">
Our methods combining the simple Bayes and
support vector machine methods obtained the
best precision among all the submitted systems.
After the contest, we tuned the parameter used
in the simple Bayes method using the 10-fold
cross validation in the training data, and it ob-
tained higher precision. The best method was
the combination of the two simple Bayes, whose
precision was 0.793.
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997049115384616">
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Taku Kudoh and Yuji Matsumoto. 2000. Use of support vec-
tor learning for chunk identification. CoNLL-2000.
Taku Kudoh. 2000. TinySVM: Support Vector Machines.
http://cLaist-nara.ac.jp/ taku-ku// software/TinySVM/
index.html.
Sadao Kurohashi and Makoto Nagao, 1998. Japanese Mor-
phological Analysis System JUMAN version 3.5. Depart-
ment of Informatics, Kyoto University. (in Japanese).
Sadao Kurohashi, 1998. Japanese Dependency/Case Struc-
ture Analyzer KNP version 2.066. Department of Infor-
matics, Kyoto University. (in Japanese).
Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing
Ma, and Hitoshi Isahara. 2001. Experiments on word
sense disambiguation using several machine-learning meth-
ods. In IEICE-WGNLC2001-2. (in Japanese).
NLRI. 1964. Bunrui Goi Hyou. Shuuei Publishing.
Kiyoaki Shirai, Wakako Kashino, Minako Hashimoto,
Takenobu Tokunaga, Eiichi Arita, Hitoshi Isahara, Shiho
Ogino, Ryuichi Kobune, HIronobu Takahashi, Katashi
Nagao, Koiti Hasida, and Masaki Murata. 2001. Text
database with word sense tags defined by Iwanami
Japanese dictionary. Information Processing Society of
Japan, WGNL 141-19. (in Japanese).
</reference>
<page confidence="0.997329">
138
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.703653">
<title confidence="0.9804495">Japanese word sense disambiguation using the simple Bayes and support vector machine methods</title>
<author confidence="0.9017475">Masaki Murata</author>
<author confidence="0.9017475">Masao Utiyama</author>
<author confidence="0.9017475">Kiyotaka Qing Ma</author>
<author confidence="0.9017475">Hitoshi</author>
<affiliation confidence="0.957488">Communications Research</affiliation>
<address confidence="0.891595">2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan</address>
<abstract confidence="0.997885785714286">We submitted four systems to the Japanese dictionary-based lexical-sample task of SENSEVAL-2. They were i) the vector machine method ii) the simple Bayes method, iii) a method combining the two, and iv) a method combining two kinds of each. The combined methods obtained the best precision among the submitted systems. After contest, we tuned the parameter used in the simple Bayes method, and it obtained higher precision. An explanation of these systems used in Japanese word sense disambiguation was provided.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Combined method</author>
</authors>
<title>3 (the combination of the two simple Bayes methods) obtained higher precision. This indicates that the simple Bayes method was effective.</title>
<marker>method, </marker>
<rawString>and Combined method 3 (the combination of the two simple Bayes methods) obtained higher precision. This indicates that the simple Bayes method was effective.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines and Other Kernel-based Learning Methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4851" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="809" endWordPosition="812">ation is carried out using that hyperplane. Although the basics of this method are the same as those described above, in the extended versions of the method, the region between the margins through the training data can include a small number of examples, and the linearity of the hyperplane can be changed to a non-linearity by using kernel functions. The classification in the extended versions is equivalent to the classification using the following function (Equation (4)), and the two categories can be classified on the basis of whether the value output by the function is positive or negative (Cristianini and Shawe-Taylor, 2000; Kudoh, 2000): 21n the SENSEVAL-2 contest, we used 0.01 as e. After the contest, we tested several values (0.1 to 0.00000001) as c. We confirmed that e = 0.0001 produced the best results using 10-fold cross validation in the training data. 31n the figure, the white and black circles indicate positive and negative examples, respectively. The solid line indicates the hyperplane that divides the space, and the broken lines indicate the planes that mark the margins. where x is the context (a set of features) of an input example, xi indicates the context of a training datum, y, (i = I, ..., /, y, </context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudoh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of support vector learning for chunk identification.</title>
<date>2000</date>
<journal>CoNLL-2000.</journal>
<contexts>
<context position="6450" citStr="Kudoh and Matsumoto, 2000" startWordPosition="1101" endWordPosition="1104">ernel functions. We have used the following polynomial function exclusively. K(x, y) = (x • y + 1)d (9) C and d are constants set by experimentation. For all of the experiments reported in this paper, C was fixed as 1 and d was fixed as 2. A set of xi that satisfies a, &gt; 0 is called a support vector (SV44. The summation portion of Equation (4) was calculated using only the examples that were support vectors. Support vector machine methods are capable of handling data consisting of two categories. In general, data consisting of more than two categories is handled by using the pair-wise method (Kudoh and Matsumoto, 2000). In this method, for data consisting of N categories, pairs of two different categories (N(N1)/2 pairs) are constructed. The better cateIn 1, the circles in the broken lines indicate support vectors. Small Margin • 0 t 0 `1( 0 e?, Large Margin * =- sgn (1 cxi yiK(xi, x) + b (4) = maxi,yi bi + mini bi 2 j=1 — — 2 L(a) i=1 136 gory is determined by using a 2-category classifier (in this paper, a support vector machine5 was used as the 2-category classifier), and the correct category is finally determined by &amp;quot;yd.- ing&amp;quot; on the N(N-1)/2 pairs that result from analysis using the 2-category classifi</context>
</contexts>
<marker>Kudoh, Matsumoto, 2000</marker>
<rawString>Taku Kudoh and Yuji Matsumoto. 2000. Use of support vector learning for chunk identification. CoNLL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudoh</author>
</authors>
<title>TinySVM: Support Vector Machines.</title>
<date>2000</date>
<note>http://cLaist-nara.ac.jp/ taku-ku// software/TinySVM/ index.html.</note>
<contexts>
<context position="4865" citStr="Kudoh, 2000" startWordPosition="813" endWordPosition="814">rplane. Although the basics of this method are the same as those described above, in the extended versions of the method, the region between the margins through the training data can include a small number of examples, and the linearity of the hyperplane can be changed to a non-linearity by using kernel functions. The classification in the extended versions is equivalent to the classification using the following function (Equation (4)), and the two categories can be classified on the basis of whether the value output by the function is positive or negative (Cristianini and Shawe-Taylor, 2000; Kudoh, 2000): 21n the SENSEVAL-2 contest, we used 0.01 as e. After the contest, we tested several values (0.1 to 0.00000001) as c. We confirmed that e = 0.0001 produced the best results using 10-fold cross validation in the training data. 31n the figure, the white and black circles indicate positive and negative examples, respectively. The solid line indicates the hyperplane that divides the space, and the broken lines indicate the planes that mark the margins. where x is the context (a set of features) of an input example, xi indicates the context of a training datum, y, (i = I, ..., /, y, E {1,-1}) indi</context>
<context position="8262" citStr="Kudoh, 2000" startWordPosition="1423" endWordPosition="1424">thod and two kinds of the support vector machine method (Here, &amp;quot;the two kinds&amp;quot; indicate an instance where all features were used and where the syntactic feature alone were not).7 • Combined method 3 a combination of two kinds of the simple Bayes method (Here, &amp;quot;the two kinds&amp;quot; indicate instance where c = 0.0001 and another where c = 0.01). 4 Features (information used in classification) In this paper, the following are defined as features. • Features based on strings — strings in the analyzed morpheme — strings of 1 to 3-grams just before the analyzed morpheme 5We used Kudoh&apos;s TinySVM software (Kudoh, 2000) as the support vector machine. 6In the 10-fold cross validation, we first divide the training data into ten parts. The answers of the instances in each part are estimated by using the instances in the remaining nine parts as the training data. We then use all the results in the ten parts for evaluation. 7We used a case where the syntactic feature alone was not used because it obtained a higher precision than when all the features had been used in our preliminary experiments. — strings of 1 to 3-grams just after the analyzed morpheme • Features based on the morphological information given by t</context>
</contexts>
<marker>Kudoh, 2000</marker>
<rawString>Taku Kudoh. 2000. TinySVM: Support Vector Machines. http://cLaist-nara.ac.jp/ taku-ku// software/TinySVM/ index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<date>1998</date>
<booktitle>Japanese Morphological Analysis System JUMAN version 3.5.</booktitle>
<institution>Department of Informatics, Kyoto University.</institution>
<note>(in Japanese).</note>
<contexts>
<context position="9389" citStr="Kurohashi and Nagao, 1998" startWordPosition="1621" endWordPosition="1624">grams just after the analyzed morpheme • Features based on the morphological information given by the RWC tags — the part of speech (POS), the minor POS, and the more minor POS of the analyzed morpheme 8 — the previous morpheme, its 5-digit category number, its 3-digit category number, its POS, its minor POS, and its more minor POS9 — the next morpheme, its 5-digit category number, its 3-digit category number, its POS, its minor POS, and its more minor POS • Features based on the morphological information given by JUMAN The corpus was analyzed using the Japanese morphological analyzer, JUMAN (Kurohashi and Nagao, 1998), and the results were used as features. — the POS, the minor POS, and the more minor POS of the analyzed morpheme, which were determined from the results of JUMAN. — the previous morpheme, its 5-digit category number, its 3-digit category number, its POS, its minor POS, and its more minor POS — the next morpheme, its 5-digit category number, its 3-digit category number, its POS, its minor POS, and its more minor POS • Features based on syntactic information The corpus was analyzed using the Japanese syntactic analyzer KNP (Kurohashi, 1998), and the results were used as features. — the bunsets</context>
<context position="12012" citStr="Kurohashi and Nagao, 1998" startWordPosition="2075" endWordPosition="2078">nd the more minor POS of the particle the main word that the bunsetsu modifies, including the analyzed morpheme and its 5-digit category number, 3-digit category number, POS, minor POS, and more minor PUS the main words of the modifiers of the bunsetsu including the analyzed morpheme and their 5-digit category numbers, 3-digit category numbers, POSs, minor POSs, and more minor POSs (In this case, the information on the particle, such as ga or o, was used as well). • Features of all words co-occurring in the same sentence The corpus was analyzed using the Japanese morphological analyzer JUMAN (Kurohashi and Nagao, 1998), and lists of the results were used as features. — each morphology in the same sentence, its 5-digit category number, and its 3-digit category number • Features of the UDC code in a document In the RWC corpus, each document has a universal decimal code (UDC), indicating its category. — the first digit, the first two-digits, and the first three-digits of the UDC in the document 5 Experiments We submitted the four systems (CRL1 to CRL4), the support vector machine method, the simple Bayes method (€ = 0.01), Combined method 1, and Combined method 2. After the contest, we carried out the experime</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>Sadao Kurohashi and Makoto Nagao, 1998. Japanese Morphological Analysis System JUMAN version 3.5. Department of Informatics, Kyoto University. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
</authors>
<title>Japanese Dependency/Case Structure Analyzer KNP version</title>
<date>1998</date>
<institution>Department of Informatics, Kyoto University.</institution>
<note>(in Japanese).</note>
<contexts>
<context position="9935" citStr="Kurohashi, 1998" startWordPosition="1717" endWordPosition="1718">the Japanese morphological analyzer, JUMAN (Kurohashi and Nagao, 1998), and the results were used as features. — the POS, the minor POS, and the more minor POS of the analyzed morpheme, which were determined from the results of JUMAN. — the previous morpheme, its 5-digit category number, its 3-digit category number, its POS, its minor POS, and its more minor POS — the next morpheme, its 5-digit category number, its 3-digit category number, its POS, its minor POS, and its more minor POS • Features based on syntactic information The corpus was analyzed using the Japanese syntactic analyzer KNP (Kurohashi, 1998), and the results were used as features. — the bunsetsu,1° including the analyzed morpheme information on whether or not 8The POS, the minor POS, and the more minor POS of a morpheme are the items in the third, fourth, and fifth fields of the RWC corpus, respectively. 9A Japanese thesaurus, the Bunrui Goi Hyou dictionary (NLRI, 1964), was used to determine the category number of each morpheme. This thesaurus is of the &apos;isa&apos; hierarchical type, in which each word has a category number, which is a 10-digit number that indicates seven levels of an &apos;is-a&apos; hierarchy. The top five levels are expresse</context>
</contexts>
<marker>Kurohashi, 1998</marker>
<rawString>Sadao Kurohashi, 1998. Japanese Dependency/Case Structure Analyzer KNP version 2.066. Department of Informatics, Kyoto University. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Masao Utiyama</author>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Experiments on word sense disambiguation using several machine-learning methods.</title>
<date>2001</date>
<booktitle>In IEICE-WGNLC2001-2. (in Japanese). NLRI.</booktitle>
<publisher>Shuuei Publishing.</publisher>
<contexts>
<context position="3359" citStr="Murata et al., 2001" startWordPosition="552" endWordPosition="556">f the occurrence of category a in the examples of the training data and the probability of feature L occurring, given category a, respectively. When we use the maximum likelihood estimation to calculate /3( fi I a), which often has a value of 0 and is therefore difficult to estimate the desired category, smoothing process is used. We used this 1We made preliminary experiments using various methods: the simple Bayes, the decision list, the maximum entropy, and the support vector machine. The results showed that the simple Bayes and support vector machine methods were better than the other two (Murata et al., 2001). We used these two methods in the contest. 135 Figure 1: Maximizing the margin equation for smoothing: f req( f,, a) + * f req(a) 13(.ii la) = (3) f req(a) c * f req(a) where f req(fi, a) is the number of events that have the feature fi and whose category is a and f req(a) is the number of events whose category is a. € is a constant set by experimentation. In this study, we used 0.01 and 0.0001 as €.2 3.2 Support Vector Machine Method In this method, data consisting of two categories is classified by using a hyperplane to divide a space. When the two categories are, for example, positive and </context>
</contexts>
<marker>Murata, Utiyama, Uchimoto, Ma, Isahara, 2001</marker>
<rawString>Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing Ma, and Hitoshi Isahara. 2001. Experiments on word sense disambiguation using several machine-learning methods. In IEICE-WGNLC2001-2. (in Japanese). NLRI. 1964. Bunrui Goi Hyou. Shuuei Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
</authors>
<title>Wakako Kashino, Minako Hashimoto, Takenobu Tokunaga, Eiichi Arita, Hitoshi Isahara, Shiho Ogino, Ryuichi Kobune, HIronobu Takahashi, Katashi Nagao, Koiti Hasida, and</title>
<date>2001</date>
<booktitle>Information Processing Society of Japan, WGNL</booktitle>
<pages>141--19</pages>
<note>(in Japanese).</note>
<marker>Shirai, 2001</marker>
<rawString>Kiyoaki Shirai, Wakako Kashino, Minako Hashimoto, Takenobu Tokunaga, Eiichi Arita, Hitoshi Isahara, Shiho Ogino, Ryuichi Kobune, HIronobu Takahashi, Katashi Nagao, Koiti Hasida, and Masaki Murata. 2001. Text database with word sense tags defined by Iwanami Japanese dictionary. Information Processing Society of Japan, WGNL 141-19. (in Japanese).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>