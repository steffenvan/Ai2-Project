<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011697">
<title confidence="0.993001">
Exploring the Use of NLP in the Disclosure of Electronic Patient Records
</title>
<author confidence="0.997827">
David Hardcastle
</author>
<affiliation confidence="0.980436">
Faculty of Mathematics and Computing
The Open University
</affiliation>
<email confidence="0.997862">
d.w.hardcastle@open.ac.uk
</email>
<author confidence="0.987489">
Catalina Hallett
</author>
<affiliation confidence="0.9767905">
Faculty of Mathematics and Computing
The Open University
</affiliation>
<email confidence="0.998349">
c.hallett@open.ac.uk
</email>
<sectionHeader confidence="0.997383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885">
This paper describes a preliminary analysis
of issues involved in the production of re-
ports aimed at patients from Electronic Pa-
tient Records. We present a system proto-
type and discuss the problems encountered.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997982482758621">
Allowing patient access to Electronic Patient
Records (EPR) in a comprehensive format is a le-
gal requirement in most European countries. Apart
from this legal aspect, research shows that the provi-
sion of clear information to patients is instrumental
in improving the quality of care (Detmer and Sin-
gleton, 2004). Current work on generating expla-
nations of EPRs to patients suffer from two major
drawbacks. Firstly, existing report generation sys-
tems have taken an intuitive approach to the gener-
ation of explanation: there is no principled way of
selecting the information that requires further expla-
nation. Secondly, most work on medical report gen-
eration systems has concentrated on explaining the
structured part of an EPR; there has been very lit-
tle work on providing automatic explanations of the
narratives (such as letters between health practition-
ers) which represent a considerable part of an EPR.
Attempting to rewrite narratives in a patient-friendly
way is in many ways more difficult than providing
suggestions for natural language generation systems
that take as input data records. In narratives, ambi-
guity can arise from a combination of aspects over
which NLG systems have full control, such as syn-
tax, discourse structure, sentence length, formatting
and readability.
This paper introduces a pilot project that attempts
to address this gap by addressing the following re-
search questions:
</bodyText>
<listItem confidence="0.9990855">
1. Given the text-based part of a patient record,
which segments require explanation before being re-
leased to patients?
2. Which types of explanation are appropriate for
various types of segment?
3. Which subparts of a segment require explanation?
</listItem>
<bodyText confidence="0.999977">
The prototype system correctly selects the seg-
ments that require explanation, but we have yet to
solve the problem of accurately identifiying the fea-
tures that contribute to the “expertness” of a doc-
ument. We discuss the underlying issues in more
detail in section 3 below.
</bodyText>
<sectionHeader confidence="0.973799" genericHeader="method">
2 Feature identification method
</sectionHeader>
<bodyText confidence="0.999952533333333">
To identify a set of features that differentiate med-
ical expert and lay language, we compared a cor-
pus of expert text with a corpus of lay texts. We
then used the selected features on a corpus of nar-
ratives extracted from a repository of Electronic Pa-
tient Records to attempt to answer the three ques-
tions posed above. First, paragraphs that contain
features characteristic to expert documents are high-
lighted using a corpus of patient information leaflets
as a background reference. Second, we prioritise the
explanations required by decomposing the classifi-
cation data. Finally, we identify within those sec-
tions the features that contribute to the classification
of the section as belonging to the expert register, and
provide suggestions for text simplification.
</bodyText>
<sectionHeader confidence="0.523068" genericHeader="method">
2.1 Features
</sectionHeader>
<bodyText confidence="0.999734166666666">
The feature identification was performed on two cor-
pora of about 200000 words each: (a) an expert
corpus, containing clinical case studies and med-
ical manuals produced for doctors and (b) a lay
corpus, containing patient testimonials and infor-
mational materials for patients. Both corpora were
</bodyText>
<page confidence="0.945025">
161
</page>
<bodyText confidence="0.956403217391304">
BioNLP 2007: Biological, translational, and clinical language processing, pages 161–162,
Prague, June 2007. c�2007 Association for Computational Linguistics
sourced from a variety of online sources. In com-
paring the corpora we considered a variety of fea-
tures in the following categories: medical content,
syntactic structure, discourse structure, readability
and layout. The features that proved to be best dis-
criminators were the frequency of medical terms,
readability indices, average NP length and the rela-
tive frequency of loan words against English equiva-
lents1. The medical content analysis is based on the
MeSH terminology (Canese, 2003) and consists of
assessing: (a) the frequency of MeSH primary con-
cepts and alternative descriptions, (b) the frequency
of medical terms types and occurences and (c) the
frequency of MeSH terms in various top-level cate-
gories. The readability features consist of two stan-
dard readability indices (FOG and Flesch-Kincaid).
Although some discourse and layout features also
proved to have a high discriminatory power, they
are strongly dependent on the distribution medium
of the analysed materials, hence not suitable for our
analysis of EPR narratives.
</bodyText>
<subsectionHeader confidence="0.999604">
2.2 Analysing EPR narratives
</subsectionHeader>
<bodyText confidence="0.999975416666667">
We performed our analysis on a corpus of 11000
narratives extracted from a large repository of Elec-
tronic Patient Records, totalling almost 2 million
words. Each segment of each narrative was then as-
sessed on the basis of the features described above,
such as Fog, sentence length, MeSH primary con-
cepts etc. We then smoothed all of the scores for
all segments for each feature forcing the minimum
to 0.0, the maximum to 1.0 and the reference corpus
score for that feature to 0.5. This made it possible to
compare scores with different gradients and scales
against a common baseline in a consistent way.
</bodyText>
<sectionHeader confidence="0.980289" genericHeader="evaluation">
3 Evaluation and discussion
</sectionHeader>
<bodyText confidence="0.999252125">
We evaluated our segment identification method on
a set of 10 narratives containing 27 paragraphs, ex-
tracted from the same repository of EPRs . The seg-
ment identification method proved succesful, with
26/27 (96.3%) segments marked correctly are re-
quiring/not requiring explanation. However, this
only addresses the first of the three questions set
out above, leaving the following research questions
</bodyText>
<footnote confidence="0.908393">
1An in-depth analysis of unfamiliar terms in medical docu-
ments can be found in (Elhadad, 2006)
</footnote>
<bodyText confidence="0.950922714285714">
open to further analysis.
Quantitative vs qualitative analysis
Many of the measures that discriminate expert from
lay texts are based on indicative features; for exam-
ple complex words are indicative of text that is dif-
ficult to read. However, there is no guarantee that
individual words or phrases that are indicative are
also representative - in other words a given complex
word or long sentence will contribute to the readabil-
ity score of the segment, but may not itself be prob-
lematic. Similarly, frequency based measures, such
as a count of medical terminology, discriminate at a
segment level but do not entail that each occurrence
requires attention.
</bodyText>
<sectionHeader confidence="0.507993" genericHeader="evaluation">
Terminology
</sectionHeader>
<bodyText confidence="0.999873857142857">
We used the MeSH terminology to analyse med-
ical terms in patient records, however (as with prac-
tically all medical terminologies) it contains many
non-expert medical terms. We are currently investi-
gating the possibility of mining a list of expert terms
from MeSH or of making use of medical-lay aligned
ontologies.
</bodyText>
<sectionHeader confidence="0.58302" genericHeader="conclusions">
Classification
</sectionHeader>
<bodyText confidence="0.997392857142857">
Narratives in the EPR are written in a completely dif-
ferent style from both our training expert corpus and
the reference patient information leaflets corpus. It
is therefore very difficult to use the reference corpus
as a threshold for feature values which can produce
good results on the corpus of narratives, suggest-
ing that a statistical thresholding technique might be
more effective.
Feature dependencies
Most document features are not independent. There-
fore, the rewriting suggestions the system provides
may themselves have an unwanted impact on the
rewritten text, leading to a circular process for the
end-user.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9987895">
Kathi Canese. 2003. New Entrez Database: MeSH.
NLM Technical Bulletin, March-April.
D. Detmer and P. Singleton. 2004. The informed pa-
tient. Technical Report TIP-2, Judge Institute of Man-
agement, University of Cambridge, Cambridge.
Noemi Elhadad. 2006. Comprehending technical texts:
Predicting and defining unfamiliar terms. In Proceed-
ing ofAMIA’06, pages 239–243.
</reference>
<page confidence="0.997785">
162
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.147011">
<title confidence="0.999734">Exploring the Use of NLP in the Disclosure of Electronic Patient Records</title>
<author confidence="0.866198">David</author>
<affiliation confidence="0.854194">Faculty of Mathematics and</affiliation>
<author confidence="0.494873">The Open</author>
<email confidence="0.961611">d.w.hardcastle@open.ac.uk</email>
<affiliation confidence="0.826545">Catalina Faculty of Mathematics and</affiliation>
<author confidence="0.496664">The Open</author>
<email confidence="0.98226">c.hallett@open.ac.uk</email>
<abstract confidence="0.990418333333333">This paper describes a preliminary analysis of issues involved in the production of reports aimed at patients from Electronic Patient Records. We present a system prototype and discuss the problems encountered.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kathi Canese</author>
</authors>
<title>New Entrez Database: MeSH.</title>
<date>2003</date>
<tech>NLM Technical Bulletin, March-April.</tech>
<contexts>
<context position="4187" citStr="Canese, 2003" startWordPosition="644" endWordPosition="645">ional, and clinical language processing, pages 161–162, Prague, June 2007. c�2007 Association for Computational Linguistics sourced from a variety of online sources. In comparing the corpora we considered a variety of features in the following categories: medical content, syntactic structure, discourse structure, readability and layout. The features that proved to be best discriminators were the frequency of medical terms, readability indices, average NP length and the relative frequency of loan words against English equivalents1. The medical content analysis is based on the MeSH terminology (Canese, 2003) and consists of assessing: (a) the frequency of MeSH primary concepts and alternative descriptions, (b) the frequency of medical terms types and occurences and (c) the frequency of MeSH terms in various top-level categories. The readability features consist of two standard readability indices (FOG and Flesch-Kincaid). Although some discourse and layout features also proved to have a high discriminatory power, they are strongly dependent on the distribution medium of the analysed materials, hence not suitable for our analysis of EPR narratives. 2.2 Analysing EPR narratives We performed our ana</context>
</contexts>
<marker>Canese, 2003</marker>
<rawString>Kathi Canese. 2003. New Entrez Database: MeSH. NLM Technical Bulletin, March-April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Detmer</author>
<author>P Singleton</author>
</authors>
<title>The informed patient.</title>
<date>2004</date>
<tech>Technical Report TIP-2,</tech>
<institution>Judge Institute of Management, University of Cambridge,</institution>
<location>Cambridge.</location>
<contexts>
<context position="814" citStr="Detmer and Singleton, 2004" startWordPosition="116" endWordPosition="120">tt Faculty of Mathematics and Computing The Open University c.hallett@open.ac.uk Abstract This paper describes a preliminary analysis of issues involved in the production of reports aimed at patients from Electronic Patient Records. We present a system prototype and discuss the problems encountered. 1 Introduction Allowing patient access to Electronic Patient Records (EPR) in a comprehensive format is a legal requirement in most European countries. Apart from this legal aspect, research shows that the provision of clear information to patients is instrumental in improving the quality of care (Detmer and Singleton, 2004). Current work on generating explanations of EPRs to patients suffer from two major drawbacks. Firstly, existing report generation systems have taken an intuitive approach to the generation of explanation: there is no principled way of selecting the information that requires further explanation. Secondly, most work on medical report generation systems has concentrated on explaining the structured part of an EPR; there has been very little work on providing automatic explanations of the narratives (such as letters between health practitioners) which represent a considerable part of an EPR. Atte</context>
</contexts>
<marker>Detmer, Singleton, 2004</marker>
<rawString>D. Detmer and P. Singleton. 2004. The informed patient. Technical Report TIP-2, Judge Institute of Management, University of Cambridge, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noemi Elhadad</author>
</authors>
<title>Comprehending technical texts: Predicting and defining unfamiliar terms.</title>
<date>2006</date>
<booktitle>In Proceeding ofAMIA’06,</booktitle>
<pages>239--243</pages>
<contexts>
<context position="5893" citStr="Elhadad, 2006" startWordPosition="916" endWordPosition="917">e scores with different gradients and scales against a common baseline in a consistent way. 3 Evaluation and discussion We evaluated our segment identification method on a set of 10 narratives containing 27 paragraphs, extracted from the same repository of EPRs . The segment identification method proved succesful, with 26/27 (96.3%) segments marked correctly are requiring/not requiring explanation. However, this only addresses the first of the three questions set out above, leaving the following research questions 1An in-depth analysis of unfamiliar terms in medical documents can be found in (Elhadad, 2006) open to further analysis. Quantitative vs qualitative analysis Many of the measures that discriminate expert from lay texts are based on indicative features; for example complex words are indicative of text that is difficult to read. However, there is no guarantee that individual words or phrases that are indicative are also representative - in other words a given complex word or long sentence will contribute to the readability score of the segment, but may not itself be problematic. Similarly, frequency based measures, such as a count of medical terminology, discriminate at a segment level b</context>
</contexts>
<marker>Elhadad, 2006</marker>
<rawString>Noemi Elhadad. 2006. Comprehending technical texts: Predicting and defining unfamiliar terms. In Proceeding ofAMIA’06, pages 239–243.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>