<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001350">
<title confidence="0.9957155">
CNGL-CORE: Referential Translation Machines
for Measuring Semantic Similarity
</title>
<author confidence="0.99167">
Ergun Bic¸ici
</author>
<affiliation confidence="0.98854">
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
</affiliation>
<email confidence="0.994641">
ebicici@computing.dcu.ie
</email>
<author confidence="0.833761">
Josef van Genabith
</author>
<affiliation confidence="0.9726865">
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
</affiliation>
<email confidence="0.995555">
josef@computing.dcu.ie
</email>
<sectionHeader confidence="0.998577" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997060695652174">
We invent referential translation machines
(RTMs), a computational model for identify-
ing the translation acts between any two data
sets with respect to a reference corpus selected
in the same domain, which can be used for
judging the semantic similarity between text.
RTMs make quality and semantic similarity
judgments possible by using retrieved rele-
vant training data as interpretants for reach-
ing shared semantics. An MTPP (machine
translation performance predictor) model de-
rives features measuring the closeness of the
test sentences to the training data, the diffi-
culty of translating them, and the presence of
acts of translation involved. We view seman-
tic similarity as paraphrasing between any two
given texts. Each view is modeled by an RTM
model, giving us a new perspective on the bi-
nary relationship between the two. Our pre-
diction model is the 15th on some tasks and
30th overall out of 89 submissions in total ac-
cording to the official results of the Semantic
Textual Similarity (STS 2013) challenge.
</bodyText>
<sectionHeader confidence="0.945256" genericHeader="method">
1 Semantic Textual Similarity Judgments
</sectionHeader>
<bodyText confidence="0.999988689655172">
We introduce a fully automated judge for semantic
similarity that performs well in the semantic textual
similarity (STS) task (Agirre et al., 2013). STS is
a degree of semantic equivalence between two texts
based on the observations that “vehicle” and “car”
are more similar than “wave” and “car”. Accurate
prediction of STS has a wide application area in-
cluding: identifying whether two tweets are talk-
ing about the same thing, whether an answer is cor-
rect by comparing it with a reference answer, and
whether a given shorter text is a valid summary of
another text.
The translation quality estimation task (Callison-
Burch et al., 2012) aims to develop quality indicators
for translations at the sentence-level and predictors
without access to a reference translation. Bicici et
al. (2013) develop a top performing machine transla-
tion performance predictor (MTPP), which uses ma-
chine learning models over features measuring how
well the test set matches the training set relying on
extrinsic and language independent features.
The semantic textual similarity (STS) task (Agirre
et al., 2013) addresses the following problem. Given
two sentences S1 and S2 in the same language, quan-
tify the degree of similarity with a similarity score,
which is a number in the range [0, 5]. The semantic
textual similarity prediction problem involves find-
ing a function f approximating the semantic textual
similarity score given two sentences, S1 and S2:
</bodyText>
<equation confidence="0.960748">
f(S1, S2) &amp;quot; q(S1, S2). (1)
</equation>
<bodyText confidence="0.99997725">
We approach f as a supervised learning problem
with (S1, S2, q(S1, S2)) tuples being the training
data and q(S1, S2) being the target similarity score.
We model the problem as a translation task where
one possible interpretation is obtained by translat-
ing S1 (the source to translate, S) to S2 (the target
translation, T). Since linguistic processing can re-
veal deeper similarity relationships, we also look at
the translation task at different granularities of infor-
mation: plain text (R for regular) , after lemmatiza-
tion (L), after part-of-speech (POS) tagging (P), and
after removing 128 English stop-words (S) 1. Thus,
</bodyText>
<footnote confidence="0.993216">
1http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/
</footnote>
<page confidence="0.89465">
234
</page>
<subsubsectionHeader confidence="0.233487">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
</subsubsectionHeader>
<bodyText confidence="0.796302666666667">
and the Shared Task, pages 234–240, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
we obtain 4 different perspectives on the binary re-
lationship between S1 and S2.
</bodyText>
<sectionHeader confidence="0.974013" genericHeader="method">
2 Referential Translation Machine (RTM)
</sectionHeader>
<bodyText confidence="0.986126846153846">
Referential translation machines (RTMs) we de-
velop provide a computational model for quality and
semantic similarity judgments using retrieval of rel-
evant training data (Bic¸ici and Yuret, 2011a; Bic¸ici,
2011) as interpretants for reaching shared seman-
tics (Bic¸ici, 2008). We show that RTM achieves very
good performance in judging the semantic similarity
of sentences and we can also use RTM to automat-
ically assess the correctness of student answers to
obtain better results (Bic¸ici and van Genabith, 2013)
than the state-of-the-art (Dzikovska et al., 2012).
RTM is a computational model for identifying
the acts of translation for translating between any
given two data sets with respect to a reference cor-
pus selected in the same domain. RTM can be used
for automatically judging the semantic similarity be-
tween texts. An RTM model is based on the selec-
tion of common training data relevant and close to
both the training set and the test set where the se-
lected relevant set of instances are called the inter-
pretants. Interpretants allow shared semantics to be
possible by behaving as a reference point for simi-
larity judgments and providing the context. In semi-
otics, an interpretant I interprets the signs used to
refer to the real objects (Bic¸ici, 2008). RTMs pro-
vide a model for computational semantics using in-
terpretants as a reference according to which seman-
tic judgments with translation acts are made. Each
RTM model is a data translation model between the
instances in the training set and the test set. We use
the FDA (Feature Decay Algorithms) instance se-
lection model for selecting the interpretants (Bic¸ici
and Yuret, 2011a) from a given corpus, which can
be monolingual when modeling paraphrasing acts,
in which case the MTPP model (Section 2.1) is built
using the interpretants themselves as both the source
and the target side of the parallel corpus. RTMs map
the training and test data to a space where translation
acts can be identified. We view that acts of transla-
tion are ubiquitously used during communication:
Every act of communication is an act of
translation (Bliss, 2012).
src/backend/snowball/stopwords/
Translation need not be between different languages
and paraphrasing or communication also contain
acts of translation. When creating sentences, we use
our background knowledge and translate informa-
tion content according to the current context.
Given a training set train, a test set test, and
some monolingual corpus C, preferably in the same
domain as the training and test sets, the RTM steps
are:
</bodyText>
<listItem confidence="0.999102">
1. T = train U test.
2. select(T, C) —* I
3. MTPP(I, train) —* Ttrain
4. MTPP(I, test) —* Ttest
5. learn(M, Ttrain) — M
6. predict(M,Ttest) —* q
</listItem>
<bodyText confidence="0.99997775">
Step 2 selects the interpretants, I, relevant to the
instances in the combined training and test data.
Steps 3, 4 use I to map train and test to a new
space where similarities between translation acts can
be derived more easily. Step 5 trains a learning
model M over the training features, Ttrain, and
Step 6 obtains the predictions. RTM relies on the
representativeness of I as a medium for building
translation models for translating between train
and test.
Our encouraging results in the STS task provides
a greater understanding of the acts of translation we
ubiquitously use when communicating and how they
can be used to predict the performance of transla-
tion, judging the semantic similarity between text,
and evaluating the quality of student answers. RTM
and MTPP models are not data or language specific
and their modeling power and good performance are
applicable across different domains and tasks. RTM
expands the applicability of MTPP by making it fea-
sible when making monolingual quality and simi-
larity judgments and it enhances the computational
scalability by building models over smaller but more
relevant training data as interpretants.
</bodyText>
<subsectionHeader confidence="0.9791735">
2.1 The Machine Translation Performance
Predictor (MTPP)
</subsectionHeader>
<bodyText confidence="0.999697">
In machine translation (MT), pairs of source and tar-
get sentences are used for training statistical MT
(SMT) models. SMT system performance is af-
fected by the amount of training data used as well
</bodyText>
<page confidence="0.988099">
235
</page>
<bodyText confidence="0.999968769230769">
as the closeness of the test set to the training set.
MTPP (Bic¸ici et al., 2013) is a top performing ma-
chine translation performance predictor, which uses
machine learning models over features measuring
how well the test set matches the training set to pre-
dict the quality of a translation without using a ref-
erence translation. MTPP measures the coverage of
individual test sentence features and syntactic struc-
tures found in the training set and derives feature
functions measuring the closeness of test sentences
to the available training data, the difficulty of trans-
lating the sentence, and the presence of acts of trans-
lation for data transformation.
</bodyText>
<subsectionHeader confidence="0.993752">
2.2 MTPP Features for Translation Acts
</subsectionHeader>
<bodyText confidence="0.999925866666667">
MTPP uses n-gram features defined over text or
common cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over which
similarity calculations are made. Unsupervised
parsing with CCL extracts links from base words
to head words, which allow us to obtain structures
representing the grammatical information instanti-
ated in the training and test data. Feature functions
use statistics involving the training set and the test
sentences to determine their closeness. Since they
are language independent, MTPP allows quality es-
timation to be performed extrinsically. Categories
for the 289 features used are listed below and their
detailed descriptions are presented in (Bic¸ici et al.,
2013) where the number of features are given in {#}.
</bodyText>
<listItem confidence="0.98589282051282">
• Coverage {110}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
• Synthetic Translation Performance {6}: Calcu-
lates translation scores achievable according to
the n-gram coverage.
• Length {4}: Calculates the number of words
and characters for S and T and their ratios.
• Feature Vector Similarity {16}: Calculates the
similarities between vector representations.
• Perplexity {90}: Measures the fluency of the
sentences according to language models (LM).
We use both forward ({30}) and backward
({15}) LM based features for S and T.
• Entropy {4}: Calculates the distributional sim-
ilarity of test sentences to the training set.
• Retrieval Closeness {24}: Measures the de-
gree to which sentences close to the test set are
found in the training set.
• Diversity {6}: Measures the diversity of co-
occurring features in the training set.
• IBM1 Translation Probability {16}: Calculates
the translation probability of test sentences us-
ing the training set (Brown et al., 1993).
• Minimum Bayes Retrieval Risk {4}: Calculates
the translation probability for the translation
having the minimum Bayes risk among the re-
trieved training instances.
• Sentence Translation Performance {3}: Calcu-
lates translation scores obtained according to
q(T, R) using BLEU (Papineni et al., 2002),
NIST (Doddington, 2002), or F1 (Bic¸ici and
Yuret, 2011b) for q.
• Character n-grams {4}: Calculates the cosine
between the character n-grams (for n=2,3,4,5)
obtained for S and T (B¨ar et al., 2012).
• LIX {2}: Calculates the LIX readability
score (Wikipedia, 2013; Bj¨ornsson, 1968) for
Sand T. 2
</listItem>
<sectionHeader confidence="0.998967" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.997745466666667">
STS contains sentence pairs from news headlines
(headlines), sense definitions from semantic lexical
resources (OnWN is from OntoNotes (Pradhan et
al., 2007) and WordNet (Miller, 1995) and FNWN is
from FrameNet (Baker et al., 1998) and WordNet),
and statistical machine translation (SMT) (Agirre et
al., 2013). STS challenge results are evaluated with
the Pearson’s correlation score (r).
The test set contains 2250 (S1, S2) sentence pairs
with 750, 561, 189, and 750 sentences from each
type respectively. The training set contains 5342
sentence pairs with 1500 each from MSRpar and
MSRvid (Microsoft Research paraphrase and video
description corpus (Agirre et al., 2012)), 1592 from
SMT, and 750 from OnWN.
</bodyText>
<subsectionHeader confidence="0.972578">
3.1 RTM Models
</subsectionHeader>
<bodyText confidence="0.856396111111111">
We obtain CNGL results for the STS task as fol-
lows. For each perspective described in Section 1,
we build an RTM model. Each RTM model views
the STS task from a different perspective using the
289 features extracted dependent on the interpre-
tants using MTPP. We extract the features both on
2LIX= a + C 100, where A is the number of words, C is
words longer than 6 characters, B is words that start or end with
any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012).
</bodyText>
<page confidence="0.971792">
236
</page>
<figure confidence="0.980143230769231">
r
R+P+L+S
S
L+S TL
L+S
L+P+S
L+P+S TL
R+P+L+S TL
P
L
R
L+P
R+S
R+P
R+L
R+P+S
R+P+L
RR
S1 → S2
SVR
RR
S2 → S1
SVR
RR
S1 = S2
SVR
</figure>
<table confidence="0.997352333333333">
.7904 .7502 .8200 .7788 .8074 .8232 .8101 .8247 .8218 .8509 .8266 .8172 .8304 .8530 .8323 .8499
.8311 .8060 .8443 .8330 .8404 .8517 .8498 .8501 .8593 .8556 .8496 .8422 .8586 .8579 .8527 .8564
.7922 .7651 .8169 .7891 .8064 .8196 .8136 .8219 .8257 .8257 .8226 .8164 .8284 .8284 .8313 .8324
.8308 .8165 .8407 .8302 .8361 .8506 .8467 .8510 .8567 .8567 .8525 .8460 .8588 .8588 .8575 .8574
.8079 .787 .8279 .8101 .8216 .8333 .8275 .8346 .8375 .8409 .8361 .8312 .8412 .8434 .8432 .844
.8397 .8237 .8554 .841 .8432 .857 .851 .8557 .8605 .8626 .8505 .8505 .8591 .8622 .8602 .8588
</table>
<tableCaption confidence="0.9963635">
Table 1: CV performance on the training set with tuning. Underlined are the settings we use in our submissions. RTM
models in directions S1 → S2, S2 → S1, and the bi-directional models S1 +---I S2 are displayed.
</tableCaption>
<bodyText confidence="0.999970193548387">
the training set and the test set. The training cor-
pus used is the English side of an out-of-domain
corpus on European parliamentary discussions, Eu-
roparl (Callison-Burch et al., 2012) 3. In-domain
corpora are likely to improve the performance. We
use the Stanford POS tagger (Toutanova et al., 2003)
to obtain the perspectives P and L. We use the train-
ing corpus to build a 5-gram target LM.
We use ridge regression (RR) and support vec-
tor regression (SVR) with RBF kernel (Smola and
Sch¨olkopf, 2004). Both of these models learn a re-
gression function using the features to estimate a nu-
merical target value. The parameters that govern the
behavior of RR and SVR are the regularization A
for RR and the C, c, and -y parameters for SVR. At
testing time, the predictions are bounded to obtain
scores in the range [0, 5]. We perform tuning on a
subset of the training set separately for each RTM
model and optimize against the performance evalu-
ated with R2, the coefficient of determination.
We do not build a separate model for different
types of sentences and instead use all of the train-
ing set for building a large prediction model. We
also use transductive learning since using only the
relevant training data for training can improve the
performance (Bic¸ici, 2011). Transductive learning
is performed at the sentence level where for each test
instance, we select 1250 relevant training instances
using the cosine similarity metric over the feature
vectors and build an individual model for the test in-
stance and predict the similarity score.
</bodyText>
<footnote confidence="0.85166">
3We use WMT’13 corpora from www.statmt.org/wmt13/.
</footnote>
<subsectionHeader confidence="0.999241">
3.2 Training Results
</subsectionHeader>
<bodyText confidence="0.999826722222222">
Table 1 lists the 10-fold cross-validation (CV) re-
sults on the training set for RR and SVR for differ-
ent RTM systems using optimized parameters. As
we combine different perspectives, the performance
improves and we use the L+S with SVR for run 1
(LSSVR), L+P+S with SVR for run 2 (LPSSVR),
and L+P+S with SVR using transductive learning
for run 3 (LPSSVRTL) all in the translation direc-
tion S1 → S2. Lemmatized RTM, L, performs the
best among the individual perspectives. We also
build RTM models in the direction S2 → S1, which
gives similar results. The last main row combines
them to obtain the bi-directional results, S1 = S2,
which improves the performance. Each additional
perspective adds another 289 features to the repre-
sentation and the bi-directional results double the
number of features. Thus, S1 = S2 L+P+S is us-
ing 1734 features.
</bodyText>
<subsectionHeader confidence="0.995294">
3.3 STS Challenge Results
</subsectionHeader>
<bodyText confidence="0.999909583333333">
Table 2 presents the STS challenge r and ranking
results containing our CNGL submissions, the best
system result, and the mean results over all submis-
sions. There were 89 submissions from 35 compet-
ing systems (Agirre et al., 2013). The results are
ranked according to the mean r obtained. We also
include the mean result over all of the submissions
and its corresponding rank.
According to the official results, CNGL-LSSVR
is the 30th system from the top based on the mean r
obtained and CNGL-LPSSVR is 15th according to
the results on OnWN out of 89 submissions in total.
</bodyText>
<page confidence="0.99365">
237
</page>
<table confidence="0.999166333333333">
System head OnWN FNWN SMT mean rank
CNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30
CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33
CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36
UMBC-EB.-PW .7642 .7529 .5818 .3804 .6181 1
mean .6071 .5089 .2906 .3004 .4538 57
</table>
<tableCaption confidence="0.894515">
Table 2: STS challenge r and ranking results ranked ac-
cording to the mean r obtained. head is headlines and
mean is the mean of all submissions.
</tableCaption>
<bodyText confidence="0.981393666666667">
CNGL submissions perform unexpectedly low in the
FNWN task and only slightly better than the average
in the SMT task. The lower performance is likely to
be due to using an out-of-domain corpus for building
the RTM models and it may also be due to using and
optimizing a single model for all types of tasks.
</bodyText>
<subsectionHeader confidence="0.990942">
3.4 Bi-directional RTM Models
</subsectionHeader>
<bodyText confidence="0.999980066666667">
The STS task similarity score is directional invari-
ant: q(S1, S2) = q(S2, S1). We develop RTM mod-
els in the reverse direction and obtain bi-directional
RTM models by combining both. Table 3 lists the
bi-directional results on the STS challenge test set
after tuning, which shows that slight improvement in
the scores are possible when compared with Table 2.
Transductive learning improves the performance in
general. We also compare with the performance ob-
tained when combining uni-directional models with
mean, min, or max functions. Taking the minimum
performs better than other combination approaches
and can achieve r = 0.5129 with TL. One can also
take the individual confidence scores obtained for
each score when combining scores.
</bodyText>
<sectionHeader confidence="0.996455" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.998869">
Referential translation machines provide a clean
and intuitive computational model for automatically
measuring semantic similarity by measuring the acts
of translation involved and achieve to be the 15th on
some tasks and 30th overall in the STS challenge out
of 89 submissions in total. RTMs make quality and
semantic similarity judgments possible based on the
retrieval of relevant training data as interpretants for
reaching shared semantics.
</bodyText>
<table confidence="0.98433252">
System head OnWN FNWN SMT mean
.6552 .6943 .2016 .3005 .5086
.6397 .6808 .1776 .3147 .5028
.6512 .6947 .2003 .2984 .5066
.6416 .6853 .1903 .3143 .5055
.6669 .6680 .1867 .2737 .4958
.6493 .6805 .1846 .3127 .5059
.6388 .6695 .1667 .2999 .4938
.6285 .6686 .0918 .2931 .4816
.6510 .6971 .1179 .2861 .4961
.6524 .6918 .1940 .3176 .5121
.6608 .6953 .1704 .2922 .5053
.6509 .6864 .1792 .3156 .5084
.6588 .6800 .1355 .2868 .4961
.6493 .6805 .1846 .3127 .5059
.6251 .6843 .0677 .2994 .4845
.6370 .6978 .0951 .2980 .4936
.6517 .7136 .1002 .2880 .4996
.6383 .6841 .2434 .3063 .5059
.6615 .7099 .1644 .2877 .5072
.6606 .6987 .1972 .3059 .5129
.6589 .7019 .0995 .2935 .5008
.6362 .6896 .2044 .3153 .5063
.6300 .7011 .0817 .2798 .4850
.6321 .6956 .1995 .3128 .5052
</table>
<tableCaption confidence="0.99501">
Table 3: Bi-directional STS challenge r and ranking re-
</tableCaption>
<bodyText confidence="0.986793">
sults ranked according to the mean r obtained. We com-
bine the two directions by taking the mean, min, or the
max or use the bi-directional RTM model S1 +---I S2.
</bodyText>
<sectionHeader confidence="0.990766" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999871125">
This work is supported in part by SFI (07/CE/I1142)
as part of the Centre for Next Generation Locali-
sation (www.cngl.ie) at Dublin City University and
in part by the European Commission through the
QTLaunchPad FP7 project (No: 296347). We also
thank the SFI/HEA Irish Centre for High-End Com-
puting (ICHEC) for the provision of computational
facilities and support.
</bodyText>
<sectionHeader confidence="0.974657" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8305227">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics – Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385–393,
Montr´eal, Canada, 7-8 June. Association for Compu-
tational Linguistics.
</reference>
<figure confidence="0.9928726">
mean
mean TL
min
LS min TL
max
max TL
S1 tom- S2
S1 ;7-t S2 TL
mean
mean TL
min
LPS min TL
max
max TL
S1 S2
S1 ;-t S2 TL
mean
mean TL
min
RLPS
min TL
max
max TL
S1 S2
S1 ;7-t S2 TL
</figure>
<page confidence="0.969979">
238
</page>
<reference confidence="0.997803093457944">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics - Volume 1,
ACL ’98, pages 86–90, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics – Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435–440, Montr´eal, Canada, 7-8 June.
Association for Computational Linguistics.
Ergun Bic¸ici and Josef van Genabith. 2013. CNGL:
Grading student answers by acts of translation. In
*SEM 2013: The First Joint Conference on Lexical
and Computational Semantics and Proceedings of the
Seventh International Workshop on Semantic Evalua-
tion (SemEval 2013), Atlanta, Georgia, USA, 14-15
June. Association for Computational Linguistics.
Ergun Bic¸ici and Deniz Yuret. 2011a. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272–283, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic¸ici and Deniz Yuret. 2011b. RegMT system for
machine translation, system combination, and evalua-
tion. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 323–329, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic¸ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using ex-
trinsic and language independent features. Machine
Translation.
Ergun Bic¸ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc¸ University. Supervisor:
Deniz Yuret.
Ergun Bic¸ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multiagent
and Grid Systems.
Carl Hugo Bj¨ornsson. 1968. L¨asbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311,
June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10–
51, Montr´eal, Canada, June. Association for Compu-
tational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138–145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200–210, Montr´eal, Canada, June. Association
for Computational Linguistics.
Kenth Hagstr¨om. 2012. Swedish readability calcula-
tor. https://github.com/keha76/Swedish-Readability-
Calculator.
George A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–41,
November.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Sameer S. Pradhan, Eduard H. Hovy, Mitchell P. Mar-
cus, Martha Palmer, Lance A. Ramshaw, and Ralph M.
Weischedel. 2007. Ontonotes: a unified relational
semantic representation. Int. J. Semantic Computing,
1(4):405–419.
Yoav Seginer. 2007. Learning Syntactic Structure. Ph.D.
thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199–222, August.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
</reference>
<page confidence="0.979938">
239
</page>
<reference confidence="0.934803">
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, pages 173–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX.
</reference>
<page confidence="0.996859">
240
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.7930595">CNGL-CORE: Referential Translation for Measuring Semantic Similarity</title>
<affiliation confidence="0.9079385">Centre for Next Generation Dublin City University, Dublin,</affiliation>
<email confidence="0.979988">ebicici@computing.dcu.ie</email>
<author confidence="0.998108">Josef van</author>
<affiliation confidence="0.987202">Centre for Next Generation Dublin City University, Dublin,</affiliation>
<email confidence="0.992457">josef@computing.dcu.ie</email>
<abstract confidence="0.998659181818182">We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for judging the semantic similarity between text. RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics. An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of acts of translation involved. We view semantic similarity as paraphrasing between any two given texts. Each view is modeled by an RTM model, giving us a new perspective on the binary relationship between the two. Our prediction model is the 15th on some tasks and 30th overall out of 89 submissions in total according to the official results of the Semantic Textual Similarity (STS 2013) challenge. 1 Semantic Textual Similarity Judgments We introduce a fully automated judge for semantic similarity that performs well in the semantic textual similarity (STS) task (Agirre et al., 2013). STS is a degree of semantic equivalence between two texts based on the observations that “vehicle” and “car” are more similar than “wave” and “car”. Accurate prediction of STS has a wide application area including: identifying whether two tweets are talking about the same thing, whether an answer is correct by comparing it with a reference answer, and whether a given shorter text is a valid summary of another text. The translation quality estimation task (Callison- Burch et al., 2012) aims to develop quality indicators for translations at the sentence-level and predictors without access to a reference translation. Bicici et al. (2013) develop a top performing machine translation performance predictor (MTPP), which uses machine learning models over features measuring how well the test set matches the training set relying on extrinsic and language independent features. The semantic textual similarity (STS) task (Agirre et al., 2013) addresses the following problem. Given sentences the same language, quantify the degree of similarity with a similarity score, is a number in the range similarity prediction problem finda function the semantic textual score given two sentences, approach a supervised learning problem tuples being the training and the target similarity score. We model the problem as a translation task where one possible interpretation is obtained by translatsource to translate, S) to target translation, T). Since linguistic processing can reveal deeper similarity relationships, we also look at the translation task at different granularities of information: plain text (R for regular) , after lemmatization (L), after part-of-speech (POS) tagging (P), and removing stop-words (S) Thus,</abstract>
<address confidence="0.439518">234</address>
<note confidence="0.8947145">Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference the Shared pages 234–240, Atlanta, Georgia, June 13-14, 2013. Association for Computational Linguistics</note>
<abstract confidence="0.998210697297297">we obtain 4 different perspectives on the binary rebetween 2 Referential Translation Machine (RTM) Referential translation machines (RTMs) we develop provide a computational model for quality and semantic similarity judgments using retrieval of reltraining data and Yuret, 2011a; 2011) as interpretants for reaching shared seman- 2008). We show that RTM achieves very good performance in judging the semantic similarity of sentences and we can also use RTM to automatically assess the correctness of student answers to better results and van Genabith, 2013) than the state-of-the-art (Dzikovska et al., 2012). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. RTM can be used for automatically judging the semantic similarity between texts. An RTM model is based on the selection of common training data relevant and close to both the training set and the test set where the selected relevant set of instances are called the interpretants. Interpretants allow shared semantics to be possible by behaving as a reference point for similarity judgments and providing the context. In semian interpretant the signs used to to the real objects 2008). RTMs provide a model for computational semantics using interpretants as a reference according to which semantic judgments with translation acts are made. Each RTM model is a data translation model between the instances in the training set and the test set. We use the FDA (Feature Decay Algorithms) instance semodel for selecting the interpretants and Yuret, 2011a) from a given corpus, which can be monolingual when modeling paraphrasing acts, in which case the MTPP model (Section 2.1) is built using the interpretants themselves as both the source and the target side of the parallel corpus. RTMs map the training and test data to a space where translation acts can be identified. We view that acts of translation are ubiquitously used during communication: Every act of communication is an act 2012). src/backend/snowball/stopwords/ Translation need not be between different languages and paraphrasing or communication also contain acts of translation. When creating sentences, we use our background knowledge and translate information content according to the current context. a training set a test set and monolingual corpus preferably in the same domain as the training and test sets, the RTM steps are: T 2. I 3. 4. 5. M 6. q 2 selects the interpretants, relevant to the instances in the combined training and test data. 3, 4 use map a new space where similarities between translation acts can be derived more easily. Step 5 trains a learning the training features, and Step 6 obtains the predictions. RTM relies on the of a medium for building models for translating between Our encouraging results in the STS task provides a greater understanding of the acts of translation we ubiquitously use when communicating and how they can be used to predict the performance of translation, judging the semantic similarity between text, and evaluating the quality of student answers. RTM and MTPP models are not data or language specific and their modeling power and good performance are applicable across different domains and tasks. RTM expands the applicability of MTPP by making it feasible when making monolingual quality and similarity judgments and it enhances the computational scalability by building models over smaller but more relevant training data as interpretants. 2.1 The Machine Translation Performance Predictor (MTPP) In machine translation (MT), pairs of source and target sentences are used for training statistical MT (SMT) models. SMT system performance is affected by the amount of training data used as well 235 the the test set to the training set. et al., 2013) is a top performing machine translation performance predictor, which uses machine learning models over features measuring how well the test set matches the training set to predict the quality of a translation without using a reference translation. MTPP measures the coverage of individual test sentence features and syntactic structures found in the training set and derives feature functions measuring the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. 2.2 MTPP Features for Translation Acts uses features defined over text or common cover link (CCL) (Seginer, 2007) structures as the basic units of information over which similarity calculations are made. Unsupervised parsing with CCL extracts links from base words to head words, which allow us to obtain structures representing the grammatical information instantiated in the training and test data. Feature functions use statistics involving the training set and the test sentences to determine their closeness. Since they are language independent, MTPP allows quality estimation to be performed extrinsically. Categories for the 289 features used are listed below and their descriptions are presented in et al., where the number of features are given in Coverage the degree to which the test features are found in the trainset for both S and T Synthetic Translation Performance Calculates translation scores achievable according to coverage. Length the number of words and characters for S and T and their ratios. Feature Vector Similarity the similarities between vector representations. Perplexity the fluency of the sentences according to language models (LM). use both forward and backward LM based features for S and T. Entropy the distributional similarity of test sentences to the training set. Retrieval Closeness the degree to which sentences close to the test set are found in the training set. Diversity the diversity of cooccurring features in the training set. IBM1 Translation Probability the translation probability of test sentences using the training set (Brown et al., 1993). Minimum Bayes Retrieval Risk the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. Sentence Translation Performance Calculates translation scores obtained according to BLEU (Papineni et al., 2002), (Doddington, 2002), or and 2011b) for Character the cosine the character (for obtained for S and T (B¨ar et al., 2012). LIX the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for T. 3 Experiments STS contains sentence pairs from news headlines (headlines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT) (Agirre et al., 2013). STS challenge results are evaluated with Pearson’s correlation score test set contains sentence pairs 561, 189, from each respectively. The training set contains pairs with from MSRpar and MSRvid (Microsoft Research paraphrase and video corpus (Agirre et al., 2012)), and OnWN. 3.1 RTM Models We obtain CNGL results for the STS task as follows. For each perspective described in Section 1, we build an RTM model. Each RTM model views the STS task from a different perspective using the 289 features extracted dependent on the interpretants using MTPP. We extract the features both on C where A is the number of words, C is words longer than 6 characters, B is words that start or end with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012).</abstract>
<address confidence="0.236758">236</address>
<email confidence="0.607742">r</email>
<title confidence="0.935978545454546">R+P+L+S S L+S TL L+S L+P+S L+P+S TL R+P+L+S TL P L R L+P R+S R+P R+L R+P+S R+P+L RR SVR RR SVR RR SVR</title>
<phone confidence="0.514177166666667">7904 .7502 .8200 .7788 .8074 .8232 .8101 .8247 .8218 .8509 .8266 .8172 .8304 .8530 .8323 .8499 .8311 .8060 .8443 .8330 .8404 .8517 .8498 .8501 .8593 .8556 .8496 .8422 .8586 .8579 .8527 .8564 .7922 .7651 .8169 .7891 .8064 .8196 .8136 .8219 .8257 .8257 .8226 .8164 .8284 .8284 .8313 .8324 .8308 .8165 .8407 .8302 .8361 .8506 .8467 .8510 .8567 .8567 .8525 .8460 .8588 .8588 .8575 .8574 .8079 .787 .8279 .8101 .8216 .8333 .8275 .8346 .8375 .8409 .8361 .8312 .8412 .8434 .8432 .844 .8397 .8237 .8554 .841 .8432 .857 .851 .8557 .8605 .8626 .8505 .8505 .8591 .8622 .8602 .8588</phone>
<abstract confidence="0.995201305555555">1: CV performance on the training set with tuning. the settings we use in our submissions. RTM in directions and the bi-directional models displayed. the training set and the test set. The training corpus used is the English side of an out-of-domain corpus on European parliamentary discussions, Eu- (Callison-Burch et al., 2012) In-domain corpora are likely to improve the performance. We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the perspectives P and L. We use the traincorpus to build a target LM. We use ridge regression (RR) and support vector regression (SVR) with RBF kernel (Smola and Sch¨olkopf, 2004). Both of these models learn a regression function using the features to estimate a numerical target value. The parameters that govern the of RR and SVR are the regularization RR and the and for SVR. At testing time, the predictions are bounded to obtain in the range perform tuning on a subset of the training set separately for each RTM model and optimize against the performance evaluwith the coefficient of determination. We do not build a separate model for different types of sentences and instead use all of the training set for building a large prediction model. We also use transductive learning since using only the relevant training data for training can improve the 2011). Transductive learning is performed at the sentence level where for each test we select training instances using the cosine similarity metric over the feature vectors and build an individual model for the test instance and predict the similarity score. use WMT’13 corpora from www.statmt.org/wmt13/. 3.2 Training Results Table 1 lists the 10-fold cross-validation (CV) results on the training set for RR and SVR for different RTM systems using optimized parameters. As we combine different perspectives, the performance improves and we use the L+S with SVR for run 1 (LSSVR), L+P+S with SVR for run 2 (LPSSVR), and L+P+S with SVR using transductive learning for run 3 (LPSSVRTL) all in the translation direc- Lemmatized RTM, L, performs the best among the individual perspectives. We also RTM models in the direction which gives similar results. The last main row combines to obtain the bi-directional results, which improves the performance. Each additional adds another to the representation and the bi-directional results double the of features. Thus, is us- 3.3 STS Challenge Results 2 presents the STS challenge ranking results containing our CNGL submissions, the best system result, and the mean results over all submis- There were from competing systems (Agirre et al., 2013). The results are according to the mean We also include the mean result over all of the submissions and its corresponding rank. According to the official results, CNGL-LSSVR the system from the top based on the mean and CNGL-LPSSVR is according to results on OnWN out of in total. 237 System head OnWN FNWN SMT mean rank CNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30 CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33 CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36 UMBC-EB.-PW .7642 .7529 .5818 .3804 .6181 1 mean .6071 .5089 .2906 .3004 .4538 57 2: STS challenge ranking results ranked acto the mean head is headlines and mean is the mean of all submissions. CNGL submissions perform unexpectedly low in the FNWN task and only slightly better than the average in the SMT task. The lower performance is likely to be due to using an out-of-domain corpus for building the RTM models and it may also be due to using and optimizing a single model for all types of tasks. 3.4 Bi-directional RTM Models The STS task similarity score is directional invari- = develop RTM models in the reverse direction and obtain bi-directional RTM models by combining both. Table 3 lists the bi-directional results on the STS challenge test set after tuning, which shows that slight improvement in the scores are possible when compared with Table 2. Transductive learning improves the performance in general. We also compare with the performance obtained when combining uni-directional models with min, Taking the minimum performs better than other combination approaches can achieve TL. One can also take the individual confidence scores obtained for each score when combining scores. 4 Conclusion Referential translation machines provide a clean and intuitive computational model for automatically measuring semantic similarity by measuring the acts translation involved and achieve to be the on tasks and overall in the STS challenge out in total. RTMs make quality and semantic similarity judgments possible based on the retrieval of relevant training data as interpretants for reaching shared semantics. System head OnWN FNWN SMT mean</abstract>
<address confidence="0.530935347826087">6552 .6943 .2016 .3005 .5086 .6397 .6808 .1776 .3147 .5028 .6512 .6947 .2003 .2984 .5066 .6416 .6853 .1903 .3143 .5055 .6669 .6680 .1867 .2737 .4958 .6493 .6805 .1846 .3127 .5059 .6388 .6695 .1667 .2999 .4938 .6285 .6686 .0918 .2931 .4816 .6510 .6971 .1179 .2861 .4961 .6524 .6918 .1940 .3176 .5121 .6608 .6953 .1704 .2922 .5053 .6509 .6864 .1792 .3156 .5084 .6588 .6800 .1355 .2868 .4961 .6493 .6805 .1846 .3127 .5059 .6251 .6843 .0677 .2994 .4845 .6370 .6978 .0951 .2980 .4936 .6517 .7136 .1002 .2880 .4996 .6383 .6841 .2434 .3063 .5059 .6615 .7099 .1644 .2877 .5072 .6606 .6987 .1972 .3059 .5129 .6589 .7019 .0995 .2935 .5008 .6362 .6896 .2044 .3153 .5063 .6300 .7011 .0817 .2798 .4850</address>
<phone confidence="0.756575">6321 .6956 .1995 .3128 .5052</phone>
<abstract confidence="0.770009394736842">3: Bi-directional STS challenge ranking reranked according to the mean We combine the two directions by taking the mean, min, or the or use the bi-directional RTM model Acknowledgments This work is supported in part by SFI (07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie) at Dublin City University and in part by the European Commission through the QTLaunchPad FP7 project (No: 296347). We also thank the SFI/HEA Irish Centre for High-End Computing (ICHEC) for the provision of computational facilities and support. References Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A on semantic textual similarity. In 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Se- Evaluation (SemEval pages 385–393, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics. mean min max mean min max mean min RLPS max 238 Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity, including a pilot on</abstract>
<note confidence="0.948457923076923">In 2013: The Second Joint on Lexical and Computational Association for Computational Linguistics. Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The berkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International on Computational Linguistics - Volume ACL ’98, pages 86–90, Stroudsburg, PA, USA. Association for Computational Linguistics. Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity mea- In 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval pages 435–440, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics. and Josef van Genabith. 2013. CNGL: Grading student answers by acts of translation. In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics and Proceedings of the Seventh International Workshop on Semantic Evalua- (SemEval Atlanta, Georgia, USA, 14-15</note>
<abstract confidence="0.886770615384615">June. Association for Computational Linguistics. and Deniz Yuret. 2011a. Instance selection for machine translation using feature decay al- In of the Sixth Workshop on Machine pages 272–283, Edinburgh, Scotland, July. Association for Computational Linguistics. and Deniz Yuret. 2011b. RegMT system for machine translation, system combination, and evalua- In of the Sixth Workshop on Sta- Machine pages 323–329, Edinburgh, Scotland, July. Association for Computational Linguistics. Declan Groves, and Josef van Genabith. 2013. Predicting sentence translation quality using exand language independent features. 2011. Regression Model of Machine Ph.D. thesis, University. Supervisor: Deniz Yuret. 2008. Consensus ontologies in socially multiagent systems. of Multiagent Grid Hugo Bj¨ornsson. 1968. Liber. Bliss. 2012. Comedy is translation, February. chris bliss comedy is translation.html.</abstract>
<author confidence="0.787173">Peter F Brown</author>
<author confidence="0.787173">Stephen A Della Pietra</author>
<author confidence="0.787173">Vincent J Della</author>
<note confidence="0.868519955555555">Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter esti- 19(2):263–311, June. Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine In of the Seventh Workon Statistical Machine pages 10– 51, Montr´eal, Canada, June. Association for Computational Linguistics. George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence In of the second international conference on Human Language Technology pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris Brew. 2012. Towards effective tutorial feedback for explanation questions: A dataset and baselines. of the 2012 Conference of the North American Chapter of the Association for Computa- Linguistics: Human Language pages 200–210, Montr´eal, Canada, June. Association for Computational Linguistics. Kenth Hagstr¨om. 2012. Swedish readability calculator. https://github.com/keha76/Swedish-Readability- Calculator. George A. Miller. 1995. Wordnet: a lexical database for of the 38(11):39–41, November. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a method for automatic evalof machine translation. In of 40th Annual Meeting of the Association for Computational pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics. Sameer S. Pradhan, Eduard H. Hovy, Mitchell P. Marcus, Martha Palmer, Lance A. Ramshaw, and Ralph M. Weischedel. 2007. Ontonotes: a unified relational representation. J. Semantic 1(4):405–419. Seginer. 2007. Syntactic Ph.D. thesis, Universiteit van Amsterdam.</note>
<author confidence="0.587641">A tutorial</author>
<affiliation confidence="0.572812">support vector regression. and Comput-</affiliation>
<address confidence="0.689139">14(3):199–222, August.</address>
<author confidence="0.875583">Kristina Toutanova</author>
<author confidence="0.875583">Dan Klein</author>
<author confidence="0.875583">Christopher D Manning</author>
<abstract confidence="0.424085">and Yoram Singer. 2003. Feature-rich part-of-speech 239 with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linon Human Language Technology - Volume</abstract>
<address confidence="0.681782">NAACL ’03, pages 173–180, Stroudsburg, PA, USA.</address>
<note confidence="0.906981">Association for Computational Linguistics. Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX. 240</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="11770" citStr="Agirre et al., 2012" startWordPosition="1857" endWordPosition="1860">dlines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT) (Agirre et al., 2013). STS challenge results are evaluated with the Pearson’s correlation score (r). The test set contains 2250 (S1, S2) sentence pairs with 750, 561, 189, and 750 sentences from each type respectively. The training set contains 5342 sentence pairs with 1500 each from MSRpar and MSRvid (Microsoft Research paraphrase and video description corpus (Agirre et al., 2012)), 1592 from SMT, and 750 from OnWN. 3.1 RTM Models We obtain CNGL results for the STS task as follows. For each perspective described in Section 1, we build an RTM model. Each RTM model views the STS task from a different perspective using the 289 features extracted dependent on the interpretants using MTPP. We extract the features both on 2LIX= a + C 100, where A is the number of words, C is words longer than 6 characters, B is words that start or end with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012). 236 r R+P+L+S S L+S TL L+S L+P+S L+P+S TL R+P+L+S TL P L R L+P R+S R+P R+L R+P+S </context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385–393, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>shared task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</booktitle>
<publisher>SEM</publisher>
<contexts>
<context position="1536" citStr="Agirre et al., 2013" startWordPosition="225" endWordPosition="228">slating them, and the presence of acts of translation involved. We view semantic similarity as paraphrasing between any two given texts. Each view is modeled by an RTM model, giving us a new perspective on the binary relationship between the two. Our prediction model is the 15th on some tasks and 30th overall out of 89 submissions in total according to the official results of the Semantic Textual Similarity (STS 2013) challenge. 1 Semantic Textual Similarity Judgments We introduce a fully automated judge for semantic similarity that performs well in the semantic textual similarity (STS) task (Agirre et al., 2013). STS is a degree of semantic equivalence between two texts based on the observations that “vehicle” and “car” are more similar than “wave” and “car”. Accurate prediction of STS has a wide application area including: identifying whether two tweets are talking about the same thing, whether an answer is correct by comparing it with a reference answer, and whether a given shorter text is a valid summary of another text. The translation quality estimation task (CallisonBurch et al., 2012) aims to develop quality indicators for translations at the sentence-level and predictors without access to a r</context>
<context position="11407" citStr="Agirre et al., 2013" startWordPosition="1801" endWordPosition="1804">IST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 2 3 Experiments STS contains sentence pairs from news headlines (headlines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT) (Agirre et al., 2013). STS challenge results are evaluated with the Pearson’s correlation score (r). The test set contains 2250 (S1, S2) sentence pairs with 750, 561, 189, and 750 sentences from each type respectively. The training set contains 5342 sentence pairs with 1500 each from MSRpar and MSRvid (Microsoft Research paraphrase and video description corpus (Agirre et al., 2012)), 1592 from SMT, and 750 from OnWN. 3.1 RTM Models We obtain CNGL results for the STS task as follows. For each perspective described in Section 1, we build an RTM model. Each RTM model views the STS task from a different perspective us</context>
<context position="15924" citStr="Agirre et al., 2013" startWordPosition="2585" endWordPosition="2588">uild RTM models in the direction S2 → S1, which gives similar results. The last main row combines them to obtain the bi-directional results, S1 = S2, which improves the performance. Each additional perspective adds another 289 features to the representation and the bi-directional results double the number of features. Thus, S1 = S2 L+P+S is using 1734 features. 3.3 STS Challenge Results Table 2 presents the STS challenge r and ranking results containing our CNGL submissions, the best system result, and the mean results over all submissions. There were 89 submissions from 35 competing systems (Agirre et al., 2013). The results are ranked according to the mean r obtained. We also include the mean result over all of the submissions and its corresponding rank. According to the official results, CNGL-LSSVR is the 30th system from the top based on the mean r obtained and CNGL-LPSSVR is 15th according to the results on OnWN out of 89 submissions in total. 237 System head OnWN FNWN SMT mean rank CNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30 CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33 CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36 UMBC-EB.-PW .7642 .7529 .5818 .3804 .6181 1 mean .6071 .5089 .2906 .3004 .4538 57</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL ’98,</booktitle>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11329" citStr="Baker et al., 1998" startWordPosition="1790" endWordPosition="1793">on scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 2 3 Experiments STS contains sentence pairs from news headlines (headlines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT) (Agirre et al., 2013). STS challenge results are evaluated with the Pearson’s correlation score (r). The test set contains 2250 (S1, S2) sentence pairs with 750, 561, 189, and 750 sentences from each type respectively. The training set contains 5342 sentence pairs with 1500 each from MSRpar and MSRvid (Microsoft Research paraphrase and video description corpus (Agirre et al., 2012)), 1592 from SMT, and 750 from OnWN. 3.1 RTM Models We obtain CNGL results for the STS task as follows. For each perspective described in Section 1, we build a</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL ’98, pages 86–90, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>435--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 435–440, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Josef van Genabith</author>
</authors>
<title>CNGL: Grading student answers by acts of translation.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics and Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013),</booktitle>
<pages>14--15</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<marker>Bic¸ici, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici and Josef van Genabith. 2013. CNGL: Grading student answers by acts of translation. In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics and Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), Atlanta, Georgia, USA, 14-15 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>Instance selection for machine translation using feature decay algorithms.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>272--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Bic¸ici, Yuret, 2011</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2011a. Instance selection for machine translation using feature decay algorithms. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 272–283, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>RegMT system for machine translation, system combination, and evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>323--329</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Bic¸ici, Yuret, 2011</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2011b. RegMT system for machine translation, system combination, and evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 323–329, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Declan Groves</author>
<author>Josef van Genabith</author>
</authors>
<title>Predicting sentence translation quality using extrinsic and language independent features. Machine Translation.</title>
<date>2013</date>
<marker>Bic¸ici, Groves, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici, Declan Groves, and Josef van Genabith. 2013. Predicting sentence translation quality using extrinsic and language independent features. Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>The Regression Model of Machine Translation.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Koc¸ University. Supervisor: Deniz Yuret.</institution>
<marker>Bic¸ici, 2011</marker>
<rawString>Ergun Bic¸ici. 2011. The Regression Model of Machine Translation. Ph.D. thesis, Koc¸ University. Supervisor: Deniz Yuret.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>Consensus ontologies in socially interacting multiagent systems.</title>
<date>2008</date>
<journal>Journal of Multiagent and Grid Systems.</journal>
<marker>Bic¸ici, 2008</marker>
<rawString>Ergun Bic¸ici. 2008. Consensus ontologies in socially interacting multiagent systems. Journal of Multiagent and Grid Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Hugo Bj¨ornsson</author>
</authors>
<date>1968</date>
<note>L¨asbarhet. Liber.</note>
<marker>Bj¨ornsson, 1968</marker>
<rawString>Carl Hugo Bj¨ornsson. 1968. L¨asbarhet. Liber.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Bliss</author>
</authors>
<title>Comedy is translation, February. http://www.ted.com/talks/ chris bliss comedy is translation.html.</title>
<date>2012</date>
<contexts>
<context position="6000" citStr="Bliss, 2012" startWordPosition="940" endWordPosition="941">ing set and the test set. We use the FDA (Feature Decay Algorithms) instance selection model for selecting the interpretants (Bic¸ici and Yuret, 2011a) from a given corpus, which can be monolingual when modeling paraphrasing acts, in which case the MTPP model (Section 2.1) is built using the interpretants themselves as both the source and the target side of the parallel corpus. RTMs map the training and test data to a space where translation acts can be identified. We view that acts of translation are ubiquitously used during communication: Every act of communication is an act of translation (Bliss, 2012). src/backend/snowball/stopwords/ Translation need not be between different languages and paraphrasing or communication also contain acts of translation. When creating sentences, we use our background knowledge and translate information content according to the current context. Given a training set train, a test set test, and some monolingual corpus C, preferably in the same domain as the training and test sets, the RTM steps are: 1. T = train U test. 2. select(T, C) —* I 3. MTPP(I, train) —* Ttrain 4. MTPP(I, test) —* Ttest 5. learn(M, Ttrain) — M 6. predict(M,Ttest) —* q Step 2 selects the i</context>
</contexts>
<marker>Bliss, 2012</marker>
<rawString>Chris Bliss. 2012. Comedy is translation, February. http://www.ted.com/talks/ chris bliss comedy is translation.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="10483" citStr="Brown et al., 1993" startWordPosition="1660" endWordPosition="1663">sentations. • Perplexity {90}: Measures the fluency of the sentences according to language models (LM). We use both forward ({30}) and backward ({15}) LM based features for S and T. • Entropy {4}: Calculates the distributional similarity of test sentences to the training set. • Retrieval Closeness {24}: Measures the degree to which sentences close to the test set are found in the training set. • Diversity {6}: Measures the diversity of cooccurring features in the training set. • IBM1 Translation Probability {16}: Calculates the translation probability of test sentences using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 2</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="13388" citStr="Callison-Burch et al., 2012" startWordPosition="2156" endWordPosition="2159">67 .8567 .8525 .8460 .8588 .8588 .8575 .8574 .8079 .787 .8279 .8101 .8216 .8333 .8275 .8346 .8375 .8409 .8361 .8312 .8412 .8434 .8432 .844 .8397 .8237 .8554 .841 .8432 .857 .851 .8557 .8605 .8626 .8505 .8505 .8591 .8622 .8602 .8588 Table 1: CV performance on the training set with tuning. Underlined are the settings we use in our submissions. RTM models in directions S1 → S2, S2 → S1, and the bi-directional models S1 +---I S2 are displayed. the training set and the test set. The training corpus used is the English side of an out-of-domain corpus on European parliamentary discussions, Europarl (Callison-Burch et al., 2012) 3. In-domain corpora are likely to improve the performance. We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the perspectives P and L. We use the training corpus to build a 5-gram target LM. We use ridge regression (RR) and support vector regression (SVR) with RBF kernel (Smola and Sch¨olkopf, 2004). Both of these models learn a regression function using the features to estimate a numerical target value. The parameters that govern the behavior of RR and SVR are the regularization A for RR and the C, c, and -y parameters for SVR. At testing time, the predictions are bounded to</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10– 51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="10809" citStr="Doddington, 2002" startWordPosition="1709" endWordPosition="1710">which sentences close to the test set are found in the training set. • Diversity {6}: Measures the diversity of cooccurring features in the training set. • IBM1 Translation Probability {16}: Calculates the translation probability of test sentences using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 2 3 Experiments STS contains sentence pairs from news headlines (headlines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT) (Agirre et al., 2013). </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Rodney D Nielsen</author>
<author>Chris Brew</author>
</authors>
<title>Towards effective tutorial feedback for explanation questions: A dataset and baselines.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>200--210</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="4453" citStr="Dzikovska et al., 2012" startWordPosition="677" endWordPosition="680">p between S1 and S2. 2 Referential Translation Machine (RTM) Referential translation machines (RTMs) we develop provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data (Bic¸ici and Yuret, 2011a; Bic¸ici, 2011) as interpretants for reaching shared semantics (Bic¸ici, 2008). We show that RTM achieves very good performance in judging the semantic similarity of sentences and we can also use RTM to automatically assess the correctness of student answers to obtain better results (Bic¸ici and van Genabith, 2013) than the state-of-the-art (Dzikovska et al., 2012). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. RTM can be used for automatically judging the semantic similarity between texts. An RTM model is based on the selection of common training data relevant and close to both the training set and the test set where the selected relevant set of instances are called the interpretants. Interpretants allow shared semantics to be possible by behaving as a reference point for similarity judgments and providing the context. </context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, 2012</marker>
<rawString>Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris Brew. 2012. Towards effective tutorial feedback for explanation questions: A dataset and baselines. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200–210, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenth Hagstr¨om</author>
</authors>
<date>2012</date>
<note>Swedish readability calculator. https://github.com/keha76/Swedish-ReadabilityCalculator.</note>
<marker>Hagstr¨om, 2012</marker>
<rawString>Kenth Hagstr¨om. 2012. Swedish readability calculator. https://github.com/keha76/Swedish-ReadabilityCalculator.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="11282" citStr="Miller, 1995" startWordPosition="1783" endWordPosition="1784">ion Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 2 3 Experiments STS contains sentence pairs from news headlines (headlines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT) (Agirre et al., 2013). STS challenge results are evaluated with the Pearson’s correlation score (r). The test set contains 2250 (S1, S2) sentence pairs with 750, 561, 189, and 750 sentences from each type respectively. The training set contains 5342 sentence pairs with 1500 each from MSRpar and MSRvid (Microsoft Research paraphrase and video description corpus (Agirre et al., 2012)), 1592 from SMT, and 750 from OnWN. 3.1 RTM Models We obtain CNGL results for the STS task as follows. For each</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="10784" citStr="Papineni et al., 2002" startWordPosition="1704" endWordPosition="1707"> {24}: Measures the degree to which sentences close to the test set are found in the training set. • Diversity {6}: Measures the diversity of cooccurring features in the training set. • IBM1 Translation Probability {16}: Calculates the translation probability of test sentences using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 2 3 Experiments STS contains sentence pairs from news headlines (headlines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Eduard H Hovy</author>
<author>Mitchell P Marcus</author>
<author>Martha Palmer</author>
<author>Lance A Ramshaw</author>
<author>Ralph M Weischedel</author>
</authors>
<title>Ontonotes: a unified relational semantic representation.</title>
<date>2007</date>
<journal>Int. J. Semantic Computing,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="11255" citStr="Pradhan et al., 2007" startWordPosition="1777" endWordPosition="1780">ning instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 2 3 Experiments STS contains sentence pairs from news headlines (headlines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT) (Agirre et al., 2013). STS challenge results are evaluated with the Pearson’s correlation score (r). The test set contains 2250 (S1, S2) sentence pairs with 750, 561, 189, and 750 sentences from each type respectively. The training set contains 5342 sentence pairs with 1500 each from MSRpar and MSRvid (Microsoft Research paraphrase and video description corpus (Agirre et al., 2012)), 1592 from SMT, and 750 from OnWN. 3.1 RTM Models We obtain CNGL results for the ST</context>
</contexts>
<marker>Pradhan, Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2007</marker>
<rawString>Sameer S. Pradhan, Eduard H. Hovy, Mitchell P. Marcus, Martha Palmer, Lance A. Ramshaw, and Ralph M. Weischedel. 2007. Ontonotes: a unified relational semantic representation. Int. J. Semantic Computing, 1(4):405–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Learning Syntactic Structure.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Universiteit van Amsterdam.</institution>
<contexts>
<context position="8776" citStr="Seginer, 2007" startWordPosition="1390" endWordPosition="1391">ing models over features measuring how well the test set matches the training set to predict the quality of a translation without using a reference translation. MTPP measures the coverage of individual test sentence features and syntactic structures found in the training set and derives feature functions measuring the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. 2.2 MTPP Features for Translation Acts MTPP uses n-gram features defined over text or common cover link (CCL) (Seginer, 2007) structures as the basic units of information over which similarity calculations are made. Unsupervised parsing with CCL extracts links from base words to head words, which allow us to obtain structures representing the grammatical information instantiated in the training and test data. Feature functions use statistics involving the training set and the test sentences to determine their closeness. Since they are language independent, MTPP allows quality estimation to be performed extrinsically. Categories for the 289 features used are listed below and their detailed descriptions are presented </context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Learning Syntactic Structure. Ph.D. thesis, Universiteit van Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>2004</date>
<journal>Statistics and Computing,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>Smola, Sch¨olkopf, 2004</marker>
<rawString>Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial on support vector regression. Statistics and Computing, 14(3):199–222, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13504" citStr="Toutanova et al., 2003" startWordPosition="2175" endWordPosition="2178"> .8434 .8432 .844 .8397 .8237 .8554 .841 .8432 .857 .851 .8557 .8605 .8626 .8505 .8505 .8591 .8622 .8602 .8588 Table 1: CV performance on the training set with tuning. Underlined are the settings we use in our submissions. RTM models in directions S1 → S2, S2 → S1, and the bi-directional models S1 +---I S2 are displayed. the training set and the test set. The training corpus used is the English side of an out-of-domain corpus on European parliamentary discussions, Europarl (Callison-Burch et al., 2012) 3. In-domain corpora are likely to improve the performance. We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the perspectives P and L. We use the training corpus to build a 5-gram target LM. We use ridge regression (RR) and support vector regression (SVR) with RBF kernel (Smola and Sch¨olkopf, 2004). Both of these models learn a regression function using the features to estimate a numerical target value. The parameters that govern the behavior of RR and SVR are the regularization A for RR and the C, c, and -y parameters for SVR. At testing time, the predictions are bounded to obtain scores in the range [0, 5]. We perform tuning on a subset of the training set separately for each RTM model </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<date>2013</date>
<note>Lix. http://en.wikipedia.org/wiki/LIX.</note>
<contexts>
<context position="11050" citStr="Wikipedia, 2013" startWordPosition="1749" endWordPosition="1750">nces using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 2 3 Experiments STS contains sentence pairs from news headlines (headlines), sense definitions from semantic lexical resources (OnWN is from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995) and FNWN is from FrameNet (Baker et al., 1998) and WordNet), and statistical machine translation (SMT) (Agirre et al., 2013). STS challenge results are evaluated with the Pearson’s correlation score (r). The test set contains 2250 (S1, S2) sentence pairs with 750, 561, 189, and 750 sentences from each type respectively. The training set contains 5342 sentence pairs</context>
</contexts>
<marker>Wikipedia, 2013</marker>
<rawString>Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>