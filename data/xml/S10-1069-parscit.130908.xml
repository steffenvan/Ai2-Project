<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013817">
<title confidence="0.929986">
SCHWA: PETE using CCG Dependencies with the C&amp;C Parser
</title>
<author confidence="0.99169">
Dominick Ng, James W. D. Constable, Matthew Honnibal and James R. Curran
</author>
<affiliation confidence="0.9798325">
-lab, School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.430604">
NSW 2006, Australia
</address>
<email confidence="0.996736">
{dong7223,jcon6353,mhonn,james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.997356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965722222222">
This paper describes the SCHWA system
entered by the University of Sydney in Se-
mEval 2010 Task 12 – Parser Evaluation
using Textual Entailments (Yuret et al.,
2010). Our system achieved an overall ac-
curacy of 70% in the task evaluation.
We used the C&amp;C parser to build CCG de-
pendency parses of the truth and hypothe-
sis sentences. We then used partial match
heuristics to determine whether the sys-
tem should predict entailment. Heuristics
were used because the dependencies gen-
erated by the parser are construction spe-
cific, making full compatibility unlikely.
We also manually annotated the develop-
ment set with CCG analyses, establishing
an upper bound for our entailment system
of 87%.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999886762711865">
The SemEval 2010 Parser Evaluation using Tex-
tual Entailments (PETE) task attempts to address
the long-standing problems in parser evaluation
caused by the diversity of syntactic formalisms
and analyses in use. The task investigates the
feasibility of a minimalist extrinsic evaluation –
that of detecting textual entailment between a truth
sentence and a hypothesis sentence. It is extrin-
sic in the sense that it evaluates parsers on a task,
rather than a direct comparison of their output
against some gold standard. However, it requires
only minimal task-specific logic, and the proposed
entailments are designed to be inferrable based on
syntactic information alone.
Our system used the C&amp;C parser (Clark and
Curran, 2007a), which uses the Combinatory Cat-
egorial Grammar formalism (CCG, Steedman,
2000). We used the CCGbank-style dependency
output of the parser (Hockenmaier and Steedman,
2007), which is a directed graph of head-child re-
lations labelled with the head’s lexical category
and the argument slot filled by the child.
We divided the dependency graphs of the truth
and hypothesis sentences into predicates that con-
sisted of a head word and its immediate children.
For instance, the parser’s analysis of the sentence
Totals include only vehicle sales reported in pe-
riod might produce predicates like include(Totals,
sales), only(include), and reported(sales). If at
least one such predicate matches in the two parses,
we predict entailment. We consider a single pred-
icate match sufficient for entailment because the
lexical categories and slots that constitute our de-
pendency labels are often different in the hypothe-
sis sentence due to the generation process used in
the task.
The single predicate heuristic gives us an over-
all accuracy of 70% on the test set. Our precision
and recall over the test set was 68% and 80% re-
spectively giving an F-score of 74%.
To investigate how many of the errors were due
to parse failures, and how many were failures of
our entailment recognition process, we manually
annotated the 66 development truth sentences with
gold standard CCG derivations. This established
an upper bound of 87% F-score for our approach.
This upper bound suggests that there is still
work to be done before the system allows trans-
parent evaluation of the parser. However, cross-
framework parser evaluation is a difficult problem:
previous attempts to evaluate the C&amp;C parser on
grammatical relations (Clark and Curran, 2007b)
and Penn Treebank-trees (Clark and Curran, 2009)
have also produced upper bounds between 80 and
90% F-score. Our PETE system was much easier
to produce than either of these previous attempts
at cross-framework parser evaluation, suggesting
that this may be a promising approach to a diffi-
cult problem.
</bodyText>
<page confidence="0.993217">
313
</page>
<note confidence="0.5626205">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 313–316,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.987026">
Totals include only vehicle sales reported in period.
NP (S\NP)/NP (S\NP)\(S\NP) N/N N S\NP ((S\NP)\(S\NP))/NP NP
&lt;B× &gt; &gt;
(S\NP)/NP N ⇒ NP
S\NP ⇒ NP\NP
&lt;
&gt;
&lt;
NP
S\NP
S
</figure>
<figureCaption confidence="0.997335">
Figure 1: An example CCG derivation, showing how the categories assigned to words are combined to
form a sentence. The arrows indicate the direction of application.
</figureCaption>
<figure confidence="0.636597">
(S\NP)\(S\NP)
&lt;
</figure>
<sectionHeader confidence="0.966894" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9995828125">
Combinatory Categorial Grammar (CCG, Steed-
man, 2000) is a lexicalised grammar formalism
based on combinatory logic. The grammar is di-
rectly encoded in the lexicon in the form of combi-
natory categories that govern how each word com-
bines with its neighbours. The parsing process de-
termines the most likely assignment of categories
to words, and finds a sequence of combinators that
allows them to form a sentence.
A sample CCG derivation for a sentence from
the test set is shown in Figure 1. The category for
each word is indicated beneath it. It can be seen
that some categories take other categories as ar-
guments; each argument slot in a category is num-
bered based on the order of application, from latest
to earliest. For example:
</bodyText>
<equation confidence="0.401746">
((S/NP1)/(S/NP)2)\NP3
</equation>
<bodyText confidence="0.99769475">
Figure 2 shows how the argument slots are
mapped to dependencies. The first two columns
list the predicate words and their categories, while
the second two show how each argument slot is
filled. For example, in the first row, only has the
category (S\NP)\(S\NP), with argument slot
1 filled by include). It is these dependencies that
form the basis for our predicates in this task.
</bodyText>
<table confidence="0.994828857142857">
only (S\NP)\(S\NP) 1 include
vehicle N/N 1 sales
in ((S\NP)\(S\NP))/NP 2 period
in ((S\NP)\(S\NP))/NP 1 reported
reported S\NP 1 sales
include (S\NP)/NP 2 sales
include (S\NP)/NP 1 Totals
</table>
<figureCaption confidence="0.991498">
Figure 2: The dependencies represented by the
derivation in Figure 1.
</figureCaption>
<bodyText confidence="0.99901445">
Recent work has seen the development of high-
performance parsers built on the CCG formalism.
Clark and Curran (2007a) demonstrate the use of
techniques like adaptive supertagging, parallelisa-
tion and a dynamic-programming chart parsing al-
gorithm to implement the C&amp;C parser, a highly
efficient CCG parser that performs well against
parsers built on different formalisms (Rimell et al.,
2009). We use this parser for the PETE task.
The performance of statistical parsers is largely
a function of the quality of the corpora they are
trained on. For this task, we used models derived
from the CCGbank corpus – a transformation of
the Penn Treebank (Marcus et al., 1993) including
CCG derivations and dependencies (Hockenmaier,
2003a). It was created to further CCG research
by providing a large corpus of appropriately anno-
tated data, and has been shown to be suitable for
the training of high-performance parsers (Hocken-
maier, 2003b; Clark and Curran, 2004).
</bodyText>
<sectionHeader confidence="0.995773" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999897705882353">
Our system used the C&amp;C parser to parse the truth
and hypothesis sentences. We took the dependen-
cies generated by the parser and processed these to
generate predicates encoding the canonical form
of the head word, its required arguments, and their
order. We then attempted to unify the predicates
from the hypothesis sentence with the predicates
in the truth sentence. A successful unification of
predicates a and b occurs when the head words of
a and b are identical and their argument slots are
also identical. If any predicate from the hypothe-
sis sentence unified with a predicate from the truth
sentence, our system returned YES, otherwise the
system returned NO.
We used the 66 sentence development set to
tune our approach. While analysing the hypoth-
esis sentences, we noticed that many examples re-
</bodyText>
<page confidence="0.995016">
314
</page>
<table confidence="0.999315166666667">
System YES entailment NO entailment Overall F-score
correct incorrect A (%) correct incorrect A (%) accuracy (%)
SCHWA 125 31 80 87 58 60 70 74
median 71 85 46 88 57 61 53 50
baseline 156 0 100 0 145 0 52 68
low 68 88 44 76 69 52 48 46
</table>
<tableCaption confidence="0.97758">
Table 1: Final results over the test set
</tableCaption>
<table confidence="0.9989385">
System YES entailment NO entailment Overall F-score
correct incorrect A (%) correct incorrect A (%) accuracy (%)
Gold deps 34 6 85 22 4 90 87 87
Parsed deps 32 8 80 20 6 77 79 82
</table>
<tableCaption confidence="0.999801">
Table 2: Results over the development set
</tableCaption>
<bodyText confidence="0.991898517241379">
placed nouns from the truth sentence with indefi-
nite pronouns such as someone or something (e.g.
Someone bought something). In most of these cases
the indefinite would not be present in the truth sen-
tence at all, so to deal with this we converted in-
definite pronouns into wildcard markers that could
be matched to any argument. We also incorporated
sensitivity to passive sentences by adjusting the ar-
gument numbers of dependents.
In its most naive form our system is heavily
biased towards excellent recall but poor preci-
sion. We evaluated a number of heuristics to prune
the predicate space and selected those which im-
proved the performance over the development set.
Our final system used the part-of-speech tags gen-
erated by the parser to remove predicates headed
by determiners, prepositions and adjectives. We
note that even after predicate pruning our system
is still likely to return better recall performance
than precision, but this discrepancy was masked in
part by the nature of the development set: most hy-
potheses are short and so the potential number of
predicates after pruning is likely to be small. The
final predicates generated by the system for the ex-
ample derivation given in Figure 1 after heuristic
pruning are:
only(include)
reported(sales)
include(totals, sales)
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.998473705882353">
We report results over the 301 sentence test set in
Table 1. Our overall accuracy was 70%, and per-
formance over YES entailments was roughly 20%
higher than accuracy over NO entailments. This
bias towards YES entailments is a reflection of our
single match heuristic that only required one pred-
icate match before answering YES. Our system
performed nearly 20% better than the baseline sys-
tem (all YES responses) and placed second overall
in the task evaluation.
Table 2 shows our results over the development
corpus. The 17% drop in accuracy and 8% drop in
F-score between the development data and the test
data suggests that our heuristics may have over-
fitted to the limited development data. More so-
phisticated heuristics over a larger corpus would
be useful for further fine-tuning our system.
</bodyText>
<subsectionHeader confidence="0.990756">
4.1 Results with Gold Standard Parses
</subsectionHeader>
<bodyText confidence="0.99947045">
Our entailment system’s errors could be broadly
divided into two classes: those due to incorrect
parses, and those due to incorrect comparison of
the parses. To investigate the relative contribu-
tions of these two classes of errors, we manually
annotated the 66 development sentences with CCG
derivations. This allowed us to evaluate our sys-
tem using gold standard parses. Only one anno-
tator was available, so we were unable to calcu-
late inter-annotator agreement scores to examine
the quality of our annotations.
The annotation was prepared with the annota-
tion tool used by Honnibal et al. (2009). The tool
presents the user with a CCG derivation produced
by the C&amp;C parser. The user can then correct the
lexical categories, or add bracket constraints to the
parser using the algorithm described by Djordjevic
and Curran (2006), and reparse the sentence until
the derivation desired is produced.
Our results with gold standard dependencies are
</bodyText>
<page confidence="0.998336">
315
</page>
<bodyText confidence="0.99984425">
shown in Table 2. The accuracy is 87%, establish-
ing a fairly low upper bound for our approach to
the task. Manual inspection of the remaining er-
rors showed that some were due to incorrect parses
for the hypothesis sentence, and some were due to
entailments which the parser’s dependency anal-
yses could not resolve, such as They ate whole
steamed grains ⇒ The grains were steamed. The
largest source of errors was our matching heuris-
tics, suggesting that our approach to the task must
be improved before it can be considered a trans-
parent evaluation of the parser.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999971736842105">
We constructed a system to evaluate the C&amp;C
parser using textual entailments. We converted the
parser output into a set of predicate structures and
used these to establish the presence of entailment.
Our system achieved an overall accuracy of 79%
on the development set and 70% over the test set.
The gap between our development and test accu-
racies suggests our heuristics may have been over-
fitted to the development data.
Our investigation using gold-standard depen-
dencies established an upper bound of 87% on
the development set for our approach to the task.
While this is not ideal, we note that previous ef-
forts at cross-parser evaluation have shown that it
is a difficult problem (Clark and Curran (2007b)
and Clark and Curran (2009)). We conclue that
the concept of a minimal extrinsic evaluation put
forward in this task is a promising avenue for
formalism-independent parser comparison.
</bodyText>
<sectionHeader confidence="0.988188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9477985625">
Stephen Clark and James R. Curran. Parsing the
WSJ using CCG and log-linear models. In Pro-
ceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages
104–111, 2004.
Stephen Clark and James R. Curran. Wide-
Coverage Efficient Statistical Parsing with CCG
and Log-Linear Models. Computational Lin-
guistics, 33(4):493–552, 2007a.
Stephen Clark and James R. Curran. Formalism-
independent parser evaluation with CCG and
DepBank. In Proceedings of the 45th Annual
Meeting of the Association for Computational
Linguistics, pages 248–255, Prague, Czech Re-
public, 25–27 June 2007b.
Stephen Clark and James R. Curran. Compar-
ing the accuracy of CCG and Penn Treebank
Parsers. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 53–56,
Suntec, Singapore, August 2009.
Bojan Djordjevic and James R. Curran. Faster
wide-coverage CCG parsing. In Proceedings of
the Australasian Language Technology Work-
shop 2006, pages 3–10, Sydney, Australia, De-
cember 2006.
Julia Hockenmaier. Data and models for sta-
tistical parsing with Combinatory Categorial
Grammar. PhD thesis, 2003a.
Julia Hockenmaier. Parsing with generative mod-
els of predicate-argument structure. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 359–
366. Association for Computational Linguistics
Morristown, NJ, USA, 2003b.
Julia Hockenmaier and Mark Steedman. CCG-
bank: a corpus of CCG derivations and depen-
dency structures extracted from the Penn Tree-
bank. Computational Linguistics, 33(3):355–
396, 2007.
Matthew Honnibal, Joel Nothman, and James R.
Curran. Evaluating a Statistical CCG Parser on
Wikipedia. In Proceedings of the 2009 Work-
shop on The People’s Web Meets NLP: Collabo-
ratively Constructed Semantic Resources, pages
38–41, Singapore, August 2009.
Mitchell P. Marcus, Mary Ann Marcinkiewicz,
and Beatrice Santorini. Building a large an-
notated corpus of English: The Penn Tree-
bank. Computational Linguistics, 19(2):313–
330, 1993.
Laura Rimell, Stephen Clark, and Mark Steedman.
Unbounded Dependency Recovery for Parser
Evaluation. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, volume 2, pages 813–821,
2009.
Mark Steedman. The Syntactic Process. MIT
Press, Massachusetts Institute of Technology,
USA, 2000.
Deniz Yuret, Aydın Han, and Zehra Turgut.
SemEval-2010 Task 12: Parser Evaluation us-
ing Textual Entailments. In Proceedings of the
SemEval-2010 Evaluation Exercises on Seman-
tic Evaluation, 2010.
</reference>
<page confidence="0.999274">
316
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.906470">
<title confidence="0.988023">with the</title>
<author confidence="0.997883">James W D Constable Ng</author>
<author confidence="0.997883">Matthew Honnibal R Curran</author>
<affiliation confidence="0.992031">lab, School of Information Technologies University of Sydney</affiliation>
<address confidence="0.983863">NSW 2006, Australia</address>
<abstract confidence="0.997258894736842">paper describes the entered by the University of Sydney in SemEval 2010 Task 12 – Parser Evaluation using Textual Entailments (Yuret et al., 2010). Our system achieved an overall accuracy of 70% in the task evaluation. used the to build dependency parses of the truth and hypothesis sentences. We then used partial match heuristics to determine whether the system should predict entailment. Heuristics were used because the dependencies generated by the parser are construction specific, making full compatibility unlikely. We also manually annotated the developset with establishing an upper bound for our entailment system of 87%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="6615" citStr="Clark and Curran, 2004" startWordPosition="1052" endWordPosition="1055">parsers built on different formalisms (Rimell et al., 2009). We use this parser for the PETE task. The performance of statistical parsers is largely a function of the quality of the corpora they are trained on. For this task, we used models derived from the CCGbank corpus – a transformation of the Penn Treebank (Marcus et al., 1993) including CCG derivations and dependencies (Hockenmaier, 2003a). It was created to further CCG research by providing a large corpus of appropriately annotated data, and has been shown to be suitable for the training of high-performance parsers (Hockenmaier, 2003b; Clark and Curran, 2004). 3 Method Our system used the C&amp;C parser to parse the truth and hypothesis sentences. We took the dependencies generated by the parser and processed these to generate predicates encoding the canonical form of the head word, its required arguments, and their order. We then attempted to unify the predicates from the hypothesis sentence with the predicates in the truth sentence. A successful unification of predicates a and b occurs when the head words of a and b are identical and their argument slots are also identical. If any predicate from the hypothesis sentence unified with a predicate from </context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 104–111, 2004.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models.</title>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<pages>2007</pages>
<marker>Clark, Curran, </marker>
<rawString>Stephen Clark and James R. Curran. WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Linguistics, 33(4):493–552, 2007a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Formalismindependent parser evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>248--255</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1693" citStr="Clark and Curran, 2007" startWordPosition="258" endWordPosition="261">standing problems in parser evaluation caused by the diversity of syntactic formalisms and analyses in use. The task investigates the feasibility of a minimalist extrinsic evaluation – that of detecting textual entailment between a truth sentence and a hypothesis sentence. It is extrinsic in the sense that it evaluates parsers on a task, rather than a direct comparison of their output against some gold standard. However, it requires only minimal task-specific logic, and the proposed entailments are designed to be inferrable based on syntactic information alone. Our system used the C&amp;C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). We used the CCGbank-style dependency output of the parser (Hockenmaier and Steedman, 2007), which is a directed graph of head-child relations labelled with the head’s lexical category and the argument slot filled by the child. We divided the dependency graphs of the truth and hypothesis sentences into predicates that consisted of a head word and its immediate children. For instance, the parser’s analysis of the sentence Totals include only vehicle sales reported in period might produce predicates like include(To</context>
<context position="3417" citStr="Clark and Curran, 2007" startWordPosition="534" endWordPosition="537"> and 80% respectively giving an F-score of 74%. To investigate how many of the errors were due to parse failures, and how many were failures of our entailment recognition process, we manually annotated the 66 development truth sentences with gold standard CCG derivations. This established an upper bound of 87% F-score for our approach. This upper bound suggests that there is still work to be done before the system allows transparent evaluation of the parser. However, crossframework parser evaluation is a difficult problem: previous attempts to evaluate the C&amp;C parser on grammatical relations (Clark and Curran, 2007b) and Penn Treebank-trees (Clark and Curran, 2009) have also produced upper bounds between 80 and 90% F-score. Our PETE system was much easier to produce than either of these previous attempts at cross-framework parser evaluation, suggesting that this may be a promising approach to a difficult problem. 313 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 313–316, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Totals include only vehicle sales reported in period. NP (S\NP)/NP (S\NP)\(S\NP) N/N N S\NP ((S\NP)\(S\NP))/NP NP</context>
<context position="5775" citStr="Clark and Curran (2007" startWordPosition="919" endWordPosition="922">s, while the second two show how each argument slot is filled. For example, in the first row, only has the category (S\NP)\(S\NP), with argument slot 1 filled by include). It is these dependencies that form the basis for our predicates in this task. only (S\NP)\(S\NP) 1 include vehicle N/N 1 sales in ((S\NP)\(S\NP))/NP 2 period in ((S\NP)\(S\NP))/NP 1 reported reported S\NP 1 sales include (S\NP)/NP 2 sales include (S\NP)/NP 1 Totals Figure 2: The dependencies represented by the derivation in Figure 1. Recent work has seen the development of highperformance parsers built on the CCG formalism. Clark and Curran (2007a) demonstrate the use of techniques like adaptive supertagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C&amp;C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al., 2009). We use this parser for the PETE task. The performance of statistical parsers is largely a function of the quality of the corpora they are trained on. For this task, we used models derived from the CCGbank corpus – a transformation of the Penn Treebank (Marcus et al., 1993) including CCG derivations and dependencies (Hock</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. Formalismindependent parser evaluation with CCG and DepBank. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 248–255, Prague, Czech Republic, 25–27 June 2007b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Comparing the accuracy of CCG and Penn Treebank Parsers.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>53--56</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="3468" citStr="Clark and Curran, 2009" startWordPosition="541" endWordPosition="544"> investigate how many of the errors were due to parse failures, and how many were failures of our entailment recognition process, we manually annotated the 66 development truth sentences with gold standard CCG derivations. This established an upper bound of 87% F-score for our approach. This upper bound suggests that there is still work to be done before the system allows transparent evaluation of the parser. However, crossframework parser evaluation is a difficult problem: previous attempts to evaluate the C&amp;C parser on grammatical relations (Clark and Curran, 2007b) and Penn Treebank-trees (Clark and Curran, 2009) have also produced upper bounds between 80 and 90% F-score. Our PETE system was much easier to produce than either of these previous attempts at cross-framework parser evaluation, suggesting that this may be a promising approach to a difficult problem. 313 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 313–316, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Totals include only vehicle sales reported in period. NP (S\NP)/NP (S\NP)\(S\NP) N/N N S\NP ((S\NP)\(S\NP))/NP NP &lt;B× &gt; &gt; (S\NP)/NP N ⇒ NP S\NP ⇒ NP\NP &lt; &gt; &lt; NP S\N</context>
</contexts>
<marker>Clark, Curran, 2009</marker>
<rawString>Stephen Clark and James R. Curran. Comparing the accuracy of CCG and Penn Treebank Parsers. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 53–56, Suntec, Singapore, August 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bojan Djordjevic</author>
<author>James R Curran</author>
</authors>
<title>Faster wide-coverage CCG parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop</booktitle>
<pages>3--10</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="10891" citStr="Djordjevic and Curran (2006)" startWordPosition="1779" endWordPosition="1782"> of these two classes of errors, we manually annotated the 66 development sentences with CCG derivations. This allowed us to evaluate our system using gold standard parses. Only one annotator was available, so we were unable to calculate inter-annotator agreement scores to examine the quality of our annotations. The annotation was prepared with the annotation tool used by Honnibal et al. (2009). The tool presents the user with a CCG derivation produced by the C&amp;C parser. The user can then correct the lexical categories, or add bracket constraints to the parser using the algorithm described by Djordjevic and Curran (2006), and reparse the sentence until the derivation desired is produced. Our results with gold standard dependencies are 315 shown in Table 2. The accuracy is 87%, establishing a fairly low upper bound for our approach to the task. Manual inspection of the remaining errors showed that some were due to incorrect parses for the hypothesis sentence, and some were due to entailments which the parser’s dependency analyses could not resolve, such as They ate whole steamed grains ⇒ The grains were steamed. The largest source of errors was our matching heuristics, suggesting that our approach to the task </context>
</contexts>
<marker>Djordjevic, Curran, 2006</marker>
<rawString>Bojan Djordjevic and James R. Curran. Faster wide-coverage CCG parsing. In Proceedings of the Australasian Language Technology Workshop 2006, pages 3–10, Sydney, Australia, December 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>PhD thesis,</tech>
<contexts>
<context position="6388" citStr="Hockenmaier, 2003" startWordPosition="1017" endWordPosition="1018">2007a) demonstrate the use of techniques like adaptive supertagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C&amp;C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al., 2009). We use this parser for the PETE task. The performance of statistical parsers is largely a function of the quality of the corpora they are trained on. For this task, we used models derived from the CCGbank corpus – a transformation of the Penn Treebank (Marcus et al., 1993) including CCG derivations and dependencies (Hockenmaier, 2003a). It was created to further CCG research by providing a large corpus of appropriately annotated data, and has been shown to be suitable for the training of high-performance parsers (Hockenmaier, 2003b; Clark and Curran, 2004). 3 Method Our system used the C&amp;C parser to parse the truth and hypothesis sentences. We took the dependencies generated by the parser and processed these to generate predicates encoding the canonical form of the head word, its required arguments, and their order. We then attempted to unify the predicates from the hypothesis sentence with the predicates in the truth sen</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. Data and models for statistical parsing with Combinatory Categorial Grammar. PhD thesis, 2003a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Parsing with generative models of predicate-argument structure.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>359--366</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics</institution>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="6388" citStr="Hockenmaier, 2003" startWordPosition="1017" endWordPosition="1018">2007a) demonstrate the use of techniques like adaptive supertagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C&amp;C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al., 2009). We use this parser for the PETE task. The performance of statistical parsers is largely a function of the quality of the corpora they are trained on. For this task, we used models derived from the CCGbank corpus – a transformation of the Penn Treebank (Marcus et al., 1993) including CCG derivations and dependencies (Hockenmaier, 2003a). It was created to further CCG research by providing a large corpus of appropriately annotated data, and has been shown to be suitable for the training of high-performance parsers (Hockenmaier, 2003b; Clark and Curran, 2004). 3 Method Our system used the C&amp;C parser to parse the truth and hypothesis sentences. We took the dependencies generated by the parser and processed these to generate predicates encoding the canonical form of the head word, its required arguments, and their order. We then attempted to unify the predicates from the hypothesis sentence with the predicates in the truth sen</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. Parsing with generative models of predicate-argument structure. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 359– 366. Association for Computational Linguistics Morristown, NJ, USA, 2003b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<pages>396</pages>
<contexts>
<context position="1866" citStr="Hockenmaier and Steedman, 2007" startWordPosition="282" endWordPosition="285">trinsic evaluation – that of detecting textual entailment between a truth sentence and a hypothesis sentence. It is extrinsic in the sense that it evaluates parsers on a task, rather than a direct comparison of their output against some gold standard. However, it requires only minimal task-specific logic, and the proposed entailments are designed to be inferrable based on syntactic information alone. Our system used the C&amp;C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). We used the CCGbank-style dependency output of the parser (Hockenmaier and Steedman, 2007), which is a directed graph of head-child relations labelled with the head’s lexical category and the argument slot filled by the child. We divided the dependency graphs of the truth and hypothesis sentences into predicates that consisted of a head word and its immediate children. For instance, the parser’s analysis of the sentence Totals include only vehicle sales reported in period might produce predicates like include(Totals, sales), only(include), and reported(sales). If at least one such predicate matches in the two parses, we predict entailment. We consider a single predicate match suffi</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355– 396, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
<author>Joel Nothman</author>
<author>James R Curran</author>
</authors>
<title>Evaluating a Statistical CCG Parser on Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources,</booktitle>
<pages>38--41</pages>
<contexts>
<context position="10660" citStr="Honnibal et al. (2009)" startWordPosition="1741" endWordPosition="1744">with Gold Standard Parses Our entailment system’s errors could be broadly divided into two classes: those due to incorrect parses, and those due to incorrect comparison of the parses. To investigate the relative contributions of these two classes of errors, we manually annotated the 66 development sentences with CCG derivations. This allowed us to evaluate our system using gold standard parses. Only one annotator was available, so we were unable to calculate inter-annotator agreement scores to examine the quality of our annotations. The annotation was prepared with the annotation tool used by Honnibal et al. (2009). The tool presents the user with a CCG derivation produced by the C&amp;C parser. The user can then correct the lexical categories, or add bracket constraints to the parser using the algorithm described by Djordjevic and Curran (2006), and reparse the sentence until the derivation desired is produced. Our results with gold standard dependencies are 315 shown in Table 2. The accuracy is 87%, establishing a fairly low upper bound for our approach to the task. Manual inspection of the remaining errors showed that some were due to incorrect parses for the hypothesis sentence, and some were due to ent</context>
</contexts>
<marker>Honnibal, Nothman, Curran, 2009</marker>
<rawString>Matthew Honnibal, Joel Nothman, and James R. Curran. Evaluating a Statistical CCG Parser on Wikipedia. In Proceedings of the 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources, pages 38–41, Singapore, August 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>330</pages>
<contexts>
<context position="6326" citStr="Marcus et al., 1993" startWordPosition="1008" endWordPosition="1011">erformance parsers built on the CCG formalism. Clark and Curran (2007a) demonstrate the use of techniques like adaptive supertagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C&amp;C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al., 2009). We use this parser for the PETE task. The performance of statistical parsers is largely a function of the quality of the corpora they are trained on. For this task, we used models derived from the CCGbank corpus – a transformation of the Penn Treebank (Marcus et al., 1993) including CCG derivations and dependencies (Hockenmaier, 2003a). It was created to further CCG research by providing a large corpus of appropriately annotated data, and has been shown to be suitable for the training of high-performance parsers (Hockenmaier, 2003b; Clark and Curran, 2004). 3 Method Our system used the C&amp;C parser to parse the truth and hypothesis sentences. We took the dependencies generated by the parser and processed these to generate predicates encoding the canonical form of the head word, its required arguments, and their order. We then attempted to unify the predicates fro</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313– 330, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
</authors>
<title>Unbounded Dependency Recovery for Parser Evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<volume>2</volume>
<pages>813--821</pages>
<contexts>
<context position="6051" citStr="Rimell et al., 2009" startWordPosition="959" endWordPosition="962"> vehicle N/N 1 sales in ((S\NP)\(S\NP))/NP 2 period in ((S\NP)\(S\NP))/NP 1 reported reported S\NP 1 sales include (S\NP)/NP 2 sales include (S\NP)/NP 1 Totals Figure 2: The dependencies represented by the derivation in Figure 1. Recent work has seen the development of highperformance parsers built on the CCG formalism. Clark and Curran (2007a) demonstrate the use of techniques like adaptive supertagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C&amp;C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al., 2009). We use this parser for the PETE task. The performance of statistical parsers is largely a function of the quality of the corpora they are trained on. For this task, we used models derived from the CCGbank corpus – a transformation of the Penn Treebank (Marcus et al., 1993) including CCG derivations and dependencies (Hockenmaier, 2003a). It was created to further CCG research by providing a large corpus of appropriately annotated data, and has been shown to be suitable for the training of high-performance parsers (Hockenmaier, 2003b; Clark and Curran, 2004). 3 Method Our system used the C&amp;C p</context>
</contexts>
<marker>Rimell, Clark, Steedman, 2009</marker>
<rawString>Laura Rimell, Stephen Clark, and Mark Steedman. Unbounded Dependency Recovery for Parser Evaluation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, volume 2, pages 813–821, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<institution>Massachusetts Institute of Technology,</institution>
<location>USA,</location>
<contexts>
<context position="1774" citStr="Steedman, 2000" startWordPosition="271" endWordPosition="272">nd analyses in use. The task investigates the feasibility of a minimalist extrinsic evaluation – that of detecting textual entailment between a truth sentence and a hypothesis sentence. It is extrinsic in the sense that it evaluates parsers on a task, rather than a direct comparison of their output against some gold standard. However, it requires only minimal task-specific logic, and the proposed entailments are designed to be inferrable based on syntactic information alone. Our system used the C&amp;C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). We used the CCGbank-style dependency output of the parser (Hockenmaier and Steedman, 2007), which is a directed graph of head-child relations labelled with the head’s lexical category and the argument slot filled by the child. We divided the dependency graphs of the truth and hypothesis sentences into predicates that consisted of a head word and its immediate children. For instance, the parser’s analysis of the sentence Totals include only vehicle sales reported in period might produce predicates like include(Totals, sales), only(include), and reported(sales). If at least one such predicate </context>
<context position="4318" citStr="Steedman, 2000" startWordPosition="676" endWordPosition="678">difficult problem. 313 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 313–316, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Totals include only vehicle sales reported in period. NP (S\NP)/NP (S\NP)\(S\NP) N/N N S\NP ((S\NP)\(S\NP))/NP NP &lt;B× &gt; &gt; (S\NP)/NP N ⇒ NP S\NP ⇒ NP\NP &lt; &gt; &lt; NP S\NP S Figure 1: An example CCG derivation, showing how the categories assigned to words are combined to form a sentence. The arrows indicate the direction of application. (S\NP)\(S\NP) &lt; 2 Background Combinatory Categorial Grammar (CCG, Steedman, 2000) is a lexicalised grammar formalism based on combinatory logic. The grammar is directly encoded in the lexicon in the form of combinatory categories that govern how each word combines with its neighbours. The parsing process determines the most likely assignment of categories to words, and finds a sequence of combinators that allows them to form a sentence. A sample CCG derivation for a sentence from the test set is shown in Figure 1. The category for each word is indicated beneath it. It can be seen that some categories take other categories as arguments; each argument slot in a category is n</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. The Syntactic Process. MIT Press, Massachusetts Institute of Technology, USA, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Aydın Han</author>
<author>Zehra Turgut</author>
</authors>
<title>SemEval-2010 Task 12: Parser Evaluation using Textual Entailments.</title>
<date>2010</date>
<booktitle>In Proceedings of the SemEval-2010 Evaluation Exercises on Semantic Evaluation,</booktitle>
<marker>Yuret, Han, Turgut, 2010</marker>
<rawString>Deniz Yuret, Aydın Han, and Zehra Turgut. SemEval-2010 Task 12: Parser Evaluation using Textual Entailments. In Proceedings of the SemEval-2010 Evaluation Exercises on Semantic Evaluation, 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>