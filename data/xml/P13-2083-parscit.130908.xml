<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000588">
<title confidence="0.997039">
A Structured Distributional Semantic Model for Event Co-reference
</title>
<author confidence="0.936923">
Kartik Goyal* Sujay Kumar Jauhar* Huiying Li*
Mrinmaya Sachan* Shashank Srivastava* Eduard Hovy
</author>
<affiliation confidence="0.988092666666667">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.998859">
{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993928" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999641529411765">
In this paper we present a novel ap-
proach to modelling distributional seman-
tics that represents meaning as distribu-
tions over relations in syntactic neighbor-
hoods. We argue that our model approxi-
mates meaning in compositional configu-
rations more effectively than standard dis-
tributional vectors or bag-of-words mod-
els. We test our hypothesis on the problem
of judging event coreferentiality, which in-
volves compositional interactions in the
predicate-argument structure of sentences,
and demonstrate that our model outper-
forms both state-of-the-art window-based
word embeddings as well as simple ap-
proaches to compositional semantics pre-
viously employed in the literature.
</bodyText>
<sectionHeader confidence="0.998962" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9990070625">
Distributional Semantic Models (DSM) are popu-
lar in computational semantics. DSMs are based
on the hypothesis that the meaning of a word or
phrase can be effectively captured by the distribu-
tion of words in its neighborhood. They have been
successfully used in a variety of NLP tasks includ-
ing information retrieval (Manning et al., 2008),
question answering (Tellex et al., 2003), word-
sense discrimination (Schütze, 1998) and disam-
biguation (McCarthy et al., 2004), semantic sim-
ilarity computation (Wong and Raghavan, 1984;
McCarthy and Carroll, 2003) and selectional pref-
erence modeling (Erk, 2007).
A shortcoming of DSMs is that they ignore the
syntax within the context, thereby reducing the
distribution to a bag of words. Composing the
</bodyText>
<subsectionHeader confidence="0.674312">
∗*Equally contributing authors
</subsectionHeader>
<bodyText confidence="0.999893928571429">
distributions for “Lincoln”, “Booth”, and “killed”
gives the same result regardless of whether the in-
put is “Booth killed Lincoln” or “Lincoln killed
Booth”. But as suggested by Pantel and Lin (2000)
and others, modeling the distribution over prefer-
ential attachments for each syntactic relation sep-
arately yields greater expressive power. Thus, to
remedy the bag-of-words failing, we extend the
generic DSM model to several relation-specific
distributions over syntactic neighborhoods. In
other words, one can think of the Structured DSM
(SDSM) representation of a word/phrase as sev-
eral vectors defined over the same vocabulary,
each vector representing the word’s selectional
preferences for its various syntactic arguments.
We argue that this representation not only cap-
tures individual word semantics more effectively
than the standard DSM, but is also better able to
express the semantics of compositional units. We
prove this on the task of judging event coreference.
Experimental results indicate that our model
achieves greater predictive accuracy on the task
than models that employ weaker forms of compo-
sition, as well as a baseline that relies on state-
of-the-art window based word embeddings. This
suggests that our formalism holds the potential of
greater expressive power in problems that involve
underlying semantic compositionality.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9764665">
Next, we relate and contrast our work to prior re-
search in the fields of Distributional Vector Space
Models, Semantic Compositionality and Event
Co-reference Resolution.
</bodyText>
<subsectionHeader confidence="0.743465">
2.1 DSMs and Compositionality
</subsectionHeader>
<bodyText confidence="0.9967255">
The underlying idea that “a word is characterized
by the company it keeps” was expressed by Firth
</bodyText>
<page confidence="0.990267">
467
</page>
<bodyText confidence="0.973333622641509">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467–473,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
(1957). Several works have defined approaches to
modelling context-word distributions anchored on
a target word, topic, or sentence position. Collec-
tively these approaches are called Distributional
Semantic Models (DSMs).
While DSMs have been very successful on a va-
riety of tasks, they are not an effective model of
semantics as they lack properties such as compo-
sitionality or the ability to handle operators such
as negation. In order to model a stronger form of
semantics, there has been a recent surge in stud-
ies that phrase the problem of DSM composition-
ality as one of vector composition. These tech-
niques derive the meaning of the combination of
two words a and b by a single vector c = f(a, b).
Mitchell and Lapata (2008) propose a framework
to define the composition c = f(a, b, r, K) where
r is the relation between a and b, and K is the
additional knowledge used to define composition.
While this framework is quite general, the actual
models considered in the literature tend to disre-
gard K and r and mostly perform component-wise
addition and multiplication, with slight variations,
of the two vectors. To the best of our knowledge
the formulation of composition we propose is the
first to account for both K and r within this com-
positional framework.
Dinu and Lapata (2010) and Séaghdha and Ko-
rhonen (2011) introduced a probabilistic model
to represent word meanings by a latent variable
model. Subsequently, other high-dimensional ex-
tensions by Rudolph and Giesbrecht (2010), Ba-
roni and Zamparelli (2010) and Grefenstette et
al. (2011), regression models by Guevara (2010),
and recursive neural network based solutions by
Socher et al. (2012) and Collobert et al. (2011)
have been proposed. However, these models do
not efficiently account for structure.
Pantel and Lin (2000) and Erk and Padó (2008)
attempt to include syntactic context in distribu-
tional models. A quasi-compositional approach
was attempted in Thater et al. (2010) by a com-
bination of first and second order context vectors.
But they do not explicitly construct phrase-level
meaning from words which limits their applicabil-
ity to real world problems. Furthermore, we also
include structure into our method of composition.
Prior work in structure aware methods to the best
of our knowledge are (Weisman et al., 2012) and
(Baroni and Lenci, 2010). However, these meth-
ods do not explicitly model composition.
</bodyText>
<subsectionHeader confidence="0.99949">
2.2 Event Co-reference Resolution
</subsectionHeader>
<bodyText confidence="0.999994545454545">
While automated resolution of entity coreference
has been an actively researched area (Haghighi
and Klein, 2009; Stoyanov et al., 2009; Raghu-
nathan et al., 2010), there has been relatively lit-
tle work on event coreference resolution. Lee
et al. (2012) perform joint cross-document entity
and event coreference resolution using the two-
way feedback between events and their arguments.
We, on the other hand, attempt a slightly different
problem of making co-referentiality judgements
on event-coreference candidate pairs.
</bodyText>
<sectionHeader confidence="0.975809" genericHeader="method">
3 Structured Distributional Semantics
</sectionHeader>
<bodyText confidence="0.999955307692308">
In this paper, we propose an approach to incorpo-
rate structure into distributional semantics (more
details in Goyal et al. (2013)). The word distribu-
tions drawn from the context defined by a set of
relations anchored on the target word (or phrase)
form a set of vectors, namely a matrix for the tar-
get word. One axis of the matrix runs over all
the relations and the other axis is over the distri-
butional word vocabulary. The cells store word
counts (or PMI scores, or other measures of word
association). Note that collapsing the rows of the
matrix provides the standard dependency based
distributional representation.
</bodyText>
<subsectionHeader confidence="0.999565">
3.1 Building Representation: The PropStore
</subsectionHeader>
<bodyText confidence="0.999850454545455">
To build a lexicon of SDSM matrices for a given
vocabulary we first construct a proposition knowl-
edge base (the PropStore) created by parsing the
Simple English Wikipedia. Dependency arcs are
stored as 3-tuples of the form (w1, r, w2), denot-
ing an occurrence of words w1, word w2 related
by r. We also store sentence indices for triples
as this allows us to achieve an intuitive technique
to achieve compositionality. In addition to the
words’ surface-forms, the PropStore also stores
their POS tags, lemmas, and Wordnet supersenses.
This helps to generalize our representation when
surface-form distributions are sparse.
The PropStore can be used to query for the ex-
pectations of words, supersenses, relations, etc.,
around a given word. In the example in Figure 1,
the query (SST(W1) = verb.consumption, ?, dobj)
i.e. “what is consumed” might return expectations
[pasta:1, spaghetti:1, mice:1 ... ]. Relations and
POS tags are obtained using a dependency parser
Tratz and Hovy (2011), supersense tags using sst-
light Ciaramita and Altun (2006), and lemmas us-
</bodyText>
<page confidence="0.99911">
468
</page>
<figureCaption confidence="0.999661">
Figure 1: Sample sentences &amp; triples
</figureCaption>
<bodyText confidence="0.809212">
ing Wordnet Fellbaum (1998).
</bodyText>
<subsectionHeader confidence="0.999852">
3.2 Mimicking Compositionality
</subsectionHeader>
<bodyText confidence="0.999872714285714">
For representing intermediate multi-word phrases,
we extend the above word-relation matrix symbol-
ism in a bottom-up fashion using the PropStore.
The combination hinges on the intuition that when
lexical units combine to form a larger syntactically
connected phrase, the representation of the phrase
is given by its own distributional neighborhood
within the embedded parse tree. The distributional
neighborhood of the net phrase can be computed
using the PropStore given syntactic relations an-
chored on its parts. For the example in Figure
1, we can compose SST(w1) = Noun.person and
Lemma(W1) = eat appearing together with a nsubj
relation to obtain expectations around “people eat”
yielding [pasta:1, spaghetti:1 ... ] for the object
relation, [room:2, restaurant:1 ...] for the location
relation, etc. Larger phrasal queries can be built to
answer queries like “What do people in China eat
with?”, “What do cows do?”, etc. All of this helps
us to account for both relation r and knowledge K
obtained from the PropStore within the composi-
tional framework c = f(a, b, r, K).
The general outline to obtain a composition
of two words is given in Algorithm 1, which
returns the distributional expectation around the
composed unit. Note that the entire algorithm can
conveniently be written in the form of database
queries to our PropStore.
</bodyText>
<construct confidence="0.4957212">
Algorithm 1 ComposePair(w1, r, w2)
M1 queryMatrix(w1) (1)
M2 queryMatrix(w2)
SentIDs M1(r) n M2(r)
return ((M1n SentIDs) U (M2n SentIDs))
</construct>
<bodyText confidence="0.9961425">
For the example “noun.person nsubj eat”, steps
(1) and (2) involve querying the PropStore for the
individual tokens, noun.person and eat. Let the re-
sulting matrices be M1 and M2, respectively. In
step (3), SentIDs (sentences where the two words
appear with the specified relation) are obtained by
taking the intersection between the nsubj compo-
nent vectors of the two matrices M1 and M2. In
step (4), the entries of the original matrices M1
and M2 are intersected with this list of common
SentIDs. Finally, the resulting matrix for the com-
position of the two words is simply the union of
all the relationwise intersected sentence IDs. Intu-
itively, through this procedure, we have computed
the expectation around the words w1 and w2 when
they are connected by the relation “r”.
Similar to the two-word composition process,
given a parse subtree T of a phrase, we obtain
its matrix representation of empirical counts over
word-relation contexts (described in Algorithm 2).
Let the E = {e1 ... en} be the set of edges in T,
ei = (wi1, ri, wi2)bi = 1... n.
</bodyText>
<equation confidence="0.500693777777778">
Algorithm 2 ComposePhrase(T)
SentIDs All Sentences in corpus
for i = 1 -+ n do
Mi1 queryMatrix(wi1)
Mi2 queryMatrix(wi2)
SentIDs SentIDs n(M1(ri) n M2(ri))
end for
return ((M11n SentIDs) U (M12n SentIDs)
· · · U (Mn1n SentIDs) U (Mn2n SentIDs))
</equation>
<bodyText confidence="0.998444666666667">
The phrase representations becomes sparser as
phrase length increases. For this study, we restrict
phrasal query length to a maximum of three words.
</bodyText>
<subsectionHeader confidence="0.992646">
3.3 Event Coreferentiality
</subsectionHeader>
<bodyText confidence="0.997982">
Given the SDSM formulation and assuming no
sparsity constraints, it is possible to calculate
</bodyText>
<page confidence="0.998456">
469
</page>
<bodyText confidence="0.991876146341463">
SDSM matrices for composed concepts. However,
are these correct? Intuitively, if they truly capture
semantics, the two SDSM matrix representations
for “Booth assassinated Lincoln” and “Booth shot
Lincoln with a gun&amp;quot; should be (almost) the same.
To test this hypothesis we turn to the task of pre-
dicting whether two event mentions are coreferent
or not, even if their surface forms differ. It may be
noted that this task is different from the task of full
event coreference and hence is not directly compa-
rable to previous experimental results in the liter-
ature. Two mentions generally refer to the same
event when their respective actions, agents, pa-
tients, locations, and times are (almost) the same.
Given the non-compositional nature of determin-
ing equality of locations and times, we represent
each event mention by a triple E = (e, a, p) for
the event, agent, and patient.
In our corpus, most event mentions are verbs.
However, when nominalized events are encoun-
tered, we replace them by their verbal forms. We
use SRL Collobert et al. (2011) to determine the
agent and patient arguments of an event mention.
When SRL fails to determine either role, its empir-
ical substitutes are obtained by querying the Prop-
Store for the most likely word expectations for
the role. It may be noted that the SDSM repre-
sentation relies on syntactic dependancy relations.
Hence, to bridge the gap between these relations
and the composition of semantic role participants
of event mentions we empirically determine those
syntactic relations which most strongly co-occur
with the semantic relations connecting events,
agents and patients. The triple (e, a, p) is thus the
composition of the triples (a, relationsetagent, e)
and (p, relationsetpatient, e), and hence a com-
plex object. To determine equality of this complex
composed representation we generate three levels
of progressively simplified event constituents for
comparison:
Level 1: Full Composition:
</bodyText>
<equation confidence="0.99710975">
Mfull = ComposePhrase(e, a, p).
Level 2: Partial Composition:
Mpart:EA = ComposePair(e, r, a)
Mpart:EP = ComposePair(e, r, p).
Level 3: No Composition:
ME = queryMatrix(e)
MA = queryMatrix(a)
MP = queryMatrix(p)
</equation>
<bodyText confidence="0.985916153846154">
To judge coreference between
events E1 and E2, we compute pair-
wise similarities Sim(M1full,M2full),
Sim(M1part:EA, M2part:EA), etc., for each
level of the composed triple representation. Fur-
thermore, we vary the computation of similarity
by considering different levels of granularity
(lemma, SST), various choices of distance
metric (Euclidean, Cityblock, Cosine), and
score normalization techniques (Row-wise, Full,
Column-collapsed). This results in 159 similarity-
based features for every pair of events, which are
used to train a classifier to decide conference.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99998025">
We evaluate our method on two datasets and com-
pare it against four baselines, two of which use
window based distributional vectors and two that
employ weaker forms of composition.
</bodyText>
<subsectionHeader confidence="0.880612">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999926">
IC Event Coreference Corpus: The dataset
(Hovy et al., 2013), drawn from 100 news articles
about violent events, contains manually created
annotations for 2214 pairs of co-referent and non-
coreferent events each. Where available, events’
semantic role-fillers for agent and patient are an-
notated as well. When missing, empirical substi-
tutes were obtained by querying the PropStore for
the preferred word attachments.
EventCorefBank (ECB) corpus: This corpus
(Bejan and Harabagiu, 2010) of 482 documents
from Google News is clustered into 45 topics,
with event coreference chains annotated over each
topic. The event mentions are enriched with se-
mantic roles to obtain the canonical event struc-
ture described above. Positive instances are ob-
tained by taking pairwise event mentions within
each chain, and negative instances are generated
from pairwise event mentions across chains, but
within the same topic. This results in 11039 posi-
tive instances and 33459 negative instances.
</bodyText>
<subsectionHeader confidence="0.980698">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999945">
To establish the efficacy of our model, we compare
SDSM against a purely window-based baseline
(DSM) trained on the same corpus. In our exper-
iments we set a window size of seven words. We
also compare SDSM against the window-based
embeddings trained using a recursive neural net-
work (SENNA) (Collobert et al., 2011) on both
datsets. SENNA embeddings are state-of-the-art
for many NLP tasks. The second baseline uses
</bodyText>
<page confidence="0.995379">
470
</page>
<table confidence="0.999476571428571">
IC Corpus ECB Corpus
Prec Rec F-1 Acc Prec Rec F-1 Acc
SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843
Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791
DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830
MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831
AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834
</table>
<tableCaption confidence="0.999941">
Table 1: Cross-validation Performance on IC and ECB dataset
</tableCaption>
<bodyText confidence="0.998943888888889">
SENNA to generate level 3 similarity features for
events’ individual words (agent, patient and ac-
tion). As our final set of baselines, we extend two
simple techniques proposed by (Mitchell and Lap-
ata, 2008) that use element-wise addition and mul-
tiplication operators to perform composition. We
extend it to our matrix representation and build
two baselines AVC (element-wise addition) and
MVC (element-wise multiplication).
</bodyText>
<subsectionHeader confidence="0.970944">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999975852941176">
Among common classifiers, decision-trees (J48)
yielded best results in our experiments. Table 1
summarizes our results on both datasets.
The results reveal that the SDSM model con-
sistently outperforms DSM, SENNA embeddings,
and the MVC and AVC models, both in terms
of F-1 score and accuracy. The IC corpus com-
prises of domain specific texts, resulting in high
lexical overlap between event mentions. Hence,
the scores on the IC corpus are consistently higher
than those on the ECB corpus.
The improvements over DSM and SENNA em-
beddings, support our hypothesis that syntax lends
greater expressive power to distributional seman-
tics in compositional configurations. Furthermore,
the increase in predictive accuracy over MVC and
AVC shows that our formulation of composition
of two words based on the relation binding them
yields a stronger form of compositionality than
simple additive and multiplicative models.
Next, we perform an ablation study to deter-
mine the most predictive features for the task of
event coreferentiality. The forward selection pro-
cedure reveals that the most informative attributes
are the level 2 compositional features involving
the agent and the action, as well as their individ-
ual level 3 features. This corresponds to the in-
tuition that the agent and the action are the prin-
cipal determiners for identifying events. Features
involving the patient and level 1 features are least
useful. This is probably because features involv-
ing full composition are sparse, and not as likely
to provide statistically significant evidence. This
may change as our PropStore grows in size.
</bodyText>
<sectionHeader confidence="0.977634" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999980625">
We outlined an approach that introduces structure
into distributed semantic representations gives us
an ability to compare the identity of two repre-
sentations derived from supposedly semantically
identical phrases with different surface realiza-
tions. We employed the task of event coreference
to validate our representation and achieved sig-
nificantly higher predictive accuracy than several
baselines.
In the future, we would like to extend our model
to other semantic tasks such as paraphrase detec-
tion, lexical substitution and recognizing textual
entailment. We would also like to replace our syn-
tactic relations to semantic relations and explore
various ways of dimensionality reduction to solve
this problem.
</bodyText>
<sectionHeader confidence="0.987295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999849166666667">
The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following
grants: NSF grant IIS-1143703, NSF award IIS-
1147810, DARPA grant FA87501220342.
</bodyText>
<sectionHeader confidence="0.993628" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.856645875">
Marco Baroni and Alessandro Lenci. 2010. Distri-
butional memory: A general framework for corpus-
based semantics. Comput. Linguist., 36(4):673–721,
December.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
</reference>
<page confidence="0.995957">
471
</page>
<reference confidence="0.986265678571429">
Methods in Natural Language Processing, EMNLP
’10, pages 1183–1193, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1412–1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’06, pages 594–602, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 999888:2493–2537,
November.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ’10, pages
1162–1172, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Katrin Erk and Sebastian Padó. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’08,
pages 897–906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1–
32.
Kartik. Goyal, Sujay Kumar Jauhar, Mrinmaya Sachan,
Shashank Srivastava, Huiying Li, and Eduard Hovy.
2013. A structured distributional semantic model
: Integrating structure with semantics. In Proceed-
ings of the 1st Continuous Vector Space Models and
their Compositionality Workshop at the conference
of ACL 2013.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics, IWCS ’11, pages 125–134, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, GEMS ’10, pages 33–37, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ’09, pages 1152–
1161, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
E.H. Hovy, T. Mitamura, M.F. Verdejo, J. Araki, and
A. Philpot. 2013. Events are not simple: Iden-
tity, non-identity, and quasi-identity. In Proceedings
of the 1st Events Workshop at the conference of the
HLT-NAACL 2013.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 489–500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Comput.
Linguist., 29(4):639–654, December.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, ACL ’04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244.
Patrick Pantel and Dekang Lin. 2000. Word-for-word
glossing with contextually similar words. In Pro-
ceedings of the 1st North American chapter of the
Association for Computational Linguistics confer-
ence, NAACL 2000, pages 78–85, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’10,
</reference>
<page confidence="0.981633">
472
</page>
<reference confidence="0.999775738461538">
pages 492–501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ’10,
pages 907–916, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hinrich Schütze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97–123.
Diarmuid Ó Séaghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’11, pages 1047–1057, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 1201–1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ’09, pages 656–664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR, pages 41–47.
Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ’10, pages
948–957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’11, pages 1257–1268, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido
Dagan. 2012. Learning verb inference rules from
linguistically-motivated evidence. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 194–204, Stroudsburg, PA, USA. Association
for Computational Linguistics.
S. K. M. Wong and Vijay V. Raghavan. 1984. Vector
space model of information retrieval: a reevaluation.
In Proceedings of the 7th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ’84, pages 167–185,
Swinton, UK. British Computer Society.
</reference>
<page confidence="0.999361">
473
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.345337">
<title confidence="0.999152">A Structured Distributional Semantic Model for Event Co-reference</title>
<author confidence="0.6994705">Sujay Kumar Huiying Shashank Eduard</author>
<affiliation confidence="0.887101">Language Technologies School of Computer Carnegie Mellon</affiliation>
<email confidence="0.998992">kartikgo@cs.cmu.edu</email>
<email confidence="0.998992">sjauhar@cs.cmu.edu</email>
<email confidence="0.998992">huiyingl@cs.cmu.edu</email>
<email confidence="0.998992">mrinmays@cs.cmu.edu</email>
<email confidence="0.998992">shashans@cs.cmu.edu</email>
<email confidence="0.998992">hovy@cs.cmu.edu</email>
<abstract confidence="0.997127111111111">In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpusbased semantics.</title>
<date>2010</date>
<journal>Comput. Linguist.,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="5961" citStr="Baroni and Lenci, 2010" startWordPosition="907" endWordPosition="910">. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 2.2 Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference c</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpusbased semantics. Comput. Linguist., 36(4):673–721, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5153" citStr="Baroni and Zamparelli (2010)" startWordPosition="778" endWordPosition="782">tion. While this framework is quite general, the actual models considered in the literature tend to disregard K and r and mostly perform component-wise addition and multiplication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real w</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1183–1193, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Unsupervised event coreference resolution with rich linguistic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1412--1422</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14901" citStr="Bejan and Harabagiu, 2010" startWordPosition="2319" endWordPosition="2322">it against four baselines, two of which use window based distributional vectors and two that employ weaker forms of composition. 4.1 Datasets IC Event Coreference Corpus: The dataset (Hovy et al., 2013), drawn from 100 news articles about violent events, contains manually created annotations for 2214 pairs of co-referent and noncoreferent events each. Where available, events’ semantic role-fillers for agent and patient are annotated as well. When missing, empirical substitutes were obtained by querying the PropStore for the preferred word attachments. EventCorefBank (ECB) corpus: This corpus (Bejan and Harabagiu, 2010) of 482 documents from Google News is clustered into 45 topics, with event coreference chains annotated over each topic. The event mentions are enriched with semantic roles to obtain the canonical event structure described above. Positive instances are obtained by taking pairwise event mentions within each chain, and negative instances are generated from pairwise event mentions across chains, but within the same topic. This results in 11039 positive instances and 33459 negative instances. 4.2 Baselines To establish the efficacy of our model, we compare SDSM against a purely window-based baseli</context>
</contexts>
<marker>Bejan, Harabagiu, 2010</marker>
<rawString>Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Unsupervised event coreference resolution with rich linguistic features. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1412–1422, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>594--602</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8322" citStr="Ciaramita and Altun (2006)" startWordPosition="1277" endWordPosition="1280">In addition to the words’ surface-forms, the PropStore also stores their POS tags, lemmas, and Wordnet supersenses. This helps to generalize our representation when surface-form distributions are sparse. The PropStore can be used to query for the expectations of words, supersenses, relations, etc., around a given word. In the example in Figure 1, the query (SST(W1) = verb.consumption, ?, dobj) i.e. “what is consumed” might return expectations [pasta:1, spaghetti:1, mice:1 ... ]. Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas us468 Figure 1: Sample sentences &amp; triples ing Wordnet Fellbaum (1998). 3.2 Mimicking Compositionality For representing intermediate multi-word phrases, we extend the above word-relation matrix symbolism in a bottom-up fashion using the PropStore. The combination hinges on the intuition that when lexical units combine to form a larger syntactically connected phrase, the representation of the phrase is given by its own distributional neighborhood within the embedded parse tree. The distributional neighborhood of the net phrase can be computed using the PropStore given syntactic rel</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 594–602, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Léon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>999888--2493</pages>
<contexts>
<context position="5319" citStr="Collobert et al. (2011)" startWordPosition="805" endWordPosition="808">lication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman </context>
<context position="12532" citStr="Collobert et al. (2011)" startWordPosition="1967" endWordPosition="1970">ifferent from the task of full event coreference and hence is not directly comparable to previous experimental results in the literature. Two mentions generally refer to the same event when their respective actions, agents, patients, locations, and times are (almost) the same. Given the non-compositional nature of determining equality of locations and times, we represent each event mention by a triple E = (e, a, p) for the event, agent, and patient. In our corpus, most event mentions are verbs. However, when nominalized events are encountered, we replace them by their verbal forms. We use SRL Collobert et al. (2011) to determine the agent and patient arguments of an event mention. When SRL fails to determine either role, its empirical substitutes are obtained by querying the PropStore for the most likely word expectations for the role. It may be noted that the SDSM representation relies on syntactic dependancy relations. Hence, to bridge the gap between these relations and the composition of semantic role participants of event mentions we empirically determine those syntactic relations which most strongly co-occur with the semantic relations connecting events, agents and patients. The triple (e, a, p) is</context>
<context position="15724" citStr="Collobert et al., 2011" startWordPosition="2451" endWordPosition="2454">nt structure described above. Positive instances are obtained by taking pairwise event mentions within each chain, and negative instances are generated from pairwise event mentions across chains, but within the same topic. This results in 11039 positive instances and 33459 negative instances. 4.2 Baselines To establish the efficacy of our model, we compare SDSM against a purely window-based baseline (DSM) trained on the same corpus. In our experiments we set a window size of seven words. We also compare SDSM against the window-based embeddings trained using a recursive neural network (SENNA) (Collobert et al., 2011) on both datsets. SENNA embeddings are state-of-the-art for many NLP tasks. The second baseline uses 470 IC Corpus ECB Corpus Prec Rec F-1 Acc Prec Rec F-1 Acc SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843 Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791 DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830 MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831 AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834 Table 1: Cross-validation Performance on IC and ECB dataset SENNA to generate level 3 similarity features for events’ individual words (agent, patient and action). As our final </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 999888:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1162--1172</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4921" citStr="Dinu and Lapata (2010)" startWordPosition="745" endWordPosition="748">and b by a single vector c = f(a, b). Mitchell and Lapata (2008) propose a framework to define the composition c = f(a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition. While this framework is quite general, the actual models considered in the literature tend to disregard K and r and mostly perform component-wise addition and multiplication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1162–1172, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Padó</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>897--906</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5449" citStr="Erk and Padó (2008)" startWordPosition="826" endWordPosition="829">rst to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 2.2 Event Co-reference Res</context>
</contexts>
<marker>Erk, Padó, 2008</marker>
<rawString>Katrin Erk and Sebastian Padó. 2008. A structured vector space model for word meaning in context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 897–906, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<contexts>
<context position="1607" citStr="Erk, 2007" startWordPosition="224" endWordPosition="225">roduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the ∗*Equally contributing authors distributions for “Lincoln”, “Booth”, and “killed” gives the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. But as suggested by Pantel and Lin (2000) and others, modeling the distribution over preferential attachments for each syntactic relation separately yields greater expressive power. Thus, to remedy the bag-of-words failing, we extend the generic DSM model to several re</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="8405" citStr="Fellbaum (1998)" startWordPosition="1293" endWordPosition="1294"> Wordnet supersenses. This helps to generalize our representation when surface-form distributions are sparse. The PropStore can be used to query for the expectations of words, supersenses, relations, etc., around a given word. In the example in Figure 1, the query (SST(W1) = verb.consumption, ?, dobj) i.e. “what is consumed” might return expectations [pasta:1, spaghetti:1, mice:1 ... ]. Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas us468 Figure 1: Sample sentences &amp; triples ing Wordnet Fellbaum (1998). 3.2 Mimicking Compositionality For representing intermediate multi-word phrases, we extend the above word-relation matrix symbolism in a bottom-up fashion using the PropStore. The combination hinges on the intuition that when lexical units combine to form a larger syntactically connected phrase, the representation of the phrase is given by its own distributional neighborhood within the embedded parse tree. The distributional neighborhood of the net phrase can be computed using the PropStore given syntactic relations anchored on its parts. For the example in Figure 1, we can compose SST(w1) =</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>A Synopsis of Linguistic Theory,</title>
<date>1957</date>
<booktitle>Studies in Linguistic Analysis,</booktitle>
<pages>1--32</pages>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. A Synopsis of Linguistic Theory, 1930-1955. Studies in Linguistic Analysis, pages 1– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujay Kumar Jauhar Goyal</author>
<author>Mrinmaya Sachan</author>
<author>Shashank Srivastava</author>
<author>Huiying Li</author>
<author>Eduard Hovy</author>
</authors>
<title>A structured distributional semantic model : Integrating structure with semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 1st Continuous Vector Space Models and their Compositionality Workshop at the conference of ACL</booktitle>
<contexts>
<context position="6744" citStr="Goyal et al. (2013)" startWordPosition="1021" endWordPosition="1024">y researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs. 3 Structured Distributional Semantics In this paper, we propose an approach to incorporate structure into distributional semantics (more details in Goyal et al. (2013)). The word distributions drawn from the context defined by a set of relations anchored on the target word (or phrase) form a set of vectors, namely a matrix for the target word. One axis of the matrix runs over all the relations and the other axis is over the distributional word vocabulary. The cells store word counts (or PMI scores, or other measures of word association). Note that collapsing the rows of the matrix provides the standard dependency based distributional representation. 3.1 Building Representation: The PropStore To build a lexicon of SDSM matrices for a given vocabulary we firs</context>
</contexts>
<marker>Goyal, Sachan, Srivastava, Li, Hovy, 2013</marker>
<rawString>Kartik. Goyal, Sujay Kumar Jauhar, Mrinmaya Sachan, Shashank Srivastava, Huiying Li, and Eduard Hovy. 2013. A structured distributional semantic model : Integrating structure with semantics. In Proceedings of the 1st Continuous Vector Space Models and their Compositionality Workshop at the conference of ACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Stephen Pulman</author>
</authors>
<title>Concrete sentence spaces for compositional distributional models of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics, IWCS ’11,</booktitle>
<pages>125--134</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5184" citStr="Grefenstette et al. (2011)" startWordPosition="784" endWordPosition="787">te general, the actual models considered in the literature tend to disregard K and r and mostly perform component-wise addition and multiplication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we </context>
</contexts>
<marker>Grefenstette, Sadrzadeh, Clark, Coecke, Pulman, 2011</marker>
<rawString>Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete sentence spaces for compositional distributional models of meaning. In Proceedings of the Ninth International Conference on Computational Semantics, IWCS ’11, pages 125–134, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, GEMS ’10,</booktitle>
<pages>33--37</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5221" citStr="Guevara (2010)" startWordPosition="791" endWordPosition="792">iterature tend to disregard K and r and mostly perform component-wise addition and multiplication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our metho</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, GEMS ’10, pages 33–37, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09,</booktitle>
<pages>1152--1161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6168" citStr="Haghighi and Klein, 2009" startWordPosition="936" endWordPosition="939">was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 2.2 Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs. 3 Structured Distributional Semantics In this paper, we propose an approach to incorporate structure into distributional semantics (more details in Goyal et al. (2013)). The word distribution</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09, pages 1152– 1161, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
<author>T Mitamura</author>
<author>M F Verdejo</author>
<author>J Araki</author>
<author>A Philpot</author>
</authors>
<title>Events are not simple: Identity, non-identity, and quasi-identity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 1st Events Workshop at the conference of the HLT-NAACL</booktitle>
<contexts>
<context position="14477" citStr="Hovy et al., 2013" startWordPosition="2258" endWordPosition="2261">utation of similarity by considering different levels of granularity (lemma, SST), various choices of distance metric (Euclidean, Cityblock, Cosine), and score normalization techniques (Row-wise, Full, Column-collapsed). This results in 159 similaritybased features for every pair of events, which are used to train a classifier to decide conference. 4 Experiments We evaluate our method on two datasets and compare it against four baselines, two of which use window based distributional vectors and two that employ weaker forms of composition. 4.1 Datasets IC Event Coreference Corpus: The dataset (Hovy et al., 2013), drawn from 100 news articles about violent events, contains manually created annotations for 2214 pairs of co-referent and noncoreferent events each. Where available, events’ semantic role-fillers for agent and patient are annotated as well. When missing, empirical substitutes were obtained by querying the PropStore for the preferred word attachments. EventCorefBank (ECB) corpus: This corpus (Bejan and Harabagiu, 2010) of 482 documents from Google News is clustered into 45 topics, with event coreference chains annotated over each topic. The event mentions are enriched with semantic roles to </context>
</contexts>
<marker>Hovy, Mitamura, Verdejo, Araki, Philpot, 2013</marker>
<rawString>E.H. Hovy, T. Mitamura, M.F. Verdejo, J. Araki, and A. Philpot. 2013. Events are not simple: Identity, non-identity, and quasi-identity. In Proceedings of the 1st Events Workshop at the conference of the HLT-NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Marta Recasens</author>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint entity and event coreference resolution across documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>489--500</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6308" citStr="Lee et al. (2012)" startWordPosition="960" endWordPosition="963"> meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 2.2 Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs. 3 Structured Distributional Semantics In this paper, we propose an approach to incorporate structure into distributional semantics (more details in Goyal et al. (2013)). The word distributions drawn from the context defined by a set of relations anchored on the target word (or phrase) form a set of vectors, namely a matrix for th</context>
</contexts>
<marker>Lee, Recasens, Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 489–500, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schütze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1345" citStr="Manning et al., 2008" startWordPosition="185" endWordPosition="188">s compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature. 1 Introduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the ∗*Equally contributing authors distributions for “Lincoln”, “Booth”, and “killed” gives the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. But as </context>
</contexts>
<marker>Manning, Raghavan, Schütze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1559" citStr="McCarthy and Carroll, 2003" startWordPosition="215" endWordPosition="218">positional semantics previously employed in the literature. 1 Introduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the ∗*Equally contributing authors distributions for “Lincoln”, “Booth”, and “killed” gives the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. But as suggested by Pantel and Lin (2000) and others, modeling the distribution over preferential attachments for each syntactic relation separately yields greater expressive power. Thus, to remedy the bag-of-words failin</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>Diana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. Comput. Linguist., 29(4):639–654, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1472" citStr="McCarthy et al., 2004" startWordPosition="203" endWordPosition="206"> state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature. 1 Introduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the ∗*Equally contributing authors distributions for “Lincoln”, “Booth”, and “killed” gives the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. But as suggested by Pantel and Lin (2000) and others, modeling the distribution over preferential attachments for each syntactic relat</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="4363" citStr="Mitchell and Lapata (2008)" startWordPosition="648" endWordPosition="651">et word, topic, or sentence position. Collectively these approaches are called Distributional Semantic Models (DSMs). While DSMs have been very successful on a variety of tasks, they are not an effective model of semantics as they lack properties such as compositionality or the ability to handle operators such as negation. In order to model a stronger form of semantics, there has been a recent surge in studies that phrase the problem of DSM compositionality as one of vector composition. These techniques derive the meaning of the combination of two words a and b by a single vector c = f(a, b). Mitchell and Lapata (2008) propose a framework to define the composition c = f(a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition. While this framework is quite general, the actual models considered in the literature tend to disregard K and r and mostly perform component-wise addition and multiplication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduc</context>
<context position="16413" citStr="Mitchell and Lapata, 2008" startWordPosition="2566" endWordPosition="2570">y NLP tasks. The second baseline uses 470 IC Corpus ECB Corpus Prec Rec F-1 Acc Prec Rec F-1 Acc SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843 Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791 DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830 MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831 AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834 Table 1: Cross-validation Performance on IC and ECB dataset SENNA to generate level 3 similarity features for events’ individual words (agent, patient and action). As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition. We extend it to our matrix representation and build two baselines AVC (element-wise addition) and MVC (element-wise multiplication). 4.3 Discussion Among common classifiers, decision-trees (J48) yielded best results in our experiments. Table 1 summarizes our results on both datasets. The results reveal that the SDSM model consistently outperforms DSM, SENNA embeddings, and the MVC and AVC models, both in terms of F-1 score and accuracy. The IC corpus comprises of domain specific texts, resulting in high lexica</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Word-for-word glossing with contextually similar words.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, NAACL</booktitle>
<pages>78--85</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1979" citStr="Pantel and Lin (2000)" startWordPosition="281" endWordPosition="284">swering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the ∗*Equally contributing authors distributions for “Lincoln”, “Booth”, and “killed” gives the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. But as suggested by Pantel and Lin (2000) and others, modeling the distribution over preferential attachments for each syntactic relation separately yields greater expressive power. Thus, to remedy the bag-of-words failing, we extend the generic DSM model to several relation-specific distributions over syntactic neighborhoods. In other words, one can think of the Structured DSM (SDSM) representation of a word/phrase as several vectors defined over the same vocabulary, each vector representing the word’s selectional preferences for its various syntactic arguments. We argue that this representation not only captures individual word sem</context>
<context position="5425" citStr="Pantel and Lin (2000)" startWordPosition="821" endWordPosition="824">ition we propose is the first to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 2.</context>
</contexts>
<marker>Pantel, Lin, 2000</marker>
<rawString>Patrick Pantel and Dekang Lin. 2000. Word-for-word glossing with contextually similar words. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, NAACL 2000, pages 78–85, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multipass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>492--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6218" citStr="Raghunathan et al., 2010" startWordPosition="944" endWordPosition="948">ation of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 2.2 Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs. 3 Structured Distributional Semantics In this paper, we propose an approach to incorporate structure into distributional semantics (more details in Goyal et al. (2013)). The word distributions drawn from the context defined by a set of relat</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multipass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 492–501, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Rudolph</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Compositional matrix-space models of language.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>907--916</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5123" citStr="Rudolph and Giesbrecht (2010)" startWordPosition="774" endWordPosition="777">nowledge used to define composition. While this framework is quite general, the actual models considered in the literature tend to disregard K and r and mostly perform component-wise addition and multiplication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits</context>
</contexts>
<marker>Rudolph, Giesbrecht, 2010</marker>
<rawString>Sebastian Rudolph and Eugenie Giesbrecht. 2010. Compositional matrix-space models of language. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 907–916, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Comput. Linguist.,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="1429" citStr="Schütze, 1998" startWordPosition="198" endWordPosition="199">ate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature. 1 Introduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the ∗*Equally contributing authors distributions for “Lincoln”, “Booth”, and “killed” gives the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. But as suggested by Pantel and Lin (2000) and others, modeling the distribution over prefer</context>
</contexts>
<marker>Schütze, 1998</marker>
<rawString>Hinrich Schütze. 1998. Automatic word sense discrimination. Comput. Linguist., 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid Ó Séaghdha</author>
<author>Anna Korhonen</author>
</authors>
<title>Probabilistic models of similarity in syntactic context.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1047--1057</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4954" citStr="Séaghdha and Korhonen (2011)" startWordPosition="750" endWordPosition="754">= f(a, b). Mitchell and Lapata (2008) propose a framework to define the composition c = f(a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition. While this framework is quite general, the actual models considered in the literature tend to disregard K and r and mostly perform component-wise addition and multiplication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attemp</context>
</contexts>
<marker>Séaghdha, Korhonen, 2011</marker>
<rawString>Diarmuid Ó Séaghdha and Anna Korhonen. 2011. Probabilistic models of similarity in syntactic context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1047–1057, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5291" citStr="Socher et al. (2012)" startWordPosition="800" endWordPosition="803">-wise addition and multiplication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1201–1211, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: making sense of the stateof-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09,</booktitle>
<pages>656--664</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6191" citStr="Stoyanov et al., 2009" startWordPosition="940" endWordPosition="943"> al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 2.2 Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs. 3 Structured Distributional Semantics In this paper, we propose an approach to incorporate structure into distributional semantics (more details in Goyal et al. (2013)). The word distributions drawn from the contex</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: making sense of the stateof-the-art. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages 656–664, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Boris Katz</author>
<author>Jimmy J Lin</author>
<author>Aaron Fernandes</author>
<author>Gregory Marton</author>
</authors>
<title>Quantitative evaluation of passage retrieval algorithms for question answering. In</title>
<date>2003</date>
<booktitle>SIGIR,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="1387" citStr="Tellex et al., 2003" startWordPosition="191" endWordPosition="194">te-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature. 1 Introduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the ∗*Equally contributing authors distributions for “Lincoln”, “Booth”, and “killed” gives the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. But as suggested by Pantel and Lin (2000) and oth</context>
</contexts>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fernandes, and Gregory Marton. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In SIGIR, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen Fürstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>948--957</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5581" citStr="Thater et al. (2010)" startWordPosition="845" endWordPosition="848">uced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 2.2 Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et</context>
</contexts>
<marker>Thater, Fürstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen Fürstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 948–957, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A fast, accurate, non-projective, semantically-enriched parser.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1257--1268</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8263" citStr="Tratz and Hovy (2011)" startWordPosition="1268" endWordPosition="1271">e an intuitive technique to achieve compositionality. In addition to the words’ surface-forms, the PropStore also stores their POS tags, lemmas, and Wordnet supersenses. This helps to generalize our representation when surface-form distributions are sparse. The PropStore can be used to query for the expectations of words, supersenses, relations, etc., around a given word. In the example in Figure 1, the query (SST(W1) = verb.consumption, ?, dobj) i.e. “what is consumed” might return expectations [pasta:1, spaghetti:1, mice:1 ... ]. Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas us468 Figure 1: Sample sentences &amp; triples ing Wordnet Fellbaum (1998). 3.2 Mimicking Compositionality For representing intermediate multi-word phrases, we extend the above word-relation matrix symbolism in a bottom-up fashion using the PropStore. The combination hinges on the intuition that when lexical units combine to form a larger syntactically connected phrase, the representation of the phrase is given by its own distributional neighborhood within the embedded parse tree. The distributional neighborhood of the net phr</context>
</contexts>
<marker>Tratz, Hovy, 2011</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2011. A fast, accurate, non-projective, semantically-enriched parser. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1257–1268, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hila Weisman</author>
<author>Jonathan Berant</author>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning verb inference rules from linguistically-motivated evidence.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>194--204</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5932" citStr="Weisman et al., 2012" startWordPosition="902" endWordPosition="905">. (2011) have been proposed. However, these models do not efficiently account for structure. Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 2.2 Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judg</context>
</contexts>
<marker>Weisman, Berant, Szpektor, Dagan, 2012</marker>
<rawString>Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido Dagan. 2012. Learning verb inference rules from linguistically-motivated evidence. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 194–204, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>Vijay V Raghavan</author>
</authors>
<title>Vector space model of information retrieval: a reevaluation.</title>
<date>1984</date>
<booktitle>In Proceedings of the 7th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’84,</booktitle>
<pages>167--185</pages>
<publisher>British Computer Society.</publisher>
<location>Swinton, UK.</location>
<contexts>
<context position="1530" citStr="Wong and Raghavan, 1984" startWordPosition="211" endWordPosition="214"> simple approaches to compositional semantics previously employed in the literature. 1 Introduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the ∗*Equally contributing authors distributions for “Lincoln”, “Booth”, and “killed” gives the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. But as suggested by Pantel and Lin (2000) and others, modeling the distribution over preferential attachments for each syntactic relation separately yields greater expressive power. Thus, to r</context>
</contexts>
<marker>Wong, Raghavan, 1984</marker>
<rawString>S. K. M. Wong and Vijay V. Raghavan. 1984. Vector space model of information retrieval: a reevaluation. In Proceedings of the 7th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’84, pages 167–185, Swinton, UK. British Computer Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>