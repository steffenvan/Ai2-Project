<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.992433">
Bilingually-Guided Monolingual Dependency Grammar Induction
</title>
<author confidence="0.997877">
Kai Liu†§, Yajuan L¨u†, Wenbin Jiang†, Qun Liu‡†
</author>
<affiliation confidence="0.9936985">
†Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
</affiliation>
<address confidence="0.811653">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.99077">
{liukai,lvyajuan,jiangwenbin,liuqun}@ict.ac.cn
</email>
<affiliation confidence="0.90458975">
‡Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
§University of Chinese Academy of Sciences
</affiliation>
<sectionHeader confidence="0.989052" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958086956522">
This paper describes a novel strategy for
automatic induction of a monolingual de-
pendency grammar under the guidance
of bilingually-projected dependency. By
moderately leveraging the dependency in-
formation projected from the parsed coun-
terpart language, and simultaneously min-
ing the underlying syntactic structure of
the language considered, it effectively in-
tegrates the advantages of bilingual pro-
jection and unsupervised induction, so as
to induce a monolingual grammar much
better than previous models only using
bilingual projection or unsupervised in-
duction. We induced dependency gram-
mar for five different languages under the
guidance of dependency information pro-
jected from the parsed English translation,
experiments show that the bilingually-
guided method achieves a significant
improvement of 28.5% over the unsuper-
vised baseline and 3.0% over the best pro-
jection baseline on average.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964571428571">
In past decades supervised methods achieved the
state-of-the-art in constituency parsing (Collins,
2003; Charniak and Johnson, 2005; Petrov et al.,
2006) and dependency parsing (McDonald et al.,
2005a; McDonald et al., 2006; Nivre et al., 2006;
Nivre et al., 2007; Koo and Collins, 2010). For
supervised models, the human-annotated corpora
on which models are trained, however, are expen-
sive and difficult to build. As alternative strate-
gies, methods which utilize raw texts have been in-
vestigated recently, including unsupervised meth-
ods which use only raw texts (Klein and Man-
ning, 2004; Smith and Eisner, 2005; William et
al., 2009), and semi-supervised methods (Koo et
al., 2008) which use both raw texts and annotat-
ed corpus. And there are a lot of efforts have also
been devoted to bilingual projection (Chen et al.,
2010), which resorts to bilingual text with one lan-
guage parsed, and projects the syntactic informa-
tion from the parsed language to the unparsed one
(Hwa et al., 2005; Ganchev et al., 2009).
In dependency grammar induction, unsuper-
vised methods achieve continuous improvements
in recent years (Klein and Manning, 2004; Smith
and Eisner, 2005; Bod, 2006; William et al., 2009;
Spitkovsky et al., 2010). Relying on a predefined
distributional assumption and iteratively maximiz-
ing an approximate indicator (entropy, likelihood,
etc.), an unsupervised model usually suffers from
two drawbacks, i.e., lower performance and high-
er computational cost. On the contrary, bilin-
gual projection (Hwa et al., 2005; Smith and Eis-
ner, 2009; Jiang and Liu, 2010) seems a promis-
ing substitute for languages with a
large amount of bilingual sentences and an exist-
ing parser of the counterpart language. By project-
ing syntactic structures directly (Hwa et al., 2005;
Smith and Eisner, 2009; Jiang and Liu, 2010)
across bilingual texts or indirectly across multi-
lingual texts (Snyder et al., 2009; McDonald et
al., 2011; Naseem et al., 2012), a better depen-
dency grammar can be easily induced, if syntactic
isomorphism is largely maintained between target
and source languages.
Unsupervised induction and bilingual projec-
tion run according to totally different principles,
the former mines the underlying structure of the
monolingual language, while the latter leverages
the syntactic knowledge of the parsed counter-
</bodyText>
<page confidence="0.670042">
1063
</page>
<note confidence="0.9194815">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063–1072,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.9923686875">
Bilingual corpus Joint Optimization
Target
sentences
Source
sentences
Random
Treebank
projection
Unsupervised
objective
Projection
objective
Bilingually-guided
Parsing model
Evolved
treebank
</figure>
<figureCaption confidence="0.99999">
Figure 1: Training the bilingually-guided parsing model by iteration.
</figureCaption>
<bodyText confidence="0.999983944444445">
part language. Considering this, we propose a
novel strategy for automatically inducing a mono-
lingual dependency grammar under the guidance
of bilingually-projected dependency information,
which integrates the advantage of bilingual pro-
jection into the unsupervised framework. A
randomly-initialized monolingual treebank
evolves in a self-training iterative procedure, and
the grammar parameters are tuned to simultane-
ously maximize both the monolingual likelihood
and bilingually-projected likelihood of the evolv-
ing treebank. The monolingual likelihood is sim-
ilar to the optimization objectives of convention-
al unsupervised models, while the bilingually-
projected likelihood is the product of the projected
probabilities of dependency trees. By moderately
leveraging the dependency information projected
from the parsed counterpart language, and simul-
taneously mining the underlying syntactic struc-
ture of the language considered, we can automat-
ically induce a monolingual dependency grammar
which is much better than previous models only
using bilingual projection or unsupervised induc-
tion. In addition, since both likelihoods are fun-
damentally factorized into dependency edges (of
the hypothesis tree), the computational complexi-
ty approaches to unsupervised models, while with
much faster convergence. We evaluate the final
automatically-induced dependency parsing mod-
el on 5 languages. Experimental results show
that our method significantly outperforms previ-
ous work based on unsupervised method or indi-
rect/direct dependency projection, where we see
an average improvement of 28.5% over unsuper-
vised baseline on all languages, and the improve-
ments are 3.9%/3.0% over indirect/direct base-
lines. And our model achieves the most signif-
icant gains on Chinese, where the improvements
are 12.0%, 4.5% over indirect and direct projec-
tion baselines respectively.
In the rest of the paper, we first describe the un-
supervised dependency grammar induction frame-
work in section 2 (where the unsupervised op-
timization objective is given), and introduce the
bilingual projection method for dependency pars-
ing in section 3 (where the projected optimiza-
tion objective is given); Then in section 4 we
present the bilingually-guided induction strategy
for dependency grammar (where the two objec-
tives above are jointly optimized, as shown in Fig-
ure 1). After giving a brief introduction of previ-
ous work in section 5, we finally give the experi-
mental results in section 6 and conclude our work
in section 7.
</bodyText>
<sectionHeader confidence="0.9903415" genericHeader="introduction">
2 Unsupervised Dependency Grammar
Induction
</sectionHeader>
<bodyText confidence="0.999768190476191">
In this section, we introduce the unsupervised ob-
jective and the unsupervised training algorithm
which is used as the framework of our bilingually-
guided method. Unlike previous unsupervised
work (Klein and Manning, 2004; Smith and Eis-
ner, 2005; Bod, 2006), we select a self-training
approach (similar to hard EM method) to train
the unsupervised model. And the framework of
our unsupervised model builds a random treebank
on the monolingual corpus firstly for initialization
and trains a discriminative parsing model on it.
Then we use the parser to build an evolved tree-
bank with the 1-best result for the next iteration
run. In this way, the parser and treebank evolve in
an iterative way until convergence. Let’s introduce
the parsing objective firstly:
Define ez as the ith word in monolingual sen-
tence E; deij denotes the word pair dependency re-
lationship (ez → ej). Based on the features around
deij, we can calculate the probability Pr(y|deij)
that the word pair deij can form a dependency arc
</bodyText>
<page confidence="0.826392">
1064
</page>
<bodyText confidence="0.46564">
as:
</bodyText>
<equation confidence="0.9990035">
Pr(y|deij) = 1 exp(EAn &apos; fn(deij, y)) (1)
Z(deij) n
</equation>
<bodyText confidence="0.980037857142857">
where y is the category of the relationship of deij:
y = + means it is the probability that the word
pair deij can form a dependency arc and y = −
means the contrary. An denotes the weight for fea-
ture function fn(deij, y), and the features we used
are presented in Table 1 (Section 6). Z(deij) is a
normalizing constant:
</bodyText>
<figure confidence="0.966769928571429">
Algorithm 1 Training unsupervised model
1: build random DE
2: A +- train(DE, DE)
3:repeat
ᏗҔ
4: for each E E E do &gt; E step
5: DE +- parse(E, A)
6: A +- train(DE, DE) &gt; M step
7: until convergence
bushi yu shalong juxing le huitan
Bush held a talk with Sharon
X Xexp( An · fn(deij , y)) (2)
Z(deij) = n
y
</figure>
<bodyText confidence="0.883295">
Given a sentence E, parsing a dependency tree
is to find a dependency tree DE with maximum
probability PE:
</bodyText>
<subsectionHeader confidence="0.985787">
2.1 Unsupervised Objective
</subsectionHeader>
<bodyText confidence="0.99400525">
We select a simple classifier objective function as
the unsupervised objective function which is in-
stinctively in accordance with the parsing objec-
tive:
</bodyText>
<equation confidence="0.98642">
Pr(+|de) Y Pr(−|de) (4)
DE
</equation>
<bodyText confidence="0.9859009">
where E is the monolingual corpus and E ∈ E,
DE is the treebank that contains all DE in the cor-
pus, and eDE denotes all other possible dependen-
cy arcs which do not exist in the treebank.
Maximizing the Formula (4) is equivalent to
maximizing the following formula:
Figure 2: Projecting a Chinese dependency tree
to English side according to DPA. Solid arrows
are projected dependency arcs; dashed arrows are
missing dependency arcs.
</bodyText>
<subsectionHeader confidence="0.99941">
2.2 Unsupervised Training Algorithm
</subsectionHeader>
<bodyText confidence="0.974327210526316">
Algorithm 1 outlines the unsupervised training in
its entirety, where the treebank DE and unsuper-
vised parsing model with A are updated iteratively.
In line 1 we build a random treebank DE on
the monolingual corpus, and then train the parsing
Ă
model with it (line 2) through a training procedure
train(·, ·) which needs DE and eDE as classifica-
tion instances. From line 3-7, we train the unsu-
pervised model in self training iterative procedure,
where line 4-5 are similar to the E-step in EM al-
gorithm where calculates objective instead of ex-
pectation of 1-best tree (line 5) which is parsed
according to the parsing objective (Formula 3) by
parsing process parse(·, ·), and update the tree
bank with the tree. Similar to M-step in EM, the
algorithm maximizes the whole treebank’s unsu-
pervised objective (Formula 6) through the train-
ing procedure (line 6).
</bodyText>
<equation confidence="0.972527333333334">
YPE = arg max P r(+|deij)
3
DE deij EDE
Y
0(A) =
deEDE
deE
X
01(A) =
deEDE
X
+
(5)
log Pr(−|de)
log Pr(+|de)
</equation>
<figure confidence="0.586688">
Since the size of edges between DE and eDE is
3 Bilingual Projection of Dependency
Grammar
DE
deE
</figure>
<bodyText confidence="0.999317">
disproportionate, we use an empirical value to re-
duce the impact of the huge number of negative
instances:
</bodyText>
<equation confidence="0.992274571428571">
02(A) = X log Pr(+|de)
deEDE
X (6)
|DE|
+ log Pr(−|de)
 |eDE|
dE, i-
</equation>
<bodyText confidence="0.993514363636363">
where |x |is the size of x.
In this section, we introduce our projection objec-
tive and training algorithm which trains the model
with arc instances.
Because of the heterogeneity between dif-
ferent languages and word alignment errors, pro-
jection methods may contain a lot of noises. Take
Figure 2 as an example, following the Direct
Projection Algorithm (DPA) (Hwa et al., 2005)
(Section 5), the dependency relationships between
words can be directly projected from the source
</bodyText>
<page confidence="0.932849">
1065
</page>
<figure confidence="0.472838">
Algorithm 2 Training projection model
1: DP, DN ← proj(F, DF, A, E)
2: repeat ⊲ train(DP, DN)
3: 00 ← grad(DP, DN, 0(λ))
4: λ ← climb(0, 00, λ)
5: until maximization
</figure>
<bodyText confidence="0.993612">
language to the target language. Therefore, we
can hardly obtain a treebank with complete trees
through direct projection. So we extract projected
discrete dependency arc instances instead of tree-
bank as training set for the projected grammar in-
duction model.
</bodyText>
<subsectionHeader confidence="0.999627">
3.1 Projection Objective
</subsectionHeader>
<bodyText confidence="0.9999895">
Correspondingly, we select an objective which has
the same form with the unsupervised one:
</bodyText>
<equation confidence="0.999001">
O(λ) = �
d,∈DP (7)
+ � log Pr(−|de)
d,∈DN
</equation>
<bodyText confidence="0.9998405">
where DP is the positive dependency arc instance
set, which is obtained by direct projection methods
(Hwa et al., 2005; Jiang and Liu, 2010) and DN is
the negative one.
</bodyText>
<subsectionHeader confidence="0.999672">
3.2 Projection Algorithm
</subsectionHeader>
<bodyText confidence="0.9978766">
Basically, the training procedure in line 2,7 of Al-
gorithm 1 can be divided into smaller iterative
steps, and Algorithm 2 outlines the training step
of projection model with instances. F in Algo-
rithm 2 is source sentences in bilingual corpus,
and A is the alignments. Function grad(·, ·, ·)
gives the gradient (∇O) and the objective is op-
timized with a generic optimization step (such as
an LBFGS iteration (Zhu et al., 1997)) in the sub-
routine climb(·, ·, ·).
</bodyText>
<sectionHeader confidence="0.901615" genericHeader="method">
4 Bilingually-Guided Dependency
Grammar Induction
</sectionHeader>
<bodyText confidence="0.999992090909091">
This section presents our bilingually-guided gram-
mar induction model, which incorporates unsuper-
vised framework and bilingual projection model
through a joint approach.
According to following observation: unsuper-
vised induction model mines underlying syntactic
structure of the monolingual language, however, it
is hard to find good grammar induction in the ex-
ponential parsing space; bilingual projection ob-
tains relatively reliable syntactic knowledge of the
parsed counterpart, but it possibly contains a lot
of noises (e.g. Figure 2). We believe that unsu-
pervised model and projection model can comple-
ment each other and a joint model which takes bet-
ter use of both unsupervised parse trees and pro-
jected dependency arcs can give us a better parser.
Based on the idea, we propose a nov-
el strategy for training monolingual grammar in-
duction model with the guidance of unsuper-
vised and bilingually-projected dependency infor-
mation. Figure 1 outlines our bilingual-guided
grammar induction process in its entirety. In our
method, we select compatible objectives for unsu-
pervised and projection models, in order to they
can share the same grammar parameters. Then
we incorporate projection model into our iterative
unsupervised framework, and jointly optimize un-
supervised and projection objectives with evolv-
ing treebank and constant projection information
respectively. In this way, our bilingually-guided
model’s parameters are tuned to simultaneous-
ly maximizing both monolingual likelihood and
bilingually-projected likelihood by 4 steps:
</bodyText>
<listItem confidence="0.994350636363636">
1. Randomly build treebank on target sentences
for initialization, and get the projected arc in-
stances through projection from bitext.
2. Train the bilingually-guided grammar induc-
tion model by multi-objective optimization
method with unsupervised objective and pro-
jection objective on treebank and projected
arc instances respectively.
3. Use the parsing model to build new treebank
on target language for next iteration.
4. Repeat steps 1, 2 and 3 until convergence.
</listItem>
<bodyText confidence="0.999872846153846">
The unsupervised objective is optimized by the
loop—”tree bank→optimized model→new tree
bank”. The treebank is evolved for runs. The
unsupervised model gets projection constraint im-
plicitly from those parse trees which contain in-
formation from projection part. The projection ob-
jective is optimized by the circulation—”projected
instances→optimized model”, these projected in-
stances will not change once we get them.
The iterative procedure proposed here is not a
co-training algorithm (Sarkar, 2001; Hwa et al.,
2003), because the input of the projection objec-
tive is static.
</bodyText>
<equation confidence="0.889314">
log Pr(+|de)
</equation>
<page confidence="0.922267">
1066
</page>
<subsectionHeader confidence="0.981027">
4.1 Joint Objective
</subsectionHeader>
<bodyText confidence="0.8800044">
For multi-objective optimization method, we em-
ploy the classical weighted-sum approach which
just calculates the weighted linear sum of the ob-
jectives:
Algorithm 3 Training joint model
</bodyText>
<listItem confidence="0.852120428571429">
1: DP, DN +- proj(F, DF, A, E)
2: build random DE
3: λ +- train(DP, DN)
4: repeat
5: for each E E E do &gt; E step
6: DE +- parse(E, λ)
7: Vf(λ) +- grad(DE, DE, DP, DN, f(λ))
</listItem>
<table confidence="0.620525666666667">
� weightmobjm (8) 8: λ +-climb(f(λ), Vf(λ), λ) &gt; M step
OBJ = 9: until convergence
m
</table>
<bodyText confidence="0.9878735">
We combine the unsupervised objective (For-
mula (6)) and projection objective (Formula (7))
together through the weighted-sum approach in
Formula (8):
</bodyText>
<equation confidence="0.999764">
f(A) = αθ2(A) + (1 − α)φ(A) (9)
</equation>
<bodyText confidence="0.999993055555556">
where f(A) is our weight-sum objective. And α
is a mixing coefficient which reflects the relative
confidence between the unsupervised and projec-
tion objectives. Equally, α and (1−α) can be seen
as the weights in Formula (8). In that case, we can
use a single parameter α to control both weights
for different objective functions. When α = 1 it
is the unsupervised objective function in Formula
(6). Contrary, if α = 0, it is the projection objec-
tive function (Formula (7)) for projected instances.
With this approach, we can optimize the mixed
parsing model by maximizing the objective in For-
mula (9). Though the function (Formula (9)) is
an interpolation function, we use it for training
instead of parsing. In the parsing procedure, our
method calculates the probability of a dependency
arc according to the Formula (2), while the inter-
polating method calculates it by:
</bodyText>
<equation confidence="0.9996185">
Pr(y|dei,) =αPr1(y|dei,) (10)
+ (1 − α)Pr2(y|dei,)
</equation>
<bodyText confidence="0.999985">
where Pr1(y|dei,) and Pr2(y|dei,) are the proba-
bilities provided by different models.
</bodyText>
<subsectionHeader confidence="0.990397">
4.2 Training Algorithm
</subsectionHeader>
<bodyText confidence="0.999941666666667">
We optimize the objective (Formula (9)) via a
gradient-based search algorithm. And the gradi-
ent with respect to Ak takes the form:
</bodyText>
<equation confidence="0.999614">
∇f(Ak) = α∂θ2( ) + (1 − α)∂∂Ak a(k) (11)
</equation>
<bodyText confidence="0.999401363636364">
Algorithm 3 outlines our joint training proce-
dure, which tunes the grammar parameter A simul-
taneously maximize both unsupervised objective
and projection objective. And it incorporates un-
supervised framework and projection model algo-
rithm together. It is grounded on the work which
uses features in the unsupervised model (Berg-
Kirkpatrick et al., 2010).
In line 1, 2 we get projected dependency in-
stances from source side according to projec-
tion methods and build a random treebank (step
</bodyText>
<listItem confidence="0.954469727272727">
1). Then we train an initial model with projection
instances in line 3. From line 4-9, the objective is
optimized with a generic optimization step in the
subroutine climb(·, ·, ·, ·, ·). For each sentence we
parse its dependency tree, and update the tree into
the treebank (step 3). Then we calculate the gra-
dient and optimize the joint objective according to
the evolved treebank and projected instances (step
2). Lines 5-6 are equivalent to the E-step of the
EM algorithm, and lines 7-8 are equivalent to the
M-step.
</listItem>
<sectionHeader confidence="0.999876" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.99995755">
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) which
is based on POS tags. And DMV learns the gram-
mar via inside-outside re-estimation (Baker, 1979)
without any smoothing, while Spitkovsky et al.
(2010) utilizes smoothing and learning strategy
during grammar learning and William et al. (2009)
improves DMV with richer context.
The dependency projection method DPA (H-
wa et al., 2005) based on Direct Correspondence
Assumption (Hwa et al., 2002) can be described
as: if there is a pair of source words with a de-
pendency relationship, the corresponding aligned
words in target sentence can be considered as hav-
ing the same dependency relationship equivalent-
ly (e.g. Figure 2). The Word Pair Classification
(WPC) method (Jiang and Liu, 2010) modifies the
DPA method and makes it more robust. Smith
and Eisner (2009) propose an adaptation method
founded on quasi-synchronous grammar features
</bodyText>
<page confidence="0.984714">
1067
</page>
<table confidence="0.991122538461539">
Type Feature Template
Unigram wordi posi wordi ◦ posi
wordj posj wordj ◦ posj
Bigram wordi ◦ posj wordj ◦ posi posi ◦ posj
wordi ◦ wordj wordi ◦ posi ◦ wordj wordi ◦ wordj ◦ posj
wordi ◦ posi ◦ posj posi ◦ wordj ◦ posj
wordi ◦ posi ◦ wordj ◦ posj
Surrounding posi−1 ◦ posi ◦ posj posi ◦ posi+1 ◦ posj posi ◦ posj−1 ◦ posj
posi ◦ posj ◦ posj+1 posi−1 ◦ posi ◦ posj−1 posi ◦ posi+1 ◦ posj+1
posi−1 ◦ posj−1 ◦ posj posi+1 ◦ posj ◦ posj+1 posi−1 ◦ posi ◦ posj+1
posi ◦ posi+1 ◦ posj−1 posi−1 ◦ posj ◦ posj+1 posi+1 ◦ posj−1 ◦ posj
posi−1 ◦ posi ◦ posj−1 ◦ posj posi ◦ posi+1 ◦ posj ◦ posj+1
posi ◦ posi+1 ◦ posj−1 ◦ posj posi−1 ◦ posi ◦ posj ◦ posj+1
</table>
<tableCaption confidence="0.911924">
Table 1: Feature templates for dependency parsing. For edge deij: wordi is the parent word and wordj
is the child word, similar to ”pos”. ”+1” denotes the preceding token of the sentence, similar to ”-1”.
</tableCaption>
<bodyText confidence="0.999692230769231">
for dependency projection and annotation, which
requires a small set of dependency annotated cor-
pus of target language.
Similarly, using indirect information from mul-
tilingual (Cohen et al., 2011; T¨ackstr¨om et al.,
2012) is an effective way to improve unsupervised
parsing. (Zeman and Resnik, 2008; McDonald et
al., 2011; Søgaard, 2011) employ non-lexicalized
parser trained on other languages to process a
target language. McDonald et al. (2011) adapts
their multi-source parser according to DCA, while
Naseem et al. (2012) selects a selective sharing
model to make better use of grammar information
in multi-sources.
Due to similar reasons, many works are devoted
to POS projection (Yarowsky et al., 2001; Shen et
al., 2007; Naseem et al., 2009), and they also suf-
fer from similar problems. Some seek for unsu-
pervised methods, e.g. Naseem et al. (2009), and
some further improve the projection by a graph-
based projection (Das and Petrov, 2011).
Our model differs from the approaches above
in its emphasis on utilizing information from both
sides of bilingual corpus in an unsupervised train-
ing framework, while most of the work above only
utilize the information from a single side.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999738">
In this section, we evaluate the performance of the
MST dependency parser (McDonald et al., 2005b)
which is trained by our bilingually-guided model
on 5 languages. And the features used in our ex-
periments are summarized in Table 1.
</bodyText>
<subsectionHeader confidence="0.992773">
6.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.993650073170732">
Datasets and Evaluation Our experiments are
run on five different languages: Chinese(ch),
Danish(da), Dutch(nl), Portuguese(pt) and
Swedish(sv) (da, nl, pt and sv are free data sets
distributed for the 2006 CoNLL Shared Tasks
(Buchholz and Marsi, 2006)). For all languages,
we only use English-target parallel data: we take
the FBIS English-Chinese bitext as bilingual cor-
pus for English-Chinese dependency projection
which contains 239K sentence pairs with about
8.9M/6.9M words in English/Chinese, and for
other languages we use the readily available data
in the Europarl corpus. Then we run tests on the
Penn Chinese Treebank (CTB) and CoNLL-X test
sets.
English sentences are tagged by the implemen-
tations of the POS tagger of Collins (2002), which
is trained on WSJ. The source sentences are then
parsed by an implementation of 2nd-ordered MST
model of McDonald and Pereira (2006), which is
trained on dependency trees extracted from Penn
Treebank.
As the evaluation metric, we use parsing accu-
racy which is the percentage of the words which
have found their correct parents. We evaluate on
sentences with all length for our method.
Training Regime In experiments, we use the
projection method proposed by Jiang and Liu
(2010) to provide the projection instances. And
we train the projection part α = 0 first for initial-
ization, on which the whole model will be trained.
Availing of the initialization method, the model
can converge very fast (about 3 iterations is suffi-
cient) and the results are more stable than the ones
trained on random initialization.
Baselines We compare our method against
three kinds of different approaches: unsupervised
method (Klein and Manning, 2004); single-
source direct projection methods (Hwa et al.,
2005; Jiang and Liu, 2010); multi-source in-
direct projection methods with multi-sources (M-
</bodyText>
<page confidence="0.962062">
1068
</page>
<figure confidence="0.99556">
60.5
nl
59.5
74.5
70.5 pt
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
alpha
</figure>
<figureCaption confidence="0.910243666666667">
Figure 3: The performance of our model with re-
spect to a series of ratio a
cDonald et al., 2011; Naseem et al., 2012).
</figureCaption>
<subsectionHeader confidence="0.74203">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.997314964285715">
We test our method on CTB and CoNLL-X free
test data sets respectively, and the performance is
summarized in Table 2. Figure 3 presents the per-
formance with different a on different languages.
Compare against Unsupervised Baseline Ex-
perimental results show that our unsupervised
framework’s performance approaches to the DMV
method. And the bilingually-guided model can
promote the unsupervised method consisten-
cy over all languages. On the best results’ aver-
age of four comparable languages (da, nl, pt, sv),
the promotion gained by our model is 28.5% over
the baseline method (DMV) (Klein and Manning,
2004).
Compare against Projection Baselines For
all languages, the model consistent-
ly outperforms on direct projection baseline.
On the average of each language’s best result, our
model outperforms all kinds of baselines, yielding
3.0% gain over the single-source direct-projection
method (Jiang and Liu, 2010) and 3.9% gain over
the multi-source indirect-projection method (Mc-
Donald et al., 2011). On the average of all results
with different parameters, our method also gain-
s more than 2.0% improvements on all baselines.
Particularly, our model achieves the most signif-
icant gains on Chinese, where the improvements
are 4.5%/12.0% on direct/indirect projection base-
</bodyText>
<table confidence="0.9980154">
Accuracy%
Model ch da nl pt sv avg
DMV 42.5∗ 33.4 38.5 20.1 44.0 —.–
DPA 53.9 —.– —.– —.– —.– —.–
WPC 56.8 50.1 58.4 70.5 60.8 59.3
Transfer 49.3 49.5 53.9 75.8 63.6 58.4
Selective 51.2 —.– 55.9 73.5 61.5 —.–
unsuper 22.6 41.6 15.2 45.7 42.4 33.5
avg 61.0 50.7 59.9 72.0 63.1 61.3
max 61.3 51.1 60.1 74.2 64.6 62.3
</table>
<tableCaption confidence="0.998061">
Table 2: The directed dependency accuracy with
</tableCaption>
<bodyText confidence="0.987619272727273">
different parameter of our model and the base-
lines. The first section of the table (row 3-7)
shows the results of the baselines: a unsupervised
method baseline (Klein and Manning, 2004)(D-
MV); a single-source projection method baseline
(Hwa et al., 2005) (DPA) and its improve-
ment (Jiang and Liu, 2010)(WPC); two multi-
source baselines (McDonald et al., 2011)(Trans-
fer) and (Naseem et al., 2012)(Selective). The
second section of the table (row 8) presents the
result of our unsupervised framework (unsuper).
The third section gives the mean value (avg) and
maximum value (max) of our model with different
a in Figure 3.
*: The result is based on sentences with 10
words or less after the removal of punctuation, it
is an incomparable result.
lines.
The results in Figure 3 prove that our unsuper-
vised framework a = 1 can promote the grammar
induction if it has a good start (well initialization),
and it will be better once we incorporate the infor-
mation from the projection side (a = 0.9). And
the maximum points are not in a = 1, which im-
plies that projection information is still available
for the unsupervised framework even if we employ
the projection model as the initialization. So we
suggest that a greater parameter is a better choice
for our model. And there are some random factors
in our model which make performance curves with
more fluctuation. And there is just a little improve-
ment shown in da, in which the same situation is
observed by (McDonald et al., 2011).
</bodyText>
<subsectionHeader confidence="0.999202">
6.3 Effects of the Size of Training Corpus
</subsectionHeader>
<bodyText confidence="0.999255">
To investigate how the size of the training corpus
influences the result, we train the model on ex-
tracted bilingual corpus with varying sizes: 10K,
50K, 100K, 150K and 200K sentences pairs.
As shown in Figure 4, our approach continu-
</bodyText>
<figure confidence="0.9919328">
61.5
60.0
da
51.2
50.3
ch
65.0
61.5
sv
accuracy%
</figure>
<page confidence="0.740635">
1069
</page>
<figureCaption confidence="0.9956525">
Figure 4: Performance on varying sizes (average
of 5 languages, α = 0.9)
</figureCaption>
<figure confidence="0.995632">
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
noise rate
</figure>
<figureCaption confidence="0.92030675">
Figure 5: Performance on different projection
quality (average of 5 languages, α = 0.9). The
noise rate is the percentage of the projected in-
stances being messed up.
</figureCaption>
<bodyText confidence="0.998603285714286">
ously outperforms the baseline with the increasing
size of training corpus. It is especially noteworthy
that the more training data is utilized the more su-
periority our model enjoys. That is, because our
method not only utilizes the projection informa-
tion but also avails itself of the monolingual cor-
pus.
</bodyText>
<subsectionHeader confidence="0.999895">
6.4 Effect of Projection Quality
</subsectionHeader>
<bodyText confidence="0.999987416666667">
The projection quality can be influenced by the
quality of the source parsing, alignments, projec-
tion methods, corpus quality and many other fac-
tors. In order to detect the effects of varying pro-
jection qualities on our approach, we simulate the
complex projection procedure by messing up the
projected instances randomly with different noise
rates. The curves in Figure 5 show the perfor-
mance of WPC baseline and our bilingual-guided
method. For different noise rates, our model’s re-
sults consistently outperform the baselines. When
the noise rate is greater than 0.2, our improvement
</bodyText>
<figure confidence="0.9454735">
0 0.02 0.04 0.06 0.08 0.1 ... 0.2 ... 0.3
alpha
</figure>
<figureCaption confidence="0.9616955">
Figure 6: The performance curve of our model
(random initialization) on Chinese, with respect to
a series of ratio α. The baseline is the result of
WPC model.
</figureCaption>
<bodyText confidence="0.995256333333333">
increases with the growth of the noise rate. The re-
sult suggests that our method can solve some prob-
lems which are caused by projection noise.
</bodyText>
<subsectionHeader confidence="0.997308">
6.5 Performance on Random Initialization
</subsectionHeader>
<bodyText confidence="0.999978307692308">
We test our model with random initialization on
different α. The curve in Figure 6 shows the per-
formance of our model on Chinese.
The results seem supporting our unsupervised
optimization method when α is in the range of
(0, 0.1). It implies that the unsupervised structure
information is useful, but it seems creating a nega-
tive effect on the model when α is greater than 0.1.
Because the unsupervised part can gain constraints
from the projection part. But with the increase of
α, the strength of constraint dwindles, and the
unsupervised part will gradually lose control. And
bad unsupervised part pulls the full model down.
</bodyText>
<sectionHeader confidence="0.99636" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999856266666667">
This paper presents a bilingually-guided strate-
gy for automatic dependency grammar induction,
which adopts an unsupervised skeleton and lever-
ages the bilingually-projected dependency infor-
mation during optimization. By simultaneous-
ly maximizing the monolingual likelihood and
bilingually-projected likelihood in the EM proce-
dure, it effectively integrates the advantages of
bilingual projection and unsupervised induction.
Experiments on 5 languages show that the novel
strategy significantly outperforms previous unsu-
pervised or bilingually-projected models.
Since its computational complexity approaches to
the skeleton unsupervised model (with much few-
er iterations), and the bilingual text aligned to
</bodyText>
<figure confidence="0.998784777777778">
10K 50K 100K 150K 200K
size of training set
accuracy%
63
62
61
60
59
58
57
56
55
54
53
our model
baseline
accuracy%
63
62
61
60
59
58
57
56
55
54
53
52
51
our model
baseline
60.2
59.8
59.4
59.0
58.6
58.2
...
54.6
...
49.5
our model
baseline(58.5)
accuracy%
</figure>
<page confidence="0.978276">
1070
</page>
<bodyText confidence="0.999873125">
resource-rich languages is easy to obtain, such a
hybrid method seems to be a better choice for au-
tomatic grammar induction. It also indicates that
the combination of bilingual constraint and unsu-
pervised methodology has a promising prospect
for grammar induction. In the future work we will
investigate such kind of strategies, such as bilin-
gually unsupervised induction.
</bodyText>
<sectionHeader confidence="0.998089" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999664076923077">
The authors were supported by National
Natural Science Foundation of China, Con-
tracts 61202216, 863 State Key Project (No.
2011AA01A207), and National Key Technology
R&amp;D Program (No. 2012BAH39B03), Key
Project of Knowledge Innovation Program of Chi-
nese Academy of Sciences (No. KGZD-EW-501).
Qun Liu’s work is partially supported by Science
Foundation Ireland (Grant No.07/CE/I1142) as
part of the CNGL at Dublin City University. We
would like to thank the anonymous reviewers for
their insightful comments and those who helped
to modify the paper.
</bodyText>
<sectionHeader confidence="0.998924" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849532467532">
H. Alshawi. 1996. Head automata for speech transla-
tion. In Proc. ofICSLP.
James K Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society
ofAmerica, 65:S132.
T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In HLT: NAACL, pages 582–590.
Rens Bod. 2006. An all-subtrees approach to unsu-
pervised parsing. In Proc. of the 21st ICCL and the
44th ACL, pages 865–872.
S. Buchholz and E. Marsi. 2006. Conll-x shared task
on multilingual dependency parsing. In Proc. of the
2002 Conference on EMNLP. Proc. CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative r-
eranking. In Proc. of the 43rd ACL, pages 173–180,
Ann Arbor, Michigan, June.
W. Chen, J. Kazama, and K. Torisawa. 2010. Bi-
text dependency parsing with bilingual subtree con-
straints. In Proc. ofACL, pages 21–29.
S.B. Cohen, D. Das, and N.A. Smith. 2011. Unsu-
pervised structure prediction with non-parallel mul-
tilingual guidance. In Proc. of the Conference on
EMNLP, pages 50–61.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proc. of the
2002 Conference on EMNLP, pages 1–8, July.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. In Computational
Linguistics.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proc. ofACL.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Proc. ofIJCNLP of the AFNLP: Vol-
ume 1-Volume 1, pages 369–377.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proc. ofACL, pages 392–399.
R. Hwa, M. Osborne, A. Sarkar, and M. Steedman.
2003. Corrected co-training for statistical parsers.
In ICML-03 Workshop on the Continuum from La-
beled to Unlabeled Data in Machine Learning and
Data Mining, Washington DC.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural language
engineering, 11(3):311–325.
W. Jiang and Q. Liu. 2010. Dependency parsing
and projection based on word-pair classification. In
Proc. ofACL, pages 12–20.
D. Klein and C.D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependen-
cy and constituency. In Proc. ofACL, page 478.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of the 48th ACL,
pages 1–11, July.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. pages 595–
603.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of the 11th Conf. of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. ofACL, pages 91–98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc.
2005b. Non-projective dependency parsing using s-
panning tree algorithms. In Proc. of EMNLP, pages
523–530.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proc. of CoNLL, pages 216–
220.
</reference>
<page confidence="0.859286">
1071
</page>
<reference confidence="0.99988503030303">
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In Proc. ofEMNLP, pages 62–72. ACL.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. Journal of Artificial Intelli-
gence Research, 36(1):341–385.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proc. of the 50th ACL, pages 629–637,
July.
J. Nivre, J. Hall, J. Nilsson, G. Eryi˜git, and S. Mari-
nov. 2006. Labeled pseudo-projective dependency
parsing with support vector machines. In Proc. of
CoNLL, pages 221–225.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95–135.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st ICCL
&amp; 44th ACL, pages 433–440, July.
A. Sarkar. 2001. Applying co-training methods to sta-
tistical parsing. In Proc. ofNAACL, pages 1–8.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learning
for bidirectional sequence classification. In Annual
Meeting-, volume 45, page 760.
N.A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proc. ofACL, pages 354–362.
D.A. Smith and J. Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous gram-
mar features. In Proc. ofEMNLP: Volume 2-Volume
2, pages 822–831.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsu-
pervised multilingual grammar induction. In Proc.
ofIJCNLP of the AFNLP: Volume 1-Volume 1, pages
73–81.
Anders Søgaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Proc.
of the 49th ACL: HLT, pages 682–686.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010. From baby steps to leapfrog: How
“less is more” in unsupervised dependency parsing.
In HLT: NAACL, pages 751–759, June.
O. T¨ackstr¨om, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure.
William, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with rich-
er contexts and smoothing. In Proc. of NAACL,
pages 101–109.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proc. of HLT,
pages 1–8.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proc. of the IJCNLP-08. Proc. CoNLL.
Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge
Nocedal. 1997. Algorithm 778: L-bfgs-b: Fortran
subroutines for large-scale bound-constrained opti-
mization. ACM Transactions on Mathematical Soft-
ware (TOMS), 23(4):550–560.
</reference>
<page confidence="0.996097">
1072
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.199760">
<title confidence="0.999676">Bilingually-Guided Monolingual Dependency Grammar Induction</title>
<author confidence="0.972974">Yajuan Wenbin Qun</author>
<affiliation confidence="0.9490545">Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of</affiliation>
<address confidence="0.700899">P.O. Box 2704, Beijing 100190,</address>
<title confidence="0.412559">for Next Generation Faculty of Engineering and Computing, Dublin City of Chinese Academy of Sciences</title>
<abstract confidence="0.998897875">This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency grammar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant of the unsuperbaseline and the best projection baseline on average.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata for speech translation.</title>
<date>1996</date>
<booktitle>In Proc. ofICSLP.</booktitle>
<contexts>
<context position="17756" citStr="Alshawi, 1996" startWordPosition="2810" endWordPosition="2811">en we train an initial model with projection instances in line 3. From line 4-9, the objective is optimized with a generic optimization step in the subroutine climb(·, ·, ·, ·, ·). For each sentence we parse its dependency tree, and update the tree into the treebank (step 3). Then we calculate the gradient and optimize the joint objective according to the evolved treebank and projected instances (step 2). Lines 5-6 are equivalent to the E-step of the EM algorithm, and lines 7-8 are equivalent to the M-step. 5 Related work The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) which is based on POS tags. And DMV learns the grammar via inside-outside re-estimation (Baker, 1979) without any smoothing, while Spitkovsky et al. (2010) utilizes smoothing and learning strategy during grammar learning and William et al. (2009) improves DMV with richer context. The dependency projection method DPA (Hwa et al., 2005) based on Direct Correspondence Assumption (Hwa et al., 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship e</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head automata for speech translation. In Proc. ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<journal>The Journal of the Acoustical Society ofAmerica,</journal>
<pages>65--132</pages>
<contexts>
<context position="17858" citStr="Baker, 1979" startWordPosition="2827" endWordPosition="2828">zed with a generic optimization step in the subroutine climb(·, ·, ·, ·, ·). For each sentence we parse its dependency tree, and update the tree into the treebank (step 3). Then we calculate the gradient and optimize the joint objective according to the evolved treebank and projected instances (step 2). Lines 5-6 are equivalent to the E-step of the EM algorithm, and lines 7-8 are equivalent to the M-step. 5 Related work The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) which is based on POS tags. And DMV learns the grammar via inside-outside re-estimation (Baker, 1979) without any smoothing, while Spitkovsky et al. (2010) utilizes smoothing and learning strategy during grammar learning and William et al. (2009) improves DMV with richer context. The dependency projection method DPA (Hwa et al., 2005) based on Direct Correspondence Assumption (Hwa et al., 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently (e.g. Figure 2). The Word Pair Classification (WPC) method (Jiang and Liu, 2010) modifies </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K Baker. 1979. Trainable grammars for speech recognition. The Journal of the Acoustical Society ofAmerica, 65:S132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>A Bouchard-Cˆot´e</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In HLT: NAACL,</booktitle>
<pages>582--590</pages>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero, and D. Klein. 2010. Painless unsupervised learning with features. In HLT: NAACL, pages 582–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An all-subtrees approach to unsupervised parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st ICCL and the 44th ACL,</booktitle>
<pages>865--872</pages>
<contexts>
<context position="2574" citStr="Bod, 2006" startWordPosition="373" endWordPosition="374">lein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smi</context>
<context position="6971" citStr="Bod, 2006" startWordPosition="1010" endWordPosition="1011">4 we present the bilingually-guided induction strategy for dependency grammar (where the two objectives above are jointly optimized, as shown in Figure 1). After giving a brief introduction of previous work in section 5, we finally give the experimental results in section 6 and conclude our work in section 7. 2 Unsupervised Dependency Grammar Induction In this section, we introduce the unsupervised objective and the unsupervised training algorithm which is used as the framework of our bilinguallyguided method. Unlike previous unsupervised work (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006), we select a self-training approach (similar to hard EM method) to train the unsupervised model. And the framework of our unsupervised model builds a random treebank on the monolingual corpus firstly for initialization and trains a discriminative parsing model on it. Then we use the parser to build an evolved treebank with the 1-best result for the next iteration run. In this way, the parser and treebank evolve in an iterative way until convergence. Let’s introduce the parsing objective firstly: Define ez as the ith word in monolingual sentence E; deij denotes the word pair dependency relatio</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod. 2006. An all-subtrees approach to unsupervised parsing. In Proc. of the 21st ICCL and the 44th ACL, pages 865–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the 2002 Conference on EMNLP. Proc. CoNLL.</booktitle>
<contexts>
<context position="21161" citStr="Buchholz and Marsi, 2006" startWordPosition="3390" endWordPosition="3393">ervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is trained by our bilingually-guided model on 5 languages. And the features used in our experiments are summarized in Table 1. 6.1 Experiment Setup Datasets and Evaluation Our experiments are run on five different languages: Chinese(ch), Danish(da), Dutch(nl), Portuguese(pt) and Swedish(sv) (da, nl, pt and sv are free data sets distributed for the 2006 CoNLL Shared Tasks (Buchholz and Marsi, 2006)). For all languages, we only use English-target parallel data: we take the FBIS English-Chinese bitext as bilingual corpus for English-Chinese dependency projection which contains 239K sentence pairs with about 8.9M/6.9M words in English/Chinese, and for other languages we use the readily available data in the Europarl corpus. Then we run tests on the Penn Chinese Treebank (CTB) and CoNLL-X test sets. English sentences are tagged by the implementations of the POS tagger of Collins (2002), which is trained on WSJ. The source sentences are then parsed by an implementation of 2nd-ordered MST mod</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proc. of the 2002 Conference on EMNLP. Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd ACL,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1530" citStr="Charniak and Johnson, 2005" startWordPosition="200" endWordPosition="203">rvised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency grammar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average. 1 Introduction In past decades supervised methods achieved the state-of-the-art in constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proc. of the 43rd ACL, pages 173–180, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chen</author>
<author>J Kazama</author>
<author>K Torisawa</author>
</authors>
<title>Bitext dependency parsing with bilingual subtree constraints.</title>
<date>2010</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>21--29</pages>
<contexts>
<context position="2227" citStr="Chen et al., 2010" startWordPosition="316" endWordPosition="319"> et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower pe</context>
</contexts>
<marker>Chen, Kazama, Torisawa, 2010</marker>
<rawString>W. Chen, J. Kazama, and K. Torisawa. 2010. Bitext dependency parsing with bilingual subtree constraints. In Proc. ofACL, pages 21–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Unsupervised structure prediction with non-parallel multilingual guidance.</title>
<date>2011</date>
<booktitle>In Proc. of the Conference on EMNLP,</booktitle>
<pages>50--61</pages>
<contexts>
<context position="19650" citStr="Cohen et al., 2011" startWordPosition="3149" endWordPosition="3152">osj+1 posi−1 ◦ posi ◦ posj+1 posi ◦ posi+1 ◦ posj−1 posi−1 ◦ posj ◦ posj+1 posi+1 ◦ posj−1 ◦ posj posi−1 ◦ posi ◦ posj−1 ◦ posj posi ◦ posi+1 ◦ posj ◦ posj+1 posi ◦ posi+1 ◦ posj−1 ◦ posj posi−1 ◦ posi ◦ posj ◦ posj+1 Table 1: Feature templates for dependency parsing. For edge deij: wordi is the parent word and wordj is the child word, similar to ”pos”. ”+1” denotes the preceding token of the sentence, similar to ”-1”. for dependency projection and annotation, which requires a small set of dependency annotated corpus of target language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. </context>
</contexts>
<marker>Cohen, Das, Smith, 2011</marker>
<rawString>S.B. Cohen, D. Das, and N.A. Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proc. of the Conference on EMNLP, pages 50–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of the 2002 Conference on EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="21654" citStr="Collins (2002)" startWordPosition="3469" endWordPosition="3470"> Swedish(sv) (da, nl, pt and sv are free data sets distributed for the 2006 CoNLL Shared Tasks (Buchholz and Marsi, 2006)). For all languages, we only use English-target parallel data: we take the FBIS English-Chinese bitext as bilingual corpus for English-Chinese dependency projection which contains 239K sentence pairs with about 8.9M/6.9M words in English/Chinese, and for other languages we use the readily available data in the Europarl corpus. Then we run tests on the Penn Chinese Treebank (CTB) and CoNLL-X test sets. English sentences are tagged by the implementations of the POS tagger of Collins (2002), which is trained on WSJ. The source sentences are then parsed by an implementation of 2nd-ordered MST model of McDonald and Pereira (2006), which is trained on dependency trees extracted from Penn Treebank. As the evaluation metric, we use parsing accuracy which is the percentage of the words which have found their correct parents. We evaluate on sentences with all length for our method. Training Regime In experiments, we use the projection method proposed by Jiang and Liu (2010) to provide the projection instances. And we train the projection part α = 0 first for initialization, on which th</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. of the 2002 Conference on EMNLP, pages 1–8, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="1502" citStr="Collins, 2003" startWordPosition="198" endWordPosition="199">tion and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency grammar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average. 1 Introduction In past decades supervised methods achieved the state-of-the-art in constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw </context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>S Petrov</author>
</authors>
<title>Unsupervised part-ofspeech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="20402" citStr="Das and Petrov, 2011" startWordPosition="3270" endWordPosition="3273">gaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petrov, 2011). Our model differs from the approaches above in its emphasis on utilizing information from both sides of bilingual corpus in an unsupervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is trained by our bilingually-guided model on 5 languages. And the features used in our experiments are summarized in Table 1. 6.1 Experiment Setup Datasets and Evaluation Our experiments are run on five different languages: Chinese(ch), Dan</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>D. Das and S. Petrov. 2011. Unsupervised part-ofspeech tagging with bilingual graph-based projections. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Proc. ofIJCNLP of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>369--377</pages>
<contexts>
<context position="2411" citStr="Ganchev et al., 2009" startWordPosition="348" endWordPosition="351"> and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for language</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>K. Ganchev, J. Gillenwater, and B. Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proc. ofIJCNLP of the AFNLP: Volume 1-Volume 1, pages 369–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
<author>P Resnik</author>
<author>A Weinberg</author>
<author>O Kolak</author>
</authors>
<title>Evaluating translational correspondence using annotation projection.</title>
<date>2002</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>392--399</pages>
<contexts>
<context position="18154" citStr="Hwa et al., 2002" startWordPosition="2870" endWordPosition="2873">ces (step 2). Lines 5-6 are equivalent to the E-step of the EM algorithm, and lines 7-8 are equivalent to the M-step. 5 Related work The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) which is based on POS tags. And DMV learns the grammar via inside-outside re-estimation (Baker, 1979) without any smoothing, while Spitkovsky et al. (2010) utilizes smoothing and learning strategy during grammar learning and William et al. (2009) improves DMV with richer context. The dependency projection method DPA (Hwa et al., 2005) based on Direct Correspondence Assumption (Hwa et al., 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently (e.g. Figure 2). The Word Pair Classification (WPC) method (Jiang and Liu, 2010) modifies the DPA method and makes it more robust. Smith and Eisner (2009) propose an adaptation method founded on quasi-synchronous grammar features 1067 Type Feature Template Unigram wordi posi wordi ◦ posi wordj posj wordj ◦ posj Bigram wordi ◦ posj wordj ◦ posi posi ◦ posj wordi ◦ wordj wordi ◦ posi ◦</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Kolak, 2002</marker>
<rawString>R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002. Evaluating translational correspondence using annotation projection. In Proc. ofACL, pages 392–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
<author>M Osborne</author>
<author>A Sarkar</author>
<author>M Steedman</author>
</authors>
<title>Corrected co-training for statistical parsers.</title>
<date>2003</date>
<booktitle>In ICML-03 Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining,</booktitle>
<location>Washington DC.</location>
<contexts>
<context position="14723" citStr="Hwa et al., 2003" startWordPosition="2292" endWordPosition="2295"> new treebank on target language for next iteration. 4. Repeat steps 1, 2 and 3 until convergence. The unsupervised objective is optimized by the loop—”tree bank→optimized model→new tree bank”. The treebank is evolved for runs. The unsupervised model gets projection constraint implicitly from those parse trees which contain information from projection part. The projection objective is optimized by the circulation—”projected instances→optimized model”, these projected instances will not change once we get them. The iterative procedure proposed here is not a co-training algorithm (Sarkar, 2001; Hwa et al., 2003), because the input of the projection objective is static. log Pr(+|de) 1066 4.1 Joint Objective For multi-objective optimization method, we employ the classical weighted-sum approach which just calculates the weighted linear sum of the objectives: Algorithm 3 Training joint model 1: DP, DN +- proj(F, DF, A, E) 2: build random DE 3: λ +- train(DP, DN) 4: repeat 5: for each E E E do &gt; E step 6: DE +- parse(E, λ) 7: Vf(λ) +- grad(DE, DE, DP, DN, f(λ)) � weightmobjm (8) 8: λ +-climb(f(λ), Vf(λ), λ) &gt; M step OBJ = 9: until convergence m We combine the unsupervised objective (Formula (6)) and proje</context>
</contexts>
<marker>Hwa, Osborne, Sarkar, Steedman, 2003</marker>
<rawString>R. Hwa, M. Osborne, A. Sarkar, and M. Steedman. 2003. Corrected co-training for statistical parsers. In ICML-03 Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
<author>P Resnik</author>
<author>A Weinberg</author>
<author>C Cabezas</author>
<author>O Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts. Natural language engineering,</title>
<date>2005</date>
<pages>11--3</pages>
<contexts>
<context position="2388" citStr="Hwa et al., 2005" startWordPosition="344" endWordPosition="347">ver, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising </context>
<context position="10800" citStr="Hwa et al., 2005" startWordPosition="1687" endWordPosition="1690">n DE and eDE is 3 Bilingual Projection of Dependency Grammar DE deE disproportionate, we use an empirical value to reduce the impact of the huge number of negative instances: 02(A) = X log Pr(+|de) deEDE X (6) |DE| + log Pr(−|de) |eDE| dE, iwhere |x |is the size of x. In this section, we introduce our projection objective and training algorithm which trains the model with arc instances. Because of the heterogeneity between different languages and word alignment errors, projection methods may contain a lot of noises. Take Figure 2 as an example, following the Direct Projection Algorithm (DPA) (Hwa et al., 2005) (Section 5), the dependency relationships between words can be directly projected from the source 1065 Algorithm 2 Training projection model 1: DP, DN ← proj(F, DF, A, E) 2: repeat ⊲ train(DP, DN) 3: 00 ← grad(DP, DN, 0(λ)) 4: λ ← climb(0, 00, λ) 5: until maximization language to the target language. Therefore, we can hardly obtain a treebank with complete trees through direct projection. So we extract projected discrete dependency arc instances instead of treebank as training set for the projected grammar induction model. 3.1 Projection Objective Correspondingly, we select an objective which</context>
<context position="18093" citStr="Hwa et al., 2005" startWordPosition="2860" endWordPosition="2864">ective according to the evolved treebank and projected instances (step 2). Lines 5-6 are equivalent to the E-step of the EM algorithm, and lines 7-8 are equivalent to the M-step. 5 Related work The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) which is based on POS tags. And DMV learns the grammar via inside-outside re-estimation (Baker, 1979) without any smoothing, while Spitkovsky et al. (2010) utilizes smoothing and learning strategy during grammar learning and William et al. (2009) improves DMV with richer context. The dependency projection method DPA (Hwa et al., 2005) based on Direct Correspondence Assumption (Hwa et al., 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently (e.g. Figure 2). The Word Pair Classification (WPC) method (Jiang and Liu, 2010) modifies the DPA method and makes it more robust. Smith and Eisner (2009) propose an adaptation method founded on quasi-synchronous grammar features 1067 Type Feature Template Unigram wordi posi wordi ◦ posi wordj posj wordj ◦ posj Bigram wordi</context>
<context position="22652" citStr="Hwa et al., 2005" startWordPosition="3629" endWordPosition="3632">ur method. Training Regime In experiments, we use the projection method proposed by Jiang and Liu (2010) to provide the projection instances. And we train the projection part α = 0 first for initialization, on which the whole model will be trained. Availing of the initialization method, the model can converge very fast (about 3 iterations is sufficient) and the results are more stable than the ones trained on random initialization. Baselines We compare our method against three kinds of different approaches: unsupervised method (Klein and Manning, 2004); singlesource direct projection methods (Hwa et al., 2005; Jiang and Liu, 2010); multi-source indirect projection methods with multi-sources (M1068 60.5 nl 59.5 74.5 70.5 pt 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 alpha Figure 3: The performance of our model with respect to a series of ratio a cDonald et al., 2011; Naseem et al., 2012). 6.2 Results We test our method on CTB and CoNLL-X free test data sets respectively, and the performance is summarized in Table 2. Figure 3 presents the performance with different a on different languages. Compare against Unsupervised Baseline Experimental results show that our unsupervised framework’s performance appro</context>
<context position="24831" citStr="Hwa et al., 2005" startWordPosition="3990" endWordPosition="3993">seAccuracy% Model ch da nl pt sv avg DMV 42.5∗ 33.4 38.5 20.1 44.0 —.– DPA 53.9 —.– —.– —.– —.– —.– WPC 56.8 50.1 58.4 70.5 60.8 59.3 Transfer 49.3 49.5 53.9 75.8 63.6 58.4 Selective 51.2 —.– 55.9 73.5 61.5 —.– unsuper 22.6 41.6 15.2 45.7 42.4 33.5 avg 61.0 50.7 59.9 72.0 63.1 61.3 max 61.3 51.1 60.1 74.2 64.6 62.3 Table 2: The directed dependency accuracy with different parameter of our model and the baselines. The first section of the table (row 3-7) shows the results of the baselines: a unsupervised method baseline (Klein and Manning, 2004)(DMV); a single-source projection method baseline (Hwa et al., 2005) (DPA) and its improvement (Jiang and Liu, 2010)(WPC); two multisource baselines (McDonald et al., 2011)(Transfer) and (Naseem et al., 2012)(Selective). The second section of the table (row 8) presents the result of our unsupervised framework (unsuper). The third section gives the mean value (avg) and maximum value (max) of our model with different a in Figure 3. *: The result is based on sentences with 10 words or less after the removal of punctuation, it is an incomparable result. lines. The results in Figure 3 prove that our unsupervised framework a = 1 can promote the grammar induction if </context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural language engineering, 11(3):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Jiang</author>
<author>Q Liu</author>
</authors>
<title>Dependency parsing and projection based on word-pair classification.</title>
<date>2010</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>12--20</pages>
<contexts>
<context position="2969" citStr="Jiang and Liu, 2010" startWordPosition="430" endWordPosition="433">e to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across bilingual texts or indirectly across multilingual texts (Snyder et al., 2009; McDonald et al., 2011; Naseem et al., 2012), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages. Unsupervised induction and bilingual projection run according to totally different princ</context>
<context position="11628" citStr="Jiang and Liu, 2010" startWordPosition="1826" endWordPosition="1829">ad(DP, DN, 0(λ)) 4: λ ← climb(0, 00, λ) 5: until maximization language to the target language. Therefore, we can hardly obtain a treebank with complete trees through direct projection. So we extract projected discrete dependency arc instances instead of treebank as training set for the projected grammar induction model. 3.1 Projection Objective Correspondingly, we select an objective which has the same form with the unsupervised one: O(λ) = � d,∈DP (7) + � log Pr(−|de) d,∈DN where DP is the positive dependency arc instance set, which is obtained by direct projection methods (Hwa et al., 2005; Jiang and Liu, 2010) and DN is the negative one. 3.2 Projection Algorithm Basically, the training procedure in line 2,7 of Algorithm 1 can be divided into smaller iterative steps, and Algorithm 2 outlines the training step of projection model with instances. F in Algorithm 2 is source sentences in bilingual corpus, and A is the alignments. Function grad(·, ·, ·) gives the gradient (∇O) and the objective is optimized with a generic optimization step (such as an LBFGS iteration (Zhu et al., 1997)) in the subroutine climb(·, ·, ·). 4 Bilingually-Guided Dependency Grammar Induction This section presents our bilingual</context>
<context position="18448" citStr="Jiang and Liu, 2010" startWordPosition="2919" endWordPosition="2922">de re-estimation (Baker, 1979) without any smoothing, while Spitkovsky et al. (2010) utilizes smoothing and learning strategy during grammar learning and William et al. (2009) improves DMV with richer context. The dependency projection method DPA (Hwa et al., 2005) based on Direct Correspondence Assumption (Hwa et al., 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently (e.g. Figure 2). The Word Pair Classification (WPC) method (Jiang and Liu, 2010) modifies the DPA method and makes it more robust. Smith and Eisner (2009) propose an adaptation method founded on quasi-synchronous grammar features 1067 Type Feature Template Unigram wordi posi wordi ◦ posi wordj posj wordj ◦ posj Bigram wordi ◦ posj wordj ◦ posi posi ◦ posj wordi ◦ wordj wordi ◦ posi ◦ wordj wordi ◦ wordj ◦ posj wordi ◦ posi ◦ posj posi ◦ wordj ◦ posj wordi ◦ posi ◦ wordj ◦ posj Surrounding posi−1 ◦ posi ◦ posj posi ◦ posi+1 ◦ posj posi ◦ posj−1 ◦ posj posi ◦ posj ◦ posj+1 posi−1 ◦ posi ◦ posj−1 posi ◦ posi+1 ◦ posj+1 posi−1 ◦ posj−1 ◦ posj posi+1 ◦ posj ◦ posj+1 posi−1 ◦ p</context>
<context position="22140" citStr="Jiang and Liu (2010)" startWordPosition="3547" endWordPosition="3550"> Chinese Treebank (CTB) and CoNLL-X test sets. English sentences are tagged by the implementations of the POS tagger of Collins (2002), which is trained on WSJ. The source sentences are then parsed by an implementation of 2nd-ordered MST model of McDonald and Pereira (2006), which is trained on dependency trees extracted from Penn Treebank. As the evaluation metric, we use parsing accuracy which is the percentage of the words which have found their correct parents. We evaluate on sentences with all length for our method. Training Regime In experiments, we use the projection method proposed by Jiang and Liu (2010) to provide the projection instances. And we train the projection part α = 0 first for initialization, on which the whole model will be trained. Availing of the initialization method, the model can converge very fast (about 3 iterations is sufficient) and the results are more stable than the ones trained on random initialization. Baselines We compare our method against three kinds of different approaches: unsupervised method (Klein and Manning, 2004); singlesource direct projection methods (Hwa et al., 2005; Jiang and Liu, 2010); multi-source indirect projection methods with multi-sources (M10</context>
<context position="23858" citStr="Jiang and Liu, 2010" startWordPosition="3824" endWordPosition="3827">formance approaches to the DMV method. And the bilingually-guided model can promote the unsupervised method consistency over all languages. On the best results’ average of four comparable languages (da, nl, pt, sv), the promotion gained by our model is 28.5% over the baseline method (DMV) (Klein and Manning, 2004). Compare against Projection Baselines For all languages, the model consistently outperforms on direct projection baseline. On the average of each language’s best result, our model outperforms all kinds of baselines, yielding 3.0% gain over the single-source direct-projection method (Jiang and Liu, 2010) and 3.9% gain over the multi-source indirect-projection method (McDonald et al., 2011). On the average of all results with different parameters, our method also gains more than 2.0% improvements on all baselines. Particularly, our model achieves the most significant gains on Chinese, where the improvements are 4.5%/12.0% on direct/indirect projection baseAccuracy% Model ch da nl pt sv avg DMV 42.5∗ 33.4 38.5 20.1 44.0 —.– DPA 53.9 —.– —.– —.– —.– —.– WPC 56.8 50.1 58.4 70.5 60.8 59.3 Transfer 49.3 49.5 53.9 75.8 63.6 58.4 Selective 51.2 —.– 55.9 73.5 61.5 —.– unsuper 22.6 41.6 15.2 45.7 42.4 </context>
</contexts>
<marker>Jiang, Liu, 2010</marker>
<rawString>W. Jiang and Q. Liu. 2010. Dependency parsing and projection based on word-pair classification. In Proc. ofACL, pages 12–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>478</pages>
<contexts>
<context position="1987" citStr="Klein and Manning, 2004" startWordPosition="273" endWordPosition="277">aseline on average. 1 Introduction In past decades supervised methods achieved the state-of-the-art in constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et </context>
<context position="6935" citStr="Klein and Manning, 2004" startWordPosition="1001" endWordPosition="1004">ptimization objective is given); Then in section 4 we present the bilingually-guided induction strategy for dependency grammar (where the two objectives above are jointly optimized, as shown in Figure 1). After giving a brief introduction of previous work in section 5, we finally give the experimental results in section 6 and conclude our work in section 7. 2 Unsupervised Dependency Grammar Induction In this section, we introduce the unsupervised objective and the unsupervised training algorithm which is used as the framework of our bilinguallyguided method. Unlike previous unsupervised work (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006), we select a self-training approach (similar to hard EM method) to train the unsupervised model. And the framework of our unsupervised model builds a random treebank on the monolingual corpus firstly for initialization and trains a discriminative parsing model on it. Then we use the parser to build an evolved treebank with the 1-best result for the next iteration run. In this way, the parser and treebank evolve in an iterative way until convergence. Let’s introduce the parsing objective firstly: Define ez as the ith word in monolingual sentence E; deij deno</context>
<context position="17703" citStr="Klein and Manning, 2004" startWordPosition="2799" endWordPosition="2802"> to projection methods and build a random treebank (step 1). Then we train an initial model with projection instances in line 3. From line 4-9, the objective is optimized with a generic optimization step in the subroutine climb(·, ·, ·, ·, ·). For each sentence we parse its dependency tree, and update the tree into the treebank (step 3). Then we calculate the gradient and optimize the joint objective according to the evolved treebank and projected instances (step 2). Lines 5-6 are equivalent to the E-step of the EM algorithm, and lines 7-8 are equivalent to the M-step. 5 Related work The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) which is based on POS tags. And DMV learns the grammar via inside-outside re-estimation (Baker, 1979) without any smoothing, while Spitkovsky et al. (2010) utilizes smoothing and learning strategy during grammar learning and William et al. (2009) improves DMV with richer context. The dependency projection method DPA (Hwa et al., 2005) based on Direct Correspondence Assumption (Hwa et al., 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be co</context>
<context position="22594" citStr="Klein and Manning, 2004" startWordPosition="3620" endWordPosition="3623">ir correct parents. We evaluate on sentences with all length for our method. Training Regime In experiments, we use the projection method proposed by Jiang and Liu (2010) to provide the projection instances. And we train the projection part α = 0 first for initialization, on which the whole model will be trained. Availing of the initialization method, the model can converge very fast (about 3 iterations is sufficient) and the results are more stable than the ones trained on random initialization. Baselines We compare our method against three kinds of different approaches: unsupervised method (Klein and Manning, 2004); singlesource direct projection methods (Hwa et al., 2005; Jiang and Liu, 2010); multi-source indirect projection methods with multi-sources (M1068 60.5 nl 59.5 74.5 70.5 pt 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 alpha Figure 3: The performance of our model with respect to a series of ratio a cDonald et al., 2011; Naseem et al., 2012). 6.2 Results We test our method on CTB and CoNLL-X free test data sets respectively, and the performance is summarized in Table 2. Figure 3 presents the performance with different a on different languages. Compare against Unsupervised Baseline Experimental result</context>
<context position="24763" citStr="Klein and Manning, 2004" startWordPosition="3980" endWordPosition="3983">ese, where the improvements are 4.5%/12.0% on direct/indirect projection baseAccuracy% Model ch da nl pt sv avg DMV 42.5∗ 33.4 38.5 20.1 44.0 —.– DPA 53.9 —.– —.– —.– —.– —.– WPC 56.8 50.1 58.4 70.5 60.8 59.3 Transfer 49.3 49.5 53.9 75.8 63.6 58.4 Selective 51.2 —.– 55.9 73.5 61.5 —.– unsuper 22.6 41.6 15.2 45.7 42.4 33.5 avg 61.0 50.7 59.9 72.0 63.1 61.3 max 61.3 51.1 60.1 74.2 64.6 62.3 Table 2: The directed dependency accuracy with different parameter of our model and the baselines. The first section of the table (row 3-7) shows the results of the baselines: a unsupervised method baseline (Klein and Manning, 2004)(DMV); a single-source projection method baseline (Hwa et al., 2005) (DPA) and its improvement (Jiang and Liu, 2010)(WPC); two multisource baselines (McDonald et al., 2011)(Transfer) and (Naseem et al., 2012)(Selective). The second section of the table (row 8) presents the result of our unsupervised framework (unsuper). The third section gives the mean value (avg) and maximum value (max) of our model with different a in Figure 3. *: The result is based on sentences with 10 words or less after the removal of punctuation, it is an incomparable result. lines. The results in Figure 3 prove that ou</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C.D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. ofACL, page 478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of the 48th ACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="1686" citStr="Koo and Collins, 2010" startWordPosition="227" endWordPosition="230">ependency grammar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average. 1 Introduction In past decades supervised methods achieved the state-of-the-art in constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed,</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proc. of the 48th ACL, pages 1–11, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<pages>595--603</pages>
<contexts>
<context position="2082" citStr="Koo et al., 2008" startWordPosition="289" endWordPosition="292">n constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iter</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. pages 595– 603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of the 11th Conf. of EACL.</booktitle>
<contexts>
<context position="21794" citStr="McDonald and Pereira (2006)" startWordPosition="3490" endWordPosition="3493"> all languages, we only use English-target parallel data: we take the FBIS English-Chinese bitext as bilingual corpus for English-Chinese dependency projection which contains 239K sentence pairs with about 8.9M/6.9M words in English/Chinese, and for other languages we use the readily available data in the Europarl corpus. Then we run tests on the Penn Chinese Treebank (CTB) and CoNLL-X test sets. English sentences are tagged by the implementations of the POS tagger of Collins (2002), which is trained on WSJ. The source sentences are then parsed by an implementation of 2nd-ordered MST model of McDonald and Pereira (2006), which is trained on dependency trees extracted from Penn Treebank. As the evaluation metric, we use parsing accuracy which is the percentage of the words which have found their correct parents. We evaluate on sentences with all length for our method. Training Regime In experiments, we use the projection method proposed by Jiang and Liu (2010) to provide the projection instances. And we train the projection part α = 0 first for initialization, on which the whole model will be trained. Availing of the initialization method, the model can converge very fast (about 3 iterations is sufficient) an</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. of the 11th Conf. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1598" citStr="McDonald et al., 2005" startWordPosition="211" endWordPosition="214">previous models only using bilingual projection or unsupervised induction. We induced dependency grammar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average. 1 Introduction In past decades supervised methods achieved the state-of-the-art in constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual p</context>
<context position="20752" citStr="McDonald et al., 2005" startWordPosition="3327" endWordPosition="3330">rojection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petrov, 2011). Our model differs from the approaches above in its emphasis on utilizing information from both sides of bilingual corpus in an unsupervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is trained by our bilingually-guided model on 5 languages. And the features used in our experiments are summarized in Table 1. 6.1 Experiment Setup Datasets and Evaluation Our experiments are run on five different languages: Chinese(ch), Danish(da), Dutch(nl), Portuguese(pt) and Swedish(sv) (da, nl, pt and sv are free data sets distributed for the 2006 CoNLL Shared Tasks (Buchholz and Marsi, 2006)). For all languages, we only use English-target parallel data: we take the FBIS English-Chinese bitext as bilingual corpus for English-Chinese dependency projection which contains 239K sente</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proc. ofACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proc. of EMNLP, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>216--220</pages>
<contexts>
<context position="1622" citStr="McDonald et al., 2006" startWordPosition="215" endWordPosition="218">ng bilingual projection or unsupervised induction. We induced dependency grammar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average. 1 Introduction In past decades supervised methods achieved the state-of-the-art in constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., </context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proc. of CoNLL, pages 216– 220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>S Petrov</author>
<author>K Hall</author>
</authors>
<title>Multisource transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proc. ofEMNLP,</booktitle>
<pages>62--72</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3322" citStr="McDonald et al., 2011" startWordPosition="488" endWordPosition="491">an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across bilingual texts or indirectly across multilingual texts (Snyder et al., 2009; McDonald et al., 2011; Naseem et al., 2012), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages. Unsupervised induction and bilingual projection run according to totally different principles, the former mines the underlying structure of the monolingual language, while the latter leverages the syntactic knowledge of the parsed counter1063 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063–1072, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Bilingual</context>
<context position="19777" citStr="McDonald et al., 2011" startWordPosition="3169" endWordPosition="3172">osj posi ◦ posi+1 ◦ posj ◦ posj+1 posi ◦ posi+1 ◦ posj−1 ◦ posj posi−1 ◦ posi ◦ posj ◦ posj+1 Table 1: Feature templates for dependency parsing. For edge deij: wordi is the parent word and wordj is the child word, similar to ”pos”. ”+1” denotes the preceding token of the sentence, similar to ”-1”. for dependency projection and annotation, which requires a small set of dependency annotated corpus of target language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projecti</context>
<context position="23945" citStr="McDonald et al., 2011" startWordPosition="3836" endWordPosition="3840">he unsupervised method consistency over all languages. On the best results’ average of four comparable languages (da, nl, pt, sv), the promotion gained by our model is 28.5% over the baseline method (DMV) (Klein and Manning, 2004). Compare against Projection Baselines For all languages, the model consistently outperforms on direct projection baseline. On the average of each language’s best result, our model outperforms all kinds of baselines, yielding 3.0% gain over the single-source direct-projection method (Jiang and Liu, 2010) and 3.9% gain over the multi-source indirect-projection method (McDonald et al., 2011). On the average of all results with different parameters, our method also gains more than 2.0% improvements on all baselines. Particularly, our model achieves the most significant gains on Chinese, where the improvements are 4.5%/12.0% on direct/indirect projection baseAccuracy% Model ch da nl pt sv avg DMV 42.5∗ 33.4 38.5 20.1 44.0 —.– DPA 53.9 —.– —.– —.– —.– —.– WPC 56.8 50.1 58.4 70.5 60.8 59.3 Transfer 49.3 49.5 53.9 75.8 63.6 58.4 Selective 51.2 —.– 55.9 73.5 61.5 —.– unsuper 22.6 41.6 15.2 45.7 42.4 33.5 avg 61.0 50.7 59.9 72.0 63.1 61.3 max 61.3 51.1 60.1 74.2 64.6 62.3 Table 2: The d</context>
<context position="26054" citStr="McDonald et al., 2011" startWordPosition="4204" endWordPosition="4207">t has a good start (well initialization), and it will be better once we incorporate the information from the projection side (a = 0.9). And the maximum points are not in a = 1, which implies that projection information is still available for the unsupervised framework even if we employ the projection model as the initialization. So we suggest that a greater parameter is a better choice for our model. And there are some random factors in our model which make performance curves with more fluctuation. And there is just a little improvement shown in da, in which the same situation is observed by (McDonald et al., 2011). 6.3 Effects of the Size of Training Corpus To investigate how the size of the training corpus influences the result, we train the model on extracted bilingual corpus with varying sizes: 10K, 50K, 100K, 150K and 200K sentences pairs. As shown in Figure 4, our approach continu61.5 60.0 da 51.2 50.3 ch 65.0 61.5 sv accuracy% 1069 Figure 4: Performance on varying sizes (average of 5 languages, α = 0.9) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 noise rate Figure 5: Performance on different projection quality (average of 5 languages, α = 0.9). The noise rate is the percentage of the projected instances be</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>R. McDonald, S. Petrov, and K. Hall. 2011. Multisource transfer of delexicalized dependency parsers. In Proc. ofEMNLP, pages 62–72. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Naseem</author>
<author>B Snyder</author>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Multilingual part-of-speech tagging: Two unsupervised approaches.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="20204" citStr="Naseem et al., 2009" startWordPosition="3236" endWordPosition="3239">, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petrov, 2011). Our model differs from the approaches above in its emphasis on utilizing information from both sides of bilingual corpus in an unsupervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is trained by our bilingually-guided model </context>
</contexts>
<marker>Naseem, Snyder, Eisenstein, Barzilay, 2009</marker>
<rawString>T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised approaches. Journal of Artificial Intelligence Research, 36(1):341–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
<author>Amir Globerson</author>
</authors>
<title>Selective sharing for multilingual dependency parsing.</title>
<date>2012</date>
<booktitle>In Proc. of the 50th ACL,</booktitle>
<pages>629--637</pages>
<contexts>
<context position="3344" citStr="Naseem et al., 2012" startWordPosition="492" endWordPosition="495">r (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across bilingual texts or indirectly across multilingual texts (Snyder et al., 2009; McDonald et al., 2011; Naseem et al., 2012), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages. Unsupervised induction and bilingual projection run according to totally different principles, the former mines the underlying structure of the monolingual language, while the latter leverages the syntactic knowledge of the parsed counter1063 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063–1072, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Bilingual corpus Joint Optimiza</context>
<context position="19981" citStr="Naseem et al. (2012)" startWordPosition="3199" endWordPosition="3202">ld word, similar to ”pos”. ”+1” denotes the preceding token of the sentence, similar to ”-1”. for dependency projection and annotation, which requires a small set of dependency annotated corpus of target language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petrov, 2011). Our model differs from the approaches above in its emphasis on utilizing information from both sides of bilingual corpus in an unsupervised training framework, while most of the</context>
<context position="22930" citStr="Naseem et al., 2012" startWordPosition="3683" endWordPosition="3686">ation method, the model can converge very fast (about 3 iterations is sufficient) and the results are more stable than the ones trained on random initialization. Baselines We compare our method against three kinds of different approaches: unsupervised method (Klein and Manning, 2004); singlesource direct projection methods (Hwa et al., 2005; Jiang and Liu, 2010); multi-source indirect projection methods with multi-sources (M1068 60.5 nl 59.5 74.5 70.5 pt 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 alpha Figure 3: The performance of our model with respect to a series of ratio a cDonald et al., 2011; Naseem et al., 2012). 6.2 Results We test our method on CTB and CoNLL-X free test data sets respectively, and the performance is summarized in Table 2. Figure 3 presents the performance with different a on different languages. Compare against Unsupervised Baseline Experimental results show that our unsupervised framework’s performance approaches to the DMV method. And the bilingually-guided model can promote the unsupervised method consistency over all languages. On the best results’ average of four comparable languages (da, nl, pt, sv), the promotion gained by our model is 28.5% over the baseline method (DMV) (K</context>
<context position="24971" citStr="Naseem et al., 2012" startWordPosition="4013" endWordPosition="4016">nsfer 49.3 49.5 53.9 75.8 63.6 58.4 Selective 51.2 —.– 55.9 73.5 61.5 —.– unsuper 22.6 41.6 15.2 45.7 42.4 33.5 avg 61.0 50.7 59.9 72.0 63.1 61.3 max 61.3 51.1 60.1 74.2 64.6 62.3 Table 2: The directed dependency accuracy with different parameter of our model and the baselines. The first section of the table (row 3-7) shows the results of the baselines: a unsupervised method baseline (Klein and Manning, 2004)(DMV); a single-source projection method baseline (Hwa et al., 2005) (DPA) and its improvement (Jiang and Liu, 2010)(WPC); two multisource baselines (McDonald et al., 2011)(Transfer) and (Naseem et al., 2012)(Selective). The second section of the table (row 8) presents the result of our unsupervised framework (unsuper). The third section gives the mean value (avg) and maximum value (max) of our model with different a in Figure 3. *: The result is based on sentences with 10 words or less after the removal of punctuation, it is an incomparable result. lines. The results in Figure 3 prove that our unsupervised framework a = 1 can promote the grammar induction if it has a good start (well initialization), and it will be better once we incorporate the information from the projection side (a = 0.9). And</context>
</contexts>
<marker>Naseem, Barzilay, Globerson, 2012</marker>
<rawString>Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective sharing for multilingual dependency parsing. In Proc. of the 50th ACL, pages 629–637, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryi˜git</author>
<author>S Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>221--225</pages>
<marker>Nivre, Hall, Nilsson, Eryi˜git, Marinov, 2006</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, G. Eryi˜git, and S. Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proc. of CoNLL, pages 221–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>Maltparser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K¨ubler, S. Marinov, and E. Marsi. 2007. Maltparser: A language-independent system for datadriven dependency parsing. Natural Language Engineering, 13(02):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st ICCL &amp; 44th ACL,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="1552" citStr="Petrov et al., 2006" startWordPosition="204" endWordPosition="207">nduce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency grammar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average. 1 Introduction In past decades supervised methods achieved the state-of-the-art in constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of the 21st ICCL &amp; 44th ACL, pages 433–440, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proc. ofNAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="14704" citStr="Sarkar, 2001" startWordPosition="2290" endWordPosition="2291">model to build new treebank on target language for next iteration. 4. Repeat steps 1, 2 and 3 until convergence. The unsupervised objective is optimized by the loop—”tree bank→optimized model→new tree bank”. The treebank is evolved for runs. The unsupervised model gets projection constraint implicitly from those parse trees which contain information from projection part. The projection objective is optimized by the circulation—”projected instances→optimized model”, these projected instances will not change once we get them. The iterative procedure proposed here is not a co-training algorithm (Sarkar, 2001; Hwa et al., 2003), because the input of the projection objective is static. log Pr(+|de) 1066 4.1 Joint Objective For multi-objective optimization method, we employ the classical weighted-sum approach which just calculates the weighted linear sum of the objectives: Algorithm 3 Training joint model 1: DP, DN +- proj(F, DF, A, E) 2: build random DE 3: λ +- train(DP, DN) 4: repeat 5: for each E E E do &gt; E step 6: DE +- parse(E, λ) 7: Vf(λ) +- grad(DE, DE, DP, DN, f(λ)) � weightmobjm (8) 8: λ +-climb(f(λ), Vf(λ), λ) &gt; M step OBJ = 9: until convergence m We combine the unsupervised objective (For</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>A. Sarkar. 2001. Applying co-training methods to statistical parsing. In Proc. ofNAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>G Satta</author>
<author>A Joshi</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In Annual Meeting-,</booktitle>
<volume>45</volume>
<pages>760</pages>
<contexts>
<context position="20182" citStr="Shen et al., 2007" startWordPosition="3232" endWordPosition="3235">language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petrov, 2011). Our model differs from the approaches above in its emphasis on utilizing information from both sides of bilingual corpus in an unsupervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is trained by our bil</context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>L. Shen, G. Satta, and A. Joshi. 2007. Guided learning for bidirectional sequence classification. In Annual Meeting-, volume 45, page 760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>354--362</pages>
<contexts>
<context position="2011" citStr="Smith and Eisner, 2005" startWordPosition="278" endWordPosition="281">roduction In past decades supervised methods achieved the state-of-the-art in constituency parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006) and dependency parsing (McDonald et al., 2005a; McDonald et al., 2006; Nivre et al., 2006; Nivre et al., 2007; Koo and Collins, 2010). For supervised models, the human-annotated corpora on which models are trained, however, are expensive and difficult to build. As alternative strategies, methods which utilize raw texts have been investigated recently, including unsupervised methods which use only raw texts (Klein and Manning, 2004; Smith and Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et</context>
<context position="6959" citStr="Smith and Eisner, 2005" startWordPosition="1005" endWordPosition="1009">given); Then in section 4 we present the bilingually-guided induction strategy for dependency grammar (where the two objectives above are jointly optimized, as shown in Figure 1). After giving a brief introduction of previous work in section 5, we finally give the experimental results in section 6 and conclude our work in section 7. 2 Unsupervised Dependency Grammar Induction In this section, we introduce the unsupervised objective and the unsupervised training algorithm which is used as the framework of our bilinguallyguided method. Unlike previous unsupervised work (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006), we select a self-training approach (similar to hard EM method) to train the unsupervised model. And the framework of our unsupervised model builds a random treebank on the monolingual corpus firstly for initialization and trains a discriminative parsing model on it. Then we use the parser to build an evolved treebank with the 1-best result for the next iteration run. In this way, the parser and treebank evolve in an iterative way until convergence. Let’s introduce the parsing objective firstly: Define ez as the ith word in monolingual sentence E; deij denotes the word pair depend</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N.A. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proc. ofACL, pages 354–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proc. ofEMNLP: Volume</booktitle>
<volume>2</volume>
<pages>822--831</pages>
<contexts>
<context position="2947" citStr="Smith and Eisner, 2009" startWordPosition="425" endWordPosition="429"> from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across bilingual texts or indirectly across multilingual texts (Snyder et al., 2009; McDonald et al., 2011; Naseem et al., 2012), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages. Unsupervised induction and bilingual projection run according to t</context>
<context position="18522" citStr="Smith and Eisner (2009)" startWordPosition="2932" endWordPosition="2935">et al. (2010) utilizes smoothing and learning strategy during grammar learning and William et al. (2009) improves DMV with richer context. The dependency projection method DPA (Hwa et al., 2005) based on Direct Correspondence Assumption (Hwa et al., 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently (e.g. Figure 2). The Word Pair Classification (WPC) method (Jiang and Liu, 2010) modifies the DPA method and makes it more robust. Smith and Eisner (2009) propose an adaptation method founded on quasi-synchronous grammar features 1067 Type Feature Template Unigram wordi posi wordi ◦ posi wordj posj wordj ◦ posj Bigram wordi ◦ posj wordj ◦ posi posi ◦ posj wordi ◦ wordj wordi ◦ posi ◦ wordj wordi ◦ wordj ◦ posj wordi ◦ posi ◦ posj posi ◦ wordj ◦ posj wordi ◦ posi ◦ wordj ◦ posj Surrounding posi−1 ◦ posi ◦ posj posi ◦ posi+1 ◦ posj posi ◦ posj−1 ◦ posj posi ◦ posj ◦ posj+1 posi−1 ◦ posi ◦ posj−1 posi ◦ posi+1 ◦ posj+1 posi−1 ◦ posj−1 ◦ posj posi+1 ◦ posj ◦ posj+1 posi−1 ◦ posi ◦ posj+1 posi ◦ posi+1 ◦ posj−1 posi−1 ◦ posj ◦ posj+1 posi+1 ◦ posj−1</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>D.A. Smith and J. Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proc. ofEMNLP: Volume 2-Volume 2, pages 822–831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>T Naseem</author>
<author>R Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. ofIJCNLP of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>73--81</pages>
<contexts>
<context position="3299" citStr="Snyder et al., 2009" startWordPosition="484" endWordPosition="487">eratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across bilingual texts or indirectly across multilingual texts (Snyder et al., 2009; McDonald et al., 2011; Naseem et al., 2012), a better dependency grammar can be easily induced, if syntactic isomorphism is largely maintained between target and source languages. Unsupervised induction and bilingual projection run according to totally different principles, the former mines the underlying structure of the monolingual language, while the latter leverages the syntactic knowledge of the parsed counter1063 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063–1072, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computationa</context>
</contexts>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsupervised multilingual grammar induction. In Proc. ofIJCNLP of the AFNLP: Volume 1-Volume 1, pages 73–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Data point selection for crosslanguage adaptation of dependency parsers.</title>
<date>2011</date>
<booktitle>In Proc. of the 49th ACL: HLT,</booktitle>
<pages>682--686</pages>
<contexts>
<context position="19793" citStr="Søgaard, 2011" startWordPosition="3173" endWordPosition="3174">j ◦ posj+1 posi ◦ posi+1 ◦ posj−1 ◦ posj posi−1 ◦ posi ◦ posj ◦ posj+1 Table 1: Feature templates for dependency parsing. For edge deij: wordi is the parent word and wordj is the child word, similar to ”pos”. ”+1” denotes the preceding token of the sentence, similar to ”-1”. for dependency projection and annotation, which requires a small set of dependency annotated corpus of target language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petr</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Data point selection for crosslanguage adaptation of dependency parsers. In Proc. of the 49th ACL: HLT, pages 682–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In HLT: NAACL,</booktitle>
<pages>751--759</pages>
<contexts>
<context position="2622" citStr="Spitkovsky et al., 2010" startWordPosition="379" endWordPosition="382"> Eisner, 2005; William et al., 2009), and semi-supervised methods (Koo et al., 2008) which use both raw texts and annotated corpus. And there are a lot of efforts have also been devoted to bilingual projection (Chen et al., 2010), which resorts to bilingual text with one language parsed, and projects the syntactic information from the parsed language to the unparsed one (Hwa et al., 2005; Ganchev et al., 2009). In dependency grammar induction, unsupervised methods achieve continuous improvements in recent years (Klein and Manning, 2004; Smith and Eisner, 2005; Bod, 2006; William et al., 2009; Spitkovsky et al., 2010). Relying on a predefined distributional assumption and iteratively maximizing an approximate indicator (entropy, likelihood, etc.), an unsupervised model usually suffers from two drawbacks, i.e., lower performance and higher computational cost. On the contrary, bilingual projection (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) seems a promising substitute for languages with a large amount of bilingual sentences and an existing parser of the counterpart language. By projecting syntactic structures directly (Hwa et al., 2005; Smith and Eisner, 2009; Jiang and Liu, 2010) across</context>
<context position="17912" citStr="Spitkovsky et al. (2010)" startWordPosition="2833" endWordPosition="2836">e subroutine climb(·, ·, ·, ·, ·). For each sentence we parse its dependency tree, and update the tree into the treebank (step 3). Then we calculate the gradient and optimize the joint objective according to the evolved treebank and projected instances (step 2). Lines 5-6 are equivalent to the E-step of the EM algorithm, and lines 7-8 are equivalent to the M-step. 5 Related work The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) which is based on POS tags. And DMV learns the grammar via inside-outside re-estimation (Baker, 1979) without any smoothing, while Spitkovsky et al. (2010) utilizes smoothing and learning strategy during grammar learning and William et al. (2009) improves DMV with richer context. The dependency projection method DPA (Hwa et al., 2005) based on Direct Correspondence Assumption (Hwa et al., 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently (e.g. Figure 2). The Word Pair Classification (WPC) method (Jiang and Liu, 2010) modifies the DPA method and makes it more robust. Smith and Eis</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2010. From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing. In HLT: NAACL, pages 751–759, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O T¨ackstr¨om</author>
<author>R McDonald</author>
<author>J Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>O. T¨ackstr¨om, R. McDonald, and J. Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson William</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>101--109</pages>
<marker>William, McClosky, 2009</marker>
<rawString>William, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proc. of NAACL, pages 101–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>G Ngai</author>
<author>R Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora. In</title>
<date>2001</date>
<booktitle>Proc. of HLT,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="20163" citStr="Yarowsky et al., 2001" startWordPosition="3228" endWordPosition="3231">tated corpus of target language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection by a graphbased projection (Das and Petrov, 2011). Our model differs from the approaches above in its emphasis on utilizing information from both sides of bilingual corpus in an unsupervised training framework, while most of the work above only utilize the information from a single side. 6 Experiments In this section, we evaluate the performance of the MST dependency parser (McDonald et al., 2005b) which is</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proc. of HLT, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Crosslanguage parser adaptation between related languages.</title>
<date>2008</date>
<booktitle>In Proc. of the IJCNLP-08. Proc. CoNLL.</booktitle>
<contexts>
<context position="19754" citStr="Zeman and Resnik, 2008" startWordPosition="3165" endWordPosition="3168">si−1 ◦ posi ◦ posj−1 ◦ posj posi ◦ posi+1 ◦ posj ◦ posj+1 posi ◦ posi+1 ◦ posj−1 ◦ posj posi−1 ◦ posi ◦ posj ◦ posj+1 Table 1: Feature templates for dependency parsing. For edge deij: wordi is the parent word and wordj is the child word, similar to ”pos”. ”+1” denotes the preceding token of the sentence, similar to ”-1”. for dependency projection and annotation, which requires a small set of dependency annotated corpus of target language. Similarly, using indirect information from multilingual (Cohen et al., 2011; T¨ackstr¨om et al., 2012) is an effective way to improve unsupervised parsing. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) employ non-lexicalized parser trained on other languages to process a target language. McDonald et al. (2011) adapts their multi-source parser according to DCA, while Naseem et al. (2012) selects a selective sharing model to make better use of grammar information in multi-sources. Due to similar reasons, many works are devoted to POS projection (Yarowsky et al., 2001; Shen et al., 2007; Naseem et al., 2009), and they also suffer from similar problems. Some seek for unsupervised methods, e.g. Naseem et al. (2009), and some further improve the projection b</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Crosslanguage parser adaptation between related languages. In Proc. of the IJCNLP-08. Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciyou Zhu</author>
<author>Richard H Byrd</author>
<author>Peihuang Lu</author>
<author>Jorge Nocedal</author>
</authors>
<title>Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale bound-constrained optimization.</title>
<date>1997</date>
<journal>ACM Transactions on Mathematical Software (TOMS),</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="12107" citStr="Zhu et al., 1997" startWordPosition="1909" endWordPosition="1912">ere DP is the positive dependency arc instance set, which is obtained by direct projection methods (Hwa et al., 2005; Jiang and Liu, 2010) and DN is the negative one. 3.2 Projection Algorithm Basically, the training procedure in line 2,7 of Algorithm 1 can be divided into smaller iterative steps, and Algorithm 2 outlines the training step of projection model with instances. F in Algorithm 2 is source sentences in bilingual corpus, and A is the alignments. Function grad(·, ·, ·) gives the gradient (∇O) and the objective is optimized with a generic optimization step (such as an LBFGS iteration (Zhu et al., 1997)) in the subroutine climb(·, ·, ·). 4 Bilingually-Guided Dependency Grammar Induction This section presents our bilingually-guided grammar induction model, which incorporates unsupervised framework and bilingual projection model through a joint approach. According to following observation: unsupervised induction model mines underlying syntactic structure of the monolingual language, however, it is hard to find good grammar induction in the exponential parsing space; bilingual projection obtains relatively reliable syntactic knowledge of the parsed counterpart, but it possibly contains a lot of</context>
</contexts>
<marker>Zhu, Byrd, Lu, Nocedal, 1997</marker>
<rawString>Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge Nocedal. 1997. Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on Mathematical Software (TOMS), 23(4):550–560.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>