<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000115">
<title confidence="0.9961885">
Finding Medical Term Variations using Parallel Corpora and
Distributional Similarity
</title>
<author confidence="0.955119">
Lonneke van der Plas J¨org Tiedemann
</author>
<affiliation confidence="0.996521">
Department of Linguistics Department of Linguistics and Philology
University of Geneva Uppsala University
</affiliation>
<email confidence="0.99164">
lonneke.vanderplas@unige.ch jorg.tiedemann@lingfil.uu.se
</email>
<sectionHeader confidence="0.993734" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940363636364">
We describe a method for the identifica-
tion of medical term variations using par-
allel corpora and measures of distribu-
tional similarity. Our approach is based
on automatic word alignment and stan-
dard phrase extraction techniques com-
monly used in statistical machine transla-
tion. Combined with pattern-based filters
we obtain encouraging results compared
to related approaches using similar data-
driven techniques.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986218649122807">
Ontologies provide a way to formally represent
knowledge, for example for a specific domain.
Ontology building has received a lot of atten-
tion in the medical domain. This interest is re-
flected in the existence of numerous medical on-
tologies, such as the Unified Medical Language
System (UMLS) (McCray and Hole, 1990) with
its metathesaurus, semantic network, and special-
ist lexicon. Although the UMLS includes infor-
mation for languages other than English, the cov-
erage for other languages is generally smaller.
In this paper we describe an approach to acquire
lexical information for the Dutch medical domain
automatically. In the medical domain variations in
terminology often include multi-word terms such
as aangeboren afwijking ‘birth defect’ for con-
genitale aandoening ‘congenital disorder’. These
multiple ways to refer to the same concept using
distinct (multi-word) terms are examples of syn-
onymy1 but are often referred to as term varia-
1Spelling variants are a type of term variations that are
not included in the definition of synonymy.
tions. These term variations could be used to en-
hance existing medical ontologies for the Dutch
language.
Our technique builds on the distributional hy-
pothesis, the idea that semantically related words
are distributed similarly over contexts (Harris,
1968). This is in line with the Firthian saying that,
’You shall know a word by the company it keeps.’
(Firth, 1957). In other words, you can grasp the
meaning of a word by looking at its contexts.
Context can be defined in many ways. Previous
work has been mainly concerned with the syntac-
tic contexts a word is found in (Lin, 1998; Cur-
ran, 2003). For example, the verbs that are in
a subject relation with a particular noun form a
part of its context. In accordance with the Firthian
tradition these contexts can be used to determine
the semantic relatedness of words. For instance,
words that occur in a object relation with the verb
to drink have something in common: they are liq-
uid. Other work has been concerned with the bag-
of-word context, where the context of a word are
the words that are found in its proximity (Wilks et
al., 1993; Sch¨utze, 1992).
Yet another context, that is much less studied, is
the translational context. The translational context
of a word is the set of translations it gets in other
languages. For example, the translational context
of cat is kat in Dutch and chat in French. This
requires a rather broad understanding of the term
context. The idea is that words that share a large
number of translations are similar. For example
both autumn and fall get the translation herfst in
Dutch, Herbst in German, and automne in French.
This indicates that autumn and fall are synonyms.
</bodyText>
<page confidence="0.982727">
28
</page>
<note confidence="0.840962">
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 28–37,
Beijing, August 2010
</note>
<bodyText confidence="0.999915979166667">
A straightforward place to start looking for
translational context is in bilingual dictionaries.
However, these are not always publicly available
for all languages. More importantly, dictionar-
ies are static and therefore often incomplete re-
sources. We have chosen to automatically acquire
word translations in multiple languages from text.
Text in this case should be understood as multi-
lingual parallel text. Automatic alignment gives
us the translations of a word in multiple lan-
guages. The so-called alignment-based distribu-
tional methods described in Van der Plas (2008)
apply the translational context for the discovery
of single word synonyms for the general domain.
Any multilingual parallel corpus can be used for
this purpose. It is thus possible to focus on
a special domain, such as the medical domain
we are considering in this paper. The automatic
alignment provides us also with domain-specific
frequency information for every translation pair,
which is helpful in case words are ambiguous.
Aligned parallel corpora have often been used
in the field of word sense discovery, the task of
discriminating the different senses words have.
The idea behind it is that a word that receives dif-
ferent translations might be polysemous. For ex-
ample, a word such as wood receives the transla-
tion woud and hout in Dutch, the former referring
to an area with many trees and the latter referring
to the solid material derived from trees. Whereas
this type of work is all built upon the divergence of
translational context, i.e. one word in the source
language is translated by many different words in
the target language, we are interested in the con-
vergence of translations, i.e. two words in the
source language receiving the same translation in
the target language. Of course these two phenom-
ena are not independent. The alleged conversion
of the target language might well be a hidden di-
version of the source language. Since the English
word might be polysemous, the fact that woud and
hout in Dutch are both translated in English by
wood does not mean that woud and hout in Dutch
are synonyms. However, the use of multiple lan-
guages overshadows the noise resulting from pol-
ysemy (van der Plas, 2008).
Van der Plas (2008) shows that the way the
context is defined influences the type of lexico-
semantic knowledge that is discovered. After
gold standard evaluations and manual inspection
the author concludes that when using translational
contexts more tight semantic relations such as
synonymy are found whereas the conventional
syntax-based approaches retrieve hypernyms, co-
hyponyms, and antonyms of the target word. The
performance on synonym acquisition when using
translational contexts is almost twice as good as
when using syntactic contexts, while the amount
of data used is much smaller. Van der Plas (2008)
ascribed the fact that the syntax-based method be-
haves in this way to the fact that loosely related
words, such as wine and beer, are often found in
the same syntactic contexts. The alignment-based
method suffers less from this indiscriminant ac-
ceptance because words are typically translated by
words with the same meaning. The word wine is
typically not translated with a word for beverage
nor with a word for beer, and neither is good trans-
lated with the equivalence of bad.
In this paper we are concerned with medical
term variations that are in fact (multi-word) syn-
onyms. We will use the translational context to
compute similarity between terms. The transla-
tional context is not only very suitable to find
tight relations between words, the transition from
single-word synonyms to multi-word term varia-
tions is also straightforward due to advances in
phrase-based machine translation. We will use
word alignment techniques in combination with
phrase extraction techniques from statistical ma-
chine translation to extract phrases and their trans-
lations from a medical parallel corpus. We com-
bine this approach with Part-of-Speech (PoS) pat-
terns from the term extraction literature to extract
candidate terms from the phrase tables. Using
similarity measures used in distributional methods
we finally compute ranked lists of term variations.
We already noted that these term variations
could be used to enhance existing ontologies for
the Dutch language. On top of that we believe that
the multi-lingual method that uses translations of
multi-word terms in several languages could be
used to expand resources built for English with
translations in other languages (semi-) automati-
cally. This last point falls outside the scope of this
paper.
</bodyText>
<page confidence="0.997592">
29
</page>
<bodyText confidence="0.999854571428571">
In the following section we will describe the
alignment-based approaches to distributional sim-
ilarity. In section 3 we will describe the method-
ology we followed in this paper in detail. We de-
scribe our evaluation in section 4 and discuss the
results in section 5. Section 6 concludes this pa-
per.
</bodyText>
<sectionHeader confidence="0.999834" genericHeader="method">
2 Alignment-based methods
</sectionHeader>
<bodyText confidence="0.999940166666667">
In this section we explain the alignment-based ap-
proaches to distributional similarity. We will give
some examples of translational context and we
will explain how measures serve to determine the
similarity of these contexts. We end this section
with a discussion of related work.
</bodyText>
<subsectionHeader confidence="0.978294">
2.1 Translational context
</subsectionHeader>
<bodyText confidence="0.9924878">
The translational context of a word or a multi-
word term is the set of translations it gets in other
languages. For the acquisition of translations for
the Dutch medical terms we rely on automatic
word alignment in parallel corpora.
</bodyText>
<figureCaption confidence="0.863715">
Figure 1: Example of bidirectional word align-
ments of two parallel sentences
Figure 1 illustrates the automatic word alignment
</figureCaption>
<bodyText confidence="0.973026357142857">
between a Dutch and an English phrase as a re-
sult of using the IBM alignment models (Brown
et al., 1993) implemented in the open-source tool
GIZA++ (Och, 2003). The alignment of two texts
is bi-directional. The Dutch text is aligned to
the English text and vice versa (dotted lines ver-
sus continuous lines). The alignment models pro-
duced are asymmetric. Several heuristics exist
to combine directional word alignments which is
usually called “symmetrization”. In order to cover
multi-word terms standard phrase extraction tech-
niques can be used to move from word alignment
to linked phrases (see section 3.2 for more de-
tails).
</bodyText>
<subsectionHeader confidence="0.9761">
2.2 Measures for computing similarity
</subsectionHeader>
<bodyText confidence="0.99997785">
Translational co-occurrence vectors are used to
find distributionally similar words. For ease of
reading, we give an example of a single-word
term kat in Table 1. In our current setting the
terms can be both single- or multi-word terms
such as werkzame stof ‘active ingredient’. Ev-
ery cell in the vector refers to a particular transla-
tional co-occurrence type. For example, kat ‘cat’
gets the translation Katze in German. The value
of these cells indicate the number of times the co-
occurrence type under consideration is found in
the corpus.
Each co-occurrence type has a cell frequency.
Likewise each head term has a row frequency.
The row frequency of a certain head term is the
sum of all its cell frequencies. In our example the
row frequency for the term kat ‘cat’ is 65. Cut-
offs for cell and row frequency can be applied to
discard certain infrequent co-occurrence types or
head terms respectively.
</bodyText>
<equation confidence="0.720008333333333">
DE
Katze
kat 17
</equation>
<tableCaption confidence="0.79421">
Table 1: Translational co-occurrence vector for
kat (’cat’) based on four languages
</tableCaption>
<bodyText confidence="0.999968375">
The more similar the vectors are, the more dis-
tributionally similar the head terms are. We need a
way to compare the vectors for any two head terms
to be able to express the similarity between them
by means of a score. Various methods can be used
to compute the distributional similarity between
terms. We will explain in section 3 what measures
we have chosen in the current experiments.
</bodyText>
<sectionHeader confidence="0.532101" genericHeader="method">
2.3 Related work
</sectionHeader>
<bodyText confidence="0.999935">
Multilingual parallel corpora have mostly been
used for tasks related to word sense disambigua-
tion such as separation of senses (Resnik and
Yarowsky, 1997; Dyvik, 1998; Ide et al., 2002).
However, taking sense separation as a basis,
Dyvik (2002) derives relations such as synonymy
and hyponymy by applying the method of se-
mantic mirrors. The paper illustrates how the
method works. First, different senses are iden-
tified on the basis of manual word translations
in sentence-aligned Norwegian-English data (2,6
million words in total). Second, senses are
grouped in semantic fields. Third, features are
</bodyText>
<figure confidence="0.988945909090909">
FR
chat
26
IT
gatto
8
EN
cat
13
total
64
</figure>
<page confidence="0.986935">
30
</page>
<bodyText confidence="0.999949050847458">
assigned on the basis of inheritance. Lastly, se-
mantic relations such synonymy and hyponymy
are detected based on intersection and inclusion
among feature sets .
Improving the syntax-based approach for syn-
onym identification using bilingual dictionaries
has been discussed in Lin et al. (2003) and Wu and
Zhou (2003). In the latter parallel corpora are also
applied as a reference to assign translation likeli-
hoods to candidates derived from the dictionary.
Both of them are limited to single-word terms.
Some researchers employ multilingual corpora
for the automatic acquisition of paraphrases (Shi-
mota and Sumita, 2002; Bannard and Callison-
Burch, 2005; Callison-Burch, 2008). The last two
are based on automatic word alignment as is our
approach.
Bannard and Callison-Burch (2005) use a
method that is also rooted in phrase-based statis-
tical machine translation. Translation probabili-
ties provide a ranking of candidate paraphrases.
These are refined by taking contextual informa-
tion into account in the form of a language model.
The Europarl corpus (Koehn, 2005) is used. It has
about 30 million words per language. 46 English
phrases are selected as a test set for manual evalu-
ation by two judges. When using automatic align-
ment, the precision reached without using contex-
tual refinement is 48.9%. A precision of 55.3%
is reached when using context information. Man-
ual alignment improves the performance by 26%.
A precision score of 55% is attained when using
multilingual data.
In a more recent publication Callison-Burch
(2008) improved this method by using syntac-
tic constraints and multiple languages in parallel.
We have implemented a combination of Bannard
and Callison-Burch (2005) and Callison-Burch
(2008), in which we use PoS filters instead of
syntactic constraints to compare our results with.
More details can be found in the Section 5.
Apart from methods that use parallel corpora
mono-lingual pattern-based methods have been
used to find term variations. Fahmi (2009) ac-
quired term variation for the medical domain us-
ing a two-step model. As a first step an initial list
of synonyms are extracted using a method adapted
from DIPRE (Brin, 99). During this step syntactic
patterns guide the extraction of candidate terms in
the same way as they will guide the extraction in
this paper. This first step results in a list of candi-
date synonyms that are further filtered following a
method described in Lin et al. (2003), which uses
Web pages as an external source to measure the
synonym compatibility hits of each pair. The pre-
cision and recall scores presented in Fahmi (2009)
are high. We will give results for this method
on our test set in Section 5 and refer to it as the
pattern- and web-based approach.
</bodyText>
<sectionHeader confidence="0.99541" genericHeader="method">
3 Materials and methods
</sectionHeader>
<bodyText confidence="0.999719">
In the following subsections we describe the setup
for our experiments.
</bodyText>
<subsectionHeader confidence="0.995071">
3.1 Data collection
</subsectionHeader>
<bodyText confidence="0.999870380952381">
Measures of distributional similarity usually re-
quire large amounts of data. For the alignment
method we need a parallel corpus of reasonable
size with Dutch either as source or as target lan-
guage coming from the domain we are interested
in. Furthermore, we would like to experiment
with various languages aligned to Dutch.
The freely available EMEA corpus (Tiede-
mann, 2009) includes 22 languages in parallel
with a reasonable size of about 12-14 million to-
kens per language. The entire corpus is aligned
at the sentence level for all possible combinations
of languages. Thus, for acquiring Dutch syn-
onyms we have 21 language pairs with Dutch as
the source language. Each language pair includes
about 1.1 million sentence pairs. Note that there
is a lot of repetition in EMEA and the number
of unique sentences (sentence fragments) is much
smaller: around 350,000 sentence pairs per lan-
guage pair with about 6-7 million tokens per lan-
guage.
</bodyText>
<subsectionHeader confidence="0.999901">
3.2 Word alignment and phrase extraction
</subsectionHeader>
<bodyText confidence="0.998622125">
For sentence alignment we applied hunalign
(Varga et al., 2005) with the ’realign’ function that
induces lexical features from the bitext to be com-
bined with length based features. Word alignment
has been performed using GIZA++ (Och, 2003).
We used standard settings defined in the Moses
toolkit (Koehn et al., 2007) to generate Viterbi
word alignments of IBM model 4 for sentences
</bodyText>
<page confidence="0.999446">
31
</page>
<bodyText confidence="0.999805173913044">
not longer than 80 tokens. In order to improve
the statistical alignment we used lowercased to-
kens and lemmas in case we had them available
(produced by the Tree-Tagger (Schmid, 1994) and
the Alpino parser (van Noord, 2006)).
We used the grow heuristics to combine the
asymmetric word alignments which starts with
the intersection of the two Viterbi alignments and
adds block-neighboring points to it in a second
step. In this way we obtain high precision links
with some many-to-many alignments. Finally we
used the phrase extraction tool from Moses to ex-
tract phrase correspondences. Phrases in statisti-
cal machine translation are defined as sequences
of consecutive words and phrase extraction refers
to the exhaustive extraction of all possible phrase
pairs that are consistent with the underlying word
alignment. Consistency in this case means that
words in a legal phrase are only aligned to words
in the corresponding phrase and not to any other
word outside of that phrase. The extraction mech-
anism can be restricted by setting a maximum
phrase length which is seven in the default set-
tings of Moses. However, we set the maximum
phrase length to four, because we do not expect
many terms in the medical domain to be longer
than 4 words.
As explained above, word alignment is carried
out on lowercased and possibly lemmatised ver-
sions of the corpus. However, for phrase extrac-
tion, we used surface wordforms and extracted
them along with the part-of-speech (PoS) tags for
Dutch taken from the corresponding Alpino parse
trees. This allows us to lowercase all words except
the words that have been tagged as name. Further-
more, the inclusion of PoS tags enabled us to fil-
ter the resulting phrase table according to typical
patterns of multi-word terms. We also removed
phrases that consist of only non-alphabetical char-
acters. Note that we rely entirely on automatic
processing of our data. Thus, the results from
automatic tagging, lemmatisation and word align-
ment include errors. Bannard and Callison-Burch
(2005) show that when using manual alignment
the percentage of correct paraphrases significantly
rises from 48.9% to 74.9%.
</bodyText>
<subsectionHeader confidence="0.999786">
3.3 Selecting candidate terms
</subsectionHeader>
<bodyText confidence="0.999987">
As we explained above we can select those
phrases that are more likely to be good terms
by using a regular expression over PoS tags.
We apply a pattern using adjectives (A), nouns
(NN), names (NM) and prepositions (P) as its
components based on Justeson and Katz. (1995)
which was adapted to Dutch by Fahmi (2009):
</bodyText>
<equation confidence="0.456165">
((A|NN|NM)+|(((A|NN|NM)*
(NN|NM P)?)(A|NN|NM)*))NN+
</equation>
<bodyText confidence="0.99980175">
To explain this regular expression in words, a
candidate term is either a sequence of adjectives
and/or nouns and/or names, ending in a noun or
name or it consists of two such strings, separated
by a single preposition.
After applying the filters and removing all ha-
paxes we are left with 9.76 M co-occurrences of a
Dutch (multi-word) term and a foreign translation.
</bodyText>
<subsectionHeader confidence="0.991013">
3.4 Comparing vectors
</subsectionHeader>
<bodyText confidence="0.999962388888889">
To compare the vectors of the terms we need a
similarity measures. We have chosen to describe
the functions used in this paper using an extension
of the notation used by Lin (1998), adapted by
Curran (2003). Co-occurrence data is described
as tuples: (word, language, word&apos;), for example,
(kat, EN, cat).
Asterisks indicate a set of values ranging over
all existing values of that component of the rela-
tion tuple. For example, (w, *, *) denotes for a
given word w all translational contexts it has been
found in in any language. For the example of
kat in, this would denote all values for all transla-
tional contexts the word is found in: Katze DE:17,
chat FR:26 etc. Everything is defined in terms
of co-occurrence data with non-zero frequencies.
The set of attributes or features for a given corpus
is defined as:
</bodyText>
<equation confidence="0.992293">
(w, *, *) = {(r, w&apos;)II(w, r, w&apos;)}
</equation>
<bodyText confidence="0.893827333333333">
Each pair yields a frequency value, and the se-
quence of values is a vector indexed by r:w&apos; val-
ues, rather than natural numbers. A subscripted
asterisk indicates that the variables are bound to-
gether:
1:(wm, *r, *w&apos;) X (wn, *r, *w&apos;)
</bodyText>
<page confidence="0.997579">
32
</page>
<bodyText confidence="0.999959">
The above refers to a dot product of the vectors
for term wm and term wn summing over all the
r:w&apos; pairs that these two terms have in common.
For example we could compare the vectors for kat
and some other term by applying the dot product
to all bound variables.
We have limited our experiments to using Co-
sine2. We chose this measure, since it performed
best in experiments reported in Van der Plas
(2008). Cosine is a geometrical measure. It re-
turns the cosine of the angle between the vectors
of the words and is calculated as the dot product
of the vectors:
</bodyText>
<equation confidence="0.999315666666667">
E (W 1, *r, *w&apos;) x (W2, *r, *w&apos;)
Cosine =
V/E (W 1, *, *)2 x E (W2, *, *)2
</equation>
<bodyText confidence="0.999194">
If the two words have the same distribution the
angle between the vectors is zero.
</bodyText>
<subsectionHeader confidence="0.996826">
3.5 Post-processing
</subsectionHeader>
<bodyText confidence="0.9999354">
A well-known problem of phrase-based meth-
ods to paraphrase or term variation acquisition
is the fact that a large proportion of the term
variations or paraphrases proposed by the sys-
tem are super- or sub-strings of the original term
(Callison-Burch, 2008). To remedy this prob-
lem we removed all term variations that are ei-
ther super- or sub-strings of the original term from
the lists of candidate term variations output by the
system.
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999777119047619">
There are several evaluation methods available to
assess lexico-semantic data. Curran (2003) distin-
guishes two types of evaluation: direct evaluation
and indirect evaluation. Direct evaluation meth-
ods compare the semantic relations given by the
2Feature weights have been used in previous work for
syntax-based methods to account for the fact that co-
occurrences have different information values. Selectionally
weak (Resnik, 1993) or light verbs such as hebben ‘to have’
have a lower information value than a verb such as uitpersen
‘squeeze’ that occurs less frequently. Although weights that
promote features with a higher information value work very
well for syntax-based methods, Van der Plas (2008) showed
that weighting only helps to get better synonyms for very in-
frequent nouns when applied in alignment-based approaches.
In the current setting we do not consider very infrequent
terms so we did not use any weighting.
system against human performance or expertise.
Indirect approaches evaluate the system by mea-
suring its performance on a specific task.
Since we are not aware of a task in which we
could test the term variations for the Dutch medi-
cal domain and ad-hoc human judgments are time
consuming and expensive, we decided to com-
pare against a gold standard. Thereby denying
the common knowledge that the drawback of us-
ing gold standard evaluations is the fact that gold
standards often prove to be incomplete. In previ-
ous work on synonym acquisition for the general
domain, Van der Plas and Tiedemann (2006) used
the synsets in Dutch EuroWordnet (Vossen, 1998)
for the evaluation of the proposed synonyms. In
an evaluation with human judgments, Van der Plas
and Tiedemann (2006) showed that in 37% of the
cases the majority of the subjects judged the syn-
onyms proposed by the system to be correct even
though they were not found to be synonyms in
Dutch EuroWordnet. For evaluating medical term
variations in Dutch there are not many gold stan-
dards available. Moreover, the gold standards that
are available are even less complete than for the
general domain.
</bodyText>
<subsectionHeader confidence="0.996653">
4.1 Gold standard
</subsectionHeader>
<bodyText confidence="0.999932705882353">
We have chosen to evaluate the nearest neighbours
of the alignment-based method on the term vari-
ations from the Elseviers medical encyclopedia
which is intended for the general audience con-
taining 379K words. The encyclopedia was made
available to us by Spectrum B.V.3.
The test set is comprised of 848 medical terms
from aambeeld ‘incus’ to zwezerik ‘thymus’ and
their term variations. About 258 of these entries
contain multiword terms. For most of the terms
the list from Elseviers medical encyclopedia gives
only one term variation, 146 terms have two term
variations and only one term has three variations.
For each of these medical terms in the test set the
system generates a ranked list of term variations
that will be evaluated against the term variations
in the gold standard.
</bodyText>
<footnote confidence="0.952917">
3http://www.kiesbeter.nl/medischeinformatie/
</footnote>
<page confidence="0.99927">
33
</page>
<bodyText confidence="0.99953475">
Certainly, for paraphrasing we are not only inter-
ested in ˆe2 but for the top-ranked paraphrase can-
didates but this essentially does not change the al-
gorithm. In their paper, Bannard and Callison-
Burch (2005) also show that systematic errors
(usually originating from bad word alignments)
can be reduced by summing over several language
pairs.
</bodyText>
<equation confidence="0.95175">
�ˆe2 ≈ argmaxe2:e2�e1
</equation>
<bodyText confidence="0.999936555555555">
This is the approach that we also adapted for our
comparison. The only difference in our imple-
mentation is that we applied a PoS-filter to extract
candidate terms as explained in section 3.3. In
some sense this is a sort of syntactic constraint in-
troduced in Callison-Burch (2008). Furthermore,
we set the maximum phrase length to 4 and ap-
plied the same post-processing as described in
Subsection 3.5 to obtain comparable results.
</bodyText>
<subsectionHeader confidence="0.911124">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999601260869565">
Table 2 shows the results for our method com-
pared with the method adapted from Bannard and
Callison-Burch (2005) and the method by Fahmi
(2009). Precision and recall are given at several
values of k. At k=1, only the top-1 term varia-
tions the system proposes are taken into account.
At k=3 the top-3 candidate term variations are in-
cluded in the calculations.
The last column shows the coverage of the sys-
tem. A coverage of 40% means that for 40% of the
850 terms in the test set one or more term varia-
tions are found. Recall is measured for the terms
covered by the system.
From Table 2 we can read that the method we
propose is able to get about 30% of the term vari-
ations right, when only the top-1 candidates are
considered. It is able to retrieve roughly a quarter
of the term variations provided in the gold stan-
dard4. If we increase k precision goes down and
recall goes up. This is expected, because the sys-
tem proposes a ranked list of candidate term vari-
ations so at higher values of k the quality is lower,
but more terms from the gold standard are found.
</bodyText>
<footnote confidence="0.6756655">
4Note that a recall of 100% is not possible, because some
terms have several term variations.
</footnote>
<sectionHeader confidence="0.795216" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9998554">
Before we present our results and give a detailed
error analysis we would like to remind the reader
of the two methods we compare our results with
and give some more detail on the implementation
of the second method.
</bodyText>
<subsectionHeader confidence="0.689733">
5.1 Two methods for comparison
</subsectionHeader>
<bodyText confidence="0.9996">
The first method is the pattern- and web-based ap-
proach described in Fahmi (2009). Note that we
did not re-implement the method, so we were not
able to run the method on the same corpus we
are using in our experiments. The corpus used
in Fahmi (2009) is a medical corpus developed
in Tilburg University (http://ilk.uvt.nl/rolaquad).
It consists of texts from a medical encyclopedia
and a medical handbook and contains 57,004 sen-
tences. The system outputs a ranked list of term
variation pairs. We selected the top-100 pairs
that are output by the system and evaluated these
on the test set described in Subsection 4.1. The
method is composed of two main steps. In the
first step candidate terms are extracted from the
corpus using a PoS filter, that is similar to the
PoS filter we applied. In the second step pairs of
candidate term variations are re-ranked on the ba-
sis of information from the Web. Phrasal patterns
such as XorY are used to get synonym compat-
ibility hits as opposed to XandY that points to
non-synonymous terms.
</bodyText>
<figureCaption confidence="0.7018935">
The second method we compare with is the
phrase-based translation method first introduced
by Bannard and Callison-Burch (2005). Statisti-
cal word alignment can be used to measure the re-
lation between source language items. Here, one
makes use of the estimated translation likelihoods
of phrases (p(f|e) and p(e|f)) that are used to
build translation models in standard phrase-based
statistical machine translation systems (Koehn et
al., 2007). Bannard and Callison-Burch (2005)
define the problem of paraphrasing as the follow-
ing search problem:
</figureCaption>
<equation confidence="0.683558666666667">
ˆe2 = argmaxe2:e2=Ae1p(e2|e1) where
�p(e2|e1) ≈ p(f|e1)p(e2|f)
f
E
C fc
p(fC|e1)p(e2|fC)
</equation>
<page confidence="0.995592">
34
</page>
<table confidence="0.999584666666667">
Method k=1 k=2 k=3 Coverage
P R P R P R
Phrase-based Distr. Sim 28.9 22.8 21.8 32.7 17.3 37.2 40.0
Bannard&amp;Callison-Burch (2005) 18.4 15.3 16.9 27.3 13.7 32.3 48.1
Fahmi (2009) 38.2 35.1 37.1 35.1 37.1 35.1 4.0
Phrase-based Distr. Sim (hapaxes) 25.4 20.9 20.4 32.1 16.1 36.8 47.8
</table>
<tableCaption confidence="0.989548">
Table 2: Percent precision and recall at several values of k and percent coverage for the method pro-
</tableCaption>
<bodyText confidence="0.991882864864865">
posed in this paper (plus a version including hapaxes), the method adapted from Bannard and Callison-
Burch (2005) and the output of the system proposed by Fahmi (2009)
In comparison, the scores resulting from our
adapted implementation of Bannard and Callison-
Burch (2005) are lower. They do however, man-
age to find more terms from the test set covering
around 48% of the words in the gold standard.
This is due to the cut-off that we use when cre-
ating the co-occurrence vector to remove unreli-
able data points. In our approach we discarded
hapaxes, whereas for the Bannard and Callison-
Burch approach the entire phrase table is used.
We therefore ran our system once again without
this cut-off. As expected, the coverage went up
in that setting – actually to 48% as well.5 How-
ever, we can see that the precision and recall re-
mained higher, than the scores we got with the
implementation following Bannard and Callison-
Burch (2005). Hence, our vector-based approach
seems to outperform the direct use of probabilities
from phrase-based MT.
Finally, we also compare our results with the
data set extracted using the pattern- and web-
based approach from Fahmi (2009). The precision
and recall figures of that data set are the highest in
our comparison. However, since the coverage of
this method is very low (which is not surprising
since a smaller corpus is used to get these results)
the precision and recall are calculated on the ba-
sis of a very small number of examples (35 to be
precise). The results are therefore not very reli-
able. The precision and recall figures presented
in Fahmi (2009), however, point in the same di-
rection. To get an idea of the actual coverage of
this method we would need to apply this extrac-
tion technique to the EMEA corpus. This is espe-
cially difficult due to the heavy use of web queries
</bodyText>
<footnote confidence="0.510534">
5The small difference in coverage is due to some mistakes
in tokenisation for our method.
</footnote>
<bodyText confidence="0.97546">
which makes it problematic to apply this method
to large data sets.
</bodyText>
<subsectionHeader confidence="0.976142">
5.3 Error analysis
</subsectionHeader>
<bodyText confidence="0.9448958">
The most important finding we did, when closely
inspecting the output of the system is that many of
the term variations proposed by the system are not
found in the gold standard, but are in fact correct.
Here, we give some examples below:
arts, dokter (‘doctor’)
ademnood, ademhalingsnood (‘respiratory distress’)
aangezichtsverlamming, gelaatsparalyse (‘facial paralysis’)
alvleesklierkanker, pancreaskanker (‘cancer of the pan-
creas’)
The scores given in Table 2 are therefore pes-
simistic and a manual evaluation with domain spe-
cialist would certainly give us more realistic and
probably much higher scores. We also found some
spelling variants which are usually not covered by
the gold standard. Look, for instance, at the fol-
lowing examples:
astma, asthma (‘asthma’)
atherosclerose, Artherosclerosis (‘atherosclerosis’)
autonoom zenuwstelsel, autonome zenuwstelsel (‘autonomic
nervous system’)
Some mistakes could have been avoided using
stemming or proper lemmatisation (plurals that
are counted as wrong):
abortus, zwangerschapsafbrekingen (‘abortion’)
adenoom, adenomen (‘adenoma’)
indigestie, spijsverteringsstoornissen (‘indigestion’)
After removing the previous cases from the data,
some of the remaining mistakes are related to the
problem we mentioned in section 3.5: Phrase-
</bodyText>
<page confidence="0.997455">
35
</page>
<bodyText confidence="0.9996324">
based methods to paraphrase or term variation ac-
quisition have the tendency to propose term vari-
ations that are super- or sub-strings of the origi-
nal term. We were able to filter out these super-
or sub-strings, but not in cases where a candidate
term is a term variation of a super- or sub-string of
the original term. Consider, for example the term
bloeddrukverlaging ‘blood pressure decrease’ and
the candidate afname ‘decrease’, where afname is
a synonym for verlaging.
</bodyText>
<sectionHeader confidence="0.999454" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999984857142857">
In this article we have shown that translational
context together with measures of distributional
similarity can be used to extract medical term vari-
ations from aligned parallel corpora. Automatic
word alignment and phrase extraction techniques
from statistical machine translation can be applied
to collect translational variations across various
languages which are then used to identify seman-
tically related words and phrases. In this study, we
additionally apply pattern-based filters using part-
of-speech labels to focus on particular patterns of
single and multi-word terms. Our method out-
performs another alignment-based approach mea-
sured on a gold standard taken from a medical en-
cyclopedia when applied to the same data set and
using the same PoS filter. Precision and recall are
still quite poor according to the automatic evalu-
ation. However, manual inspection suggests that
many candidates are simply misjudged because of
the low coverage of the gold standard data. We
are currently setting up a manual evaluation. Alto-
gether our approach provides a promising strategy
for the extraction of term variations using straight-
forward and fully automatic techniques. We be-
lieve that our results could be useful for a range of
applications and resources and that the approach
in general is robust and flexible enough to be ap-
plied to various languages and domains.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9992665">
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLAS-
SIC project: www.classic-project.org).
</bodyText>
<sectionHeader confidence="0.989647" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998952204081633">
Bannard, C. and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings
of the annual Meeting of the Association for Com-
putational Linguistics (ACL).
Brin, S. 99. Extracting patterns and relations from the
World Wide Web. In WebDB ’98: Selected papers
from the International Workshop on The World Wide
Web and Databases.
Brown, P.F., S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263–296.
Callison-Burch, C. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Curran, J.R. 2003. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Dyvik, H. 1998. Translations as semantic mirrors.
In Proceedings of Workshop Multilinguality in the
Lexicon II (ECAI).
Dyvik, H. 2002. Translations as semantic mirrors:
from parallel corpus to wordnet. Language and
Computers, Advances in Corpus Linguistics. Pa-
pers from the 23rd International Conference on En-
glish Language Research on Computerized Corpora
(ICAME 23), 16:311–326.
Fahmi, I. 2009. Automatic Term and Relation Extrac-
tion for Medical Question Answering System. Ph.D.
thesis, University of Groningen.
Firth, J.R. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis (special vol-
ume of the Philological Society), pages 1–32.
Harris, Z.S. 1968. Mathematical structures of lan-
guage. Wiley.
Ide, N., T. Erjavec, and D. Tufis. 2002. Sense discrim-
ination with parallel corpora. In Proceedings of the
ACL Workshop on Sense Disambiguation: Recent
Successes and Future Directions.
Justeson, J. and S. Katz. 1995. Technical terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural Language Engineer-
ing, 1:9–27.
Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,
M.Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A.Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics.
</reference>
<page confidence="0.975074">
36
</page>
<reference confidence="0.999641314285715">
Koehn, P. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proceedings of the MT
Summit, pages 79–86, Phuket, Thailand.
Lin, D., S. Zhao, L. Qin, and M. Zhou. 2003. Identify-
ing synonyms among distributionally similar words.
In Proceedings of the International Joint Confer-
ence on Artificial Intelligence (IJCAI).
Lin, D. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL.
McCray, A. and W. Hole. 1990. The scope and struc-
ture of the first version of the umls semantic net-
work. In Symposium on Computer Applications in
Primary Care (SCAMC-90), IEEE Computer Soci-
ety, pages 126–130, , Washington DC, IEEE Com-
puter Society. 126-130.
Och, F.J. 2003. GIZA++: Training of sta-
tistical translation models. Available from
http://www.isi.edu/˜och/GIZA++.html.
Resnik, P. and D. Yarowsky. 1997. A perspective on
word sense disambiguation methods and their eval-
uation. In Proceedings of ACL SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, what,
and how?
Resnik, P. 1993. Selection and information. Unpub-
lished doctoral thesis, University of Pennsylvania.
Schmid, Helmut. 1994. Probabilistic part-of-
speech tagging using decision trees. In Pro-
ceedings of International Conference on New
Methods in Language Processing, pages 44–49,
Manchester, UK, September. http://www.ims.uni-
stuttgart.de/˜schmid/.
Sch¨utze, H. 1992. Dimensions of meaning. In Pro-
ceedings of the ACM/IEEE conference on Super-
computing.
Shimota, M. and E. Sumita. 2002. Automatic para-
phrasing based on parallel corpus for normalization.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).
Tiedemann, J¨org. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In Nicolov, N., K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances
in Natural Language Processing, volume V, pages
237–248, Borovets, Bulgaria. John Benjamins, Am-
sterdam/Philadelphia.
van der Plas, L. and J. Tiedemann. 2006. Finding syn-
onyms using automatic word alignment and mea-
sures of distributional similarity. In Proceedings of
COLING/ACL.
van der Plas. 2008. Automatic lexico-semantic acqui-
sition for question answering. Groningen disserta-
tions in linguistics.
van Noord, G. 2006. At last parsing is now oper-
ational. In Actes de la 13eme Conference sur le
Traitement Automatique des Langues Naturelles.
Varga, D., L. Nmeth, P. Halcsy, A. Kornai, V. Trn, and
V. Nagy. 2005. Parallel corpora for medium density
languages. In Proceedings of RANLP 2005, pages
590–596.
Vossen, P. 1998. EuroWordNet a multilingual
database with lexical semantic networks.
Wilks, Y., D. Fass, Ch. M. Guo, J. E. McDonald,
and B. M. Slator T. Plate. 1993. Providing ma-
chine tractable dictionary tools. Machine Transla-
tion, 5(2):99–154.
Wu, H. and M. Zhou. 2003. Optimizing synonym ex-
traction using monolingual and bilingual resources.
In Proceedings of the International Workshop on
Paraphrasing: Paraphrase Acquisition and Appli-
cations (IWP).
</reference>
<page confidence="0.999597">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495098">
<title confidence="0.929842">Finding Medical Term Variations using Parallel Corpora Distributional Similarity</title>
<author confidence="0.998384">Lonneke van_der_Plas J¨org Tiedemann</author>
<affiliation confidence="0.999843">Department of Linguistics Department of Linguistics and Philology University of Geneva Uppsala University</affiliation>
<email confidence="0.599058">lonneke.vanderplas@unige.chjorg.tiedemann@lingfil.uu.se</email>
<abstract confidence="0.996821416666667">We describe a method for the identification of medical term variations using parallel corpora and measures of distributional similarity. Our approach is based on automatic word alignment and standard phrase extraction techniques commonly used in statistical machine translation. Combined with pattern-based filters we obtain encouraging results compared to related approaches using similar datadriven techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>C Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="12615" citStr="Bannard and Callison-Burch (2005)" startWordPosition="2027" endWordPosition="2030">ature sets . Improving the syntax-based approach for synonym identification using bilingual dictionaries has been discussed in Lin et al. (2003) and Wu and Zhou (2003). In the latter parallel corpora are also applied as a reference to assign translation likelihoods to candidates derived from the dictionary. Both of them are limited to single-word terms. Some researchers employ multilingual corpora for the automatic acquisition of paraphrases (Shimota and Sumita, 2002; Bannard and CallisonBurch, 2005; Callison-Burch, 2008). The last two are based on automatic word alignment as is our approach. Bannard and Callison-Burch (2005) use a method that is also rooted in phrase-based statistical machine translation. Translation probabilities provide a ranking of candidate paraphrases. These are refined by taking contextual information into account in the form of a language model. The Europarl corpus (Koehn, 2005) is used. It has about 30 million words per language. 46 English phrases are selected as a test set for manual evaluation by two judges. When using automatic alignment, the precision reached without using contextual refinement is 48.9%. A precision of 55.3% is reached when using context information. Manual alignment</context>
<context position="18061" citStr="Bannard and Callison-Burch (2005)" startWordPosition="2924" endWordPosition="2927">, we used surface wordforms and extracted them along with the part-of-speech (PoS) tags for Dutch taken from the corresponding Alpino parse trees. This allows us to lowercase all words except the words that have been tagged as name. Furthermore, the inclusion of PoS tags enabled us to filter the resulting phrase table according to typical patterns of multi-word terms. We also removed phrases that consist of only non-alphabetical characters. Note that we rely entirely on automatic processing of our data. Thus, the results from automatic tagging, lemmatisation and word alignment include errors. Bannard and Callison-Burch (2005) show that when using manual alignment the percentage of correct paraphrases significantly rises from 48.9% to 74.9%. 3.3 Selecting candidate terms As we explained above we can select those phrases that are more likely to be good terms by using a regular expression over PoS tags. We apply a pattern using adjectives (A), nouns (NN), names (NM) and prepositions (P) as its components based on Justeson and Katz. (1995) which was adapted to Dutch by Fahmi (2009): ((A|NN|NM)+|(((A|NN|NM)* (NN|NM P)?)(A|NN|NM)*))NN+ To explain this regular expression in words, a candidate term is either a sequence of</context>
<context position="25087" citStr="Bannard and Callison-Burch (2005)" startWordPosition="4104" endWordPosition="4107"> can be reduced by summing over several language pairs. �ˆe2 ≈ argmaxe2:e2�e1 This is the approach that we also adapted for our comparison. The only difference in our implementation is that we applied a PoS-filter to extract candidate terms as explained in section 3.3. In some sense this is a sort of syntactic constraint introduced in Callison-Burch (2008). Furthermore, we set the maximum phrase length to 4 and applied the same post-processing as described in Subsection 3.5 to obtain comparable results. 5.2 Results Table 2 shows the results for our method compared with the method adapted from Bannard and Callison-Burch (2005) and the method by Fahmi (2009). Precision and recall are given at several values of k. At k=1, only the top-1 term variations the system proposes are taken into account. At k=3 the top-3 candidate term variations are included in the calculations. The last column shows the coverage of the system. A coverage of 40% means that for 40% of the 850 terms in the test set one or more term variations are found. Recall is measured for the terms covered by the system. From Table 2 we can read that the method we propose is able to get about 30% of the term variations right, when only the top-1 candidates</context>
<context position="27566" citStr="Bannard and Callison-Burch (2005)" startWordPosition="4547" endWordPosition="4550">that are output by the system and evaluated these on the test set described in Subsection 4.1. The method is composed of two main steps. In the first step candidate terms are extracted from the corpus using a PoS filter, that is similar to the PoS filter we applied. In the second step pairs of candidate term variations are re-ranked on the basis of information from the Web. Phrasal patterns such as XorY are used to get synonym compatibility hits as opposed to XandY that points to non-synonymous terms. The second method we compare with is the phrase-based translation method first introduced by Bannard and Callison-Burch (2005). Statistical word alignment can be used to measure the relation between source language items. Here, one makes use of the estimated translation likelihoods of phrases (p(f|e) and p(e|f)) that are used to build translation models in standard phrase-based statistical machine translation systems (Koehn et al., 2007). Bannard and Callison-Burch (2005) define the problem of paraphrasing as the following search problem: ˆe2 = argmaxe2:e2=Ae1p(e2|e1) where �p(e2|e1) ≈ p(f|e1)p(e2|f) f E C fc p(fC|e1)p(e2|fC) 34 Method k=1 k=2 k=3 Coverage P R P R P R Phrase-based Distr. Sim 28.9 22.8 21.8 32.7 17.3 </context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Bannard, C. and C. Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Brin</author>
</authors>
<title>Extracting patterns and relations from the World Wide Web. In WebDB ’98: Selected papers from the International Workshop on The World Wide Web and Databases.</title>
<marker>Brin, </marker>
<rawString>Brin, S. 99. Extracting patterns and relations from the World Wide Web. In WebDB ’98: Selected papers from the International Workshop on The World Wide Web and Databases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9215" citStr="Brown et al., 1993" startWordPosition="1474" endWordPosition="1477"> explain how measures serve to determine the similarity of these contexts. We end this section with a discussion of related work. 2.1 Translational context The translational context of a word or a multiword term is the set of translations it gets in other languages. For the acquisition of translations for the Dutch medical terms we rely on automatic word alignment in parallel corpora. Figure 1: Example of bidirectional word alignments of two parallel sentences Figure 1 illustrates the automatic word alignment between a Dutch and an English phrase as a result of using the IBM alignment models (Brown et al., 1993) implemented in the open-source tool GIZA++ (Och, 2003). The alignment of two texts is bi-directional. The Dutch text is aligned to the English text and vice versa (dotted lines versus continuous lines). The alignment models produced are asymmetric. Several heuristics exist to combine directional word alignments which is usually called “symmetrization”. In order to cover multi-word terms standard phrase extraction techniques can be used to move from word alignment to linked phrases (see section 3.2 for more details). 2.2 Measures for computing similarity Translational co-occurrence vectors are</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, P.F., S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="12509" citStr="Callison-Burch, 2008" startWordPosition="2012" endWordPosition="2013">relations such synonymy and hyponymy are detected based on intersection and inclusion among feature sets . Improving the syntax-based approach for synonym identification using bilingual dictionaries has been discussed in Lin et al. (2003) and Wu and Zhou (2003). In the latter parallel corpora are also applied as a reference to assign translation likelihoods to candidates derived from the dictionary. Both of them are limited to single-word terms. Some researchers employ multilingual corpora for the automatic acquisition of paraphrases (Shimota and Sumita, 2002; Bannard and CallisonBurch, 2005; Callison-Burch, 2008). The last two are based on automatic word alignment as is our approach. Bannard and Callison-Burch (2005) use a method that is also rooted in phrase-based statistical machine translation. Translation probabilities provide a ranking of candidate paraphrases. These are refined by taking contextual information into account in the form of a language model. The Europarl corpus (Koehn, 2005) is used. It has about 30 million words per language. 46 English phrases are selected as a test set for manual evaluation by two judges. When using automatic alignment, the precision reached without using contex</context>
<context position="21044" citStr="Callison-Burch, 2008" startWordPosition="3449" endWordPosition="3450"> reported in Van der Plas (2008). Cosine is a geometrical measure. It returns the cosine of the angle between the vectors of the words and is calculated as the dot product of the vectors: E (W 1, *r, *w&apos;) x (W2, *r, *w&apos;) Cosine = V/E (W 1, *, *)2 x E (W2, *, *)2 If the two words have the same distribution the angle between the vectors is zero. 3.5 Post-processing A well-known problem of phrase-based methods to paraphrase or term variation acquisition is the fact that a large proportion of the term variations or paraphrases proposed by the system are super- or sub-strings of the original term (Callison-Burch, 2008). To remedy this problem we removed all term variations that are either super- or sub-strings of the original term from the lists of candidate term variations output by the system. 4 Evaluation There are several evaluation methods available to assess lexico-semantic data. Curran (2003) distinguishes two types of evaluation: direct evaluation and indirect evaluation. Direct evaluation methods compare the semantic relations given by the 2Feature weights have been used in previous work for syntax-based methods to account for the fact that cooccurrences have different information values. Selection</context>
<context position="24812" citStr="Callison-Burch (2008)" startWordPosition="4061" endWordPosition="4062">ing we are not only interested in ˆe2 but for the top-ranked paraphrase candidates but this essentially does not change the algorithm. In their paper, Bannard and CallisonBurch (2005) also show that systematic errors (usually originating from bad word alignments) can be reduced by summing over several language pairs. �ˆe2 ≈ argmaxe2:e2�e1 This is the approach that we also adapted for our comparison. The only difference in our implementation is that we applied a PoS-filter to extract candidate terms as explained in section 3.3. In some sense this is a sort of syntactic constraint introduced in Callison-Burch (2008). Furthermore, we set the maximum phrase length to 4 and applied the same post-processing as described in Subsection 3.5 to obtain comparable results. 5.2 Results Table 2 shows the results for our method compared with the method adapted from Bannard and Callison-Burch (2005) and the method by Fahmi (2009). Precision and recall are given at several values of k. At k=1, only the top-1 term variations the system proposes are taken into account. At k=3 the top-3 candidate term variations are included in the calculations. The last column shows the coverage of the system. A coverage of 40% means tha</context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Callison-Burch, C. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="2367" citStr="Curran, 2003" startWordPosition="359" endWordPosition="361">n of synonymy. tions. These term variations could be used to enhance existing medical ontologies for the Dutch language. Our technique builds on the distributional hypothesis, the idea that semantically related words are distributed similarly over contexts (Harris, 1968). This is in line with the Firthian saying that, ’You shall know a word by the company it keeps.’ (Firth, 1957). In other words, you can grasp the meaning of a word by looking at its contexts. Context can be defined in many ways. Previous work has been mainly concerned with the syntactic contexts a word is found in (Lin, 1998; Curran, 2003). For example, the verbs that are in a subject relation with a particular noun form a part of its context. In accordance with the Firthian tradition these contexts can be used to determine the semantic relatedness of words. For instance, words that occur in a object relation with the verb to drink have something in common: they are liquid. Other work has been concerned with the bagof-word context, where the context of a word are the words that are found in its proximity (Wilks et al., 1993; Sch¨utze, 1992). Yet another context, that is much less studied, is the translational context. The trans</context>
<context position="19171" citStr="Curran (2003)" startWordPosition="3109" endWordPosition="3110">?)(A|NN|NM)*))NN+ To explain this regular expression in words, a candidate term is either a sequence of adjectives and/or nouns and/or names, ending in a noun or name or it consists of two such strings, separated by a single preposition. After applying the filters and removing all hapaxes we are left with 9.76 M co-occurrences of a Dutch (multi-word) term and a foreign translation. 3.4 Comparing vectors To compare the vectors of the terms we need a similarity measures. We have chosen to describe the functions used in this paper using an extension of the notation used by Lin (1998), adapted by Curran (2003). Co-occurrence data is described as tuples: (word, language, word&apos;), for example, (kat, EN, cat). Asterisks indicate a set of values ranging over all existing values of that component of the relation tuple. For example, (w, *, *) denotes for a given word w all translational contexts it has been found in in any language. For the example of kat in, this would denote all values for all translational contexts the word is found in: Katze DE:17, chat FR:26 etc. Everything is defined in terms of co-occurrence data with non-zero frequencies. The set of attributes or features for a given corpus is def</context>
<context position="21330" citStr="Curran (2003)" startWordPosition="3495" endWordPosition="3496">distribution the angle between the vectors is zero. 3.5 Post-processing A well-known problem of phrase-based methods to paraphrase or term variation acquisition is the fact that a large proportion of the term variations or paraphrases proposed by the system are super- or sub-strings of the original term (Callison-Burch, 2008). To remedy this problem we removed all term variations that are either super- or sub-strings of the original term from the lists of candidate term variations output by the system. 4 Evaluation There are several evaluation methods available to assess lexico-semantic data. Curran (2003) distinguishes two types of evaluation: direct evaluation and indirect evaluation. Direct evaluation methods compare the semantic relations given by the 2Feature weights have been used in previous work for syntax-based methods to account for the fact that cooccurrences have different information values. Selectionally weak (Resnik, 1993) or light verbs such as hebben ‘to have’ have a lower information value than a verb such as uitpersen ‘squeeze’ that occurs less frequently. Although weights that promote features with a higher information value work very well for syntax-based methods, Van der P</context>
</contexts>
<marker>Curran, 2003</marker>
<rawString>Curran, J.R. 2003. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dyvik</author>
</authors>
<title>Translations as semantic mirrors.</title>
<date>1998</date>
<booktitle>In Proceedings of Workshop Multilinguality in the Lexicon II (ECAI).</booktitle>
<contexts>
<context position="11354" citStr="Dyvik, 1998" startWordPosition="1831" endWordPosition="1832">r kat (’cat’) based on four languages The more similar the vectors are, the more distributionally similar the head terms are. We need a way to compare the vectors for any two head terms to be able to express the similarity between them by means of a score. Various methods can be used to compute the distributional similarity between terms. We will explain in section 3 what measures we have chosen in the current experiments. 2.3 Related work Multilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as separation of senses (Resnik and Yarowsky, 1997; Dyvik, 1998; Ide et al., 2002). However, taking sense separation as a basis, Dyvik (2002) derives relations such as synonymy and hyponymy by applying the method of semantic mirrors. The paper illustrates how the method works. First, different senses are identified on the basis of manual word translations in sentence-aligned Norwegian-English data (2,6 million words in total). Second, senses are grouped in semantic fields. Third, features are FR chat 26 IT gatto 8 EN cat 13 total 64 30 assigned on the basis of inheritance. Lastly, semantic relations such synonymy and hyponymy are detected based on interse</context>
</contexts>
<marker>Dyvik, 1998</marker>
<rawString>Dyvik, H. 1998. Translations as semantic mirrors. In Proceedings of Workshop Multilinguality in the Lexicon II (ECAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dyvik</author>
</authors>
<title>Translations as semantic mirrors: from parallel corpus to wordnet. Language and Computers,</title>
<date>2002</date>
<booktitle>Advances in Corpus Linguistics. Papers from the 23rd International Conference on English Language Research on Computerized Corpora (ICAME</booktitle>
<volume>23</volume>
<pages>16--311</pages>
<contexts>
<context position="11432" citStr="Dyvik (2002)" startWordPosition="1844" endWordPosition="1845">ore distributionally similar the head terms are. We need a way to compare the vectors for any two head terms to be able to express the similarity between them by means of a score. Various methods can be used to compute the distributional similarity between terms. We will explain in section 3 what measures we have chosen in the current experiments. 2.3 Related work Multilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as separation of senses (Resnik and Yarowsky, 1997; Dyvik, 1998; Ide et al., 2002). However, taking sense separation as a basis, Dyvik (2002) derives relations such as synonymy and hyponymy by applying the method of semantic mirrors. The paper illustrates how the method works. First, different senses are identified on the basis of manual word translations in sentence-aligned Norwegian-English data (2,6 million words in total). Second, senses are grouped in semantic fields. Third, features are FR chat 26 IT gatto 8 EN cat 13 total 64 30 assigned on the basis of inheritance. Lastly, semantic relations such synonymy and hyponymy are detected based on intersection and inclusion among feature sets . Improving the syntax-based approach f</context>
</contexts>
<marker>Dyvik, 2002</marker>
<rawString>Dyvik, H. 2002. Translations as semantic mirrors: from parallel corpus to wordnet. Language and Computers, Advances in Corpus Linguistics. Papers from the 23rd International Conference on English Language Research on Computerized Corpora (ICAME 23), 16:311–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Fahmi</author>
</authors>
<title>Automatic Term and Relation Extraction for Medical Question Answering System.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Groningen.</institution>
<contexts>
<context position="13819" citStr="Fahmi (2009)" startWordPosition="2219" endWordPosition="2220">lignment improves the performance by 26%. A precision score of 55% is attained when using multilingual data. In a more recent publication Callison-Burch (2008) improved this method by using syntactic constraints and multiple languages in parallel. We have implemented a combination of Bannard and Callison-Burch (2005) and Callison-Burch (2008), in which we use PoS filters instead of syntactic constraints to compare our results with. More details can be found in the Section 5. Apart from methods that use parallel corpora mono-lingual pattern-based methods have been used to find term variations. Fahmi (2009) acquired term variation for the medical domain using a two-step model. As a first step an initial list of synonyms are extracted using a method adapted from DIPRE (Brin, 99). During this step syntactic patterns guide the extraction of candidate terms in the same way as they will guide the extraction in this paper. This first step results in a list of candidate synonyms that are further filtered following a method described in Lin et al. (2003), which uses Web pages as an external source to measure the synonym compatibility hits of each pair. The precision and recall scores presented in Fahmi </context>
<context position="18522" citStr="Fahmi (2009)" startWordPosition="3003" endWordPosition="3004">ic processing of our data. Thus, the results from automatic tagging, lemmatisation and word alignment include errors. Bannard and Callison-Burch (2005) show that when using manual alignment the percentage of correct paraphrases significantly rises from 48.9% to 74.9%. 3.3 Selecting candidate terms As we explained above we can select those phrases that are more likely to be good terms by using a regular expression over PoS tags. We apply a pattern using adjectives (A), nouns (NN), names (NM) and prepositions (P) as its components based on Justeson and Katz. (1995) which was adapted to Dutch by Fahmi (2009): ((A|NN|NM)+|(((A|NN|NM)* (NN|NM P)?)(A|NN|NM)*))NN+ To explain this regular expression in words, a candidate term is either a sequence of adjectives and/or nouns and/or names, ending in a noun or name or it consists of two such strings, separated by a single preposition. After applying the filters and removing all hapaxes we are left with 9.76 M co-occurrences of a Dutch (multi-word) term and a foreign translation. 3.4 Comparing vectors To compare the vectors of the terms we need a similarity measures. We have chosen to describe the functions used in this paper using an extension of the nota</context>
<context position="25118" citStr="Fahmi (2009)" startWordPosition="4112" endWordPosition="4113">rs. �ˆe2 ≈ argmaxe2:e2�e1 This is the approach that we also adapted for our comparison. The only difference in our implementation is that we applied a PoS-filter to extract candidate terms as explained in section 3.3. In some sense this is a sort of syntactic constraint introduced in Callison-Burch (2008). Furthermore, we set the maximum phrase length to 4 and applied the same post-processing as described in Subsection 3.5 to obtain comparable results. 5.2 Results Table 2 shows the results for our method compared with the method adapted from Bannard and Callison-Burch (2005) and the method by Fahmi (2009). Precision and recall are given at several values of k. At k=1, only the top-1 term variations the system proposes are taken into account. At k=3 the top-3 candidate term variations are included in the calculations. The last column shows the coverage of the system. A coverage of 40% means that for 40% of the 850 terms in the test set one or more term variations are found. Recall is measured for the terms covered by the system. From Table 2 we can read that the method we propose is able to get about 30% of the term variations right, when only the top-1 candidates are considered. It is able to </context>
<context position="26490" citStr="Fahmi (2009)" startWordPosition="4366" endWordPosition="4367">ause the system proposes a ranked list of candidate term variations so at higher values of k the quality is lower, but more terms from the gold standard are found. 4Note that a recall of 100% is not possible, because some terms have several term variations. 5 Results and Discussion Before we present our results and give a detailed error analysis we would like to remind the reader of the two methods we compare our results with and give some more detail on the implementation of the second method. 5.1 Two methods for comparison The first method is the pattern- and web-based approach described in Fahmi (2009). Note that we did not re-implement the method, so we were not able to run the method on the same corpus we are using in our experiments. The corpus used in Fahmi (2009) is a medical corpus developed in Tilburg University (http://ilk.uvt.nl/rolaquad). It consists of texts from a medical encyclopedia and a medical handbook and contains 57,004 sentences. The system outputs a ranked list of term variation pairs. We selected the top-100 pairs that are output by the system and evaluated these on the test set described in Subsection 4.1. The method is composed of two main steps. In the first step ca</context>
<context position="28253" citStr="Fahmi (2009)" startWordPosition="4657" endWordPosition="4658">urce language items. Here, one makes use of the estimated translation likelihoods of phrases (p(f|e) and p(e|f)) that are used to build translation models in standard phrase-based statistical machine translation systems (Koehn et al., 2007). Bannard and Callison-Burch (2005) define the problem of paraphrasing as the following search problem: ˆe2 = argmaxe2:e2=Ae1p(e2|e1) where �p(e2|e1) ≈ p(f|e1)p(e2|f) f E C fc p(fC|e1)p(e2|fC) 34 Method k=1 k=2 k=3 Coverage P R P R P R Phrase-based Distr. Sim 28.9 22.8 21.8 32.7 17.3 37.2 40.0 Bannard&amp;Callison-Burch (2005) 18.4 15.3 16.9 27.3 13.7 32.3 48.1 Fahmi (2009) 38.2 35.1 37.1 35.1 37.1 35.1 4.0 Phrase-based Distr. Sim (hapaxes) 25.4 20.9 20.4 32.1 16.1 36.8 47.8 Table 2: Percent precision and recall at several values of k and percent coverage for the method proposed in this paper (plus a version including hapaxes), the method adapted from Bannard and CallisonBurch (2005) and the output of the system proposed by Fahmi (2009) In comparison, the scores resulting from our adapted implementation of Bannard and CallisonBurch (2005) are lower. They do however, manage to find more terms from the test set covering around 48% of the words in the gold standard</context>
<context position="29616" citStr="Fahmi (2009)" startWordPosition="4890" endWordPosition="4891"> whereas for the Bannard and CallisonBurch approach the entire phrase table is used. We therefore ran our system once again without this cut-off. As expected, the coverage went up in that setting – actually to 48% as well.5 However, we can see that the precision and recall remained higher, than the scores we got with the implementation following Bannard and CallisonBurch (2005). Hence, our vector-based approach seems to outperform the direct use of probabilities from phrase-based MT. Finally, we also compare our results with the data set extracted using the pattern- and webbased approach from Fahmi (2009). The precision and recall figures of that data set are the highest in our comparison. However, since the coverage of this method is very low (which is not surprising since a smaller corpus is used to get these results) the precision and recall are calculated on the basis of a very small number of examples (35 to be precise). The results are therefore not very reliable. The precision and recall figures presented in Fahmi (2009), however, point in the same direction. To get an idea of the actual coverage of this method we would need to apply this extraction technique to the EMEA corpus. This is</context>
</contexts>
<marker>Fahmi, 2009</marker>
<rawString>Fahmi, I. 2009. Automatic Term and Relation Extraction for Medical Question Answering System. Ph.D. thesis, University of Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955. Studies in Linguistic Analysis (special volume of the Philological Society),</title>
<date>1957</date>
<pages>1--32</pages>
<contexts>
<context position="2136" citStr="Firth, 1957" startWordPosition="316" endWordPosition="317"> multiple ways to refer to the same concept using distinct (multi-word) terms are examples of synonymy1 but are often referred to as term varia1Spelling variants are a type of term variations that are not included in the definition of synonymy. tions. These term variations could be used to enhance existing medical ontologies for the Dutch language. Our technique builds on the distributional hypothesis, the idea that semantically related words are distributed similarly over contexts (Harris, 1968). This is in line with the Firthian saying that, ’You shall know a word by the company it keeps.’ (Firth, 1957). In other words, you can grasp the meaning of a word by looking at its contexts. Context can be defined in many ways. Previous work has been mainly concerned with the syntactic contexts a word is found in (Lin, 1998; Curran, 2003). For example, the verbs that are in a subject relation with a particular noun form a part of its context. In accordance with the Firthian tradition these contexts can be used to determine the semantic relatedness of words. For instance, words that occur in a object relation with the verb to drink have something in common: they are liquid. Other work has been concern</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, J.R. 1957. A synopsis of linguistic theory 1930-1955. Studies in Linguistic Analysis (special volume of the Philological Society), pages 1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>Mathematical structures of language.</title>
<date>1968</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="2025" citStr="Harris, 1968" startWordPosition="295" endWordPosition="296">i-word terms such as aangeboren afwijking ‘birth defect’ for congenitale aandoening ‘congenital disorder’. These multiple ways to refer to the same concept using distinct (multi-word) terms are examples of synonymy1 but are often referred to as term varia1Spelling variants are a type of term variations that are not included in the definition of synonymy. tions. These term variations could be used to enhance existing medical ontologies for the Dutch language. Our technique builds on the distributional hypothesis, the idea that semantically related words are distributed similarly over contexts (Harris, 1968). This is in line with the Firthian saying that, ’You shall know a word by the company it keeps.’ (Firth, 1957). In other words, you can grasp the meaning of a word by looking at its contexts. Context can be defined in many ways. Previous work has been mainly concerned with the syntactic contexts a word is found in (Lin, 1998; Curran, 2003). For example, the verbs that are in a subject relation with a particular noun form a part of its context. In accordance with the Firthian tradition these contexts can be used to determine the semantic relatedness of words. For instance, words that occur in </context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Harris, Z.S. 1968. Mathematical structures of language. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>T Erjavec</author>
<author>D Tufis</author>
</authors>
<title>Sense discrimination with parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Sense Disambiguation: Recent Successes and Future Directions.</booktitle>
<contexts>
<context position="11373" citStr="Ide et al., 2002" startWordPosition="1833" endWordPosition="1836"> based on four languages The more similar the vectors are, the more distributionally similar the head terms are. We need a way to compare the vectors for any two head terms to be able to express the similarity between them by means of a score. Various methods can be used to compute the distributional similarity between terms. We will explain in section 3 what measures we have chosen in the current experiments. 2.3 Related work Multilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as separation of senses (Resnik and Yarowsky, 1997; Dyvik, 1998; Ide et al., 2002). However, taking sense separation as a basis, Dyvik (2002) derives relations such as synonymy and hyponymy by applying the method of semantic mirrors. The paper illustrates how the method works. First, different senses are identified on the basis of manual word translations in sentence-aligned Norwegian-English data (2,6 million words in total). Second, senses are grouped in semantic fields. Third, features are FR chat 26 IT gatto 8 EN cat 13 total 64 30 assigned on the basis of inheritance. Lastly, semantic relations such synonymy and hyponymy are detected based on intersection and inclusion</context>
</contexts>
<marker>Ide, Erjavec, Tufis, 2002</marker>
<rawString>Ide, N., T. Erjavec, and D. Tufis. 2002. Sense discrimination with parallel corpora. In Proceedings of the ACL Workshop on Sense Disambiguation: Recent Successes and Future Directions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Justeson</author>
<author>S Katz</author>
</authors>
<title>Technical terminology: some linguistic properties and an algorithm for identification in text. Natural Language Engineering,</title>
<date>1995</date>
<marker>Justeson, Katz, 1995</marker>
<rawString>Justeson, J. and S. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for identification in text. Natural Language Engineering, 1:9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>N Bertoldi M Federico</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="15972" citStr="Koehn et al., 2007" startWordPosition="2581" endWordPosition="2584">e pair includes about 1.1 million sentence pairs. Note that there is a lot of repetition in EMEA and the number of unique sentences (sentence fragments) is much smaller: around 350,000 sentence pairs per language pair with about 6-7 million tokens per language. 3.2 Word alignment and phrase extraction For sentence alignment we applied hunalign (Varga et al., 2005) with the ’realign’ function that induces lexical features from the bitext to be combined with length based features. Word alignment has been performed using GIZA++ (Och, 2003). We used standard settings defined in the Moses toolkit (Koehn et al., 2007) to generate Viterbi word alignments of IBM model 4 for sentences 31 not longer than 80 tokens. In order to improve the statistical alignment we used lowercased tokens and lemmas in case we had them available (produced by the Tree-Tagger (Schmid, 1994) and the Alpino parser (van Noord, 2006)). We used the grow heuristics to combine the asymmetric word alignments which starts with the intersection of the two Viterbi alignments and adds block-neighboring points to it in a second step. In this way we obtain high precision links with some many-to-many alignments. Finally we used the phrase extract</context>
<context position="27881" citStr="Koehn et al., 2007" startWordPosition="4595" endWordPosition="4598">anked on the basis of information from the Web. Phrasal patterns such as XorY are used to get synonym compatibility hits as opposed to XandY that points to non-synonymous terms. The second method we compare with is the phrase-based translation method first introduced by Bannard and Callison-Burch (2005). Statistical word alignment can be used to measure the relation between source language items. Here, one makes use of the estimated translation likelihoods of phrases (p(f|e) and p(e|f)) that are used to build translation models in standard phrase-based statistical machine translation systems (Koehn et al., 2007). Bannard and Callison-Burch (2005) define the problem of paraphrasing as the following search problem: ˆe2 = argmaxe2:e2=Ae1p(e2|e1) where �p(e2|e1) ≈ p(f|e1)p(e2|f) f E C fc p(fC|e1)p(e2|fC) 34 Method k=1 k=2 k=3 Coverage P R P R P R Phrase-based Distr. Sim 28.9 22.8 21.8 32.7 17.3 37.2 40.0 Bannard&amp;Callison-Burch (2005) 18.4 15.3 16.9 27.3 13.7 32.3 48.1 Fahmi (2009) 38.2 35.1 37.1 35.1 37.1 35.1 4.0 Phrase-based Distr. Sim (hapaxes) 25.4 20.9 20.4 32.1 16.1 36.8 47.8 Table 2: Percent precision and recall at several values of k and percent coverage for the method proposed in this paper (plu</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Koehn, P., H. Hoang, A. Birch, C. Callison-Burch, M.Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A.Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the MT Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="12898" citStr="Koehn, 2005" startWordPosition="2073" endWordPosition="2074">ary. Both of them are limited to single-word terms. Some researchers employ multilingual corpora for the automatic acquisition of paraphrases (Shimota and Sumita, 2002; Bannard and CallisonBurch, 2005; Callison-Burch, 2008). The last two are based on automatic word alignment as is our approach. Bannard and Callison-Burch (2005) use a method that is also rooted in phrase-based statistical machine translation. Translation probabilities provide a ranking of candidate paraphrases. These are refined by taking contextual information into account in the form of a language model. The Europarl corpus (Koehn, 2005) is used. It has about 30 million words per language. 46 English phrases are selected as a test set for manual evaluation by two judges. When using automatic alignment, the precision reached without using contextual refinement is 48.9%. A precision of 55.3% is reached when using context information. Manual alignment improves the performance by 26%. A precision score of 55% is attained when using multilingual data. In a more recent publication Callison-Burch (2008) improved this method by using syntactic constraints and multiple languages in parallel. We have implemented a combination of Bannar</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Koehn, P. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the MT Summit, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>S Zhao</author>
<author>L Qin</author>
<author>M Zhou</author>
</authors>
<title>Identifying synonyms among distributionally similar words.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="12126" citStr="Lin et al. (2003)" startWordPosition="1952" endWordPosition="1955">mantic mirrors. The paper illustrates how the method works. First, different senses are identified on the basis of manual word translations in sentence-aligned Norwegian-English data (2,6 million words in total). Second, senses are grouped in semantic fields. Third, features are FR chat 26 IT gatto 8 EN cat 13 total 64 30 assigned on the basis of inheritance. Lastly, semantic relations such synonymy and hyponymy are detected based on intersection and inclusion among feature sets . Improving the syntax-based approach for synonym identification using bilingual dictionaries has been discussed in Lin et al. (2003) and Wu and Zhou (2003). In the latter parallel corpora are also applied as a reference to assign translation likelihoods to candidates derived from the dictionary. Both of them are limited to single-word terms. Some researchers employ multilingual corpora for the automatic acquisition of paraphrases (Shimota and Sumita, 2002; Bannard and CallisonBurch, 2005; Callison-Burch, 2008). The last two are based on automatic word alignment as is our approach. Bannard and Callison-Burch (2005) use a method that is also rooted in phrase-based statistical machine translation. Translation probabilities pr</context>
<context position="14267" citStr="Lin et al. (2003)" startWordPosition="2297" endWordPosition="2300">tails can be found in the Section 5. Apart from methods that use parallel corpora mono-lingual pattern-based methods have been used to find term variations. Fahmi (2009) acquired term variation for the medical domain using a two-step model. As a first step an initial list of synonyms are extracted using a method adapted from DIPRE (Brin, 99). During this step syntactic patterns guide the extraction of candidate terms in the same way as they will guide the extraction in this paper. This first step results in a list of candidate synonyms that are further filtered following a method described in Lin et al. (2003), which uses Web pages as an external source to measure the synonym compatibility hits of each pair. The precision and recall scores presented in Fahmi (2009) are high. We will give results for this method on our test set in Section 5 and refer to it as the pattern- and web-based approach. 3 Materials and methods In the following subsections we describe the setup for our experiments. 3.1 Data collection Measures of distributional similarity usually require large amounts of data. For the alignment method we need a parallel corpus of reasonable size with Dutch either as source or as target langu</context>
</contexts>
<marker>Lin, Zhao, Qin, Zhou, 2003</marker>
<rawString>Lin, D., S. Zhao, L. Qin, and M. Zhou. 2003. Identifying synonyms among distributionally similar words. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<contexts>
<context position="2352" citStr="Lin, 1998" startWordPosition="357" endWordPosition="358">e definition of synonymy. tions. These term variations could be used to enhance existing medical ontologies for the Dutch language. Our technique builds on the distributional hypothesis, the idea that semantically related words are distributed similarly over contexts (Harris, 1968). This is in line with the Firthian saying that, ’You shall know a word by the company it keeps.’ (Firth, 1957). In other words, you can grasp the meaning of a word by looking at its contexts. Context can be defined in many ways. Previous work has been mainly concerned with the syntactic contexts a word is found in (Lin, 1998; Curran, 2003). For example, the verbs that are in a subject relation with a particular noun form a part of its context. In accordance with the Firthian tradition these contexts can be used to determine the semantic relatedness of words. For instance, words that occur in a object relation with the verb to drink have something in common: they are liquid. Other work has been concerned with the bagof-word context, where the context of a word are the words that are found in its proximity (Wilks et al., 1993; Sch¨utze, 1992). Yet another context, that is much less studied, is the translational con</context>
<context position="19145" citStr="Lin (1998)" startWordPosition="3105" endWordPosition="3106">|(((A|NN|NM)* (NN|NM P)?)(A|NN|NM)*))NN+ To explain this regular expression in words, a candidate term is either a sequence of adjectives and/or nouns and/or names, ending in a noun or name or it consists of two such strings, separated by a single preposition. After applying the filters and removing all hapaxes we are left with 9.76 M co-occurrences of a Dutch (multi-word) term and a foreign translation. 3.4 Comparing vectors To compare the vectors of the terms we need a similarity measures. We have chosen to describe the functions used in this paper using an extension of the notation used by Lin (1998), adapted by Curran (2003). Co-occurrence data is described as tuples: (word, language, word&apos;), for example, (kat, EN, cat). Asterisks indicate a set of values ranging over all existing values of that component of the relation tuple. For example, (w, *, *) denotes for a given word w all translational contexts it has been found in in any language. For the example of kat in, this would denote all values for all translational contexts the word is found in: Katze DE:17, chat FR:26 etc. Everything is defined in terms of co-occurrence data with non-zero frequencies. The set of attributes or features</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCray</author>
<author>W Hole</author>
</authors>
<title>The scope and structure of the first version of the umls semantic network. In</title>
<date>1990</date>
<journal>DC, IEEE Computer Society.</journal>
<booktitle>Symposium on Computer Applications in Primary Care (SCAMC-90), IEEE Computer Society,</booktitle>
<pages>126--130</pages>
<location>Washington</location>
<contexts>
<context position="1038" citStr="McCray and Hole, 1990" startWordPosition="143" endWordPosition="146">al similarity. Our approach is based on automatic word alignment and standard phrase extraction techniques commonly used in statistical machine translation. Combined with pattern-based filters we obtain encouraging results compared to related approaches using similar datadriven techniques. 1 Introduction Ontologies provide a way to formally represent knowledge, for example for a specific domain. Ontology building has received a lot of attention in the medical domain. This interest is reflected in the existence of numerous medical ontologies, such as the Unified Medical Language System (UMLS) (McCray and Hole, 1990) with its metathesaurus, semantic network, and specialist lexicon. Although the UMLS includes information for languages other than English, the coverage for other languages is generally smaller. In this paper we describe an approach to acquire lexical information for the Dutch medical domain automatically. In the medical domain variations in terminology often include multi-word terms such as aangeboren afwijking ‘birth defect’ for congenitale aandoening ‘congenital disorder’. These multiple ways to refer to the same concept using distinct (multi-word) terms are examples of synonymy1 but are of</context>
</contexts>
<marker>McCray, Hole, 1990</marker>
<rawString>McCray, A. and W. Hole. 1990. The scope and structure of the first version of the umls semantic network. In Symposium on Computer Applications in Primary Care (SCAMC-90), IEEE Computer Society, pages 126–130, , Washington DC, IEEE Computer Society. 126-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>GIZA++: Training of statistical translation models. Available from http://www.isi.edu/˜och/GIZA++.html.</title>
<date>2003</date>
<contexts>
<context position="9270" citStr="Och, 2003" startWordPosition="1484" endWordPosition="1485"> contexts. We end this section with a discussion of related work. 2.1 Translational context The translational context of a word or a multiword term is the set of translations it gets in other languages. For the acquisition of translations for the Dutch medical terms we rely on automatic word alignment in parallel corpora. Figure 1: Example of bidirectional word alignments of two parallel sentences Figure 1 illustrates the automatic word alignment between a Dutch and an English phrase as a result of using the IBM alignment models (Brown et al., 1993) implemented in the open-source tool GIZA++ (Och, 2003). The alignment of two texts is bi-directional. The Dutch text is aligned to the English text and vice versa (dotted lines versus continuous lines). The alignment models produced are asymmetric. Several heuristics exist to combine directional word alignments which is usually called “symmetrization”. In order to cover multi-word terms standard phrase extraction techniques can be used to move from word alignment to linked phrases (see section 3.2 for more details). 2.2 Measures for computing similarity Translational co-occurrence vectors are used to find distributionally similar words. For ease </context>
<context position="15895" citStr="Och, 2003" startWordPosition="2570" endWordPosition="2571">ve 21 language pairs with Dutch as the source language. Each language pair includes about 1.1 million sentence pairs. Note that there is a lot of repetition in EMEA and the number of unique sentences (sentence fragments) is much smaller: around 350,000 sentence pairs per language pair with about 6-7 million tokens per language. 3.2 Word alignment and phrase extraction For sentence alignment we applied hunalign (Varga et al., 2005) with the ’realign’ function that induces lexical features from the bitext to be combined with length based features. Word alignment has been performed using GIZA++ (Och, 2003). We used standard settings defined in the Moses toolkit (Koehn et al., 2007) to generate Viterbi word alignments of IBM model 4 for sentences 31 not longer than 80 tokens. In order to improve the statistical alignment we used lowercased tokens and lemmas in case we had them available (produced by the Tree-Tagger (Schmid, 1994) and the Alpino parser (van Noord, 2006)). We used the grow heuristics to combine the asymmetric word alignments which starts with the intersection of the two Viterbi alignments and adds block-neighboring points to it in a second step. In this way we obtain high precisio</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F.J. 2003. GIZA++: Training of statistical translation models. Available from http://www.isi.edu/˜och/GIZA++.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>D Yarowsky</author>
</authors>
<title>A perspective on word sense disambiguation methods and their evaluation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why,</booktitle>
<note>what, and how?</note>
<contexts>
<context position="11341" citStr="Resnik and Yarowsky, 1997" startWordPosition="1827" endWordPosition="1830">nal co-occurrence vector for kat (’cat’) based on four languages The more similar the vectors are, the more distributionally similar the head terms are. We need a way to compare the vectors for any two head terms to be able to express the similarity between them by means of a score. Various methods can be used to compute the distributional similarity between terms. We will explain in section 3 what measures we have chosen in the current experiments. 2.3 Related work Multilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as separation of senses (Resnik and Yarowsky, 1997; Dyvik, 1998; Ide et al., 2002). However, taking sense separation as a basis, Dyvik (2002) derives relations such as synonymy and hyponymy by applying the method of semantic mirrors. The paper illustrates how the method works. First, different senses are identified on the basis of manual word translations in sentence-aligned Norwegian-English data (2,6 million words in total). Second, senses are grouped in semantic fields. Third, features are FR chat 26 IT gatto 8 EN cat 13 total 64 30 assigned on the basis of inheritance. Lastly, semantic relations such synonymy and hyponymy are detected bas</context>
</contexts>
<marker>Resnik, Yarowsky, 1997</marker>
<rawString>Resnik, P. and D. Yarowsky. 1997. A perspective on word sense disambiguation methods and their evaluation. In Proceedings of ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, what, and how?</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selection and information. Unpublished doctoral thesis,</title>
<date>1993</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="21668" citStr="Resnik, 1993" startWordPosition="3544" endWordPosition="3545"> this problem we removed all term variations that are either super- or sub-strings of the original term from the lists of candidate term variations output by the system. 4 Evaluation There are several evaluation methods available to assess lexico-semantic data. Curran (2003) distinguishes two types of evaluation: direct evaluation and indirect evaluation. Direct evaluation methods compare the semantic relations given by the 2Feature weights have been used in previous work for syntax-based methods to account for the fact that cooccurrences have different information values. Selectionally weak (Resnik, 1993) or light verbs such as hebben ‘to have’ have a lower information value than a verb such as uitpersen ‘squeeze’ that occurs less frequently. Although weights that promote features with a higher information value work very well for syntax-based methods, Van der Plas (2008) showed that weighting only helps to get better synonyms for very infrequent nouns when applied in alignment-based approaches. In the current setting we do not consider very infrequent terms so we did not use any weighting. system against human performance or expertise. Indirect approaches evaluate the system by measuring its </context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Resnik, P. 1993. Selection and information. Unpublished doctoral thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-ofspeech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK,</location>
<note>http://www.ims.unistuttgart.de/˜schmid/.</note>
<contexts>
<context position="16224" citStr="Schmid, 1994" startWordPosition="2626" endWordPosition="2627">ge. 3.2 Word alignment and phrase extraction For sentence alignment we applied hunalign (Varga et al., 2005) with the ’realign’ function that induces lexical features from the bitext to be combined with length based features. Word alignment has been performed using GIZA++ (Och, 2003). We used standard settings defined in the Moses toolkit (Koehn et al., 2007) to generate Viterbi word alignments of IBM model 4 for sentences 31 not longer than 80 tokens. In order to improve the statistical alignment we used lowercased tokens and lemmas in case we had them available (produced by the Tree-Tagger (Schmid, 1994) and the Alpino parser (van Noord, 2006)). We used the grow heuristics to combine the asymmetric word alignments which starts with the intersection of the two Viterbi alignments and adds block-neighboring points to it in a second step. In this way we obtain high precision links with some many-to-many alignments. Finally we used the phrase extraction tool from Moses to extract phrase correspondences. Phrases in statistical machine translation are defined as sequences of consecutive words and phrase extraction refers to the exhaustive extraction of all possible phrase pairs that are consistent w</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, Helmut. 1994. Probabilistic part-ofspeech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK, September. http://www.ims.unistuttgart.de/˜schmid/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of the ACM/IEEE conference on Supercomputing.</booktitle>
<marker>Sch¨utze, 1992</marker>
<rawString>Sch¨utze, H. 1992. Dimensions of meaning. In Proceedings of the ACM/IEEE conference on Supercomputing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Shimota</author>
<author>E Sumita</author>
</authors>
<title>Automatic paraphrasing based on parallel corpus for normalization.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="12453" citStr="Shimota and Sumita, 2002" startWordPosition="2002" endWordPosition="2006"> 30 assigned on the basis of inheritance. Lastly, semantic relations such synonymy and hyponymy are detected based on intersection and inclusion among feature sets . Improving the syntax-based approach for synonym identification using bilingual dictionaries has been discussed in Lin et al. (2003) and Wu and Zhou (2003). In the latter parallel corpora are also applied as a reference to assign translation likelihoods to candidates derived from the dictionary. Both of them are limited to single-word terms. Some researchers employ multilingual corpora for the automatic acquisition of paraphrases (Shimota and Sumita, 2002; Bannard and CallisonBurch, 2005; Callison-Burch, 2008). The last two are based on automatic word alignment as is our approach. Bannard and Callison-Burch (2005) use a method that is also rooted in phrase-based statistical machine translation. Translation probabilities provide a ranking of candidate paraphrases. These are refined by taking contextual information into account in the form of a language model. The Europarl corpus (Koehn, 2005) is used. It has about 30 million words per language. 46 English phrases are selected as a test set for manual evaluation by two judges. When using automat</context>
</contexts>
<marker>Shimota, Sumita, 2002</marker>
<rawString>Shimota, M. and E. Sumita. 2002. Automatic paraphrasing based on parallel corpus for normalization. In Proceedings of the International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS - A collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing,</booktitle>
<volume>volume V,</volume>
<pages>237--248</pages>
<editor>In Nicolov, N., K. Bontcheva, G. Angelova, and R. Mitkov, editors,</editor>
<location>Borovets, Bulgaria. John Benjamins, Amsterdam/Philadelphia.</location>
<contexts>
<context position="15048" citStr="Tiedemann, 2009" startWordPosition="2430" endWordPosition="2432">gh. We will give results for this method on our test set in Section 5 and refer to it as the pattern- and web-based approach. 3 Materials and methods In the following subsections we describe the setup for our experiments. 3.1 Data collection Measures of distributional similarity usually require large amounts of data. For the alignment method we need a parallel corpus of reasonable size with Dutch either as source or as target language coming from the domain we are interested in. Furthermore, we would like to experiment with various languages aligned to Dutch. The freely available EMEA corpus (Tiedemann, 2009) includes 22 languages in parallel with a reasonable size of about 12-14 million tokens per language. The entire corpus is aligned at the sentence level for all possible combinations of languages. Thus, for acquiring Dutch synonyms we have 21 language pairs with Dutch as the source language. Each language pair includes about 1.1 million sentence pairs. Note that there is a lot of repetition in EMEA and the number of unique sentences (sentence fragments) is much smaller: around 350,000 sentence pairs per language pair with about 6-7 million tokens per language. 3.2 Word alignment and phrase ext</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>Tiedemann, J¨org. 2009. News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In Nicolov, N., K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing, volume V, pages 237–248, Borovets, Bulgaria. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L van der Plas</author>
<author>J Tiedemann</author>
</authors>
<title>Finding synonyms using automatic word alignment and measures of distributional similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<marker>van der Plas, Tiedemann, 2006</marker>
<rawString>van der Plas, L. and J. Tiedemann. 2006. Finding synonyms using automatic word alignment and measures of distributional similarity. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>van der Plas</author>
</authors>
<title>Automatic lexico-semantic acquisition for question answering. Groningen dissertations in linguistics.</title>
<date>2008</date>
<marker>van der Plas, 2008</marker>
<rawString>van der Plas. 2008. Automatic lexico-semantic acquisition for question answering. Groningen dissertations in linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>At last parsing is now operational.</title>
<date>2006</date>
<booktitle>In Actes de la 13eme Conference sur le Traitement Automatique des Langues Naturelles.</booktitle>
<marker>van Noord, 2006</marker>
<rawString>van Noord, G. 2006. At last parsing is now operational. In Actes de la 13eme Conference sur le Traitement Automatique des Langues Naturelles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Varga</author>
<author>L Nmeth</author>
<author>P Halcsy</author>
<author>A Kornai</author>
<author>V Trn</author>
<author>V Nagy</author>
</authors>
<title>Parallel corpora for medium density languages.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP</booktitle>
<pages>590--596</pages>
<contexts>
<context position="15719" citStr="Varga et al., 2005" startWordPosition="2540" endWordPosition="2543"> size of about 12-14 million tokens per language. The entire corpus is aligned at the sentence level for all possible combinations of languages. Thus, for acquiring Dutch synonyms we have 21 language pairs with Dutch as the source language. Each language pair includes about 1.1 million sentence pairs. Note that there is a lot of repetition in EMEA and the number of unique sentences (sentence fragments) is much smaller: around 350,000 sentence pairs per language pair with about 6-7 million tokens per language. 3.2 Word alignment and phrase extraction For sentence alignment we applied hunalign (Varga et al., 2005) with the ’realign’ function that induces lexical features from the bitext to be combined with length based features. Word alignment has been performed using GIZA++ (Och, 2003). We used standard settings defined in the Moses toolkit (Koehn et al., 2007) to generate Viterbi word alignments of IBM model 4 for sentences 31 not longer than 80 tokens. In order to improve the statistical alignment we used lowercased tokens and lemmas in case we had them available (produced by the Tree-Tagger (Schmid, 1994) and the Alpino parser (van Noord, 2006)). We used the grow heuristics to combine the asymmetri</context>
</contexts>
<marker>Varga, Nmeth, Halcsy, Kornai, Trn, Nagy, 2005</marker>
<rawString>Varga, D., L. Nmeth, P. Halcsy, A. Kornai, V. Trn, and V. Nagy. 2005. Parallel corpora for medium density languages. In Proceedings of RANLP 2005, pages 590–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
</authors>
<title>EuroWordNet a multilingual database with lexical semantic networks.</title>
<date>1998</date>
<contexts>
<context position="22815" citStr="Vossen, 1998" startWordPosition="3735" endWordPosition="3736">pertise. Indirect approaches evaluate the system by measuring its performance on a specific task. Since we are not aware of a task in which we could test the term variations for the Dutch medical domain and ad-hoc human judgments are time consuming and expensive, we decided to compare against a gold standard. Thereby denying the common knowledge that the drawback of using gold standard evaluations is the fact that gold standards often prove to be incomplete. In previous work on synonym acquisition for the general domain, Van der Plas and Tiedemann (2006) used the synsets in Dutch EuroWordnet (Vossen, 1998) for the evaluation of the proposed synonyms. In an evaluation with human judgments, Van der Plas and Tiedemann (2006) showed that in 37% of the cases the majority of the subjects judged the synonyms proposed by the system to be correct even though they were not found to be synonyms in Dutch EuroWordnet. For evaluating medical term variations in Dutch there are not many gold standards available. Moreover, the gold standards that are available are even less complete than for the general domain. 4.1 Gold standard We have chosen to evaluate the nearest neighbours of the alignment-based method on </context>
</contexts>
<marker>Vossen, 1998</marker>
<rawString>Vossen, P. 1998. EuroWordNet a multilingual database with lexical semantic networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Guo</author>
<author>J E McDonald</author>
<author>B M Slator T Plate</author>
</authors>
<title>Providing machine tractable dictionary tools.</title>
<date>1993</date>
<journal>Machine Translation,</journal>
<volume>5</volume>
<issue>2</issue>
<marker>Guo, McDonald, Plate, 1993</marker>
<rawString>Wilks, Y., D. Fass, Ch. M. Guo, J. E. McDonald, and B. M. Slator T. Plate. 1993. Providing machine tractable dictionary tools. Machine Translation, 5(2):99–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wu</author>
<author>M Zhou</author>
</authors>
<title>Optimizing synonym extraction using monolingual and bilingual resources.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Workshop on Paraphrasing: Paraphrase Acquisition and Applications (IWP).</booktitle>
<contexts>
<context position="12149" citStr="Wu and Zhou (2003)" startWordPosition="1957" endWordPosition="1960">per illustrates how the method works. First, different senses are identified on the basis of manual word translations in sentence-aligned Norwegian-English data (2,6 million words in total). Second, senses are grouped in semantic fields. Third, features are FR chat 26 IT gatto 8 EN cat 13 total 64 30 assigned on the basis of inheritance. Lastly, semantic relations such synonymy and hyponymy are detected based on intersection and inclusion among feature sets . Improving the syntax-based approach for synonym identification using bilingual dictionaries has been discussed in Lin et al. (2003) and Wu and Zhou (2003). In the latter parallel corpora are also applied as a reference to assign translation likelihoods to candidates derived from the dictionary. Both of them are limited to single-word terms. Some researchers employ multilingual corpora for the automatic acquisition of paraphrases (Shimota and Sumita, 2002; Bannard and CallisonBurch, 2005; Callison-Burch, 2008). The last two are based on automatic word alignment as is our approach. Bannard and Callison-Burch (2005) use a method that is also rooted in phrase-based statistical machine translation. Translation probabilities provide a ranking of cand</context>
</contexts>
<marker>Wu, Zhou, 2003</marker>
<rawString>Wu, H. and M. Zhou. 2003. Optimizing synonym extraction using monolingual and bilingual resources. In Proceedings of the International Workshop on Paraphrasing: Paraphrase Acquisition and Applications (IWP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>