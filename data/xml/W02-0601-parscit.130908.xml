<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.992951">
Morphological and Phonological Learning: Proceedings of the 6th Workshop of the
ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,
July 2002, pp. 1-10. Association for Computational Linguistics.
</note>
<sectionHeader confidence="0.675288" genericHeader="method">
2 Assamese word derivation
</sectionHeader>
<bodyText confidence="0.95353">
Broadly there are two kinds of derivations. First,
multiple root words can be combined to form com-
pounds, e.g.,
</bodyText>
<equation confidence="0.887977">
grantha + melA = granthamelA
</equation>
<bodyText confidence="0.971328454545455">
This often occurs with some change in spellings of
the root words, e.g.,
par + adhin = parAdhin (91-41).
Compound formation in the languages of the In-
dic branch is of two types namely, using sandhi
and samas ([10], [11]). In this work, we do not
target analysis of compounds. The second kind of
derivation is by the use of affixes (prefixes and suf-
fixes). Affixation is sometimes differentiated into
two types, namely inflectional and derivational
(such as in [3]), e.g.,
</bodyText>
<equation confidence="0.630131">
Inflectional : mAnuh + e = mAnuhe
Derivational: driha + tA= drihatA
= firm, firmness)
</equation>
<bodyText confidence="0.999938941176471">
Our system, however, treats them alike. Lan-
guages of the Indic branch inherit many affixes
from Sanskrit, as well as have many affixes specific
to each language. To begin with, we lay emphasis
on processing suffixes since suffixes are more com-
mon in Assamese and embody significant linguistic
information. Most common among the Assamese
suffixes are inflectional suffixes called bibhaktis ([4],
[5], [7]) and are distinct from bibhaktis of other In-
dic languages. In Indic languages, particularly in
Assamese, in a large number of cases of application
of suffixes, spelling changes do not occur. It is also
important to note that a very common morpholog-
ical phenomenon in Assamese is sequences of suf-
fixes in a single word. For example, PrAkeiTAkeino
= lirA + keiTA + k + ei + no
boys+some+ob j ective+emphasis+emphasis) .
The frequent occurence of such sequences and the
large number of suffixes in some of these sequences
is a phenomenon that distinguishes Assamese from
most other languages. In some cases Assamese
also allows certain suffixes to be detached from
the base part. i.e., write the suffix as a separate
word following the root word.
Assamese inherits 20 prefixes (upasargas) from
Sanskrit ([4], [10], [11]). There are additional pre-
fixes specific to Assamese. Prefixes in many cases
change the meaning of words in such a way that
the derived words may be treated as root words
themselves. Few prefixes, viz., e, du, Ca, (these
denote numbers one, two, and six of the object)
and na (also its other forms - ne, nA, nu, ni, etc.,
all of which denote negation) only qualifies the ba-
sic meaning of the root word.
</bodyText>
<sectionHeader confidence="0.996568" genericHeader="method">
3 Decomposing words
</sectionHeader>
<bodyText confidence="0.997069625">
From the set of words in the input text, we identify
cases where one word can be derived from another
word by addition of some suffix (or prefix) in that
set. We refer to such a case as a &amp;quot;decomposition&amp;quot;,
the former word a &amp;quot;derivative&amp;quot;, the latter a &amp;quot;base&amp;quot;
(or root), and the suffix a &amp;quot;rule&amp;quot;. i.e.,
Decomposition: derivative = base + rule
For example,
</bodyText>
<equation confidence="0.999306">
karA = kar + A (-4T = + za)
bahalAi = bahal + Ai +
</equation>
<bodyText confidence="0.895136">
We may be able to find more than one derivative
for a single word from the same base by adding dif-
ferent rules. A base may be found to be a deriva-
tive with respect to another base. For example,
suppose we have the words, a, ab, ac, abd (a,b,c
and d are strings possibly longer than a single char-
acter). We can represent these as the following.
Abstraction Example
ab = a + b kalamar = kalam+ ar
ac = a + c kalamat = kalam+ at
abd = a + bd, kalamarhe= kalam+ arhe
or, abd = ab + d kalamarhe= kalamar + he
In the case of abd, we would like to record the
decomposition ab + d, since it reflects more de-
composition information than the other alterna-
tive, when considered along with ab = a + b.
We summarize our algorithm below
</bodyText>
<listItem confidence="0.952077580645161">
Preprocessing
1. Take a input text T
2. Form a sorted list, L, of distinct words in T.
Phase 1
3. For each word w in L, identify another word b
in L such that w can be obtained by append-
ing some (non-null) suffix s to b. If there are
multiple candidates for b, select the longest
among them and record the decomposition
w = b s
If no b can be identified for a w, w is &amp;quot;unde-
composed&amp;quot;.
4. From the decompositions identified above,
• for each base b count the number of decom-
positions where it is a base. Call it the base
count of b, bcb.
• for each suffix (rule), s, count the number
of decompositions where it is a suffix. Call it
the rule frequency of s, r fs.
5. Discard the decompositions in which the rule
has a very low value a of r fs. a can be ad-
justed experimentally. We have used a = 1
in our experiments. Also, discard the decom-
positions in which the base has a very high
value of bcb.
Phase 2
6. For each word that could not be decomposed
in phase 1 (some of which may have come out
as base in some decomposition), try each of
the rules identified in that phase. That is,
see if a rule (suffix) is the ending part of an
</listItem>
<bodyText confidence="0.870062333333333">
undecomposed word. Record such decompo-
sitions, and the base forms.
Phase 3
</bodyText>
<listItem confidence="0.622504666666667">
7. For each word w in L, identify the set of suf-
fixes (rules) that appear with it and call it
the characteristic of that word, C.
</listItem>
<bodyText confidence="0.9221038">
Words are classified using their characteris-
tics.
Example: [Numbered according to the steps out-
lined above.] Note that in our experiment the in-
put corpus uses Roman orthography. The mapping
between the Assamese letters and the Roman let-
ters is chosen so as to facilitate lossless translation
between the two representations. The following
example uses this orthography (though it reduces
readability to an unaccustomed reader).
</bodyText>
<listItem confidence="0.506917">
2. Suppose the list of distinct words is,
</listItem>
<construct confidence="0.486015">
...,asrn, asmk, asinklE, asmlE, asinr, br-
dlE, bhArtk, bhArtr, bhArte, kAlilE,
k, Urn, klinr, kr, mAnulte,
(...,
</construct>
<equation confidence="0.702618">
t11n1, t171c.n, &lt;nirc4Z,I,
&lt;PG&apos;11, &lt;I)G111, 7 7 777)
3. Decompositions
(asmk = asm + k), (asmklE = asmk + 1E),
(asmlE = asm + 1E), (asmr = asm + r),
(klm = k + (klmr = klm + r),
(kr = k + r)*.
Undecomposed: asrn, brdlE, bhrtk, bhArtr,
bhArte, kAlilE, Urn, inAntche.
4. Base counts
asm : 3, asmk : 1, k : 2, klm : 1
asinklE, asmlE, asinr, brdlE, bhArtk, bhArtr,
bhArte, kAlilE, klinr, kr, inAntche : 0
(The suffixes identified are k, 1E, r and
/m)
Suffix frequencies:
k: 1,1E : 2, r : 3, /m : 1
</equation>
<listItem confidence="0.98452">
5. Discard the decompositions involving the
suffixes k, and /m (since they have frequency
a = 1). The base count of asm becomes 2,
and k becomes 1.
6. Decompositions
</listItem>
<equation confidence="0.999745666666667">
(brdlE = brd + 1E)*,
(bhArtr = bhArt + r),
(kAlilE = kAli + 1E)
</equation>
<listItem confidence="0.364418">
7. Characteristics
</listItem>
<construct confidence="0.64182125">
asm : {1E,r}, asmk : {1E}, klm : {r},
bhArt : {r}, brd : {1E}, kAli : {1E}
asniklE, asndE, asTnr, brdlE, bhArtk, bhArtr,
kAlilE, klnir : {}
</construct>
<bodyText confidence="0.994753692307693">
(The decompositions marked * are linguisti-
cally incorrect.)
In the first phase, we extract rules by using base
forms that are known, i.e., words that are present
in the list of words. After this analysis, there may
be words which could not be decomposed. It is
possible that some of these words are actually root
words and cannot be decomposed. But there may
be words that could not be decomposed because
no base form exists in the list of words provided.
So in the second phase, we consider the &amp;quot;undecom-
posed&amp;quot; words and see if any &amp;quot;rule&amp;quot; identified so far
can be applied to decompose such a word, i.e., if
ab is word that could not be decomposed so far, is
there a rule b so that we can decompose ab as a +
b? In fact multiple rules might be applicable for a
single word. In such a case, in a language where
suffix sequences are a common phenomenon, we
should give preference to rules that are longer and
can be obtained as a suffix sequence (See Section
8). Otherwise suffixes with higher frequencies may
be given preference. With more base forms known
following the second phase, we can repeat the first
phase analysis, then the second phase analysis, and
theoretically, so on. In the example given above,
if we were to iterate the steps after the first pass,
due to the word bhArt in the list now, we can de-
compose the hitherto undecomposed word bhArtk
as bhArt + k and bhArte as bhArt + e. This
gives us the new suffix e. Assuming e does not get
discarded, we can decompose the word mAnuhe
as mAnuh + e, obtaining the new word mAnuh.
This process may be stopped when in a particular
pass we do not detect any new rule or base. Phase
3 can then be undertaken to determine the nature
of the words based on the suffixes that each takes.
Reaching such a stage, however, does not imply
that all possible decompositions have been de-
tected. Since our method works by comparing
related words, the presence or absence of certain
words makes significant difference. For instance, if
the input list of words in the example given above
did not contain the word bhArtr, the base bhArt
would not be obtained, and consequently the suffix
e would not be obtained from bhArte, and further
mAnuhe would not be decomposed. In general,
providing a large number of words for analysis will
cause more rules to be detected in phase 1, and
with the larger number of rules so obtained, more
base forms can be detected through phase 2. In
practice, it is typical to expect texts in chunks in
successive analysis runs. So, during implementa-
tion of the algorithm, we refine it so that while a
new chunk of text is processed, the words in that
text are analyzed along with the words existing in
the lexicon. The focus is put on the new words
and their effects.
An analysis based simply on detection of presence
of common substrings may fail to detect decompo-
sitions where the spelling of the derived word is not
simply a concatenation of the base word and the
suffix but a modification of that. The proportion
of such words in a corpus may vary across lan-
guages, and so will the effectiveness of the above
algorithm. In a language such as Assamese (as
also in other Indic languages), it is seen that the
modification of spelling is generally related to the
actual pronunciation of the words and the suffixes.
More specifically, the sounds of the ending part of
the base word and the beginning of the suffix com-
bine to form a sound which is represented in the
result by another string of letters than the combi-
nation of the original letters. That is, a word ab
may combine with a rule xy to form acy instead of
abxy. We feel that some amount of phonological
knowledge, either acquired by a learning method,
or provided directly, may be used to detect and
handle such cases.
</bodyText>
<sectionHeader confidence="0.99169" genericHeader="method">
4 Information schema
</sectionHeader>
<bodyText confidence="0.9984496">
We accumulate the information extracted by the
analysis process in two primary data structures
a lexicon, and a rule base. In the lexicon we keep
an entry for each word that has been encountered
so far in various runs. Each entry contains the
</bodyText>
<listItem confidence="0.912033928571429">
• the word, w
• the base, b (`-&apos; if the word is not decomposed)
• the number of times it is encountered in text,
w fw.
• the number of times it participates as base
in decompositions, kw
• other attributes
Similarly, the rule base contains entries each of
which comprises
• the rule, s (i.e., suffix, or prefix)
• the number of times it participates in differ-
ent decompositions, r fs.
• other attributes
5 Effect of additional text
</listItem>
<bodyText confidence="0.9999724375">
An effort to build a lexicon and enumerate mor-
phological rules of a natural language, is unlikely
to be complete in a single run of the algorithm.
It is likely after processing a number of chunks of
texts, no new rules are generated. However, base
forms are likely to be discovered for a much longer
time, and quite possibly, for ever, with new texts.
But then, this is the truth about vocabulary! This
expectation is borne out by the actual observa-
tions in our experiment with 111 chunks of texts
depicted in Figure 1 and Figure 2. In Figure 1 we
have considered the proportion of distinct unseen
words (not base forms, however) to the total words
in each text chunk, and in Figure 2 it is the pro-
portion of distinct unseen rules to the total distinct
rules in each text chunk.
</bodyText>
<sectionHeader confidence="0.99696" genericHeader="method">
6 Experimental results
</sectionHeader>
<bodyText confidence="0.999644">
In Table 1 we present the quantitative summary of
observations in an experiment with a corpus of 111
news articles containing over 49,000 words in all.
These results cover only Phase I of the algorithm
described.
With Phase 1 processing only, about 77.28% of
the words are decomposed of which 65.4% are cor-
rect decompositions. Of the undecomposed words,
about 71% are actually root words that should not
be decomposed. Of the around 29% words that
should have been decomposed but were not, about
3% are compounds and we are not keen on decom-
posing anyway. In other words we missed about
26% (29 - 3) of possible decompositions.
On applying two simple criteria to eliminate in-
correct decompositions (point 5 in the algorithm),
viz., decompositions that have root frequencies 20
or higher (because bases occuring with too many
suffixes are very short words, usually one or two
letters, and match the leading portion of longer
words not related to them), and decompositions
that have suffix frequencies only a = 1, about
64% of the decompositions were retained. Of these
about 90.39% were correct decompositions. In the
36% decompositions that were discarded, less than
21% were correct decompositions. It should also
be noted that of the 90% correct decompositions,
12.5% are incomplete in that there were more than
</bodyText>
<figure confidence="0.999498130434783">
0 20 40 60 80 100 120
text chunk #
x
0 20 40 60 80 100 120
text chunk #
% age of new words
70
40
20
60
50
30
10
0
y
x
%-age of new rules
100y
40
20
80
60
0
</figure>
<bodyText confidence="0.952593375">
two morphemes in the words some of which could
not be separated.
Intuitively the value of a should be low due to
the &amp;quot;conservative&amp;quot; way of identifying suffixes in
phase 1. In the experiment we have chosen a =
1 since while higher values only marginally im-
proved the precision, the recall was worse; for
a = 2 : precision = 91.76%, recall = 67.16% and
for a = 3 : precision = 92.83%, recall = 65.11%.
The recall is likely to improve for larger corpus.
The choice of upper limit for the root fre-
quency is, on the other hand, less rigid. We
kept it 19. For a value 18, precision =
90.53%, recall = 70.38%; for a value 20,
precision = 90.09%, recall = 70.60%; and for
a value 21, precision = 89.97%, recall = 70.60%.
</bodyText>
<subsectionHeader confidence="0.923691">
6.1 Results from Linguistica
</subsectionHeader>
<bodyText confidence="0.999982666666667">
Upon running Linguistica ([14]) over the same
corpus of 11450 distinct words, the output re-
ported the analysis of only 6040 of the distinct
input words, i.e., 52.75%. In these 6040 analysis
the precision of decompositions is 92.05% (includ-
ing about 24% correct but incomplete decomposi-
tions), and the recall is 40.62%. If we consider the
fraction of the actual base words left rightly unde-
composed together with the fraction of the derived
words correctly decomposed, then this extended
precision is 61.54%. In the results of our program,
when no filtering is done then the extended preci-
sion is 67.5%, and when decompositions with suffix
frequency 1 or base frequency above 19 are dis-
carded, then the extended precision is 77.45%.
</bodyText>
<sectionHeader confidence="0.902976" genericHeader="method">
7 Quality of decompositions
</sectionHeader>
<bodyText confidence="0.999875333333333">
Among the linguistically correct decompositions,
our process leaves some decompositions incom-
plete, and also produces some &amp;quot;false&amp;quot; decomposi-
tions. An incomplete decomposition means detect-
ing only some of the multiple suffixes occurring in
sequence in a word. To reduce such cases the valid
suffix sequences in the language can be identified
and this knowledge can be used to (See Section 8),
say, break up suffixes identified by the basic algo-
rithm into valid suffix parts. The false decompo-
sitions, in general, may happen due to cases such
as
</bodyText>
<listItem confidence="0.875053">
• both a and ab are valid words but b is not a
true &amp;quot;rule&amp;quot; in the language. We refer to this
case as &amp;quot;false rule&amp;quot;. An even more difficult
case is that b is a true rule in the language,
but not in the case of ab, i.e, ab is actually
not derived from a. We refer to this case as
&amp;quot;false decomposition&amp;quot;.
</listItem>
<bodyText confidence="0.989118692307692">
In the example presented earlier, the
rule /m obtained from the decomposition
klm = k + lm is a false rule. The decompo-
sition kr = k + r is a false decomposition.
• ab is a word and b is a known rule, but a
is not the &amp;quot;base&amp;quot; of ab. We shall refer to
this case as &amp;quot;false base&amp;quot;. Here, the difficult
case occurs when a is a valid word, but it has
nothing to do with the word ab. This too is a
case of &amp;quot;false decomposition&amp;quot;. In the exam-
ple presented earlier, the base brd obtained
from the decomposition brdlE = brd + 1 E
is a false base.
In short, we have the problem of unconfirmed de-
compositions (i.e., whether the base forms, rules
and decompositions are correct). To tackle this
problem of unconfirmed decompositions we com-
pute the probabilities that various entities, i.e.,
(base form, rule and decomposition) tuples are cor-
rect. For this, we assign a numeric &amp;quot;confirmation&amp;quot;
value to each entity. A higher confirmation value
of an entity indicates that it is more likely to be
correct, and it is updated as the system processes
more and more input. Qualitatively, we may apply
the following heuristics to obtain the confirmation
values of the entities:
</bodyText>
<listItem confidence="0.996585133333333">
• The rules and decompositions discovered in
phase 1 have a high confirmation value, since
the base forms assumed in that phase are ac-
tually present in the text.
• In phase 2 (where we discover base forms
and decompositions) the confirmation value
is low. Among such base forms and decom-
positions, the confirmation value is higher for
rules (suffixes) which are long, say at least
two consonants. Also, base forms identified
in phase 2 have higher confirmation values if
it participates in more such decompositions.
• The confirmation value of a decomposition
depends on the confirmation value of the
base forms and the rules that are involved.
</listItem>
<sectionHeader confidence="0.525042" genericHeader="method">
8 Identifying suffix sequences
</sectionHeader>
<bodyText confidence="0.996552277777778">
A language such as Assamese allows certain suf-
fixes to occur together in sequence in words. For
example, suffixes x, y and z may occur with a word
w as wxzy. These suffixes may or may not appear
in other arrangements. Identification of such se-
quences may help extract more linguistic informa-
tion about the words. To classify words using the
suffix characteristics, identification of valid suffix
sequences can help in obtaining a kind of canoni-
calization of the suffix characteristic of a word. For
example, if it is known that the suffix xzy is ac-
tually a sequence x, z, y, due to the words w and
wxzy being in a corpus we associate only the suffix
x with w and not xzy.
The following simple algorithm can be used to
identify valid suffix sequences in the language us-
ing the decompositions performed on the words in
the input corpus
</bodyText>
<listItem confidence="0.998762555555556">
1. Start from beginning of the lexicon sorted
in reverse alphabetical order. Let a string
variable s f x _seq contain a suffix sequence,
and n_s f x contain the number of suffix com-
ponents in s f x _seq. Initially s f x _seq con-
tains the NULL string, n_s f x contains 0, and
next decomposition means the first decompo-
sition.
2. For the next decomposition encountered, say
</listItem>
<equation confidence="0.754500333333333">
wxzy = wxz + y,
s f x _seq y,
n_s f x 1
</equation>
<bodyText confidence="0.9644602">
3. Take the base of the decomposition, wxz,
and identify the entry for it in the subse-
quent part of the lexicon.
If the base appears as a decomposed
entry, say, wxz = wx + z, then
</bodyText>
<equation confidence="0.8900775">
s f x _seq &amp;quot;z&amp;quot; + s f x_seq,
n_sf x n_sfx + 1
</equation>
<bodyText confidence="0.974051875">
repeat this step 3 for the word wx.
Else (i.e., if the base appears as an un-
decomposed entry), then if n_sf x &gt; 1
then record the contents of s f x _seq as a valid
sequence of suffixes.
In our current example suppose we find that
w is an undecomposed word in the lexicon.
So we obtain the suffix sequence x z y.
</bodyText>
<listItem confidence="0.93244">
4. Go to step 2, unless the end of the lexicon is
reached.
</listItem>
<bodyText confidence="0.911236">
In our experiment we have identified 488 sequences
of suffixes.
</bodyText>
<listItem confidence="0.66598">
9 Word classification using affix
knowledge
</listItem>
<bodyText confidence="0.999681777777778">
We have carried out some experiments on classify-
ing words based on the suffixes that have been en-
countered. These classes should resemble the var-
ious known linguistic categories of words. A brief
account of our experiment is given below. Recall
that we have called the set of suffixes for a word
its characteristic. We consider suffixes that occur
with a frequency of 10 or more. There were 81
such suffixes.
</bodyText>
<listItem confidence="0.296138">
1. Direct classification based on characteristics:
</listItem>
<bodyText confidence="0.981141583333333">
Form classes of words by exact matching of
respective characteristics. This leads to too
many classes of words, because in a corpus
many words are likely to occur only with a
subset of the set of linguistically valid suffixes
for it. Our analysis forms the characteristic
for a word on the basis of the suffixes that
have been found with the word in that cor-
pus. So if different words of the same linguis-
tic category have different subsets of suffixes
in the corpus, they are classified into differ-
ent categories.
2. Identifying subsets of characteristics: One
attempt to overcome the drawback of the
above method is to assume that at least some
words from each true linguistic category will
occur with all or almost all valid suffixes for
that category. This implies that the charac-
teristics of all words which are actually of the
same linguistic category will be subset of the
characteristic of the word which has occurred
with all possible suffixes. For example, w oc-
curs with all possible suffixes for its linguistic
category, and p and q with two different sub-
sets of the suffixes with w. Then the char-
acteristics of p and q will be subsets of the
characteristic of w. Thus we classify w ,p and
q into the same class. Also note that after a
characteristic (e.g., of p) has been found to
be a subset of another (e.g., of w), other char-
acteristics are not tested for being its (i.e.,
characteristic of p) subset. It turns out that
words that occur with very few suffixes falls
in more than one class. This is because in
Assamese there are some suffixes that occur
with words of multiple linguistic categories.
</bodyText>
<listItem confidence="0.419821">
3. Classification based on Closures of charac-
</listItem>
<bodyText confidence="0.9612231875">
teristics: The drawback of the idea of sub-
sets described above is that for a linguis-
tic category hardly any word occurs with all
valid suffixes for that class. To overcome
this we modified the idea to synthesize the
master characteristic of each linguistic cat-
egory by taking union of its tentative sub-
sets that have occurred. We have called this
synthesized master characteristic, a closure.
To compute a closure we decide on a pos-
itive number k which we call the degree of
closure. We start by selecting the largest of
all characteristics, and assume that it is the
closure. Then sequentially for each remain-
ing characteristic, c, we determine if c has at
least k elements common with the closure or
c has less than k elements which are all com-
mon with the closure. If so, we update the
closure by taking its union with c. If dur-
ing one pass of such testing of characteristics
the closure actually gets updated, we have to
perform another pass considering the char-
acteristics that failed the test in the previ-
ous pass(es). This continues till the closure
is not updated in a particular pass. Then
we proceed to generate another closure by
starting with the largest characteristic from
among the ones not included in the previous
closures. Higher degree of closure leads to
more categories to be identified. In our ex-
periment, closure degree of 3 has resulted in
5 categories of words.
4. Classification based on mutual exclusion of
suffixes: Another observation in this regard
is that while there are certain suffixes that
apply to words of multiple linguistic cate-
gory, there are other suffixes which are spe-
cific to certain such categories. So we have
tried another idea. In this, for each suffix s,
we find out the set of suffixes which have oc-
curred together with s by scanning through
all the characteristics. This implies that for
the remaining suffixes there is no evidence
that they occur in the linguistic category
where s applies. That is, they are excluded by
s. Hence we partition the set of characteris-
tics into two: one with characteristics which
contain any suffix excluded by s, and another
with the remaining characteristics. However,
our experiment using this approach has so
far produced too many categories of words.
Though these categories do not very much
map to the known linguistic categories, still
it leads us to consider, what we might call
hierarchical classification of words.
Classification of words based on characteristics is
likely to work better for linguistic categories that
take more suffixes. Some of the problems faced
in such classification is due to certain words be-
ing actually of multiple linguistic categories (word
sense ambiguity). In Assamese this is compara-
tively rare, though not altogether absent. For ex-
ample, kar in one sense means &amp;quot;tax&amp;quot; (a noun) and
in another it means &amp;quot;do&amp;quot; (verb imperative).
</bodyText>
<sectionHeader confidence="0.99519" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.99996475">
We have presented results of on-going unsuper-
vised morphology learning experiments in As-
samese, an Indic language. There are no published
computational linguistic work in Assamese. There
is no available corpus, and we had to build one for
our experiments. There is not even one electronic
dictionary in Assamese. Our work is preliminary,
but with sufficient potential for the future.
</bodyText>
<sectionHeader confidence="0.998254" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99965614">
[1] Shwartz, Steven C., 1986. Applied Natural Lan-
guage Processing. Petrocelli Books, Princeton,
New Jersey
[2] Rich, Alaine and Knight, Kelvin, 1991. Arti-
ficial Intelligence, 2e. Tata McGraw-Hill Pub-
lishing Company Limited, New Delhi
[3] Allen, James, 1995. Natural Language Under-
standing, 2e. The Benjamin/Cummings Pub-
lishing Company Inc., Redwood City
[4] Bora, Satyanath, 1968. bahal byaakaran.
Jnananath Bora, Guwahati
[5] Goswami, Golokchandra, 1990. asamiyaa
byaakaranar moulik bisaar. Bina Library,
Guwahati
[6] Choudhury, Bhupendranath, 18e,
1973. asamiyaa bhaashaar byaakaran, pratharn
bhaag. Lawyer&apos;s Book Stall, Guwahati
[7] Sarma, Durgashankar Dev, 1977. sahaj
byaakaran. Assam State Textbook Production
and Publication Corporation Ltd., Guwahati-1
[8] Baruah, Hemchandra, 1985 Hem Kosha, 6e.
Hemkosh Prakashan, Guwahati
[9] Verma, Shyamji Gokul, 1981. Maanak Hindi
Byaakaran Tatha Rachnaa. Arya Book Depot,
New Delhi-5
[10] Whitney, William Dwight, 1977. Sanskrit
Grammar. Motilal Banarasidass, Delhi.
[11] Whitney, William Dwight, 1979. Roots, Verb
Forms and Primary Derivatives of the Sanskrit
Language. Motilal Banarasidass, Delhi.
[12] Gabor Proszeky
and Balazs Kis, &amp;quot;A Unification-based Approach
to Morpho-syntactic Parsing of Agglutinative
and Other (Highly) Inflectional Languages&amp;quot;.
A C1&apos;99 37th Annual Meeting of the Association
of Computational Linguistics
[13] Bharati, Akshar, Chaitanya, Vineet and San-
gal, Rajeev, 1995 Natural Language Processing
- A Paninian Perspective. Prentice-Hall of In-
dia Pvt Ltd., New Delhi
[14] Goldsmith, John, &amp;quot;Unsupervised Learning of
the Morphology of a Natural Language&amp;quot; Com-
putational Linguistics, 27:2 (2001), pp 153-193,
Association of Computational Linguistics
[15] Kazakov, Dimitar, &amp;quot;Unsupervised Learning of
Naive Morphology with Genetic Algorithms&amp;quot;
Workshop Notes of the ECML/MLnet Work-
shop on Empirical Learning of Natural Lan-
guage Processing Tasks, pp 105-112, April 26,
1997, Prague, Czech Republic
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000003">
<note confidence="0.886971">Morphological and Phonological Learning: Proceedings of the 6th Workshop of the ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia, July 2002, pp. 1-10. Association for Computational Linguistics.</note>
<title confidence="0.350955">2 Assamese word derivation</title>
<author confidence="0.340001">First</author>
<abstract confidence="0.993666218954249">root words can be combined to form comgrantha + melA = granthamelA This often occurs with some change in spellings of the root words, e.g., + adhin = parAdhin Compound formation in the languages of the Inbranch is of two types namely, using [11]). In this work, we do not target analysis of compounds. The second kind of is by the use of and suffixes). Affixation is sometimes differentiated into types, namely (such as in [3]), e.g., : + e = mAnuhe + tA= drihatA = firm, firmness) Our system, however, treats them alike. Languages of the Indic branch inherit many affixes from Sanskrit, as well as have many affixes specific to each language. To begin with, we lay emphasis on processing suffixes since suffixes are more common in Assamese and embody significant linguistic information. Most common among the Assamese are inflectional suffixes called [7]) and are distinct from other Indic languages. In Indic languages, particularly in Assamese, in a large number of cases of application of suffixes, spelling changes do not occur. It is also important to note that a very common morphologphenomenon in Assamese is of sufa single word. For example, + keiTA + k + ei + no boys+some+ob j ective+emphasis+emphasis) . The frequent occurence of such sequences and the large number of suffixes in some of these sequences is a phenomenon that distinguishes Assamese from most other languages. In some cases Assamese also allows certain suffixes to be detached from the base part. i.e., write the suffix as a separate word following the root word. inherits 20 prefixes Sanskrit ([4], [10], [11]). There are additional prefixes specific to Assamese. Prefixes in many cases change the meaning of words in such a way that the derived words may be treated as root words Few prefixes, viz., e, Ca, denote numbers one, two, and six of the object) na (also its other forms ne, nA, etc., all of which denote negation) only qualifies the basic meaning of the root word. 3 Decomposing words From the set of words in the input text, we identify cases where one word can be derived from another word by addition of some suffix (or prefix) in that set. We refer to such a case as a &amp;quot;decomposition&amp;quot;, the former word a &amp;quot;derivative&amp;quot;, the latter a &amp;quot;base&amp;quot; (or root), and the suffix a &amp;quot;rule&amp;quot;. i.e., Decomposition: derivative = base + rule For example, = kar + A = + za) = bahal + Ai We may be able to find more than one derivative for a single word from the same base by adding different rules. A base may be found to be a derivative with respect to another base. For example, we have the words, ab, ac, abd (a,b,c strings possibly longer than a single character). We can represent these as the following. Abstraction ab = a + b ac = a + c abd = a + bd, Example kalamar = kalam+ ar kalamat = kalam+ at kalamarhe= kalam+ arhe abd = ab + d kalamar + he the case of would like to record the + since it reflects more decomposition information than the other alternawhen considered along with = a + b. We summarize our algorithm below Preprocessing 1. Take a input text T Form a sorted list, L, of distinct words in Phase 1 3. For each word w in L, identify another word b in L such that w can be obtained by appendsome (non-null) suffix b. If there are multiple candidates for b, select the longest among them and record the decomposition = b If no b can be identified for a w, w is &amp;quot;undecomposed&amp;quot;. 4. From the decompositions identified above, • for each base b count the number of decompositions where it is a base. Call it the base of b, • for each suffix (rule), s, count the number of decompositions where it is a suffix. Call it rule frequency of s, r 5. Discard the decompositions in which the rule a a of r can be adjusted experimentally. We have used a = 1 in our experiments. Also, discard the decomin which the base has a of Phase 2 6. For each word that could not be decomposed in phase 1 (some of which may have come out as base in some decomposition), try each of the rules identified in that phase. That is, a rule (suffix) is the ending part of an undecomposed word. Record such decompositions, and the base forms. Phase 3 7. For each word w in L, identify the set of suffixes (rules) that appear with it and call it that word, C. Words are classified using their characteristics. according to the steps outlined above.] Note that in our experiment the input corpus uses Roman orthography. The mapping between the Assamese letters and the Roman letters is chosen so as to facilitate lossless translation between the two representations. The following example uses this orthography (though it reduces readability to an unaccustomed reader). 2. Suppose the list of distinct words is, asinklE, brdlE, bhArtk, bhArtr, bhArte, kAlilE, k, Urn, klinr, kr, (..., t171c.n, &lt;I)G111, 7 777) 3. Decompositions (asmk = asm + k), (asmklE = asmk + 1E), (asmlE = asm + 1E), (asmr = asm + r), (klm = k + (klmr = klm + = k + bhrtk, bhArtr, bhArte, kAlilE, Urn, inAntche. 4. Base counts : : : : 1 brdlE, bhArtk, bhArtr, kAlilE, klinr, kr, inAntche : suffixes identified are 1E, and /m) Suffix frequencies: 1,1E : r : 3, /m : 1 5. Discard the decompositions involving the /m (since they have frequency = 1). The base count of 2, 1. 6. Decompositions (brdlE = brd + 1E)*, (bhArtr = bhArt + r), (kAlilE = kAli + 1E) 7. Characteristics : {1E,r}, asmk : {1E}, klm : : : {1E}, kAli : {1E} asniklE, asndE, asTnr, brdlE, bhArtk, bhArtr, kAlilE, klnir : {} (The decompositions marked * are linguistically incorrect.) In the first phase, we extract rules by using base forms that are known, i.e., words that are present in the list of words. After this analysis, there may be words which could not be decomposed. It is possible that some of these words are actually root words and cannot be decomposed. But there may be words that could not be decomposed because no base form exists in the list of words provided. So in the second phase, we consider the &amp;quot;undecomposed&amp;quot; words and see if any &amp;quot;rule&amp;quot; identified so far can be applied to decompose such a word, i.e., if word that could not be decomposed so far, is a rule that we can decompose a + fact multiple rules might be applicable for a single word. In such a case, in a language where suffix sequences are a common phenomenon, we should give preference to rules that are longer and can be obtained as a suffix sequence (See Section 8). Otherwise suffixes with higher frequencies may be given preference. With more base forms known following the second phase, we can repeat the first phase analysis, then the second phase analysis, and theoretically, so on. In the example given above, if we were to iterate the steps after the first pass, to the word the list now, we can dethe hitherto undecomposed word + k + This gives us the new suffix e. Assuming e does not get we can decompose the word + obtaining the new word This process may be stopped when in a particular pass we do not detect any new rule or base. Phase can then be undertaken to determine the of the words based on the suffixes that each takes. Reaching such a stage, however, does not imply that all possible decompositions have been detected. Since our method works by comparing related words, the presence or absence of certain words makes significant difference. For instance, if the input list of words in the example given above not contain the word base would not be obtained, and consequently the suffix would not be obtained from further not be decomposed. In general, providing a large number of words for analysis will cause more rules to be detected in phase 1, and with the larger number of rules so obtained, more base forms can be detected through phase 2. In practice, it is typical to expect texts in chunks in successive analysis runs. So, during implementation of the algorithm, we refine it so that while a new chunk of text is processed, the words in that text are analyzed along with the words existing in the lexicon. The focus is put on the new words and their effects. An analysis based simply on detection of presence of common substrings may fail to detect decompositions where the spelling of the derived word is not simply a concatenation of the base word and the suffix but a modification of that. The proportion of such words in a corpus may vary across languages, and so will the effectiveness of the above algorithm. In a language such as Assamese (as also in other Indic languages), it is seen that the modification of spelling is generally related to the actual pronunciation of the words and the suffixes. More specifically, the sounds of the ending part of the base word and the beginning of the suffix combine to form a sound which is represented in the result by another string of letters than the combination of the original letters. That is, a word ab may combine with a rule xy to form acy instead of abxy. We feel that some amount of phonological knowledge, either acquired by a learning method, or provided directly, may be used to detect and handle such cases. 4 Information schema We accumulate the information extracted by the analysis process in two primary data structures a lexicon, and a rule base. In the lexicon we keep an entry for each word that has been encountered so far in various runs. Each entry contains the • the word, w • the base, b (`-&apos; if the word is not decomposed) • the number of times it is encountered in text, • the number of times it participates as base decompositions, • other attributes Similarly, the rule base contains entries each of which comprises the rule, suffix, or prefix) • the number of times it participates in differdecompositions, r • other attributes 5 Effect of additional text An effort to build a lexicon and enumerate morphological rules of a natural language, is unlikely to be complete in a single run of the algorithm. It is likely after processing a number of chunks of texts, no new rules are generated. However, base forms are likely to be discovered for a much longer time, and quite possibly, for ever, with new texts. But then, this is the truth about vocabulary! This expectation is borne out by the actual observations in our experiment with 111 chunks of texts depicted in Figure 1 and Figure 2. In Figure 1 we have considered the proportion of distinct unseen words (not base forms, however) to the total words in each text chunk, and in Figure 2 it is the proportion of distinct unseen rules to the total distinct rules in each text chunk. 6 Experimental results In Table 1 we present the quantitative summary of observations in an experiment with a corpus of 111 news articles containing over 49,000 words in all. results cover only I the algorithm described. With Phase 1 processing only, about 77.28% of the words are decomposed of which 65.4% are correct decompositions. Of the undecomposed words, about 71% are actually root words that should not be decomposed. Of the around 29% words that should have been decomposed but were not, about 3% are compounds and we are not keen on decomposing anyway. In other words we missed about 26% (29 - 3) of possible decompositions. On applying two simple criteria to eliminate incorrect decompositions (point 5 in the algorithm), viz., decompositions that have root frequencies 20 or higher (because bases occuring with too many suffixes are very short words, usually one or two letters, and match the leading portion of longer words not related to them), and decompositions that have suffix frequencies only a = 1, about 64% of the decompositions were retained. Of these about 90.39% were correct decompositions. In the 36% decompositions that were discarded, less than 21% were correct decompositions. It should also be noted that of the 90% correct decompositions, 12.5% are incomplete in that there were more than 0 20 40 60 80 100 120 text chunk # x 0 20 40 60 80 100 120 text chunk # % age of new words</abstract>
<note confidence="0.6915930625">70 40 20 60 50 30 10 0 y x %-age of new rules 40 20 80 60 0</note>
<abstract confidence="0.999776498113207">two morphemes in the words some of which could not be separated. Intuitively the value of a should be low due to the &amp;quot;conservative&amp;quot; way of identifying suffixes in phase 1. In the experiment we have chosen a = 1 since while higher values only marginally imthe worse; for = 2 : = = and a = 3 : = = likely to improve for larger corpus. The choice of upper limit for the root frequency is, on the other hand, less rigid. We it 19. For a value 18, = = for a value 20, = = and for value 21, = = Results from Upon running Linguistica ([14]) over the same corpus of 11450 distinct words, the output reported the analysis of only 6040 of the distinct input words, i.e., 52.75%. In these 6040 analysis the precision of decompositions is 92.05% (including about 24% correct but incomplete decompositions), and the recall is 40.62%. If we consider the fraction of the actual base words left rightly undecomposed together with the fraction of the derived correctly decomposed, then this precision is 61.54%. In the results of our program, when no filtering is done then the extended precision is 67.5%, and when decompositions with suffix frequency 1 or base frequency above 19 are discarded, then the extended precision is 77.45%. 7 Quality of decompositions Among the linguistically correct decompositions, our process leaves some decompositions incomplete, and also produces some &amp;quot;false&amp;quot; decompositions. An incomplete decomposition means detecting only some of the multiple suffixes occurring in sequence in a word. To reduce such cases the valid suffix sequences in the language can be identified and this knowledge can be used to (See Section 8), say, break up suffixes identified by the basic algorithm into valid suffix parts. The false decompositions, in general, may happen due to cases such as • both a and ab are valid words but b is not a true &amp;quot;rule&amp;quot; in the language. We refer to this case as &amp;quot;false rule&amp;quot;. An even more difficult case is that b is a true rule in the language, but not in the case of ab, i.e, ab is actually not derived from a. We refer to this case as &amp;quot;false decomposition&amp;quot;. In the example presented earlier, the rule /m obtained from the decomposition = k + lm a false rule. The decompo- = k + r a false decomposition. • ab is a word and b is a known rule, but a is not the &amp;quot;base&amp;quot; of ab. We shall refer to this case as &amp;quot;false base&amp;quot;. Here, the difficult case occurs when a is a valid word, but it has nothing to do with the word ab. This too is a case of &amp;quot;false decomposition&amp;quot;. In the exampresented earlier, the base the decomposition = brd + 1 E is a false base. In short, we have the problem of unconfirmed decompositions (i.e., whether the base forms, rules and decompositions are correct). To tackle this problem of unconfirmed decompositions we compute the probabilities that various entities, i.e., (base form, rule and decomposition) tuples are correct. For this, we assign a numeric &amp;quot;confirmation&amp;quot; value to each entity. A higher confirmation value of an entity indicates that it is more likely to be correct, and it is updated as the system processes more and more input. Qualitatively, we may apply the following heuristics to obtain the confirmation values of the entities: • The rules and decompositions discovered in phase 1 have a high confirmation value, since the base forms assumed in that phase are actually present in the text. • In phase 2 (where we discover base forms and decompositions) the confirmation value is low. Among such base forms and decompositions, the confirmation value is higher for rules (suffixes) which are long, say at least two consonants. Also, base forms identified in phase 2 have higher confirmation values if it participates in more such decompositions. • The confirmation value of a decomposition depends on the confirmation value of the base forms and the rules that are involved. 8 Identifying suffix sequences A language such as Assamese allows certain suffixes to occur together in sequence in words. For suffixes y occur with a word as suffixes may or may not appear in other arrangements. Identification of such sequences may help extract more linguistic information about the words. To classify words using the suffix characteristics, identification of valid suffix can help in obtaining a kind of canonithe suffix characteristic of a word. For if it is known that the suffix aca sequence z, due to the words w and in a corpus we associate only the suffix w and not The following simple algorithm can be used to identify valid suffix sequences in the language using the decompositions performed on the words in the input corpus 1. Start from beginning of the lexicon sorted in reverse alphabetical order. Let a string f x _seq a suffix sequence, f x the number of suffix comin f x _seq. f x _seq conthe NULL string, f x 0, and decomposition the first decomposition. 2. For the next decomposition encountered, say wxzy = wxz + y, s f x _seq y, n_s f x 1 Take the base of the decomposition, and identify the entry for it in the subsequent part of the lexicon. If the base appears as a decomposed say, s f x _seq &amp;quot;z&amp;quot; + s f x_seq, x n_sfx + this step 3 for the word Else (i.e., if the base appears as an unentry), then if x &gt; record the contents of f x _seq a valid sequence of suffixes. In our current example suppose we find that w is an undecomposed word in the lexicon. we obtain the suffix sequence z 4. Go to step 2, unless the end of the lexicon is reached. In our experiment we have identified 488 sequences of suffixes. 9 Word classification using affix knowledge We have carried out some experiments on classifying words based on the suffixes that have been encountered. These classes should resemble the various known linguistic categories of words. A brief account of our experiment is given below. Recall that we have called the set of suffixes for a word consider suffixes that occur with a frequency of 10 or more. There were 81 such suffixes. 1. Direct classification based on characteristics: Form classes of words by exact matching of respective characteristics. This leads to too many classes of words, because in a corpus many words are likely to occur only with a subset of the set of linguistically valid suffixes for it. Our analysis forms the characteristic for a word on the basis of the suffixes that have been found with the word in that corpus. So if different words of the same linguistic category have different subsets of suffixes in the corpus, they are classified into different categories. Identifying subsets of characteristics: attempt to overcome the drawback of the above method is to assume that at least some words from each true linguistic category will occur with all or almost all valid suffixes for that category. This implies that the characteristics of all words which are actually of the same linguistic category will be subset of the characteristic of the word which has occurred with all possible suffixes. For example, w occurs with all possible suffixes for its linguistic and two different subsets of the suffixes with w. Then the charof be subsets of the of w. Thus we classify ,p the same class. Also note that after a (e.g., of been found to be a subset of another (e.g., of w), other characteristics are not tested for being its (i.e., of It turns out that words that occur with very few suffixes falls in more than one class. This is because in Assamese there are some suffixes that occur with words of multiple linguistic categories. 3. Classification based on Closures of characdrawback of the idea of subsets described above is that for a linguistic category hardly any word occurs with all valid suffixes for that class. To overcome we modified the idea to master characteristic of each linguistic catby taking its tentative subsets that have occurred. We have called this master characteristic, a To compute a closure we decide on a posnumber we call the of start by selecting the largest of all characteristics, and assume that it is the closure. Then sequentially for each remaining characteristic, c, we determine if c has at common with the closure or has less than which are all common with the closure. If so, we update the closure by taking its union with c. If durone such testing of characteristics the closure actually gets updated, we have to perform another pass considering the characteristics that failed the test in the previous pass(es). This continues till the closure is not updated in a particular pass. Then we proceed to generate another closure by starting with the largest characteristic from among the ones not included in the previous closures. Higher degree of closure leads to more categories to be identified. In our experiment, closure degree of 3 has resulted in 5 categories of words. 4. Classification based on mutual exclusion of observation in this regard is that while there are certain suffixes that apply to words of multiple linguistic category, there are other suffixes which are specific to certain such categories. So we have another idea. In this, for each suffix we find out the set of suffixes which have octogether with scanning through all the characteristics. This implies that for the remaining suffixes there is no evidence that they occur in the linguistic category That is, they are by we partition the set of characteristics into two: one with characteristics which any suffix excluded by another with the remaining characteristics. However, our experiment using this approach has so far produced too many categories of words. Though these categories do not very much map to the known linguistic categories, still it leads us to consider, what we might call classification words. Classification of words based on characteristics is likely to work better for linguistic categories that Some of the problems faced in such classification is due to certain words beactually of multiple linguistic categories ambiguity). Assamese this is comparatively rare, though not altogether absent. For exone sense means &amp;quot;tax&amp;quot; (a noun) and in another it means &amp;quot;do&amp;quot; (verb imperative). 10 Conclusion We have presented results of on-going unsupervised morphology learning experiments in Assamese, an Indic language. There are no published computational linguistic work in Assamese. There is no available corpus, and we had to build one for our experiments. There is not even one electronic dictionary in Assamese. Our work is preliminary, but with sufficient potential for the future.</abstract>
<title confidence="0.422614">References</title>
<author confidence="0.387229">Steven C Shwartz</author>
<affiliation confidence="0.555744">Processing. Books, Princeton,</affiliation>
<address confidence="0.916837333333333">New Jersey Rich, Alaine and Knight, Kelvin, 1991. Arti- Intelligence, 2e. McGraw-Hill Pub-</address>
<affiliation confidence="0.706366">lishing Company Limited, New Delhi</affiliation>
<address confidence="0.957284">Allen, James, 1995. Language Under-</address>
<note confidence="0.644505631578947">2e. Benjamin/Cummings Publishing Company Inc., Redwood City Bora, Satyanath, 1968. byaakaran. Jnananath Bora, Guwahati Goswami, Golokchandra, 1990. moulik bisaar. Library, Guwahati [6] Choudhury, Bhupendranath, bhaashaar byaakaran, pratharn Book Stall, Guwahati Sarma, Durgashankar Dev, 1977. State Textbook Production and Publication Corporation Ltd., Guwahati-1 Baruah, Hemchandra, 1985 Kosha, 6e. Hemkosh Prakashan, Guwahati Verma, Shyamji Gokul, 1981. Hindi Tatha Rachnaa. Book Depot, New Delhi-5 Whitney, William Dwight, 1977. Banarasidass, Delhi. Whitney, William Dwight, 1979. Verb Forms and Primary Derivatives of the Sanskrit Banarasidass, Delhi. [12] Gabor and Balazs Kis, &amp;quot;A Unification-based Approach to Morpho-syntactic Parsing of Agglutinative and Other (Highly) Inflectional Languages&amp;quot;. 37th Annual Meeting of the Association of Computational Linguistics [13] Bharati, Akshar, Chaitanya, Vineet and San- Rajeev, 1995 Language Processing A Paninian Perspective. of India Pvt Ltd., New Delhi [14] Goldsmith, John, &amp;quot;Unsupervised Learning of Morphology of a Natural Language&amp;quot; Com- Linguistics, (2001), pp 153-193, Association of Computational Linguistics [15] Kazakov, Dimitar, &amp;quot;Unsupervised Learning of</note>
<title confidence="0.820136">Naive Morphology with Genetic Algorithms&amp;quot;</title>
<author confidence="0.409399">Workshop Notes of the ECMLMLnet Work-</author>
<affiliation confidence="0.4927135">shop on Empirical Learning of Natural Lan- Processing Tasks, 105-112, April 26,</affiliation>
<address confidence="0.862667">1997, Prague, Czech Republic</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven C Shwartz</author>
</authors>
<title>Applied Natural Language Processing.</title>
<date>1986</date>
<publisher>Petrocelli Books,</publisher>
<location>Princeton, New Jersey</location>
<marker>[1]</marker>
<rawString>Shwartz, Steven C., 1986. Applied Natural Language Processing. Petrocelli Books, Princeton, New Jersey</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alaine Rich</author>
<author>Kelvin Knight</author>
</authors>
<date>1991</date>
<journal>Artificial Intelligence,</journal>
<volume>2</volume>
<publisher>Tata McGraw-Hill Publishing Company Limited,</publisher>
<location>New Delhi</location>
<marker>[2]</marker>
<rawString>Rich, Alaine and Knight, Kelvin, 1991. Artificial Intelligence, 2e. Tata McGraw-Hill Publishing Company Limited, New Delhi</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Natural Language Understanding,</title>
<date>1995</date>
<publisher>The Benjamin/Cummings Publishing Company Inc.,</publisher>
<location>Redwood City</location>
<contexts>
<context position="854" citStr="[3]" startWordPosition="135" endWordPosition="135">roadly there are two kinds of derivations. First, multiple root words can be combined to form compounds, e.g., grantha + melA = granthamelA This often occurs with some change in spellings of the root words, e.g., par + adhin = parAdhin (91-41). Compound formation in the languages of the Indic branch is of two types namely, using sandhi and samas ([10], [11]). In this work, we do not target analysis of compounds. The second kind of derivation is by the use of affixes (prefixes and suffixes). Affixation is sometimes differentiated into two types, namely inflectional and derivational (such as in [3]), e.g., Inflectional : mAnuh + e = mAnuhe Derivational: driha + tA= drihatA = firm, firmness) Our system, however, treats them alike. Languages of the Indic branch inherit many affixes from Sanskrit, as well as have many affixes specific to each language. To begin with, we lay emphasis on processing suffixes since suffixes are more common in Assamese and embody significant linguistic information. Most common among the Assamese suffixes are inflectional suffixes called bibhaktis ([4], [5], [7]) and are distinct from bibhaktis of other Indic languages. In Indic languages, particularly in Assame</context>
</contexts>
<marker>[3]</marker>
<rawString>Allen, James, 1995. Natural Language Understanding, 2e. The Benjamin/Cummings Publishing Company Inc., Redwood City</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satyanath Bora</author>
</authors>
<title>bahal byaakaran.</title>
<date>1968</date>
<journal>Jnananath Bora, Guwahati</journal>
<contexts>
<context position="1342" citStr="[4]" startWordPosition="212" endWordPosition="212">fixes). Affixation is sometimes differentiated into two types, namely inflectional and derivational (such as in [3]), e.g., Inflectional : mAnuh + e = mAnuhe Derivational: driha + tA= drihatA = firm, firmness) Our system, however, treats them alike. Languages of the Indic branch inherit many affixes from Sanskrit, as well as have many affixes specific to each language. To begin with, we lay emphasis on processing suffixes since suffixes are more common in Assamese and embody significant linguistic information. Most common among the Assamese suffixes are inflectional suffixes called bibhaktis ([4], [5], [7]) and are distinct from bibhaktis of other Indic languages. In Indic languages, particularly in Assamese, in a large number of cases of application of suffixes, spelling changes do not occur. It is also important to note that a very common morphological phenomenon in Assamese is sequences of suffixes in a single word. For example, PrAkeiTAkeino = lirA + keiTA + k + ei + no boys+some+ob j ective+emphasis+emphasis) . The frequent occurence of such sequences and the large number of suffixes in some of these sequences is a phenomenon that distinguishes Assamese from most other languages.</context>
</contexts>
<marker>[4]</marker>
<rawString>Bora, Satyanath, 1968. bahal byaakaran. Jnananath Bora, Guwahati</rawString>
</citation>
<citation valid="true">
<authors>
<author>Golokchandra Goswami</author>
</authors>
<title>asamiyaa byaakaranar moulik bisaar.</title>
<date>1990</date>
<journal>Bina Library, Guwahati</journal>
<contexts>
<context position="1347" citStr="[5]" startWordPosition="213" endWordPosition="213">). Affixation is sometimes differentiated into two types, namely inflectional and derivational (such as in [3]), e.g., Inflectional : mAnuh + e = mAnuhe Derivational: driha + tA= drihatA = firm, firmness) Our system, however, treats them alike. Languages of the Indic branch inherit many affixes from Sanskrit, as well as have many affixes specific to each language. To begin with, we lay emphasis on processing suffixes since suffixes are more common in Assamese and embody significant linguistic information. Most common among the Assamese suffixes are inflectional suffixes called bibhaktis ([4], [5], [7]) and are distinct from bibhaktis of other Indic languages. In Indic languages, particularly in Assamese, in a large number of cases of application of suffixes, spelling changes do not occur. It is also important to note that a very common morphological phenomenon in Assamese is sequences of suffixes in a single word. For example, PrAkeiTAkeino = lirA + keiTA + k + ei + no boys+some+ob j ective+emphasis+emphasis) . The frequent occurence of such sequences and the large number of suffixes in some of these sequences is a phenomenon that distinguishes Assamese from most other languages. In s</context>
</contexts>
<marker>[5]</marker>
<rawString>Goswami, Golokchandra, 1990. asamiyaa byaakaranar moulik bisaar. Bina Library, Guwahati</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bhupendranath Choudhury</author>
</authors>
<title>asamiyaa bhaashaar byaakaran, pratharn bhaag. Lawyer&apos;s Book Stall,</title>
<date>1973</date>
<location>Guwahati</location>
<marker>[6]</marker>
<rawString>Choudhury, Bhupendranath, 18e, 1973. asamiyaa bhaashaar byaakaran, pratharn bhaag. Lawyer&apos;s Book Stall, Guwahati</rawString>
</citation>
<citation valid="true">
<authors>
<author>Durgashankar Dev Sarma</author>
</authors>
<title>sahaj byaakaran.</title>
<date>1977</date>
<booktitle>Assam State Textbook Production and Publication Corporation Ltd., Guwahati-1</booktitle>
<contexts>
<context position="1352" citStr="[7]" startWordPosition="214" endWordPosition="214">fixation is sometimes differentiated into two types, namely inflectional and derivational (such as in [3]), e.g., Inflectional : mAnuh + e = mAnuhe Derivational: driha + tA= drihatA = firm, firmness) Our system, however, treats them alike. Languages of the Indic branch inherit many affixes from Sanskrit, as well as have many affixes specific to each language. To begin with, we lay emphasis on processing suffixes since suffixes are more common in Assamese and embody significant linguistic information. Most common among the Assamese suffixes are inflectional suffixes called bibhaktis ([4], [5], [7]) and are distinct from bibhaktis of other Indic languages. In Indic languages, particularly in Assamese, in a large number of cases of application of suffixes, spelling changes do not occur. It is also important to note that a very common morphological phenomenon in Assamese is sequences of suffixes in a single word. For example, PrAkeiTAkeino = lirA + keiTA + k + ei + no boys+some+ob j ective+emphasis+emphasis) . The frequent occurence of such sequences and the large number of suffixes in some of these sequences is a phenomenon that distinguishes Assamese from most other languages. In some c</context>
</contexts>
<marker>[7]</marker>
<rawString>Sarma, Durgashankar Dev, 1977. sahaj byaakaran. Assam State Textbook Production and Publication Corporation Ltd., Guwahati-1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hemchandra Baruah</author>
</authors>
<date>1985</date>
<booktitle>Hem Kosha, 6e. Hemkosh Prakashan, Guwahati</booktitle>
<marker>[8]</marker>
<rawString>Baruah, Hemchandra, 1985 Hem Kosha, 6e. Hemkosh Prakashan, Guwahati</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shyamji Gokul Verma</author>
</authors>
<title>Maanak Hindi Byaakaran Tatha Rachnaa. Arya Book Depot,</title>
<date>1981</date>
<location>New Delhi-5</location>
<marker>[9]</marker>
<rawString>Verma, Shyamji Gokul, 1981. Maanak Hindi Byaakaran Tatha Rachnaa. Arya Book Depot, New Delhi-5</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Dwight Whitney</author>
</authors>
<title>Sanskrit Grammar. Motilal Banarasidass,</title>
<date>1977</date>
<location>Delhi.</location>
<contexts>
<context position="604" citStr="[10]" startWordPosition="94" endWordPosition="94">Morphological and Phonological Learning: Proceedings of the 6th Workshop of the ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia, July 2002, pp. 1-10. Association for Computational Linguistics. 2 Assamese word derivation Broadly there are two kinds of derivations. First, multiple root words can be combined to form compounds, e.g., grantha + melA = granthamelA This often occurs with some change in spellings of the root words, e.g., par + adhin = parAdhin (91-41). Compound formation in the languages of the Indic branch is of two types namely, using sandhi and samas ([10], [11]). In this work, we do not target analysis of compounds. The second kind of derivation is by the use of affixes (prefixes and suffixes). Affixation is sometimes differentiated into two types, namely inflectional and derivational (such as in [3]), e.g., Inflectional : mAnuh + e = mAnuhe Derivational: driha + tA= drihatA = firm, firmness) Our system, however, treats them alike. Languages of the Indic branch inherit many affixes from Sanskrit, as well as have many affixes specific to each language. To begin with, we lay emphasis on processing suffixes since suffixes are more common in Assam</context>
<context position="2163" citStr="[10]" startWordPosition="349" endWordPosition="349">rtant to note that a very common morphological phenomenon in Assamese is sequences of suffixes in a single word. For example, PrAkeiTAkeino = lirA + keiTA + k + ei + no boys+some+ob j ective+emphasis+emphasis) . The frequent occurence of such sequences and the large number of suffixes in some of these sequences is a phenomenon that distinguishes Assamese from most other languages. In some cases Assamese also allows certain suffixes to be detached from the base part. i.e., write the suffix as a separate word following the root word. Assamese inherits 20 prefixes (upasargas) from Sanskrit ([4], [10], [11]). There are additional prefixes specific to Assamese. Prefixes in many cases change the meaning of words in such a way that the derived words may be treated as root words themselves. Few prefixes, viz., e, du, Ca, (these denote numbers one, two, and six of the object) and na (also its other forms - ne, nA, nu, ni, etc., all of which denote negation) only qualifies the basic meaning of the root word. 3 Decomposing words From the set of words in the input text, we identify cases where one word can be derived from another word by addition of some suffix (or prefix) in that set. We refer to</context>
</contexts>
<marker>[10]</marker>
<rawString>Whitney, William Dwight, 1977. Sanskrit Grammar. Motilal Banarasidass, Delhi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Dwight Whitney</author>
</authors>
<title>Roots, Verb Forms and Primary Derivatives of the Sanskrit Language. Motilal Banarasidass,</title>
<date>1979</date>
<location>Delhi.</location>
<contexts>
<context position="610" citStr="[11]" startWordPosition="95" endWordPosition="95">logical and Phonological Learning: Proceedings of the 6th Workshop of the ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia, July 2002, pp. 1-10. Association for Computational Linguistics. 2 Assamese word derivation Broadly there are two kinds of derivations. First, multiple root words can be combined to form compounds, e.g., grantha + melA = granthamelA This often occurs with some change in spellings of the root words, e.g., par + adhin = parAdhin (91-41). Compound formation in the languages of the Indic branch is of two types namely, using sandhi and samas ([10], [11]). In this work, we do not target analysis of compounds. The second kind of derivation is by the use of affixes (prefixes and suffixes). Affixation is sometimes differentiated into two types, namely inflectional and derivational (such as in [3]), e.g., Inflectional : mAnuh + e = mAnuhe Derivational: driha + tA= drihatA = firm, firmness) Our system, however, treats them alike. Languages of the Indic branch inherit many affixes from Sanskrit, as well as have many affixes specific to each language. To begin with, we lay emphasis on processing suffixes since suffixes are more common in Assamese an</context>
<context position="2169" citStr="[11]" startWordPosition="350" endWordPosition="350">to note that a very common morphological phenomenon in Assamese is sequences of suffixes in a single word. For example, PrAkeiTAkeino = lirA + keiTA + k + ei + no boys+some+ob j ective+emphasis+emphasis) . The frequent occurence of such sequences and the large number of suffixes in some of these sequences is a phenomenon that distinguishes Assamese from most other languages. In some cases Assamese also allows certain suffixes to be detached from the base part. i.e., write the suffix as a separate word following the root word. Assamese inherits 20 prefixes (upasargas) from Sanskrit ([4], [10], [11]). There are additional prefixes specific to Assamese. Prefixes in many cases change the meaning of words in such a way that the derived words may be treated as root words themselves. Few prefixes, viz., e, du, Ca, (these denote numbers one, two, and six of the object) and na (also its other forms - ne, nA, nu, ni, etc., all of which denote negation) only qualifies the basic meaning of the root word. 3 Decomposing words From the set of words in the input text, we identify cases where one word can be derived from another word by addition of some suffix (or prefix) in that set. We refer to such </context>
</contexts>
<marker>[11]</marker>
<rawString>Whitney, William Dwight, 1979. Roots, Verb Forms and Primary Derivatives of the Sanskrit Language. Motilal Banarasidass, Delhi.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gabor Proszeky</author>
<author>Balazs Kis</author>
</authors>
<title>A Unification-based Approach to Morpho-syntactic Parsing of Agglutinative and Other (Highly) Inflectional Languages&amp;quot;. A</title>
<booktitle>C1&apos;99 37th Annual Meeting of the Association of Computational Linguistics</booktitle>
<marker>[12]</marker>
<rawString>Gabor Proszeky and Balazs Kis, &amp;quot;A Unification-based Approach to Morpho-syntactic Parsing of Agglutinative and Other (Highly) Inflectional Languages&amp;quot;. A C1&apos;99 37th Annual Meeting of the Association of Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Vineet Chaitanya</author>
<author>Rajeev Sangal</author>
</authors>
<title>Natural Language Processing - A Paninian Perspective.</title>
<date>1995</date>
<institution>Prentice-Hall of India Pvt Ltd.,</institution>
<location>New Delhi</location>
<marker>[13]</marker>
<rawString>Bharati, Akshar, Chaitanya, Vineet and Sangal, Rajeev, 1995 Natural Language Processing - A Paninian Perspective. Prentice-Hall of India Pvt Ltd., New Delhi</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised Learning of the Morphology of a Natural Language&amp;quot;</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<pages>153--193</pages>
<contexts>
<context position="14008" citStr="[14]" startWordPosition="2559" endWordPosition="2559"> 1. In the experiment we have chosen a = 1 since while higher values only marginally improved the precision, the recall was worse; for a = 2 : precision = 91.76%, recall = 67.16% and for a = 3 : precision = 92.83%, recall = 65.11%. The recall is likely to improve for larger corpus. The choice of upper limit for the root frequency is, on the other hand, less rigid. We kept it 19. For a value 18, precision = 90.53%, recall = 70.38%; for a value 20, precision = 90.09%, recall = 70.60%; and for a value 21, precision = 89.97%, recall = 70.60%. 6.1 Results from Linguistica Upon running Linguistica ([14]) over the same corpus of 11450 distinct words, the output reported the analysis of only 6040 of the distinct input words, i.e., 52.75%. In these 6040 analysis the precision of decompositions is 92.05% (including about 24% correct but incomplete decompositions), and the recall is 40.62%. If we consider the fraction of the actual base words left rightly undecomposed together with the fraction of the derived words correctly decomposed, then this extended precision is 61.54%. In the results of our program, when no filtering is done then the extended precision is 67.5%, and when decompositions wit</context>
</contexts>
<marker>[14]</marker>
<rawString>Goldsmith, John, &amp;quot;Unsupervised Learning of the Morphology of a Natural Language&amp;quot; Computational Linguistics, 27:2 (2001), pp 153-193, Association of Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Kazakov</author>
</authors>
<title>Unsupervised Learning of Naive Morphology with Genetic Algorithms&amp;quot;</title>
<date>1997</date>
<booktitle>Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural Language Processing Tasks,</booktitle>
<pages>105--112</pages>
<location>Prague, Czech Republic</location>
<marker>[15]</marker>
<rawString>Kazakov, Dimitar, &amp;quot;Unsupervised Learning of Naive Morphology with Genetic Algorithms&amp;quot; Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural Language Processing Tasks, pp 105-112, April 26, 1997, Prague, Czech Republic</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>