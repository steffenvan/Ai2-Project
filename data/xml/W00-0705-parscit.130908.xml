<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025386">
<note confidence="0.813759">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 25-30, Lisbon, Portugal, 2000.
</note>
<title confidence="0.9912675">
Increasing our Ignorance of Language: Identifying Language
Structure in an Unknown &apos;Signal&apos;
</title>
<author confidence="0.998799">
John Elliott and Eric Atwell and Bill Whyte
</author>
<affiliation confidence="0.9994755">
Centre for Computer Analysis of Language and Speech, School of Computer Studies
University of Leeds, Leeds, Yorkshire, LS2 9JT England
</affiliation>
<email confidence="0.780881">
Ore, eric, billwl@scs.leeds.ac.uk
</email>
<sectionHeader confidence="0.996189" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917090909091">
This paper describes algorithms and software
developed to characterise and detect generic
intelligent language-like features in an input
signal, using natural language learning tech-
niques: looking for characteristic statistical
&amp;quot;language-signatures&amp;quot; in test corpora. As
a first step towards such species-independent
language-detection, we present a suite of pro-
grams to analyse digital representations of a
range of data, and use the results to extrap-
olate whether or not there are language-like
structures which distinguish this data from
other sources, such as music, images, and white
noise. Outside our own immediate NLP sphere,
generic communication techniques are of par-
ticular interest in the astronautical community,
where two sessions are dedicated to SETI at
their annual International conference with top-
ics ranging from detecting ET technology to the
ethics and logistics of message construction (El-
liott and Atwell, 1999; 011ongren, 2000; Vakoch,
2000).
</bodyText>
<sectionHeader confidence="0.998735" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979095238095">
A useful thought experiment is to imagine
eavesdropping on a signal from outer space.
How can you decide that it is a message be-
tween intelligent life forms? We need a &apos;lan-
guage detector&apos;: or, to put it more accu-
rately, something that separates language from
non-language. But what is special about the
language signal that separates it from non-
language? Is it, indeed, separable?
The problem goal is to separate language
from non-language without dialogue, and learn
something about the structure of language in
the passing. The language may not be human
(animals, aliens, computers...), the perceptual
space can be unknown, and we cannot assume
human language structure but must begin some-
where. We need to approach the language signal
from a naive viewpoint, in effect, increasing our
ignorance and assuming as little as possible.
Given this standpoint, an informal descrip-
tion of &apos;language&apos; might include that it:
</bodyText>
<listItem confidence="0.99566744">
• has structure at several interrelated levels
• is not random
• has grammar
• has letters/characters, words, phrases and
sentences
• has parts of speech
• is recursive
• has a theme with variations
• is aperiodic but evolving
• is generative
• has transformation rules
• is designed for communication
• has Zipfian type-token distributions at sev-
eral levels
Language as a &apos;signal&apos;
• has some signalling elements (a &apos;script&apos;)
• has a hierarchy of signalling elements?
(&apos;Words&apos;, &apos;phrases&apos; etc.)
• is serial?
• is correlated across a distance of several sig-
nalling elements applying at various levels
in the hierarchy
• is usually not truly periodic
• is quasi-stationary?
• is non-ergodic?
</listItem>
<bodyText confidence="0.929575">
We assume that a language-like signal will be
encoded symbolically, i.e. with some kind of
character-stream. Our language-detection al-
gorithm for symbolic input uses a number of
</bodyText>
<page confidence="0.996565">
25
</page>
<bodyText confidence="0.999816">
statistical clues such as entropy, &amp;quot;chunking&amp;quot; to
find character bit-length and boundaries, and
matching against a Zipfian type-token distribu-
tion for &amp;quot;letters&amp;quot; and &amp;quot;words&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.359211">
2 Identifying Structure and the
&apos;Character Set&apos;
</subsectionHeader>
<bodyText confidence="0.999181166666667">
The initial task, given an incoming bit-stream,
is to identify if a language-like structure ex-
ists and if detected what are the unique pat-
terns/symbols, which constitute its &apos;character
set&apos;. A visualisation of the alternative possible
byte-lengths is gleaned by plotting the entropy
calculated for a range of possible byte-lengths
(fig 1).
In &apos;real&apos; decoding of unknown scripts it is ac-
cepted that identifying the correct set of dis-
crete symbols is no mean feat (Chadwick, 1967).
To make life simple for ourselves we assume
a digital signal with a fixed number of bits
per character. Very different techniques are re-
quired to deal with audio or analogue equivalent
waveforms (Elliott and Atwell, 2000; Elliott and
Atwell, 1999). We have reason to believe that
the following method can be modified to relax
this constraint, but this needs to be tested fur-
ther. The task then reduces to trying to iden-
tify the number of bits per character. Given the
probability of a bit is Pi; the message entropy
of a string of length N will be given by the first
order measure:
</bodyText>
<equation confidence="0.995836">
E = SU M[Piln13,]; i = 1,N
</equation>
<bodyText confidence="0.999978513513513">
If the signal contains merely a set of random dig-
its, the expected value of this function will rise
monotonically as N increases. However, if the
string contains a set of symbols of fixed length
representing a character set used for commu-
nication, it is likely to show some decrease in
entropy when analysed in blocks of this length,
because the signal is &apos;less random&apos; when thus
blocked. Of course, we need to analyse blocks
that begin and end at character boundaries. We
simply carry out the measurements in sliding
windows along the data. In figure 1, we see
what happens when we applied this to samples
of 8-bit ASCII text. We notice a clear drop,
as predicted, for a bit length of 8. Modest
progress though it may be, it is not unreason-
able to assume that the first piece of ev-
idence for the presence of language-like
structure, would be the identification of a
low-entropy, character set within the sig-
nal.
The next task, still below the stages normally
tackled by NLL researchers, is to chunk the in-
coming character-stream into words. Looking
at a range of (admittedly human language) text,
if the text includes a space-like word-separator
character, this will be the most frequent charac-
ter. So, a plausible hypothesis would be that the
most frequent character is a word-separator&apos;;
then plot type-token frequency distributions for
words, and for word-lengths. If the distribu-
tions are Zipfian, and there are no significant
&apos;outliers&apos; (very large gaps between &apos;spaces&apos; sig-
nifying very long words) then we have evidence
corroborating our space hypothesis; this also
corroborates our byte-length hypothesis, since
the two are interdependent.
</bodyText>
<sectionHeader confidence="0.988082" genericHeader="method">
3 Identifying &apos;Words&apos;
</sectionHeader>
<bodyText confidence="0.999877785714286">
Again, work by crytopaleologists suggests that,
once the character set has been found, the sep-
aration into word-like units, is not trivial and
again we cheat, slightly: we assume that the
language possesses something akin to a &apos;space&apos;
character. Taking our entropy measurement de-
scribed above as a way of separating characters,
we now try to identify which character repre-
sents &apos;space&apos;. It is not unreasonable to believe
that, in a word-based language, it is likely to be
one of the most frequently used characters.
Using a number of texts in a variety of lan-
guages, we first identified the top three most
used characters. For each of these we hy-
pothesised in turn that it represented &apos;space&apos;.
This then allowed us to segment the signal into
words-like units (&apos;words&apos; for simplicity). We
could then compute the frequency distribution
of words as a function of word length, for each
of the three candidate &apos;space&apos; characters (fig 2).
It can be seen that one &apos;separator&apos; candidate
(unsurprisingly, in fact, the most frequent char-
acter of all) results in a very varied distribu-
tion of word lengths. This is an interesting
distribution, which, on the right hand side of
the peak, approximately follows the well-known
&apos;law&apos; according to Zipf (1949), which predicts
this behaviour on the grounds of minimum ef-
</bodyText>
<footnote confidence="0.967756">
1Work is currently progressing on techniques for un-
supervised word separation without spaces.
</footnote>
<page confidence="0.996318">
26
</page>
<bodyText confidence="0.999918322580645">
fort in a communication act. Conversely, re-
sults obtained similar to the &apos;flatter&apos; distribu-
tions above, when using the most frequent char-
acter, is likely to indicate the absence of word
separators in the signal.
To ascertain whether the word-length fre-
quency distribution holds for language in gen-
eral, multiple samples from 20 different lan-
guages from Indo-European, Bantu, Semitic,
Finno-Ugrian and Malayo-Polynesian groups
were analysed (fig 3). Using statistical measures
of significance, it was found that most groups
fell well within 5- only two individual languages
were near exceeding these limits - of the pro-
posed Human language word-length profile (El-
liott et al., 2000).
Zipf&apos;s law is a strong indication of
language-like behaviour. It can be used
to segment the signal provided a &apos;space&apos;
character exists. However, we should not
assume Zipf to be an infallible language detec-
tor. Natural phenomena such as molecular dis-
tribution in yeast DNA possess characteristics
of power laws (Jenson, 1998). Nevertheless, it
is worth noting, that such non-language posses-
sors of power law characteristics generally dis-
play distribution ranges far greater than lan-
guage with long repeats far from each other
(Baldi and Brunak, 1998); characteristics de-
tectable at this level or at least higher order
entropic evaluation.
</bodyText>
<sectionHeader confidence="0.992804" genericHeader="method">
4 Identifying &apos;Phrase-like&apos; chunks
</sectionHeader>
<bodyText confidence="0.999952744680851">
Having detected a signal which satisfies cri-
teria indicating language-like structures at a
physical level (Elliott and Atwell, 2000; Elliott
and Atwell, 1999), second stage analysis is re-
quired to begin the process of identifying inter-
nal grammatical components, which constitute
the basic building blocks of the symbol system.
With the use of embedded clauses and phrases,
humans are able to represent an expression or
description, however complex, as a single com-
ponent of another description. This allows us to
build up complex structures far beyond our oth-
erwise restrictive cognitive capabilities (Minsky,
1984). Without committing ourselves to a for-
mal phrase structure approach, (in the Chom-
skian sense) or even to a less formal &apos;chunk-
ing&apos; of language (Sparkle Project, 2000), it is
this universal hierarchical structure, evident in
all human languages and believed necessary for
any advanced communicator, that constitutes
the next phase in our signal analysis (Elliott and
Atwell, 2000). It is from these &apos;discovered&apos; ba-
sic syntactic units that analysis of behavioural
trends and inter-relationships amongst termi-
nals and non-terminals alike can begin to unlock
the encoded internal grammatical structure and
indicate candidate parts of speech. To do this,
we make use of a particular feature common to
many known languages, the &apos;function&apos; words,
which occur in corpora with approximately the
same statistics. These tend to act as bound-
aries to fairly self-contained semantic/syntactic
&apos;chunks.&apos; They can be identified in corpora by
their usually high frequency of occurrence and
cross-corpora invariance, as opposed to &apos;con-
tent&apos; words which are usually less frequent and
much more context dependent.
Now suppose the function words arrived in a
text independent of the other words, then they
would have a Poisson distribution, with some
long tails (distance between successive function
words.) But this is NOT what happens. In-
stead, there is empirical evidence that function
word separation is constrained to within short
limits, with very few more than nine words
apart (see fig 4). We conjecture that this is
highly suggestive of chunking.
</bodyText>
<sectionHeader confidence="0.929061" genericHeader="method">
5 Clustering into
syntactico-semantic classes
</sectionHeader>
<bodyText confidence="0.999822894736842">
Unlike traditional natural language process-
ing, a solution cannot be assisted using vast
amounts of training data with well-documented
&apos;legal&apos; syntax and semantic interpretation or
known statistical behaviour of speech cate-
gories. Therefore, at this stage we are endeav-
ouring to extract the syntactic elements with-
out a &apos;Rossetta&apos; stone and by making as few as-
sumptions as possible. Given this, a generic sys-
tem is required to facilitate the analysis of be-
havioural trends amongst selected pairs of ter-
minals and non-terminals alike, regardless of the
target language.
Therefore, an intermediate research goal is to
apply Natural Language Learning techniques to
the identification of &amp;quot;higher-level&amp;quot; lexical and
grammatical patterns and structure in a lin-
guistic signal. We have begun the development
of tools to visualise the correlation profiles be-
</bodyText>
<page confidence="0.995482">
27
</page>
<bodyText confidence="0.999765921568628">
tween pairs of words or parts of speech, as a pre-
cursor to deducing general principles for &apos;typing&apos;
and clustering into syntactico-semantic lexical
classes. Linguists have long known that collo-
cation and combinational patterns are charac-
teristic features of natural languages, which set
them apart (Sinclair, 1991). Speech and lan-
guage technology researchers have used word-
bigram and n-gram models in speech recogni-
tion, and variants of PoS-bigram models for
Part-of-Speech tagging. In general, these mod-
els focus on immediate neighbouring words, but
pairs of words may have bonds despite sepa-
ration by intervening words; this is more rele-
vant in semantic analysis, eg Wilson and Rayson
(1993), Demetriou (1997). We sought to in-
vestigate possible bonding between type tokens
(i.e., pairs of words or between parts of speech
tags) at a range of separations, by mapping the
correlation profile between a pair of words or
tags. This can be computed for given word-pair
type (wl ,w2) by recording each word-pair token
(wl,w2,d) in a corpus, where d is the distance or
number of intervening words. The distribution
of these word-pair tokens can be visualised by
plotting d (distance between wl and w2) against
frequency (how many (wl,w2,d) tokens found at
this distance). Distance can be negative, mean-
ing that w2 occurred before wl and for any size
window (i.e., 2 to n). In other words, we postu-
late that it might be possible to deduce part-of-
speech membership and, indeed, identify a set
of part-of-speech classes, using the joint proba-
bility of words themselves. But is this possible?
One test would be to take an already tagged
corpus and see if the parts-of-speech did indeed
fall into separable clusters.
Using a five thousand-word extract from the
LOB corpus (Johansson et al., 1986) to test this
tool, a number of parts-of-speech pairings were
analysed for their cohesive profiles. The arbi-
trary figure of five thousand was chosen, as it
both represents a sample large enough to re-
flect trends seen in samples much larger (with-
out loosing any valuable data) and a sample
size, which we see as at least plausible when
analysing ancient or extra-terrestrial languages
where data is at a premium.
Figure 5 shows the results for the relationship
between a pair of content and function words, so
identified by looking at their cross-corpus statis-
tics. It can be seen that the function word has a
high probability of preceding the content word
but has no instance of directly following it. At
least metaphorically, the graph can be consid-
ered to show the &apos;binding force&apos; between the two
words varying with their separation. We are
looking at how this metaphor might be used in
order to describe language as a molecular struc-
ture, whose &apos;inter-molecular forces&apos; can be re-
lated to part-of-speech interaction and the de-
velopment of potential semantic categories for
the unknown language.
Examining language in such a manner also
lends itself to summarising (&apos;compressing&apos;) the
behaviour to its more notable features when
forming profiles. Figure 6 depicts a 3D repre-
sentation of results obtained from profiling VB-
tags with six other major syntactic categories;
figure 7 shows the main syntactic behavioural
features found for the co-occurrence of some of
the major syntactic classes ranging over the cho-
sen window of ten words.
Such a tool may also be useful in other areas,
such a lexico-grammatical analysis or tagging
of corpora. Data-oriented approaches to cor-
pus annotation use statistical n-grams and/or
constraint-based models; n-grams or constraints
with wider windows can improve error-rates,
by examining the topology of the annotation-
combination space. Such information could be
used to guide development of Constraint Gram-
mars. The English Constraint Grammar de-
scribed in (1995) includes constraint rules up
to 4 words either side of the current word (see
Table 16, p352); the peaks and troughs in the
visualisation tool might be used to find candi-
date patterns for such long-distance constraints.
Our research topic NLL4SETI (Natural Lan-
guage Learning for the Search for Extra-
Terrestrial Intelligence) is distinctive in that -
it is potentially a VERY useful application of
unsupervised NLL; - it starts from more ba-
sic assumptions than most NLL research: we
do not assume tokenisation into characters and
words, and have no tagged/parsed training cor-
pus; - it focuses on utilising statistical distri-
butional universals of language which are com-
putable and diagnostic; - this focus has led us
to develop distributional visualisation tools to
explore type/token combination distributions; -
the goal is NOT learning algorithms which anal-
</bodyText>
<page confidence="0.99643">
28
</page>
<bodyText confidence="0.9999625">
yse/annotate human language in a way which
human experts would approve of (eg phrase-
chunking corresponding to a human linguist&apos;s
parsing of English text); but algorithms which
recognise language-like structuring in a poten-
tially much wider range of digital data sets.
</bodyText>
<sectionHeader confidence="0.7319465" genericHeader="evaluation">
6 Summary and future
developments
</sectionHeader>
<bodyText confidence="0.999985692307692">
To summarise, our achievements to date include
- a method for splitting a binary digit-stream
into characters, by using entropy to diagnose
byte-length; - a method for tokenising unknown
character-streams into words of language; -
an approach to chunking words into phrase-
like sub-sequences, by assuming high-frequency
function words act as phrase-delimiters; - a vi-
sualisation tool for exploring word-combination
patterns, where word-pairs need not be imme-
diate neighbours but characteristically combine
despite several intervening words.
So far, our approaches have involved working
with languages with which we are most familiar
and, to a certain extent, making use of linguistic
&apos;knowns&apos; such as pre-tagged corpora. It is early
days yet and we make no apology for this initial
approach. However, we feel that by deliberately
reducing our dependence on prior knowledge
(Increasing our ignorance of language&apos;) and by
treating language as a &apos;signal&apos;, we might be con-
tributing a novel approach to natural language
processing which might ultimately lead to a bet-
ter, more fundamental understanding of what
distinguishes language from the rest of the sig-
nal universe.
</bodyText>
<sectionHeader confidence="0.997113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999730417910448">
P. Baldi and S. Brunak. 1998. Bioinformatics - The
Machine Learning Approach. MIT press, Cam-
bridge Massachusetts.
J. Chadwick. 1967. The Decipherment of Linear B.
Cambridge University Press.
George Demetriou. 1997. PhD thesis. School of
Computer Studies, University of Leeds.
John Elliott and Eric Atwell. 1999. Language in sig-
nals: the detection of generic species-independent
intelligent language features in symbolic and oral
communications. In Proceedings of the 50th In-
ternational Astronautical Congress. paper IAA-
99-IAA.9.1.08, International Astronautical Feder-
ation, Paris.
John Elliott and Eric Atwell. 2000. Is there any-
body out there?: The detection of intelligent
and generic language-like features. Journal of the
British Interplanetary Society, 53:1/2:13-22.
John Elliott, Eric Atwell, and Bill Whyte. 2000.
Language identification in unknown signals.
In Proceedings of COLING &apos;2000 International
Conference on Computational Linguistics. Saar-
bruecken.
H. Jenson. 1998. Self Organised Criticality. Cam-
bridge University Press.
Stig Johansson, Eric Atwell, Roger Garside,
and Geoffrey Leech. 1986. The Tagged LOB
corpus: users&apos; manual. Bergen University,
Norway: ICAME, The Norwegian Comput-
ing Centre for the Humanities. Available
from http://www.hit.uib.no/icame/lobman/lob-
cont.html.
Fred Karlsson, Atro Voutilainen, Juha Heikkila,
and Arto Anttila. 1995. Constraint Grammar:
a language-independent system for parsing unre-
stricted text. Berlin: Mouton de Gruyter.
Geoffrey Leech, Roger Garside, and Eric Atwell.
1983. The automatic grammatical tagging of the
lob corpus. ICAME Journal, 7:13-33.
Christopher Manning and Hinrich Schutze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. Cambridge: MIT Press.
M. Minsky. 1984. Why Intelligent Aliens will be In-
telligible. Cambridge University Press.
Alexander 011ongren. 2000. Large-size message con-
struction for eti. In Proceedings of the 50th In-
ternational Astronautical Congress. paper IAA-
99-IAA.9.1.09, International Astronautical Feder-
ation, Paris.
Sparkle Project. 2000. http://www.ilc.pi.cnr.it/spa
rkle/wpl-prefinal/node25.html.
John Sinclair. 1991. Corpus, concordance, colloca-
tion describing English language. Oxford Univer-
sity Press.
Doug Vakoch. 2000. Communicating scientifi-
cally formulated spiritual principles in interstel-
lar messages. In Proceedings of the 50th Inter-
national Astronautical Congress. paper IAA-99-
IAA.9.1.10, International Astronautical Federa-
tion, Paris.
Andrew Wilson and Paul Rayson. 1993. The au-
tomatic content analysis of spoken discourse. In
C. Souter and E. Atwell, editors, Corpus based
computational linguistics. Rodopi, Amsterdam.
G.K. Zipf. 1949. Human Behaviour and The Prin-
ciple of Least Effort. Addison Wesley Press, New
York. (1965 reprint).
</reference>
<page confidence="0.996288">
29
</page>
<figure confidence="0.999004">
Entropy Figure 1
8
7
6
5
4
3
2
Language
4 6 7 9
400 Figure 2: Candidate word-length distributions
350 using the 3 most .freauent characters.
300 • &apos;— &apos;•:•••• &amp;quot;••
250
200
150
100
50
0
Entropy profile as an indicator of character bit-length
Correlation profile for word pair
I 2 3 4 5 6 7 8 9
_ .
Number of words between candidate functional words
</figure>
<figureCaption confidence="0.993449">
Figure 6: V13-tag profile
</figureCaption>
<figure confidence="0.990092884615385">
Multiple samples from Indo-European, Bantu, Semkic, Fr no-
Ugrian, and Malayo-Polynesian languaae groups
25.00 -
Z% 20.00
a 15.00
a. 10.00
cu
It 5.00
0.00
Figure 3
(111111I ,r,i,
E/ CO
Word length
&apos;I&apos; I
CNI
Csl CN1
iFigure 5
I 1
P(w I ftthetionitl,w2 ten-tent)
•--
Figure 4
Function
Word
separation in
English.
Frequency
</figure>
<figureCaption confidence="0.637785875">
Figure 7 Cnoun Jj Rb Prep Cc Vb Art
Cnoun 13, A3 8* A2 13* 13*, A6 8, AZ 8, A2
Jj 13* 13 8, A5,9 A2 A2,4 8 8, A3
Rb Z, X5 A7 13 R* 6,A9 13 A2
Prep 8*, A2 A2 8*, A7 8, A3 A3 Z*,A.9 0
Cc 8*, A3:4 13 (3, Z6 A4 Z AS 13*
Vb A2 A2 13 13* 6,Z9 Z f3
Art 13 p* 8, A3,8 Z, A2 Z* Z Z, A4
</figureCaption>
<bodyText confidence="0.49718375">
Key: Z = Zero bigram - or at offset specified - occurrences. 8 = Very weak bonding- near zero - at
bigram occurrences. 13 = Strong bonding at bigram co-occurrences. * = Indicates opposing cohesive
trend when P.O.S. reversed. An = High peak beyond bigram at offset distance of &apos;n&apos;. 0:13 = Flat
distribution across offsets - bigram bonding evident.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.150715">
<note confidence="0.981881">of CoNLL-2000 and LLL-2000, 25-30, Lisbon, Portugal, 2000.</note>
<title confidence="0.9880175">Increasing our Ignorance of Language: Identifying Structure in an Unknown &apos;Signal&apos;</title>
<author confidence="0.999922">Elliott Atwell</author>
<affiliation confidence="0.9441355">Centre for Computer Analysis of Language and Speech, School of Computer University of Leeds, Leeds, Yorkshire, LS2 9JT</affiliation>
<email confidence="0.747697">eric,</email>
<abstract confidence="0.985713">This paper describes algorithms and software developed to characterise and detect generic intelligent language-like features in an input signal, using natural language learning techniques: looking for characteristic statistical &amp;quot;language-signatures&amp;quot; in test corpora. a first step towards such species-independent language-detection, we present a suite of programs to analyse digital representations of a range of data, and use the results to extrapolate whether or not there are language-like structures which distinguish this data from other sources, such as music, images, and white noise. Outside our own immediate NLP sphere, generic communication techniques are of particular interest in the astronautical community, where two sessions are dedicated to SETI at their annual International conference with topics ranging from detecting ET technology to the ethics and logistics of message construction (Elliott and Atwell, 1999; 011ongren, 2000; Vakoch,</abstract>
<intro confidence="0.28183">2000).</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Baldi</author>
<author>S Brunak</author>
</authors>
<title>Bioinformatics - The Machine Learning Approach.</title>
<date>1998</date>
<publisher>MIT press,</publisher>
<location>Cambridge Massachusetts.</location>
<contexts>
<context position="8777" citStr="Baldi and Brunak, 1998" startWordPosition="1407" endWordPosition="1410">f the proposed Human language word-length profile (Elliott et al., 2000). Zipf&apos;s law is a strong indication of language-like behaviour. It can be used to segment the signal provided a &apos;space&apos; character exists. However, we should not assume Zipf to be an infallible language detector. Natural phenomena such as molecular distribution in yeast DNA possess characteristics of power laws (Jenson, 1998). Nevertheless, it is worth noting, that such non-language possessors of power law characteristics generally display distribution ranges far greater than language with long repeats far from each other (Baldi and Brunak, 1998); characteristics detectable at this level or at least higher order entropic evaluation. 4 Identifying &apos;Phrase-like&apos; chunks Having detected a signal which satisfies criteria indicating language-like structures at a physical level (Elliott and Atwell, 2000; Elliott and Atwell, 1999), second stage analysis is required to begin the process of identifying internal grammatical components, which constitute the basic building blocks of the symbol system. With the use of embedded clauses and phrases, humans are able to represent an expression or description, however complex, as a single component of a</context>
</contexts>
<marker>Baldi, Brunak, 1998</marker>
<rawString>P. Baldi and S. Brunak. 1998. Bioinformatics - The Machine Learning Approach. MIT press, Cambridge Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chadwick</author>
</authors>
<title>The Decipherment of Linear B.</title>
<date>1967</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3864" citStr="Chadwick, 1967" startWordPosition="595" endWordPosition="596">aries, and matching against a Zipfian type-token distribution for &amp;quot;letters&amp;quot; and &amp;quot;words&amp;quot;. 2 Identifying Structure and the &apos;Character Set&apos; The initial task, given an incoming bit-stream, is to identify if a language-like structure exists and if detected what are the unique patterns/symbols, which constitute its &apos;character set&apos;. A visualisation of the alternative possible byte-lengths is gleaned by plotting the entropy calculated for a range of possible byte-lengths (fig 1). In &apos;real&apos; decoding of unknown scripts it is accepted that identifying the correct set of discrete symbols is no mean feat (Chadwick, 1967). To make life simple for ourselves we assume a digital signal with a fixed number of bits per character. Very different techniques are required to deal with audio or analogue equivalent waveforms (Elliott and Atwell, 2000; Elliott and Atwell, 1999). We have reason to believe that the following method can be modified to relax this constraint, but this needs to be tested further. The task then reduces to trying to identify the number of bits per character. Given the probability of a bit is Pi; the message entropy of a string of length N will be given by the first order measure: E = SU M[Piln13,</context>
</contexts>
<marker>Chadwick, 1967</marker>
<rawString>J. Chadwick. 1967. The Decipherment of Linear B. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Demetriou</author>
</authors>
<date>1997</date>
<tech>PhD thesis.</tech>
<institution>School of Computer Studies, University of Leeds.</institution>
<contexts>
<context position="12661" citStr="Demetriou (1997)" startWordPosition="2005" endWordPosition="2006">ing&apos; and clustering into syntactico-semantic lexical classes. Linguists have long known that collocation and combinational patterns are characteristic features of natural languages, which set them apart (Sinclair, 1991). Speech and language technology researchers have used wordbigram and n-gram models in speech recognition, and variants of PoS-bigram models for Part-of-Speech tagging. In general, these models focus on immediate neighbouring words, but pairs of words may have bonds despite separation by intervening words; this is more relevant in semantic analysis, eg Wilson and Rayson (1993), Demetriou (1997). We sought to investigate possible bonding between type tokens (i.e., pairs of words or between parts of speech tags) at a range of separations, by mapping the correlation profile between a pair of words or tags. This can be computed for given word-pair type (wl ,w2) by recording each word-pair token (wl,w2,d) in a corpus, where d is the distance or number of intervening words. The distribution of these word-pair tokens can be visualised by plotting d (distance between wl and w2) against frequency (how many (wl,w2,d) tokens found at this distance). Distance can be negative, meaning that w2 oc</context>
</contexts>
<marker>Demetriou, 1997</marker>
<rawString>George Demetriou. 1997. PhD thesis. School of Computer Studies, University of Leeds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Elliott</author>
<author>Eric Atwell</author>
</authors>
<title>Language in signals: the detection of generic species-independent intelligent language features in symbolic and oral communications.</title>
<date>1999</date>
<booktitle>In Proceedings of the 50th International Astronautical Congress. paper IAA99-IAA.9.1.08, International Astronautical Federation,</booktitle>
<location>Paris.</location>
<contexts>
<context position="1328" citStr="Elliott and Atwell, 1999" startWordPosition="186" endWordPosition="190">endent language-detection, we present a suite of programs to analyse digital representations of a range of data, and use the results to extrapolate whether or not there are language-like structures which distinguish this data from other sources, such as music, images, and white noise. Outside our own immediate NLP sphere, generic communication techniques are of particular interest in the astronautical community, where two sessions are dedicated to SETI at their annual International conference with topics ranging from detecting ET technology to the ethics and logistics of message construction (Elliott and Atwell, 1999; 011ongren, 2000; Vakoch, 2000). 1 Introduction A useful thought experiment is to imagine eavesdropping on a signal from outer space. How can you decide that it is a message between intelligent life forms? We need a &apos;language detector&apos;: or, to put it more accurately, something that separates language from non-language. But what is special about the language signal that separates it from nonlanguage? Is it, indeed, separable? The problem goal is to separate language from non-language without dialogue, and learn something about the structure of language in the passing. The language may not be h</context>
<context position="4113" citStr="Elliott and Atwell, 1999" startWordPosition="634" endWordPosition="637"> if detected what are the unique patterns/symbols, which constitute its &apos;character set&apos;. A visualisation of the alternative possible byte-lengths is gleaned by plotting the entropy calculated for a range of possible byte-lengths (fig 1). In &apos;real&apos; decoding of unknown scripts it is accepted that identifying the correct set of discrete symbols is no mean feat (Chadwick, 1967). To make life simple for ourselves we assume a digital signal with a fixed number of bits per character. Very different techniques are required to deal with audio or analogue equivalent waveforms (Elliott and Atwell, 2000; Elliott and Atwell, 1999). We have reason to believe that the following method can be modified to relax this constraint, but this needs to be tested further. The task then reduces to trying to identify the number of bits per character. Given the probability of a bit is Pi; the message entropy of a string of length N will be given by the first order measure: E = SU M[Piln13,]; i = 1,N If the signal contains merely a set of random digits, the expected value of this function will rise monotonically as N increases. However, if the string contains a set of symbols of fixed length representing a character set used for commu</context>
<context position="9059" citStr="Elliott and Atwell, 1999" startWordPosition="1447" endWordPosition="1450">or. Natural phenomena such as molecular distribution in yeast DNA possess characteristics of power laws (Jenson, 1998). Nevertheless, it is worth noting, that such non-language possessors of power law characteristics generally display distribution ranges far greater than language with long repeats far from each other (Baldi and Brunak, 1998); characteristics detectable at this level or at least higher order entropic evaluation. 4 Identifying &apos;Phrase-like&apos; chunks Having detected a signal which satisfies criteria indicating language-like structures at a physical level (Elliott and Atwell, 2000; Elliott and Atwell, 1999), second stage analysis is required to begin the process of identifying internal grammatical components, which constitute the basic building blocks of the symbol system. With the use of embedded clauses and phrases, humans are able to represent an expression or description, however complex, as a single component of another description. This allows us to build up complex structures far beyond our otherwise restrictive cognitive capabilities (Minsky, 1984). Without committing ourselves to a formal phrase structure approach, (in the Chomskian sense) or even to a less formal &apos;chunking&apos; of language</context>
</contexts>
<marker>Elliott, Atwell, 1999</marker>
<rawString>John Elliott and Eric Atwell. 1999. Language in signals: the detection of generic species-independent intelligent language features in symbolic and oral communications. In Proceedings of the 50th International Astronautical Congress. paper IAA99-IAA.9.1.08, International Astronautical Federation, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Elliott</author>
<author>Eric Atwell</author>
</authors>
<title>Is there anybody out there?: The detection of intelligent and generic language-like features.</title>
<date>2000</date>
<journal>Journal of the British Interplanetary Society,</journal>
<pages>53--1</pages>
<contexts>
<context position="4086" citStr="Elliott and Atwell, 2000" startWordPosition="630" endWordPosition="633">-like structure exists and if detected what are the unique patterns/symbols, which constitute its &apos;character set&apos;. A visualisation of the alternative possible byte-lengths is gleaned by plotting the entropy calculated for a range of possible byte-lengths (fig 1). In &apos;real&apos; decoding of unknown scripts it is accepted that identifying the correct set of discrete symbols is no mean feat (Chadwick, 1967). To make life simple for ourselves we assume a digital signal with a fixed number of bits per character. Very different techniques are required to deal with audio or analogue equivalent waveforms (Elliott and Atwell, 2000; Elliott and Atwell, 1999). We have reason to believe that the following method can be modified to relax this constraint, but this needs to be tested further. The task then reduces to trying to identify the number of bits per character. Given the probability of a bit is Pi; the message entropy of a string of length N will be given by the first order measure: E = SU M[Piln13,]; i = 1,N If the signal contains merely a set of random digits, the expected value of this function will rise monotonically as N increases. However, if the string contains a set of symbols of fixed length representing a c</context>
<context position="9032" citStr="Elliott and Atwell, 2000" startWordPosition="1443" endWordPosition="1446">infallible language detector. Natural phenomena such as molecular distribution in yeast DNA possess characteristics of power laws (Jenson, 1998). Nevertheless, it is worth noting, that such non-language possessors of power law characteristics generally display distribution ranges far greater than language with long repeats far from each other (Baldi and Brunak, 1998); characteristics detectable at this level or at least higher order entropic evaluation. 4 Identifying &apos;Phrase-like&apos; chunks Having detected a signal which satisfies criteria indicating language-like structures at a physical level (Elliott and Atwell, 2000; Elliott and Atwell, 1999), second stage analysis is required to begin the process of identifying internal grammatical components, which constitute the basic building blocks of the symbol system. With the use of embedded clauses and phrases, humans are able to represent an expression or description, however complex, as a single component of another description. This allows us to build up complex structures far beyond our otherwise restrictive cognitive capabilities (Minsky, 1984). Without committing ourselves to a formal phrase structure approach, (in the Chomskian sense) or even to a less fo</context>
</contexts>
<marker>Elliott, Atwell, 2000</marker>
<rawString>John Elliott and Eric Atwell. 2000. Is there anybody out there?: The detection of intelligent and generic language-like features. Journal of the British Interplanetary Society, 53:1/2:13-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Elliott</author>
<author>Eric Atwell</author>
<author>Bill Whyte</author>
</authors>
<title>Language identification in unknown signals.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING &apos;2000 International Conference on Computational Linguistics. Saarbruecken.</booktitle>
<contexts>
<context position="8226" citStr="Elliott et al., 2000" startWordPosition="1319" endWordPosition="1323">milar to the &apos;flatter&apos; distributions above, when using the most frequent character, is likely to indicate the absence of word separators in the signal. To ascertain whether the word-length frequency distribution holds for language in general, multiple samples from 20 different languages from Indo-European, Bantu, Semitic, Finno-Ugrian and Malayo-Polynesian groups were analysed (fig 3). Using statistical measures of significance, it was found that most groups fell well within 5- only two individual languages were near exceeding these limits - of the proposed Human language word-length profile (Elliott et al., 2000). Zipf&apos;s law is a strong indication of language-like behaviour. It can be used to segment the signal provided a &apos;space&apos; character exists. However, we should not assume Zipf to be an infallible language detector. Natural phenomena such as molecular distribution in yeast DNA possess characteristics of power laws (Jenson, 1998). Nevertheless, it is worth noting, that such non-language possessors of power law characteristics generally display distribution ranges far greater than language with long repeats far from each other (Baldi and Brunak, 1998); characteristics detectable at this level or at </context>
</contexts>
<marker>Elliott, Atwell, Whyte, 2000</marker>
<rawString>John Elliott, Eric Atwell, and Bill Whyte. 2000. Language identification in unknown signals. In Proceedings of COLING &apos;2000 International Conference on Computational Linguistics. Saarbruecken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jenson</author>
</authors>
<title>Self Organised Criticality.</title>
<date>1998</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="8552" citStr="Jenson, 1998" startWordPosition="1374" endWordPosition="1375">an and Malayo-Polynesian groups were analysed (fig 3). Using statistical measures of significance, it was found that most groups fell well within 5- only two individual languages were near exceeding these limits - of the proposed Human language word-length profile (Elliott et al., 2000). Zipf&apos;s law is a strong indication of language-like behaviour. It can be used to segment the signal provided a &apos;space&apos; character exists. However, we should not assume Zipf to be an infallible language detector. Natural phenomena such as molecular distribution in yeast DNA possess characteristics of power laws (Jenson, 1998). Nevertheless, it is worth noting, that such non-language possessors of power law characteristics generally display distribution ranges far greater than language with long repeats far from each other (Baldi and Brunak, 1998); characteristics detectable at this level or at least higher order entropic evaluation. 4 Identifying &apos;Phrase-like&apos; chunks Having detected a signal which satisfies criteria indicating language-like structures at a physical level (Elliott and Atwell, 2000; Elliott and Atwell, 1999), second stage analysis is required to begin the process of identifying internal grammatical </context>
</contexts>
<marker>Jenson, 1998</marker>
<rawString>H. Jenson. 1998. Self Organised Criticality. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stig Johansson</author>
<author>Eric Atwell</author>
<author>Roger Garside</author>
<author>Geoffrey Leech</author>
</authors>
<title>The Tagged LOB corpus: users&apos; manual.</title>
<date>1986</date>
<institution>Bergen University,</institution>
<contexts>
<context position="13736" citStr="Johansson et al., 1986" startWordPosition="2186" endWordPosition="2189">ng d (distance between wl and w2) against frequency (how many (wl,w2,d) tokens found at this distance). Distance can be negative, meaning that w2 occurred before wl and for any size window (i.e., 2 to n). In other words, we postulate that it might be possible to deduce part-ofspeech membership and, indeed, identify a set of part-of-speech classes, using the joint probability of words themselves. But is this possible? One test would be to take an already tagged corpus and see if the parts-of-speech did indeed fall into separable clusters. Using a five thousand-word extract from the LOB corpus (Johansson et al., 1986) to test this tool, a number of parts-of-speech pairings were analysed for their cohesive profiles. The arbitrary figure of five thousand was chosen, as it both represents a sample large enough to reflect trends seen in samples much larger (without loosing any valuable data) and a sample size, which we see as at least plausible when analysing ancient or extra-terrestrial languages where data is at a premium. Figure 5 shows the results for the relationship between a pair of content and function words, so identified by looking at their cross-corpus statistics. It can be seen that the function wo</context>
</contexts>
<marker>Johansson, Atwell, Garside, Leech, 1986</marker>
<rawString>Stig Johansson, Eric Atwell, Roger Garside, and Geoffrey Leech. 1986. The Tagged LOB corpus: users&apos; manual. Bergen University, Norway: ICAME, The Norwegian Computing Centre for the Humanities. Available from http://www.hit.uib.no/icame/lobman/lobcont.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
<author>Atro Voutilainen</author>
<author>Juha Heikkila</author>
<author>Arto Anttila</author>
</authors>
<title>Constraint Grammar: a language-independent system for parsing unrestricted text.</title>
<date>1995</date>
<location>Berlin: Mouton</location>
<note>de Gruyter.</note>
<marker>Karlsson, Voutilainen, Heikkila, Anttila, 1995</marker>
<rawString>Fred Karlsson, Atro Voutilainen, Juha Heikkila, and Arto Anttila. 1995. Constraint Grammar: a language-independent system for parsing unrestricted text. Berlin: Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Eric Atwell</author>
</authors>
<title>The automatic grammatical tagging of the lob corpus.</title>
<date>1983</date>
<journal>ICAME Journal,</journal>
<pages>7--13</pages>
<marker>Leech, Garside, Atwell, 1983</marker>
<rawString>Geoffrey Leech, Roger Garside, and Eric Atwell. 1983. The automatic grammatical tagging of the lob corpus. ICAME Journal, 7:13-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Hinrich Schutze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge:</location>
<marker>Manning, Schutze, 1999</marker>
<rawString>Christopher Manning and Hinrich Schutze. 1999. Foundations of Statistical Natural Language Processing. Cambridge: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Minsky</author>
</authors>
<title>Why Intelligent Aliens will be Intelligible.</title>
<date>1984</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9517" citStr="Minsky, 1984" startWordPosition="1520" endWordPosition="1521">ing detected a signal which satisfies criteria indicating language-like structures at a physical level (Elliott and Atwell, 2000; Elliott and Atwell, 1999), second stage analysis is required to begin the process of identifying internal grammatical components, which constitute the basic building blocks of the symbol system. With the use of embedded clauses and phrases, humans are able to represent an expression or description, however complex, as a single component of another description. This allows us to build up complex structures far beyond our otherwise restrictive cognitive capabilities (Minsky, 1984). Without committing ourselves to a formal phrase structure approach, (in the Chomskian sense) or even to a less formal &apos;chunking&apos; of language (Sparkle Project, 2000), it is this universal hierarchical structure, evident in all human languages and believed necessary for any advanced communicator, that constitutes the next phase in our signal analysis (Elliott and Atwell, 2000). It is from these &apos;discovered&apos; basic syntactic units that analysis of behavioural trends and inter-relationships amongst terminals and non-terminals alike can begin to unlock the encoded internal grammatical structure an</context>
</contexts>
<marker>Minsky, 1984</marker>
<rawString>M. Minsky. 1984. Why Intelligent Aliens will be Intelligible. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander 011ongren</author>
</authors>
<title>Large-size message construction for eti.</title>
<date>2000</date>
<booktitle>In Proceedings of the 50th International Astronautical Congress. paper IAA99-IAA.9.1.09, International Astronautical Federation,</booktitle>
<location>Paris.</location>
<marker>011ongren, 2000</marker>
<rawString>Alexander 011ongren. 2000. Large-size message construction for eti. In Proceedings of the 50th International Astronautical Congress. paper IAA99-IAA.9.1.09, International Astronautical Federation, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparkle Project</author>
</authors>
<date>2000</date>
<note>http://www.ilc.pi.cnr.it/spa rkle/wpl-prefinal/node25.html.</note>
<contexts>
<context position="9683" citStr="Project, 2000" startWordPosition="1548" endWordPosition="1549">tage analysis is required to begin the process of identifying internal grammatical components, which constitute the basic building blocks of the symbol system. With the use of embedded clauses and phrases, humans are able to represent an expression or description, however complex, as a single component of another description. This allows us to build up complex structures far beyond our otherwise restrictive cognitive capabilities (Minsky, 1984). Without committing ourselves to a formal phrase structure approach, (in the Chomskian sense) or even to a less formal &apos;chunking&apos; of language (Sparkle Project, 2000), it is this universal hierarchical structure, evident in all human languages and believed necessary for any advanced communicator, that constitutes the next phase in our signal analysis (Elliott and Atwell, 2000). It is from these &apos;discovered&apos; basic syntactic units that analysis of behavioural trends and inter-relationships amongst terminals and non-terminals alike can begin to unlock the encoded internal grammatical structure and indicate candidate parts of speech. To do this, we make use of a particular feature common to many known languages, the &apos;function&apos; words, which occur in corpora wit</context>
</contexts>
<marker>Project, 2000</marker>
<rawString>Sparkle Project. 2000. http://www.ilc.pi.cnr.it/spa rkle/wpl-prefinal/node25.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
</authors>
<title>Corpus, concordance, collocation describing English language.</title>
<date>1991</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="12264" citStr="Sinclair, 1991" startWordPosition="1942" endWordPosition="1943"> target language. Therefore, an intermediate research goal is to apply Natural Language Learning techniques to the identification of &amp;quot;higher-level&amp;quot; lexical and grammatical patterns and structure in a linguistic signal. We have begun the development of tools to visualise the correlation profiles be27 tween pairs of words or parts of speech, as a precursor to deducing general principles for &apos;typing&apos; and clustering into syntactico-semantic lexical classes. Linguists have long known that collocation and combinational patterns are characteristic features of natural languages, which set them apart (Sinclair, 1991). Speech and language technology researchers have used wordbigram and n-gram models in speech recognition, and variants of PoS-bigram models for Part-of-Speech tagging. In general, these models focus on immediate neighbouring words, but pairs of words may have bonds despite separation by intervening words; this is more relevant in semantic analysis, eg Wilson and Rayson (1993), Demetriou (1997). We sought to investigate possible bonding between type tokens (i.e., pairs of words or between parts of speech tags) at a range of separations, by mapping the correlation profile between a pair of word</context>
</contexts>
<marker>Sinclair, 1991</marker>
<rawString>John Sinclair. 1991. Corpus, concordance, collocation describing English language. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Vakoch</author>
</authors>
<title>Communicating scientifically formulated spiritual principles in interstellar messages.</title>
<date>2000</date>
<booktitle>In Proceedings of the 50th International Astronautical Congress. paper IAA-99-IAA.9.1.10, International Astronautical Federation,</booktitle>
<location>Paris.</location>
<contexts>
<context position="1360" citStr="Vakoch, 2000" startWordPosition="193" endWordPosition="194">te of programs to analyse digital representations of a range of data, and use the results to extrapolate whether or not there are language-like structures which distinguish this data from other sources, such as music, images, and white noise. Outside our own immediate NLP sphere, generic communication techniques are of particular interest in the astronautical community, where two sessions are dedicated to SETI at their annual International conference with topics ranging from detecting ET technology to the ethics and logistics of message construction (Elliott and Atwell, 1999; 011ongren, 2000; Vakoch, 2000). 1 Introduction A useful thought experiment is to imagine eavesdropping on a signal from outer space. How can you decide that it is a message between intelligent life forms? We need a &apos;language detector&apos;: or, to put it more accurately, something that separates language from non-language. But what is special about the language signal that separates it from nonlanguage? Is it, indeed, separable? The problem goal is to separate language from non-language without dialogue, and learn something about the structure of language in the passing. The language may not be human (animals, aliens, computers</context>
</contexts>
<marker>Vakoch, 2000</marker>
<rawString>Doug Vakoch. 2000. Communicating scientifically formulated spiritual principles in interstellar messages. In Proceedings of the 50th International Astronautical Congress. paper IAA-99-IAA.9.1.10, International Astronautical Federation, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Wilson</author>
<author>Paul Rayson</author>
</authors>
<title>The automatic content analysis of spoken discourse.</title>
<date>1993</date>
<booktitle>Corpus based computational linguistics. Rodopi,</booktitle>
<editor>In C. Souter and E. Atwell, editors,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="12643" citStr="Wilson and Rayson (1993)" startWordPosition="2001" endWordPosition="2004">eneral principles for &apos;typing&apos; and clustering into syntactico-semantic lexical classes. Linguists have long known that collocation and combinational patterns are characteristic features of natural languages, which set them apart (Sinclair, 1991). Speech and language technology researchers have used wordbigram and n-gram models in speech recognition, and variants of PoS-bigram models for Part-of-Speech tagging. In general, these models focus on immediate neighbouring words, but pairs of words may have bonds despite separation by intervening words; this is more relevant in semantic analysis, eg Wilson and Rayson (1993), Demetriou (1997). We sought to investigate possible bonding between type tokens (i.e., pairs of words or between parts of speech tags) at a range of separations, by mapping the correlation profile between a pair of words or tags. This can be computed for given word-pair type (wl ,w2) by recording each word-pair token (wl,w2,d) in a corpus, where d is the distance or number of intervening words. The distribution of these word-pair tokens can be visualised by plotting d (distance between wl and w2) against frequency (how many (wl,w2,d) tokens found at this distance). Distance can be negative, </context>
</contexts>
<marker>Wilson, Rayson, 1993</marker>
<rawString>Andrew Wilson and Paul Rayson. 1993. The automatic content analysis of spoken discourse. In C. Souter and E. Atwell, editors, Corpus based computational linguistics. Rodopi, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Zipf</author>
</authors>
<title>Human Behaviour and The Principle of Least Effort.</title>
<date>1949</date>
<publisher>Addison Wesley Press,</publisher>
<location>New York.</location>
<contexts>
<context position="7388" citStr="Zipf (1949)" startWordPosition="1192" endWordPosition="1193">of these we hypothesised in turn that it represented &apos;space&apos;. This then allowed us to segment the signal into words-like units (&apos;words&apos; for simplicity). We could then compute the frequency distribution of words as a function of word length, for each of the three candidate &apos;space&apos; characters (fig 2). It can be seen that one &apos;separator&apos; candidate (unsurprisingly, in fact, the most frequent character of all) results in a very varied distribution of word lengths. This is an interesting distribution, which, on the right hand side of the peak, approximately follows the well-known &apos;law&apos; according to Zipf (1949), which predicts this behaviour on the grounds of minimum ef1Work is currently progressing on techniques for unsupervised word separation without spaces. 26 fort in a communication act. Conversely, results obtained similar to the &apos;flatter&apos; distributions above, when using the most frequent character, is likely to indicate the absence of word separators in the signal. To ascertain whether the word-length frequency distribution holds for language in general, multiple samples from 20 different languages from Indo-European, Bantu, Semitic, Finno-Ugrian and Malayo-Polynesian groups were analysed (fi</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>G.K. Zipf. 1949. Human Behaviour and The Principle of Least Effort. Addison Wesley Press, New York. (1965 reprint).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>