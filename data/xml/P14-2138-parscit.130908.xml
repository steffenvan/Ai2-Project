<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000936">
<title confidence="0.991564">
Does the Phonology of L1 Show Up in L2 Texts?
</title>
<author confidence="0.994534">
Garrett Nicolai and Grzegorz Kondrak
</author>
<affiliation confidence="0.998434">
Department of Computing Science
University of Alberta
</affiliation>
<email confidence="0.993219">
{nicolai,gkondrak}@ualberta.ca
</email>
<sectionHeader confidence="0.997317" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999995">
The relative frequencies of character bi-
grams appear to contain much information
for predicting the first language (L1) of the
writer of a text in another language (L2).
Tsur and Rappoport (2007) interpret this
fact as evidence that word choice is dic-
tated by the phonology of L1. In order to
test their hypothesis, we design an algo-
rithm to identify the most discriminative
words and the corresponding character bi-
grams, and perform two experiments to
quantify their impact on the L1 identifica-
tion task. The results strongly suggest an
alternative explanation of the effectiveness
of character bigrams in identifying the na-
tive language of a writer.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998226403225807">
The task of Native Language Identification (NLI)
is to determine the first language of the writer of a
text in another language. In a ground-breaking pa-
per, Koppel et al. (2005) propose a set of features
for this task: function words, character n-grams,
rare part-of-speech bigrams, and various types of
errors. They report 80% accuracy in classifying a
set of English texts into five L1 languages using a
multi-class linear SVM.
The First Shared Task on Native Language
Identification (Tetreault et al., 2013) attracted sub-
missions from 29 teams. The accuracy on a set
of English texts representing eleven L1 languages
ranged from 31% to 83%. Many types of fea-
tures were employed, including word length, sen-
tence length, paragraph length, document length,
sentence complexity, punctuation and capitaliza-
tion, cognates, dependency parses, topic mod-
els, word suffixes, collocations, function word n-
grams, skip-grams, word networks, Tree Substi-
tution Grammars, string kernels, cohesion, and
passive constructions (Abu-Jbara et al., 2013; Li,
2013; Brooke and Hirst, 2013; Cimino et al., 2013;
Daudaravicius, 2013; Goutte et al., 2013; Hender-
son et al., 2013; Hladka et al., 2013; Bykh et al.,
2013; Lahiri and Mihalcea, 2013; Lynum, 2013;
Malmasi et al., 2013; Mizumoto et al., 2013; Nico-
lai et al., 2013; Popescu and Ionescu, 2013; Swan-
son, 2013; Tsvetkov et al., 2013). In particular,
word n-gram features appear to be particularly ef-
fective, as they were used by the most competitive
teams, including the one that achieved the highest
overall accuracy (Jarvis et al., 2013). Furthermore,
the most discriminative word n-grams often con-
tained the name of the native language, or coun-
tries where it is commonly spoken (Gebre et al.,
2013; Malmasi et al., 2013; Nicolai et al., 2013).
We refer to such words as toponymic terms.
There is no doubt that the toponymic terms
are useful for increasing the NLI accuracy; how-
ever, from the psycho-linguistic perspective, we
are more interested in what characteristics of L1
show up in L2 texts. Clearly, L1 affects the L2
writing in general, and the choice of words in par-
ticular, but what is the role played by the phonol-
ogy? Tsur and Rappoport (2007) observe that lim-
iting the set of features to the relative frequency of
the 200 most frequent character bigrams yields a
respectable 66% accuracy on a 5-language classi-
fication task. The authors propose the following
hypothesis to explain this finding: “the choice of
words [emphasis added] people make when writ-
ing in a second language is strongly influenced by
the phonology of their native language”. As the
orthography of alphabetic languages is at least par-
tially representative of the underlying phonology,
character bigrams may capture these phonological
preferences.
In this paper, we provide evidence against the
above hypothesis. We design an algorithm to iden-
tify the most discriminative words and the char-
acter bigrams that are indicative of such words,
</bodyText>
<page confidence="0.984145">
854
</page>
<bodyText confidence="0.907352">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 854–859,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
and perform two experiments to quantify their im-
pact on the NLI task. The results of the first ex-
periment demonstrate that the removal of a rela-
tively small set of discriminative words from the
training data significantly impairs the accuracy of
a bigram-based classifier. The results of the sec-
ond experiment reveal that the most indicative bi-
grams are quite similar across different language
sets. We conclude that character bigrams are ef-
fective in determining L1 of the author because
they reflect differences in L2 word usage that are
unrelated to the phonology of L1.
</bodyText>
<sectionHeader confidence="0.953605" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.997042038461538">
Tsur and Rappoport (2007) report that character
bigrams are more effective for the NLI task than
either unigrams or trigrams. We are interested in
identifying the character bigrams that are indica-
tive of the most discriminative words in order to
quantify their impact on the bigram-based classi-
fier.
We follow both Koppel et al. (2005) and Tsur
and Rappoport (2007) in using a multi-class SVM
classifier for the NLI task. The classifier computes
a weight for each feature coupled with each L1
language by attempting to maximize the overall
accuracy on the training set. For example, if we
train the classifier using words as features, with
values representing their frequency relative to the
length of the document, the features correspond-
ing to the word China might receive the following
weights:
Arabic Chinese Hindi Japanese Telugu
-770 1720 -276 -254 -180
These weights indicate that the word provides
strong positive evidence for Chinese as L1, as op-
posed to the other four languages.
We propose to quantify the importance of each
word by converting its SVM feature weights into
a single score using the following formula:
</bodyText>
<equation confidence="0.665864">
WordScorei =
</equation>
<bodyText confidence="0.992924">
where N is the number of languages, and wij
is the feature weight of word i in language j.
The formula assigns higher scores to words with
weights of high magnitude, either positive or neg-
ative. We use the Euclidean norm rather than the
Algorithm 1 Computing the scores of words and
bigrams in the data.
</bodyText>
<listItem confidence="0.944488714285714">
1: create list of words in training data
2: train SVM using words as features
3: for all words i do
��N
4: WordScorei = j=1 wij2
5: end for
6: sort words by WordScore
7: NormValue = WordScore200
8: create list of 200 most frequent bigrams
9: for bigrams k = 1 to 200 do
10: Bi ram5core 11 �7 WordScorei
g k = k∈i NormValue
11: end for
12: sort character bigrams by BigramScore
</listItem>
<bodyText confidence="0.99943390625">
sum of raw weights because we are interested in
the discriminative power of the words.
We normalize the word scores by dividing them
by the score of the 200th word. Consequently,
only the top 200 words have scores greater than
or equal to 1.0. For our previous example, the
200th word has a word score of 1493, while China
has a word score of 1930, which is normalized to
1930/1493 = 1.29. On the other hand, the 1000th
word gets a normalized score of 0.43.
In order to identify the bigrams that are indica-
tive of the most discriminative words, we promote
those that appear in the high-scoring words, and
downgrade those that appear in the low-scoring
words. Some bigrams that appear often in the
high-scoring words may be very common. For ex-
ample, the bigram an occurs in words like Japan,
German, and Italian, but also by itself as a deter-
miner, as an adjectival suffix, and as part of the
conjunction and. Therefore, we calculate the im-
portance score for each character bigram by multi-
plying the scores of each word in which the bigram
occurs.
Algorithm 1 summarizes our method of identi-
fying the discriminative words and indicative char-
acter bigrams. In line 2, we train an SVM on the
words encountered in the training data. In lines 3
and 4, we assign the Euclidean norm of the weight
vector of each word as its score. Starting in line
7, we determine which character bigrams are rep-
resentative of high scoring words. In line 10, we
calculate the bigram scores.
</bodyText>
<figure confidence="0.764076">
� � �N wij2
�j=1
</figure>
<page confidence="0.992647">
855
</page>
<sectionHeader confidence="0.99907" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999956">
In this section, we describe two experiments aimed
at quantifying the importance of the discriminative
words and the indicative character bigrams that are
identified by Algorithm 1.
</bodyText>
<subsectionHeader confidence="0.998526">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999986529411765">
We use two different NLI corpora. We follow the
setup of Tsur and Rappoport (2007) by extracting
two sets, denoted I1 and I2 (Table 1), from the
International Corpus of Learner English (ICLE),
Version 2 (Granger et al., 2009). Each set con-
sists of 238 documents per language, randomly se-
lected from the ICLE corpus. Each of the docu-
ments corresponds to a different author, and con-
tains between 500 and 1000 words. We follow the
methodology of the paper in performing 10-fold
cross-validation on the sets of languages used by
the authors.
For the development of the method described in
Section 2, we used a different corpus, namely the
TOEFL Non-Native English Corpus (Blanchard et
al., 2013). It consists of essays written by native
speakers of eleven languages, divided into three
English proficiency levels. In order to maintain
consistency with the ICLE sets, we extracted three
sets of five languages apiece (Table 1), with each
set including both related and unrelated languages:
European languages that use Latin script (T1),
non-European languages that use non-Latin scripts
(T2), and a mixture of both types (T3). Each sub-
corpus was divided into a training set of 80%, and
development and test sets of 10% each. The train-
ing sets are composed of approximately 700 docu-
ments per language, with an average length of 350
words per document. There are over 5000 word
types per language, and over 1000 character bi-
grams in total. The test sets include approximately
90 documents per language. We report results on
the test sets, after training on both the training and
development sets.
</bodyText>
<subsectionHeader confidence="0.997038">
3.2 Setup
</subsectionHeader>
<bodyText confidence="0.99993375">
We replicate the experiments of Tsur and Rap-
poport (2007) by limiting the features to the 200
most frequent character bigrams.1 The feature val-
ues are set to the frequency of the character bi-
</bodyText>
<footnote confidence="0.9845472">
1Our development experiments suggest that using the full
set of bigrams results in a higher accuracy of a bigram-based
classifier. However, we limit the set of features to the 200
most frequent bigrams for the sake of consistency with previ-
ous work.
</footnote>
<table confidence="0.993486">
ICLE:
I1 Bulgarian Czech French Russian Spanish
I2 Czech Dutch Italian Russian Spanish
TOEFL:
T1 French German Italian Spanish Turkish
T2 Arabic Chinese Hindi Japanese Telugu
T3 French German Japanese Korean Telugu
</table>
<tableCaption confidence="0.999932">
Table 1: The L1 language sets.
</tableCaption>
<bodyText confidence="0.9998905">
grams normalized by the length of the document.
We use these feature vectors as input to the SVM-
Multiclass classifier (Joachims, 1999). The results
are shown in the Baseline column of Table 2.
</bodyText>
<subsectionHeader confidence="0.983864">
3.3 Discriminative Words
</subsectionHeader>
<bodyText confidence="0.999976114285714">
The objective of the first experiment is to quantify
the influence of the most discriminative words on
the accuracy of the bigram-based classifier. Using
Algorithm 1, we identify the 100 most discrimi-
native words, and remove them from the training
data. The bigram counts are then recalculated, and
the new 200 most frequent bigrams are used as
features for the character-level SVM. Note that the
number of the features in the classifier remains un-
changed.
The results are shown in the Discriminative
Words column of Table 2. We see a statistically
significant drop in the accuracy of the classifier
with respect to the baseline in all sets except T3.
The words that are identified as the most discrim-
inative include function words, punctuation, very
common content words, and the toponymic terms.
The 10 highest scoring words from T1 are: indeed,
often, statement, : (colon), question, instance, ...
(ellipsis), opinion, conclude, and however. In ad-
dition, France, Turkey, Italian, Germany, and Italy
are all found among the top 70 words.
For comparison, we attempt to quantify the ef-
fect of removing the same number of randomly-
selected words from the training data. Specifically,
we discard all tokens that correspond to 100 word
types that have the same or slightly higher fre-
quency as the discriminative words. The results
are shown in the Random Words column of Ta-
ble 2. The decrease is much smaller for I1, I2, and
T1, while the accuracy actually increases for T2
and T3. This illustrates the impact that the most
discriminative words have on the bigram-based
classifier beyond simple reduction in the amount
of the training data.
</bodyText>
<page confidence="0.997604">
856
</page>
<table confidence="0.999817571428571">
Set Baseline Random Discriminative Random Indicative
Words Words Bigrams Bigrams
I1 67.5 −0.2 −3.6 −1.0 −2.2
I2 66.9 −2.5 −5.5 −0.7 −2.8
T1 60.7 −3.3 −7.7 −2.5 −3.9
T2 60.6 +0.5 −3.8 −1.1 −5.9
T3 62.2 +0.3 −0.0 −0.5 −4.1
</table>
<tableCaption confidence="0.984303">
Table 2: The impact of subsets of word types and bigram features on the accuracy of a bigram-based NLI
classifier.
</tableCaption>
<subsectionHeader confidence="0.902732">
3.4 Indicative Bigrams
</subsectionHeader>
<bodyText confidence="0.809081370967742">
Using Algorithm 1, we identify the top 20 charac-
ter bigrams, and replace them with randomly se-
lected bigrams. The results of this experiment are
reported in the Indicative Bigrams column of Ta-
ble 2. It is to be expected that the replacement of
any 20 of the top bigrams with 20 less useful bi-
grams will result in some drop in accuracy, regard-
less of which bigrams are chosen for replacement.
For comparison, the Random Bigrams column of
Table 2 shows the mean accuracy over 100 trials
obtained when 20 bigrams randomly selected from
the set of 200 bigrams are replaced with random
bigrams from outside of the set.
The results indicate that our algorithm indeed
identifies 20 bigrams that are on average more im-
portant than the other 180 bigrams. What is really
striking is that the sets of 20 indicative character
bigrams overlap substantially across different sets.
Table 3 shows 17 bigrams that are common across
the three TOEFL corpora, ordered by their score,
together with some of the highly scored words in
which they occur. Four of the bigrams consist
of punctuation marks and a space.2 The remain-
ing bigrams indicate function words, toponymic
terms like Germany, and frequent content words
like take and new.
The situation is similar in the ICLE sets, where
likewise 17 out of 20 bigrams are common. The
inter-fold overlap is even greater, with 19 out of
20 bigrams appearing in each of the 10 folds. In
particular, the bigrams fr and bu can be traced
to both the function words from and but, and the
presence of French and Bulgarian in I1. However,
the fact that the two bigrams are also on the list for
2It appears that only the relatively low frequency of most
of the punctuation bigrams prevents them from dominating
the sets of the indicative bigrams. When using all bigrams
instead of the top 200, the majority of the indicative bigrams
contain punctuation.
Bigram Words
,
,
.
.
u you Telugu
f of
ny any many Germany
yo you your
w now how
i I
y you your
ew new knew
kn know knew
ey they Turkey
wh what why where etc.
of of
ak make take
Table 3: The most indicative character bigrams in
the TOEFL corpus (sorted by score).
the I2 set, which does not include these languages,
suggests that their importance is mostly due to the
function words.
</bodyText>
<subsectionHeader confidence="0.807479">
3.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999935692307692">
In the first experiment, we showed that the re-
moval of the 100 most discriminative words from
the training data results in a significant drop in the
accuracy of the classifier that is based exclusively
on character bigrams. If the hypothesis of Tsur
and Rappoport (2007) was true, this should not be
the case, as the phonology of L1 would influence
the choice of words across the lexicon.
In the second experiment, we found that the ma-
jority of the most indicative character bigrams are
shared among different language sets. The bi-
grams appear to reflect primarily high-frequency
function words. If the hypothesis was true, this
</bodyText>
<page confidence="0.994657">
857
</page>
<bodyText confidence="0.999941">
should not be the case, as the diverse L1 phonolo-
gies would induce different sets of bigrams. In
fact, the highest scoring bigrams reflect punctu-
ation patterns, which have little to do with word
choice.
</bodyText>
<sectionHeader confidence="0.995584" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999980363636364">
We have provided experimental evidence against
the hypothesis that the phonology of L1 strongly
affects the choice of words in L2. We showed
that a small set of high-frequency function words
have disproportionate influence on the accuracy of
a bigram-based NLI classifier, and that the major-
ity of the indicative bigrams appear to be indepen-
dent of L1. This suggests an alternative explana-
tion of the effectiveness of a bigram-based classi-
fier in identifying the native language of a writer
— that the character bigrams simply mirror differ-
ences in the word usage rather than the phonology
of L1.
Our explanation concurs with the findings of
Daland (2013) that unigram frequency differences
in certain types of phonological segments between
child-directed and adult-directed speech are due to
a small number of word types, such as you, what,
and want, rather than to any general phonological
preferences. He argues that the relative frequency
of sounds in speech is driven by the relative fre-
quency of words. In a similar vein, Koppel et al.
(2005) see the usefulness of character n-grams as
“simply an artifact of variable usage of particular
words, which in turn might be the result of differ-
ent thematic preferences,” or as a reflection of the
L1 orthography.
We conclude by noting that our experimental re-
sults do not imply that the phonology of L1 has ab-
solutely no influence on L2 writing. Rather, they
show that the evidence from the Native Language
Identification task has so far been inconclusive in
this regard.
</bodyText>
<sectionHeader confidence="0.986912" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999945333333333">
We thank the participants and the organizers of
the shared task on NLI at the BEA8 workshop for
sharing their reflections on the task. We also thank
an anonymous reviewer for pointing out the study
of Daland (2013).
This research was supported by the Natural
Sciences and Engineering Research Council of
Canada and the Alberta Innovates Technology Fu-
tures.
</bodyText>
<sectionHeader confidence="0.995625" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99816520754717">
Amjad Abu-Jbara, Rahul Jha, Eric Morley, and
Dragomir Radev. 2013. Experimental results on
the native language identification shared task. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 82–88.
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.
Julian Brooke and Graeme Hirst. 2013. Using other
learner corpora in the 2013 NLI shared task. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 188–196.
Serhiy Bykh, Sowmya Vajjala, Julia Krivanek, and
Detmar Meurers. 2013. Combining shallow and
linguistically motivated features in native language
identification. In Proceedings of the Eighth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 197–206.
Andrea Cimino, Felice Dell’Orletta, Giulia Venturi,
and Simonetta Montemagni. 2013. Linguistic pro-
filing based on general–purpose features and na-
tive language identification. In Proceedings of the
Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 207–215.
Robert Daland. 2013. Variation in the input: a case
study of manner class frequencies. Journal of Child
Language, 40(5):1091–1122.
Vidas Daudaravicius. 2013. VTEX system descrip-
tion for the NLI 2013 shared task. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 89–95.
Binyam Gebrekidan Gebre, Marcos Zampieri, Peter
Wittenburg, and Tom Heskes. 2013. Improving na-
tive language identification with TF-IDF weighting.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 216–223.
Cyril Goutte, Serge L´eger, and Marine Carpuat. 2013.
Feature space selection and combination for na-
tive language identification. In Proceedings of the
Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 96–100.
Sylvaine Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. INTERNATIONAL
CORPUS OF LEARNER ENGLISH: VERSION 2.
John Henderson, Guido Zarrella, Craig Pfeifer, and
John D. Burger. 2013. Discriminating non-native
English with 350 words. In Proceedings of the
Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 101–110.
</reference>
<page confidence="0.992729">
858
</page>
<reference confidence="0.99963282051282">
Barbora Hladka, Martin Holub, and Vincent Kriz.
2013. Feature engineering in the NLI shared task
2013: Charles University submission report. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 232–241.
Scott Jarvis, Yves Bestgen, and Steve Pepper. 2013.
Maximizing classification accuracy in native lan-
guage identification. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 111–118.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods, pages 169–184. MIT Press.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author’s native language by
mining a text for errors. In Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 624–
628, Chicago, IL. ACM.
Shibamouli Lahiri and Rada Mihalcea. 2013. Using n-
gram and word network features for native language
identification. In Proceedings of the Eighth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 251–259.
Baoli Li. 2013. Recognizing English learners. na-
tive language from their writings. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 119–123.
Andr´e Lynum. 2013. Native language identification
using large scale lexical features. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 266–269.
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. NLI shared task 2013: MQ submis-
sion. In Proceedings of the Eighth Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 124–133.
Tomoya Mizumoto, Yuta Hayashibe, Keisuke Sak-
aguchi, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at the NLI 2013 shared task. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 134–139.
Garrett Nicolai, Bradley Hauer, Mohammad Salameh,
Lei Yao, and Grzegorz Kondrak. 2013. Cognate
and misspelling features for natural language iden-
tification. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 140–145.
Marius Popescu and Radu Tudor Ionescu. 2013. The
story of the characters, the DNA and the native lan-
guage. In Proceedings of the Eighth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 270–278.
Ben Swanson. 2013. Exploring syntactic representa-
tions for native language identification. In Proceed-
ings of the Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, pages
146–151.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A Report on the First Native Language Iden-
tification Shared Task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications.
Oren Tsur and Ari Rappoport. 2007. Using Classi-
fier Features for Studying the Effect of Native Lan-
guage on the Choice of Written Second Language
Words. In Proceedings of the Workshop on Cog-
nitive Aspects of Computational Language Acquisi-
tion, pages 9–16, Prague, Czech Republic.
Yulia Tsvetkov, Naama Twitto, Nathan Schneider,
Noam Ordan, Manaal Faruqui, Victor Chahuneau,
Shuly Wintner, and Chris Dyer. 2013. Identifying
the L1 of non-native writers: the CMU-Haifa sys-
tem. In Proceedings of the Eighth Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 279–287.
</reference>
<page confidence="0.999064">
859
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961655">
<title confidence="0.991969">Does the Phonology of L1 Show Up in L2 Texts?</title>
<author confidence="0.997249">Nicolai</author>
<affiliation confidence="0.996991">Department of Computing University of</affiliation>
<abstract confidence="0.998535117647059">The relative frequencies of character bigrams appear to contain much information for predicting the first language (L1) of the writer of a text in another language (L2). Tsur and Rappoport (2007) interpret this fact as evidence that word choice is dictated by the phonology of L1. In order to test their hypothesis, we design an algorithm to identify the most discriminative words and the corresponding character bigrams, and perform two experiments to quantify their impact on the L1 identification task. The results strongly suggest an alternative explanation of the effectiveness of character bigrams in identifying the native language of a writer.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Rahul Jha</author>
<author>Eric Morley</author>
<author>Dragomir Radev</author>
</authors>
<title>Experimental results on the native language identification shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>82--88</pages>
<contexts>
<context position="1878" citStr="Abu-Jbara et al., 2013" startWordPosition="288" endWordPosition="291">class linear SVM. The First Shared Task on Native Language Identification (Tetreault et al., 2013) attracted submissions from 29 teams. The accuracy on a set of English texts representing eleven L1 languages ranged from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often co</context>
</contexts>
<marker>Abu-Jbara, Jha, Morley, Radev, 2013</marker>
<rawString>Amjad Abu-Jbara, Rahul Jha, Eric Morley, and Dragomir Radev. 2013. Experimental results on the native language identification shared task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–88.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel Blanchard</author>
<author>Joel Tetreault</author>
</authors>
<location>Derrick Higgins,</location>
<marker>Blanchard, Tetreault, </marker>
<rawString>Daniel Blanchard, Joel Tetreault, Derrick Higgins,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Chodorow</author>
</authors>
<title>TOEFL11: A Corpus of Non-Native English.</title>
<date>2013</date>
<tech>Technical report, Educational Testing Service.</tech>
<marker>Cahill, Chodorow, 2013</marker>
<rawString>Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A Corpus of Non-Native English. Technical report, Educational Testing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Graeme Hirst</author>
</authors>
<title>Using other learner corpora in the 2013 NLI shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>188--196</pages>
<contexts>
<context position="1912" citStr="Brooke and Hirst, 2013" startWordPosition="294" endWordPosition="297"> Task on Native Language Identification (Tetreault et al., 2013) attracted submissions from 29 teams. The accuracy on a set of English texts representing eleven L1 languages ranged from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native lan</context>
</contexts>
<marker>Brooke, Hirst, 2013</marker>
<rawString>Julian Brooke and Graeme Hirst. 2013. Using other learner corpora in the 2013 NLI shared task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 188–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serhiy Bykh</author>
<author>Sowmya Vajjala</author>
<author>Julia Krivanek</author>
<author>Detmar Meurers</author>
</authors>
<title>Combining shallow and linguistically motivated features in native language identification.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>197--206</pages>
<contexts>
<context position="2039" citStr="Bykh et al., 2013" startWordPosition="317" endWordPosition="320">sh texts representing eleven L1 languages ranged from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to s</context>
</contexts>
<marker>Bykh, Vajjala, Krivanek, Meurers, 2013</marker>
<rawString>Serhiy Bykh, Sowmya Vajjala, Julia Krivanek, and Detmar Meurers. 2013. Combining shallow and linguistically motivated features in native language identification. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 197–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Cimino</author>
<author>Felice Dell’Orletta</author>
<author>Giulia Venturi</author>
<author>Simonetta Montemagni</author>
</authors>
<title>Linguistic profiling based on general–purpose features and native language identification.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>207--215</pages>
<marker>Cimino, Dell’Orletta, Venturi, Montemagni, 2013</marker>
<rawString>Andrea Cimino, Felice Dell’Orletta, Giulia Venturi, and Simonetta Montemagni. 2013. Linguistic profiling based on general–purpose features and native language identification. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 207–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Daland</author>
</authors>
<title>Variation in the input: a case study of manner class frequencies.</title>
<date>2013</date>
<journal>Journal of Child Language,</journal>
<volume>40</volume>
<issue>5</issue>
<contexts>
<context position="16395" citStr="Daland (2013)" startWordPosition="2767" endWordPosition="2768">inst the hypothesis that the phonology of L1 strongly affects the choice of words in L2. We showed that a small set of high-frequency function words have disproportionate influence on the accuracy of a bigram-based NLI classifier, and that the majority of the indicative bigrams appear to be independent of L1. This suggests an alternative explanation of the effectiveness of a bigram-based classifier in identifying the native language of a writer — that the character bigrams simply mirror differences in the word usage rather than the phonology of L1. Our explanation concurs with the findings of Daland (2013) that unigram frequency differences in certain types of phonological segments between child-directed and adult-directed speech are due to a small number of word types, such as you, what, and want, rather than to any general phonological preferences. He argues that the relative frequency of sounds in speech is driven by the relative frequency of words. In a similar vein, Koppel et al. (2005) see the usefulness of character n-grams as “simply an artifact of variable usage of particular words, which in turn might be the result of different thematic preferences,” or as a reflection of the L1 ortho</context>
</contexts>
<marker>Daland, 2013</marker>
<rawString>Robert Daland. 2013. Variation in the input: a case study of manner class frequencies. Journal of Child Language, 40(5):1091–1122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vidas Daudaravicius</author>
</authors>
<title>VTEX system description for the NLI 2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>89--95</pages>
<contexts>
<context position="1954" citStr="Daudaravicius, 2013" startWordPosition="302" endWordPosition="303">eault et al., 2013) attracted submissions from 29 teams. The accuracy on a set of English texts representing eleven L1 languages ranged from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly s</context>
</contexts>
<marker>Daudaravicius, 2013</marker>
<rawString>Vidas Daudaravicius. 2013. VTEX system description for the NLI 2013 shared task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 89–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binyam Gebrekidan Gebre</author>
<author>Marcos Zampieri</author>
<author>Peter Wittenburg</author>
<author>Tom Heskes</author>
</authors>
<title>Improving native language identification with TF-IDF weighting.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="2579" citStr="Gebre et al., 2013" startWordPosition="407" endWordPosition="410">e et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic terms. There is no doubt that the toponymic terms are useful for increasing the NLI accuracy; however, from the psycho-linguistic perspective, we are more interested in what characteristics of L1 show up in L2 texts. Clearly, L1 affects the L2 writing in general, and the choice of words in particular, but what is the role played by the phonology? Tsur and Rappoport (2007) observe that limiting the set of features to the relative frequency of the 200 most frequent character bigrams yields a respectable 66% accura</context>
</contexts>
<marker>Gebre, Zampieri, Wittenburg, Heskes, 2013</marker>
<rawString>Binyam Gebrekidan Gebre, Marcos Zampieri, Peter Wittenburg, and Tom Heskes. 2013. Improving native language identification with TF-IDF weighting. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Goutte</author>
<author>Serge L´eger</author>
<author>Marine Carpuat</author>
</authors>
<title>Feature space selection and combination for native language identification.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>96--100</pages>
<marker>Goutte, L´eger, Carpuat, 2013</marker>
<rawString>Cyril Goutte, Serge L´eger, and Marine Carpuat. 2013. Feature space selection and combination for native language identification. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 96–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvaine Granger</author>
<author>Estelle Dagneaux</author>
<author>Fanny Meunier</author>
<author>Magali Paquot</author>
</authors>
<date>2009</date>
<journal>INTERNATIONAL CORPUS OF LEARNER ENGLISH: VERSION</journal>
<volume>2</volume>
<contexts>
<context position="8340" citStr="Granger et al., 2009" startWordPosition="1392" endWordPosition="1395">of each word as its score. Starting in line 7, we determine which character bigrams are representative of high scoring words. In line 10, we calculate the bigram scores. � � �N wij2 �j=1 855 3 Experiments In this section, we describe two experiments aimed at quantifying the importance of the discriminative words and the indicative character bigrams that are identified by Algorithm 1. 3.1 Data We use two different NLI corpora. We follow the setup of Tsur and Rappoport (2007) by extracting two sets, denoted I1 and I2 (Table 1), from the International Corpus of Learner English (ICLE), Version 2 (Granger et al., 2009). Each set consists of 238 documents per language, randomly selected from the ICLE corpus. Each of the documents corresponds to a different author, and contains between 500 and 1000 words. We follow the methodology of the paper in performing 10-fold cross-validation on the sets of languages used by the authors. For the development of the method described in Section 2, we used a different corpus, namely the TOEFL Non-Native English Corpus (Blanchard et al., 2013). It consists of essays written by native speakers of eleven languages, divided into three English proficiency levels. In order to mai</context>
</contexts>
<marker>Granger, Dagneaux, Meunier, Paquot, 2009</marker>
<rawString>Sylvaine Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot. 2009. INTERNATIONAL CORPUS OF LEARNER ENGLISH: VERSION 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Henderson</author>
<author>Guido Zarrella</author>
<author>Craig Pfeifer</author>
<author>John D Burger</author>
</authors>
<title>Discriminating non-native English with 350 words.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>101--110</pages>
<contexts>
<context position="1999" citStr="Henderson et al., 2013" startWordPosition="308" endWordPosition="312">from 29 teams. The accuracy on a set of English texts representing eleven L1 languages ranged from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 20</context>
</contexts>
<marker>Henderson, Zarrella, Pfeifer, Burger, 2013</marker>
<rawString>John Henderson, Guido Zarrella, Craig Pfeifer, and John D. Burger. 2013. Discriminating non-native English with 350 words. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 101–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbora Hladka</author>
<author>Martin Holub</author>
<author>Vincent Kriz</author>
</authors>
<title>Feature engineering in the NLI shared task 2013: Charles University submission report.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>232--241</pages>
<contexts>
<context position="2020" citStr="Hladka et al., 2013" startWordPosition="313" endWordPosition="316">acy on a set of English texts representing eleven L1 languages ranged from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2</context>
</contexts>
<marker>Hladka, Holub, Kriz, 2013</marker>
<rawString>Barbora Hladka, Martin Holub, and Vincent Kriz. 2013. Feature engineering in the NLI shared task 2013: Charles University submission report. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 232–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Jarvis</author>
<author>Yves Bestgen</author>
<author>Steve Pepper</author>
</authors>
<title>Maximizing classification accuracy in native language identification.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="2418" citStr="Jarvis et al., 2013" startWordPosition="380" endWordPosition="383">ars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic terms. There is no doubt that the toponymic terms are useful for increasing the NLI accuracy; however, from the psycho-linguistic perspective, we are more interested in what characteristics of L1 show up in L2 texts. Clearly, L1 affects the L2 writing in general, and the choice of words in particular, but what is the role played by the phonology? Tsur an</context>
</contexts>
<marker>Jarvis, Bestgen, Pepper, 2013</marker>
<rawString>Scott Jarvis, Yves Bestgen, and Steve Pepper. 2013. Maximizing classification accuracy in native language identification. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<booktitle>In Advances in kernel methods,</booktitle>
<pages>169--184</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10536" citStr="Joachims, 1999" startWordPosition="1759" endWordPosition="1760">t experiments suggest that using the full set of bigrams results in a higher accuracy of a bigram-based classifier. However, we limit the set of features to the 200 most frequent bigrams for the sake of consistency with previous work. ICLE: I1 Bulgarian Czech French Russian Spanish I2 Czech Dutch Italian Russian Spanish TOEFL: T1 French German Italian Spanish Turkish T2 Arabic Chinese Hindi Japanese Telugu T3 French German Japanese Korean Telugu Table 1: The L1 language sets. grams normalized by the length of the document. We use these feature vectors as input to the SVMMulticlass classifier (Joachims, 1999). The results are shown in the Baseline column of Table 2. 3.3 Discriminative Words The objective of the first experiment is to quantify the influence of the most discriminative words on the accuracy of the bigram-based classifier. Using Algorithm 1, we identify the 100 most discriminative words, and remove them from the training data. The bigram counts are then recalculated, and the new 200 most frequent bigrams are used as features for the character-level SVM. Note that the number of the features in the classifier remains unchanged. The results are shown in the Discriminative Words column of</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale support vector machine learning practical. In Advances in kernel methods, pages 169–184. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Kfir Zigdon</author>
</authors>
<title>Determining an author’s native language by mining a text for errors.</title>
<date>2005</date>
<booktitle>In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,</booktitle>
<pages>624--628</pages>
<publisher>ACM.</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="1021" citStr="Koppel et al. (2005)" startWordPosition="160" endWordPosition="163">nce that word choice is dictated by the phonology of L1. In order to test their hypothesis, we design an algorithm to identify the most discriminative words and the corresponding character bigrams, and perform two experiments to quantify their impact on the L1 identification task. The results strongly suggest an alternative explanation of the effectiveness of character bigrams in identifying the native language of a writer. 1 Introduction The task of Native Language Identification (NLI) is to determine the first language of the writer of a text in another language. In a ground-breaking paper, Koppel et al. (2005) propose a set of features for this task: function words, character n-grams, rare part-of-speech bigrams, and various types of errors. They report 80% accuracy in classifying a set of English texts into five L1 languages using a multi-class linear SVM. The First Shared Task on Native Language Identification (Tetreault et al., 2013) attracted submissions from 29 teams. The accuracy on a set of English texts representing eleven L1 languages ranged from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity,</context>
<context position="4935" citStr="Koppel et al. (2005)" startWordPosition="789" endWordPosition="792"> experiment reveal that the most indicative bigrams are quite similar across different language sets. We conclude that character bigrams are effective in determining L1 of the author because they reflect differences in L2 word usage that are unrelated to the phonology of L1. 2 Method Tsur and Rappoport (2007) report that character bigrams are more effective for the NLI task than either unigrams or trigrams. We are interested in identifying the character bigrams that are indicative of the most discriminative words in order to quantify their impact on the bigram-based classifier. We follow both Koppel et al. (2005) and Tsur and Rappoport (2007) in using a multi-class SVM classifier for the NLI task. The classifier computes a weight for each feature coupled with each L1 language by attempting to maximize the overall accuracy on the training set. For example, if we train the classifier using words as features, with values representing their frequency relative to the length of the document, the features corresponding to the word China might receive the following weights: Arabic Chinese Hindi Japanese Telugu -770 1720 -276 -254 -180 These weights indicate that the word provides strong positive evidence for </context>
<context position="16788" citStr="Koppel et al. (2005)" startWordPosition="2829" endWordPosition="2832">ased classifier in identifying the native language of a writer — that the character bigrams simply mirror differences in the word usage rather than the phonology of L1. Our explanation concurs with the findings of Daland (2013) that unigram frequency differences in certain types of phonological segments between child-directed and adult-directed speech are due to a small number of word types, such as you, what, and want, rather than to any general phonological preferences. He argues that the relative frequency of sounds in speech is driven by the relative frequency of words. In a similar vein, Koppel et al. (2005) see the usefulness of character n-grams as “simply an artifact of variable usage of particular words, which in turn might be the result of different thematic preferences,” or as a reflection of the L1 orthography. We conclude by noting that our experimental results do not imply that the phonology of L1 has absolutely no influence on L2 writing. Rather, they show that the evidence from the Native Language Identification task has so far been inconclusive in this regard. Acknowledgments We thank the participants and the organizers of the shared task on NLI at the BEA8 workshop for sharing their </context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005. Determining an author’s native language by mining a text for errors. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 624– 628, Chicago, IL. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shibamouli Lahiri</author>
<author>Rada Mihalcea</author>
</authors>
<title>Using ngram and word network features for native language identification.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>251--259</pages>
<contexts>
<context position="2066" citStr="Lahiri and Mihalcea, 2013" startWordPosition="321" endWordPosition="324">ng eleven L1 languages ranged from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic term</context>
</contexts>
<marker>Lahiri, Mihalcea, 2013</marker>
<rawString>Shibamouli Lahiri and Rada Mihalcea. 2013. Using ngram and word network features for native language identification. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 251–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baoli Li</author>
</authors>
<title>Recognizing English learners. native language from their writings.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>119--123</pages>
<contexts>
<context position="1888" citStr="Li, 2013" startWordPosition="292" endWordPosition="293">rst Shared Task on Native Language Identification (Tetreault et al., 2013) attracted submissions from 29 teams. The accuracy on a set of English texts representing eleven L1 languages ranged from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained th</context>
</contexts>
<marker>Li, 2013</marker>
<rawString>Baoli Li. 2013. Recognizing English learners. native language from their writings. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 119–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Lynum</author>
</authors>
<title>Native language identification using large scale lexical features.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>266--269</pages>
<contexts>
<context position="2079" citStr="Lynum, 2013" startWordPosition="325" endWordPosition="326">ed from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic terms. There is n</context>
</contexts>
<marker>Lynum, 2013</marker>
<rawString>Andr´e Lynum. 2013. Native language identification using large scale lexical features. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 266–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shervin Malmasi</author>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>NLI shared task 2013: MQ submission.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>124--133</pages>
<contexts>
<context position="2101" citStr="Malmasi et al., 2013" startWordPosition="327" endWordPosition="330">o 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic terms. There is no doubt that the topon</context>
</contexts>
<marker>Malmasi, Wong, Dras, 2013</marker>
<rawString>Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras. 2013. NLI shared task 2013: MQ submission. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 124–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoya Mizumoto</author>
<author>Yuta Hayashibe</author>
<author>Keisuke Sakaguchi</author>
<author>Mamoru Komachi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>NAIST at the NLI 2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>134--139</pages>
<contexts>
<context position="2124" citStr="Mizumoto et al., 2013" startWordPosition="331" endWordPosition="334">eatures were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic terms. There is no doubt that the toponymic terms are useful f</context>
</contexts>
<marker>Mizumoto, Hayashibe, Sakaguchi, Komachi, Matsumoto, 2013</marker>
<rawString>Tomoya Mizumoto, Yuta Hayashibe, Keisuke Sakaguchi, Mamoru Komachi, and Yuji Matsumoto. 2013. NAIST at the NLI 2013 shared task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 134–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Garrett Nicolai</author>
<author>Bradley Hauer</author>
<author>Mohammad Salameh</author>
<author>Lei Yao</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Cognate and misspelling features for natural language identification.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>140--145</pages>
<contexts>
<context position="2146" citStr="Nicolai et al., 2013" startWordPosition="335" endWordPosition="339">including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic terms. There is no doubt that the toponymic terms are useful for increasing the NLI </context>
</contexts>
<marker>Nicolai, Hauer, Salameh, Yao, Kondrak, 2013</marker>
<rawString>Garrett Nicolai, Bradley Hauer, Mohammad Salameh, Lei Yao, and Grzegorz Kondrak. 2013. Cognate and misspelling features for natural language identification. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 140–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Popescu</author>
<author>Radu Tudor Ionescu</author>
</authors>
<title>The story of the characters, the DNA and the native language.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>270--278</pages>
<contexts>
<context position="2173" citStr="Popescu and Ionescu, 2013" startWordPosition="340" endWordPosition="343"> sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic terms. There is no doubt that the toponymic terms are useful for increasing the NLI accuracy; however, from the</context>
</contexts>
<marker>Popescu, Ionescu, 2013</marker>
<rawString>Marius Popescu and Radu Tudor Ionescu. 2013. The story of the characters, the DNA and the native language. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 270–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Swanson</author>
</authors>
<title>Exploring syntactic representations for native language identification.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>146--151</pages>
<contexts>
<context position="2188" citStr="Swanson, 2013" startWordPosition="344" endWordPosition="346"> length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic terms. There is no doubt that the toponymic terms are useful for increasing the NLI accuracy; however, from the psycho-linguis</context>
</contexts>
<marker>Swanson, 2013</marker>
<rawString>Ben Swanson. 2013. Exploring syntactic representations for native language identification. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 146–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Daniel Blanchard</author>
<author>Aoife Cahill</author>
</authors>
<title>A Report on the First Native Language Identification Shared Task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<contexts>
<context position="1354" citStr="Tetreault et al., 2013" startWordPosition="212" endWordPosition="215">anation of the effectiveness of character bigrams in identifying the native language of a writer. 1 Introduction The task of Native Language Identification (NLI) is to determine the first language of the writer of a text in another language. In a ground-breaking paper, Koppel et al. (2005) propose a set of features for this task: function words, character n-grams, rare part-of-speech bigrams, and various types of errors. They report 80% accuracy in classifying a set of English texts into five L1 languages using a multi-class linear SVM. The First Shared Task on Native Language Identification (Tetreault et al., 2013) attracted submissions from 29 teams. The accuracy on a set of English texts representing eleven L1 languages ranged from 31% to 83%. Many types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013</context>
</contexts>
<marker>Tetreault, Blanchard, Cahill, 2013</marker>
<rawString>Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A Report on the First Native Language Identification Shared Task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Using Classifier Features for Studying the Effect of Native Language on the Choice of Written Second Language Words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition,</booktitle>
<pages>9--16</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3036" citStr="Tsur and Rappoport (2007)" startWordPosition="488" endWordPosition="491">, 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic terms. There is no doubt that the toponymic terms are useful for increasing the NLI accuracy; however, from the psycho-linguistic perspective, we are more interested in what characteristics of L1 show up in L2 texts. Clearly, L1 affects the L2 writing in general, and the choice of words in particular, but what is the role played by the phonology? Tsur and Rappoport (2007) observe that limiting the set of features to the relative frequency of the 200 most frequent character bigrams yields a respectable 66% accuracy on a 5-language classification task. The authors propose the following hypothesis to explain this finding: “the choice of words [emphasis added] people make when writing in a second language is strongly influenced by the phonology of their native language”. As the orthography of alphabetic languages is at least partially representative of the underlying phonology, character bigrams may capture these phonological preferences. In this paper, we provide</context>
<context position="4625" citStr="Tsur and Rappoport (2007)" startWordPosition="738" endWordPosition="741">onal Linguistics and perform two experiments to quantify their impact on the NLI task. The results of the first experiment demonstrate that the removal of a relatively small set of discriminative words from the training data significantly impairs the accuracy of a bigram-based classifier. The results of the second experiment reveal that the most indicative bigrams are quite similar across different language sets. We conclude that character bigrams are effective in determining L1 of the author because they reflect differences in L2 word usage that are unrelated to the phonology of L1. 2 Method Tsur and Rappoport (2007) report that character bigrams are more effective for the NLI task than either unigrams or trigrams. We are interested in identifying the character bigrams that are indicative of the most discriminative words in order to quantify their impact on the bigram-based classifier. We follow both Koppel et al. (2005) and Tsur and Rappoport (2007) in using a multi-class SVM classifier for the NLI task. The classifier computes a weight for each feature coupled with each L1 language by attempting to maximize the overall accuracy on the training set. For example, if we train the classifier using words as </context>
<context position="8197" citStr="Tsur and Rappoport (2007)" startWordPosition="1368" endWordPosition="1371">ams. In line 2, we train an SVM on the words encountered in the training data. In lines 3 and 4, we assign the Euclidean norm of the weight vector of each word as its score. Starting in line 7, we determine which character bigrams are representative of high scoring words. In line 10, we calculate the bigram scores. � � �N wij2 �j=1 855 3 Experiments In this section, we describe two experiments aimed at quantifying the importance of the discriminative words and the indicative character bigrams that are identified by Algorithm 1. 3.1 Data We use two different NLI corpora. We follow the setup of Tsur and Rappoport (2007) by extracting two sets, denoted I1 and I2 (Table 1), from the International Corpus of Learner English (ICLE), Version 2 (Granger et al., 2009). Each set consists of 238 documents per language, randomly selected from the ICLE corpus. Each of the documents corresponds to a different author, and contains between 500 and 1000 words. We follow the methodology of the paper in performing 10-fold cross-validation on the sets of languages used by the authors. For the development of the method described in Section 2, we used a different corpus, namely the TOEFL Non-Native English Corpus (Blanchard et a</context>
<context position="9772" citStr="Tsur and Rappoport (2007)" startWordPosition="1630" endWordPosition="1634">non-European languages that use non-Latin scripts (T2), and a mixture of both types (T3). Each subcorpus was divided into a training set of 80%, and development and test sets of 10% each. The training sets are composed of approximately 700 documents per language, with an average length of 350 words per document. There are over 5000 word types per language, and over 1000 character bigrams in total. The test sets include approximately 90 documents per language. We report results on the test sets, after training on both the training and development sets. 3.2 Setup We replicate the experiments of Tsur and Rappoport (2007) by limiting the features to the 200 most frequent character bigrams.1 The feature values are set to the frequency of the character bi1Our development experiments suggest that using the full set of bigrams results in a higher accuracy of a bigram-based classifier. However, we limit the set of features to the 200 most frequent bigrams for the sake of consistency with previous work. ICLE: I1 Bulgarian Czech French Russian Spanish I2 Czech Dutch Italian Russian Spanish TOEFL: T1 French German Italian Spanish Turkish T2 Arabic Chinese Hindi Japanese Telugu T3 French German Japanese Korean Telugu T</context>
<context position="15161" citStr="Tsur and Rappoport (2007)" startWordPosition="2559" endWordPosition="2562">y many Germany yo you your w now how i I y you your ew new knew kn know knew ey they Turkey wh what why where etc. of of ak make take Table 3: The most indicative character bigrams in the TOEFL corpus (sorted by score). the I2 set, which does not include these languages, suggests that their importance is mostly due to the function words. 3.5 Discussion In the first experiment, we showed that the removal of the 100 most discriminative words from the training data results in a significant drop in the accuracy of the classifier that is based exclusively on character bigrams. If the hypothesis of Tsur and Rappoport (2007) was true, this should not be the case, as the phonology of L1 would influence the choice of words across the lexicon. In the second experiment, we found that the majority of the most indicative character bigrams are shared among different language sets. The bigrams appear to reflect primarily high-frequency function words. If the hypothesis was true, this 857 should not be the case, as the diverse L1 phonologies would induce different sets of bigrams. In fact, the highest scoring bigrams reflect punctuation patterns, which have little to do with word choice. 4 Conclusion We have provided expe</context>
</contexts>
<marker>Tsur, Rappoport, 2007</marker>
<rawString>Oren Tsur and Ari Rappoport. 2007. Using Classifier Features for Studying the Effect of Native Language on the Choice of Written Second Language Words. In Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 9–16, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Naama Twitto</author>
<author>Nathan Schneider</author>
<author>Noam Ordan</author>
<author>Manaal Faruqui</author>
<author>Victor Chahuneau</author>
<author>Shuly Wintner</author>
<author>Chris Dyer</author>
</authors>
<title>Identifying the L1 of non-native writers: the CMU-Haifa system.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>279--287</pages>
<contexts>
<context position="2212" citStr="Tsvetkov et al., 2013" startWordPosition="347" endWordPosition="350">nt length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word ngrams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions (Abu-Jbara et al., 2013; Li, 2013; Brooke and Hirst, 2013; Cimino et al., 2013; Daudaravicius, 2013; Goutte et al., 2013; Henderson et al., 2013; Hladka et al., 2013; Bykh et al., 2013; Lahiri and Mihalcea, 2013; Lynum, 2013; Malmasi et al., 2013; Mizumoto et al., 2013; Nicolai et al., 2013; Popescu and Ionescu, 2013; Swanson, 2013; Tsvetkov et al., 2013). In particular, word n-gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy (Jarvis et al., 2013). Furthermore, the most discriminative word n-grams often contained the name of the native language, or countries where it is commonly spoken (Gebre et al., 2013; Malmasi et al., 2013; Nicolai et al., 2013). We refer to such words as toponymic terms. There is no doubt that the toponymic terms are useful for increasing the NLI accuracy; however, from the psycho-linguistic perspective, we are </context>
</contexts>
<marker>Tsvetkov, Twitto, Schneider, Ordan, Faruqui, Chahuneau, Wintner, Dyer, 2013</marker>
<rawString>Yulia Tsvetkov, Naama Twitto, Nathan Schneider, Noam Ordan, Manaal Faruqui, Victor Chahuneau, Shuly Wintner, and Chris Dyer. 2013. Identifying the L1 of non-native writers: the CMU-Haifa system. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 279–287.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>