<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000105">
<title confidence="0.9916015">
Centroid-based summarization of multiple documents: sentence
extraction, utility-based evaluation, and user studies
</title>
<author confidence="0.95667">
Dragomir R. Radev
</author>
<affiliation confidence="0.9942935">
School of Information
University of Michigan
</affiliation>
<address confidence="0.990764">
Ann Arbor, MI 48103
</address>
<email confidence="0.998508">
radev@umich.edu
</email>
<author confidence="0.860201">
Hongyan Jing
</author>
<affiliation confidence="0.9964285">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.993087">
New York, NY 10027
</address>
<email confidence="0.995789">
hjing@cs.columbia.edu
</email>
<note confidence="0.82692125">
Malgorzata Budzikowslca
IBM TJ Watson Research Center
30 Saw Mill River Road
Hawthorne, NY 10532
</note>
<email confidence="0.870781">
sm I @us. ibm.com
</email>
<sectionHeader confidence="0.988355" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998163">
We present a multi-document summarizer, called
MEAD, which generates summaries using
cluster centroids produced by a topic detection
and tracking system. We also describe two new
techniques, based on sentence utility and
subsumption, which we have applied to the
evaluation of both single and multiple document
summaries. Finally, we describe two user studies
that test our models of multi-document
summarization.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946545454546">
On October 12, 1999, a relatively small number of
news sources mentioned in passing that Pakistani
Defense Minister Gen. Pervaiz Musharraf was away
visiting Sri Lanka. However, all world agencies
would be actively reporting on the major events that
were to happen in Pakistan in the following days:
Prime Minister Nawaz Sharif announced that in Gen.
Musharrafs absence, the Defense Minister had been
-sacked and replaced by General Zia Addin. Large
numbers of messages from various sources started to
inundate the newswire: about the army&apos;s occupation
of the capital, the Prime Minister&apos;s ouster and his
subsequent placement under house arrest, Gen.
Musharraf s return to his country, his ascendancy to
power, and the imposition of military control over
Pakistan.
The paragraph above summarizes a large amount of
news from different sources. While it was not
automatically generated, one can imagine the use of
such automatically generated summaries. In this
paper we will describe how multi-document
summaries are built and evaluated.
</bodyText>
<subsectionHeader confidence="0.9701615">
1.1 Topic detection and multi-document
summarization
</subsectionHeader>
<bodyText confidence="0.999540594594595">
The process of identifying all articles on an emerging
event is called Topic Detection and Tracking (TDT).
A large body of research in TDT has been created
over the past two years [Allan et al., 98]. We will
present an extension of our own research on TDT
[Radev et al., 1999] to cover summarization of multi-
document clusters.
Our entry in the official TDT evaluation, called
CIDR [Radev et al., 1999], uses modified TF*IDF to
produce clusters of news articles on the same event.
We developed a new technique for multi-document
summarization (or MDS), called centroid-based
summarization (CBS) which uses as input the
centroids of the clusters produced by C1DR to
identify which sentences are central to the topic of
the cluster, rather than the individual articles. We
have implemented CBS in a system, named MEAD.
The main contributions of this paper are: the
development of a centroid-based multi-document
summarizer, the use of cluster-based sentence utility
(CBSU) and cross-sentence informational
subsumption (CSIS) for evaluation of single and
multi-document summaries, two user studies that
support our findings, and an evaluation of MEAD.
An event cluster, produced by a TDT system,
consists of chronologically ordered news articles
from multiple sources, which describe an event as it
develops over time. Event clusters range from2 to 10
documents from which MEAD produces summaries
in the form of sentence extracts.
A key feature of MEAD is its use of cluster centroids,
which consist of words which are central not only to
one article in a cluster, but to all the articles.
MEAD is significantly different from previous work
on multi-document summarization [Radev &amp;
McKeown, 1998; Carbonell and Goldstein, 1998;
Mani and Bloedorn, 1999; McKeown et al., 1999],
</bodyText>
<page confidence="0.998465">
21
</page>
<bodyText confidence="0.999484375">
which use techniques such as graph matching,
maximal marginal relevance, or language generation.
Finally, evaluation of multi-document summaries is a
difficult problem. There is not yet a widely accepted
evaluation scheme. We propose a utility-based
evaluation scheme, which can be used to evaluate
both single-document and multi-document
summaries.
</bodyText>
<sectionHeader confidence="0.998403" genericHeader="method">
2 Informational content of sentences
</sectionHeader>
<subsectionHeader confidence="0.985786">
2.1 Cluster-based sentence utility (CBSU)
</subsectionHeader>
<bodyText confidence="0.999709571428571">
Cluster-based sentence utility (CBSU, or utility)
refers to the degree of relevance (from 0 to 10) of a &apos;
particular sentence to the general topic of the entire
cluster (for a dis cussion of what is a topic, see [Allan
et al. 1998]). A utility of 0 means that the sentence is
not relevant to the cluster and a 10 marks an essential
sentence.
</bodyText>
<subsectionHeader confidence="0.9903475">
2.2 Cross-sentence informational
subsumption (CSIS)
</subsectionHeader>
<bodyText confidence="0.999956888888889">
A related notion to CBSU is cross-sentence
informational subsumption (CSIS, or subsumption),
which reflects that certain sentences repeat some of
the information present in other sentences and may,
therefore, be omitted during summarization. If the
information content of sentence a (denoted as i(a)) is
contained within sentence b, then a becomes
informationally redundant and the content of b is said
to subsume that of a:
</bodyText>
<equation confidence="0.39935">
i(a) c i(b)
</equation>
<bodyText confidence="0.99969575">
In the example below, (2) subsumes (1) because the
crucial information in (1) is also included in (2)
which presents additional content: &amp;quot;the court&amp;quot;, &amp;quot;last
August&amp;quot;, and &amp;quot;sentenced him to life&amp;quot;.
</bodyText>
<listItem confidence="0.972391666666667">
(1) John Doe was found guilty of the murder.
(2) The court found John Doe guilty of the murder
of Jane Doe last August and sentenced him to life.
</listItem>
<bodyText confidence="0.998591583333333">
The cluster shown in Figure I shows subsumption
links across two articles about recent terrorist
activities in Algeria (ALG 18853 and ALG 18854).
An arrow from sentence A to sentence B indicates
that the information content of A is subsumed by the
information content of B. Sentences 2, 4, and 5 from
the first article repeat the information from sentence
The full text of these articles is shown in the
Appendix.
2 in the second article, while sentence 9 from the
former article is later repeated in sentences 3 and 4 of
the latter article.
</bodyText>
<figureCaption confidence="0.97381">
Figure 1: Subsumption links across two articles:
ALG 18853 and ALG 18854.
</figureCaption>
<subsectionHeader confidence="0.998064">
2.3 Equivalence classes of sentences
</subsectionHeader>
<bodyText confidence="0.996962454545455">
Sentences subsuming each other are said to belong to
the same equivalence class. An equivalence class
may contain more than two sentences within the
same or different articles. In the following example,
although sentences (3) and (4) are not exact
paraphrases of each other, they can be substituted for
each other without crucial loss of information and
therefore belong to the same equivalence class, i.e.
i(3) c 1(4) and i(4) c i(3). In the user study section
we will take a look at the way humans perceive CSIS
and equivalence class.
</bodyText>
<listItem confidence="0.98668875">
(3) Eighteen decapitated bodies have been found
in a mass grave in northern Algeria, press reports
said Thursday.
(4) Algerian newspapers have reported on
</listItem>
<bodyText confidence="0.571755">
Thursday that 18 decapitated bodies have been
found by the authorities.
</bodyText>
<subsectionHeader confidence="0.999848">
2.4 Comparison with MMR
</subsectionHeader>
<bodyText confidence="0.999640333333334">
Maximal marginal relevance (or MMR) is a
technique similar to CSIS and was introduced in
[Carbonell and Goldstein, 1998]. In that paper, MMR
is used to produce summaries of single documents
that avoid redundancy. The authors mention that their
preliminary results indicate that multiple documents
on the same topic also contain redundancy but they
fall short of using MMR for multi-document
summarization. Their metric is used as an
enhancement to a query-based summary whereas
CSIS is designed for query-independent (a.k.a.,
generic) summaries.
</bodyText>
<page confidence="0.976914">
22
</page>
<bodyText confidence="0.992562333333333">
We now describe the corpus used for the evaluation
of MEAD, and later in this section we present
MEAD&apos;s algorithm.
</bodyText>
<figure confidence="0.9904646875">
3 MEAD: a centroid-based multi-
document summarizer
Cluster # docs # sent source
A 2
• 3
• 2
news sources topic
• 7
• 10
• 3
25 clari.world.africa.northwestem
45 clari.world.terrorism
65 clari.world.europe.russia
189 clari.world.europe.russia
151 TDT-3 corpus topic 78
83 TDT-3 corpus topic 67
</figure>
<bodyText confidence="0.954079466666667">
AFP, UPI
AFP, UPI
AP, AFP
AP, AFP, UPI
AP, PRI, VOA
AP, NYT
Algerian terrorists threaten Belgium
The FBI puts Osama bin Laden on
the most wanted list
Explosion in a Moscow apartment
building (September 9, 1999)
Explosion in a Moscow apartment
building (September 13, 1999)
General strike in Denmark
Toxic spill in Spain
</bodyText>
<tableCaption confidence="0.99683">
Table 1: Corpus composition
</tableCaption>
<subsectionHeader confidence="0.999642">
3.1 Description of the corpus
</subsectionHeader>
<bodyText confidence="0.999991125">
For our experiments, we prepared a snail corpus
consisting of a total of 558 sentences in 27
documents, organized in 6 clusters (Table 1), all
extracted by CIDR. Four of the clusters are from
Usenet newsgroups. The remaining two clusters are
from the official TDT corpus2. Among the factors for
our selection of clusters are: coverage of as many
news sources as possible, coverage of both TDT and
non-TDT data, coverage of different types of news
(e.g., terrorism, internal affairs, and environment),
and diversity in cluster sizes (in our case, from 2 to
10 articles). The test corpus is used in the evaluation
in such a way that each cluster is summarized at 9
different compression rates, thus giving nine times as
many sample points as one would expect from the
size of the corpus.
</bodyText>
<subsectionHeader confidence="0.999457">
3.2 Cluster centroids
</subsectionHeader>
<bodyText confidence="0.9869831875">
Table 2 shows a sample centroid, produced by CIDR
[Radev et al., 1999] from cluster A. The &amp;quot;count&amp;quot;
column indicates the average number of occurrences
of a word *across the entire cluster. The IDF values
were computed from the TDT corpus. A centroid, in
this context, is a pseudo-document which consists of
words which have Count*IDF scores above a pre-
defined threshold in the documents that constitute the
cluster. CIDR computes Count*IDF in an iterative
fashion, updating its values as more articles are
inserted in a given cluster. We hypothesize that
sentences that contain the words from the centroid
are more indicative of the topic of the cluster.
2 The selection of Cluster E is due to an idea by the
participants in the Novelty Detection Workshop, led
by James Allan.
</bodyText>
<table confidence="0.9997511">
Word Count IDF Count * IDF
belgium 15.50 4.96 76.86
gia 7.50 839 62.90
algerian 6.00 636 38.15
hayat 3.00 8.90 26.69
algeria 4.50 5.63 25.32
islamic 6.00 4.13 24.76
melouk 2.00 10.00 19.99
arabic 3.00 5.99 17.97
battalion 2.50 7.16 17.91
</table>
<tableCaption confidence="0.997902">
Table 2: Sample cantroid produced by CIDR
</tableCaption>
<subsectionHeader confidence="0.994052">
3.3 Centroid-based algorithm
</subsectionHeader>
<bodyText confidence="0.998021909090909">
MEAD decides which sentences to include in the
extract by ranking them according to a set of
parameters. The input to MEAD is a cluster of
articles (e.g., extracted by CIDR) and a value for the
compression rate r. For example, if the cluster
contains a total of 50 sentences (n = 50) and the
value of r is 20%, the output of MEAD will contain
10 sentences. Sentences are laid in the same order as
they appear in the original documents with
documents ordered chronologically. We benefit here
from the time stamps associated with each document.
</bodyText>
<subsubsectionHeader confidence="0.682639">
SCORE (s) =Ij (w,Ci wpPi wiFd
</subsubsectionHeader>
<bodyText confidence="0.5666085">
where i (/ n) is the sentence number within
the cluster.
INPUT: Cluster of d documents 3 with n sentences
(compression rate = r)
3 Note that currently, MEAD requires that sentence
boundaries be marked.
</bodyText>
<page confidence="0.990548">
23
</page>
<subsubsectionHeader confidence="0.930668">
4.2.3 System performance (S)
</subsubsectionHeader>
<bodyText confidence="0.999917">
The system performance S is one of the numbers6
described in the previous subsection. For {13), the
value of S is 0.627 (which is lower than random). For
{14}, S is 0.833, which is between R and J. In the
example, only two of the six possible sentence
selections, {14) and {24} are between R and J. Three
others, {13}, (231, and {34) are below R. while {12}
is better than J.
</bodyText>
<subsubsectionHeader confidence="0.918256">
4.2.4. Normalized system performance (D)
</subsubsectionHeader>
<bodyText confidence="0.9948234">
To restrict system performance (mostly) between 0
and I, we use a mapping between R and J in such a
way that when S = R, the normalized system
performance, D, is equal to 0 and when S = J, D
becomes I. The corresponding linear function7 is:
</bodyText>
<equation confidence="0.90706">
D = (S-R)/(J-R)
</equation>
<bodyText confidence="0.944992142857143">
Figure 2 shows the mapping •between system
performance S on the left (a) and normalized system
performance D on the right (b). A small part of the 0-
I segment is mapped to the entire 0-1 segment;
therefore the difference between two systems,
performing at e.g., 0.785 and 0.812 can be
significant!
</bodyText>
<figure confidence="0.990569333333333">
0.5
OD
(a)
</figure>
<figureCaption confidence="0.999651">
Figure 2: Performance mapping
</figureCaption>
<bodyText confidence="0.9619128">
Example: the normalized system performance for the
{14) system then becomes (0.833 - 0.732) / (0.841 —
0.732) or 0.927. Since the score is close to 1, the
{14) system is almost as good as the interjudge
agreement. The normalized system performance for
the {24} system is similarly (0.837 — 0.732) / (0.841
7 The formula is valid when J &gt; R (that is, the judges
agree among each other better than randomly).
— 0.732) or 0.963. Of the two systems, {24}
outperforms {14).
</bodyText>
<subsectionHeader confidence="0.983307">
4.3 Using CSIS to evaluate multi-document
summaries
</subsectionHeader>
<bodyText confidence="0.999754909090909">
To use CSIS in the evaluation, we introduce a new
parameter, E, which tells us how much to penalize a
system that includes redundant information. In the
example from Table 7 (arrows indicate subsumption),
a summarizer with r = 20% needs to pick 2 out of 12
sentences. Suppose that it picks 1/1 and 2/1 (in bold).
If E = 1, it should get full credit of 20 utility points. If
E = 0, it should get no credit for the second sentence
as it is subsumed by the first sentence. By varying E
between 0 and I, the evaluation may favor or ignore
subsumption.
</bodyText>
<table confidence="0.996325333333333">
Article] Article2 Article3
Sentl 10 --1•10 5
Sent2 8 9 8
Sent3 2 3 4
Sent4 5 6 9
•
</table>
<tableCaption confidence="0.998059">
Table 7: Sample subsumption table (12 sentences,
</tableCaption>
<sectionHeader confidence="0.813529" genericHeader="method">
3 articles)
</sectionHeader>
<subsectionHeader confidence="0.525066">
5 User studies and system evaluation
</subsectionHeader>
<bodyText confidence="0.9999545">
We ran two user experiments. First, six judges were
each given six clusters and asked to ascribe an
importance score from 0 to 10 to each sentence
within a particular cluster. Next, five judges had to
indicate for each sentence which other sentence(s), if
any, it subsumes 8.
</bodyText>
<subsectionHeader confidence="0.775567">
5.1 CBSU: interjudge agreement
</subsectionHeader>
<bodyText confidence="0.999002846153846">
Using the techniques described in Section 0, we
computed the cross-judge agreement (J) for the 6
clusters for various r (Figure 3). Overall, interjudge
agreement was quite high. An interesting drop in
interjudge agreement occurs for 20-30% summaries.
The drop most likely results from the fact that 10%
summaries are typically easier to produce because the
few most important sentences in a cluster are easier
to identify.
8 We should note that both annotation tasks were
quite time consuming and frustrating for the users
who took anywhere from 6 to 10 hours each to
complete their part.
</bodyText>
<figure confidence="0.929134">
r-i.o
y-ogn-D
—05
26
006.41
SO 10
</figure>
<figureCaption confidence="0.9981365">
Figure 3: Cross-judge agreement (J) on the CBSU
annotation task.
</figureCaption>
<subsectionHeader confidence="0.925233">
5.2 CSIS: interjudge agreement
</subsectionHeader>
<bodyText confidence="0.980631">
In the second experiment, we asked users to indicate
all cases when within a cluster, a sentence is
subsumed by another. The judges&apos; data on the first
seven sentences of cluster A are shown in Table 8.
The &amp;quot;-F score&amp;quot; indicates the number of judges who
agree on the most frequent subsumption. The t
score&amp;quot; indicates that the consensus was no
subsumption. We found relatively low interjudge
agreement on the cases in which at least one judge
indicated evidence of subsumption. Overall, out of
558 sentences, there was full agreement (5 judges) on
292 sentences (Table 9). Unfortunately, h 291 of
these 292 sentences the agreement was that there is
no subsumption. When the bar of agreement was
lowered to four judges, 23 out of 406 agreements are
on sentences with subsumption. Overall, out of 80
</bodyText>
<figureCaption confidence="0.690546">
sentences with subsumption, only 24 had an
agreement of four or more judges. However, in 54
cases at least three judges agreed on the presence of a
particular instance of subsumption.
</figureCaption>
<table confidence="0.999722">
Sentence Judge I Judge2 Judge3 Judge4 Judge5 + score -score
A1-1 A2-1 A2-1 A2-1 3
A1-2 A2-5 A2-5 A2-5 3
A1-3 A2-10 4
A1-4 A2-10 A2-10 A2-10 A2-10 4
A1-5 A2-1 A2-2 A2-4 2
A1-6 A2-7 4
A1-7 A2-8 4
</table>
<tableCaption confidence="0.985556">
Table 8 Judges&apos; indication for subsumption for the first seven sentences in cluster A
</tableCaption>
<table confidence="0.999364333333333">
Cluster A Cluster B Cluster C Cluster D Cluster E Cluster F
# judges agreeinz + - + - + - + - + - + -
5 0 7 0 24 0 45 0 88 1 73 0 61
4 1 6 3 6 1 10 9 37 8 35 0 11
3 3 6 4 5 4 4 28 20 5 23 3 7
2 1 1 2 1 1 0 7 0 7. 0 1
</table>
<tableCaption confidence="0.998714">
Table 9: Interjudge
</tableCaption>
<bodyText confidence="0.999902">
In conclusion, we found very high interjudge
agreement in the first experiment and moderately
low agreement in the second experiment. We
concede that the time necessary to do a proper job
at the second task is partly to blame.
</bodyText>
<subsectionHeader confidence="0.979119">
53 Evaluation of MEAD
</subsectionHeader>
<bodyText confidence="0.984369333333333">
Since the baseline of random sentence selection is
already included in the evaluation formulae, we
used the Lead-based method (selecting the
</bodyText>
<subsectionHeader confidence="0.961495">
CSIS agreement
</subsectionHeader>
<bodyText confidence="0.998723222222222">
positionally first (n*r/c) sentences from each cluster
where c = number of clusters) as the baseline to
evaluate our system.
In Table 10 we show the normalized performance
(D) of MEAD, for the six clusters at nine
compression rates. MEAD performed better than
Lead in 29 (in bold) out of 54 cases. Note that for
the largest cluster, Cluster D, MEAD outperformed
Lead at all compression rates.
</bodyText>
<page confidence="0.994263">
27
</page>
<table confidence="0.970716625">
10% 20% 30% 40% 50% 60% 70% 80% 90%
Cluster A 0.855 0.572 0.427 0.759 0.862 0.910 0.554 1.001 0.584
Cluster B 0365 0.402 0.690 0.714 0.867 0.640 0.845 0.713 1.317
Cluster C 0.753 0.938 0.841 1.029 0.751 0.819 0.595 0.611 0.683
Cluster D 0.739 0.764 0.683 0.723 0.614 0.568 0.668 0.719 1.100
Cluster E 1.083 0.937 0.581 0373 0.438 0.369 0A29 0.487 0.261
Cluster F 1.064 0.893 0.928 1.000 0.732 0.805 0.910 0.689 0.199
performance (D) of MEAD
</table>
<bodyText confidence="0.952867904761905">
showed how MEAD&apos;s sentence scoring weights can
- be modified to produce summaries significantly
better than the alternatives.
We also looked at a property of multi-document
clusters, namely cross-sentence information
subsumption (which is related to the MMR metric
proposed in [Carbonell and Goldstein, 1998]) and
showed how it can be used in evaluating multi-
document summaries.
All our findings are backed by the analysis of two
experiments that we performed with human subjects.
We found that the interjudge agreement on sentence
utility is very high while the agreement on cross-
sentence subsumption is moderately low, ahhough
promising.
In the future, we would like to test our
multidocument summarizer on a larger corpus and
improve the summarization algorithm. We would
also like to explore how the techniques we proposed
here can be used for multiligual multidocument
summarization.
</bodyText>
<tableCaption confidence="0.909054">
Table 10: Normalized
</tableCaption>
<bodyText confidence="0.9999524">
We then modified the MEAD algorithm to include
lead information as well as centroids (see Section 0).
In this case, MEAD+Lead performed better than the
Lead baseline in 41 cases. We are in the process of
running experiments with other SCORE formulas.
</bodyText>
<subsectionHeader confidence="0.94961">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.9999934">
It may seem that utility-based evaluation requires too
much effort and is prone to low interjudge agreement.
We believe that our results show that interjudge
agreement is quite high. As far as the amount of
effort required, we believe that the larger effort on
the part of the judges is more or less compensated
with the ability to evaluate summaries off-line and at
variable compression rates. Alternative evaluations
don&apos;t make such evaluations possible. We should
concede that a utility-based approach is probably not
feasible for query-based summaries as these are
typically done only on-line.
We discussed the possibility of a sentence
contributing negatively to the utility of another
sentence due to redundancy. We should also point out
that sentences can also reinforce one another
positively. For example, if a sentence mentioning a
new entity is included in a summary, one might also
want to include a sentence that puts the entity in the
context of the reit of the article or cluster.
</bodyText>
<sectionHeader confidence="0.987195" genericHeader="method">
6 Contributions and future work
</sectionHeader>
<bodyText confidence="0.999987916666667">
We presented a new multi-document summarizer,
MEAD. It summarizes clusters of news articles
automatically grouped by a topic detection system.
MEAD uses information from the centroids of the
clusters to select sentences that are most likely to be
relevant to the cluster topic.
We used a new utility-based technique, CBSU, for
the evaluation of MEAD and of summarizers in
general. We found that MEAD produces summaries
that are similar in quality to the ones produced by
humans. We also compared MEAD&apos;s performance to
an alternative method, multi-document lead, and
</bodyText>
<sectionHeader confidence="0.998945" genericHeader="evaluation">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974666666667">
We would like to thank Inderjeet Mani, Wlodek
Zadrozny, Rie Kubota Ando, Joyce Chai, and Nanda
Kambhatla for their valuable feedback. We would
also like to thank Carl Sable, Min-Yen Kan, Dave
Evans, Adam Budzikowski, and Veronika Horvath
for their help with the evaluation.
</bodyText>
<sectionHeader confidence="0.997342" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.886647857142857">
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang, Topic
detection and tracking pilot study: final report, In
Proceedings of the Broadcast News Understanding
and Transcription Workshop, 1998.
Jaime Carbonell and Jade Goldstein. The use of
AMR, diversity-based reranking for reordering
</reference>
<page confidence="0.992078">
28
</page>
<reference confidence="0.998287681818182">
documents and producing summaries. In
Proceedings of ACM -SIGIR&apos;98, Melbourne,
Australia, August 1998.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell, Summarizing Text Documents:
Sentence Selection and Evaluation Metrics, In
Proceedings of ACM-SIGIR&apos;99, Berkeley, CA,
August 1999.
Therese Hand. A Proposal for Task-Based Evaluation
of Text Summarization Systems, in Mani, I., and
Maybury, M., eds., Proceedings of the
ACL/EACL&apos;97 Workshop on Intelligent Scalable
Text Summarization, Madrid, Spain, July 1997.
Hongyan Jing, Regina Barzilay, Kathleen McKeown,
and Michael Elhadad, Summarization Evaluation
Methods: Experiments and Analysis, In Working
Notes, AAAI Spring Symposium on Intelligent
Text Summarization, Stanford, CA, April 1998.
Inderjeet Mani and Eric Bloedom, Summarizing
Similarities and Differences Among Related
Documents, Information Retrieval 1 (1-2), pages
35-67, June 1999.
Inderjeet Mani, David House, Gary Klein, Lynette
Hirschman, Leo Orbst, Therese Firmin, Michael
Chrzanowski, and Beth Sundheim The TIPSTER
SUMMAC text summarization evaluation.
Technical Report MTR98W0000138, MITRE,
McLean, Virginia, October 1998.
Inderjeet Mani and Mark Maybury. Advances in
Automatic Text Summarization. MIT Press, 1999.
Kathleen McKeown, Judith Klavans, Vasileios
Hatzivassiloglou, Regina Barzilay, and Eleazar
Eskin, Towards Multidocument Summarization by
Reformulation: Progress and Prospects, in
Proceedings of AAA1&apos;99, Orlando, FL, July 1999.
Dragomir R. Radev and Kathleen McKeown.
Generating natural language summaries from
multiple on-line sources. Computational
Linguistics, 24 (3), pages 469-500, September
1998.
Dragomir R. Radev, Vasileios Hatzivassiloglou, and
Kathleen R. McKeown. A description of the CIDR
system as used for TDT-2. In DARPA Broadcast
News Workshop, Herndon, VA, February 1999.
</reference>
<page confidence="0.999617">
29
</page>
<sectionHeader confidence="0.997687" genericHeader="references">
Appendix
</sectionHeader>
<reference confidence="0.998843078125">
ARTICLE 18853: ALGIERS, May 20 (AFP)
ARTICLE 18854: ALGIERS, May 20 (UPI)
I. Eighteen decapitated bodies have been found in a
mass grave in northern Algeria, press reports said
Thursday, adding that two shepherds were murdered
earlier this week.
2. Security forces found the mass grave on Wednesday
at Chbika, near Djelfa, 275 kilometers (170 miles)
south of the capital
3. It contained the bodies of people killed last year
during a wedding ceremony, according to Le Quotidien
Liberte.
4. The victims included women, children and old men.
5. Most of them had been decapitated and their heads
thrown on a road, reported the Es Sahafa.
6. Another mass grave containing the bodies of around
10 people was discovered recently near Algiers, in the
Eucalyptus district.
7. The two shepherds were killed Monday evening by a
group of nine armed Islamists near the Moulay Slissen
forest.
8. After being injured in a hail of automatic weapons
fire, the pair were finished off with machete blows
before being decapitated, Le Quotidien d&apos;Oran reported.
9. Seven people, six of them children, were killed and
two injured Wednesday by armed Islamists near
Medea, 120 kilometers (75 miles) south of Algiers,
security forces said.
10. The same day a parcel bomb explosion injured 17
people in Algiers itself.
11. Since early March, violence linked to armed
Islamists has claimed more than 500 lives, according to
press tallies.
I. Algerian newspapers have reported that 18
decapitated bodies have been found by authorities
in the south of the country.
2. Police found the &amp;quot;decapitated bodies of women,
children and old men,with their heads thrown on a
road&amp;quot; near the town ofJelfa, 275 kilometers (170
- miles) south of the capital Algiers.
3. In another incident on Wednesday, seven people
-- including six children -- were killed by terrorists,
Algerian security forces said.
4. Extremist Muslim militants were responsible for
the slaughter of the seven people in the province of
Medea, 120 kilometers (74 miles) south of Algiers.
5. The killers also kidnapped three girls during the
same attack, authorities said, and one of the girls
was found wounded on a nearby road.
6. Meanwhile, the Algerian daily Le Matin today
quoted Interior Minister Abdul Malik Silal as
saying that &amp;quot;terrorism has not been eradicated, but
the movement of the terrorists has significantly
declined.&amp;quot;
7. Algerian violence has claimed the lives of more
than 70,000 people since the army cancelled the
1992 general elections that Islamic parties were
likely to win.
8. Mainstream Islamic groups, most of which are
banned in the country, insist their members are not
responsible for the violence against civilians.
9. Some Muslim groups have blamed the army,
while others accuse &amp;quot;foreign elements conspiring
against Algeria.&amp;quot;
</reference>
<page confidence="0.998806">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.407379">
<title confidence="0.952919">Centroid-based summarization of multiple documents: extraction, utility-based evaluation, and user studies</title>
<author confidence="0.99989">Dragomir R Radev</author>
<affiliation confidence="0.9999575">School of Information University of Michigan</affiliation>
<address confidence="0.999282">Ann Arbor, MI 48103</address>
<email confidence="0.999747">radev@umich.edu</email>
<author confidence="0.934629">Hongyan</author>
<affiliation confidence="0.999851">Department of Computer</affiliation>
<address confidence="0.836596">Columbia New York, NY</address>
<email confidence="0.998897">hjing@cs.columbia.edu</email>
<author confidence="0.774365">Malgorzata</author>
<affiliation confidence="0.993564">IBM TJ Watson Research</affiliation>
<address confidence="0.98947">30 Saw Mill River Hawthorne, NY</address>
<email confidence="0.981513">smI@us.ibm.com</email>
<abstract confidence="0.994110545454545">We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jaime Carbonell</author>
<author>George Doddington</author>
<author>Jonathan Yamron</author>
<author>Yiming Yang</author>
</authors>
<title>Topic detection and tracking pilot study: final report,</title>
<date>1998</date>
<booktitle>In Proceedings of the Broadcast News Understanding and Transcription Workshop,</booktitle>
<contexts>
<context position="4396" citStr="Allan et al. 1998" startWordPosition="667" endWordPosition="670">g, maximal marginal relevance, or language generation. Finally, evaluation of multi-document summaries is a difficult problem. There is not yet a widely accepted evaluation scheme. We propose a utility-based evaluation scheme, which can be used to evaluate both single-document and multi-document summaries. 2 Informational content of sentences 2.1 Cluster-based sentence utility (CBSU) Cluster-based sentence utility (CBSU, or utility) refers to the degree of relevance (from 0 to 10) of a &apos; particular sentence to the general topic of the entire cluster (for a dis cussion of what is a topic, see [Allan et al. 1998]). A utility of 0 means that the sentence is not relevant to the cluster and a 10 marks an essential sentence. 2.2 Cross-sentence informational subsumption (CSIS) A related notion to CBSU is cross-sentence informational subsumption (CSIS, or subsumption), which reflects that certain sentences repeat some of the information present in other sentences and may, therefore, be omitted during summarization. If the information content of sentence a (denoted as i(a)) is contained within sentence b, then a becomes informationally redundant and the content of b is said to subsume that of a: i(a) c i(b)</context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>James Allan, Jaime Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang, Topic detection and tracking pilot study: final report, In Proceedings of the Broadcast News Understanding and Transcription Workshop, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of AMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of ACM -SIGIR&apos;98,</booktitle>
<location>Melbourne, Australia,</location>
<contexts>
<context position="3684" citStr="Carbonell and Goldstein, 1998" startWordPosition="557" endWordPosition="560"> that support our findings, and an evaluation of MEAD. An event cluster, produced by a TDT system, consists of chronologically ordered news articles from multiple sources, which describe an event as it develops over time. Event clusters range from2 to 10 documents from which MEAD produces summaries in the form of sentence extracts. A key feature of MEAD is its use of cluster centroids, which consist of words which are central not only to one article in a cluster, but to all the articles. MEAD is significantly different from previous work on multi-document summarization [Radev &amp; McKeown, 1998; Carbonell and Goldstein, 1998; Mani and Bloedorn, 1999; McKeown et al., 1999], 21 which use techniques such as graph matching, maximal marginal relevance, or language generation. Finally, evaluation of multi-document summaries is a difficult problem. There is not yet a widely accepted evaluation scheme. We propose a utility-based evaluation scheme, which can be used to evaluate both single-document and multi-document summaries. 2 Informational content of sentences 2.1 Cluster-based sentence utility (CBSU) Cluster-based sentence utility (CBSU, or utility) refers to the degree of relevance (from 0 to 10) of a &apos; particular s</context>
<context position="6897" citStr="Carbonell and Goldstein, 1998" startWordPosition="1082" endWordPosition="1085">y can be substituted for each other without crucial loss of information and therefore belong to the same equivalence class, i.e. i(3) c 1(4) and i(4) c i(3). In the user study section we will take a look at the way humans perceive CSIS and equivalence class. (3) Eighteen decapitated bodies have been found in a mass grave in northern Algeria, press reports said Thursday. (4) Algerian newspapers have reported on Thursday that 18 decapitated bodies have been found by the authorities. 2.4 Comparison with MMR Maximal marginal relevance (or MMR) is a technique similar to CSIS and was introduced in [Carbonell and Goldstein, 1998]. In that paper, MMR is used to produce summaries of single documents that avoid redundancy. The authors mention that their preliminary results indicate that multiple documents on the same topic also contain redundancy but they fall short of using MMR for multi-document summarization. Their metric is used as an enhancement to a query-based summary whereas CSIS is designed for query-independent (a.k.a., generic) summaries. 22 We now describe the corpus used for the evaluation of MEAD, and later in this section we present MEAD&apos;s algorithm. 3 MEAD: a centroid-based multidocument summarizer Clust</context>
<context position="17147" citStr="Carbonell and Goldstein, 1998" startWordPosition="2887" endWordPosition="2890">2 0.690 0.714 0.867 0.640 0.845 0.713 1.317 Cluster C 0.753 0.938 0.841 1.029 0.751 0.819 0.595 0.611 0.683 Cluster D 0.739 0.764 0.683 0.723 0.614 0.568 0.668 0.719 1.100 Cluster E 1.083 0.937 0.581 0373 0.438 0.369 0A29 0.487 0.261 Cluster F 1.064 0.893 0.928 1.000 0.732 0.805 0.910 0.689 0.199 performance (D) of MEAD showed how MEAD&apos;s sentence scoring weights can - be modified to produce summaries significantly better than the alternatives. We also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in [Carbonell and Goldstein, 1998]) and showed how it can be used in evaluating multidocument summaries. All our findings are backed by the analysis of two experiments that we performed with human subjects. We found that the interjudge agreement on sentence utility is very high while the agreement on crosssentence subsumption is moderately low, ahhough promising. In the future, we would like to test our multidocument summarizer on a larger corpus and improve the summarization algorithm. We would also like to explore how the techniques we proposed here can be used for multiligual multidocument summarization. Table 10: Normaliz</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. The use of AMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of ACM -SIGIR&apos;98, Melbourne, Australia, August 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
</authors>
<title>Summarizing Text Documents: Sentence Selection and Evaluation Metrics,</title>
<date>1999</date>
<booktitle>In Proceedings of ACM-SIGIR&apos;99,</booktitle>
<location>Berkeley, CA,</location>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and Jaime Carbonell, Summarizing Text Documents: Sentence Selection and Evaluation Metrics, In Proceedings of ACM-SIGIR&apos;99, Berkeley, CA, August 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Therese Hand</author>
</authors>
<title>A Proposal for Task-Based Evaluation of Text Summarization Systems,</title>
<date>1997</date>
<booktitle>Proceedings of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<editor>in Mani, I., and Maybury, M., eds.,</editor>
<location>Madrid, Spain,</location>
<marker>Hand, 1997</marker>
<rawString>Therese Hand. A Proposal for Task-Based Evaluation of Text Summarization Systems, in Mani, I., and Maybury, M., eds., Proceedings of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization, Madrid, Spain, July 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Summarization Evaluation Methods: Experiments and Analysis,</title>
<date>1998</date>
<booktitle>In Working Notes, AAAI Spring Symposium on Intelligent Text Summarization,</booktitle>
<location>Stanford, CA,</location>
<marker>Jing, Barzilay, McKeown, Elhadad, 1998</marker>
<rawString>Hongyan Jing, Regina Barzilay, Kathleen McKeown, and Michael Elhadad, Summarization Evaluation Methods: Experiments and Analysis, In Working Notes, AAAI Spring Symposium on Intelligent Text Summarization, Stanford, CA, April 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Eric Bloedom</author>
</authors>
<title>Summarizing Similarities and Differences Among Related Documents,</title>
<date>1999</date>
<journal>Information Retrieval</journal>
<volume>1</volume>
<pages>1--2</pages>
<marker>Mani, Bloedom, 1999</marker>
<rawString>Inderjeet Mani and Eric Bloedom, Summarizing Similarities and Differences Among Related Documents, Information Retrieval 1 (1-2), pages 35-67, June 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>David House</author>
<author>Gary Klein</author>
<author>Lynette Hirschman</author>
<author>Leo Orbst</author>
<author>Therese Firmin</author>
<author>Michael Chrzanowski</author>
</authors>
<title>and Beth Sundheim The TIPSTER SUMMAC text summarization evaluation.</title>
<date>1998</date>
<tech>Technical Report MTR98W0000138,</tech>
<location>MITRE, McLean, Virginia,</location>
<marker>Mani, House, Klein, Hirschman, Orbst, Firmin, Chrzanowski, 1998</marker>
<rawString>Inderjeet Mani, David House, Gary Klein, Lynette Hirschman, Leo Orbst, Therese Firmin, Michael Chrzanowski, and Beth Sundheim The TIPSTER SUMMAC text summarization evaluation. Technical Report MTR98W0000138, MITRE, McLean, Virginia, October 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Mark Maybury</author>
</authors>
<title>Advances in Automatic Text Summarization.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<marker>Mani, Maybury, 1999</marker>
<rawString>Inderjeet Mani and Mark Maybury. Advances in Automatic Text Summarization. MIT Press, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
</authors>
<title>Judith Klavans, Vasileios Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin, Towards Multidocument Summarization by Reformulation: Progress and Prospects,</title>
<date>1999</date>
<booktitle>in Proceedings of AAA1&apos;99,</booktitle>
<location>Orlando, FL,</location>
<marker>McKeown, 1999</marker>
<rawString>Kathleen McKeown, Judith Klavans, Vasileios Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin, Towards Multidocument Summarization by Reformulation: Progress and Prospects, in Proceedings of AAA1&apos;99, Orlando, FL, July 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Kathleen McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<pages>469--500</pages>
<contexts>
<context position="3653" citStr="Radev &amp; McKeown, 1998" startWordPosition="553" endWordPosition="556">aries, two user studies that support our findings, and an evaluation of MEAD. An event cluster, produced by a TDT system, consists of chronologically ordered news articles from multiple sources, which describe an event as it develops over time. Event clusters range from2 to 10 documents from which MEAD produces summaries in the form of sentence extracts. A key feature of MEAD is its use of cluster centroids, which consist of words which are central not only to one article in a cluster, but to all the articles. MEAD is significantly different from previous work on multi-document summarization [Radev &amp; McKeown, 1998; Carbonell and Goldstein, 1998; Mani and Bloedorn, 1999; McKeown et al., 1999], 21 which use techniques such as graph matching, maximal marginal relevance, or language generation. Finally, evaluation of multi-document summaries is a difficult problem. There is not yet a widely accepted evaluation scheme. We propose a utility-based evaluation scheme, which can be used to evaluate both single-document and multi-document summaries. 2 Informational content of sentences 2.1 Cluster-based sentence utility (CBSU) Cluster-based sentence utility (CBSU, or utility) refers to the degree of relevance (fr</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>Dragomir R. Radev and Kathleen McKeown. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24 (3), pages 469-500, September 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>A description of the CIDR system as used for TDT-2.</title>
<date>1999</date>
<booktitle>In DARPA Broadcast News Workshop,</booktitle>
<location>Herndon, VA,</location>
<contexts>
<context position="2237" citStr="Radev et al., 1999" startWordPosition="334" endWordPosition="337">ol over Pakistan. The paragraph above summarizes a large amount of news from different sources. While it was not automatically generated, one can imagine the use of such automatically generated summaries. In this paper we will describe how multi-document summaries are built and evaluated. 1.1 Topic detection and multi-document summarization The process of identifying all articles on an emerging event is called Topic Detection and Tracking (TDT). A large body of research in TDT has been created over the past two years [Allan et al., 98]. We will present an extension of our own research on TDT [Radev et al., 1999] to cover summarization of multidocument clusters. Our entry in the official TDT evaluation, called CIDR [Radev et al., 1999], uses modified TF*IDF to produce clusters of news articles on the same event. We developed a new technique for multi-document summarization (or MDS), called centroid-based summarization (CBS) which uses as input the centroids of the clusters produced by C1DR to identify which sentences are central to the topic of the cluster, rather than the individual articles. We have implemented CBS in a system, named MEAD. The main contributions of this paper are: the development o</context>
<context position="8989" citStr="Radev et al., 1999" startWordPosition="1427" endWordPosition="1430"> TDT corpus2. Among the factors for our selection of clusters are: coverage of as many news sources as possible, coverage of both TDT and non-TDT data, coverage of different types of news (e.g., terrorism, internal affairs, and environment), and diversity in cluster sizes (in our case, from 2 to 10 articles). The test corpus is used in the evaluation in such a way that each cluster is summarized at 9 different compression rates, thus giving nine times as many sample points as one would expect from the size of the corpus. 3.2 Cluster centroids Table 2 shows a sample centroid, produced by CIDR [Radev et al., 1999] from cluster A. The &amp;quot;count&amp;quot; column indicates the average number of occurrences of a word *across the entire cluster. The IDF values were computed from the TDT corpus. A centroid, in this context, is a pseudo-document which consists of words which have Count*IDF scores above a predefined threshold in the documents that constitute the cluster. CIDR computes Count*IDF in an iterative fashion, updating its values as more articles are inserted in a given cluster. We hypothesize that sentences that contain the words from the centroid are more indicative of the topic of the cluster. 2 The selection</context>
</contexts>
<marker>Radev, Hatzivassiloglou, McKeown, 1999</marker>
<rawString>Dragomir R. Radev, Vasileios Hatzivassiloglou, and Kathleen R. McKeown. A description of the CIDR system as used for TDT-2. In DARPA Broadcast News Workshop, Herndon, VA, February 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ARTICLE</author>
</authors>
<title>(AFP) ARTICLE 18854: ALGIERS,</title>
<date></date>
<marker>ARTICLE, </marker>
<rawString>ARTICLE 18853: ALGIERS, May 20 (AFP) ARTICLE 18854: ALGIERS, May 20 (UPI) I. Eighteen decapitated bodies have been found in a mass grave in northern Algeria, press reports said Thursday, adding that two shepherds were murdered earlier this week.</rawString>
</citation>
<citation valid="false">
<title>Security forces found the mass grave on Wednesday at Chbika, near Djelfa, 275 kilometers (170 miles) south of the capital 3. It contained the bodies of people killed last year during a wedding ceremony, according to Le Quotidien Liberte.</title>
<marker></marker>
<rawString>2. Security forces found the mass grave on Wednesday at Chbika, near Djelfa, 275 kilometers (170 miles) south of the capital 3. It contained the bodies of people killed last year during a wedding ceremony, according to Le Quotidien Liberte.</rawString>
</citation>
<citation valid="false">
<title>The victims included women, children and old men. 5. Most of them had been decapitated and their heads thrown on a road, reported the Es Sahafa. 6. Another mass grave containing the bodies of around 10 people was discovered recently near Algiers, in the Eucalyptus district.</title>
<marker></marker>
<rawString>4. The victims included women, children and old men. 5. Most of them had been decapitated and their heads thrown on a road, reported the Es Sahafa. 6. Another mass grave containing the bodies of around 10 people was discovered recently near Algiers, in the Eucalyptus district.</rawString>
</citation>
<citation valid="false">
<title>The two shepherds were killed Monday evening by a group of nine armed Islamists near the Moulay Slissen forest.</title>
<marker></marker>
<rawString>7. The two shepherds were killed Monday evening by a group of nine armed Islamists near the Moulay Slissen forest.</rawString>
</citation>
<citation valid="false">
<title>After being injured in a hail of automatic weapons fire, the pair were finished off with machete blows before being decapitated, Le Quotidien d&apos;Oran reported. 9. Seven people, six of them children, were killed and two injured Wednesday by armed Islamists near Medea, 120 kilometers (75 miles) south of Algiers, security forces said. 10. The same day a parcel bomb explosion injured 17 people in Algiers itself.</title>
<marker></marker>
<rawString>8. After being injured in a hail of automatic weapons fire, the pair were finished off with machete blows before being decapitated, Le Quotidien d&apos;Oran reported. 9. Seven people, six of them children, were killed and two injured Wednesday by armed Islamists near Medea, 120 kilometers (75 miles) south of Algiers, security forces said. 10. The same day a parcel bomb explosion injured 17 people in Algiers itself.</rawString>
</citation>
<citation valid="false">
<title>Since early March, violence linked to armed Islamists has claimed more than 500 lives, according to press tallies.</title>
<marker></marker>
<rawString>11. Since early March, violence linked to armed Islamists has claimed more than 500 lives, according to press tallies.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I</author>
</authors>
<title>Algerian newspapers have reported that 18 decapitated bodies have been found by authorities in the south of the country.</title>
<marker>I, </marker>
<rawString>I. Algerian newspapers have reported that 18 decapitated bodies have been found by authorities in the south of the country.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Police</author>
</authors>
<title>found the &amp;quot;decapitated bodies of women, children and old men,with their heads thrown on a road&amp;quot; near the town ofJelfa, 275 kilometers</title>
<date></date>
<marker>Police, </marker>
<rawString>2. Police found the &amp;quot;decapitated bodies of women, children and old men,with their heads thrown on a road&amp;quot; near the town ofJelfa, 275 kilometers (170</rawString>
</citation>
<citation valid="false">
<title>south of the capital Algiers. 3. In another incident on Wednesday, seven people -- including six children -- were killed by terrorists, Algerian security forces said.</title>
<marker></marker>
<rawString>- miles) south of the capital Algiers. 3. In another incident on Wednesday, seven people -- including six children -- were killed by terrorists, Algerian security forces said.</rawString>
</citation>
<citation valid="false">
<title>Extremist Muslim militants were responsible for the slaughter of the seven people in the province of Medea, 120 kilometers (74 miles) south of Algiers. 5. The killers also kidnapped three girls during the same attack, authorities said, and one of the girls was found wounded on a nearby road.</title>
<marker></marker>
<rawString>4. Extremist Muslim militants were responsible for the slaughter of the seven people in the province of Medea, 120 kilometers (74 miles) south of Algiers. 5. The killers also kidnapped three girls during the same attack, authorities said, and one of the girls was found wounded on a nearby road.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Meanwhile</author>
</authors>
<title>the Algerian daily Le Matin today quoted Interior Minister Abdul Malik Silal as saying that &amp;quot;terrorism has not been eradicated, but the movement of the terrorists has significantly declined.&amp;quot;</title>
<marker>Meanwhile, </marker>
<rawString>6. Meanwhile, the Algerian daily Le Matin today quoted Interior Minister Abdul Malik Silal as saying that &amp;quot;terrorism has not been eradicated, but the movement of the terrorists has significantly declined.&amp;quot;</rawString>
</citation>
<citation valid="false">
<title>Algerian violence has claimed the lives of more than 70,000 people since the army cancelled the 1992 general elections that Islamic parties were likely to win.</title>
<marker></marker>
<rawString>7. Algerian violence has claimed the lives of more than 70,000 people since the army cancelled the 1992 general elections that Islamic parties were likely to win.</rawString>
</citation>
<citation valid="false">
<title>Mainstream Islamic groups, most of which are banned in the country, insist their members are not responsible for the violence against civilians. 9. Some Muslim groups have blamed the army, while others accuse &amp;quot;foreign elements conspiring against Algeria.&amp;quot;</title>
<marker></marker>
<rawString>8. Mainstream Islamic groups, most of which are banned in the country, insist their members are not responsible for the violence against civilians. 9. Some Muslim groups have blamed the army, while others accuse &amp;quot;foreign elements conspiring against Algeria.&amp;quot;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>