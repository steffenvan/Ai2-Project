<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000371">
<title confidence="0.593912">
Comparing Canonicalizations of Historical German Text
</title>
<author confidence="0.661352">
Bryan Jurish
</author>
<affiliation confidence="0.598729">
Berlin-Brandenburg Academy of Sciences
</affiliation>
<address confidence="0.513007">
Berlin, Germany
</address>
<email confidence="0.934091">
jurish@bbaw.de
</email>
<sectionHeader confidence="0.993638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999684357142857">
Historical text presents numerous chal-
lenges for contemporary natural language
processing techniques. In particular, the
absence of consistent orthographic con-
ventions in historical text presents difficul-
ties for any system requiring reference to
a static lexicon accessed by orthographic
form. In this paper, we present three
methods for associating unknown histori-
cal word forms with synchronically active
canonical cognates and evaluate their per-
formance on an information retrieval task
over a manually annotated corpus of his-
torical German verse.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999868114285714">
Historical text presents numerous challenges for
contemporary natural language processing tech-
niques. In particular, the absence of consistent or-
thographic conventions in historical text presents
difficulties for any system requiring reference to a
fixed lexicon accessed by orthographic form, such
as document indexing systems (Sokirko, 2003;
Cafarella and Cutting, 2004), part-of-speech tag-
gers (DeRose, 1988; Brill, 1992; Schmid, 1994),
simple word stemmers (Lovins, 1968; Porter,
1980), or more sophisticated morphological ana-
lyzers (Geyken and Hanneforth, 2006; Clematide,
2008).
When adopting historical text into such a sys-
tem, one of the most crucial tasks is the associa-
tion of one or more extant equivalents with each
word of the input text: synchronically active types
which best represent the relevant features of the
input word. Which features are considered “rel-
evant” here depends on the application in ques-
tion: for a lemmatization task only the root lex-
eme is relevant, whereas syntactic parsing may
require additional morphosyntactic features. For
current purposes, extant equivalents are to be un-
derstood as canonical cognates, preserving both
the root(s) and morphosyntactic features of the as-
sociated historical form(s), which should suffice
(modulo major grammatical and/or lexical seman-
tic shifts) for most natural language processing
tasks.
In this paper, we present three methods for au-
tomatic discovery of extant canonical cognates
for historical German text, and evaluate their per-
formance on an information retrieval task over a
small gold-standard corpus.
</bodyText>
<sectionHeader confidence="0.999303" genericHeader="method">
2 Canonicalization Methods
</sectionHeader>
<bodyText confidence="0.993714466666667">
In this section, we present three methods for au-
tomatic discovery of extant canonical cognates
for historical German input: phonetic conflation
(Pho), Levenshtein edit distance (Lev), and a
heuristic rewrite transducer (rw). The various
methods are presented individually below, and
characterized in terms of the linguistic resources
required for their application. Formally, each
canonicalization method R is defined by a char-
acteristic conflation relation —R, a binary rela-
tion on the set A* of all strings over the finite
grapheme alphabet A. Prototypically, —R will be
a true equivalence relation, inducing a partitioning
of A* into equivalence classes or “conflation sets”
[w]R = {v ∈ A* : v —R w}.
</bodyText>
<subsectionHeader confidence="0.979964">
2.1 Phonetic Conflation
</subsectionHeader>
<bodyText confidence="0.999987777777778">
If we assume despite the lack of consistent or-
thographic conventions that historical graphemic
forms were constructed to reflect phonetic forms,
and if the phonetic system of the target language
is diachronically more stable than the graphematic
system, then the phonetic form of a word should
provide a better clue to its extant cognates (if any)
than a historical graphemic form alone. Taken to-
gether, these assumptions lead to the canonicaliza-
</bodyText>
<page confidence="0.978398">
72
</page>
<bodyText confidence="0.95790819047619">
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 72–77,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
tion technique referred to here as phonetic confla-
tion.
In order to map graphemic forms to phonetic
forms, we may avail ourselves of previous work
in the realm of text-to-speech synthesis, a domain
in which the discovery of phonetic forms for ar-
bitrary text is an often-studied problem (Allen et
al., 1987; Dutoit, 1997), the so-called “letter-to-
sound” (LTS) conversion problem. The phonetic
conversion module used here was adapted from
the LTS rule-set distributed with the IMS German
Festival package (M¨ohler et al., 2001), and com-
piled as a finite-state transducer (Jurish, 2008).
In general, the phonetic conflation strategy
maps each (historical or extant) input word w E
A* to a unique phonetic form pho(w) by means of
a computable function pho : A* → P*,1 conflat-
ing those strings which share a common phonetic
form:
</bodyText>
<equation confidence="0.419376">
w —Pho v :⇔ pho(w) = pho(v) (1)
</equation>
<subsectionHeader confidence="0.99877">
2.2 Levenshtein Edit Distance
</subsectionHeader>
<bodyText confidence="0.971363259259259">
Although the phonetic conflation technique de-
scribed in the previous section is capable of suc-
cessfully identifying a number of common histor-
ical graphematic variation patterns such as ey/ei,
œ/¨o, th/t, and tz/z, it fails to conflate historical
forms with any extant equivalent whenever the
graphematic variation leads to non-identity of the
respective phonetic forms, as determined by the
LTS rule-set employed. In particular, whenever
a historical variation would effect a pronuncia-
tion difference in synchronic forms, that varia-
tion will remain uncaptured by a phonetic con-
flation technique. Examples of such phonetically
salient variations with respect to the simplified
IMS German Festival rule-set include guot/gut
“good”, liecht/licht “light”, tiuvel/teufel “devil”,
and wolln/wollen “want”.
In order to accommodate graphematic variation
phenomena beyond those for which strict pho-
netic identity of the variant forms obtains, we may
employ an approximate search strategy based on
the simple Levenshtein edit distance (Levenshtein,
1966; Navarro, 2001). Formally, let Lex C A*
be the lexicon of all extant forms, and let dLev :
A* x A* → N represent the Levenshtein distance
over grapheme strings, then define for every input
word w E A* the “best” synchronic equivalent
</bodyText>
<footnote confidence="0.948781">
1P is a finite phonetic alphabet.
</footnote>
<bodyText confidence="0.9938665">
bestLev(w) as the unique extant word v E Lex
with minimal edit-distance to the input word:2
</bodyText>
<equation confidence="0.9124745">
bestLev(w) = arg min dLev(w, v) (2)
vELex
</equation>
<bodyText confidence="0.99939625">
Ideally, the image of a word w under bestLev will
itself be the canonical cognate sought,3 leading to
conflation of all strings which share a common im-
age under bestLev:
</bodyText>
<equation confidence="0.512355">
w —Lev v :⇔ bestLev(w) = bestLev(v) (3)
</equation>
<bodyText confidence="0.999945416666667">
The function bestLev(w) : A* → Lex can be
computed using a variant of the Dijkstra algorithm
(Dijkstra, 1959) even when the lexicon is infinite
(as in the case of productive nominal composition
in German) whenever the set Lex can be repre-
sented by a finite-state acceptor (Mohri, 2002; Al-
lauzen and Mohri, 2009; Jurish, 2010). For current
purposes, we used the (infinite) input language
of the TAGH morphology transducer (Geyken and
Hanneforth, 2006) stripped of proper names, ab-
breviations, and foreign-language material to ap-
proximate Lex.
</bodyText>
<subsectionHeader confidence="0.99959">
2.3 Rewrite Transducer
</subsectionHeader>
<bodyText confidence="0.999987818181818">
While the simple edit distance conflation tech-
nique from the previous section is quite powerful
and requires for its implementation only a lexicon
of extant forms, the Levenshtein distance itself ap-
pears in many cases too coarse to function as a
reliable predictor of etymological relations, since
each edit operation (deletion, insertion, or substi-
tution) is assigned a cost independent of the char-
acters operated on and of the immediate context
in the strings under consideration. This operand-
independence of the traditional Levenshtein dis-
tance results in a number of spurious conflations
such as those given in Table 1.
In order to achieve a finer-grained and thus
more precise mapping from historical forms to ex-
tant canonical cognates while preserving some de-
gree of the robustness provided by the relaxation
of the strict identity criterion implicit in the edit-
distance conflation technique, a non-deterministic
weighted finite-state “rewrite” transducer was de-
veloped to replace the simple Levenshtein met-
ric. The rewrite transducer was compiled from a
</bodyText>
<footnote confidence="0.902015">
2We assume that whenever multiple extant minimal-
distance candidate forms exist, one is chosen randomly.
3Note here that every extant form is its own “best”
equivalent: w ∈ Lex implies bestLev(w) = w, since
dLev(w, w) = 0 &lt; dLev(w, v) for all v =� w.
</footnote>
<page confidence="0.99567">
73
</page>
<bodyText confidence="0.991808">
w bestLev(w) Extant Equivalent
aug aus “out” auge “eye”
faszt fast “almost” fasst “grabs”
ouch buch “book” auch “also”
ram rat “advice” rahm “cream”
vol volk “people” voll “full”
</bodyText>
<tableCaption confidence="0.9582655">
Table 1: Example spurious Levenshtein distance
conflations
</tableCaption>
<bodyText confidence="0.999300740740741">
heuristic two-level rule-set (Karttunen et al., 1987;
Kaplan and Kay, 1994; Laporte, 1997) whose 306
rules were manually constructed to reflect linguis-
tically plausible patterns of diachronic variation
as observed in the lemma-instance pairs automat-
ically extracted from the full 5.5 million word
DWB verse corpus (Jurish, 2008). In particu-
lar, phonetic phenomena such as schwa deletion,
vowel shift, voicing alternation, and articulatory
location shift are easily captured by such rules.
Of the 306 heuristic rewrite rules, 131 manipu-
late consonant-like strings, 115 deal with vowel-
like strings, and 14 operate directly on syllable-
like units. The remaining 46 rules define expan-
sions for explicitly marked elisions and unrecog-
nized input. Some examples of rules used by the
rewrite transducer are given in Table 2.
Formally, the rewrite transducer Arw defines
a pseudo-metric QArw� : A* x A* —* 1[8,,,, on
all string pairs (Mohri, 2009). Assuming the
non-negative tropical semiring (Simon, 1987) is
used to represent transducer weights, analagous to
the transducer representation of the Levenshtein
metric (Allauzen and Mohri, 2009), the rewrite
pseudo-metric can be used as a drop-in replace-
ment for the Levenshtein distance in Equations (2)
and (3), yielding Equations (4) and (5):
</bodyText>
<equation confidence="0.998178">
bestrw(w) = arg min QArw�(w, v) (4)
VELex
w —rw v :.&lt;---&gt; bestrw(w) = bestrw(v) (5)
</equation>
<sectionHeader confidence="0.998947" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998318">
3.1 Test Corpus
</subsectionHeader>
<bodyText confidence="0.999899769230769">
The conflation techniques described above were
tested on a corpus of historical German verse
extracted from the quotation evidence in a sin-
gle volume of the digital first edition of the dic-
tionary Deutsches W¨orterbuch “DWB” (Bartz et
al., 2004). The test corpus contained 11,242 to-
kens of 4157 distinct word types, discounting non-
alphabetic types such as punctuation. Each cor-
pus type was manually assigned one or more ex-
tant equivalents based on inspection of its occur-
rences in the whole 5.5 million word DWB verse
corpus in addition to secondary sources. Only ex-
tinct roots, proper names, foreign and other non-
lexical material were not explicitly assigned any
extant equivalent at all; such types were flagged
and treated as their own canonical cognates, i.e.
identical to their respective “extant” equivalents.
In all other cases, equivalence was determined by
direct etymological relation of the root in addition
to matching morphosyntactic features. Problem-
atic types were marked as such and subjected to
expert review. 296 test corpus types represent-
ing 585 tokens were ambiguously associated with
more than one canonical cognate. In a second an-
notation pass, these remaining ambiguities were
resolved on a per-token basis.
</bodyText>
<subsectionHeader confidence="0.990882">
3.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999962833333333">
The three conflation strategies from Section 2
were evaluated using the gold-standard test corpus
to simulate a document indexing and query sce-
nario. Formally, let G C A* x A* represent the
finite set of all gold-standard pairs (w, w) with we
the manually determined canonical cognate for the
corpus type w, and let Q = { we : ](w, w) E G}
be the set of all canonical cognates represented in
the corpus. Then define for a binary conflation re-
lation —R on A* and a query string q E Q the sets
relevant(q), retrievedR(q) C_ G of relevant and
retrieved gold-standard pairs as:
</bodyText>
<equation confidence="0.9919195">
relevant(q) = {(w, w) E G : we = q}
retrievedR(q) = {(w, w) E G : w —R q}
</equation>
<bodyText confidence="0.9910995">
Type-wise precision and recall can then be de-
fined directly as:
</bodyText>
<equation confidence="0.994108285714286">
���U���
gEQ retrievedR(q) n relevant(q)
���U���
gEQ retrievedR(q)
���U���
gEQ retrievedR(q) n relevant(q)
UgEQ relevant(q)
</equation>
<bodyText confidence="0.9972155">
If tpR(q) = retrievedR(q) n relevant(q) rep-
resents the set of true positives for a query q,
then token-wise precision and recall are defined
in terms of the gold-standard frequency function
</bodyText>
<equation confidence="0.9964605">
prG =
rcG =
</equation>
<page confidence="0.983254">
74
</page>
<bodyText confidence="0.456120888888889">
From —* To / Left Right (Cost) Example(s)
ε —* e / (A\{e}) # ( 5 ) aug ❀ auge “eye”
z —* s / s ( 1 ) faszt ❀ fasst “grabs”
o —* a / u ( 1 ) ouch ❀ auch “also”
ε —* h / V C ( 5 ) ram ❀ rahm “cream”
l —* ll / ( 8 ) vol ❀ voll “full”
Table 2: Some example heuristics used by the rewrite transducer. Here, ε represents the empty string,
# represents a word boundary, and V, C C A are sets of vowel-like and consonant-like characters,
respectively.
</bodyText>
<equation confidence="0.979876333333333">
fG:G—*Nas:
E
qEQ,gEtpR(q) fG(g)
E
qEQ,gEretrievedR(q) fG(g)
E
qEQ,gEtpR(q) fG(g)
E
qEQ,gErelevant(q) fG(g)
</equation>
<bodyText confidence="0.9995975">
We use the unweighted harmonic precision-
recall average F (van Rijsbergen, 1979) as a com-
posite measure for both type- and token-wise eval-
uation modes:
</bodyText>
<subsectionHeader confidence="0.758575">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.97448228">
The elementary canonicalization function for each
of the conflation techniques4 was applied to the
entire test corpus to simulate a corpus indexing
run. Running times for the various methods on
a 1.8GHz Linux workstation using the gfsmxl
C library are given in Table 3. The Levenshtein
edit-distance technique is at a clear disadvantage
here, roughly 150 times slower than the phonetic
technique and 40 times slower than the special-
ized heuristic rewrite transducer. This effect is
assumedly due to the density of the search space
(which is maximal for an unrestricted Levenshtein
editor), since the gfsmxl greedy k-best search
of a Levenshtein transducer cascade generates at
least |A |configurations per character, and a sin-
gle backtracking step requires an additional 3|A|
heap extractions (Jurish, 2010). Use of specialized
lookup algorithms (Oflazer, 1996) might amelio-
rate such problems.
Qualitative results for several conflation tech-
niques with respect to the DWB verse test corpus
are given in Table 4. An additional conflation rela-
tion “Id” using strict identity of grapheme strings
4pho, bestLev and bestrw for the phonetic, Levenshtein,
and heuristic rewrite transducer methods respectively
</bodyText>
<table confidence="0.99853325">
Method Time Throughput
Pho 1.82 sec 7322 tok/sec
Lev 278.03 sec 48 tok/sec
rw 7.02 sec 1898 tok/sec
</table>
<tableCaption confidence="0.975904">
Table 3: Processing time for elementary canoni-
calization functions
</tableCaption>
<bodyText confidence="0.999009903225806">
(w —Id v :.#&gt; w = v) was tested to provide a
baseline for the methods described in Section 2.
As expected, the strict identity baseline relation
was the most precise of all methods tested, achiev-
ing 99.9% type-wise and 99.1% token-wise pre-
cision. This is unsurprising, since the Id method
yields false positives only when a historical form
is indistinguishable from a non-equivalent extant
form, as in the case of the mapping wider ❀
wieder (“again”) and the non-equivalent extant
form wider (“against”). Despite its excellent pre-
cision, the baseline method’s recall was the low-
est of any tested method, which supports the claim
that a synchronically-oriented lexicon cannot ad-
equately account for a corpus of historical text.
Type-wise recall was particularly low (70.8%), in-
dicating that diachronic variation was more com-
mon in low-frequency types.
Surprisingly, the phonetic and Levenshtein
edit-distance methods performed similarly for
all measures except token-wise precision, in
which Lev incurred 61.6% fewer errors than
Pho. Given their near-identical type-wise
precision, this difference can be attributed
to a small number of phonetic misconfla-
tions involving high-frequency types, such as
wider—wieder (“against”—“again”), statt—stadt,
(“instead”—“city”), and in—ihn (“in”—“him”).
Contrary to expectations, Lev did not yield
any recall improvements over Pho, although the
union of the two underlying conflation relations
</bodyText>
<equation confidence="0.976984">
prfG =
rcfG =
2 · pr · rc
F(pr, rc) =
pr+rc
</equation>
<page confidence="0.995794">
75
</page>
<table confidence="0.99940375">
R Type-wise % Token-wise %
prG rcG FG prfG rcfG FfG
Id 99.9 70.8 82.9 99.1 83.7 90.7
Pho 96.7 80.1 87.6 92.7 89.6 91.1
Lev 96.6 78.9 86.9 97.2 87.8 92.2
rw 98.5 88.4 93.2 98.2 93.4 95.8
Pho  |Lev 94.1 84.3 88.9 91.3 91.6 91.5
Pho  |rw 96.1 89.8 92.8 92.5 94.5 93.5
</table>
<tableCaption confidence="0.999811">
Table 4: Qualitative evaluation of various conflation techniques
</tableCaption>
<bodyText confidence="0.999780214285714">
(-Pho I Lev = -Pho U -Lev) achieved a type-wise
recall of 84.3% (token-wise recall 91.6%), which
suggests that these two methods complement one
another when both an LTS module and a high-
coverage lexicon of extant types are available.
Of the methods described in Section 2, the
heuristic rewrite transducer Orw performed best
overall, with a type-wise harmonic mean F of
93.2% and a token-wise F of 95.8%. While Orw
incurred some additional precision errors com-
pared to the naive graphemic identity method Id,
these were not as devastating as those incurred
by the phonetic or Levenshtein distance meth-
ods, which supports the claim from Section 2.3
that a fine-grained context-sensitive pseudo-metric
incorporating linguistic knowledge can more ac-
curately model diachronic processes than an all-
purpose metric like the Levenshtein distance.
Recall was highest for the composite phonetic-
rewrite relation -Pho I rw=-Pho U -rw, although
the precision errors induced by the phonetic com-
ponent outweighed the comparatively small gain
in recall. The best overall performance is achieved
by the heuristic rewrite transducer Orw on its own,
yielding a reduction of 60.3% in type-wise recall
errors and of 59.5% in token-wise recall errors,
while minimizing the number of newly introduced
precision errors.
</bodyText>
<sectionHeader confidence="0.958525" genericHeader="conclusions">
4 Conclusion &amp; Outlook
</sectionHeader>
<bodyText confidence="0.99964524">
We have presented three different methods for
associating unknown historical word forms with
synchronically active canonical cognates. The
heuristic mapping of unknown forms to extant
equivalents by means of linguistically motivated
context-sensitive rewrite rules yielded the best re-
sults in an information retrieval task on a corpus
of historical German verse, reducing type-wise
recall errors by over 60% compared to a naive
text-matching strategy. Depending on the avail-
ability of linguistic resources (e.g. phonetization
rule-sets, lexica), use of phonetic canonicalization
and/or Levenshtein edit distance may provide a
more immediately accessible route to improved re-
call for other languages or applications, at the ex-
pense of some additional loss of precision.
We are interested in verifying our results us-
ing larger corpora than the small test corpus used
here, as well as extending the techniques described
here to other languages and domains. In par-
ticular, we are interested in comparing the per-
formance of the domain-specific rewrite trans-
ducer used here to other linguistically motivated
language-independent metrics such as (Covington,
1996; Kondrak, 2000).
</bodyText>
<sectionHeader confidence="0.994601" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999920888888889">
The work described above was funded by a
Deutsche Forschungsgemeinschaft (DFG) grant to
the project Deutsches Textarchiv. Additionally,
the author would like to thank J¨org Didakowski,
Oliver Duntze, Alexander Geyken, Thomas Han-
neforth, Henriette Scharnhorst, Wolfgang Seeker,
Kay-Michael W¨urzner, and this paper’s anony-
mous reviewers for their helpful feedback and
comments.
</bodyText>
<sectionHeader confidence="0.991765" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.927948636363636">
Cyril Allauzen and Mehryar Mohri. 2009. Linear-
space computation of the edit-distance between a
string and a finite automaton. In London Algorith-
mics 2008: Theory and Practice. College Publica-
tions.
Jonathan Allen, M. Sharon Hunnicutt, and Dennis
Klatt. 1987. From Text to Speech: the MITalk sys-
tem. Cambridge University Press.
Hans-Werner Bartz, Thomas Burch, Ruth Christmann,
Kurt G¨artner, Vera Hildenbrandt, Thomas Schares,
and Klaudia Wegge, editors. 2004. Der Digitale
</reference>
<page confidence="0.935925">
76
</page>
<reference confidence="0.994036168421053">
Grimm. Deutsches W¨orterbuch von Jacob und Wil-
helm Grimm. Zweitausendeins, Frankfurt am Main.
Eric Brill. 1992. A simple rule-based part-of-speech
tagger. In Proceedings ofANLP-92, 3rd Conference
on Applied Natural Language Processing, pages
152–155, Trento, Italy.
Mike Cafarella and Doug Cutting. 2004. Building
Nutch: Open source search. Queue, 2(2):54–61.
Simon Clematide. 2008. An OLIF-based open inflec-
tion resource and yet another morphological system
for German. In Storrer et al. (Storrer et al., 2008),
pages 183–194.
Michael A. Covington. 1996. An algorithm to align
words for historical comparison. Computational
Linguistics, 22:481–496.
Stephen DeRose. 1988. Grammatical category disam-
biguation by statistical optimization. Computational
Linguistics, 14(1):31–39.
Edsger W. Dijkstra. 1959. A note on two problems
in connexion with graphs. Numerische Mathematik,
1:269–271.
Thierry Dutoit. 1997. An Introduction to Text-to-
Speech Synthesis. Kluwer, Dordrecht.
Alexander Geyken and Thomas Hanneforth. 2006.
TAGH: A complete morphology for German based
on weighted finite state automata. In Proceedings
FSMNLP 2005, pages 55–66, Berlin. Springer.
Bryan Jurish. 2008. Finding canonical forms for his-
torical German text. In Storrer et al. (Storrer et al.,
2008), pages 27–37.
Bryan Jurish. 2010. Efficient online k-best lookup in
weighted finite-state cascades. To appear in Studia
Grammatica.
Ronald M. Kaplan and Martin Kay. 1994. Regu-
lar models of phonological rule systems. Compu-
tational Linguistics, 20(3):331–378.
Lauri Karttunen, Ronald M. Kay, and Kimmo Kosken-
niemi. 1987. A compiler for two-level phonological
rules. In M. Dalrymple, R. Kaplan, L. Karttunen,
K. Koskenniemi, S. Shaio, and M. Wescoat, editors,
Tools for Morphological Analysis, volume 87-108 of
CSLI Reports, pages 1–61. CSLI, Stanford Univer-
sity, Palo Alto, CA.
Gregorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proceedings
NAACL, pages 288–295.
´Eric Laporte. 1997. Rational transductions for pho-
netic conversion and phonology. In Emmanuel
Roche and Yves Schabes, editors, Finite-State Lan-
guage Processing. MIT Press, Cambridge, MA.
Vladimir I. Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(1966):707–710.
Julie Beth Lovins. 1968. Development of a stemming
algorithm. Mechanical Translation and Computa-
tional Linguistics, 11:22–31.
Mehryar Mohri. 2002. Semiring frameworks and
algorithms for shortest-distance problems. Jour-
nal of Automata, Languages and Combinatorics,
7(3):321–350.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Handbook of Weighted Automata, Monographs
in Theoretical Computer Science, pages 213–254.
Springer, Berlin.
Gregor M¨ohler, Antje Schweitzer, and Mark Breit-
enb¨ucher, 2001. IMS German Festival manual, ver-
sion 1.2. Institute for Natural Language Processing,
University of Stuttgart.
Gonzalo Navarro. 2001. A guided tour to approx-
imate string matching. ACM Computing Surveys,
33(1):31–88.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22(1):73–89.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130–137.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44–49, Manchester, UK.
Imre Simon. 1987. The nondeterministic complex-
ity of finite automata. Technical Report RT-MAP-
8073, Instituto de Matem´atica e Estatistica da Uni-
versidade de S˜ao Paulo.
Alexey Sokirko. 2003. A technical overview of
DWDS/dialing concordance. Talk delivered at the
meeting Computational linguistics and intellectual
technologies, Protvino, Russia.
Angelika Storrer, Alexander Geyken, Alexander
Siebert, and Kay-Michael W¨urzner, editors. 2008.
Text Resources and Lexical Knowledge. Mouton de
Gruyter, Berlin.
C. J. van Rijsbergen. 1979. Information Retrieval.
Butterworth-Heinemann, Newton, MA.
</reference>
<page confidence="0.999129">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.292580">
<title confidence="0.999315">Comparing Canonicalizations of Historical German Text</title>
<author confidence="0.962599">Bryan</author>
<affiliation confidence="0.629884">Berlin-Brandenburg Academy of Berlin,</affiliation>
<email confidence="0.995746">jurish@bbaw.de</email>
<abstract confidence="0.9904082">Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a static lexicon accessed by orthographic form. In this paper, we present three methods for associating unknown historical word forms with synchronically active canonical cognates and evaluate their performance on an information retrieval task over a manually annotated corpus of historical German verse.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
</authors>
<title>Linearspace computation of the edit-distance between a string and a finite automaton. In London Algorithmics 2008: Theory and Practice.</title>
<date>2009</date>
<publisher>College Publications.</publisher>
<contexts>
<context position="6513" citStr="Allauzen and Mohri, 2009" startWordPosition="998" endWordPosition="1002">t word v E Lex with minimal edit-distance to the input word:2 bestLev(w) = arg min dLev(w, v) (2) vELex Ideally, the image of a word w under bestLev will itself be the canonical cognate sought,3 leading to conflation of all strings which share a common image under bestLev: w —Lev v :⇔ bestLev(w) = bestLev(v) (3) The function bestLev(w) : A* → Lex can be computed using a variant of the Dijkstra algorithm (Dijkstra, 1959) even when the lexicon is infinite (as in the case of productive nominal composition in German) whenever the set Lex can be represented by a finite-state acceptor (Mohri, 2002; Allauzen and Mohri, 2009; Jurish, 2010). For current purposes, we used the (infinite) input language of the TAGH morphology transducer (Geyken and Hanneforth, 2006) stripped of proper names, abbreviations, and foreign-language material to approximate Lex. 2.3 Rewrite Transducer While the simple edit distance conflation technique from the previous section is quite powerful and requires for its implementation only a lexicon of extant forms, the Levenshtein distance itself appears in many cases too coarse to function as a reliable predictor of etymological relations, since each edit operation (deletion, insertion, or su</context>
<context position="9453" citStr="Allauzen and Mohri, 2009" startWordPosition="1455" endWordPosition="1458">c rewrite rules, 131 manipulate consonant-like strings, 115 deal with vowellike strings, and 14 operate directly on syllablelike units. The remaining 46 rules define expansions for explicitly marked elisions and unrecognized input. Some examples of rules used by the rewrite transducer are given in Table 2. Formally, the rewrite transducer Arw defines a pseudo-metric QArw� : A* x A* —* 1[8,,,, on all string pairs (Mohri, 2009). Assuming the non-negative tropical semiring (Simon, 1987) is used to represent transducer weights, analagous to the transducer representation of the Levenshtein metric (Allauzen and Mohri, 2009), the rewrite pseudo-metric can be used as a drop-in replacement for the Levenshtein distance in Equations (2) and (3), yielding Equations (4) and (5): bestrw(w) = arg min QArw�(w, v) (4) VELex w —rw v :.&lt;---&gt; bestrw(w) = bestrw(v) (5) 3 Evaluation 3.1 Test Corpus The conflation techniques described above were tested on a corpus of historical German verse extracted from the quotation evidence in a single volume of the digital first edition of the dictionary Deutsches W¨orterbuch “DWB” (Bartz et al., 2004). The test corpus contained 11,242 tokens of 4157 distinct word types, discounting nonalph</context>
</contexts>
<marker>Allauzen, Mohri, 2009</marker>
<rawString>Cyril Allauzen and Mehryar Mohri. 2009. Linearspace computation of the edit-distance between a string and a finite automaton. In London Algorithmics 2008: Theory and Practice. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Allen</author>
<author>M Sharon Hunnicutt</author>
<author>Dennis Klatt</author>
</authors>
<title>From Text to Speech: the MITalk system.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3965" citStr="Allen et al., 1987" startWordPosition="588" endWordPosition="591">provide a better clue to its extant cognates (if any) than a historical graphemic form alone. Taken together, these assumptions lead to the canonicaliza72 Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 72–77, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics tion technique referred to here as phonetic conflation. In order to map graphemic forms to phonetic forms, we may avail ourselves of previous work in the realm of text-to-speech synthesis, a domain in which the discovery of phonetic forms for arbitrary text is an often-studied problem (Allen et al., 1987; Dutoit, 1997), the so-called “letter-tosound” (LTS) conversion problem. The phonetic conversion module used here was adapted from the LTS rule-set distributed with the IMS German Festival package (M¨ohler et al., 2001), and compiled as a finite-state transducer (Jurish, 2008). In general, the phonetic conflation strategy maps each (historical or extant) input word w E A* to a unique phonetic form pho(w) by means of a computable function pho : A* → P*,1 conflating those strings which share a common phonetic form: w —Pho v :⇔ pho(w) = pho(v) (1) 2.2 Levenshtein Edit Distance Although the phone</context>
</contexts>
<marker>Allen, Hunnicutt, Klatt, 1987</marker>
<rawString>Jonathan Allen, M. Sharon Hunnicutt, and Dennis Klatt. 1987. From Text to Speech: the MITalk system. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<date>2004</date>
<booktitle>Der Digitale Grimm. Deutsches W¨orterbuch von Jacob und Wilhelm Grimm. Zweitausendeins, Frankfurt am Main.</booktitle>
<editor>Hans-Werner Bartz, Thomas Burch, Ruth Christmann, Kurt G¨artner, Vera Hildenbrandt, Thomas Schares, and Klaudia Wegge, editors.</editor>
<marker>2004</marker>
<rawString>Hans-Werner Bartz, Thomas Burch, Ruth Christmann, Kurt G¨artner, Vera Hildenbrandt, Thomas Schares, and Klaudia Wegge, editors. 2004. Der Digitale Grimm. Deutsches W¨orterbuch von Jacob und Wilhelm Grimm. Zweitausendeins, Frankfurt am Main.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part-of-speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings ofANLP-92, 3rd Conference on Applied Natural Language Processing,</booktitle>
<pages>152--155</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="1136" citStr="Brill, 1992" startWordPosition="152" endWordPosition="153">s with synchronically active canonical cognates and evaluate their performance on an information retrieval task over a manually annotated corpus of historical German verse. 1 Introduction Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as document indexing systems (Sokirko, 2003; Cafarella and Cutting, 2004), part-of-speech taggers (DeRose, 1988; Brill, 1992; Schmid, 1994), simple word stemmers (Lovins, 1968; Porter, 1980), or more sophisticated morphological analyzers (Geyken and Hanneforth, 2006; Clematide, 2008). When adopting historical text into such a system, one of the most crucial tasks is the association of one or more extant equivalents with each word of the input text: synchronically active types which best represent the relevant features of the input word. Which features are considered “relevant” here depends on the application in question: for a lemmatization task only the root lexeme is relevant, whereas syntactic parsing may requir</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Eric Brill. 1992. A simple rule-based part-of-speech tagger. In Proceedings ofANLP-92, 3rd Conference on Applied Natural Language Processing, pages 152–155, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Cafarella</author>
<author>Doug Cutting</author>
</authors>
<title>Building Nutch: Open source search.</title>
<date>2004</date>
<journal>Queue,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="1085" citStr="Cafarella and Cutting, 2004" startWordPosition="143" endWordPosition="146">e present three methods for associating unknown historical word forms with synchronically active canonical cognates and evaluate their performance on an information retrieval task over a manually annotated corpus of historical German verse. 1 Introduction Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as document indexing systems (Sokirko, 2003; Cafarella and Cutting, 2004), part-of-speech taggers (DeRose, 1988; Brill, 1992; Schmid, 1994), simple word stemmers (Lovins, 1968; Porter, 1980), or more sophisticated morphological analyzers (Geyken and Hanneforth, 2006; Clematide, 2008). When adopting historical text into such a system, one of the most crucial tasks is the association of one or more extant equivalents with each word of the input text: synchronically active types which best represent the relevant features of the input word. Which features are considered “relevant” here depends on the application in question: for a lemmatization task only the root lexem</context>
</contexts>
<marker>Cafarella, Cutting, 2004</marker>
<rawString>Mike Cafarella and Doug Cutting. 2004. Building Nutch: Open source search. Queue, 2(2):54–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Clematide</author>
</authors>
<title>An OLIF-based open inflection resource and yet another morphological system for German.</title>
<date>2008</date>
<journal>In Storrer</journal>
<pages>183--194</pages>
<contexts>
<context position="1296" citStr="Clematide, 2008" startWordPosition="173" endWordPosition="174">cal German verse. 1 Introduction Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as document indexing systems (Sokirko, 2003; Cafarella and Cutting, 2004), part-of-speech taggers (DeRose, 1988; Brill, 1992; Schmid, 1994), simple word stemmers (Lovins, 1968; Porter, 1980), or more sophisticated morphological analyzers (Geyken and Hanneforth, 2006; Clematide, 2008). When adopting historical text into such a system, one of the most crucial tasks is the association of one or more extant equivalents with each word of the input text: synchronically active types which best represent the relevant features of the input word. Which features are considered “relevant” here depends on the application in question: for a lemmatization task only the root lexeme is relevant, whereas syntactic parsing may require additional morphosyntactic features. For current purposes, extant equivalents are to be understood as canonical cognates, preserving both the root(s) and morp</context>
</contexts>
<marker>Clematide, 2008</marker>
<rawString>Simon Clematide. 2008. An OLIF-based open inflection resource and yet another morphological system for German. In Storrer et al. (Storrer et al., 2008), pages 183–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>An algorithm to align words for historical comparison.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--481</pages>
<marker>Covington, 1996</marker>
<rawString>Michael A. Covington. 1996. An algorithm to align words for historical comparison. Computational Linguistics, 22:481–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="1123" citStr="DeRose, 1988" startWordPosition="150" endWordPosition="151">ical word forms with synchronically active canonical cognates and evaluate their performance on an information retrieval task over a manually annotated corpus of historical German verse. 1 Introduction Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as document indexing systems (Sokirko, 2003; Cafarella and Cutting, 2004), part-of-speech taggers (DeRose, 1988; Brill, 1992; Schmid, 1994), simple word stemmers (Lovins, 1968; Porter, 1980), or more sophisticated morphological analyzers (Geyken and Hanneforth, 2006; Clematide, 2008). When adopting historical text into such a system, one of the most crucial tasks is the association of one or more extant equivalents with each word of the input text: synchronically active types which best represent the relevant features of the input word. Which features are considered “relevant” here depends on the application in question: for a lemmatization task only the root lexeme is relevant, whereas syntactic parsi</context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>Stephen DeRose. 1988. Grammatical category disambiguation by statistical optimization. Computational Linguistics, 14(1):31–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edsger W Dijkstra</author>
</authors>
<title>A note on two problems in connexion with graphs.</title>
<date>1959</date>
<journal>Numerische Mathematik,</journal>
<pages>1--269</pages>
<contexts>
<context position="6312" citStr="Dijkstra, 1959" startWordPosition="966" endWordPosition="967">resent the Levenshtein distance over grapheme strings, then define for every input word w E A* the “best” synchronic equivalent 1P is a finite phonetic alphabet. bestLev(w) as the unique extant word v E Lex with minimal edit-distance to the input word:2 bestLev(w) = arg min dLev(w, v) (2) vELex Ideally, the image of a word w under bestLev will itself be the canonical cognate sought,3 leading to conflation of all strings which share a common image under bestLev: w —Lev v :⇔ bestLev(w) = bestLev(v) (3) The function bestLev(w) : A* → Lex can be computed using a variant of the Dijkstra algorithm (Dijkstra, 1959) even when the lexicon is infinite (as in the case of productive nominal composition in German) whenever the set Lex can be represented by a finite-state acceptor (Mohri, 2002; Allauzen and Mohri, 2009; Jurish, 2010). For current purposes, we used the (infinite) input language of the TAGH morphology transducer (Geyken and Hanneforth, 2006) stripped of proper names, abbreviations, and foreign-language material to approximate Lex. 2.3 Rewrite Transducer While the simple edit distance conflation technique from the previous section is quite powerful and requires for its implementation only a lexic</context>
</contexts>
<marker>Dijkstra, 1959</marker>
<rawString>Edsger W. Dijkstra. 1959. A note on two problems in connexion with graphs. Numerische Mathematik, 1:269–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Dutoit</author>
</authors>
<title>An Introduction to Text-toSpeech Synthesis.</title>
<date>1997</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="3980" citStr="Dutoit, 1997" startWordPosition="592" endWordPosition="593">e to its extant cognates (if any) than a historical graphemic form alone. Taken together, these assumptions lead to the canonicaliza72 Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 72–77, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics tion technique referred to here as phonetic conflation. In order to map graphemic forms to phonetic forms, we may avail ourselves of previous work in the realm of text-to-speech synthesis, a domain in which the discovery of phonetic forms for arbitrary text is an often-studied problem (Allen et al., 1987; Dutoit, 1997), the so-called “letter-tosound” (LTS) conversion problem. The phonetic conversion module used here was adapted from the LTS rule-set distributed with the IMS German Festival package (M¨ohler et al., 2001), and compiled as a finite-state transducer (Jurish, 2008). In general, the phonetic conflation strategy maps each (historical or extant) input word w E A* to a unique phonetic form pho(w) by means of a computable function pho : A* → P*,1 conflating those strings which share a common phonetic form: w —Pho v :⇔ pho(w) = pho(v) (1) 2.2 Levenshtein Edit Distance Although the phonetic conflation </context>
</contexts>
<marker>Dutoit, 1997</marker>
<rawString>Thierry Dutoit. 1997. An Introduction to Text-toSpeech Synthesis. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Geyken</author>
<author>Thomas Hanneforth</author>
</authors>
<title>TAGH: A complete morphology for German based on weighted finite state automata.</title>
<date>2006</date>
<booktitle>In Proceedings FSMNLP 2005,</booktitle>
<pages>55--66</pages>
<publisher>Springer.</publisher>
<location>Berlin.</location>
<contexts>
<context position="1278" citStr="Geyken and Hanneforth, 2006" startWordPosition="169" endWordPosition="172">y annotated corpus of historical German verse. 1 Introduction Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as document indexing systems (Sokirko, 2003; Cafarella and Cutting, 2004), part-of-speech taggers (DeRose, 1988; Brill, 1992; Schmid, 1994), simple word stemmers (Lovins, 1968; Porter, 1980), or more sophisticated morphological analyzers (Geyken and Hanneforth, 2006; Clematide, 2008). When adopting historical text into such a system, one of the most crucial tasks is the association of one or more extant equivalents with each word of the input text: synchronically active types which best represent the relevant features of the input word. Which features are considered “relevant” here depends on the application in question: for a lemmatization task only the root lexeme is relevant, whereas syntactic parsing may require additional morphosyntactic features. For current purposes, extant equivalents are to be understood as canonical cognates, preserving both th</context>
<context position="6653" citStr="Geyken and Hanneforth, 2006" startWordPosition="1019" endWordPosition="1022">nder bestLev will itself be the canonical cognate sought,3 leading to conflation of all strings which share a common image under bestLev: w —Lev v :⇔ bestLev(w) = bestLev(v) (3) The function bestLev(w) : A* → Lex can be computed using a variant of the Dijkstra algorithm (Dijkstra, 1959) even when the lexicon is infinite (as in the case of productive nominal composition in German) whenever the set Lex can be represented by a finite-state acceptor (Mohri, 2002; Allauzen and Mohri, 2009; Jurish, 2010). For current purposes, we used the (infinite) input language of the TAGH morphology transducer (Geyken and Hanneforth, 2006) stripped of proper names, abbreviations, and foreign-language material to approximate Lex. 2.3 Rewrite Transducer While the simple edit distance conflation technique from the previous section is quite powerful and requires for its implementation only a lexicon of extant forms, the Levenshtein distance itself appears in many cases too coarse to function as a reliable predictor of etymological relations, since each edit operation (deletion, insertion, or substitution) is assigned a cost independent of the characters operated on and of the immediate context in the strings under consideration. Th</context>
</contexts>
<marker>Geyken, Hanneforth, 2006</marker>
<rawString>Alexander Geyken and Thomas Hanneforth. 2006. TAGH: A complete morphology for German based on weighted finite state automata. In Proceedings FSMNLP 2005, pages 55–66, Berlin. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Jurish</author>
</authors>
<title>Finding canonical forms for historical German text.</title>
<date>2008</date>
<booktitle>In Storrer et</booktitle>
<pages>27--37</pages>
<contexts>
<context position="4243" citStr="Jurish, 2008" startWordPosition="631" endWordPosition="632">for Computational Linguistics tion technique referred to here as phonetic conflation. In order to map graphemic forms to phonetic forms, we may avail ourselves of previous work in the realm of text-to-speech synthesis, a domain in which the discovery of phonetic forms for arbitrary text is an often-studied problem (Allen et al., 1987; Dutoit, 1997), the so-called “letter-tosound” (LTS) conversion problem. The phonetic conversion module used here was adapted from the LTS rule-set distributed with the IMS German Festival package (M¨ohler et al., 2001), and compiled as a finite-state transducer (Jurish, 2008). In general, the phonetic conflation strategy maps each (historical or extant) input word w E A* to a unique phonetic form pho(w) by means of a computable function pho : A* → P*,1 conflating those strings which share a common phonetic form: w —Pho v :⇔ pho(w) = pho(v) (1) 2.2 Levenshtein Edit Distance Although the phonetic conflation technique described in the previous section is capable of successfully identifying a number of common historical graphematic variation patterns such as ey/ei, œ/¨o, th/t, and tz/z, it fails to conflate historical forms with any extant equivalent whenever the grap</context>
<context position="8648" citStr="Jurish, 2008" startWordPosition="1333" endWordPosition="1334">dLev(w, w) = 0 &lt; dLev(w, v) for all v =� w. 73 w bestLev(w) Extant Equivalent aug aus “out” auge “eye” faszt fast “almost” fasst “grabs” ouch buch “book” auch “also” ram rat “advice” rahm “cream” vol volk “people” voll “full” Table 1: Example spurious Levenshtein distance conflations heuristic two-level rule-set (Karttunen et al., 1987; Kaplan and Kay, 1994; Laporte, 1997) whose 306 rules were manually constructed to reflect linguistically plausible patterns of diachronic variation as observed in the lemma-instance pairs automatically extracted from the full 5.5 million word DWB verse corpus (Jurish, 2008). In particular, phonetic phenomena such as schwa deletion, vowel shift, voicing alternation, and articulatory location shift are easily captured by such rules. Of the 306 heuristic rewrite rules, 131 manipulate consonant-like strings, 115 deal with vowellike strings, and 14 operate directly on syllablelike units. The remaining 46 rules define expansions for explicitly marked elisions and unrecognized input. Some examples of rules used by the rewrite transducer are given in Table 2. Formally, the rewrite transducer Arw defines a pseudo-metric QArw� : A* x A* —* 1[8,,,, on all string pairs (Moh</context>
</contexts>
<marker>Jurish, 2008</marker>
<rawString>Bryan Jurish. 2008. Finding canonical forms for historical German text. In Storrer et al. (Storrer et al., 2008), pages 27–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Jurish</author>
</authors>
<title>Efficient online k-best lookup in weighted finite-state cascades.</title>
<date>2010</date>
<note>To appear in Studia Grammatica.</note>
<contexts>
<context position="6528" citStr="Jurish, 2010" startWordPosition="1003" endWordPosition="1004">l edit-distance to the input word:2 bestLev(w) = arg min dLev(w, v) (2) vELex Ideally, the image of a word w under bestLev will itself be the canonical cognate sought,3 leading to conflation of all strings which share a common image under bestLev: w —Lev v :⇔ bestLev(w) = bestLev(v) (3) The function bestLev(w) : A* → Lex can be computed using a variant of the Dijkstra algorithm (Dijkstra, 1959) even when the lexicon is infinite (as in the case of productive nominal composition in German) whenever the set Lex can be represented by a finite-state acceptor (Mohri, 2002; Allauzen and Mohri, 2009; Jurish, 2010). For current purposes, we used the (infinite) input language of the TAGH morphology transducer (Geyken and Hanneforth, 2006) stripped of proper names, abbreviations, and foreign-language material to approximate Lex. 2.3 Rewrite Transducer While the simple edit distance conflation technique from the previous section is quite powerful and requires for its implementation only a lexicon of extant forms, the Levenshtein distance itself appears in many cases too coarse to function as a reliable predictor of etymological relations, since each edit operation (deletion, insertion, or substitution) is </context>
<context position="13535" citStr="Jurish, 2010" startWordPosition="2157" endWordPosition="2158">a 1.8GHz Linux workstation using the gfsmxl C library are given in Table 3. The Levenshtein edit-distance technique is at a clear disadvantage here, roughly 150 times slower than the phonetic technique and 40 times slower than the specialized heuristic rewrite transducer. This effect is assumedly due to the density of the search space (which is maximal for an unrestricted Levenshtein editor), since the gfsmxl greedy k-best search of a Levenshtein transducer cascade generates at least |A |configurations per character, and a single backtracking step requires an additional 3|A| heap extractions (Jurish, 2010). Use of specialized lookup algorithms (Oflazer, 1996) might ameliorate such problems. Qualitative results for several conflation techniques with respect to the DWB verse test corpus are given in Table 4. An additional conflation relation “Id” using strict identity of grapheme strings 4pho, bestLev and bestrw for the phonetic, Levenshtein, and heuristic rewrite transducer methods respectively Method Time Throughput Pho 1.82 sec 7322 tok/sec Lev 278.03 sec 48 tok/sec rw 7.02 sec 1898 tok/sec Table 3: Processing time for elementary canonicalization functions (w —Id v :.#&gt; w = v) was tested to pr</context>
</contexts>
<marker>Jurish, 2010</marker>
<rawString>Bryan Jurish. 2010. Efficient online k-best lookup in weighted finite-state cascades. To appear in Studia Grammatica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="8394" citStr="Kaplan and Kay, 1994" startWordPosition="1294" endWordPosition="1297">ein metric. The rewrite transducer was compiled from a 2We assume that whenever multiple extant minimaldistance candidate forms exist, one is chosen randomly. 3Note here that every extant form is its own “best” equivalent: w ∈ Lex implies bestLev(w) = w, since dLev(w, w) = 0 &lt; dLev(w, v) for all v =� w. 73 w bestLev(w) Extant Equivalent aug aus “out” auge “eye” faszt fast “almost” fasst “grabs” ouch buch “book” auch “also” ram rat “advice” rahm “cream” vol volk “people” voll “full” Table 1: Example spurious Levenshtein distance conflations heuristic two-level rule-set (Karttunen et al., 1987; Kaplan and Kay, 1994; Laporte, 1997) whose 306 rules were manually constructed to reflect linguistically plausible patterns of diachronic variation as observed in the lemma-instance pairs automatically extracted from the full 5.5 million word DWB verse corpus (Jurish, 2008). In particular, phonetic phenomena such as schwa deletion, vowel shift, voicing alternation, and articulatory location shift are easily captured by such rules. Of the 306 heuristic rewrite rules, 131 manipulate consonant-like strings, 115 deal with vowellike strings, and 14 operate directly on syllablelike units. The remaining 46 rules define </context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Ronald M. Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Ronald M Kay</author>
<author>Kimmo Koskenniemi</author>
</authors>
<title>A compiler for two-level phonological rules.</title>
<date>1987</date>
<booktitle>Tools for Morphological Analysis, volume 87-108 of CSLI Reports,</booktitle>
<pages>1--61</pages>
<editor>In M. Dalrymple, R. Kaplan, L. Karttunen, K. Koskenniemi, S. Shaio, and M. Wescoat, editors,</editor>
<publisher>CSLI, Stanford University,</publisher>
<location>Palo Alto, CA.</location>
<contexts>
<context position="8372" citStr="Karttunen et al., 1987" startWordPosition="1290" endWordPosition="1293">lace the simple Levenshtein metric. The rewrite transducer was compiled from a 2We assume that whenever multiple extant minimaldistance candidate forms exist, one is chosen randomly. 3Note here that every extant form is its own “best” equivalent: w ∈ Lex implies bestLev(w) = w, since dLev(w, w) = 0 &lt; dLev(w, v) for all v =� w. 73 w bestLev(w) Extant Equivalent aug aus “out” auge “eye” faszt fast “almost” fasst “grabs” ouch buch “book” auch “also” ram rat “advice” rahm “cream” vol volk “people” voll “full” Table 1: Example spurious Levenshtein distance conflations heuristic two-level rule-set (Karttunen et al., 1987; Kaplan and Kay, 1994; Laporte, 1997) whose 306 rules were manually constructed to reflect linguistically plausible patterns of diachronic variation as observed in the lemma-instance pairs automatically extracted from the full 5.5 million word DWB verse corpus (Jurish, 2008). In particular, phonetic phenomena such as schwa deletion, vowel shift, voicing alternation, and articulatory location shift are easily captured by such rules. Of the 306 heuristic rewrite rules, 131 manipulate consonant-like strings, 115 deal with vowellike strings, and 14 operate directly on syllablelike units. The rema</context>
</contexts>
<marker>Karttunen, Kay, Koskenniemi, 1987</marker>
<rawString>Lauri Karttunen, Ronald M. Kay, and Kimmo Koskenniemi. 1987. A compiler for two-level phonological rules. In M. Dalrymple, R. Kaplan, L. Karttunen, K. Koskenniemi, S. Shaio, and M. Wescoat, editors, Tools for Morphological Analysis, volume 87-108 of CSLI Reports, pages 1–61. CSLI, Stanford University, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregorz Kondrak</author>
</authors>
<title>A new algorithm for the alignment of phonetic sequences.</title>
<date>2000</date>
<booktitle>In Proceedings NAACL,</booktitle>
<pages>288--295</pages>
<marker>Kondrak, 2000</marker>
<rawString>Gregorz Kondrak. 2000. A new algorithm for the alignment of phonetic sequences. In Proceedings NAACL, pages 288–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Eric Laporte</author>
</authors>
<title>Rational transductions for phonetic conversion and phonology.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8410" citStr="Laporte, 1997" startWordPosition="1298" endWordPosition="1299">e transducer was compiled from a 2We assume that whenever multiple extant minimaldistance candidate forms exist, one is chosen randomly. 3Note here that every extant form is its own “best” equivalent: w ∈ Lex implies bestLev(w) = w, since dLev(w, w) = 0 &lt; dLev(w, v) for all v =� w. 73 w bestLev(w) Extant Equivalent aug aus “out” auge “eye” faszt fast “almost” fasst “grabs” ouch buch “book” auch “also” ram rat “advice” rahm “cream” vol volk “people” voll “full” Table 1: Example spurious Levenshtein distance conflations heuristic two-level rule-set (Karttunen et al., 1987; Kaplan and Kay, 1994; Laporte, 1997) whose 306 rules were manually constructed to reflect linguistically plausible patterns of diachronic variation as observed in the lemma-instance pairs automatically extracted from the full 5.5 million word DWB verse corpus (Jurish, 2008). In particular, phonetic phenomena such as schwa deletion, vowel shift, voicing alternation, and articulatory location shift are easily captured by such rules. Of the 306 heuristic rewrite rules, 131 manipulate consonant-like strings, 115 deal with vowellike strings, and 14 operate directly on syllablelike units. The remaining 46 rules define expansions for e</context>
</contexts>
<marker>Laporte, 1997</marker>
<rawString>´Eric Laporte. 1997. Rational transductions for phonetic conversion and phonology. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>1966</issue>
<contexts>
<context position="5590" citStr="Levenshtein, 1966" startWordPosition="836" endWordPosition="837">r, whenever a historical variation would effect a pronunciation difference in synchronic forms, that variation will remain uncaptured by a phonetic conflation technique. Examples of such phonetically salient variations with respect to the simplified IMS German Festival rule-set include guot/gut “good”, liecht/licht “light”, tiuvel/teufel “devil”, and wolln/wollen “want”. In order to accommodate graphematic variation phenomena beyond those for which strict phonetic identity of the variant forms obtains, we may employ an approximate search strategy based on the simple Levenshtein edit distance (Levenshtein, 1966; Navarro, 2001). Formally, let Lex C A* be the lexicon of all extant forms, and let dLev : A* x A* → N represent the Levenshtein distance over grapheme strings, then define for every input word w E A* the “best” synchronic equivalent 1P is a finite phonetic alphabet. bestLev(w) as the unique extant word v E Lex with minimal edit-distance to the input word:2 bestLev(w) = arg min dLev(w, v) (2) vELex Ideally, the image of a word w under bestLev will itself be the canonical cognate sought,3 leading to conflation of all strings which share a common image under bestLev: w —Lev v :⇔ bestLev(w) = be</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(1966):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Beth Lovins</author>
</authors>
<title>Development of a stemming algorithm.</title>
<date>1968</date>
<booktitle>Mechanical Translation and Computational Linguistics,</booktitle>
<pages>11--22</pages>
<contexts>
<context position="1187" citStr="Lovins, 1968" startWordPosition="159" endWordPosition="160">d evaluate their performance on an information retrieval task over a manually annotated corpus of historical German verse. 1 Introduction Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as document indexing systems (Sokirko, 2003; Cafarella and Cutting, 2004), part-of-speech taggers (DeRose, 1988; Brill, 1992; Schmid, 1994), simple word stemmers (Lovins, 1968; Porter, 1980), or more sophisticated morphological analyzers (Geyken and Hanneforth, 2006; Clematide, 2008). When adopting historical text into such a system, one of the most crucial tasks is the association of one or more extant equivalents with each word of the input text: synchronically active types which best represent the relevant features of the input word. Which features are considered “relevant” here depends on the application in question: for a lemmatization task only the root lexeme is relevant, whereas syntactic parsing may require additional morphosyntactic features. For current </context>
</contexts>
<marker>Lovins, 1968</marker>
<rawString>Julie Beth Lovins. 1968. Development of a stemming algorithm. Mechanical Translation and Computational Linguistics, 11:22–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Semiring frameworks and algorithms for shortest-distance problems.</title>
<date>2002</date>
<journal>Journal of Automata, Languages and Combinatorics,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="6487" citStr="Mohri, 2002" startWordPosition="996" endWordPosition="997"> unique extant word v E Lex with minimal edit-distance to the input word:2 bestLev(w) = arg min dLev(w, v) (2) vELex Ideally, the image of a word w under bestLev will itself be the canonical cognate sought,3 leading to conflation of all strings which share a common image under bestLev: w —Lev v :⇔ bestLev(w) = bestLev(v) (3) The function bestLev(w) : A* → Lex can be computed using a variant of the Dijkstra algorithm (Dijkstra, 1959) even when the lexicon is infinite (as in the case of productive nominal composition in German) whenever the set Lex can be represented by a finite-state acceptor (Mohri, 2002; Allauzen and Mohri, 2009; Jurish, 2010). For current purposes, we used the (infinite) input language of the TAGH morphology transducer (Geyken and Hanneforth, 2006) stripped of proper names, abbreviations, and foreign-language material to approximate Lex. 2.3 Rewrite Transducer While the simple edit distance conflation technique from the previous section is quite powerful and requires for its implementation only a lexicon of extant forms, the Levenshtein distance itself appears in many cases too coarse to function as a reliable predictor of etymological relations, since each edit operation (</context>
</contexts>
<marker>Mohri, 2002</marker>
<rawString>Mehryar Mohri. 2002. Semiring frameworks and algorithms for shortest-distance problems. Journal of Automata, Languages and Combinatorics, 7(3):321–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Weighted automata algorithms.</title>
<date>2009</date>
<booktitle>In Handbook of Weighted Automata, Monographs in Theoretical Computer Science,</booktitle>
<pages>213--254</pages>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="6513" citStr="Mohri, 2009" startWordPosition="1001" endWordPosition="1002">x with minimal edit-distance to the input word:2 bestLev(w) = arg min dLev(w, v) (2) vELex Ideally, the image of a word w under bestLev will itself be the canonical cognate sought,3 leading to conflation of all strings which share a common image under bestLev: w —Lev v :⇔ bestLev(w) = bestLev(v) (3) The function bestLev(w) : A* → Lex can be computed using a variant of the Dijkstra algorithm (Dijkstra, 1959) even when the lexicon is infinite (as in the case of productive nominal composition in German) whenever the set Lex can be represented by a finite-state acceptor (Mohri, 2002; Allauzen and Mohri, 2009; Jurish, 2010). For current purposes, we used the (infinite) input language of the TAGH morphology transducer (Geyken and Hanneforth, 2006) stripped of proper names, abbreviations, and foreign-language material to approximate Lex. 2.3 Rewrite Transducer While the simple edit distance conflation technique from the previous section is quite powerful and requires for its implementation only a lexicon of extant forms, the Levenshtein distance itself appears in many cases too coarse to function as a reliable predictor of etymological relations, since each edit operation (deletion, insertion, or su</context>
<context position="9257" citStr="Mohri, 2009" startWordPosition="1431" endWordPosition="1432">08). In particular, phonetic phenomena such as schwa deletion, vowel shift, voicing alternation, and articulatory location shift are easily captured by such rules. Of the 306 heuristic rewrite rules, 131 manipulate consonant-like strings, 115 deal with vowellike strings, and 14 operate directly on syllablelike units. The remaining 46 rules define expansions for explicitly marked elisions and unrecognized input. Some examples of rules used by the rewrite transducer are given in Table 2. Formally, the rewrite transducer Arw defines a pseudo-metric QArw� : A* x A* —* 1[8,,,, on all string pairs (Mohri, 2009). Assuming the non-negative tropical semiring (Simon, 1987) is used to represent transducer weights, analagous to the transducer representation of the Levenshtein metric (Allauzen and Mohri, 2009), the rewrite pseudo-metric can be used as a drop-in replacement for the Levenshtein distance in Equations (2) and (3), yielding Equations (4) and (5): bestrw(w) = arg min QArw�(w, v) (4) VELex w —rw v :.&lt;---&gt; bestrw(w) = bestrw(v) (5) 3 Evaluation 3.1 Test Corpus The conflation techniques described above were tested on a corpus of historical German verse extracted from the quotation evidence in a sin</context>
</contexts>
<marker>Mohri, 2009</marker>
<rawString>Mehryar Mohri. 2009. Weighted automata algorithms. In Handbook of Weighted Automata, Monographs in Theoretical Computer Science, pages 213–254. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor M¨ohler</author>
<author>Antje Schweitzer</author>
<author>Mark Breitenb¨ucher</author>
</authors>
<title>IMS German Festival manual, version 1.2.</title>
<date>2001</date>
<institution>Institute for Natural Language Processing, University of Stuttgart.</institution>
<marker>M¨ohler, Schweitzer, Breitenb¨ucher, 2001</marker>
<rawString>Gregor M¨ohler, Antje Schweitzer, and Mark Breitenb¨ucher, 2001. IMS German Festival manual, version 1.2. Institute for Natural Language Processing, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
</authors>
<title>A guided tour to approximate string matching.</title>
<date>2001</date>
<journal>ACM Computing Surveys,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="5606" citStr="Navarro, 2001" startWordPosition="838" endWordPosition="839">rical variation would effect a pronunciation difference in synchronic forms, that variation will remain uncaptured by a phonetic conflation technique. Examples of such phonetically salient variations with respect to the simplified IMS German Festival rule-set include guot/gut “good”, liecht/licht “light”, tiuvel/teufel “devil”, and wolln/wollen “want”. In order to accommodate graphematic variation phenomena beyond those for which strict phonetic identity of the variant forms obtains, we may employ an approximate search strategy based on the simple Levenshtein edit distance (Levenshtein, 1966; Navarro, 2001). Formally, let Lex C A* be the lexicon of all extant forms, and let dLev : A* x A* → N represent the Levenshtein distance over grapheme strings, then define for every input word w E A* the “best” synchronic equivalent 1P is a finite phonetic alphabet. bestLev(w) as the unique extant word v E Lex with minimal edit-distance to the input word:2 bestLev(w) = arg min dLev(w, v) (2) vELex Ideally, the image of a word w under bestLev will itself be the canonical cognate sought,3 leading to conflation of all strings which share a common image under bestLev: w —Lev v :⇔ bestLev(w) = bestLev(v) (3) The</context>
</contexts>
<marker>Navarro, 2001</marker>
<rawString>Gonzalo Navarro. 2001. A guided tour to approximate string matching. ACM Computing Surveys, 33(1):31–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="13589" citStr="Oflazer, 1996" startWordPosition="2164" endWordPosition="2165"> are given in Table 3. The Levenshtein edit-distance technique is at a clear disadvantage here, roughly 150 times slower than the phonetic technique and 40 times slower than the specialized heuristic rewrite transducer. This effect is assumedly due to the density of the search space (which is maximal for an unrestricted Levenshtein editor), since the gfsmxl greedy k-best search of a Levenshtein transducer cascade generates at least |A |configurations per character, and a single backtracking step requires an additional 3|A| heap extractions (Jurish, 2010). Use of specialized lookup algorithms (Oflazer, 1996) might ameliorate such problems. Qualitative results for several conflation techniques with respect to the DWB verse test corpus are given in Table 4. An additional conflation relation “Id” using strict identity of grapheme strings 4pho, bestLev and bestrw for the phonetic, Levenshtein, and heuristic rewrite transducer methods respectively Method Time Throughput Pho 1.82 sec 7322 tok/sec Lev 278.03 sec 48 tok/sec rw 7.02 sec 1898 tok/sec Table 3: Processing time for elementary canonicalization functions (w —Id v :.#&gt; w = v) was tested to provide a baseline for the methods described in Section </context>
</contexts>
<marker>Oflazer, 1996</marker>
<rawString>Kemal Oflazer. 1996. Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction. Computational Linguistics, 22(1):73–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="1202" citStr="Porter, 1980" startWordPosition="161" endWordPosition="162">ir performance on an information retrieval task over a manually annotated corpus of historical German verse. 1 Introduction Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as document indexing systems (Sokirko, 2003; Cafarella and Cutting, 2004), part-of-speech taggers (DeRose, 1988; Brill, 1992; Schmid, 1994), simple word stemmers (Lovins, 1968; Porter, 1980), or more sophisticated morphological analyzers (Geyken and Hanneforth, 2006; Clematide, 2008). When adopting historical text into such a system, one of the most crucial tasks is the association of one or more extant equivalents with each word of the input text: synchronically active types which best represent the relevant features of the input word. Which features are considered “relevant” here depends on the application in question: for a lemmatization task only the root lexeme is relevant, whereas syntactic parsing may require additional morphosyntactic features. For current purposes, extan</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="1151" citStr="Schmid, 1994" startWordPosition="154" endWordPosition="155">onically active canonical cognates and evaluate their performance on an information retrieval task over a manually annotated corpus of historical German verse. 1 Introduction Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as document indexing systems (Sokirko, 2003; Cafarella and Cutting, 2004), part-of-speech taggers (DeRose, 1988; Brill, 1992; Schmid, 1994), simple word stemmers (Lovins, 1968; Porter, 1980), or more sophisticated morphological analyzers (Geyken and Hanneforth, 2006; Clematide, 2008). When adopting historical text into such a system, one of the most crucial tasks is the association of one or more extant equivalents with each word of the input text: synchronically active types which best represent the relevant features of the input word. Which features are considered “relevant” here depends on the application in question: for a lemmatization task only the root lexeme is relevant, whereas syntactic parsing may require additional mo</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imre Simon</author>
</authors>
<title>The nondeterministic complexity of finite automata.</title>
<date>1987</date>
<booktitle>Instituto de Matem´atica e Estatistica da Universidade de S˜ao Paulo.</booktitle>
<tech>Technical Report RT-MAP8073,</tech>
<contexts>
<context position="9316" citStr="Simon, 1987" startWordPosition="1438" endWordPosition="1439">on, vowel shift, voicing alternation, and articulatory location shift are easily captured by such rules. Of the 306 heuristic rewrite rules, 131 manipulate consonant-like strings, 115 deal with vowellike strings, and 14 operate directly on syllablelike units. The remaining 46 rules define expansions for explicitly marked elisions and unrecognized input. Some examples of rules used by the rewrite transducer are given in Table 2. Formally, the rewrite transducer Arw defines a pseudo-metric QArw� : A* x A* —* 1[8,,,, on all string pairs (Mohri, 2009). Assuming the non-negative tropical semiring (Simon, 1987) is used to represent transducer weights, analagous to the transducer representation of the Levenshtein metric (Allauzen and Mohri, 2009), the rewrite pseudo-metric can be used as a drop-in replacement for the Levenshtein distance in Equations (2) and (3), yielding Equations (4) and (5): bestrw(w) = arg min QArw�(w, v) (4) VELex w —rw v :.&lt;---&gt; bestrw(w) = bestrw(v) (5) 3 Evaluation 3.1 Test Corpus The conflation techniques described above were tested on a corpus of historical German verse extracted from the quotation evidence in a single volume of the digital first edition of the dictionary D</context>
</contexts>
<marker>Simon, 1987</marker>
<rawString>Imre Simon. 1987. The nondeterministic complexity of finite automata. Technical Report RT-MAP8073, Instituto de Matem´atica e Estatistica da Universidade de S˜ao Paulo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexey Sokirko</author>
</authors>
<title>A technical overview of DWDS/dialing concordance. Talk delivered at the meeting Computational linguistics and intellectual technologies,</title>
<date>2003</date>
<location>Protvino, Russia.</location>
<contexts>
<context position="1055" citStr="Sokirko, 2003" startWordPosition="141" endWordPosition="142">n this paper, we present three methods for associating unknown historical word forms with synchronically active canonical cognates and evaluate their performance on an information retrieval task over a manually annotated corpus of historical German verse. 1 Introduction Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as document indexing systems (Sokirko, 2003; Cafarella and Cutting, 2004), part-of-speech taggers (DeRose, 1988; Brill, 1992; Schmid, 1994), simple word stemmers (Lovins, 1968; Porter, 1980), or more sophisticated morphological analyzers (Geyken and Hanneforth, 2006; Clematide, 2008). When adopting historical text into such a system, one of the most crucial tasks is the association of one or more extant equivalents with each word of the input text: synchronically active types which best represent the relevant features of the input word. Which features are considered “relevant” here depends on the application in question: for a lemmatiz</context>
</contexts>
<marker>Sokirko, 2003</marker>
<rawString>Alexey Sokirko. 2003. A technical overview of DWDS/dialing concordance. Talk delivered at the meeting Computational linguistics and intellectual technologies, Protvino, Russia.</rawString>
</citation>
<citation valid="true">
<date>2008</date>
<booktitle>Text Resources and Lexical Knowledge. Mouton de Gruyter,</booktitle>
<editor>Angelika Storrer, Alexander Geyken, Alexander Siebert, and Kay-Michael W¨urzner, editors.</editor>
<location>Berlin.</location>
<marker>2008</marker>
<rawString>Angelika Storrer, Alexander Geyken, Alexander Siebert, and Kay-Michael W¨urzner, editors. 2008. Text Resources and Lexical Knowledge. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworth-Heinemann,</publisher>
<location>Newton, MA.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. Butterworth-Heinemann, Newton, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>