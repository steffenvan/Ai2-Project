<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.976253">
Bilingual Data Cleaning for SMT using Graph-based Random Walk∗
</title>
<author confidence="0.998936">
Lei Cui†, Dongdong Zhang‡, Shujie Liu‡, Mu Li‡, and Ming Zhou‡
</author>
<affiliation confidence="0.93394675">
†School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
leicui@hit.edu.cn
‡Microsoft Research Asia, Beijing, China
</affiliation>
<email confidence="0.998777">
{dozhang,shujliu,muli,mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999598190476191">
The quality of bilingual data is a key factor
in Statistical Machine Translation (SMT).
Low-quality bilingual data tends to pro-
duce incorrect translation knowledge and
also degrades translation modeling per-
formance. Previous work often used su-
pervised learning methods to filter low-
quality data, but a fair amount of human
labeled examples are needed which are
not easy to obtain. To reduce the re-
liance on labeled examples, we propose
an unsupervised method to clean bilin-
gual data. The method leverages the mu-
tual reinforcement between the sentence
pairs and the extracted phrase pairs, based
on the observation that better sentence
pairs often lead to better phrase extraction
and vice versa. End-to-end experiments
show that the proposed method substan-
tially improves the performance in large-
scale Chinese-to-English translation tasks.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995475142857143">
Statistical machine translation (SMT) depends on
the amount of bilingual data and its quality. In
real-world SMT systems, bilingual data is often
mined from the web where low-quality data is in-
evitable. The low-quality bilingual data degrades
the quality of word alignment and leads to the in-
correct phrase pairs, which will hurt the transla-
tion performance of phrase-based SMT systems
(Koehn et al., 2003; Och and Ney, 2004). There-
fore, it is very important to exploit data quality in-
formation to improve the translation modeling.
Previous work on bilingual data cleaning often
involves some supervised learning methods. Sev-
eral bilingual data mining systems (Resnik and
</bodyText>
<footnote confidence="0.9028525">
∗This work has been done while the first author was visit-
ing Microsoft Research Asia.
</footnote>
<note confidence="0.642298">
Smith, 2003; Shi et al., 2006; Munteanu and
Marcu, 2005; Jiang et al., 2009) have a post-
</note>
<bodyText confidence="0.999779461538462">
processing step for data cleaning. Maximum en-
tropy or SVM based classifiers are built to filter
some non-parallel data or partial-parallel data. Al-
though these methods can filter some low-quality
bilingual data, they need sufficient human labeled
training instances to build the model, which may
not be easy to acquire.
To this end, we propose an unsupervised ap-
proach to clean the bilingual data. It is intuitive
that high-quality parallel data tends to produce
better phrase pairs than low-quality data. Mean-
while, it is also observed that the phrase pairs that
appear frequently in the bilingual corpus are more
reliable than less frequent ones because they are
more reusable, hence most good sentence pairs are
prone to contain more frequent phrase pairs (Fos-
ter et al., 2006; Wuebker et al., 2010). This kind of
mutual reinforcement fits well into the framework
of graph-based random walk. When a phrase pair
p is extracted from a sentence pair s, s is consid-
ered casting a vote for p. The higher the number
of votes a phrase pair has, the more reliable of the
phrase pair. Similarly, the quality of the sentence
pair s is determined by the number of votes casted
by the extracted phrase pairs from s.
In this paper, a PageRank-style random walk al-
gorithm (Brin and Page, 1998; Mihalcea and Ta-
rau, 2004; Wan et al., 2007) is conducted to itera-
tively compute the importance score of each sen-
tence pair that indicates its quality: the higher the
better. Unlike other data filtering methods, our
proposed method utilizes the importance scores
of sentence pairs as fractional counts to calculate
the phrase translation probabilities based on Maxi-
mum Likelihood Estimation (MLE), thereby none
of the bilingual data is filtered out. Experimen-
tal results show that our proposed approach sub-
stantially improves the performance in large-scale
Chinese-to-English translation tasks.
</bodyText>
<page confidence="0.971012">
340
</page>
<note confidence="0.5289265">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340–345,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.960806" genericHeader="method">
2 The Proposed Approach
</sectionHeader>
<subsectionHeader confidence="0.997645">
2.1 Graph-based random walk
</subsectionHeader>
<bodyText confidence="0.999980923076923">
Graph-based random walk is a general algorithm
to approximate the importance of a vertex within
the graph in a global view. In our method, the ver-
tices denote the sentence pairs and phrase pairs.
The importance of each vertex is propagated to
other vertices along the edges. Depending on dif-
ferent scenarios, the graph can take directed or
undirected, weighted or un-weighted forms. Start-
ing from the initial scores assigned in the graph,
the algorithm is applied to recursively compute the
importance scores of vertices until it converges, or
the difference between two consecutive iterations
falls below a pre-defined threshold.
</bodyText>
<subsectionHeader confidence="0.999169">
2.2 Graph construction
</subsectionHeader>
<bodyText confidence="0.999980363636364">
Given the sentence pairs that are word-aligned
automatically, an undirected, weighted bipartite
graph is constructed which maps the sentence
pairs and the extracted phrase pairs to the ver-
tices. An edge between a sentence pair vertex and
a phrase pair vertex is added if the phrase pair can
be extracted from the sentence pair. Mutual re-
inforcement scores are defined on edges, through
which the importance scores are propagated be-
tween vertices. Figure 1 illustrates the graph struc-
ture. Formally, the bipartite graph is defined as:
</bodyText>
<equation confidence="0.996877">
G = (V,E)
</equation>
<bodyText confidence="0.615609">
where V = S U P is the vertex set, S = {si|1 &lt;
</bodyText>
<equation confidence="0.941233777777778">
i &lt; n} is the set of all sentence pairs. P =
{pj|1 &lt; j &lt; m} is the set of all phrase pairs
which are extracted from S based on the word
alignment. E is the edge set in which the edges
are between S and P, thereby E = {(si, pj)|si E
S, pj E P, 0(si, pj) = 1}.
�
1 if pj can be extracted from si
0(si, pj) =0 otherwise
</equation>
<subsectionHeader confidence="0.999258">
2.3 Graph parameters
</subsectionHeader>
<bodyText confidence="0.992247">
For sentence-phrase mutual reinforcement, a non-
negative score r(si, pj) is defined using the stan-
dard TF-IDF formula:
</bodyText>
<subsectionHeader confidence="0.520986">
Phrase Pair Vertices
Sentence Pair Vertices
</subsectionHeader>
<figureCaption confidence="0.945016">
Figure 1: The circular nodes stand for S and
</figureCaption>
<bodyText confidence="0.948671666666667">
square nodes stand for P. The lines capture the
sentence-phrase mutual reinforcement.
where PF(si, pj) is the phrase pair frequency in
a sentence pair and IPF(pj) is the inverse phrase
pair frequency of pj in the whole bilingual corpus.
r(si, pj) is abbreviated as rij.
Inspired by (Brin and Page, 1998; Mihalcea
and Tarau, 2004; Wan et al., 2007), we com-
pute the importance scores of sentence pairs and
phrase pairs using a PageRank-style algorithm.
The weights rij are leveraged to reflect the rela-
tionships between two types of vertices. Let u(si)
and v(pj) denote the scores of a sentence pair ver-
tex and a phrase pair vertex. They are computed
iteratively by:
</bodyText>
<equation confidence="0.963870833333333">
� rij v(pj)
u(si) = (1−d)+dx EkEM(pj) rkj
jEN(si)
rij
Eu(si)
kEN(si) rik
</equation>
<bodyText confidence="0.998097888888889">
where d is empirically set to the default value 0.85
that is same as the original PageRank, N(si) =
{j|(si,pj) E E}, M(pj) = {i|(si,pj) E E}.
The detailed process is illustrated in Algorithm 1.
Algorithm 1 iteratively updates the scores of sen-
tence pairs and phrase pairs (lines 10-26). The
computation ends when difference between two
consecutive iterations is lower than a pre-defined
threshold δ (10−12 in this study).
</bodyText>
<subsectionHeader confidence="0.982138">
2.4 Parallelization
</subsectionHeader>
<equation confidence="0.96085225">
S1
S2
S3
P2
P1
P3
P4
P5
P6
�
v(pj) = (1−d)+dx
jEM(pj)
</equation>
<bodyText confidence="0.3479972">
r(si,pj) = When the random walk runs on some large bilin-
� PF(si,pj)XIPF(pj) if gual corpora, even filtering phrase pairs that ap-
0(si,pj) = 1 pear only once would still require several days of
Ep&apos;∈{p|φ(si,p)=1} PF(si,p�)XIPF(p&apos;) CPU time for a number of iterations. To over-
0 otherwise come this problem, we use a distributed algorithm
</bodyText>
<page confidence="0.939613">
341
</page>
<listItem confidence="0.765892428571428">
Algorithm 1 Modified Random Walk
1: for all i ∈ {0 ... |S |− 1} do
2: u(si)(0) ← 1
3: end for
4: for all j ∈ {0 ... |P |− 1} do
5: v(pj)(0) ← 1
6: end for
7: δ ← Infinity
8: E ← threshold
9: n ← 1
10: while δ &gt; E do
11: for all i ∈ {0 ... |S |− 1} do
12: F(si) ← 0
13: for all j ∈ N(si) do
</listItem>
<figure confidence="0.970617681818182">
r%j
14: F(si) ← F(si) + v(pj )(n−1)
Ek∈M(pj) rkj
15: end for
16: u(si)(n) ← (1 − d) + d · F(si)
17: end for
18: for all j ∈ {0 ... |P |− 1} do
19: G(pj) ← 0
20: for all i ∈ M(pj) do
r%j
21: G(pj) ← G(pj) + � · u(si)(n−1)
k∈N(s%) r%k
22: end for
23: v(pj)(n) ← (1 − d) + d · G(pj)
24: end for
25: δ ← max(4u(si)||S|−1
i=1 , 4v(pj)||P |−1
j=1 )
26: n ← n + 1
27: end while
28: return u(si)(n)||S|−1
i=0
</figure>
<bodyText confidence="0.999464923076923">
based on the iterative computation in the Sec-
tion 2.3. Before the iterative computation starts,
the sum of the outlink weights for each vertex
is computed first. The edges are randomly par-
titioned into sets of roughly equal size. Each
edge (si, pj) can generate two key-value pairs
in the format (si, rij) and (pj, rij). The pairs
with the same key are summed locally and ac-
cumulated across different machines. Then, in
each iteration, the score of each vertex is up-
dated according to the sum of the normalized
inlink weights. The key-value pairs are gener-
ated in the format (si, k · v(pj)) and
</bodyText>
<equation confidence="0.99609225">
EEM(pj) rkj
rik
(p C&gt; rij · u(si)). These key-value pairs
&apos;�&apos; rkEN(si)
</equation>
<bodyText confidence="0.999893428571428">
are also randomly partitioned and summed across
different machines. Since long sentence pairs usu-
ally extract more phrase pairs, we need to normal-
ize the importance scores based on the sentence
length. The algorithm fits well into the MapRe-
duce programming model (Dean and Ghemawat,
2008) and we use it as our implementation.
</bodyText>
<subsectionHeader confidence="0.932218">
2.5 Integration into translation modeling
</subsectionHeader>
<bodyText confidence="0.999786777777778">
After sufficient number of iterations, the impor-
tance scores of sentence pairs (i.e., u(si)) are ob-
tained. Instead of simple filtering, we use the
scores of sentence pairs as the fractional counts to
re-estimate the translation probabilities of phrase
pairs. Given a phrase pair p = ( ¯f, ¯e), A(¯f) and
B(¯e) indicate the sets of sentences that f and e¯
appear. Then the translation probability is defined
as:
</bodyText>
<equation confidence="0.9986445">
E&apos;EA(¯f)∩B(¯e) u(si) X ci ( f, e)
PCW(¯f |¯e) _ — z Ej∈B(¯e) u(sj) X cj(¯e)
</equation>
<bodyText confidence="0.999972666666667">
where ci(·) denotes the count of the phrase or
phrase pair in si. PCW(¯f|¯e) and PCW(¯e |¯f) are
named as Corpus Weighting (CW) based transla-
tion probability, which are integrated into the log-
linear model in addition to the conventional phrase
translation probabilities (Koehn et al., 2003).
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.980619">
3.1 Setup
</subsectionHeader>
<bodyText confidence="0.999736818181818">
We evaluated our bilingual data cleaning ap-
proach on large-scale Chinese-to-English machine
translation tasks. The bilingual data we used
was mainly mined from the web (Jiang et al.,
2009)1, as well as the United Nations parallel cor-
pus released by LDC and the parallel corpus re-
leased by China Workshop on Machine Transla-
tion (CWMT), which contain around 30 million
sentence pairs in total after removing duplicated
ones. The development data and testing data is
shown in Table 1.
</bodyText>
<table confidence="0.999859333333333">
Data Set #Sentences Source
NIST 2003 (dev) 919 open test
NIST 2005 (test) 1,082 open test
NIST 2006 (test) 1,664 open test
NIST 2008 (test) 1,357 open test
CWMT 2008 (test) 1,006 open test
In-house dataset 1 (test) 1,002 web data
In-house dataset 2 (test) 5,000 web data
In-house dataset 3 (test) 2,999 web data
</table>
<tableCaption confidence="0.840557">
Table 1: Development and testing data used in the
experiments.
</tableCaption>
<bodyText confidence="0.999887142857143">
A phrase-based decoder was implemented
based on inversion transduction grammar (Wu,
1997). The performance of this decoder is simi-
lar to the state-of-the-art phrase-based decoder in
Moses, but the implementation is more straight-
forward. We use the following feature functions
in the log-linear model:
</bodyText>
<footnote confidence="0.991808">
1Although supervised data cleaning has been done in the
post-processing, the corpus still contains a fair amount of
noisy data based on our random sampling.
</footnote>
<page confidence="0.990759">
342
</page>
<table confidence="0.996807">
dev NIST 2005 NIST 2006 NIST 2008 CWMT 2008 IH 1 IH 2 IH 3
baseline 41.24 37.34 35.20 29.38 31.14 24.29 22.61 24.19
(Wuebker et al., 2010) 41.20 37.48 35.30 29.33 31.10 24.33 22.52 24.18
-0.25M 41.28 37.62 35.31 29.70 31.40 24.52 22.69 24.64
-0.5M 41.45 37.71 35.52 29.76 31.77 24.64 22.68 24.69
-1M 41.28 37.41 35.28 29.65 31.73 24.23 23.06 24.20
+CW 41.75 38.08 35.84 30.03 31.82 25.23 23.18 24.80
</table>
<tableCaption confidence="0.96206">
Table 2: BLEU(%) of Chinese-to-English translation tasks on multiple testing datasets (p &lt; 0.05), where
</tableCaption>
<bodyText confidence="0.914269333333333">
”-numberM” denotes we simply filter number million low scored sentence pairs from the bilingual data
and use others to extract the phrase table. ”CW” means the corpus weighting feature, which incorporates
sentence scores from random walk as fractional counts to re-estimate the phrase translation probabilities.
</bodyText>
<listItem confidence="0.9985665">
• phrase translation probabilities and lexical
weights in both directions (4 features);
• 5-gram language model with Kneser-Ney
smoothing (1 feature);
• lexicalized reordering model (1 feature);
• phrase count and word count (2 features).
</listItem>
<bodyText confidence="0.999969941176471">
The translation model was trained over the
word-aligned bilingual corpus conducted by
GIZA++ (Och and Ney, 2003) in both directions,
and the diag-grow-final heuristic was used to re-
fine the symmetric word alignment. The language
model was trained on the LDC English Gigaword
Version 4.0 plus the English part of the bilingual
corpus. The lexicalized reordering model (Xiong
et al., 2006) was trained over the 40% randomly
sampled sentence pairs from our parallel data.
Case-insensitive BLEU4 (Papineni et al., 2002)
was used as the evaluation metric. The parame-
ters of the log-linear model are tuned by optimiz-
ing BLEU on the development data using MERT
(Och, 2003). Statistical significance test was per-
formed using the bootstrap re-sampling method
proposed by Koehn (2004).
</bodyText>
<subsectionHeader confidence="0.99668">
3.2 Baseline
</subsectionHeader>
<bodyText confidence="0.936048705882353">
The experimental results are shown in Table 2. In
the baseline system, the phrase pairs that appear
only once in the bilingual data are simply dis-
carded because most of them are noisy. In ad-
dition, the fix-discount method in (Foster et al.,
2006) for phrase table smoothing is also used.
This implementation makes the baseline system
perform much better and the model size is much
smaller. In fact, the basic idea of our ”one count”
cutoff is very similar to the idea of ”leaving-one-
out” in (Wuebker et al., 2010). The results show
Figure 2: The left one is the non-literal translation
in our bilingual corpus. The right one is the literal
translation made by human for comparison.
that the ”leaving-one-out” method performs al-
most the same as our baseline, thereby cannot
bring other benefits to the system.
</bodyText>
<subsectionHeader confidence="0.958944">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.978976076923077">
We evaluate the proposed bilingual data clean-
ing method by incorporating sentence scores into
translation modeling. In addition, we also com-
pare with several settings that filtering low-quality
sentence pairs from the bilingual data based on
the importance scores. The last N = { 0.25M,
0.5M, 1M } sentence pairs are filtered before the
modeling process. Although the simple bilin-
gual data filtering can improve the performance on
some datasets, it is difficult to determine the bor-
der line and translation performance is fluctuated.
One main reason is in the proposed random walk
approach, the bilingual sentence pairs with non-
literal translations may get lower scores because
they appear less frequently compared with those
literal translations. Crudely filtering out these data
may degrade the translation performance. For ex-
ample, we have a sentence pair in the bilingual
corpus shown in the left part of Figure 2. Although
the translation is correct in this situation, translat-
ing the Chinese word ”lingyu” to ”waters” appears
very few times since the common translations are
”areas” or ”fields”. However, simply filtering out
this kind of sentence pairs may lead to some loss
of native English expressions, thereby the trans-
weijing tansuo de xin lingyu
</bodyText>
<equation confidence="0.58782525">
未经 探索 的 新 领域
uncharted waters
未经 探索 的 新 领域
unexplored new areas
</equation>
<page confidence="0.996803">
343
</page>
<bodyText confidence="0.999934888888889">
lation performance is unstable since both non-
parallel sentence pairs and non-literal but parallel
sentence pairs are filtered. Therefore, we use the
importance score of each sentence pair to estimate
the phrase translation probabilities. It consistently
brings substantial improvements compared to the
baseline, which demonstrates graph-based random
walk indeed improves the translation modeling
performance for our SMT system.
</bodyText>
<subsectionHeader confidence="0.959744">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999981090909091">
In (Goutte et al., 2012), they evaluated phrase-
based SMT systems trained on parallel data with
different proportions of synthetic noisy data. They
suggested that when collecting larger, noisy par-
allel data for training phrase-based SMT, clean-
ing up by trying to detect and remove incor-
rect alignments can actually degrade performance.
Our experimental results confirm their findings
on some datasets. Based on our method, some-
times filtering noisy data leads to unexpected re-
sults. The reason is two-fold: on the one hand,
the non-literal parallel data makes false positive in
noisy data detection; on the other hand, large-scale
SMT systems is relatively robust and tolerant to
noisy data, especially when we remove frequency-
1 phrase pairs. Therefore, we propose to integrate
the importance scores when re-estimating phrase
pair probabilities in this paper. The importance
scores can be considered as a kind of contribution
constraint, thereby high-quality parallel data con-
tributes more while noisy parallel data contributes
less.
</bodyText>
<sectionHeader confidence="0.998469" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999932625">
In this paper, we develop an effective approach
to clean the bilingual data using graph-based ran-
dom walk. Significant improvements on several
datasets are achieved in our experiments. For
future work, we will extend our method to ex-
plore the relationships of sentence-to-sentence and
phrase-to-phrase, which is beyond the existing
sentence-to-phrase mutual reinforcement.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995735">
We are especially grateful to Yajuan Duan, Hong
Sun, Nan Yang and Xilun Chen for the helpful dis-
cussions. We also thank the anonymous reviewers
for their insightful comments.
</bodyText>
<sectionHeader confidence="0.995952" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999494519230769">
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30(1):107–
117.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapre-
duce: simplified data processing on large clusters.
Communications of the ACM, 51(1):107–113.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 53–61, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Cyril Goutte, Marine Carpuat, and George Foster.
2012. The impact of sentence alignment errors on
phrase-based machine translation performance. In
Proceedings of AMTA 2012, San Diego, California,
October. Association for Machine Translation in the
Americas.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,
and Qingsheng Zhu. 2009. Mining bilingual data
from the web with adaptively learnt patterns. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 870–878, Suntec, Singapore, August.
Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003 Main Papers, pages
48–54, Edmonton, May-June. Association for Com-
putational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages
404–411, Barcelona, Spain, July. Association for
Computational Linguistics.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477–504.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.
</reference>
<page confidence="0.988488">
344
</page>
<reference confidence="0.999767541666667">
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Philip Resnik and Noah A Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349–380.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.
2006. A dom tree alignment model for mining paral-
lel data from the web. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 489–496, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for si-
multaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 552–559, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 475–484, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 521–528,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.999137">
345
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.684977">
<title confidence="0.989797">Data Cleaning for SMT using Graph-based Random</title>
<affiliation confidence="0.998741">of Computer Science and Harbin Institute of Technology, Harbin,</affiliation>
<address confidence="0.719181">Research Asia, Beijing,</address>
<abstract confidence="0.999118454545455">The quality of bilingual data is a key factor in Statistical Machine Translation (SMT). Low-quality bilingual data tends to produce incorrect translation knowledge and also degrades translation modeling performance. Previous work often used supervised learning methods to filter lowquality data, but a fair amount of human labeled examples are needed which are not easy to obtain. To reduce the reliance on labeled examples, we propose an unsupervised method to clean bilingual data. The method leverages the mutual reinforcement between the sentence pairs and the extracted phrase pairs, based on the observation that better sentence pairs often lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<journal>Computer networks and ISDN systems,</journal>
<volume>30</volume>
<issue>1</issue>
<pages>117</pages>
<contexts>
<context position="3311" citStr="Brin and Page, 1998" startWordPosition="524" endWordPosition="527">re more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence pair that indicates its quality: the higher the better. Unlike other data filtering methods, our proposed method utilizes the importance scores of sentence pairs as fractional counts to calculate the phrase translation probabilities based on Maximum Likelihood Estimation (MLE), thereby none of the bilingual data is filtered out. Experimental results show that our proposed approach substantially improves the performance in large-scale Chinese-to-English translation tasks. 340 </context>
<context position="6242" citStr="Brin and Page, 1998" startWordPosition="1014" endWordPosition="1017">i E S, pj E P, 0(si, pj) = 1}. � 1 if pj can be extracted from si 0(si, pj) =0 otherwise 2.3 Graph parameters For sentence-phrase mutual reinforcement, a nonnegative score r(si, pj) is defined using the standard TF-IDF formula: Phrase Pair Vertices Sentence Pair Vertices Figure 1: The circular nodes stand for S and square nodes stand for P. The lines capture the sentence-phrase mutual reinforcement. where PF(si, pj) is the phrase pair frequency in a sentence pair and IPF(pj) is the inverse phrase pair frequency of pj in the whole bilingual corpus. r(si, pj) is abbreviated as rij. Inspired by (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007), we compute the importance scores of sentence pairs and phrase pairs using a PageRank-style algorithm. The weights rij are leveraged to reflect the relationships between two types of vertices. Let u(si) and v(pj) denote the scores of a sentence pair vertex and a phrase pair vertex. They are computed iteratively by: � rij v(pj) u(si) = (1−d)+dx EkEM(pj) rkj jEN(si) rij Eu(si) kEN(si) rik where d is empirically set to the default value 0.85 that is same as the original PageRank, N(si) = {j|(si,pj) E E}, M(pj) = {i|(si,pj) E E}. The detailed process i</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1):107– 117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>Mapreduce: simplified data processing on large clusters.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<issue>1</issue>
<contexts>
<context position="9157" citStr="Dean and Ghemawat, 2008" startWordPosition="1575" endWordPosition="1578"> with the same key are summed locally and accumulated across different machines. Then, in each iteration, the score of each vertex is updated according to the sum of the normalized inlink weights. The key-value pairs are generated in the format (si, k · v(pj)) and EEM(pj) rkj rik (p C&gt; rij · u(si)). These key-value pairs &apos;�&apos; rkEN(si) are also randomly partitioned and summed across different machines. Since long sentence pairs usually extract more phrase pairs, we need to normalize the importance scores based on the sentence length. The algorithm fits well into the MapReduce programming model (Dean and Ghemawat, 2008) and we use it as our implementation. 2.5 Integration into translation modeling After sufficient number of iterations, the importance scores of sentence pairs (i.e., u(si)) are obtained. Instead of simple filtering, we use the scores of sentence pairs as the fractional counts to re-estimate the translation probabilities of phrase pairs. Given a phrase pair p = ( ¯f, ¯e), A(¯f) and B(¯e) indicate the sets of sentences that f and e¯ appear. Then the translation probability is defined as: E&apos;EA(¯f)∩B(¯e) u(si) X ci ( f, e) PCW(¯f |¯e) _ — z Ej∈B(¯e) u(sj) X cj(¯e) where ci(·) denotes the count of </context>
</contexts>
<marker>Dean, Ghemawat, 2008</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce: simplified data processing on large clusters. Communications of the ACM, 51(1):107–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrasetable smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>53--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2809" citStr="Foster et al., 2006" startWordPosition="431" endWordPosition="435">methods can filter some low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance s</context>
<context position="13449" citStr="Foster et al., 2006" startWordPosition="2272" endWordPosition="2275">ndomly sampled sentence pairs from our parallel data. Case-insensitive BLEU4 (Papineni et al., 2002) was used as the evaluation metric. The parameters of the log-linear model are tuned by optimizing BLEU on the development data using MERT (Och, 2003). Statistical significance test was performed using the bootstrap re-sampling method proposed by Koehn (2004). 3.2 Baseline The experimental results are shown in Table 2. In the baseline system, the phrase pairs that appear only once in the bilingual data are simply discarded because most of them are noisy. In addition, the fix-discount method in (Foster et al., 2006) for phrase table smoothing is also used. This implementation makes the baseline system perform much better and the model size is much smaller. In fact, the basic idea of our ”one count” cutoff is very similar to the idea of ”leaving-oneout” in (Wuebker et al., 2010). The results show Figure 2: The left one is the non-literal translation in our bilingual corpus. The right one is the literal translation made by human for comparison. that the ”leaving-one-out” method performs almost the same as our baseline, thereby cannot bring other benefits to the system. 3.3 Results We evaluate the proposed </context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George Foster, Roland Kuhn, and Howard Johnson. 2006. Phrasetable smoothing for statistical machine translation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 53–61, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Goutte</author>
<author>Marine Carpuat</author>
<author>George Foster</author>
</authors>
<title>The impact of sentence alignment errors on phrase-based machine translation performance.</title>
<date>2012</date>
<booktitle>In Proceedings of AMTA 2012,</booktitle>
<location>San Diego, California,</location>
<contexts>
<context position="15818" citStr="Goutte et al., 2012" startWordPosition="2650" endWordPosition="2653">me loss of native English expressions, thereby the transweijing tansuo de xin lingyu 未经 探索 的 新 领域 uncharted waters 未经 探索 的 新 领域 unexplored new areas 343 lation performance is unstable since both nonparallel sentence pairs and non-literal but parallel sentence pairs are filtered. Therefore, we use the importance score of each sentence pair to estimate the phrase translation probabilities. It consistently brings substantial improvements compared to the baseline, which demonstrates graph-based random walk indeed improves the translation modeling performance for our SMT system. 3.4 Discussion In (Goutte et al., 2012), they evaluated phrasebased SMT systems trained on parallel data with different proportions of synthetic noisy data. They suggested that when collecting larger, noisy parallel data for training phrase-based SMT, cleaning up by trying to detect and remove incorrect alignments can actually degrade performance. Our experimental results confirm their findings on some datasets. Based on our method, sometimes filtering noisy data leads to unexpected results. The reason is two-fold: on the one hand, the non-literal parallel data makes false positive in noisy data detection; on the other hand, large-</context>
</contexts>
<marker>Goutte, Carpuat, Foster, 2012</marker>
<rawString>Cyril Goutte, Marine Carpuat, and George Foster. 2012. The impact of sentence alignment errors on phrase-based machine translation performance. In Proceedings of AMTA 2012, San Diego, California, October. Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Shiquan Yang</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Qingsheng Zhu</author>
</authors>
<title>Mining bilingual data from the web with adaptively learnt patterns.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>870--878</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2018" citStr="Jiang et al., 2009" startWordPosition="302" endWordPosition="305">ity bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗This work has been done while the first author was visiting Microsoft Research Asia. Smith, 2003; Shi et al., 2006; Munteanu and Marcu, 2005; Jiang et al., 2009) have a postprocessing step for data cleaning. Maximum entropy or SVM based classifiers are built to filter some non-parallel data or partial-parallel data. Although these methods can filter some low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the</context>
<context position="10228" citStr="Jiang et al., 2009" startWordPosition="1752" endWordPosition="1755">translation probability is defined as: E&apos;EA(¯f)∩B(¯e) u(si) X ci ( f, e) PCW(¯f |¯e) _ — z Ej∈B(¯e) u(sj) X cj(¯e) where ci(·) denotes the count of the phrase or phrase pair in si. PCW(¯f|¯e) and PCW(¯e |¯f) are named as Corpus Weighting (CW) based translation probability, which are integrated into the loglinear model in addition to the conventional phrase translation probabilities (Koehn et al., 2003). 3 Experiments 3.1 Setup We evaluated our bilingual data cleaning approach on large-scale Chinese-to-English machine translation tasks. The bilingual data we used was mainly mined from the web (Jiang et al., 2009)1, as well as the United Nations parallel corpus released by LDC and the parallel corpus released by China Workshop on Machine Translation (CWMT), which contain around 30 million sentence pairs in total after removing duplicated ones. The development data and testing data is shown in Table 1. Data Set #Sentences Source NIST 2003 (dev) 919 open test NIST 2005 (test) 1,082 open test NIST 2006 (test) 1,664 open test NIST 2008 (test) 1,357 open test CWMT 2008 (test) 1,006 open test In-house dataset 1 (test) 1,002 web data In-house dataset 2 (test) 5,000 web data In-house dataset 3 (test) 2,999 web</context>
</contexts>
<marker>Jiang, Yang, Zhou, Liu, Zhu, 2009</marker>
<rawString>Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and Qingsheng Zhu. 2009. Mining bilingual data from the web with adaptively learnt patterns. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 870–878, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003 Main Papers,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edmonton, May-June.</location>
<contexts>
<context position="1589" citStr="Koehn et al., 2003" startWordPosition="232" endWordPosition="235">ften lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 1 Introduction Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗This work has been done while the first author was visiting Microsoft Research Asia. Smith, 2003; Shi et al., 2006; Munteanu and Marcu, 2005; Jiang et al., 2009) have a postprocessing step for data cleaning. Maximum entropy or SVM based classifiers are built to filter some non-parallel data or partial-parallel data. Although these</context>
<context position="10014" citStr="Koehn et al., 2003" startWordPosition="1719" endWordPosition="1722">entence pairs as the fractional counts to re-estimate the translation probabilities of phrase pairs. Given a phrase pair p = ( ¯f, ¯e), A(¯f) and B(¯e) indicate the sets of sentences that f and e¯ appear. Then the translation probability is defined as: E&apos;EA(¯f)∩B(¯e) u(si) X ci ( f, e) PCW(¯f |¯e) _ — z Ej∈B(¯e) u(sj) X cj(¯e) where ci(·) denotes the count of the phrase or phrase pair in si. PCW(¯f|¯e) and PCW(¯e |¯f) are named as Corpus Weighting (CW) based translation probability, which are integrated into the loglinear model in addition to the conventional phrase translation probabilities (Koehn et al., 2003). 3 Experiments 3.1 Setup We evaluated our bilingual data cleaning approach on large-scale Chinese-to-English machine translation tasks. The bilingual data we used was mainly mined from the web (Jiang et al., 2009)1, as well as the United Nations parallel corpus released by LDC and the parallel corpus released by China Workshop on Machine Translation (CWMT), which contain around 30 million sentence pairs in total after removing duplicated ones. The development data and testing data is shown in Table 1. Data Set #Sentences Source NIST 2003 (dev) 919 open test NIST 2005 (test) 1,082 open test NI</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003 Main Papers, pages 48–54, Edmonton, May-June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="13188" citStr="Koehn (2004)" startWordPosition="2228" endWordPosition="2229">stic was used to refine the symmetric word alignment. The language model was trained on the LDC English Gigaword Version 4.0 plus the English part of the bilingual corpus. The lexicalized reordering model (Xiong et al., 2006) was trained over the 40% randomly sampled sentence pairs from our parallel data. Case-insensitive BLEU4 (Papineni et al., 2002) was used as the evaluation metric. The parameters of the log-linear model are tuned by optimizing BLEU on the development data using MERT (Och, 2003). Statistical significance test was performed using the bootstrap re-sampling method proposed by Koehn (2004). 3.2 Baseline The experimental results are shown in Table 2. In the baseline system, the phrase pairs that appear only once in the bilingual data are simply discarded because most of them are noisy. In addition, the fix-discount method in (Foster et al., 2006) for phrase table smoothing is also used. This implementation makes the baseline system perform much better and the model size is much smaller. In fact, the basic idea of our ”one count” cutoff is very similar to the idea of ”leaving-oneout” in (Wuebker et al., 2010). The results show Figure 2: The left one is the non-literal translation</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>404--411</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3337" citStr="Mihalcea and Tarau, 2004" startWordPosition="528" endWordPosition="532">ce most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence pair that indicates its quality: the higher the better. Unlike other data filtering methods, our proposed method utilizes the importance scores of sentence pairs as fractional counts to calculate the phrase translation probabilities based on Maximum Likelihood Estimation (MLE), thereby none of the bilingual data is filtered out. Experimental results show that our proposed approach substantially improves the performance in large-scale Chinese-to-English translation tasks. 340 Proceedings of the 51st An</context>
<context position="6268" citStr="Mihalcea and Tarau, 2004" startWordPosition="1018" endWordPosition="1021">pj) = 1}. � 1 if pj can be extracted from si 0(si, pj) =0 otherwise 2.3 Graph parameters For sentence-phrase mutual reinforcement, a nonnegative score r(si, pj) is defined using the standard TF-IDF formula: Phrase Pair Vertices Sentence Pair Vertices Figure 1: The circular nodes stand for S and square nodes stand for P. The lines capture the sentence-phrase mutual reinforcement. where PF(si, pj) is the phrase pair frequency in a sentence pair and IPF(pj) is the inverse phrase pair frequency of pj in the whole bilingual corpus. r(si, pj) is abbreviated as rij. Inspired by (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007), we compute the importance scores of sentence pairs and phrase pairs using a PageRank-style algorithm. The weights rij are leveraged to reflect the relationships between two types of vertices. Let u(si) and v(pj) denote the scores of a sentence pair vertex and a phrase pair vertex. They are computed iteratively by: � rij v(pj) u(si) = (1−d)+dx EkEM(pj) rkj jEN(si) rij Eu(si) kEN(si) rik where d is empirically set to the default value 0.85 that is same as the original PageRank, N(si) = {j|(si,pj) E E}, M(pj) = {i|(si,pj) E E}. The detailed process is illustrated in Algorithm</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 404–411, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="1997" citStr="Munteanu and Marcu, 2005" startWordPosition="298" endWordPosition="301">s inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗This work has been done while the first author was visiting Microsoft Research Asia. Smith, 2003; Shi et al., 2006; Munteanu and Marcu, 2005; Jiang et al., 2009) have a postprocessing step for data cleaning. Maximum entropy or SVM based classifiers are built to filter some non-parallel data or partial-parallel data. Although these methods can filter some low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that app</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12526" citStr="Och and Ney, 2003" startWordPosition="2121" endWordPosition="2124">llion low scored sentence pairs from the bilingual data and use others to extract the phrase table. ”CW” means the corpus weighting feature, which incorporates sentence scores from random walk as fractional counts to re-estimate the phrase translation probabilities. • phrase translation probabilities and lexical weights in both directions (4 features); • 5-gram language model with Kneser-Ney smoothing (1 feature); • lexicalized reordering model (1 feature); • phrase count and word count (2 features). The translation model was trained over the word-aligned bilingual corpus conducted by GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The language model was trained on the LDC English Gigaword Version 4.0 plus the English part of the bilingual corpus. The lexicalized reordering model (Xiong et al., 2006) was trained over the 40% randomly sampled sentence pairs from our parallel data. Case-insensitive BLEU4 (Papineni et al., 2002) was used as the evaluation metric. The parameters of the log-linear model are tuned by optimizing BLEU on the development data using MERT (Och, 2003). Statistical significance test was performed u</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1609" citStr="Och and Ney, 2004" startWordPosition="236" endWordPosition="239">phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 1 Introduction Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗This work has been done while the first author was visiting Microsoft Research Asia. Smith, 2003; Shi et al., 2006; Munteanu and Marcu, 2005; Jiang et al., 2009) have a postprocessing step for data cleaning. Maximum entropy or SVM based classifiers are built to filter some non-parallel data or partial-parallel data. Although these methods can filter </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="13079" citStr="Och, 2003" startWordPosition="2213" endWordPosition="2214"> bilingual corpus conducted by GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The language model was trained on the LDC English Gigaword Version 4.0 plus the English part of the bilingual corpus. The lexicalized reordering model (Xiong et al., 2006) was trained over the 40% randomly sampled sentence pairs from our parallel data. Case-insensitive BLEU4 (Papineni et al., 2002) was used as the evaluation metric. The parameters of the log-linear model are tuned by optimizing BLEU on the development data using MERT (Och, 2003). Statistical significance test was performed using the bootstrap re-sampling method proposed by Koehn (2004). 3.2 Baseline The experimental results are shown in Table 2. In the baseline system, the phrase pairs that appear only once in the bilingual data are simply discarded because most of them are noisy. In addition, the fix-discount method in (Foster et al., 2006) for phrase table smoothing is also used. This implementation makes the baseline system perform much better and the model size is much smaller. In fact, the basic idea of our ”one count” cutoff is very similar to the idea of ”leav</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="12929" citStr="Papineni et al., 2002" startWordPosition="2184" endWordPosition="2187">thing (1 feature); • lexicalized reordering model (1 feature); • phrase count and word count (2 features). The translation model was trained over the word-aligned bilingual corpus conducted by GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The language model was trained on the LDC English Gigaword Version 4.0 plus the English part of the bilingual corpus. The lexicalized reordering model (Xiong et al., 2006) was trained over the 40% randomly sampled sentence pairs from our parallel data. Case-insensitive BLEU4 (Papineni et al., 2002) was used as the evaluation metric. The parameters of the log-linear model are tuned by optimizing BLEU on the development data using MERT (Och, 2003). Statistical significance test was performed using the bootstrap re-sampling method proposed by Koehn (2004). 3.2 Baseline The experimental results are shown in Table 2. In the baseline system, the phrase pairs that appear only once in the bilingual data are simply discarded because most of them are noisy. In addition, the fix-discount method in (Foster et al., 2006) for phrase table smoothing is also used. This implementation makes the baseline</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Cheng Niu</author>
<author>Ming Zhou</author>
<author>Jianfeng Gao</author>
</authors>
<title>A dom tree alignment model for mining parallel data from the web.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>489--496</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1971" citStr="Shi et al., 2006" startWordPosition="294" endWordPosition="297">low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗This work has been done while the first author was visiting Microsoft Research Asia. Smith, 2003; Shi et al., 2006; Munteanu and Marcu, 2005; Jiang et al., 2009) have a postprocessing step for data cleaning. Maximum entropy or SVM based classifiers are built to filter some non-parallel data or partial-parallel data. Although these methods can filter some low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that</context>
</contexts>
<marker>Shi, Niu, Zhou, Gao, 2006</marker>
<rawString>Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao. 2006. A dom tree alignment model for mining parallel data from the web. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 489–496, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>552--559</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3356" citStr="Wan et al., 2007" startWordPosition="533" endWordPosition="536">s are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence pair that indicates its quality: the higher the better. Unlike other data filtering methods, our proposed method utilizes the importance scores of sentence pairs as fractional counts to calculate the phrase translation probabilities based on Maximum Likelihood Estimation (MLE), thereby none of the bilingual data is filtered out. Experimental results show that our proposed approach substantially improves the performance in large-scale Chinese-to-English translation tasks. 340 Proceedings of the 51st Annual Meeting of the</context>
<context position="6287" citStr="Wan et al., 2007" startWordPosition="1022" endWordPosition="1025"> extracted from si 0(si, pj) =0 otherwise 2.3 Graph parameters For sentence-phrase mutual reinforcement, a nonnegative score r(si, pj) is defined using the standard TF-IDF formula: Phrase Pair Vertices Sentence Pair Vertices Figure 1: The circular nodes stand for S and square nodes stand for P. The lines capture the sentence-phrase mutual reinforcement. where PF(si, pj) is the phrase pair frequency in a sentence pair and IPF(pj) is the inverse phrase pair frequency of pj in the whole bilingual corpus. r(si, pj) is abbreviated as rij. Inspired by (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007), we compute the importance scores of sentence pairs and phrase pairs using a PageRank-style algorithm. The weights rij are leveraged to reflect the relationships between two types of vertices. Let u(si) and v(pj) denote the scores of a sentence pair vertex and a phrase pair vertex. They are computed iteratively by: � rij v(pj) u(si) = (1−d)+dx EkEM(pj) rkj jEN(si) rij Eu(si) kEN(si) rik where d is empirically set to the default value 0.85 that is same as the original PageRank, N(si) = {j|(si,pj) E E}, M(pj) = {i|(si,pj) E E}. The detailed process is illustrated in Algorithm 1. Algorithm 1 ite</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552–559, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="10986" citStr="Wu, 1997" startWordPosition="1881" endWordPosition="1882">ich contain around 30 million sentence pairs in total after removing duplicated ones. The development data and testing data is shown in Table 1. Data Set #Sentences Source NIST 2003 (dev) 919 open test NIST 2005 (test) 1,082 open test NIST 2006 (test) 1,664 open test NIST 2008 (test) 1,357 open test CWMT 2008 (test) 1,006 open test In-house dataset 1 (test) 1,002 web data In-house dataset 2 (test) 5,000 web data In-house dataset 3 (test) 2,999 web data Table 1: Development and testing data used in the experiments. A phrase-based decoder was implemented based on inversion transduction grammar (Wu, 1997). The performance of this decoder is similar to the state-of-the-art phrase-based decoder in Moses, but the implementation is more straightforward. We use the following feature functions in the log-linear model: 1Although supervised data cleaning has been done in the post-processing, the corpus still contains a fair amount of noisy data based on our random sampling. 342 dev NIST 2005 NIST 2006 NIST 2008 CWMT 2008 IH 1 IH 2 IH 3 baseline 41.24 37.34 35.20 29.38 31.14 24.29 22.61 24.19 (Wuebker et al., 2010) 41.20 37.48 35.30 29.33 31.10 24.33 22.52 24.18 -0.25M 41.28 37.62 35.31 29.70 31.40 24.</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Training phrase translation models with leaving-one-out.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>475--484</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2832" citStr="Wuebker et al., 2010" startWordPosition="436" endWordPosition="439">me low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence p</context>
<context position="11497" citStr="Wuebker et al., 2010" startWordPosition="1965" endWordPosition="1968"> in the experiments. A phrase-based decoder was implemented based on inversion transduction grammar (Wu, 1997). The performance of this decoder is similar to the state-of-the-art phrase-based decoder in Moses, but the implementation is more straightforward. We use the following feature functions in the log-linear model: 1Although supervised data cleaning has been done in the post-processing, the corpus still contains a fair amount of noisy data based on our random sampling. 342 dev NIST 2005 NIST 2006 NIST 2008 CWMT 2008 IH 1 IH 2 IH 3 baseline 41.24 37.34 35.20 29.38 31.14 24.29 22.61 24.19 (Wuebker et al., 2010) 41.20 37.48 35.30 29.33 31.10 24.33 22.52 24.18 -0.25M 41.28 37.62 35.31 29.70 31.40 24.52 22.69 24.64 -0.5M 41.45 37.71 35.52 29.76 31.77 24.64 22.68 24.69 -1M 41.28 37.41 35.28 29.65 31.73 24.23 23.06 24.20 +CW 41.75 38.08 35.84 30.03 31.82 25.23 23.18 24.80 Table 2: BLEU(%) of Chinese-to-English translation tasks on multiple testing datasets (p &lt; 0.05), where ”-numberM” denotes we simply filter number million low scored sentence pairs from the bilingual data and use others to extract the phrase table. ”CW” means the corpus weighting feature, which incorporates sentence scores from random w</context>
<context position="13716" citStr="Wuebker et al., 2010" startWordPosition="2319" endWordPosition="2322">gnificance test was performed using the bootstrap re-sampling method proposed by Koehn (2004). 3.2 Baseline The experimental results are shown in Table 2. In the baseline system, the phrase pairs that appear only once in the bilingual data are simply discarded because most of them are noisy. In addition, the fix-discount method in (Foster et al., 2006) for phrase table smoothing is also used. This implementation makes the baseline system perform much better and the model size is much smaller. In fact, the basic idea of our ”one count” cutoff is very similar to the idea of ”leaving-oneout” in (Wuebker et al., 2010). The results show Figure 2: The left one is the non-literal translation in our bilingual corpus. The right one is the literal translation made by human for comparison. that the ”leaving-one-out” method performs almost the same as our baseline, thereby cannot bring other benefits to the system. 3.3 Results We evaluate the proposed bilingual data cleaning method by incorporating sentence scores into translation modeling. In addition, we also compare with several settings that filtering low-quality sentence pairs from the bilingual data based on the importance scores. The last N = { 0.25M, 0.5M,</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Joern Wuebker, Arne Mauser, and Hermann Ney. 2010. Training phrase translation models with leaving-one-out. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 475–484, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>521--528</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="12801" citStr="Xiong et al., 2006" startWordPosition="2165" endWordPosition="2168">e translation probabilities and lexical weights in both directions (4 features); • 5-gram language model with Kneser-Ney smoothing (1 feature); • lexicalized reordering model (1 feature); • phrase count and word count (2 features). The translation model was trained over the word-aligned bilingual corpus conducted by GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The language model was trained on the LDC English Gigaword Version 4.0 plus the English part of the bilingual corpus. The lexicalized reordering model (Xiong et al., 2006) was trained over the 40% randomly sampled sentence pairs from our parallel data. Case-insensitive BLEU4 (Papineni et al., 2002) was used as the evaluation metric. The parameters of the log-linear model are tuned by optimizing BLEU on the development data using MERT (Och, 2003). Statistical significance test was performed using the bootstrap re-sampling method proposed by Koehn (2004). 3.2 Baseline The experimental results are shown in Table 2. In the baseline system, the phrase pairs that appear only once in the bilingual data are simply discarded because most of them are noisy. In addition, </context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 521–528, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>