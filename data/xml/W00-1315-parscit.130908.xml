<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.984995">
Empirical Term Weighting and Expansion Frequency
</title>
<author confidence="0.997034">
Kyoji Umemura
</author>
<affiliation confidence="0.9574115">
Toyohashi University of Technology
Toyohashi Aichi 441-8580 Japan
</affiliation>
<email confidence="0.970051">
umemura@tutics.tut.ac.jp
</email>
<note confidence="0.769212">
Kenneth W. Church
</note>
<address confidence="0.4830985">
AT&amp;T Labs-Research
180 Park Ave., Florham Park, NJ.
</address>
<email confidence="0.962817">
kwc@research.att.com
</email>
<sectionHeader confidence="0.993192" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99999803030303">
We propose an empirical method for estimating
term weights directly from relevance judgements,
avoiding various standard but potentially trouble-
some assumptions. It is common to assume, for ex-
ample, that weights vary with term frequency (tf)
and inverse document frequency (idf) in a particu-
lar way, e.g., tf • idf, but the fact that there are so
many variants of this formula in the literature sug-
gests that there remains considerable uncertainty
about these assumptions. Our method is similar to
the Berkeley regression method where labeled rel-
evance judgements are fit as a linear combination
of (transforms of) tf, idf, etc. Training meth-
ods not only improve performance, but also ex-
tend naturally to include additional factors such
as burstiness and query expansion. The proposed
histogram-based training method provides a sim-
ple way to model complicated interactions among
factors such as tf, idf, burstiness and expansion
frequency (a generalization of query expansion).
The correct handling of expanded term is realized
based on statistical information. Expansion fre-
quency dramatically improves performance from
a level comparable to BKJJBIDS, Berkeley&apos;s en-
try in the Japanese NACSIS NTCIR-1 evaluation
for short queries, to the level of JCB1, the top
system in the evaluation. JCB1 uses sophisti-
cated (and proprietary) natural language process-
ing techniques developed by Just System, a leader
in the Japanese word-processing industry. We are
encouraged that the proposed method, which is
simple to understand and replicate, can reach this
level of performance.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999849333333333">
An empirical method for estimating term weights
directly from relevance judgements is proposed.
The method is designed to make as few assump-
tions as possible. It is similar to Berkeley&apos;s use
of regression (Cooper et al., 1994) (Chen et al.,
1999) where labeled relevance judgements are fit
as a linear combination of (transforms of) tf, idf,
etc., but avoids potentially troublesome assump-
tions by introducing histogram methods. Terms
are grouped into bins. Weights are computed
based on the number of relevant and irrelevant
documents associated with each bin. The result-
</bodyText>
<listItem confidence="0.999906266666667">
• t: a term
• d: a document
• t f (t, d): term freq = # of instances of tin d
• df (t): doc freq = # of docs d with t f (t, d) &gt; 1
• N: # of documents in collection
• idf (t): inverse document freq:
• df (t, rel, t fo): # of relevant documents d with
t f (t, = tfo
• df (t, rel, t fo): # of irrelevant documents d
with t f (t, d) = tfo
• e f (t): expansion frequency = # docs d in
query expansion with tf(t, d) &gt; 1
• TF(t): standard notion of frequency in
corpus-based NLP: T F(t) = Ed t f (t, d)
• B(t): burstiness: B(t) = 1 if TO is large.
</listItem>
<tableCaption confidence="0.720881">
Table 1: Notation
</tableCaption>
<bodyText confidence="0.999890875">
ing weights usually lie between 0 and idf, which
is a surprise; standard formulas like tf idf would
assign values well outside this range.
The method extends naturally to include ad-
ditional factors such as query expansion. Terms
mentioned explicitly in the query receive much
larger weights than terms brought in via query
expansion. In addition, whether or not a term
t is mentioned explicitly in the query, if t ap-
pears in documents brought in by query expan-
sion (e (t) &gt; 1) then t will receive a much larger
weight than it would have otherwise (ef (t) = 0).
The interactions among these factors, however, are
complicated and collection dependent. It is safer
to use histogram methods than to impose unnec-
essary and potentially troublesome assumptions
such as normality and independence.
Under the vector space model, the score for a
document d and a query q is computed by sum-
ming a contribution for each term t over an ap-
propriate set of terms, T. T is often limited to
terms shared by both the document and the query
(minus stop words), though not always (e.g, query
expansion).
</bodyText>
<page confidence="0.995457">
117
</page>
<table confidence="0.864826923076923">
idf tf = 0 tf = 1 tf := 2 tf = 3 tf &gt; 4
12.89 -0.37 9.73 11.69 12.45 13.59
10.87 -0.49 8.00 9.95 11.47 1206. eq
9.79 -0.86 7.36 9.38 10.63 10.88
8.96 -0.60 6.26 7.99 8.99 9.41
7.75 -0.34 4.62 5.82 6.62 7.98
6.82 -1.26 3.94 6.05 7.59 8.98 Co
5.78 -0.83 3.16 5.17 5.77 7.00
4.74 -0.84 2.46 3.91 4.54 5.58 eq
3.85 -0.60 1.58 2.76 3.57 4.55
2.85 -1.02 1.00 1.72 2.55 3.96
1.78 -1.33 -0.06 1.05 2.46 4.50
0.88 -0.16 0.17 0.19 -0.10 -0.37
</table>
<figure confidence="0.962235923076923">
Train
2
9
2 1
-- 1
•
3.7-1-j 2 ---
•&amp;quot; 2i_1
-- -7 1
0 - 0 -- - 6
0 2 4 6 8 10 12
IDF
Test
</figure>
<tableCaption confidence="0.883715">
Table 2: Empirical estimates of A as a function of
</tableCaption>
<bodyText confidence="0.9572616">
tf and idf. Terms are assieed to bins based on
idf. The column labeled idf is the mean idf for
the terms in each bin. A is estimated separately for
each bin and each tf value, based on the labeled
relevance judgements.
</bodyText>
<equation confidence="0.9980125">
score„(d,q) = E t f (t, d) • idf (t)
tET
</equation>
<bodyText confidence="0.96459475">
Under the probabilistic retrieval model, docu-
ments are scored by summing a similar contribu-
tion for each term t.
In this work, we use A to refer to term weights.
</bodyText>
<equation confidence="0.995672">
score(d,q) = E A(t, d,q)
tET
</equation>
<bodyText confidence="0.999814625">
This paper will start by showing how to estimate A
from relevance judgements. Three parameteriza-
dons will be considered: (1) fit-G, (2) fit-B, which
introduces burstiness, and (3) fit-E, which intro-
duces expansion frequency. The evaluation section
shows that each model improves on the previous
one. But in addition to performance, we are also
interested in the interpretations of the parameters.
</bodyText>
<sectionHeader confidence="0.923454" genericHeader="method">
2 Supervised Training
</sectionHeader>
<bodyText confidence="0.99944225">
The statistical task is to compute A, our best esti-
mate of A, based on a training set. This paper will
use supervised methods where the training mate-
rials not only include a large number of documents
but also a few queries labeled with relevance judge-
ments.
To make the training task more manageable, it
is common practice to map the space of all terms
into a lower dimensional feature space. In other
words, instead of estimating a different A for each
term in the vocabulary, we can model A as a func-
tion of tf and idf and various other features of
</bodyText>
<figure confidence="0.8866575">
0 2 4 6 8 10 12
IDF
</figure>
<figureCaption confidence="0.934995083333333">
Figure 1: Empirical weights, A. Top panel shows
values in previous table. Most points fall between
the dashed lines (lower limit of A = 0 and upper
limit of A = idf). The plotting character denotes
tf. Note that the line with tf = 4 is above the
line with tf = 3, which is above the line with
tf = 2, and so on. The higher lines have larger
intercepts and larger slopes than the lower lines.
That is, when we fit A a(tf) + b(tf) - idf, with
separate regression coefficients, a(tf) and b(tf),
for each value of tf, we find that both a(tf) and
b(tf) increase with tf.
</figureCaption>
<bodyText confidence="0.999106722222222">
terms. In this way, all of the terms in a bin are
assigned the weight, A. The common practice,
for example, of assigning tf idf weights can be
interpreted as grouping all terms with the same
idf into a bin and assigning them all the same
weight, namely tf idf. Cooper and his colleagues
at Berkeley (Cooper et al., 1994) (Chen et al.,
1999) have been using regression methods to fit
A as a linear combination of idf, log(tf) and var-
ious other features. This method is also grouping
terms into bins based on their features and assign-
ing similar weights to terms with similar features.
In general, term weighting methods that are fit
to data are more flexible than weighting methods
that are not fit to data. We believe this additional
flexibility improves precision and recall (table 8).
Instead of multiple regression, though, we
choose a more empirical approach. Parametric as-
</bodyText>
<figure confidence="0.972860266666667">
137
_1-1
2
-
4
2
2
1 1
scorep(d, q) = E 1og2P (tire°
tET
P(t17.:-W)
CM
Co
ID
0.1
</figure>
<page confidence="0.98556">
118
</page>
<bodyText confidence="0.956748615384615">
where df (bin, rel, t f) is
df (bin, rel , t f) E E df ,t f)
tEb.in
Similarly, the denominator can be approximated
as:
i&apos;(bin,t fire° Pe, log2df (bin, ret, t f)
where df (bin, rel,t f) is
df (bin, rel, t f) E ; E df (t, rel, t f)
1&amp;quot;67-11 tEbin
Ar.ret is an estimate of the total number of relevant
documents. Since some queries have more rele-
vant documents than others, Nrei is computed by
averaging:
</bodyText>
<equation confidence="0.921439333333333">
RT
1Crrel -11,7iTti lyre&apos;
I I tEbin
</equation>
<bodyText confidence="0.9968825">
To ensure that grei + ./C/77j = N, where N is the
number of documents in the collection, we define
</bodyText>
<equation confidence="0.47364">
E N
</equation>
<bodyText confidence="0.999959086956522">
This estimation procedure is implemented with
the simple awk program in figure 2. The awk pro-
gram reads each line of the training file, which con-
tains a line for each term in each training query.
As described in table 3, each training line contains
25 fields. The first five fields contain df (t,rel,t f)
for five values of tf, , and the next five fields con-
tain df(t,rel,tf) for the same five values of tf.
The next two fields contain Nrei and N. As the
awk program reads each of these lines from the
training file, it assigns each term in each train-
ing query to a bin (based on idog2(df)1, except
when df &lt; 100), and maintains running sums of
the first dozen fields which are used for comput-
ing df (bin, rel , t f), df (bin, rel , t f), grel and ICr,
for five values of t f . Finally, after reading all the
training material, the program outputs the table
of As shown in table 2. The table contains a col-
umn for each of the five t f values and a row for
each of the dozen idf bins. Later, we will consider
more interesting binning rules that make use of
additional statistics such as burstiness and query
expansion.
</bodyText>
<subsectionHeader confidence="0.997333">
2.1 Interpolating Between Bins
</subsectionHeader>
<bodyText confidence="0.999633625">
Recall that the task is to apply the As to new un-
seen test data. One could simply use the As in
table 2 as is. That is, when we see a new term
in the test material, we find the closest bin in ta-
ble 2 and report the corresponding A value. But
since the idf of a term in the test set could easily
fall between two bins, it seems preferable to find
the two closest bins and interpolate between them.
</bodyText>
<figure confidence="0.994885947368421">
Description (function of term t)
1 df ,0) E # rel docs d with t f (t, d) = 0
2 df (t, ret, 1) # rel docs d with t f (t, d) = 1
3 df (t,re1,2) E- # rel docs d with t f (t, d) = 2
4 df (t,re1,3) -a: # rel docs d with t f (t, d) = 3
5 df (t, rel, 4+) E # r el docs d with tf(t, d) &gt; 4
6 df (t, ret, 0) EE # rel docs d with t f (t,d) = 0
7 df (t,re1,1) # rel docs d with t f (t, d) = 1
8 df (t,re1,2) E # rd docs d with t f (t, d) = 2
9 df (t, ret, 3) a. # rel docs d with t f (t, d) = 3
10 df (t, rel, 4+) a- # rel docs d with t f (t, &gt; 4
11 # rel docs d
12 # rel docs d
13 freq of term in corpus: TF(t) =Edtf (t, d)
14 # docs d in collection = N
15 df = # docs d with tf (t,d) &gt; 1
20 ef = # docs d in query exp. with t f (t,d) &gt; 1
21 where: D (description), E (query expansion)
25 burstiness: B
</figure>
<tableCaption confidence="0.911959">
Table 3: Training file schema: a record of 25 fields
</tableCaption>
<bodyText confidence="0.99754024137931">
is computed for each term (ngram) in each query
in training set.
sumptions, when appropriate, can be very pow-
erful (better estimates from less training data),
but errors resulting from inappropriate assump-
tions can outweigh the benefits. In this empirical
investigation of term weighting we decided to use
conservative non-parametric histogram methods
to hedge against the risk of inappropriate para-
metric assumptions.
Terms are assigned to bins based on features
such as idf, as illustrated in table 2. (Later we
will also use B and/or ef in the binning process.)
A is computed separately for each bin, based on the
use of terms in relevant and irrelevant documents,
according to the labeled training material.
The estimation method starts with a training
file which indicates, among other things, the num-
ber of relevant and irrelevant documents for each
term t in each training query, q. That is, for
each t and q, we are are given df (t, rel, t fo) and
df (t, rel, t fo), where df (t,rel,tfo) is the number
of relevant documents d with tf(t, d) = tfo, and
df (t,rel,t fo) is the number of irrelevant docu-
ments d with tf(t, d) = t fo. The schema for the
training file is described in table 3. From these
training observations we wish to obtain a mapping
from bins to As that can be applied to unseen test
material. We interpret A as a log likelihood ratio:
</bodyText>
<table confidence="0.91674475">
P(bin,t fire°
3t(bin,t f) = lo92
P(bin,t f Ire°
where the numerator can be approximated as:
df (bin, rel,t f)
P(bin,t f Ird) /092
IST‘rel
Ii
</table>
<page confidence="0.578419">
119
</page>
<equation confidence="0.763834461538461">
awk &apos;function log2(x) {
return log(x)/10g(2)}
$21 - /-D/ { N = $14; df=$15;
# binning rule
if(df &lt; 100) {bin =
else {bin=int(log2(df))1;
docfreq[bin] += df;
Nbin[bin]++;
# average df(t,rel,tf), df(t,irrel,tf)
for(i=1;i&lt;=12;i++) n[i,bin]+=$i 1
END {for(bin in Nbin) {
nbin = Nbin[bin]
Nrel = n[11,bin]/nbin
Nirrel = N-Nrel
idf = -log2((docfreq[bin] fnbin)/N)
printf(&amp;quot;%6.2f &amp;quot;, idf)
for(i=1;i&lt;=5;i++) {
if(Nrel==0) prel = 0
else prel = (n[i,bin]/nbin)/Nrel
if(Nirrel == 0) pirrel = 0
else pirrel = (n[i+5,bin]/nbin)/Nirrel
if(prel &lt;= 0 II pirrel &lt;= 0) {
printf &amp;quot;%6s &amp;quot;, &amp;quot;NA&amp;quot; 1
else 1
printf &amp;quot;%6.2f &amp;quot;, log2(prel/pirre1)1
print &amp;quot;11&apos;
</equation>
<figureCaption confidence="0.994416">
Figure 2: awk program for computing As.
</figureCaption>
<bodyText confidence="0.999897483870968">
We use linear regression to interpolate along the
idf dimension, as illustrated in table 4. Table 4 is
a smoothed version of table 2 where A a + b-idf.
There are five pairs of coefficients, a and b, one for
each value of tf.
Note that interpolation is generally not neces-
sary on the tf dimension because tf is highly
quantized. As long as tf &lt; 4, which it usually
is, the closest bin is an exact match. Even when
tf &gt; 4, there is very little room for adjustments if
we accept the upper limit of A &lt; idf.
Although we interpolate along the idf dimen-
sion, interpolation is not all that important along
that dimension either. Figure 1 shows that the
differences between the test data and the train-
ing data dominate the issues that interpolation is
attempting to deal with. The main advantage of
regression is computational convenience; it is eas-
ier to compute a +b-idf than to perform a binary
search to find the closest bin.
Previous work (Cooper et al., 1994) used mul-
tiple regression techniques. Although our perfor-
mance is similar (until we include query expan-
sion) we believe that it is safer and easier to treat
each value of tf as a separate regression for rea-
sons discussed in table 5. In so doing, we are ba-
sically restricting the regression analysis to such
an extent that it is unlikely to do much harm (or
much good). Imposing the limits of 0 &lt; A &lt; idf
also serves the purpose of preventing the regres-
sion from wandering too far astray.
</bodyText>
<table confidence="0.8844845">
tf a
0 -0.95 0.05
1 -0.98 0.69
2 -0.15 0.78
3 0.53 0.81
4+ 1.32 0.77
</table>
<tableCaption confidence="0.980873">
Table 4: Regression coefficients for method fit-G.
</tableCaption>
<bodyText confidence="0.737528">
This table approximates the data in table 1 with
A a(tf) + b(tf) • idf. Note that both the inter-
cepts, a(tf), and the slopes, b(tf), increase with
tf (with a minor exception for b(4+)).
</bodyText>
<table confidence="0.997578857142857">
tf a(tf) b(tf) az + cz - log(1 + t.f) b2
0 -0.95 0.05 -4.1 0.66
1 -0.98 0.69 -1.4 0.66
2 -0.15 0.78 0.18 0.66
3 0.53 0.81 1.3 0.66
4 1.32 0.77 2.2 0.66
5 1.32 0.77 2.9 0.66
</table>
<tableCaption confidence="0.997037">
Table 5: A comparison of the regression coeffi-
</tableCaption>
<bodyText confidence="0.9658721">
cients for method fit-G with comparable coeffi-
cients from the multiple regression: A = az + bz •
idf + c2 • log(1+ tf) where az = -4.1, bz = 0.66
and cz = 3.9. The differences in the two fits are
particularly large when tf = 0; note that b(0) is
negligible (0.05) and bz is quite large (0.66). Re-
ducing the number of parameters from 10 to 3 in
this way increases the sum of square errors, which
may or may not result in a large degradation in
precision and recall. Why take the chance?
</bodyText>
<sectionHeader confidence="0.996023" genericHeader="method">
3 thirstiness
</sectionHeader>
<bodyText confidence="0.998669444444444">
Table 6 is like tables 4 but the binning rule not
only uses idf, but also burstiness (B). Burstiness
(Church and Gale, 1995)(Katz, 1996)(Church,
2000) is intended to account for the fact that some
very good keywords such as &amp;quot;Kennedy&amp;quot; tend to
be mentioned quite a few times in a document
or not at all, whereas less good keywords such as
&amp;quot;except&amp;quot; tend to be mentioned about the same
number of times no matter what the document
</bodyText>
<figure confidence="0.517259333333333">
tf B=0 B=1
a b a b
0 -0.05 -0.00 -0.61 0.02
</figure>
<table confidence="0.974144">
1 -1.23 0.63 -0.80 0.79
2 -0.76 0.71 -0.05 0.79
3 0.00 0.69 0.23 0.82
4+ 0.68 0.71 0.75 0.83
</table>
<tableCaption confidence="0.991495">
Table 6: Regression coefficients for method fit-B.
</tableCaption>
<bodyText confidence="0.48877975">
Note that the slopes and intercepts are larger when
B = 1 than when B = 0 (except when tf = 0).
Even though A usually lies .between- 0 and idf, we
restrict A to 0 &lt; A &lt; idf, just to make sure.
</bodyText>
<page confidence="0.980081">
120
</page>
<table confidence="0.996570666666666">
tf ef where=D where=E
a a
1 0 -1.57 0.37
2 0 -3.41 0.82
3 0 -1.30 0.11
4+ 0 0.40 0.06
1 1 -1.84 0.87 -2.64 0.68
2 1 -2.12 1.10 -2.70 0.71
3 1 -0.66 0.95 -2.98 0.74
4+ 1 0.84 0.98 -3.35 0.78
1 2 -1.87 0.92 -3.00 0.86
2 2 -1.77 1.12 -2.78 0.85
3 2 -1.72 1.10 -3.07 0.93
4+ 2 -3.06 1.71* -3.25 0.79
1 3 -2.52 0.95 -2.71 0.91
2 3 -1.81 1.02 -2.28 0.88
3 3 0.45 0.85 -2.63 0.97
4+ 3 0.38 1.22 -3.66 1.14
</table>
<tableCaption confidence="0.992636">
Table 7: Many of the regression coefficients for
</tableCaption>
<bodyText confidence="0.964234896551724">
method fit-E. (The coefficients marked with an
asterisk are worrisome because the bins are too
small and/or the slopes fall well outside the nor-
mal range of 0 to 1.) The slopes rarely exceeded .8
is previous models (fit-G and fit-B), whereas fit-E
has more slopes closer to 1. The larger slopes are
associated with robust conditions, e.g., terms ap-
pearing in the query (where = D), the document
(tf &gt; 1) and the expansion (ef &gt; 1). If a term
appears in several documents brought in by query
expansion (ef &gt; 2), then the slope can be large
even if the term is not explicitly mentioned in the
query (where = E). The interactions among tf,
idf, ef and where are complicated and not easily
captured with a straightforward multiple regres-
sion.
is about. Since &amp;quot;Kennedy&amp;quot; and &amp;quot;except&amp;quot; have
similar idf values, they would normally receive
similar term weights, which doesn&apos;t seem right.
Kwok (1996) suggested average term frequency,
avt f = T F (t) I df (t), be used as a tie-breaker for
cases like this, where T F(t) = Edtf (t, is the
standard notion of frequency in the corpus-based
NLP. Table 6 shows how Kwok&apos;s suggestion can
be reformulated in our empirical framework. The
table shows the slopes and intercepts for ten re-
gressions, one for each combination of tf and B
(B = 1 if avt f is large. That is, B = 1 if
T F (t) I df (t) &gt; 1.83 - 0.048- idf).
</bodyText>
<sectionHeader confidence="0.994679" genericHeader="method">
4 Query Expansion
</sectionHeader>
<bodyText confidence="0.999818449275363">
We applied query expansion (Buckley et al., 1995)
to generate an expanded part of the query. The
original query is referred to as the description (D)
and the new part is referred to as the expansion
(E). (Queries also contain a narrative (N) part that
is not used in the experiments below so that our
results -could be compared to previously published
results.)
The expansion is formed by applying a base-
line query engine (fit-B model) to the description
part of the query. Terms that appear in the top
k = 10 retrieved documents are assigned to the E
portion of the query (where(t) = E), unless they
were previously assigned to some other portion of
the query (e.g., where(t) = D). All terms, t, no
matter where they appear in the query, also re-
ceive an expansion frequency ef, an integer from
0 to k = 10 indicating how many of the top k
documents contain t.
The fit-E model is: A = a(t f , where, ef) +
b(t f ,where, ef) idf, where the regression coeffi-
cients, a and b, not only depend on tf as in fit-G,
but also depend on where the term appears in the
query and expansion frequency ef. We consider 5
values of tf, 2 values of where (D and E) and 6
values of ef (0, 1, 2, 3, 4 or more). 32 of these
60 pairs of coefficients are shown in table 7. As
before, most of the slopes are between 0 and 1. A
is usually between 0 and idf, but we restrict A to
0 &lt; A &lt; idf, just to make sure.
In tables 4-7, the slopes usually lie between 0
and 1. In the previous models, fit-B and fit-G,
the largest slopes were about 0.8, whereas in fit-
E, the slope can be much closer to 1. The larger
slopes are associated with very robust conditions,
e.g., terms mentioned explicitly in all three areas of
interest: (1) the query (where = D), (2) the doc-
ument (tf &gt; 1) and (3) the expansion (ef &gt; 1).
Under such robust conditions, we would expect to
find very little shrinking (downweighting to com-
pensate for uncertainty).
On the other hand, when the term is not men-
tioned in one of these areas, there can be quite
a bit of shrinking. Table 7 shows that the slopes
are generally much smaller when the term is not
in the query (where = E) or when the term is
not in the expansion (ef = 0). However, there are
some exceptions. The bottom right corner of ta-
ble 7 contains some large slopes even though these
terms are not mentioned explicitly in the query
(where = E). The mitigating factor in this case
is the large ef. If a term is mentioned in several
documents in the expansion (ef &gt; 2), then it is
not as essential that it be mentioned explicitly in
the query.
With this model, as with fit-G and fit-B, A tends
to increase monotonically with tf and idf, though
there are some interesting exceptions. When the
term appears in the query (where = D) but not
in the expansion (ef = 0), the slopes are quite
small (e.g., b(3, D , 0) = 0.11), and the slopes actu-
ally decrease as tf increases (b(2, D, 0) = 0.83 &gt;
b(3, D, 0) = 0.11). We normally expect to see
slopes of .7 or more when tf &gt; 3, but in this case
(b(3, D, 0) = 0.11), there is a considerable shrink-
ing because we very much expected to see the term
in the expansion and we didn&apos;t. - -
As we have seen, the interactions among tf, idf,
ef and where are complicated and probably de-
</bodyText>
<page confidence="0.99203">
121
</page>
<table confidence="0.99594625">
filter trained on sys.
NA ? JCB1
2+, E1 tf,where,ef fit-E
2 B,tf fit-B
2, K tf + ... BKJJBIDS
2, K B,tf fit-B
2, K tf fit-G
2,K none log(1 + t f) - idf
2,K none t f - idf
11 R • 2: restrict terms to bigrams explicitly men-
tioned in query (where D)
.360 .351
.354 .363 • 2+: restrict terms to bigrams, but include
.283 .293 where =-- E as well as where = D
.272 .282
.264 .282 • W: restrict terms to words, as identified by
.257 .267 Chasen (Matsumoto et al., 1997)
.249 .262 • K: restrict terms to sequences of Katakana
.112 .138 and/or Kanji characters
• B: restrict terms to bursty (B = 1) terms
</table>
<tableCaption confidence="0.998359">
Table 8: Training helps: methods above the line
</tableCaption>
<bodyText confidence="0.945349222222222">
use training (with the possible exception of JCB1);
methods below the line do not.
pend on many factors such as language, collection,
typical query patterns and so on. To cope with
such complications, we believe that it is safer to
use histogram methods than to try to account for
all of these interactions at once in a single multiple
regression. The next section will show that fit-E
has very encouraging performance.
</bodyText>
<sectionHeader confidence="0.999816" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999850405405406">
Two measures of performance are reported: (1) 11
point average precision and (2) R, precision after
retrieving Nrei documents, where Nrel is the num-
ber of relevant documents. We used the &amp;quot;short
query&amp;quot; condition of the NACSIS NTCIR-1 Test
Collection (Kando et al., 1999) which consists of
about 300,000 documents in Japanese, plus about
30 queries with labeled relevance judgement for
training and 53 queries with relevance judgements
for testing. The result of &amp;quot;short query&amp;quot; is shown in
page 25 of(Kando et al., 1999), which shows that
&amp;quot;short query&amp;quot; is hard for statistical methods.
Two previously published systems are included
in the tables below: JCB1 and BKJJBIDS. JCB1,
submitted by Just System, a company with a com-
mercially successful product for Japanese word-
processing, produced the best results using sophis-
ticated (and proprietary) natural language pro-
cessing techniques.(Fujita, 1999) BKJJBIDS used
Berkeley&apos;s logistic regression methods (with about
half a dozen variables) to fit term weights to the
labeled training material.
Table 8 shows that training often helps. The
methods above the line (with the possible excep-
tion of JCB1) use training; the methods below the
line do not. Fit-E has very respectable perfor-
mance, nearly up to the level of JCB1, not bad for
a purely statistical method.
The performance of fit-B is close to that of
BKJJBIDS. For comparison sake, fit-B is shown
both with and without the K filter. The K filter
restricts terms to sequences of Katalcana and Kanji
characters. BKJJBIDS uses a similar heuristic to
eliminate Japanese function words. Although the
K filter does not change performance very much,
the use of this filter changes the relative order of
fit-B and BKJJBIDS. These results suggest that
</bodyText>
<listItem confidence="0.995771333333333">
• Ek: require terms to appear in more than k
docs brought in by query expansion (e f (t) &gt;
k).
</listItem>
<tableCaption confidence="0.84606975">
Table 9: Filters: results vary somewhat depending
on these choices, though not too much, which is
fortunate, since since we don&apos;t understand stop
lists very well.
</tableCaption>
<table confidence="0.913821666666667">
filter trained on sys. 11 R
2+, E1 tf,where,ef fit-E .354 .363
2+, E2 tf,where,ef fit-E .350 .359
2+, E4 tf,where,ef fit-E .333 .341
2+ tf,where,ef fit-E .332 .366
NA NA JCB1 .360 .351
</table>
<tableCaption confidence="0.714392">
Table 10: The best filters (Ek) improve the per-
</tableCaption>
<bodyText confidence="0.998673833333333">
formance of the best method (fit-E) to nearly the
level of JCB1.
the K filter is slightly unhelpful.
A number of filters have been considered (ta-
ble 9). Results vary somewhat depending on these
choices, though not too much, which is fortunate,
since since we don&apos;t understand stop lists very
well. To the extent that there is a pattern, we sus-
pect that words are slightly better than bigrams,
and that the E filter is slightly better than the B
filter which is slightly better than the K filter. Ta-
ble 10 shows that the best filters (Ek) improve the
performance of the best method (fit-E) to nearly
the level of JCB1.
Table 11: Limits do no harm: two limits are
slightly better than one, and one is slightly bet-
ter than none. (UL = upper limit of A &lt; idf ; LL
= lower limit of 0 &lt; A)
</bodyText>
<footnote confidence="0.800434444444444">
filter sys.
2 fit-B
2 fit-B
2 fit-B
2 fit-B
2 fit-G
2 fit-G
2 fit-G
2 fit-G
</footnote>
<table confidence="0.999678444444445">
UL LL 11 R
+ + .283 .293
+ - .280 .296
- + .280 .296
- - .275 .288
+ + .266 .279
- + .251 .268
+ - .248 .259
- - .232 .249
</table>
<page confidence="0.994229">
122
</page>
<bodyText confidence="0.999096">
The final experiment (table 11) shows that re-
stricting A to 0 &lt; A &lt; idf improves performance
slightly. The combination of both the upper limit
and the lower limit is slightly better than just one
limit which is better than none. We view limits as
a robustness device. Hopefully, they won&apos;t have
to do much but every once in a while they prevent
the system from wandering far astray.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999961878048781">
This paper introduced an empirical histogram-
based supervised learning method for estimating
term weights, A. Terms are assigned to bins based
on features such as inverse document frequency,
burstiness and expansion frequency. A different A
is estimated for each bin and each tf by counting
the number of relevant and irrelevant documents
associated with the bin and tf value. Regression
techniques are used to interpolate between bins,
but care is taken so that the regression cannot do
too much harm (or too much good). Three varia-
tions were considered: fit-G, fit-B and fit-E. The
performance of query expansion (fit-E) is particu-
larly encouraging. Using simple purely statistical
methods, fit-E is nearly comparable to JCB1, a
sophisticated natural language processing system
developed by Just System, a leader in the Japanese
word processing industry.
In addition to performance, we are also inter-
ested in the interpretation of the weights. Empiri-
cal weights tend to lie between 0 and idf. We find
these limits to be a surprise given that standard
term weighting formulas such as tf idf generally
do not conform to these limits. In addition, we
find that A generally grows linearly with idf, and
that the slope is between 0 and 1. We interpret the
slope as a statistical shrink. The larger slopes are
associated with very robust conditions, e.g., terms
mentioned explicitly in all three areas of interest:
(1) the query (where = D), (2) the document
(tf &gt; 1) and (3) the expansion (e f &gt; 1). There
is generally more shrinking for terms brought in
by query expansion (where -= E), but if a term
is mentioned in several documents in the expan-
sion (ef &gt; 2), then it is not as essential that the
term be mentioned explicitly in the query. The
interactions among tf, idf, where, B, e f , etc., are
complicated, and therefore, we have found it safer
and easier to use histogram methods than to try
to account for all of the interactions at once in a
single multiple regression.
</bodyText>
<sectionHeader confidence="0.968837" genericHeader="acknowledgments">
Acknowdedgement
</sectionHeader>
<bodyText confidence="0.9880508">
Authors thank Prof. Mitchell P. Marcus of Uni-
versity of Pennsylvania for the valuable discussion
about noise reduction in context of information
retrieval. This reseach is supported by Sumitomo
Electric.
</bodyText>
<sectionHeader confidence="0.994171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833525">
Chris Buckley, Gerard Salton, James Allan, and Amit
Singhal. 1995. Automatic query expansion us-
ing smart: Trec 3. In The Third Text REtrieval
Conference(TREC-3), pages 69-80.
Aitao Chen, Fredric C. Gey, Kazuaki Kishida, Hailing
Jiang, and Qua Liang. 1999. Comparing multiple
methods for japanese and japanese-english text re-
trieval. In NTCIR Workshop 1, pages 49-58, Tokyo
Japan, Sep.
Kenneth W. Church and William A. Gale. 1995.
Poisson mixture. Natural Language Engineering,
1(2):163-190.
Kenneth W. Church. 2000. Empirical estimates of
adaptation: The chance of two noriegas is closer
to p/2 than p2. In Coling-2000, pages 180-186.
William S. Cooper, Aitao Chen, and Fredric C. Gey.
1994. Full text retrieval based on probabilistic equa-
tion with coefficients fitted by logistic regressions.
In The Second Text REtrieval Conference(TREC-
2), pages 57-66.
Sumio Fujita. 1999. Notes on phrasal index-
ing: Jscb evaluation experiments at ntcir ad
hoc&amp;quot;. In NTCIR Workshop 1, pages 101-108,
http://vrww.rd.nacsis.ac.jp/ &amp;quot;ntcadm/, Sep.
Noriko Kando, Kazuko Kuriyarna, Toshihiko Nozue,
Koji Eguchi, and Hiroyuki Katoand Souichiro Hi-
daka. 1999. Overview of ir tasks at the first nt-
cir workshop. In NTCIR Workshop I, pages 11-44,
http://ww-w.rd.nacsis.ac.jp/ &amp;quot;ntcadm/, Sep.
Slava M. Katz. 1996. Distribution of content words
and phrases in text and language modelling. Natural
Language Engineering, 2(1):15-59.
K. L. Kwok. 1996. A new method of weighting query
terms for ad-hoc retrieval. In SIGIR96, pages 187-
195, Zurich, Switzerland.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Osamu Imaichi, and Tomoaki
Imamura. 1997. Japanese morphological analysis
system chasen manual. Technical Report NAIST-
IS-TR97007, NAIST, Nara, Japan, Feb.
</reference>
<page confidence="0.998952">
123
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.811570">
<title confidence="0.991228">Empirical Term Weighting and Expansion Frequency</title>
<author confidence="0.901371">Kyoji</author>
<affiliation confidence="0.998993">Toyohashi University of</affiliation>
<address confidence="0.981803">Toyohashi Aichi 441-8580</address>
<email confidence="0.948493">umemura@tutics.tut.ac.jp</email>
<author confidence="0.988953">W Kenneth</author>
<affiliation confidence="0.989468">AT&amp;T</affiliation>
<address confidence="0.986286">180 Park Ave., Florham Park,</address>
<email confidence="0.999875">kwc@research.att.com</email>
<abstract confidence="0.999781676470588">We propose an empirical method for estimating term weights directly from relevance judgements, avoiding various standard but potentially troublesome assumptions. It is common to assume, for exthat weights vary with term frequency inverse document frequency a particuway, e.g., • idf, the fact that there are so many variants of this formula in the literature suggests that there remains considerable uncertainty about these assumptions. Our method is similar to the Berkeley regression method where labeled relevance judgements are fit as a linear combination (transforms of) idf, Training methods not only improve performance, but also extend naturally to include additional factors such as burstiness and query expansion. The proposed histogram-based training method provides a simple way to model complicated interactions among such as idf, and expansion frequency (a generalization of query expansion). The correct handling of expanded term is realized based on statistical information. Expansion frequency dramatically improves performance from a level comparable to BKJJBIDS, Berkeley&apos;s entry in the Japanese NACSIS NTCIR-1 evaluation for short queries, to the level of JCB1, the top system in the evaluation. JCB1 uses sophisticated (and proprietary) natural language processing techniques developed by Just System, a leader in the Japanese word-processing industry. We are encouraged that the proposed method, which is simple to understand and replicate, can reach this level of performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Gerard Salton</author>
<author>James Allan</author>
<author>Amit Singhal</author>
</authors>
<title>Automatic query expansion using smart: Trec 3.</title>
<date>1995</date>
<booktitle>In The Third Text REtrieval Conference(TREC-3),</booktitle>
<pages>69--80</pages>
<contexts>
<context position="17850" citStr="Buckley et al., 1995" startWordPosition="3383" endWordPosition="3386">values, they would normally receive similar term weights, which doesn&apos;t seem right. Kwok (1996) suggested average term frequency, avt f = T F (t) I df (t), be used as a tie-breaker for cases like this, where T F(t) = Edtf (t, is the standard notion of frequency in the corpus-based NLP. Table 6 shows how Kwok&apos;s suggestion can be reformulated in our empirical framework. The table shows the slopes and intercepts for ten regressions, one for each combination of tf and B (B = 1 if avt f is large. That is, B = 1 if T F (t) I df (t) &gt; 1.83 - 0.048- idf). 4 Query Expansion We applied query expansion (Buckley et al., 1995) to generate an expanded part of the query. The original query is referred to as the description (D) and the new part is referred to as the expansion (E). (Queries also contain a narrative (N) part that is not used in the experiments below so that our results -could be compared to previously published results.) The expansion is formed by applying a baseline query engine (fit-B model) to the description part of the query. Terms that appear in the top k = 10 retrieved documents are assigned to the E portion of the query (where(t) = E), unless they were previously assigned to some other portion o</context>
</contexts>
<marker>Buckley, Salton, Allan, Singhal, 1995</marker>
<rawString>Chris Buckley, Gerard Salton, James Allan, and Amit Singhal. 1995. Automatic query expansion using smart: Trec 3. In The Third Text REtrieval Conference(TREC-3), pages 69-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitao Chen</author>
<author>Fredric C Gey</author>
<author>Kazuaki Kishida</author>
<author>Hailing Jiang</author>
<author>Qua Liang</author>
</authors>
<title>Comparing multiple methods for japanese and japanese-english text retrieval.</title>
<date>1999</date>
<booktitle>In NTCIR Workshop 1,</booktitle>
<pages>49--58</pages>
<location>Tokyo Japan,</location>
<contexts>
<context position="2086" citStr="Chen et al., 1999" startWordPosition="310" endWordPosition="313">r short queries, to the level of JCB1, the top system in the evaluation. JCB1 uses sophisticated (and proprietary) natural language processing techniques developed by Just System, a leader in the Japanese word-processing industry. We are encouraged that the proposed method, which is simple to understand and replicate, can reach this level of performance. 1 Introduction An empirical method for estimating term weights directly from relevance judgements is proposed. The method is designed to make as few assumptions as possible. It is similar to Berkeley&apos;s use of regression (Cooper et al., 1994) (Chen et al., 1999) where labeled relevance judgements are fit as a linear combination of (transforms of) tf, idf, etc., but avoids potentially troublesome assumptions by introducing histogram methods. Terms are grouped into bins. Weights are computed based on the number of relevant and irrelevant documents associated with each bin. The result• t: a term • d: a document • t f (t, d): term freq = # of instances of tin d • df (t): doc freq = # of docs d with t f (t, d) &gt; 1 • N: # of documents in collection • idf (t): inverse document freq: • df (t, rel, t fo): # of relevant documents d with t f (t, = tfo • df (t, </context>
<context position="6980" citStr="Chen et al., 1999" startWordPosition="1256" endWordPosition="1259">with tf = 2, and so on. The higher lines have larger intercepts and larger slopes than the lower lines. That is, when we fit A a(tf) + b(tf) - idf, with separate regression coefficients, a(tf) and b(tf), for each value of tf, we find that both a(tf) and b(tf) increase with tf. terms. In this way, all of the terms in a bin are assigned the weight, A. The common practice, for example, of assigning tf idf weights can be interpreted as grouping all terms with the same idf into a bin and assigning them all the same weight, namely tf idf. Cooper and his colleagues at Berkeley (Cooper et al., 1994) (Chen et al., 1999) have been using regression methods to fit A as a linear combination of idf, log(tf) and various other features. This method is also grouping terms into bins based on their features and assigning similar weights to terms with similar features. In general, term weighting methods that are fit to data are more flexible than weighting methods that are not fit to data. We believe this additional flexibility improves precision and recall (table 8). Instead of multiple regression, though, we choose a more empirical approach. Parametric as137 _1-1 2 - 4 2 2 1 1 scorep(d, q) = E 1og2P (tire° tET P(t17.</context>
</contexts>
<marker>Chen, Gey, Kishida, Jiang, Liang, 1999</marker>
<rawString>Aitao Chen, Fredric C. Gey, Kazuaki Kishida, Hailing Jiang, and Qua Liang. 1999. Comparing multiple methods for japanese and japanese-english text retrieval. In NTCIR Workshop 1, pages 49-58, Tokyo Japan, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<date>1995</date>
<booktitle>Poisson mixture. Natural Language Engineering,</booktitle>
<pages>1--2</pages>
<contexts>
<context position="15299" citStr="Church and Gale, 1995" startWordPosition="2881" endWordPosition="2884">ents for method fit-G with comparable coefficients from the multiple regression: A = az + bz • idf + c2 • log(1+ tf) where az = -4.1, bz = 0.66 and cz = 3.9. The differences in the two fits are particularly large when tf = 0; note that b(0) is negligible (0.05) and bz is quite large (0.66). Reducing the number of parameters from 10 to 3 in this way increases the sum of square errors, which may or may not result in a large degradation in precision and recall. Why take the chance? 3 thirstiness Table 6 is like tables 4 but the binning rule not only uses idf, but also burstiness (B). Burstiness (Church and Gale, 1995)(Katz, 1996)(Church, 2000) is intended to account for the fact that some very good keywords such as &amp;quot;Kennedy&amp;quot; tend to be mentioned quite a few times in a document or not at all, whereas less good keywords such as &amp;quot;except&amp;quot; tend to be mentioned about the same number of times no matter what the document tf B=0 B=1 a b a b 0 -0.05 -0.00 -0.61 0.02 1 -1.23 0.63 -0.80 0.79 2 -0.76 0.71 -0.05 0.79 3 0.00 0.69 0.23 0.82 4+ 0.68 0.71 0.75 0.83 Table 6: Regression coefficients for method fit-B. Note that the slopes and intercepts are larger when B = 1 than when B = 0 (except when tf = 0). Even though A </context>
</contexts>
<marker>Church, Gale, 1995</marker>
<rawString>Kenneth W. Church and William A. Gale. 1995. Poisson mixture. Natural Language Engineering, 1(2):163-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Empirical estimates of adaptation: The chance of two noriegas is closer to p/2 than p2. In</title>
<date>2000</date>
<booktitle>Coling-2000,</booktitle>
<pages>180--186</pages>
<contexts>
<context position="15325" citStr="Church, 2000" startWordPosition="2885" endWordPosition="2886">le coefficients from the multiple regression: A = az + bz • idf + c2 • log(1+ tf) where az = -4.1, bz = 0.66 and cz = 3.9. The differences in the two fits are particularly large when tf = 0; note that b(0) is negligible (0.05) and bz is quite large (0.66). Reducing the number of parameters from 10 to 3 in this way increases the sum of square errors, which may or may not result in a large degradation in precision and recall. Why take the chance? 3 thirstiness Table 6 is like tables 4 but the binning rule not only uses idf, but also burstiness (B). Burstiness (Church and Gale, 1995)(Katz, 1996)(Church, 2000) is intended to account for the fact that some very good keywords such as &amp;quot;Kennedy&amp;quot; tend to be mentioned quite a few times in a document or not at all, whereas less good keywords such as &amp;quot;except&amp;quot; tend to be mentioned about the same number of times no matter what the document tf B=0 B=1 a b a b 0 -0.05 -0.00 -0.61 0.02 1 -1.23 0.63 -0.80 0.79 2 -0.76 0.71 -0.05 0.79 3 0.00 0.69 0.23 0.82 4+ 0.68 0.71 0.75 0.83 Table 6: Regression coefficients for method fit-B. Note that the slopes and intercepts are larger when B = 1 than when B = 0 (except when tf = 0). Even though A usually lies .between- 0 a</context>
</contexts>
<marker>Church, 2000</marker>
<rawString>Kenneth W. Church. 2000. Empirical estimates of adaptation: The chance of two noriegas is closer to p/2 than p2. In Coling-2000, pages 180-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William S Cooper</author>
<author>Aitao Chen</author>
<author>Fredric C Gey</author>
</authors>
<title>Full text retrieval based on probabilistic equation with coefficients fitted by logistic regressions.</title>
<date>1994</date>
<booktitle>In The Second Text REtrieval Conference(TREC2),</booktitle>
<pages>57--66</pages>
<contexts>
<context position="2066" citStr="Cooper et al., 1994" startWordPosition="306" endWordPosition="309"> NTCIR-1 evaluation for short queries, to the level of JCB1, the top system in the evaluation. JCB1 uses sophisticated (and proprietary) natural language processing techniques developed by Just System, a leader in the Japanese word-processing industry. We are encouraged that the proposed method, which is simple to understand and replicate, can reach this level of performance. 1 Introduction An empirical method for estimating term weights directly from relevance judgements is proposed. The method is designed to make as few assumptions as possible. It is similar to Berkeley&apos;s use of regression (Cooper et al., 1994) (Chen et al., 1999) where labeled relevance judgements are fit as a linear combination of (transforms of) tf, idf, etc., but avoids potentially troublesome assumptions by introducing histogram methods. Terms are grouped into bins. Weights are computed based on the number of relevant and irrelevant documents associated with each bin. The result• t: a term • d: a document • t f (t, d): term freq = # of instances of tin d • df (t): doc freq = # of docs d with t f (t, d) &gt; 1 • N: # of documents in collection • idf (t): inverse document freq: • df (t, rel, t fo): # of relevant documents d with t f</context>
<context position="6960" citStr="Cooper et al., 1994" startWordPosition="1252" endWordPosition="1255">ich is above the line with tf = 2, and so on. The higher lines have larger intercepts and larger slopes than the lower lines. That is, when we fit A a(tf) + b(tf) - idf, with separate regression coefficients, a(tf) and b(tf), for each value of tf, we find that both a(tf) and b(tf) increase with tf. terms. In this way, all of the terms in a bin are assigned the weight, A. The common practice, for example, of assigning tf idf weights can be interpreted as grouping all terms with the same idf into a bin and assigning them all the same weight, namely tf idf. Cooper and his colleagues at Berkeley (Cooper et al., 1994) (Chen et al., 1999) have been using regression methods to fit A as a linear combination of idf, log(tf) and various other features. This method is also grouping terms into bins based on their features and assigning similar weights to terms with similar features. In general, term weighting methods that are fit to data are more flexible than weighting methods that are not fit to data. We believe this additional flexibility improves precision and recall (table 8). Instead of multiple regression, though, we choose a more empirical approach. Parametric as137 _1-1 2 - 4 2 2 1 1 scorep(d, q) = E 1og</context>
<context position="13662" citStr="Cooper et al., 1994" startWordPosition="2560" endWordPosition="2563">s tf &lt; 4, which it usually is, the closest bin is an exact match. Even when tf &gt; 4, there is very little room for adjustments if we accept the upper limit of A &lt; idf. Although we interpolate along the idf dimension, interpolation is not all that important along that dimension either. Figure 1 shows that the differences between the test data and the training data dominate the issues that interpolation is attempting to deal with. The main advantage of regression is computational convenience; it is easier to compute a +b-idf than to perform a binary search to find the closest bin. Previous work (Cooper et al., 1994) used multiple regression techniques. Although our performance is similar (until we include query expansion) we believe that it is safer and easier to treat each value of tf as a separate regression for reasons discussed in table 5. In so doing, we are basically restricting the regression analysis to such an extent that it is unlikely to do much harm (or much good). Imposing the limits of 0 &lt; A &lt; idf also serves the purpose of preventing the regression from wandering too far astray. tf a 0 -0.95 0.05 1 -0.98 0.69 2 -0.15 0.78 3 0.53 0.81 4+ 1.32 0.77 Table 4: Regression coefficients for method</context>
</contexts>
<marker>Cooper, Chen, Gey, 1994</marker>
<rawString>William S. Cooper, Aitao Chen, and Fredric C. Gey. 1994. Full text retrieval based on probabilistic equation with coefficients fitted by logistic regressions. In The Second Text REtrieval Conference(TREC2), pages 57-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sumio Fujita</author>
</authors>
<title>Notes on phrasal indexing: Jscb evaluation experiments at ntcir ad hoc&amp;quot;.</title>
<date>1999</date>
<booktitle>In NTCIR Workshop 1,</booktitle>
<pages>101--108</pages>
<note>http://vrww.rd.nacsis.ac.jp/ &amp;quot;ntcadm/,</note>
<contexts>
<context position="22997" citStr="Fujita, 1999" startWordPosition="4353" endWordPosition="4354">s of about 300,000 documents in Japanese, plus about 30 queries with labeled relevance judgement for training and 53 queries with relevance judgements for testing. The result of &amp;quot;short query&amp;quot; is shown in page 25 of(Kando et al., 1999), which shows that &amp;quot;short query&amp;quot; is hard for statistical methods. Two previously published systems are included in the tables below: JCB1 and BKJJBIDS. JCB1, submitted by Just System, a company with a commercially successful product for Japanese wordprocessing, produced the best results using sophisticated (and proprietary) natural language processing techniques.(Fujita, 1999) BKJJBIDS used Berkeley&apos;s logistic regression methods (with about half a dozen variables) to fit term weights to the labeled training material. Table 8 shows that training often helps. The methods above the line (with the possible exception of JCB1) use training; the methods below the line do not. Fit-E has very respectable performance, nearly up to the level of JCB1, not bad for a purely statistical method. The performance of fit-B is close to that of BKJJBIDS. For comparison sake, fit-B is shown both with and without the K filter. The K filter restricts terms to sequences of Katalcana and Ka</context>
</contexts>
<marker>Fujita, 1999</marker>
<rawString>Sumio Fujita. 1999. Notes on phrasal indexing: Jscb evaluation experiments at ntcir ad hoc&amp;quot;. In NTCIR Workshop 1, pages 101-108, http://vrww.rd.nacsis.ac.jp/ &amp;quot;ntcadm/, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Kando</author>
</authors>
<title>Kazuko Kuriyarna, Toshihiko Nozue, Koji Eguchi, and Hiroyuki Katoand Souichiro Hidaka.</title>
<date>1999</date>
<booktitle>In NTCIR Workshop I,</booktitle>
<pages>11--44</pages>
<note>http://ww-w.rd.nacsis.ac.jp/ &amp;quot;ntcadm/,</note>
<marker>Kando, 1999</marker>
<rawString>Noriko Kando, Kazuko Kuriyarna, Toshihiko Nozue, Koji Eguchi, and Hiroyuki Katoand Souichiro Hidaka. 1999. Overview of ir tasks at the first ntcir workshop. In NTCIR Workshop I, pages 11-44, http://ww-w.rd.nacsis.ac.jp/ &amp;quot;ntcadm/, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Distribution of content words and phrases in text and language modelling.</title>
<date>1996</date>
<journal>Natural Language Engineering,</journal>
<pages>2--1</pages>
<contexts>
<context position="15311" citStr="Katz, 1996" startWordPosition="2884" endWordPosition="2885">ith comparable coefficients from the multiple regression: A = az + bz • idf + c2 • log(1+ tf) where az = -4.1, bz = 0.66 and cz = 3.9. The differences in the two fits are particularly large when tf = 0; note that b(0) is negligible (0.05) and bz is quite large (0.66). Reducing the number of parameters from 10 to 3 in this way increases the sum of square errors, which may or may not result in a large degradation in precision and recall. Why take the chance? 3 thirstiness Table 6 is like tables 4 but the binning rule not only uses idf, but also burstiness (B). Burstiness (Church and Gale, 1995)(Katz, 1996)(Church, 2000) is intended to account for the fact that some very good keywords such as &amp;quot;Kennedy&amp;quot; tend to be mentioned quite a few times in a document or not at all, whereas less good keywords such as &amp;quot;except&amp;quot; tend to be mentioned about the same number of times no matter what the document tf B=0 B=1 a b a b 0 -0.05 -0.00 -0.61 0.02 1 -1.23 0.63 -0.80 0.79 2 -0.76 0.71 -0.05 0.79 3 0.00 0.69 0.23 0.82 4+ 0.68 0.71 0.75 0.83 Table 6: Regression coefficients for method fit-B. Note that the slopes and intercepts are larger when B = 1 than when B = 0 (except when tf = 0). Even though A usually lies</context>
</contexts>
<marker>Katz, 1996</marker>
<rawString>Slava M. Katz. 1996. Distribution of content words and phrases in text and language modelling. Natural Language Engineering, 2(1):15-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Kwok</author>
</authors>
<title>A new method of weighting query terms for ad-hoc retrieval.</title>
<date>1996</date>
<booktitle>In SIGIR96,</booktitle>
<pages>187--195</pages>
<location>Zurich, Switzerland.</location>
<contexts>
<context position="17324" citStr="Kwok (1996)" startWordPosition="3279" endWordPosition="3280">e larger slopes are associated with robust conditions, e.g., terms appearing in the query (where = D), the document (tf &gt; 1) and the expansion (ef &gt; 1). If a term appears in several documents brought in by query expansion (ef &gt; 2), then the slope can be large even if the term is not explicitly mentioned in the query (where = E). The interactions among tf, idf, ef and where are complicated and not easily captured with a straightforward multiple regression. is about. Since &amp;quot;Kennedy&amp;quot; and &amp;quot;except&amp;quot; have similar idf values, they would normally receive similar term weights, which doesn&apos;t seem right. Kwok (1996) suggested average term frequency, avt f = T F (t) I df (t), be used as a tie-breaker for cases like this, where T F(t) = Edtf (t, is the standard notion of frequency in the corpus-based NLP. Table 6 shows how Kwok&apos;s suggestion can be reformulated in our empirical framework. The table shows the slopes and intercepts for ten regressions, one for each combination of tf and B (B = 1 if avt f is large. That is, B = 1 if T F (t) I df (t) &gt; 1.83 - 0.048- idf). 4 Query Expansion We applied query expansion (Buckley et al., 1995) to generate an expanded part of the query. The original query is referred</context>
</contexts>
<marker>Kwok, 1996</marker>
<rawString>K. L. Kwok. 1996. A new method of weighting query terms for ad-hoc retrieval. In SIGIR96, pages 187-195, Zurich, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
</authors>
<title>Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Osamu Imaichi, and Tomoaki Imamura.</title>
<date>1997</date>
<tech>Technical Report NAISTIS-TR97007,</tech>
<location>NAIST, Nara, Japan,</location>
<marker>Matsumoto, 1997</marker>
<rawString>Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Osamu Imaichi, and Tomoaki Imamura. 1997. Japanese morphological analysis system chasen manual. Technical Report NAISTIS-TR97007, NAIST, Nara, Japan, Feb.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>