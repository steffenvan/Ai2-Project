<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.607979">
<title confidence="0.993376">
Named Entity Generation using Sampling-based Structured Prediction
</title>
<author confidence="0.981089">
Guillaume Bouchard
</author>
<affiliation confidence="0.938591">
Xerox Research Centre Europe
</affiliation>
<address confidence="0.8864355">
6 Chemin de Maupertuis
38240 Meylan, France
</address>
<email confidence="0.999164">
guillaume.bouchard@xerox.com
</email>
<sectionHeader confidence="0.997391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999637571428572">
The problem of Named Entity Generation
is expressed as a conditional probability
model over a structured domain. By defin-
ing a factor-graph model over the men-
tions of a text, we obtain a compact pa-
rameterization of what is learned using the
SampleRank algorithm.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997492">
This document describes the participa-
tion of the Xerox Research Centre Eu-
rope team in the GREC-NEG’10 challenge
(http://www.nltg.brighton.ac.uk/research/gen
chal10/grec/)
</bodyText>
<sectionHeader confidence="0.98699" genericHeader="method">
2 Model
</sectionHeader>
<bodyText confidence="0.994512071428571">
Conditional random fields are conditional prob-
ability models that define a distribution over
a complex output space. In the context of
the Named-Entity Generation challenge, the
output space is the set of possible referring
expressions for all the possible mentions of the
text. For example, assuming that we have the
following text with holes (numbers are entity IDs):
#1 was a Scottish mathematician,
son of #2. #1 is most remembered
as the inventor of logarithms
and Napier’s bones.
Then the possibilities associated with the entity #1
are:
</bodyText>
<listItem confidence="0.990040555555556">
1. John Napier of Merchistoun,
2. Napier,
3. he,
4. who,
and the possibilities associated with the entity #2
are:
1. Sir Archibald Napier of Merchiston,
2. he,
3. who.
</listItem>
<bodyText confidence="0.991481947368421">
Then, the output space is Y = 11, 2, 3, 4} x
11, 2,3} x 11, 2, 3, 4}, representing all the possi-
ble combination of choices for the mentions. The
solution y = (1, 1, 3) corresponds to inserting the
texts ‘John Napier of Merchiston’, ‘Sir Archibald
Napier of Merchiston’ and ‘he’ in the holes of the
text in the same order. This is the combination
that is the closest to the original text, but a human
could also consider that solution y = (1, 1, 2) as
being equally valid.
Denoting x the input, i.e. the text with the typed
holes, the objective of the task is to find the combi-
nation y E Y that is as close as possible to natural
texts.
We model the distribution of y given x by a fac-
tor graph: p(y|x) a jl,∈C φ,(x, y), where Cis
the set of factors defined over the input and output
variables. In this work, we considered 3 types of
exponential potentials:
</bodyText>
<listItem confidence="0.997430461538462">
• Unary potentials defined on each individual
output yz. They include more than 100 fea-
tures corresponding to the position of the
mention in the sentence, the previous and
next part of speech (POS), the syntactic cat-
egory and funciton of the mention, the type
and case of the corresponding referring ex-
pression, etc.
• Binary potentials over contiguous mentions
include the distance between them, and the
joint distribution of the types and cases.
• Binary potentials that are activated only be-
tween mentions and the previous time the
</listItem>
<bodyText confidence="0.999868333333333">
same entity was referred to by a name. The
purpose of this is to reduce the use of pro-
nouns referring to a person when the men-
tions are distant to each other.
To learn the parameter of the factor graph, we used
the SampleRank algorithm (Wick et al., 2009)
which casts the prediction problem as a stochas-
tic search algorithms. During learning, an optimal
ranking function is estimated.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.994107818181818">
Using the evaluation software supplied by the
GREC-NEG organizers, we obtained the folloing
performances:
total slots : 907
reg08 type matches : 693
reg08 type accuracy : 0.764057331863286
reg08 type matches
including embedded : 723
reg08 type precision : 0.770788912579957
reg08 type recall : 0.770788912579957
total peer REFs : 938
total reference REFs : 938
string matches : 637
string accuracy : 0.702315325248071
mean edit distance : 0.724366041896362
mean normalised
edit distance : 0.279965348873838
BLEU 1 score : 0.7206
BLEU 2 score : 0.7685
BLEU 3 score : 0.7702
BLEU 4 score : 0.754
NIST score : 5.1208
</bodyText>
<sectionHeader confidence="0.999398" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998005">
Michael Wick, Khashayar Rohanimanesh, Aron Cu-
lotta, and Andrew McCallum. 2009. SampleRank:
Learning preferences from atomic gradients. Neural
Information Processing Systems (NIPS) Workshop
on Advances in Ranking.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.688403">
<title confidence="0.999559">Named Entity Generation using Sampling-based Structured Prediction</title>
<author confidence="0.976066">Guillaume</author>
<affiliation confidence="0.998683">Xerox Research Centre</affiliation>
<address confidence="0.972966">6 Chemin de 38240 Meylan,</address>
<email confidence="0.999958">guillaume.bouchard@xerox.com</email>
<abstract confidence="0.967754875">The problem of Named Entity Generation is expressed as a conditional probability model over a structured domain. By defining a factor-graph model over the mentions of a text, we obtain a compact parameterization of what is learned using the SampleRank algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Wick</author>
<author>Khashayar Rohanimanesh</author>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>SampleRank: Learning preferences from atomic gradients.</title>
<date>2009</date>
<booktitle>Neural Information Processing Systems (NIPS) Workshop on Advances in Ranking.</booktitle>
<contexts>
<context position="3012" citStr="Wick et al., 2009" startWordPosition="509" endWordPosition="512">next part of speech (POS), the syntactic category and funciton of the mention, the type and case of the corresponding referring expression, etc. • Binary potentials over contiguous mentions include the distance between them, and the joint distribution of the types and cases. • Binary potentials that are activated only between mentions and the previous time the same entity was referred to by a name. The purpose of this is to reduce the use of pronouns referring to a person when the mentions are distant to each other. To learn the parameter of the factor graph, we used the SampleRank algorithm (Wick et al., 2009) which casts the prediction problem as a stochastic search algorithms. During learning, an optimal ranking function is estimated. 3 Results Using the evaluation software supplied by the GREC-NEG organizers, we obtained the folloing performances: total slots : 907 reg08 type matches : 693 reg08 type accuracy : 0.764057331863286 reg08 type matches including embedded : 723 reg08 type precision : 0.770788912579957 reg08 type recall : 0.770788912579957 total peer REFs : 938 total reference REFs : 938 string matches : 637 string accuracy : 0.702315325248071 mean edit distance : 0.724366041896362 mea</context>
</contexts>
<marker>Wick, Rohanimanesh, Culotta, McCallum, 2009</marker>
<rawString>Michael Wick, Khashayar Rohanimanesh, Aron Culotta, and Andrew McCallum. 2009. SampleRank: Learning preferences from atomic gradients. Neural Information Processing Systems (NIPS) Workshop on Advances in Ranking.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>