<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000290">
<title confidence="0.98697">
Models of Translation Competitions
</title>
<author confidence="0.950801">
Mark Hopkins and Jonathan May
</author>
<affiliation confidence="0.789491">
SDL Research
</affiliation>
<address confidence="0.890554">
6060 Center Drive, Suite 150
Los Angeles, CA 90045
</address>
<email confidence="0.999361">
{mhopkins,jmay}@sdl.com
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998939846153846">
What do we want to learn from a trans-
lation competition and how do we learn
it with confidence? We argue that a dis-
proportionate focus on ranking competi-
tion participants has led to lots of differ-
ent rankings, but little insight about which
rankings we should trust. In response, we
provide the first framework that allows an
empirical comparison of different analy-
ses of competition results. We then use
this framework to compare several analyt-
ical models on data from the Workshop on
Machine Translation (WMT).
</bodyText>
<sectionHeader confidence="0.993593" genericHeader="method">
1 The WMT Translation Competition
</sectionHeader>
<bodyText confidence="0.999939772727273">
Every year, the Workshop on Machine Transla-
tion (WMT) conducts a competition between ma-
chine translation systems. The WMT organizers
invite research groups to submit translation sys-
tems in eight different tracks: Czech to/from En-
glish, French to/from English, German to/from
English, and Spanish to/from English.
For each track, the organizers also assemble a
panel of judges, typically machine translation spe-
cialists.1 The role of a judge is to repeatedly rank
five different translations of the same source text.
Ties are permitted. In Table 1, we show an ex-
ample2 where a judge (we’ll call him “jdoe”) has
ranked five translations of the French sentence “Il
ne va pas.”
Each such elicitation encodes ten pairwise com-
parisons, as shown in Table 2. For each compe-
tition track, WMT typically elicits between 5000
and 20000 comparisons. Once the elicitation pro-
cess is complete, WMT faces a large database
of comparisons and a question that must be an-
swered: whose system is the best?
</bodyText>
<footnote confidence="0.999198">
1Although in recent competitions, some of the judging has
also been crowdsourced (Callison-Burch et al., 2010).
2The example does not use actual system output.
</footnote>
<table confidence="0.965274833333333">
rank system translation
1 bbn “He does not go.”
2 (tie) uedin “He goes not.”
2 (tie) jhu “He did not go.”
4 cmu “He go not.”
5 kit “He not go.”
</table>
<tableCaption confidence="0.99627">
Table 1: WMT elicits preferences by asking
</tableCaption>
<bodyText confidence="0.925078388888889">
judges to simultaneously rank five translations,
with ties permitted. In this (fictional) example, the
source sentence is the French “Il ne va pas.”
source text sys1 sys2 judge preference
“Il ne va pas.” bbn cmu jdoe 1
“Il ne va pas.” bbn jhu jdoe 1
“Il ne va pas.” bbn kit jdoe 1
“Il ne va pas.” bbn uedin jdoe 1
“Il ne va pas.” cmu jhu jdoe 2
“Il ne va pas.” cmu kit jdoe 1
“Il ne va pas.” cmu uedin jdoe 2
“Il ne va pas.” jhu kit jdoe 1
“Il ne va pas.” jhu uedin jdoe 0
“Il ne va pas.” kit uedin jdoe 2
Table 2: Pairwise comparisons encoded by Ta-
ble 1. A preference of 0 means neither translation
was preferred. Otherwise the preference specifies
the preferred system.
</bodyText>
<sectionHeader confidence="0.977305" genericHeader="method">
2 A Ranking Problem
</sectionHeader>
<bodyText confidence="0.9994735">
For several years, WMT used the following heuris-
tic for ranking the translation systems:
</bodyText>
<equation confidence="0.999853">
win(s) + tie(s)
ORIGWMT(s) =
win(s) + tie(s) + loss(s)
</equation>
<bodyText confidence="0.996473125">
For system s, win(s) is the number of pairwise
comparisons in which s was preferred, loss(s) is
the number of comparisons in which s was dispre-
ferred, and tie(s) is the number of comparisons in
which s participated but neither system was pre-
ferred.
Recently, (Bojar et al., 2011) questioned the ad-
equacy of this heuristic through the following ar-
</bodyText>
<page confidence="0.927191">
1416
</page>
<note confidence="0.9135485">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1416–1424,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999026666666667">
gument. Consider a competition with systems A
and B. Suppose that the systems are different but
equally good, such that one third of the time A
is judged better than B, one third of the time B
is judged better than A, and one third of the time
they are judged to be equal. The expected values
of ORIGWMT(A) and ORIGWMT(B) are both
2/3, so the heuristic accurately judges the systems
to be equivalently good. Suppose however that
we had duplicated B and had submitted it to the
competition a second time as system C. Since B
and C produce identical translations, they should
always tie with one another. The expected value
of ORIGWMT(A) would not change, but the ex-
pected value of ORIGWMT(B) would increase to
5/6, buoyed by its ties with system C.
This vulnerability prompted (Bojar et al., 2011)
to offer the following revision:
</bodyText>
<equation confidence="0.999667333333333">
win(s)
BOJAR(s) =
win(s) + loss(s)
</equation>
<bodyText confidence="0.999960823529412">
The following year, it was BOJAR’s turn to be crit-
icized, this time by (Lopez, 2012):
Superficially, this appears to be an im-
provement....couldn’t a system still be
penalized simply by being compared
to [good systems] more frequently than
its competitors? On the other hand,
couldn’t a system be rewarded simply
by being compared against a bad system
more frequently than its competitors?
Lopez’s concern, while reasonable, is less obvi-
ously damning than (Bojar et al., 2011)’s criti-
cism of ORIGWMT. It depends on whether the
collected set of comparisons is small enough or
biased enough to make the variance in competi-
tion significant. While this hypothesis is plausi-
ble, Lopez makes no attempt to verify it. Instead,
he offers a ranking heuristic of his own, based on
a Minimum Feedback Arc solver.
The proliferation of ranking heuristics contin-
ued from there. The WMT 2012 organizers
(Callison-Burch et al., 2012) took Lopez’s ranking
scheme and provided a variant called Most Proba-
ble Ranking. Then, noting some potential pitfalls
with that, they created two more, called Monte
Carlo Playoffs and Expected Wins. While one
could raise philosophical objections about each of
these, where would it end? Ultimately, the WMT
2012 findings presented five different rankings for
the English-German competition track, with no
guidance about which ranking we should pay at-
tention to. How can we know whether one rank-
ing is better than other? Or is this even the right
question to ask?
</bodyText>
<sectionHeader confidence="0.994044" genericHeader="method">
3 A Problem with Rankings
</sectionHeader>
<bodyText confidence="0.961586666666667">
Suppose four systems participate in a translation
competition. Three of these systems are extremely
close in quality. We’ll call these close1, close2,
and close3. Nevertheless, close1 is very slightly
better3 than close2, and close2 is very slightly bet-
ter than close3. The fourth system, called terrific,
is a really terrific system that far exceeds the other
three.
Now which is the better ranking?
</bodyText>
<equation confidence="0.9958005">
terrific, close3, close1, close2 (1)
close1, terrific, close2, close3 (2)
</equation>
<bodyText confidence="0.9997463125">
Spearman’s rho4 would favor the second ranking,
since it is a less disruptive permutation of the gold
ranking. But intuition favors the first. While its
mistakes are minor, the second ranking makes the
hard-to-forgive mistake of placing close1 ahead of
the terrific system.
The problem is not with Spearman’s rho. The
problem is the disconnnect between the knowl-
edge that we want a ranking to reflect and the
knowledge that a ranking actually contains. With-
out this additional knowledge, we cannot deter-
mine whether one ranking is better than another,
even if we know the gold ranking. We need to
determine what information they lack, and define
more rigorously what we hope to learn from a
translation competition.
</bodyText>
<sectionHeader confidence="0.982154" genericHeader="method">
4 From Rankings to Relative Ability
</sectionHeader>
<bodyText confidence="0.995805454545455">
Ostensibly the purpose of a translation competi-
tion is to determine the relative ability of a set
of translation systems. Let S be the space of all
translation systems. Hereafter, we will refer to S
as the space of students. We choose this term to
evoke the metaphor of a translation competition as
a standardized test, which shares the same goal: to
assess the relative abilities of a set of participants.
But what exactly do we mean by “ability”? Be-
fore formally defining this term, first recognize
that it means little without context, namely:
</bodyText>
<footnote confidence="0.999744">
3What does “better” mean? We’ll return to this question.
4Or Pearson’s correlation coefficient.
</footnote>
<page confidence="0.987198">
1417
</page>
<listItem confidence="0.995336739130435">
1. What kind of source text do we want the
systems to translate well? Say system A is
great at translating travel-related documents,
but terrible at translating newswire. Mean-
while, system B is pretty good at both. The
question “which system is better?” requires
us to state how much we care about travel
versus newswire documents – otherwise the
question is underspecified.
2. Who are we trying to impress? While it’s
tempting to think that translation quality is
a universal notion, the 50-60% interannota-
tor agreement in WMT evaluations (Callison-
Burch et al., 2012) suggests otherwise. It’s
also easy to imagine reasons why one group
of judges might have different priorities than
another. Think a Fortune 500 company ver-
sus web forum users. Lawyers versus lay-
men. Non-native versus native speakers.
Posteditors versus Google Translate users.
Different groups have different uses for trans-
lation, and therefore different definitions of
what “better” means.
</listItem>
<bodyText confidence="0.99810985">
With this in mind, let’s define some additional el-
ements of a translation competition. Let X be the
space of all possible segments of source text, J be
the space of all possible judges, and H = 10, 1, 21
be the space of pairwise preferences.5 We as-
sume all spaces are countable. Unless stated oth-
erwise, variables s1 and s2 represent students from
S, variable x represents a segment from X, vari-
able j represents a judge from J , and variable π
represents a preference from H. Moreover, define
the negation πˆ of preference π such that πˆ = 2 (if
π = 1), πˆ = 1 (if π = 2), and πˆ = 0 (if π = 0).
Now assume a joint distribution
P(s1, s2, x, j, π) specifying the probability
that we ask judge j to evaluate students s1 and
s2’s respective translations of source text x, and
that judge j’s preference is π. We will further
assume that the choice of student pair, source
text, and judge are marginally independent of one
another. In other words:
</bodyText>
<equation confidence="0.964719285714286">
P(s1, s2, x, j, π)
P(π|s1, s2, x, j) &apos; P(x|s1, s2, j)
=
&apos;P(j|s1, s2) &apos; P(s1, s2)
= P(π|s1, s2, x, j) &apos; P(x) &apos; P(j) &apos; P(s1, s2)
= PX(x) &apos; PJ (j) &apos; P(s1, s2) &apos; P(π|s1, s2, x, j)
5As a reminder, 0 indicates no preference.
</equation>
<bodyText confidence="0.9999565">
It will be useful to reserve notation PX and PJ
for the marginal distributions over source text and
judges. We can marginalize over the source seg-
ments and judges to obtain a useful quantity:
</bodyText>
<equation confidence="0.996744333333333">
P(π|s1, s2)
�= � PX(x) &apos; PJ (j) &apos; P(π|s1, s2, x, j)
x∈X j∈J
</equation>
<bodyText confidence="0.999717588235294">
We refer to this as the (PX, PJ)-relative ability of
students s1 and s2. By using different marginal
distributions PX, we can specify what kinds of
source text interest us (for instance, PX could
focus most of its probability mass on German
tweets). Similarly, by using different marginal
distributions PJ , we can specify what judges we
want to impress (for instance, PJ could focus all
of its mass on one important corporate customer
or evenly among all fluent bilingual speakers of a
language pair).
With this machinery, we can express the pur-
pose of a translation competition more clearly:
to estimate the (PX, PJ)-relative ability of a set
of students. In the case of WMT, PJ presum-
ably6 defines a space of competent source-to-
target bilingual speakers, while PX defines a space
of newswire documents.
We’ll refer to an estimate of P(π|s1, s2) as
a preference model. In other words, a prefer-
ence model is a distribution Q(π|s1, s2). Given
a set of pairwise comparisons (e.g., Table 2),
the challenge is to estimate a preference model
Q(π|s1, s2) such that Q is “close” to P. For mea-
suring distributional proximity, a natural choice is
KL-divergence (Kullback and Leibler, 1951), but
we cannot use it here because P is unknown.
Fortunately, if we have i.i.d. data drawn from P,
then we can do the next best thing and compute the
perplexity of preference model Q on this heldout
test data. Let D be a sequence of triples (s1, s2, π)
where the preferences π are i.i.d. samples from
P(π|s1, s2). The perplexity of preference model
Q on test data D is:
</bodyText>
<equation confidence="0.911679">
perplexity(Q|D) = 2− rhs1,s2,πi∈D |D |log2 Q(π|s1,s2)
</equation>
<bodyText confidence="0.999090666666667">
How do we obtain such a test set from competi-
tion data? Recall that a WMT competition pro-
duces pairwise comparisons like those in Table 2.
</bodyText>
<footnote confidence="0.925782666666667">
6One could argue that it specifies a space of machine
translation specialists, but likely these individuals are thought
to be a representative sample of a broader community.
</footnote>
<page confidence="0.990862">
1418
</page>
<bodyText confidence="0.999762928571429">
Let C be the set of comparisons (s1, s2, x, j, 7r)
obtained from a translation competition. Com-
petition data C is not necessarily7 sampled i.i.d.
from P(s1, s2, x, j, 7r) because we may intention-
ally8 bias data collection towards certain students,
judges or source text. Also, because WMT elicits
its data in batches (see Table 1), every segment x
of source text appears in at least ten comparisons.
To create an appropriately-sized test set that
closely resembles i.i.d. data, we isolate the sub-
set C&apos; of comparisons whose source text appears
in at most k comparisons, where k is the smallest
positive integer such that |C&apos; |&gt;= 2000. We then
create the test set D from C&apos;:
</bodyText>
<equation confidence="0.943845">
D = {(s1, s2, 7r)|(s1, s2, x, j, 7r) E C&apos;}
</equation>
<bodyText confidence="0.9999355">
We reserve the remaining comparisons for training
preference models. Table 3 shows the resulting
dataset sizes for each competition track.
Unlike with raw rankings, the claim that
one preference model is better than another has
testable implications. Given two competing mod-
els, we can train them on the same comparisons,
and compare their perplexities on the test set. This
gives us a quantitative9 answer to the question of
which is the better model. We can then publish
a system ranking based on the most trustworthy
preference model.
</bodyText>
<sectionHeader confidence="0.997379" genericHeader="method">
5 Baselines
</sectionHeader>
<bodyText confidence="0.9974475">
Let’s begin then, and create some simple prefer-
ence models to serve as baselines.
</bodyText>
<subsectionHeader confidence="0.97172">
5.1 Uniform
</subsectionHeader>
<bodyText confidence="0.9980775">
The simplest preference model is a uniform distri-
bution over preferences, for any choice of students
</bodyText>
<equation confidence="0.856786">
s1, s2:
1
Q(7r|s1, s2) = 3 b7r E Π
</equation>
<bodyText confidence="0.999834333333333">
This will be our only model that does not require
training data, and its perplexity on any test set will
be 3 (i.e. equal to number of possible preferences).
</bodyText>
<subsectionHeader confidence="0.997213">
5.2 Adjusted Uniform
</subsectionHeader>
<bodyText confidence="0.993027666666667">
Now suppose we have a set C of comparisons
available for training. Let Cπ C C denote the
subset of comparisons with preference 7r, and let
</bodyText>
<footnote confidence="0.999503">
7In WMT, it certainly is not.
8To collect judge agreement statistics, for instance.
9As opposed to philosophical.
</footnote>
<bodyText confidence="0.998763428571429">
C(s1, s2) denote the subset comparing students s1
and s2.
Perhaps the simplest thing we can do with the
training data is to estimate the probability of ties
(i.e. preference 0). We can then distribute the
remaining probability mass uniformly among the
other two preferences:
</bodyText>
<equation confidence="0.9331464">
|C0 |if 7r = 0
|C|
1 _ |c0|
|� |otherwise
2
</equation>
<sectionHeader confidence="0.985357" genericHeader="method">
6 Simple Bayesian Models
</sectionHeader>
<subsectionHeader confidence="0.960984">
6.1 Independent Pairs
</subsectionHeader>
<bodyText confidence="0.997728">
Another simple model is the direct estimation of
each relative ability P(7r|s1, s2) independently. In
other words, for each pair of students s1 and s2, we
estimate a separate preference distribution. The
maximum likelihood estimate of each distribution
would be:
</bodyText>
<equation confidence="0.931411">
Q(7r|s1, s2) = |C(s1, s2) |+ |C(s2, s1)|
</equation>
<bodyText confidence="0.999855285714286">
However the maximum likelihood estimate would
test poorly, since any zero probability estimates
for test set preferences would result in infinite per-
plexity. To make this model practical, we assume a
symmetric Dirichlet prior with strength α for each
preference distribution. This gives us the follow-
ing Bayesian estimate:
</bodyText>
<equation confidence="0.788418">
Q(7r|s1, s2) = 3α + |C(s1, s2) |+ |C(s2, s1)|
</equation>
<bodyText confidence="0.9995635">
We call this the Independent Pairs preference
model.
</bodyText>
<subsectionHeader confidence="0.999263">
6.2 Independent Students
</subsectionHeader>
<bodyText confidence="0.999810692307692">
The Independent Pairs model makes a strong inde-
pendence assumption. It assumes that even if we
know that student A is much better than student B,
and that student B is much better than student C,
we can infer nothing about how student A will fare
versus student C. Instead of directly estimating the
relative ability P(7r|s1, s2) of students s1 and s2,
we could instead try to estimate the universal abil-
ity P(7r|s1) = Es2ES P(7r|s1, s2) · P(s2|s1) of
each individual student s1 and then try to recon-
struct the relative abilities from these estimates.
For the same reasons as before, we assume a
symmetric Dirichlet prior with strength α for each
</bodyText>
<equation confidence="0.999569">
Q(7r|s1, s2) = {
|Cπ(s1, s2) |+ |Cˆπ(s2, s1)|
α + |Cπ(s1, s2) |+ |Cˆπ(s2, s1)|
</equation>
<page confidence="0.947081">
1419
</page>
<bodyText confidence="0.9795255">
preference distribution, which gives us the follow-
ing Bayesian estimate:
</bodyText>
<equation confidence="0.999016">
α + Es2cS |Cπ(s1, s2) |+ |Cˆπ(s2, s1)|
3α + Es2cS |C(s1, s2) |+ |C(s2, s1)|
</equation>
<bodyText confidence="0.999819">
The estimates Q(7r|s1) do not yet constitute a pref-
erence model. A downside of this approach is that
there is no principled way to reconstruct a pref-
erence model from the universal ability estimates.
We experiment with three ad-hoc reconstructions.
The asymmetric reconstruction simply ignores any
information we have about student s2:
</bodyText>
<equation confidence="0.97644">
Q(7r|s1, s2) = Q(7r|s1)
</equation>
<bodyText confidence="0.999134333333333">
The arithmetic and geometric reconstructions
compute an arithmetic/geometric average of the
two universal abilities:
</bodyText>
<equation confidence="0.999785666666667">
Q(7r|s1, s2) = Q(7r|s1) + Q(ˆ7r|s2)
2
Q(7r|s1, s2) = [Q(7r|s1) * Q(ˆ7r|s2)]1 2
</equation>
<bodyText confidence="0.999960428571429">
We respectively call these the (Asymmet-
ric/Arithmetic/Geometric) Independent Students
preference models. Notice the similarities be-
tween the universal ability estimates Q(7r|s1) and
the BOJAR ranking heuristic. These three models
are our attempt to render the BOJAR heuristic as
preference models.
</bodyText>
<sectionHeader confidence="0.994223" genericHeader="method">
7 Item-Response Theoretic (IRT) Models
</sectionHeader>
<bodyText confidence="0.999974457142857">
Let’s revisit (Lopez, 2012)’s objection to the BO-
JAR ranking heuristic: “...couldn’t a system still be
penalized simply by being compared to [good sys-
tems] more frequently than its competitors?” The
official WMT 2012 findings (Callison-Burch et al.,
2012) echoes this concern in justifying the exclu-
sion of reference translations from the 2012 com-
petition:
[W]orkers have a very clear preference
for reference translations, so includ-
ing them unduly penalized systems that,
through (un)luck of the draw, were pit-
ted against the references more often.
Presuming the students are paired uniformly at
random, this issue diminishes as more compar-
isons are elicited. But preference elicitation is ex-
pensive, so it makes sense to assess the relative
ability of the students with as few elicitations as
possible. Still, WMT 2012’s decision to eliminate
references entirely is a bit of a draconian mea-
sure, a treatment of the symptom rather than the
(perceived) disease. If our models cannot function
in the presence of training data variation, then we
should change the models, not the data. A model
that only works when the students are all about the
same level is not one we should rely on.
We experiment with a simple model that relaxes
some independence assumptions made by previ-
ous models, in order to allow training data vari-
ation (e.g. who a student has been paired with)
to influence the estimation of the student abili-
ties. Figure 1(left) shows plate notation (Koller
and Friedman, 2009) for the model’s indepen-
dence structure. First, each student’s ability dis-
tribution is drawn from a common prior distribu-
tion. Then a number of translation items are gen-
erated. Each item is authored by a student and has
a quality drawn from the student’s ability distri-
bution. Then a number of pairwise comparisons
are generated. Each comparison has two options,
each a translation item. The quality of each item
is observed by a judge (possibly noisily) and then
the judge states a preference by comparing the two
observations.
We investigate two parameterizations of this
model: Gaussian and categorical. Figure 1(right)
shows an example of the Gaussian parameteriza-
tion. The student ability distributions are Gaus-
sians with a known standard deviation σa, drawn
from a zero-mean Gaussian prior with known stan-
dard deviation σ0. In the example, we show
the ability distributions for students 6 (an above-
average student, whose mean is 0.4) and 14 (a
poor student, whose mean is -0.6). We also show
an item authored by each student. Item 43 has
a somewhat low quality of -0.3 (drawn from stu-
dent 14’s ability distribution), while item 205 is
not student 6’s best work (he produces a mean
quality of 0.4), but still has a decent quality at 0.2.
Comparison 1 pits these items against one another.
A judge draws noise from a zero-mean Gaussian
with known standard deviation σobs, then adds this
to the item’s actual quality to get an observed qual-
ity. For the first option (item 43), the judge draws a
noise of -0.12 to observe a quality of -0.42 (worse
than it actually is). For the second option (item
205), the judge draws a noise of 0.15 to observe a
quality of 0.35 (better than it actually is). Finally,
the judge compares the two observed qualities. If
the absolute difference is lower than his decision
</bodyText>
<equation confidence="0.904399">
Q(7r|s1) =
</equation>
<page confidence="0.902449">
1420
</page>
<figure confidence="0.997145951219512">
decision.radius
student.prior
obs.parameters
student.s.ability item.i.author
comp.c.opt1.obs
comp.c.opt1
S
comp.c.pref
item.i.quality
comp.c.opt2
comp.c.opt2.obs
I
C
obs.parameters
Gauee(0.0, σobe)
student.14.ability
Gauee(-0.6, σa)
student.prior
Gauee(0.0, σ0)
student.6.ability
Gauee(0.4, σa)
decision.radius
0.5
comp.1.opt1.obs
-0.42
comp.1.opt1
item.43.author
14
item.43.quality
-0.3
43
comp.1.pref
2
item.205.author
6
comp.1.opt2.obs
0.35
item.205.quality
0.2
comp.1.opt2
205
</figure>
<figureCaption confidence="0.773297">
Figure 1: Plate notation (left) showing the independence structure of the IRT Models. Example instan-
tiated subnetwork (right) for the Gaussian parameterization. Shaded rectangles are hyperparameters.
Shaded ellipses are variables observable from a set of comparisons.
</figureCaption>
<bodyText confidence="0.992140565217391">
radius (which here is 0.5), then he states no prefer-
ence (i.e. a preference of 0). Otherwise he prefers
the item with the higher observed quality.
The categorical parameterization is similar to
the Gaussian parameterization, with the following
differences. Item quality is not continuous, but
rather a member of the discrete set 11, 2, ...,Al.
The student ability distributions are categorical
distributions over 11, 2, ..., Al, and the student
ability prior is a symmetric Dirichlet with strength
αa. Finally, the observed quality is the item qual-
ity A plus an integer-valued noise v E 11 −
A, ..., A − Al. Noise v is drawn from a discretized
zero-mean Gaussian with standard deviation σobs.
Specifically, Pr(v) is proportional to the value of
the probability density function of the zero-mean
Gaussian N(0, σobs).
We estimated the model parameters with Gibbs
sampling (Geman and Geman, 1984). We found
that Gibbs sampling converged quickly and con-
sistently10 for both parameterizations. Given the
parameter estimates, we obtain a preference model
Q(7r|s1, s2) through the inference query:
</bodyText>
<equation confidence="0.99984175">
Pr(comp.c0.pref = 7r  |item.i0.author = s1,
item.i00.author = s2,
comp.c0.opt1 = i0,
comp.c0.opt2 = i00)
</equation>
<bodyText confidence="0.991898888888889">
10We ran 200 iterations with a burn-in of 50.
where c0, i0, i00 are new comparison and item ids
that do not appear in the training data.
We call these models Item-Response Theo-
retic (IRT) models, to acknowledge their roots
in the psychometrics (Thurstone, 1927; Bradley
and Terry, 1952; Luce, 1959) and item-response
theory (Hambleton, 1991; van der Linden and
Hambleton, 1996; Baker, 2001) literature. Item-
response theory is the basis of modern testing
theory and drives adaptive standardized tests like
the Graduate Record Exam (GRE). In particular,
the Gaussian parameterization of our IRT models
strongly resembles11 the Thurstone (Thurstone,
1927) and Bradley-Terry-Luce (Bradley and Terry,
1952; Luce, 1959) models of paired compari-
son and the 1PL normal-ogive and Rasch (Rasch,
1960) models of student testing. From the test-
ing perspective, we can view each comparison as
two students simultaneously posing a test question
to the other: “Give me a translation of the source
text which is better than mine.” The students can
answer the question correctly, incorrectly, or they
can provide a translation of analogous quality. An
extra dimension of our models is judge noise, not
a factor when modeling multiple-choice tests, for
which the right answer is not subject to opinion.
</bodyText>
<footnote confidence="0.976174666666667">
11These models are not traditionally expressed using
graphical models, although it is not unprecedented (Mislevy
and Almond, 1997; Mislevy et al., 1999).
</footnote>
<page confidence="0.987666">
1421
</page>
<figureCaption confidence="0.986316666666667">
Figure 2: WMT10 model perplexities. The per-
plexity of the uniform preference model is 3.0 for
all training sizes.
</figureCaption>
<figure confidence="0.77149">
8 Experiments
</figure>
<figureCaption confidence="0.976966">
Figure 4: WMT12 model perplexities.
</figureCaption>
<table confidence="0.9981926">
lp wmt10 test wmt11 test wmt12 test
train train train
ce 3166 2209 1706 3216 5969 6806
fe 5918 2376 2556 4430 7982 5840
ge 7422 3002 3708 5371 8106 6032
se 8411 2896 1968 3684 3910 7376
ec 10490 3048 8859 9016 13770 9112
ef 5720 2242 3328 5758 7841 7508
eg 10852 2842 5964 7032 10210 7191
es 2962 2212 4768 6362 5664 8928
</table>
<tableCaption confidence="0.8487315">
Table 3: Dataset sizes for each competition track
(number of comparisons).
</tableCaption>
<figureCaption confidence="0.997517">
Figure 3: WMT11 model perplexities.
</figureCaption>
<bodyText confidence="0.999993333333333">
We organized the competition data as described at
the end of Section 4. To compare the preference
models, we did the following:
</bodyText>
<listItem confidence="0.965471833333333">
• Randomly chose a subset of k compar-
isons from the training set, for k E
{100, 200, 400, 800,1600, 3200}.12
• Trained the preference model on these com-
parisons.
• Evaluated the perplexity of the trained model
</listItem>
<bodyText confidence="0.987291857142857">
on the test preferences, as described in Sec-
tion 4.
For each model and training size, we averaged
the perplexities from 5 trials of each competition
track. We then plotted average perplexity as a
function of training size. These graphs are shown
12If k was greater than the total number of training com-
parisons, then we took the entire set.
in Figure 2 (WMT10)13, and Figure 4 (WMT12).
For WMT10 and WMT11, the best models were
the IRT models, with the Gaussian parameteriza-
tion converging the most rapidly and reaching the
lowest perplexity. For WMT12, in which refer-
ence translations were excluded from the compe-
tition, four models were nearly indistinguishable:
the two IRT models and the two averaged Indepen-
dent Student models. This somewhat validates the
organizers’ decision to exclude the references, par-
ticularly given WMT’s use of the BOJAR ranking
heuristic (the nucleus of the Independent Student
models) for its official rankings.
13Results for WMT10 exclude the German-English and
English-German tracks, since we used these to tune our
model hyperparameters. These were set as follows. The
Dirichlet strength for each baseline was 1. For IRT-Gaussian:
y0 = 1.0, yobs = 1.0, ya = 0.5, and the decision radius was
0.4. For IRT-Categorical: A = 8, yobs = 1.0, αa = 0.5, and
the decision radius was 0.
</bodyText>
<page confidence="0.99438">
1422
</page>
<figureCaption confidence="0.998819">
Figure 6: English-Czech WMT11 results (average of 5 trainings on 1600 comparisons). Error bars
(left) indicate one stddev of the estimated ability means. In the heatmap (right), cell (s1, s2) is darker if
preference model Q(π|s1, s2) skews in favor of student s1, lighter if it skews in favor of student s2.
Figure 5: WMT10 model perplexities (crowd-
sourced versus expert training).
</figureCaption>
<bodyText confidence="0.991736684210527">
The IRT models proved the most robust at han-
dling judge noise. We repeated the WMT10 ex-
periment using the same test sets, but using the
unfiltered crowdsourced comparisons (rather than
“expert”14 comparisons) for training. Figure 5
shows the results. Whereas the crowdsourced
noise considerably degraded the Geometric Inde-
pendent Students model, the IRT models were re-
markably robust. IRT-Gaussian in particular came
close to replicating the performance of Geometric
Independent Students trained on the much cleaner
expert data. This is rather impressive, since the
crowdsourced judges agree only 46.6% of the
time, compared to a 65.8% agreement rate among
14I.e., machine translation specialists.
expert judges (Callison-Burch et al., 2010).
Another nice property of the IRT models is
that they explicitly model student ability, so they
yield a natural ranking. For training size 1600 of
the WMT11 English-Czech track, Figure 6 (left)
shows the mean student abilities learned by the
IRT-Gaussian model. The error bars show one
standard deviation of the ability means (recall that
we performed 5 trials, each with a random training
subset of size 1600). These results provide fur-
ther insight into a case analyzed by (Lopez, 2012),
which raised concern about the relative ordering
of online-B, cu-bojar, and cu-marecek. Accord-
ing to IRT-Gaussian’s analysis of the data, these
three students are so close in ability that any order-
ing is essentially arbitrary. Short of a full ranking,
the analysis does suggest four strata. Viewing one
of IRT-Gaussian’s induced preference models as
a heatmap15 (Figure 6, right), four bands are dis-
cernable. First, the reference sentences are clearly
the darkest (best). Next come students 2-7, fol-
lowed by the slightly lighter (weaker) students 8-
10, followed by the lightest (weakest) student 11.
</bodyText>
<sectionHeader confidence="0.997934" genericHeader="evaluation">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99969575">
WMT has faced a crisis of confidence lately, with
researchers raising (real and conjectured) issues
with its analytical methodology. In this paper,
we showed how WMT can restore confidence in
</bodyText>
<footnote confidence="0.944146333333333">
15In the heatmap, cell (s1, s2) is darker if preference model
Q(π|s1, s2) skews in favor of student s1, lighter if it skews
in favor of student s2.
</footnote>
<page confidence="0.951733">
1423
</page>
<bodyText confidence="0.969441833333333">
its conclusions – by shifting the focus from rank-
ings to relative ability. Estimates of relative ability
(the expected head-to-head performance of system
pairs over a probability space of judges and source
text) can be empirically compared, granting sub-
stance to previously nebulous questions like:
1. Is my analysis better than your analysis?
Rather than the current anecdotal approach
to comparing competition analyses (e.g. pre-
senting example rankings that seem some-
how wrong), we can empirically compare the
predictive power of the models on test data.
</bodyText>
<sectionHeader confidence="0.632618" genericHeader="conclusions">
2. How much of an impact does judge noise
</sectionHeader>
<bodyText confidence="0.9470894375">
have on my conclusions? We showed
that judge noise can have a significant im-
pact on the quality of our conclusions, if we
use the wrong models. However, the IRT-
Gaussian appears to be quite noise-tolerant,
giving similar-quality conclusions on both
expert and crowdsourced comparisons.
3. How many comparisons should I elicit?
Many of our preference models (including
IRT-Gaussian and Geometric Independent
Students) are close to convergence at around
1000 comparisons. This suggests that we can
elicit far fewer comparisons and still derive
confident conclusions. This is the first time
a concrete answer to this question has been
provided.
</bodyText>
<sectionHeader confidence="0.999325" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999616413793103">
F.B. Baker. 2001. The basics of item response theory.
ERIC.
Ondej Bojar, Miloˇs Ercegovˇcevi´c, Martin Popel, and
Omar Zaidan. 2011. A grain of salt for the wmt
manual evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
1–11, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Ralph Allan Bradley and Milton E Terry. 1952. Rank
analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika, 39(3/4):324–
345.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O.F. Zaidan. 2010. Findings of
the 2010 joint workshop on statistical machine trans-
lation and metrics for machine translation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 17–
53. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation.
S. Geman and D. Geman. 1984. Stochastic relaxation,
gibbs distributions, and the bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6(6):721–741.
R.K. Hambleton. 1991. Fundamentals of item re-
sponse theory, volume 2. Sage Publications, Incor-
porated.
D. Koller and N. Friedman. 2009. Probabilistic graph-
ical models: principles and techniques. MIT press.
S. Kullback and R.A. Leibler. 1951. On information
and sufficiency. The Annals of Mathematical Statis-
tics, 22(1):79–86.
Adam Lopez. 2012. Putting human assessments of
machine translation systems in order. In Proceed-
ings of WMT.
R. Ducan Luce. 1959. Individual Choice Behavior a
Theoretical Analysis. John Wiley and sons.
R.J. Mislevy and R.G. Almond. 1997. Graphical mod-
els and computerized adaptive testing. UCLA CSE
Technical Report 434.
R.J. Mislevy, R.G. Almond, D. Yan, and L.S. Stein-
berg. 1999. Bayes nets in educational assessment:
Where the numbers come from. In Proceedings
of the fifteenth conference on uncertainty in artifi-
cial intelligence, pages 437–446. Morgan Kaufmann
Publishers Inc.
G. Rasch. 1960. Studies in mathematical psychology:
I. probabilistic models for some intelligence and at-
tainment tests.
Louis L Thurstone. 1927. A law of comparative judg-
ment. Psychological review, 34(4):273–286.
W.J. van der Linden and R.K. Hambleton. 1996.
Handbook of modern item response theory.
Springer.
</reference>
<page confidence="0.994642">
1424
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000519">
<title confidence="0.99991">Models of Translation Competitions</title>
<author confidence="0.999698">Mark Hopkins</author>
<author confidence="0.999698">Jonathan</author>
<affiliation confidence="0.789903">SDL</affiliation>
<address confidence="0.975308">6060 Center Drive, Suite Los Angeles, CA</address>
<abstract confidence="0.975287492917847">What do we want to learn from a translation competition and how do we learn it with confidence? We argue that a disproportionate focus on ranking competition participants has led to lots of different rankings, but little insight about which rankings we should trust. In response, we provide the first framework that allows an of different analyses of competition results. We then use this framework to compare several analytical models on data from the Workshop on Machine Translation (WMT). 1 The WMT Translation Competition Every year, the Workshop on Machine Translation (WMT) conducts a competition between machine translation systems. The WMT organizers invite research groups to submit translation systems in eight different tracks: Czech to/from English, French to/from English, German to/from English, and Spanish to/from English. For each track, the organizers also assemble a panel of judges, typically machine translation spe- The role of a judge is to repeatedly rank five different translations of the same source text. Ties are permitted. In Table 1, we show an exwhere a judge (we’ll call him “jdoe”) has ranked five translations of the French sentence “Il ne va pas.” Each such elicitation encodes ten pairwise comparisons, as shown in Table 2. For each competition track, WMT typically elicits between 5000 and 20000 comparisons. Once the elicitation process is complete, WMT faces a large database of comparisons and a question that must be answered: whose system is the best? in recent competitions, some of the judging has also been crowdsourced (Callison-Burch et al., 2010). example does not use actual system output. rank system translation 1 bbn uedin jhu cmu kit “He does not 2 (tie) 2 (tie) “He goes 4 “He did not 5 “He go “He not go.” Table 1: WMT elicits preferences by asking judges to simultaneously rank five translations, with ties permitted. In this (fictional) example, the source sentence is the French “Il ne va pas.” source text sys1 sys2 judge preference “Il ne va pas.” bbn cmu jdoe 1 “Il ne va pas.” bbn jhu jdoe 1 “Il ne va pas.” bbn kit jdoe 1 “Il ne va pas.” bbn uedin jdoe 1 “Il ne va pas.” cmu jhu jdoe 2 “Il ne va pas.” cmu kit jdoe 1 “Il ne va pas.” cmu uedin jdoe 2 “Il ne va pas.” jhu kit jdoe 1 “Il ne va pas.” jhu uedin jdoe 0 “Il ne va pas.” kit uedin jdoe 2 Table 2: Pairwise comparisons encoded by Table 1. A preference of 0 means neither translation was preferred. Otherwise the preference specifies the preferred system. 2 A Ranking Problem For several years, WMT used the following heuristic for ranking the translation systems: + = + + system is the number of pairwise in which preferred, is number of comparisons in which dispreand is the number of comparisons in but neither system was preferred. Recently, (Bojar et al., 2011) questioned the adof this heuristic through the following ar- 1416 of the 51st Annual Meeting of the Association for Computational pages Bulgaria, August 4-9 2013. Association for Computational Linguistics Consider a competition with systems Suppose that the systems are different but good, such that one third of the time judged better than one third of the time judged better than and one third of the time they are judged to be equal. The expected values and are both 2/3, so the heuristic accurately judges the systems to be equivalently good. Suppose however that had duplicated had submitted it to the a second time as system Since identical translations, they should always tie with one another. The expected value would not change, but the exvalue of would increase to buoyed by its ties with system This vulnerability prompted (Bojar et al., 2011) to offer the following revision: = + following year, it was turn to be criticized, this time by (Lopez, 2012): Superficially, this appears to be an improvement....couldn’t a system still be penalized simply by being compared to [good systems] more frequently than its competitors? On the other hand, couldn’t a system be rewarded simply by being compared against a bad system more frequently than its competitors? Lopez’s concern, while reasonable, is less obviously damning than (Bojar et al., 2011)’s critiof It depends on whether the collected set of comparisons is small enough or biased enough to make the variance in competition significant. While this hypothesis is plausible, Lopez makes no attempt to verify it. Instead, he offers a ranking heuristic of his own, based on a Minimum Feedback Arc solver. The proliferation of ranking heuristics continued from there. The WMT 2012 organizers (Callison-Burch et al., 2012) took Lopez’s ranking scheme and provided a variant called Most Probable Ranking. Then, noting some potential pitfalls with that, they created two more, called Monte Carlo Playoffs and Expected Wins. While one could raise philosophical objections about each of these, where would it end? Ultimately, the WMT 2012 findings presented five different rankings for the English-German competition track, with no guidance about which ranking we should pay attention to. How can we know whether one ranking is better than other? Or is this even the right question to ask? 3 A Problem with Rankings Suppose four systems participate in a translation competition. Three of these systems are extremely in quality. We’ll call these close2, very slightly than very slightly betthan fourth system, called is a really terrific system that far exceeds the other three. Now which is the better ranking? close3, close1, close2 terrific, close2, close3 would favor the second ranking, since it is a less disruptive permutation of the gold ranking. But intuition favors the first. While its mistakes are minor, the second ranking makes the mistake of placing of The problem is not with Spearman’s rho. The problem is the disconnnect between the knowledge that we want a ranking to reflect and the knowledge that a ranking actually contains. Without this additional knowledge, we cannot determine whether one ranking is better than another, even if we know the gold ranking. We need to determine what information they lack, and define more rigorously what we hope to learn from a translation competition. 4 From Rankings to Relative Ability Ostensibly the purpose of a translation competition is to determine the relative ability of a set translation systems. Let the space of all systems. Hereafter, we will refer to the space of We choose this term to evoke the metaphor of a translation competition as a standardized test, which shares the same goal: to assess the relative abilities of a set of participants. But what exactly do we mean by “ability”? Before formally defining this term, first recognize that it means little without context, namely: does “better” mean? We’ll return to this question. Pearson’s correlation coefficient. 1417 1. What kind of source text do we want the to translate well? system A is great at translating travel-related documents, but terrible at translating newswire. Meanwhile, system B is pretty good at both. The question “which system is better?” requires us to state how much we care about travel versus newswire documents – otherwise the question is underspecified. Who are we trying to impress? it’s tempting to think that translation quality is a universal notion, the 50-60% interannotator agreement in WMT evaluations (Callison- Burch et al., 2012) suggests otherwise. It’s also easy to imagine reasons why one group of judges might have different priorities than another. Think a Fortune 500 company versus web forum users. Lawyers versus laymen. Non-native versus native speakers. Posteditors versus Google Translate users. Different groups have different uses for translation, and therefore different definitions of what “better” means. With this in mind, let’s define some additional elof a translation competition. Let the of all possible segments of source text, space of all possible judges, and = the space of pairwise assume all spaces are countable. Unless stated othvariables students from variable a segment from varia judge from and variable a preference from Moreover, define preference that = 2 = 1 and = 0 Now assume a joint distribution x, j, the probability we ask judge evaluate students respective translations of source text and judge preference is We will further assume that the choice of student pair, source text, and judge are marginally independent of one another. In other words: x, j, x, = x, x, a reminder, 0 indicates no preference. will be useful to reserve notation for the marginal distributions over source text and judges. We can marginalize over the source segments and judges to obtain a useful quantity: x, refer to this as the ability By using different marginal we can specify what kinds of text interest us (for instance, focus most of its probability mass on German tweets). Similarly, by using different marginal we can specify what judges we to impress (for instance, focus all of its mass on one important corporate customer or evenly among all fluent bilingual speakers of a language pair). With this machinery, we can express the purpose of a translation competition more clearly: estimate the ability a set students. In the case of WMT, presuma space of competent source-tobilingual speakers, while a space of newswire documents. refer to an estimate of In other words, a prefermodel is a distribution Given a set of pairwise comparisons (e.g., Table 2), the challenge is to estimate a preference model that “close” to For measuring distributional proximity, a natural choice is KL-divergence (Kullback and Leibler, 1951), but cannot use it here because unknown. if we have i.i.d. data drawn from then we can do the next best thing and compute the of preference model this heldout data. Let a sequence of triples the preferences i.i.d. samples from The perplexity of preference model test data = |D| How do we obtain such a test set from competition data? Recall that a WMT competition produces pairwise comparisons like those in Table 2. could argue that it specifies a space of machine translation specialists, but likely these individuals are thought to be a representative sample of a broader community. 1418 the set of x, j, obtained from a translation competition. Comdata not sampled i.i.d. x, j, we may intentionbias data collection towards certain students, judges or source text. Also, because WMT elicits data in batches (see Table 1), every segment of source text appears in at least ten comparisons. To create an appropriately-sized test set that closely resembles i.i.d. data, we isolate the subcomparisons whose source text appears at most where the smallest integer such that We then the test set x, j, E We reserve the remaining comparisons for training preference models. Table 3 shows the resulting dataset sizes for each competition track. Unlike with raw rankings, the claim that one preference model is better than another has testable implications. Given two competing models, we can train them on the same comparisons, and compare their perplexities on the test set. This us a answer to the question of which is the better model. We can then publish a system ranking based on the most trustworthy preference model. 5 Baselines Let’s begin then, and create some simple preference models to serve as baselines. 5.1 Uniform The simplest preference model is a uniform distribution over preferences, for any choice of students 1 = 3 This will be our only model that does not require training data, and its perplexity on any test set will be 3 (i.e. equal to number of possible preferences). 5.2 Adjusted Uniform suppose we have a set comparisons for training. Let C C the of comparisons with preference and let WMT, it certainly is not. collect judge agreement statistics, for instance. opposed to philosophical. the subset comparing students Perhaps the simplest thing we can do with the training data is to estimate the probability of ties (i.e. preference 0). We can then distribute the remaining probability mass uniformly among the other two preferences: if 0 |C| |�|otherwise 2 6 Simple Bayesian Models 6.1 Independent Pairs Another simple model is the direct estimation of relative ability In words, for each pair of students we estimate a separate preference distribution. The maximum likelihood estimate of each distribution would be: = However the maximum likelihood estimate would test poorly, since any zero probability estimates for test set preferences would result in infinite perplexity. To make this model practical, we assume a Dirichlet prior with strength each preference distribution. This gives us the following Bayesian estimate: = We call this the Independent Pairs preference model. 6.2 Independent Students The Independent Pairs model makes a strong independence assumption. It assumes that even if we know that student A is much better than student B, and that student B is much better than student C, we can infer nothing about how student A will fare versus student C. Instead of directly estimating the ability students could instead try to estimate the abil- = individual student then try to reconstruct the relative abilities from these estimates. For the same reasons as before, we assume a Dirichlet prior with strength each = + + 1419 preference distribution, which gives us the following Bayesian estimate: estimates not yet constitute a preference model. A downside of this approach is that there is no principled way to reconstruct a preference model from the universal ability estimates. We experiment with three ad-hoc reconstructions. simply ignores any we have about student = compute an arithmetic/geometric average of the two universal abilities: = + 2 = 2 We respectively call these the (Asymmetric/Arithmetic/Geometric) Independent Students preference models. Notice the similarities bethe universal ability estimates heuristic. These three models our attempt to render the as preference models. 7 Item-Response Theoretic (IRT) Models revisit (Lopez, 2012)’s objection to the heuristic: “...couldn’t a system still be penalized simply by being compared to [good systems] more frequently than its competitors?” The official WMT 2012 findings (Callison-Burch et al., 2012) echoes this concern in justifying the exclusion of reference translations from the 2012 competition: [W]orkers have a very clear preference for reference translations, so including them unduly penalized systems that, through (un)luck of the draw, were pitted against the references more often. Presuming the students are paired uniformly at random, this issue diminishes as more comparisons are elicited. But preference elicitation is expensive, so it makes sense to assess the relative ability of the students with as few elicitations as possible. Still, WMT 2012’s decision to eliminate references entirely is a bit of a draconian measure, a treatment of the symptom rather than the (perceived) disease. If our models cannot function in the presence of training data variation, then we should change the models, not the data. A model that only works when the students are all about the same level is not one we should rely on. We experiment with a simple model that relaxes some independence assumptions made by previous models, in order to allow training data variation (e.g. who a student has been paired with) to influence the estimation of the student abilities. Figure 1(left) shows plate notation (Koller and Friedman, 2009) for the model’s indepenstructure. First, each student’s distribution is drawn from a common prior distribu- Then a number of translation gen- Each a student and has from the student’s distri- Then a number of pairwise generated. Each two a translation The each item a judge (possibly noisily) and then judge states a comparing the two We investigate two parameterizations of this model: Gaussian and categorical. Figure 1(right) shows an example of the Gaussian parameterization. The student ability distributions are Gauswith a known standard deviation drawn from a zero-mean Gaussian prior with known standeviation In the example, we show the ability distributions for students 6 (an aboveaverage student, whose mean is 0.4) and 14 (a poor student, whose mean is -0.6). We also show an item authored by each student. Item 43 has a somewhat low quality of -0.3 (drawn from student 14’s ability distribution), while item 205 is not student 6’s best work (he produces a mean quality of 0.4), but still has a decent quality at 0.2. Comparison 1 pits these items against one another. A judge draws noise from a zero-mean Gaussian known standard deviation then adds this to the item’s actual quality to get an observed quality. For the first option (item 43), the judge draws a noise of -0.12 to observe a quality of -0.42 (worse than it actually is). For the second option (item 205), the judge draws a noise of 0.15 to observe a quality of 0.35 (better than it actually is). Finally, the judge compares the two observed qualities. If the absolute difference is lower than his decision = 1420 decision.radius student.prior obs.parameters student.s.ability item.i.author comp.c.opt1.obs comp.c.opt1 S comp.c.pref item.i.quality comp.c.opt2 comp.c.opt2.obs I C obs.parameters student.14.ability student.6.ability decision.radius 0.5 comp.1.opt1.obs -0.42 comp.1.opt1 item.43.author 14 item.43.quality -0.3 43 comp.1.pref 2 item.205.author 6 comp.1.opt2.obs 0.35 item.205.quality 0.2 comp.1.opt2 205 Figure 1: Plate notation (left) showing the independence structure of the IRT Models. Example instantiated subnetwork (right) for the Gaussian parameterization. Shaded rectangles are hyperparameters. Shaded ellipses are variables observable from a set of comparisons. radius (which here is 0.5), then he states no preference (i.e. a preference of 0). Otherwise he prefers the item with the higher observed quality. The categorical parameterization is similar to the Gaussian parameterization, with the following differences. Item quality is not continuous, but a member of the discrete set The student ability distributions are categorical over ..., and the student ability prior is a symmetric Dirichlet with strength Finally, the observed quality is the item qualan integer-valued noise E ..., Noise drawn from a discretized Gaussian with standard deviation is proportional to the value of the probability density function of the zero-mean We estimated the model parameters with Gibbs sampling (Geman and Geman, 1984). We found that Gibbs sampling converged quickly and confor both parameterizations. Given the parameter estimates, we obtain a preference model through the inference query: | ran 200 iterations with a burn-in of 50. are new comparison and item ids that do not appear in the training data. We call these models Item-Response Theoretic (IRT) models, to acknowledge their roots in the psychometrics (Thurstone, 1927; Bradley and Terry, 1952; Luce, 1959) and item-response theory (Hambleton, 1991; van der Linden and Hambleton, 1996; Baker, 2001) literature. Itemresponse theory is the basis of modern testing theory and drives adaptive standardized tests like the Graduate Record Exam (GRE). In particular, the Gaussian parameterization of our IRT models the Thurstone (Thurstone, 1927) and Bradley-Terry-Luce (Bradley and Terry, 1952; Luce, 1959) models of paired comparison and the 1PL normal-ogive and Rasch (Rasch, 1960) models of student testing. From the testing perspective, we can view each comparison as two students simultaneously posing a test question to the other: “Give me a translation of the source text which is better than mine.” The students can answer the question correctly, incorrectly, or they can provide a translation of analogous quality. An extra dimension of our models is judge noise, not a factor when modeling multiple-choice tests, for which the right answer is not subject to opinion. models are not traditionally expressed using graphical models, although it is not unprecedented (Mislevy and Almond, 1997; Mislevy et al., 1999). 1421 Figure 2: WMT10 model perplexities. The perplexity of the uniform preference model is 3.0 for all training sizes. 8 Experiments Figure 4: WMT12 model perplexities. lp wmt10 train test wmt11 train test wmt12 train test ce 3166 2209 1706 3216 5969 6806 fe 5918 2376 2556 4430 7982 5840 ge 7422 3002 3708 5371 8106 6032 se 8411 2896 1968 3684 3910 7376 ec 10490 3048 8859 9016 13770 9112 ef 5720 2242 3328 5758 7841 7508 eg 10852 2842 5964 7032 10210 7191 es 2962 2212 4768 6362 5664 8928 Table 3: Dataset sizes for each competition track (number of comparisons). Figure 3: WMT11 model perplexities. We organized the competition data as described at the end of Section 4. To compare the preference models, we did the following: Randomly chose a subset of comparfrom the training set, for • Trained the preference model on these comparisons. • Evaluated the perplexity of the trained model on the test preferences, as described in Section 4. For each model and training size, we averaged the perplexities from 5 trials of each competition track. We then plotted average perplexity as a function of training size. These graphs are shown greater than the total number of training comparisons, then we took the entire set. Figure 2 and Figure 4 (WMT12). For WMT10 and WMT11, the best models were the IRT models, with the Gaussian parameterization converging the most rapidly and reaching the lowest perplexity. For WMT12, in which reference translations were excluded from the competition, four models were nearly indistinguishable: the two IRT models and the two averaged Independent Student models. This somewhat validates the organizers’ decision to exclude the references, pargiven WMT’s use of the heuristic (the nucleus of the Independent Student models) for its official rankings. for WMT10 exclude the German-English and English-German tracks, since we used these to tune our model hyperparameters. These were set as follows. The Dirichlet strength for each baseline was 1. For IRT-Gaussian: 1.0, 1.0, = 0.5, the decision radius was For IRT-Categorical: = 8, 1.0, = 0.5, the decision radius was 0. 1422 Figure 6: English-Czech WMT11 results (average of 5 trainings on 1600 comparisons). Error bars indicate one stddev of the estimated ability means. In the heatmap (right), cell darker if model in favor of student lighter if it skews in favor of student Figure 5: WMT10 model perplexities (crowdsourced versus expert training). The IRT models proved the most robust at handling judge noise. We repeated the WMT10 experiment using the same test sets, but using the unfiltered crowdsourced comparisons (rather than comparisons) for training. Figure 5 shows the results. Whereas the crowdsourced noise considerably degraded the Geometric Independent Students model, the IRT models were remarkably robust. IRT-Gaussian in particular came close to replicating the performance of Geometric Independent Students trained on the much cleaner expert data. This is rather impressive, since the crowdsourced judges agree only 46.6% of the time, compared to a 65.8% agreement rate among machine translation specialists. expert judges (Callison-Burch et al., 2010). Another nice property of the IRT models is that they explicitly model student ability, so they yield a natural ranking. For training size 1600 of the WMT11 English-Czech track, Figure 6 (left) shows the mean student abilities learned by the IRT-Gaussian model. The error bars show one standard deviation of the ability means (recall that we performed 5 trials, each with a random training subset of size 1600). These results provide further insight into a case analyzed by (Lopez, 2012), which raised concern about the relative ordering cu-bojar, According to IRT-Gaussian’s analysis of the data, these three students are so close in ability that any ordering is essentially arbitrary. Short of a full ranking, the analysis does suggest four strata. Viewing one of IRT-Gaussian’s induced preference models as (Figure 6, right), four bands are discernable. First, the reference sentences are clearly the darkest (best). Next come students 2-7, followed by the slightly lighter (weaker) students 8- 10, followed by the lightest (weakest) student 11. 9 Conclusion WMT has faced a crisis of confidence lately, with researchers raising (real and conjectured) issues with its analytical methodology. In this paper, we showed how WMT can restore confidence in the heatmap, cell darker if preference model in favor of student lighter if it skews favor of student 1423 conclusions – by shifting the focus from rank- Estimates of relative ability (the expected head-to-head performance of system pairs over a probability space of judges and source text) can be empirically compared, granting substance to previously nebulous questions like: 1. Is my analysis better than your analysis? Rather than the current anecdotal approach to comparing competition analyses (e.g. presenting example rankings that seem somehow wrong), we can empirically compare the predictive power of the models on test data. 2. How much of an impact does judge noise on my conclusions? that judge noise can have a significant impact on the quality of our conclusions, if we use the wrong models. However, the IRT- Gaussian appears to be quite noise-tolerant, giving similar-quality conclusions on both expert and crowdsourced comparisons. 3. How many comparisons should I elicit? Many of our preference models (including IRT-Gaussian and Geometric Independent Students) are close to convergence at around 1000 comparisons. This suggests that we can elicit far fewer comparisons and still derive confident conclusions. This is the first time a concrete answer to this question has been provided. References Baker. 2001. basics of item response ERIC. Ondej Bojar, Miloˇs Ercegovˇcevi´c, Martin Popel, and Omar Zaidan. 2011. A grain of salt for the wmt evaluation. In of the Sixth on Statistical Machine pages 1–11, Edinburgh, Scotland, July. Association for Computational Linguistics. Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. the method paired comparisons. 39(3/4):324– 345. C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, M. Przybocki, and O.F. Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine transand metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical</abstract>
<note confidence="0.9087898">Translation and pages 17– 53. Association for Computational Linguistics. Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical ma-</note>
<title confidence="0.5381675">translation. In of the Seventh on Statistical Machine</title>
<author confidence="0.464356">Stochastic relaxation</author>
<abstract confidence="0.90903952173913">gibbs distributions, and the bayesian restoration of Transactions on Pattern Analysis and 6(6):721–741. Hambleton. 1991. of item revolume 2. Sage Publications, Incorporated. Koller and N. Friedman. 2009. graphmodels: principles and MIT press. S. Kullback and R.A. Leibler. 1951. On information sufficiency. Annals of Mathematical Statis- 22(1):79–86. Adam Lopez. 2012. Putting human assessments of translation systems in order. In Proceedof Ducan Luce. 1959. Choice Behavior a John Wiley and sons. R.J. Mislevy and R.G. Almond. 1997. Graphical modand computerized adaptive testing. CSE Report R.J. Mislevy, R.G. Almond, D. Yan, and L.S. Steinberg. 1999. Bayes nets in educational assessment: the numbers come from. In of the fifteenth conference on uncertainty in artifi-</abstract>
<author confidence="0.673079">Morgan Kaufmann</author>
<affiliation confidence="0.71754">Publishers Inc.</affiliation>
<address confidence="0.47019">G. Rasch. 1960. Studies in mathematical psychology:</address>
<abstract confidence="0.919013">I. probabilistic models for some intelligence and attainment tests. Louis L Thurstone. 1927. A law of comparative judg- 34(4):273–286. W.J. van der Linden and R.K. Hambleton. 1996. of modern item response Springer.</abstract>
<address confidence="0.42647">1424</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F B Baker</author>
</authors>
<title>The basics of item response theory.</title>
<date>2001</date>
<publisher>ERIC.</publisher>
<contexts>
<context position="22426" citStr="Baker, 2001" startWordPosition="3749" endWordPosition="3750">ameterizations. Given the parameter estimates, we obtain a preference model Q(7r|s1, s2) through the inference query: Pr(comp.c0.pref = 7r |item.i0.author = s1, item.i00.author = s2, comp.c0.opt1 = i0, comp.c0.opt2 = i00) 10We ran 200 iterations with a burn-in of 50. where c0, i0, i00 are new comparison and item ids that do not appear in the training data. We call these models Item-Response Theoretic (IRT) models, to acknowledge their roots in the psychometrics (Thurstone, 1927; Bradley and Terry, 1952; Luce, 1959) and item-response theory (Hambleton, 1991; van der Linden and Hambleton, 1996; Baker, 2001) literature. Itemresponse theory is the basis of modern testing theory and drives adaptive standardized tests like the Graduate Record Exam (GRE). In particular, the Gaussian parameterization of our IRT models strongly resembles11 the Thurstone (Thurstone, 1927) and Bradley-Terry-Luce (Bradley and Terry, 1952; Luce, 1959) models of paired comparison and the 1PL normal-ogive and Rasch (Rasch, 1960) models of student testing. From the testing perspective, we can view each comparison as two students simultaneously posing a test question to the other: “Give me a translation of the source text whic</context>
</contexts>
<marker>Baker, 2001</marker>
<rawString>F.B. Baker. 2001. The basics of item response theory. ERIC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondej Bojar</author>
<author>Miloˇs Ercegovˇcevi´c</author>
<author>Martin Popel</author>
<author>Omar Zaidan</author>
</authors>
<title>A grain of salt for the wmt manual evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Bojar, Ercegovˇcevi´c, Popel, Zaidan, 2011</marker>
<rawString>Ondej Bojar, Miloˇs Ercegovˇcevi´c, Martin Popel, and Omar Zaidan. 2011. A grain of salt for the wmt manual evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 1–11, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Allan Bradley</author>
<author>Milton E Terry</author>
</authors>
<title>Rank analysis of incomplete block designs: I. the method of paired comparisons.</title>
<date>1952</date>
<journal>Biometrika,</journal>
<volume>39</volume>
<issue>3</issue>
<pages>345</pages>
<contexts>
<context position="22321" citStr="Bradley and Terry, 1952" startWordPosition="3732" endWordPosition="3735">bbs sampling (Geman and Geman, 1984). We found that Gibbs sampling converged quickly and consistently10 for both parameterizations. Given the parameter estimates, we obtain a preference model Q(7r|s1, s2) through the inference query: Pr(comp.c0.pref = 7r |item.i0.author = s1, item.i00.author = s2, comp.c0.opt1 = i0, comp.c0.opt2 = i00) 10We ran 200 iterations with a burn-in of 50. where c0, i0, i00 are new comparison and item ids that do not appear in the training data. We call these models Item-Response Theoretic (IRT) models, to acknowledge their roots in the psychometrics (Thurstone, 1927; Bradley and Terry, 1952; Luce, 1959) and item-response theory (Hambleton, 1991; van der Linden and Hambleton, 1996; Baker, 2001) literature. Itemresponse theory is the basis of modern testing theory and drives adaptive standardized tests like the Graduate Record Exam (GRE). In particular, the Gaussian parameterization of our IRT models strongly resembles11 the Thurstone (Thurstone, 1927) and Bradley-Terry-Luce (Bradley and Terry, 1952; Luce, 1959) models of paired comparison and the 1PL normal-ogive and Rasch (Rasch, 1960) models of student testing. From the testing perspective, we can view each comparison as two st</context>
</contexts>
<marker>Bradley, Terry, 1952</marker>
<rawString>Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324– 345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>K Peterson</author>
<author>M Przybocki</author>
<author>O F Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>pages</pages>
<contexts>
<context position="1804" citStr="Callison-Burch et al., 2010" startWordPosition="288" endWordPosition="291"> different translations of the same source text. Ties are permitted. In Table 1, we show an example2 where a judge (we’ll call him “jdoe”) has ranked five translations of the French sentence “Il ne va pas.” Each such elicitation encodes ten pairwise comparisons, as shown in Table 2. For each competition track, WMT typically elicits between 5000 and 20000 comparisons. Once the elicitation process is complete, WMT faces a large database of comparisons and a question that must be answered: whose system is the best? 1Although in recent competitions, some of the judging has also been crowdsourced (Callison-Burch et al., 2010). 2The example does not use actual system output. rank system translation 1 bbn “He does not go.” 2 (tie) uedin “He goes not.” 2 (tie) jhu “He did not go.” 4 cmu “He go not.” 5 kit “He not go.” Table 1: WMT elicits preferences by asking judges to simultaneously rank five translations, with ties permitted. In this (fictional) example, the source sentence is the French “Il ne va pas.” source text sys1 sys2 judge preference “Il ne va pas.” bbn cmu jdoe 1 “Il ne va pas.” bbn jhu jdoe 1 “Il ne va pas.” bbn kit jdoe 1 “Il ne va pas.” bbn uedin jdoe 1 “Il ne va pas.” cmu jhu jdoe 2 “Il ne va pas.” cm</context>
<context position="26854" citStr="Callison-Burch et al., 2010" startWordPosition="4469" endWordPosition="4472">st sets, but using the unfiltered crowdsourced comparisons (rather than “expert”14 comparisons) for training. Figure 5 shows the results. Whereas the crowdsourced noise considerably degraded the Geometric Independent Students model, the IRT models were remarkably robust. IRT-Gaussian in particular came close to replicating the performance of Geometric Independent Students trained on the much cleaner expert data. This is rather impressive, since the crowdsourced judges agree only 46.6% of the time, compared to a 65.8% agreement rate among 14I.e., machine translation specialists. expert judges (Callison-Burch et al., 2010). Another nice property of the IRT models is that they explicitly model student ability, so they yield a natural ranking. For training size 1600 of the WMT11 English-Czech track, Figure 6 (left) shows the mean student abilities learned by the IRT-Gaussian model. The error bars show one standard deviation of the ability means (recall that we performed 5 trials, each with a random training subset of size 1600). These results provide further insight into a case analyzed by (Lopez, 2012), which raised concern about the relative ordering of online-B, cu-bojar, and cu-marecek. According to IRT-Gauss</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, M. Przybocki, and O.F. Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 17– 53. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="5198" citStr="Callison-Burch et al., 2012" startWordPosition="885" endWordPosition="888">stem be rewarded simply by being compared against a bad system more frequently than its competitors? Lopez’s concern, while reasonable, is less obviously damning than (Bojar et al., 2011)’s criticism of ORIGWMT. It depends on whether the collected set of comparisons is small enough or biased enough to make the variance in competition significant. While this hypothesis is plausible, Lopez makes no attempt to verify it. Instead, he offers a ranking heuristic of his own, based on a Minimum Feedback Arc solver. The proliferation of ranking heuristics continued from there. The WMT 2012 organizers (Callison-Burch et al., 2012) took Lopez’s ranking scheme and provided a variant called Most Probable Ranking. Then, noting some potential pitfalls with that, they created two more, called Monte Carlo Playoffs and Expected Wins. While one could raise philosophical objections about each of these, where would it end? Ultimately, the WMT 2012 findings presented five different rankings for the English-German competition track, with no guidance about which ranking we should pay attention to. How can we know whether one ranking is better than other? Or is this even the right question to ask? 3 A Problem with Rankings Suppose fo</context>
<context position="17036" citStr="Callison-Burch et al., 2012" startWordPosition="2895" endWordPosition="2898">s1, s2) = [Q(7r|s1) * Q(ˆ7r|s2)]1 2 We respectively call these the (Asymmetric/Arithmetic/Geometric) Independent Students preference models. Notice the similarities between the universal ability estimates Q(7r|s1) and the BOJAR ranking heuristic. These three models are our attempt to render the BOJAR heuristic as preference models. 7 Item-Response Theoretic (IRT) Models Let’s revisit (Lopez, 2012)’s objection to the BOJAR ranking heuristic: “...couldn’t a system still be penalized simply by being compared to [good systems] more frequently than its competitors?” The official WMT 2012 findings (Callison-Burch et al., 2012) echoes this concern in justifying the exclusion of reference translations from the 2012 competition: [W]orkers have a very clear preference for reference translations, so including them unduly penalized systems that, through (un)luck of the draw, were pitted against the references more often. Presuming the students are paired uniformly at random, this issue diminishes as more comparisons are elicited. But preference elicitation is expensive, so it makes sense to assess the relative ability of the students with as few elicitations as possible. Still, WMT 2012’s decision to eliminate references</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, gibbs distributions, and the bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="21734" citStr="Geman and Geman, 1984" startWordPosition="3640" endWordPosition="3643">s not continuous, but rather a member of the discrete set 11, 2, ...,Al. The student ability distributions are categorical distributions over 11, 2, ..., Al, and the student ability prior is a symmetric Dirichlet with strength αa. Finally, the observed quality is the item quality A plus an integer-valued noise v E 11 − A, ..., A − Al. Noise v is drawn from a discretized zero-mean Gaussian with standard deviation σobs. Specifically, Pr(v) is proportional to the value of the probability density function of the zero-mean Gaussian N(0, σobs). We estimated the model parameters with Gibbs sampling (Geman and Geman, 1984). We found that Gibbs sampling converged quickly and consistently10 for both parameterizations. Given the parameter estimates, we obtain a preference model Q(7r|s1, s2) through the inference query: Pr(comp.c0.pref = 7r |item.i0.author = s1, item.i00.author = s2, comp.c0.opt1 = i0, comp.c0.opt2 = i00) 10We ran 200 iterations with a burn-in of 50. where c0, i0, i00 are new comparison and item ids that do not appear in the training data. We call these models Item-Response Theoretic (IRT) models, to acknowledge their roots in the psychometrics (Thurstone, 1927; Bradley and Terry, 1952; Luce, 1959)</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6(6):721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R K Hambleton</author>
</authors>
<title>Fundamentals of item response theory,</title>
<date>1991</date>
<volume>2</volume>
<publisher>Sage Publications, Incorporated.</publisher>
<contexts>
<context position="22376" citStr="Hambleton, 1991" startWordPosition="3741" endWordPosition="3742">ing converged quickly and consistently10 for both parameterizations. Given the parameter estimates, we obtain a preference model Q(7r|s1, s2) through the inference query: Pr(comp.c0.pref = 7r |item.i0.author = s1, item.i00.author = s2, comp.c0.opt1 = i0, comp.c0.opt2 = i00) 10We ran 200 iterations with a burn-in of 50. where c0, i0, i00 are new comparison and item ids that do not appear in the training data. We call these models Item-Response Theoretic (IRT) models, to acknowledge their roots in the psychometrics (Thurstone, 1927; Bradley and Terry, 1952; Luce, 1959) and item-response theory (Hambleton, 1991; van der Linden and Hambleton, 1996; Baker, 2001) literature. Itemresponse theory is the basis of modern testing theory and drives adaptive standardized tests like the Graduate Record Exam (GRE). In particular, the Gaussian parameterization of our IRT models strongly resembles11 the Thurstone (Thurstone, 1927) and Bradley-Terry-Luce (Bradley and Terry, 1952; Luce, 1959) models of paired comparison and the 1PL normal-ogive and Rasch (Rasch, 1960) models of student testing. From the testing perspective, we can view each comparison as two students simultaneously posing a test question to the oth</context>
</contexts>
<marker>Hambleton, 1991</marker>
<rawString>R.K. Hambleton. 1991. Fundamentals of item response theory, volume 2. Sage Publications, Incorporated.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>N Friedman</author>
</authors>
<title>Probabilistic graphical models: principles and techniques.</title>
<date>2009</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="18269" citStr="Koller and Friedman, 2009" startWordPosition="3100" endWordPosition="3103">ly is a bit of a draconian measure, a treatment of the symptom rather than the (perceived) disease. If our models cannot function in the presence of training data variation, then we should change the models, not the data. A model that only works when the students are all about the same level is not one we should rely on. We experiment with a simple model that relaxes some independence assumptions made by previous models, in order to allow training data variation (e.g. who a student has been paired with) to influence the estimation of the student abilities. Figure 1(left) shows plate notation (Koller and Friedman, 2009) for the model’s independence structure. First, each student’s ability distribution is drawn from a common prior distribution. Then a number of translation items are generated. Each item is authored by a student and has a quality drawn from the student’s ability distribution. Then a number of pairwise comparisons are generated. Each comparison has two options, each a translation item. The quality of each item is observed by a judge (possibly noisily) and then the judge states a preference by comparing the two observations. We investigate two parameterizations of this model: Gaussian and catego</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>D. Koller and N. Friedman. 2009. Probabilistic graphical models: principles and techniques. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kullback</author>
<author>R A Leibler</author>
</authors>
<title>On information and sufficiency.</title>
<date>1951</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="11219" citStr="Kullback and Leibler, 1951" startWordPosition="1920" endWordPosition="1923">a translation competition more clearly: to estimate the (PX, PJ)-relative ability of a set of students. In the case of WMT, PJ presumably6 defines a space of competent source-totarget bilingual speakers, while PX defines a space of newswire documents. We’ll refer to an estimate of P(π|s1, s2) as a preference model. In other words, a preference model is a distribution Q(π|s1, s2). Given a set of pairwise comparisons (e.g., Table 2), the challenge is to estimate a preference model Q(π|s1, s2) such that Q is “close” to P. For measuring distributional proximity, a natural choice is KL-divergence (Kullback and Leibler, 1951), but we cannot use it here because P is unknown. Fortunately, if we have i.i.d. data drawn from P, then we can do the next best thing and compute the perplexity of preference model Q on this heldout test data. Let D be a sequence of triples (s1, s2, π) where the preferences π are i.i.d. samples from P(π|s1, s2). The perplexity of preference model Q on test data D is: perplexity(Q|D) = 2− rhs1,s2,πi∈D |D |log2 Q(π|s1,s2) How do we obtain such a test set from competition data? Recall that a WMT competition produces pairwise comparisons like those in Table 2. 6One could argue that it specifies a</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>S. Kullback and R.A. Leibler. 1951. On information and sufficiency. The Annals of Mathematical Statistics, 22(1):79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Putting human assessments of machine translation systems in order.</title>
<date>2012</date>
<booktitle>In Proceedings of WMT.</booktitle>
<contexts>
<context position="4366" citStr="Lopez, 2012" startWordPosition="753" endWordPosition="754">o the heuristic accurately judges the systems to be equivalently good. Suppose however that we had duplicated B and had submitted it to the competition a second time as system C. Since B and C produce identical translations, they should always tie with one another. The expected value of ORIGWMT(A) would not change, but the expected value of ORIGWMT(B) would increase to 5/6, buoyed by its ties with system C. This vulnerability prompted (Bojar et al., 2011) to offer the following revision: win(s) BOJAR(s) = win(s) + loss(s) The following year, it was BOJAR’s turn to be criticized, this time by (Lopez, 2012): Superficially, this appears to be an improvement....couldn’t a system still be penalized simply by being compared to [good systems] more frequently than its competitors? On the other hand, couldn’t a system be rewarded simply by being compared against a bad system more frequently than its competitors? Lopez’s concern, while reasonable, is less obviously damning than (Bojar et al., 2011)’s criticism of ORIGWMT. It depends on whether the collected set of comparisons is small enough or biased enough to make the variance in competition significant. While this hypothesis is plausible, Lopez makes</context>
<context position="16808" citStr="Lopez, 2012" startWordPosition="2862" endWordPosition="2863">n we have about student s2: Q(7r|s1, s2) = Q(7r|s1) The arithmetic and geometric reconstructions compute an arithmetic/geometric average of the two universal abilities: Q(7r|s1, s2) = Q(7r|s1) + Q(ˆ7r|s2) 2 Q(7r|s1, s2) = [Q(7r|s1) * Q(ˆ7r|s2)]1 2 We respectively call these the (Asymmetric/Arithmetic/Geometric) Independent Students preference models. Notice the similarities between the universal ability estimates Q(7r|s1) and the BOJAR ranking heuristic. These three models are our attempt to render the BOJAR heuristic as preference models. 7 Item-Response Theoretic (IRT) Models Let’s revisit (Lopez, 2012)’s objection to the BOJAR ranking heuristic: “...couldn’t a system still be penalized simply by being compared to [good systems] more frequently than its competitors?” The official WMT 2012 findings (Callison-Burch et al., 2012) echoes this concern in justifying the exclusion of reference translations from the 2012 competition: [W]orkers have a very clear preference for reference translations, so including them unduly penalized systems that, through (un)luck of the draw, were pitted against the references more often. Presuming the students are paired uniformly at random, this issue diminishes </context>
<context position="27342" citStr="Lopez, 2012" startWordPosition="4552" endWordPosition="4553">ompared to a 65.8% agreement rate among 14I.e., machine translation specialists. expert judges (Callison-Burch et al., 2010). Another nice property of the IRT models is that they explicitly model student ability, so they yield a natural ranking. For training size 1600 of the WMT11 English-Czech track, Figure 6 (left) shows the mean student abilities learned by the IRT-Gaussian model. The error bars show one standard deviation of the ability means (recall that we performed 5 trials, each with a random training subset of size 1600). These results provide further insight into a case analyzed by (Lopez, 2012), which raised concern about the relative ordering of online-B, cu-bojar, and cu-marecek. According to IRT-Gaussian’s analysis of the data, these three students are so close in ability that any ordering is essentially arbitrary. Short of a full ranking, the analysis does suggest four strata. Viewing one of IRT-Gaussian’s induced preference models as a heatmap15 (Figure 6, right), four bands are discernable. First, the reference sentences are clearly the darkest (best). Next come students 2-7, followed by the slightly lighter (weaker) students 8- 10, followed by the lightest (weakest) student 1</context>
</contexts>
<marker>Lopez, 2012</marker>
<rawString>Adam Lopez. 2012. Putting human assessments of machine translation systems in order. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ducan Luce</author>
</authors>
<title>Individual Choice Behavior a Theoretical Analysis.</title>
<date>1959</date>
<publisher>John Wiley and sons.</publisher>
<contexts>
<context position="22334" citStr="Luce, 1959" startWordPosition="3736" endWordPosition="3737">eman, 1984). We found that Gibbs sampling converged quickly and consistently10 for both parameterizations. Given the parameter estimates, we obtain a preference model Q(7r|s1, s2) through the inference query: Pr(comp.c0.pref = 7r |item.i0.author = s1, item.i00.author = s2, comp.c0.opt1 = i0, comp.c0.opt2 = i00) 10We ran 200 iterations with a burn-in of 50. where c0, i0, i00 are new comparison and item ids that do not appear in the training data. We call these models Item-Response Theoretic (IRT) models, to acknowledge their roots in the psychometrics (Thurstone, 1927; Bradley and Terry, 1952; Luce, 1959) and item-response theory (Hambleton, 1991; van der Linden and Hambleton, 1996; Baker, 2001) literature. Itemresponse theory is the basis of modern testing theory and drives adaptive standardized tests like the Graduate Record Exam (GRE). In particular, the Gaussian parameterization of our IRT models strongly resembles11 the Thurstone (Thurstone, 1927) and Bradley-Terry-Luce (Bradley and Terry, 1952; Luce, 1959) models of paired comparison and the 1PL normal-ogive and Rasch (Rasch, 1960) models of student testing. From the testing perspective, we can view each comparison as two students simult</context>
</contexts>
<marker>Luce, 1959</marker>
<rawString>R. Ducan Luce. 1959. Individual Choice Behavior a Theoretical Analysis. John Wiley and sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Mislevy</author>
<author>R G Almond</author>
</authors>
<title>Graphical models and computerized adaptive testing.</title>
<date>1997</date>
<tech>UCLA CSE Technical Report 434.</tech>
<contexts>
<context position="23449" citStr="Mislevy and Almond, 1997" startWordPosition="3904" endWordPosition="3907">) models of student testing. From the testing perspective, we can view each comparison as two students simultaneously posing a test question to the other: “Give me a translation of the source text which is better than mine.” The students can answer the question correctly, incorrectly, or they can provide a translation of analogous quality. An extra dimension of our models is judge noise, not a factor when modeling multiple-choice tests, for which the right answer is not subject to opinion. 11These models are not traditionally expressed using graphical models, although it is not unprecedented (Mislevy and Almond, 1997; Mislevy et al., 1999). 1421 Figure 2: WMT10 model perplexities. The perplexity of the uniform preference model is 3.0 for all training sizes. 8 Experiments Figure 4: WMT12 model perplexities. lp wmt10 test wmt11 test wmt12 test train train train ce 3166 2209 1706 3216 5969 6806 fe 5918 2376 2556 4430 7982 5840 ge 7422 3002 3708 5371 8106 6032 se 8411 2896 1968 3684 3910 7376 ec 10490 3048 8859 9016 13770 9112 ef 5720 2242 3328 5758 7841 7508 eg 10852 2842 5964 7032 10210 7191 es 2962 2212 4768 6362 5664 8928 Table 3: Dataset sizes for each competition track (number of comparisons). Figure 3:</context>
</contexts>
<marker>Mislevy, Almond, 1997</marker>
<rawString>R.J. Mislevy and R.G. Almond. 1997. Graphical models and computerized adaptive testing. UCLA CSE Technical Report 434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Mislevy</author>
<author>R G Almond</author>
<author>D Yan</author>
<author>L S Steinberg</author>
</authors>
<title>Bayes nets in educational assessment: Where the numbers come from.</title>
<date>1999</date>
<booktitle>In Proceedings of the fifteenth conference on uncertainty in artificial intelligence,</booktitle>
<pages>437--446</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="23472" citStr="Mislevy et al., 1999" startWordPosition="3908" endWordPosition="3911">g. From the testing perspective, we can view each comparison as two students simultaneously posing a test question to the other: “Give me a translation of the source text which is better than mine.” The students can answer the question correctly, incorrectly, or they can provide a translation of analogous quality. An extra dimension of our models is judge noise, not a factor when modeling multiple-choice tests, for which the right answer is not subject to opinion. 11These models are not traditionally expressed using graphical models, although it is not unprecedented (Mislevy and Almond, 1997; Mislevy et al., 1999). 1421 Figure 2: WMT10 model perplexities. The perplexity of the uniform preference model is 3.0 for all training sizes. 8 Experiments Figure 4: WMT12 model perplexities. lp wmt10 test wmt11 test wmt12 test train train train ce 3166 2209 1706 3216 5969 6806 fe 5918 2376 2556 4430 7982 5840 ge 7422 3002 3708 5371 8106 6032 se 8411 2896 1968 3684 3910 7376 ec 10490 3048 8859 9016 13770 9112 ef 5720 2242 3328 5758 7841 7508 eg 10852 2842 5964 7032 10210 7191 es 2962 2212 4768 6362 5664 8928 Table 3: Dataset sizes for each competition track (number of comparisons). Figure 3: WMT11 model perplexiti</context>
</contexts>
<marker>Mislevy, Almond, Yan, Steinberg, 1999</marker>
<rawString>R.J. Mislevy, R.G. Almond, D. Yan, and L.S. Steinberg. 1999. Bayes nets in educational assessment: Where the numbers come from. In Proceedings of the fifteenth conference on uncertainty in artificial intelligence, pages 437–446. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Rasch</author>
</authors>
<title>Studies in mathematical psychology: I. probabilistic models for some intelligence and attainment tests.</title>
<date>1960</date>
<contexts>
<context position="22826" citStr="Rasch, 1960" startWordPosition="3807" endWordPosition="3808">tic (IRT) models, to acknowledge their roots in the psychometrics (Thurstone, 1927; Bradley and Terry, 1952; Luce, 1959) and item-response theory (Hambleton, 1991; van der Linden and Hambleton, 1996; Baker, 2001) literature. Itemresponse theory is the basis of modern testing theory and drives adaptive standardized tests like the Graduate Record Exam (GRE). In particular, the Gaussian parameterization of our IRT models strongly resembles11 the Thurstone (Thurstone, 1927) and Bradley-Terry-Luce (Bradley and Terry, 1952; Luce, 1959) models of paired comparison and the 1PL normal-ogive and Rasch (Rasch, 1960) models of student testing. From the testing perspective, we can view each comparison as two students simultaneously posing a test question to the other: “Give me a translation of the source text which is better than mine.” The students can answer the question correctly, incorrectly, or they can provide a translation of analogous quality. An extra dimension of our models is judge noise, not a factor when modeling multiple-choice tests, for which the right answer is not subject to opinion. 11These models are not traditionally expressed using graphical models, although it is not unprecedented (M</context>
</contexts>
<marker>Rasch, 1960</marker>
<rawString>G. Rasch. 1960. Studies in mathematical psychology: I. probabilistic models for some intelligence and attainment tests.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louis L Thurstone</author>
</authors>
<title>A law of comparative judgment.</title>
<date>1927</date>
<journal>Psychological review,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="22296" citStr="Thurstone, 1927" startWordPosition="3730" endWordPosition="3731">arameters with Gibbs sampling (Geman and Geman, 1984). We found that Gibbs sampling converged quickly and consistently10 for both parameterizations. Given the parameter estimates, we obtain a preference model Q(7r|s1, s2) through the inference query: Pr(comp.c0.pref = 7r |item.i0.author = s1, item.i00.author = s2, comp.c0.opt1 = i0, comp.c0.opt2 = i00) 10We ran 200 iterations with a burn-in of 50. where c0, i0, i00 are new comparison and item ids that do not appear in the training data. We call these models Item-Response Theoretic (IRT) models, to acknowledge their roots in the psychometrics (Thurstone, 1927; Bradley and Terry, 1952; Luce, 1959) and item-response theory (Hambleton, 1991; van der Linden and Hambleton, 1996; Baker, 2001) literature. Itemresponse theory is the basis of modern testing theory and drives adaptive standardized tests like the Graduate Record Exam (GRE). In particular, the Gaussian parameterization of our IRT models strongly resembles11 the Thurstone (Thurstone, 1927) and Bradley-Terry-Luce (Bradley and Terry, 1952; Luce, 1959) models of paired comparison and the 1PL normal-ogive and Rasch (Rasch, 1960) models of student testing. From the testing perspective, we can view </context>
</contexts>
<marker>Thurstone, 1927</marker>
<rawString>Louis L Thurstone. 1927. A law of comparative judgment. Psychological review, 34(4):273–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J van der Linden</author>
<author>R K Hambleton</author>
</authors>
<title>Handbook of modern item response theory.</title>
<date>1996</date>
<publisher>Springer.</publisher>
<marker>van der Linden, Hambleton, 1996</marker>
<rawString>W.J. van der Linden and R.K. Hambleton. 1996. Handbook of modern item response theory. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>