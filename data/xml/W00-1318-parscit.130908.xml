<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.986768">
Automatic WordNet mapping using word sense disambiguation.
</title>
<author confidence="0.782911">
ChangIci Lee
Geunbae Leet
</author>
<affiliation confidence="0.915267">
Natural Language Processing Lab
Dept. of Computer Science and Engineering
Pohang University of Science &amp; Technology
</affiliation>
<address confidence="0.94038">
San 31, Hyoja-Dong, Pohang, 790-784, Korea
</address>
<email confidence="0.992958">
{leeck,gblee )@postech.ac.lcr
</email>
<sectionHeader confidence="0.976134" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998887">
This paper presents the automatic
construction of a Korean WordNet from
pre-existing lexical resources. A set of
automatic WSD techniques is described for
linking Korean words collected from a
bilingual MRD to English WordNet synsets.
We will show how individual linking
provided by each WSD method is then
combined to produce a Korean WordNet for
nouns.
</bodyText>
<sectionHeader confidence="0.994443" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999416681818182">
There is no doubt on the increasing
importance of using wide coverage ontologies
for NLP tasks especially for information
retrieval and cross-language information
retrieval. While these ontologies exist in English,
there are very few available wide range
ontologies for other languages. Manual
construction of the ontology by experts is the
most reliable technique but is costly and highly
time-consuming. This is the reason for many
researchers having focused on massive
acquisition of lexical knowledge and semantic
information from pre-existing lexical resources
as automatically as possible.
This paper presents a novel approach for
automatic WordNet mapping using word sense
disambiguation. The method has been applied to
link Korean words from a bilingual dictionary to
English WordNet synsets.
To clarify the description, an example is given.
To link the first sense of Korean word
&amp;quot;gwan-mog&amp;quot; to WordNet synset, we employ a
</bodyText>
<author confidence="0.260797">
Seo JungYun
</author>
<affiliation confidence="0.885705333333333">
Natural Language Processing Lab
Dept. of Computer Science
Sogang University
</affiliation>
<address confidence="0.905548">
Sinsu-dong 1, Mapo-gu, Seoul, Korea
</address>
<email confidence="0.941582">
seojy@ccs.sogang.ac.kr
</email>
<bodyText confidence="0.9666046">
bilingual Korean-English dictionary. The first
sense of `gwan-mog&apos; has &apos;bush&apos; as a translation
in English and &apos;bush&apos; has five synsets in
WordNet. Therefore the first sense of
`gwan-mog&apos; has five candidate synsets.
Somehow we decide a synset {shrub, bush)
among five candidate synsets and link the sense
of `gwan-mog&apos; to this synset.
As seen from this example, when we link the
senses of Korean words to WordNet synsets,
there are semantic ambiguities. To remove the
ambiguities we develop new word sense
disambiguation heuristics and automatic mapping
method to construct Korean WordNet based on
the existing English WordNet.
This paper is organized as follows. In section 2,
we describe multiple heuristics for word sense
disambiguation for sense linking. In section 3, we
explain the method of combination for these
heuristics. Section 4 presents some experiment
results, and section 5 will discuss some related
researches. Finally we draw some conclusions
and future researches in section 6. The automatic
mapping-based Korean WordNet plays a natural
Korean-English bilingual thesaurus, so it will be
directly applied to Korean-English cross-lingual
information retrieval as well as Korean
monolingual information retrieval.
2 Multiple heuristics for word sense
disambiguation
As the mapping method described in this paper
has been developed for combining multiple
individual solutions, each single heuristic must be
seen as a container for some part of the linguistic
knowledge needed to disambiguate the
</bodyText>
<footnote confidence="0.9548375">
* This research was supported by KOSEF special purpose basic research (1997.9— 2000.8 #970-1020-301-3)
t Corresponding author
</footnote>
<page confidence="0.997146">
142
</page>
<bodyText confidence="0.930763571428571">
ambiguous WordNet synsets. Therefore, not a
single heuristic is suitable to all Korean words
collected from a bilingual dictionary. We will
describe each individual WSD (word sense
disambiguation) heuristic for Korean word
mapping into corresponding English senses.
Korean word English word WordNet synset
</bodyText>
<equation confidence="0.852819">
wsi
ew,
kw,4
wsk
ewm
wsn
</equation>
<figureCaption confidence="0.689858">
Figure 1: Word-to-Concept Association
Figure 1 shows the Korean word to WordNet
synset association. The j-th sense of Korean
word kw, has m translations in English and n
WordNet synsets as candidate senses. Each
heuristic is applied to the candidate senses (ws,,
</figureCaption>
<bodyText confidence="0.78465">
,ws„) and provides scores for them.
</bodyText>
<subsectionHeader confidence="0.999356">
2.1 Heuristic 1: Maximum Similarity
</subsectionHeader>
<bodyText confidence="0.999747083333333">
This heuristic comes from our previous
Korean WSD research (Lee and Lee, 2000) and
assumes that all the translations in English for
the same Korean word sense are semantically
similar. So this heuristic provides the maximum
score to the sense that is most similar to the
senses of the other translations. This heuristic is
applied when the number of translations for the
same Korean word sense is more than 1. The
following formula explains the idea.
In this formula, H,(s) is a heuristic score of
synset sp s, is a candidate synset, ew is a
translation into English, n is the number of
translations and synset(ew) is the set of synsets
of the translation ew. So Ew, becomes the set of
translations which have the synset sr The
parameter a controls the relative contribution of
candidate synsets in different number of
translations: as the value of a increases, the
candidate synsets in smaller number of
translations get relatively less weight (a=0.5 was
tuned experimentally). support(s,,ew) calculates
the maximum similarity with the synset s, and the
translation ew, which is defined as:
</bodyText>
<equation confidence="0.9927476">
support(si,ew)= max S(si,$)
sesynset(ew)
Sin(s I, s2) if sim(si,s2) ?_
451,52) =
0 otherwise
</equation>
<bodyText confidence="0.9987594">
Similarity measures lower than a threshold 0
are considered to be noise and are ignored. In our
experiments, 050.3 was used. sim(s„s2) computes
the conceptual similarity between concepts s, and
s, as in the following formula:
</bodyText>
<equation confidence="0.999014333333333">
, 2 x level(MSCA(si,s2))
SiM(SI, S2) =
level(si)+ level(s2)
</equation>
<bodyText confidence="0.9999585">
where MSCA(s,,s2) represents the most specific
common ancestor of concepts s, and s, and
level(s) refers to the depth of concept s from the
root node in the WordNet&apos;.
</bodyText>
<subsectionHeader confidence="0.997954">
2.2 Heuristic 2: Prior Probability
</subsectionHeader>
<bodyText confidence="0.9999945">
This heuristic provides prior probability to
each sense of a single translation as score.
Therefore we will give maximum score to the
synset of a monosemous translation, that is, the
translation which has only one corresponding
synset. The following formula explains the idea.
</bodyText>
<equation confidence="0.7056245">
H2(s1)= max P(si I ew)
eweEW,
</equation>
<bodyText confidence="0.809643571428571">
where EW = {ew I si e synset(ew))
P(si I ew,)
where s E synset(ew,),n, =lsynset (ew
In this formula, n, is the number of synsets of
the translation ew,.
23 Heuristic 3: Sense Ordering
(Gale et al., 1992) reports that word sense
disambiguation would be at least 75% correct if a
system assigns the most frequently occurring
sense. (Miller et al., 1994) found that automatic
We use English WordNet version 1.6
(n —1)+a
Esupport(ss,ewi) —1
where EW = {ew I si e synset(ew))
</bodyText>
<figure confidence="0.85222525">
Fli(st)= max
e.EEw,
1
• • -
</figure>
<page confidence="0.993816">
143
</page>
<bodyText confidence="0.999669333333333">
assignment of polysemous words in Brown
Corpus to senses in WordNet was 58% correct
with a heuristic of most frequently occurring
sense. We adopt these previous results to
develop sense ordering heuristic.
The sense ordering heuristic provides the
maximum score to the most frequently used
sense of a translation. The following formula
explains the heuristic.
</bodyText>
<equation confidence="0.9734695">
H3(si) .7-- max SO(si,ew)
ewe£144
</equation>
<bodyText confidence="0.946074">
where EW = few I Si E synset(ew)}
</bodyText>
<equation confidence="0.666513">
SO(sz,ew)
xfl
</equation>
<bodyText confidence="0.9855505">
where Si E synset(ew)
synset(ew) is sorted by frequency
st is the x - th synset in synset(ew)
In this formula, x refers to the sense order of s,
in synset(ew): x is 1 when s, is the most
frequently used sense of ew. The information
about the sense order of synsets of an English
word was extracted from the WordNet.
</bodyText>
<figure confidence="0.7968055">
&apos;sense_wele.ine
0.705 / ke•Q.2
9
9 —40- •
2 3 7 8 9 10
order
</figure>
<figureCaption confidence="0.998879">
Figure 2: Sense distribution in SemCor
</figureCaption>
<bodyText confidence="0.989896666666667">
The value a=0.705 and f?2.2 was acquired
from a regression of Figure 2 semcor corpus2
data distribution.
</bodyText>
<subsectionHeader confidence="0.880882">
2.4 Heuristic 4: IS-A relation
</subsectionHeader>
<bodyText confidence="0.972156692307692">
This heuristic is based on the following facts:
2 semcor is a sense tagged corpus from part of
Brown corpus.
If two Korean words have an IS-A relation,
their translations in English should also
have an IS-A relation.
Figure 3 explains IS-A relation heuristic. In
figure 3, hkw is a hypemym of a Korean word kw
and hew is a translation of hkw and ew is a
translation of kw.
This heuristic assigns score 1 to the synsets
which satisfy the above assumption according to
the following formula:
</bodyText>
<equation confidence="0.9856265">
114(si) = max IR(si,ew)
eweEW,
</equation>
<bodyText confidence="0.631723">
where EW i {ewl Si E synset(ew)}
</bodyText>
<equation confidence="0.973548666666667">
{1 if IsA(s,,si)
IR(s,,ew) =
0 otherwise
</equation>
<bodyText confidence="0.947630333333333">
where Si E synset(ew) , s, E synset(hew)
In this formula, IsA(s„s2) returns true if s, is a
kind of s2.
</bodyText>
<subsectionHeader confidence="0.812074">
2.5 Heuristic 5: Word Match
</subsectionHeader>
<bodyText confidence="0.999856166666667">
This heuristic assumes that related concepts
will be expressed using the same content words.
Given two definitions — that of the bilingual
dictionary and that of the WordNet — this
heuristic computes the total amount of shared
content words.
</bodyText>
<equation confidence="0.878785333333333">
H5(51)= max WM (Si, ew)
oveLlir,
where LW = {ew I Si E synset(ew)}
WM (si,ew)= sinz(X ,Y0
sim(X ,Y) =IX n yj
lx u yl
</equation>
<bodyText confidence="0.982919">
In this formula, X is the set of content words in
English examples of bilingual dictionary and Y, is
</bodyText>
<figure confidence="0.97678025">
WordNet
Korean word
■ hew
kw b. ew
</figure>
<figureCaption confidence="0.960953">
Figure 3: IS-A relation
</figureCaption>
<figure confidence="0.99960175">
English word
hew
0.8
Drab
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<page confidence="0.996097">
144
</page>
<bodyText confidence="0.992108">
the set of content words of definition and
example of the synset s, in WordNet.
</bodyText>
<subsectionHeader confidence="0.910564">
2.6 Heuristic 6: Cooccurrence
</subsectionHeader>
<bodyText confidence="0.999915714285714">
This heuristic uses cooccurrence measure
acquired from the sense tagged Korean
definition sentences of bilingual dictionary. To
build sense tagged corpus, we use the definition
sentences which have monosemous translation
in bilingual dictionary. And we uses the 25
semantic tags of WordNet as sense tag:
</bodyText>
<equation confidence="0.9987695">
H6(sz) = max p(t,I x)
x€Def
P)
with p = /3 — Z(f,),2 x n
13(ti I x) = Freq(ti,x)
Freq(x)
</equation>
<bodyText confidence="0.999830666666667">
In this formula, Def is the set of content words
of a Korean definition sentence, t, is a semantic
tag corresponding to the synset s, and n refers to
</bodyText>
<figure confidence="0.438178">
Freq(x).
3 Combining heuristics with decision tree
learning
</figure>
<bodyText confidence="0.966745857142857">
Given a candidate synset of a translation and
6 individual heuristic scores, our goal is to use
all these 6 scores in combination to classify the
synset as linking or discarding.
The combination of heuristics is performed by
decision tree learning for non-linear relationship.
Each internal node of a decision tree is a choice
point, dividing an individual method into ranges
of possible values. Each leaf node is labeled
with a classification Oinking or discarding). The
most popular method of decision tree induction,
employed here, is C4.5 (Quinlan, 1993).
Figure 4 shows a training phase in decision
,tree based combination method. In the training
phase, the candidate synset ws, of a Korean
word is manually classified as linking or
discarding and get assigned scores by each
heuristic. A training data set is constructed by
these scores and manual classification. The
training data set is used to optimize a model for
combining heuristics.
</bodyText>
<figureCaption confidence="0.999993">
Figure 4: &apos;framing phase
Figure 5: Mapping phase
</figureCaption>
<bodyText confidence="0.996869571428571">
Figure 5 shows a mapping phase. In the
mapping phase, the new candidate synset ws, of a
Korean word is rated using 6 heuristics, and then
the decision tree, which is learned in the training
phase, classifies ws, as linking or discarding. The
synset classified as linking is linked to the
corresponding Korean word.
</bodyText>
<sectionHeader confidence="0.982608" genericHeader="introduction">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.998772578947369">
In this section, we evaluate the performance of
each six heuristics as well as the combination
method. To evaluate the performance of WordNet
mapping, the candidate synsets of 3260 senses of
Korean words in bilingual dictionary was
manually classified as linking or discarding.
We define &apos;precision&apos; as the proportion of
correctly linked senses of Korean words to all the
linked senses of Korean words in a test set. We
also define &apos;coverage&apos; as the proportion of linked
senses of Korean words to all the senses of
Korean words in a test set.
Table 1 contains the results for each heuristic
evaluated individually against the manually
classified data. The test set here consists of the
3260 manually classified senses.
In general, the results of each heuristic seem to
be poor, but are always better than the random
choice baseline. The best heuristic according to
</bodyText>
<figure confidence="0.9881534">
Trainng
data
Training
data set
Heuristic I
Heuristic 6
Manual
Classification
Decision
tree learning
Heuristic 3
Heuristic 4
Heuristic 5
Decision Linking or
tree I Discarding
</figure>
<page confidence="0.995574">
145
</page>
<bodyText confidence="0.8262832">
the precision is the maximum similarity heuristic.
But it was applied to only 59.51% of 3260
senses of Korean words. The results of each
heuristic are better than the random mapping,
with a statistically significance at the 99% level.
</bodyText>
<table confidence="0.999545111111111">
Precision(%) Coverage(%)
Random 49.85 100.0
mapping
Heuristic 1 75.21 59.51
Heuristic 2 74.66 100.0
Heuristic 3 71.87 100.0
Heuristic 4 55.49 29.36
Heuristic 5 56.48 63.01
Heuristic 6 67.24 64.14
</table>
<tableCaption confidence="0.990122">
Table 1: Individual heuristics performance
</tableCaption>
<table confidence="0.9992304">
Precision(%) Coverage(%)
100.0
Summing 84.61
Logistic regression 86.41 100.0
Decisioin tree 93.59 77.12
</table>
<tableCaption confidence="0.9842315">
Table 2: Performance and comparison of the
decision tree based combination
</tableCaption>
<bodyText confidence="0.999843303030303">
We performed 10-fold cross validation to
evaluate the performance of the combination of
all the heuristics using the decision tree — we
split the data into ten parts, reserved one part as
a validation set, trained the decision tree on the
other nine parts and then evaluate the reserved
part. This process is repeated nine times using
each of the other nine parts as a validation set.
Table 2 shows the results of the other trials of
the combination of all the heuristics. Summing
is a way to simply sum all the scores of each
heuristic. Then the candidate synset which has
the highest summation of the scores is selected.
Logistic regression, as described in (Hosmer and
Lemeshow, 1989), is a popular technique for
binary classification. This technique applies an
inverse logit function and employs the iterative
reweighted least squares algorithm. This
technique determines the weight of each
heuristic.
With the combination of the heuristics using
summing, we obtained an improvement over
maximum similarity heuristic (heuristic 1) of 9%,
maintaining a coverage 100%. The decision tree
is able to correctly map 93.59% of the senses of
Korean words in bilingual dictionary,
maintaining a coverage 77.12%.
Applying the decision tree to combine all the
heuristics for all Korean words in bilingual
dictionary, we obtain a preliminary version of the
Korean WordNet containing 21654 senses of
17696 Korean nouns with an accuracy of 93.59%
(±0.84% with 99% confidence).
</bodyText>
<sectionHeader confidence="0.998651" genericHeader="related work">
5 Related works
</sectionHeader>
<bodyText confidence="0.950425545454545">
Several attempts have been performed to
automatically produce multilingual ontologies.
(Knight &amp; Luk 1994) focuses on the construction
of Sensus, a large knowledge base for supporting
the Pangloss Machine Translation system,
merging ontologies (ONTOS and UpperModel)
and WordNet with monolingual and bilingual
dictionaries. (Okumura &amp; Hovy 1994) describes a
semi-automatic method for associating a Japanese
lexicon to an ontology using a Japanese/English
bilingual dictionary as a &apos;bridge&apos;. Several lexical
resources and techniques are combined in
(Atserias et al., 1997) to map Spanish words from
a bilingual dictionary to WordNet. In (Farreres et
al., 1998), use of a taxonomic structure derived
from a monolingual MRD is proposed as an aid
to the mapping process.
This research is contrasted that it utilized
bilingual dictionary to build monolingual
thesaurus based on the existing popular lexical
resources and used the combination of multiple
unsupervided WSD heuristics.
</bodyText>
<sectionHeader confidence="0.997546" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999098416666667">
This paper has explored the automatic
construction of a Korean WordNet from
pre-existing lexical resources — English wordNet
and Korean/English bilingual dictionary. We
presented several techniques for word sense
disambiguation and their application to
disambiguate the translations in bilingual
dictionary. We obtained a preliminary version of
the Korean WordNet containing 21654 senses of
17696 Korean nouns. In a series of experiments,
we observed that the accuracy of mapping is over
90%.
</bodyText>
<sectionHeader confidence="0.995768" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981026666666667">
Atserias J., Climent S., Farreras J., Rigau G. and
Rodriguez H. (1997) Combining Multiple
Methods for the Automatic Construction of
</reference>
<page confidence="0.987816">
146
</page>
<reference confidence="0.998856277777778">
Multilingual WordNets. In proceeding of the
Conference on Recent Advances on NLP.
Farreres X., Rigau G., and Rodriguez H.. (1998)
Using WordNet for building WordNets. In
Proceedings of COLING-ACL Workshop on Usage
of WordNet in Natural Language Processing
Systems.
Gale W., Church K., and Yarowsky D. (1992)
Estimating upper and lower bounds on the
performance of word-sense disambiguation
programs. In Proceeding of 30th Annual Meeting
of the Association for Computational Linguistics.
Hosmer Jr. and Lemeshow S. (1989) Applied
Logistic Regression. Wiley, New York.
Knight K. and Luk S. (1994) Building a large-scale
knowledge base for machine translation. In
Proceeding of the American Association for
Artificial Intelligence.
Miller G. (1990) Five papers on WordNet.
Special Issue of International Journal of
Lexicography.
Miller G., Chodorow M., Landes S., Leacock C. and
Thomas R.. (1994) Using a semantic
concordance for sense identification. In
Proceedings of the Human Language Technology
Workshop.
Okumura A. and Hovy E. (1994) Building
Japanese-English Dictionary based on Ontology for
Machine Translation. In Proceedings of ARPA
Workshop on Human Language Technology.
Quinlan R.. (1993) C4.5: Programs For Machine
Learning. Morgan Kaufmann Publishers.
Seungwoo Lee and Geunbae Lee. (2000)
Unsupervised Noun Sense Disambiguation Using
Local Context and Co-occurrence. In Journal of
Korean Information Science Society. (in press)
</reference>
<page confidence="0.998084">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.687729">
<title confidence="0.92898825">Automatic WordNet mapping using word sense disambiguation. ChangIci Geunbae Natural Language Processing</title>
<affiliation confidence="0.99914">Dept. of Computer Science and Pohang University of Science &amp; Technology</affiliation>
<address confidence="0.998799">San 31, Hyoja-Dong, Pohang, 790-784,</address>
<email confidence="0.916227">{leeck,gblee)@postech.ac.lcr</email>
<abstract confidence="0.981548545454546">This paper presents the automatic construction of a Korean WordNet from pre-existing lexical resources. A set of automatic WSD techniques is described for linking Korean words collected from a bilingual MRD to English WordNet synsets. We will show how individual linking provided by each WSD method is then combined to produce a Korean WordNet for nouns.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Atserias</author>
<author>S Climent</author>
<author>J Farreras</author>
<author>G Rigau</author>
<author>H Rodriguez</author>
</authors>
<title>Combining Multiple Methods for the Automatic Construction of Multilingual WordNets.</title>
<date>1997</date>
<booktitle>In proceeding of the Conference on Recent Advances on NLP.</booktitle>
<contexts>
<context position="14605" citStr="Atserias et al., 1997" startWordPosition="2349" endWordPosition="2352">.59% (±0.84% with 99% confidence). 5 Related works Several attempts have been performed to automatically produce multilingual ontologies. (Knight &amp; Luk 1994) focuses on the construction of Sensus, a large knowledge base for supporting the Pangloss Machine Translation system, merging ontologies (ONTOS and UpperModel) and WordNet with monolingual and bilingual dictionaries. (Okumura &amp; Hovy 1994) describes a semi-automatic method for associating a Japanese lexicon to an ontology using a Japanese/English bilingual dictionary as a &apos;bridge&apos;. Several lexical resources and techniques are combined in (Atserias et al., 1997) to map Spanish words from a bilingual dictionary to WordNet. In (Farreres et al., 1998), use of a taxonomic structure derived from a monolingual MRD is proposed as an aid to the mapping process. This research is contrasted that it utilized bilingual dictionary to build monolingual thesaurus based on the existing popular lexical resources and used the combination of multiple unsupervided WSD heuristics. 6 Conclusion This paper has explored the automatic construction of a Korean WordNet from pre-existing lexical resources — English wordNet and Korean/English bilingual dictionary. We presented s</context>
</contexts>
<marker>Atserias, Climent, Farreras, Rigau, Rodriguez, 1997</marker>
<rawString>Atserias J., Climent S., Farreras J., Rigau G. and Rodriguez H. (1997) Combining Multiple Methods for the Automatic Construction of Multilingual WordNets. In proceeding of the Conference on Recent Advances on NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Farreres</author>
<author>G Rigau</author>
<author>H Rodriguez</author>
</authors>
<title>Using WordNet for building WordNets.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL Workshop on Usage of WordNet in Natural Language Processing Systems.</booktitle>
<contexts>
<context position="14693" citStr="Farreres et al., 1998" startWordPosition="2364" endWordPosition="2367">to automatically produce multilingual ontologies. (Knight &amp; Luk 1994) focuses on the construction of Sensus, a large knowledge base for supporting the Pangloss Machine Translation system, merging ontologies (ONTOS and UpperModel) and WordNet with monolingual and bilingual dictionaries. (Okumura &amp; Hovy 1994) describes a semi-automatic method for associating a Japanese lexicon to an ontology using a Japanese/English bilingual dictionary as a &apos;bridge&apos;. Several lexical resources and techniques are combined in (Atserias et al., 1997) to map Spanish words from a bilingual dictionary to WordNet. In (Farreres et al., 1998), use of a taxonomic structure derived from a monolingual MRD is proposed as an aid to the mapping process. This research is contrasted that it utilized bilingual dictionary to build monolingual thesaurus based on the existing popular lexical resources and used the combination of multiple unsupervided WSD heuristics. 6 Conclusion This paper has explored the automatic construction of a Korean WordNet from pre-existing lexical resources — English wordNet and Korean/English bilingual dictionary. We presented several techniques for word sense disambiguation and their application to disambiguate th</context>
</contexts>
<marker>Farreres, Rigau, Rodriguez, 1998</marker>
<rawString>Farreres X., Rigau G., and Rodriguez H.. (1998) Using WordNet for building WordNets. In Proceedings of COLING-ACL Workshop on Usage of WordNet in Natural Language Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>Estimating upper and lower bounds on the performance of word-sense disambiguation programs.</title>
<date>1992</date>
<booktitle>In Proceeding of 30th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6219" citStr="Gale et al., 1992" startWordPosition="955" endWordPosition="958">(s) refers to the depth of concept s from the root node in the WordNet&apos;. 2.2 Heuristic 2: Prior Probability This heuristic provides prior probability to each sense of a single translation as score. Therefore we will give maximum score to the synset of a monosemous translation, that is, the translation which has only one corresponding synset. The following formula explains the idea. H2(s1)= max P(si I ew) eweEW, where EW = {ew I si e synset(ew)) P(si I ew,) where s E synset(ew,),n, =lsynset (ew In this formula, n, is the number of synsets of the translation ew,. 23 Heuristic 3: Sense Ordering (Gale et al., 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. (Miller et al., 1994) found that automatic We use English WordNet version 1.6 (n —1)+a Esupport(ss,ewi) —1 where EW = {ew I si e synset(ew)) Fli(st)= max e.EEw, 1 • • - 143 assignment of polysemous words in Brown Corpus to senses in WordNet was 58% correct with a heuristic of most frequently occurring sense. We adopt these previous results to develop sense ordering heuristic. The sense ordering heuristic provides the maximum score to the most frequently used sense of a</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale W., Church K., and Yarowsky D. (1992) Estimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proceeding of 30th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lemeshow</author>
</authors>
<title>Applied Logistic Regression.</title>
<date>1989</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="13238" citStr="Lemeshow, 1989" startWordPosition="2152" endWordPosition="2153"> combination of all the heuristics using the decision tree — we split the data into ten parts, reserved one part as a validation set, trained the decision tree on the other nine parts and then evaluate the reserved part. This process is repeated nine times using each of the other nine parts as a validation set. Table 2 shows the results of the other trials of the combination of all the heuristics. Summing is a way to simply sum all the scores of each heuristic. Then the candidate synset which has the highest summation of the scores is selected. Logistic regression, as described in (Hosmer and Lemeshow, 1989), is a popular technique for binary classification. This technique applies an inverse logit function and employs the iterative reweighted least squares algorithm. This technique determines the weight of each heuristic. With the combination of the heuristics using summing, we obtained an improvement over maximum similarity heuristic (heuristic 1) of 9%, maintaining a coverage 100%. The decision tree is able to correctly map 93.59% of the senses of Korean words in bilingual dictionary, maintaining a coverage 77.12%. Applying the decision tree to combine all the heuristics for all Korean words in</context>
</contexts>
<marker>Lemeshow, 1989</marker>
<rawString>Hosmer Jr. and Lemeshow S. (1989) Applied Logistic Regression. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>S Luk</author>
</authors>
<title>Building a large-scale knowledge base for machine translation.</title>
<date>1994</date>
<booktitle>In Proceeding of the American Association for Artificial Intelligence.</booktitle>
<contexts>
<context position="14140" citStr="Knight &amp; Luk 1994" startWordPosition="2284" endWordPosition="2287"> an improvement over maximum similarity heuristic (heuristic 1) of 9%, maintaining a coverage 100%. The decision tree is able to correctly map 93.59% of the senses of Korean words in bilingual dictionary, maintaining a coverage 77.12%. Applying the decision tree to combine all the heuristics for all Korean words in bilingual dictionary, we obtain a preliminary version of the Korean WordNet containing 21654 senses of 17696 Korean nouns with an accuracy of 93.59% (±0.84% with 99% confidence). 5 Related works Several attempts have been performed to automatically produce multilingual ontologies. (Knight &amp; Luk 1994) focuses on the construction of Sensus, a large knowledge base for supporting the Pangloss Machine Translation system, merging ontologies (ONTOS and UpperModel) and WordNet with monolingual and bilingual dictionaries. (Okumura &amp; Hovy 1994) describes a semi-automatic method for associating a Japanese lexicon to an ontology using a Japanese/English bilingual dictionary as a &apos;bridge&apos;. Several lexical resources and techniques are combined in (Atserias et al., 1997) to map Spanish words from a bilingual dictionary to WordNet. In (Farreres et al., 1998), use of a taxonomic structure derived from a m</context>
</contexts>
<marker>Knight, Luk, 1994</marker>
<rawString>Knight K. and Luk S. (1994) Building a large-scale knowledge base for machine translation. In Proceeding of the American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Five papers on WordNet.</title>
<date>1990</date>
<journal>Special Issue of International Journal of Lexicography.</journal>
<marker>Miller, 1990</marker>
<rawString>Miller G. (1990) Five papers on WordNet. Special Issue of International Journal of Lexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>M Chodorow</author>
<author>S Landes</author>
<author>C Leacock</author>
<author>R Thomas</author>
</authors>
<title>Using a semantic concordance for sense identification.</title>
<date>1994</date>
<booktitle>In Proceedings of the Human Language Technology Workshop.</booktitle>
<contexts>
<context position="6367" citStr="Miller et al., 1994" startWordPosition="979" endWordPosition="982">ity to each sense of a single translation as score. Therefore we will give maximum score to the synset of a monosemous translation, that is, the translation which has only one corresponding synset. The following formula explains the idea. H2(s1)= max P(si I ew) eweEW, where EW = {ew I si e synset(ew)) P(si I ew,) where s E synset(ew,),n, =lsynset (ew In this formula, n, is the number of synsets of the translation ew,. 23 Heuristic 3: Sense Ordering (Gale et al., 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. (Miller et al., 1994) found that automatic We use English WordNet version 1.6 (n —1)+a Esupport(ss,ewi) —1 where EW = {ew I si e synset(ew)) Fli(st)= max e.EEw, 1 • • - 143 assignment of polysemous words in Brown Corpus to senses in WordNet was 58% correct with a heuristic of most frequently occurring sense. We adopt these previous results to develop sense ordering heuristic. The sense ordering heuristic provides the maximum score to the most frequently used sense of a translation. The following formula explains the heuristic. H3(si) .7-- max SO(si,ew) ewe£144 where EW = few I Si E synset(ew)} SO(sz,ew) xfl where </context>
</contexts>
<marker>Miller, Chodorow, Landes, Leacock, Thomas, 1994</marker>
<rawString>Miller G., Chodorow M., Landes S., Leacock C. and Thomas R.. (1994) Using a semantic concordance for sense identification. In Proceedings of the Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Okumura</author>
<author>E Hovy</author>
</authors>
<title>Building Japanese-English Dictionary based on Ontology for Machine Translation.</title>
<date>1994</date>
<booktitle>In Proceedings of ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="14379" citStr="Okumura &amp; Hovy 1994" startWordPosition="2317" endWordPosition="2320">pplying the decision tree to combine all the heuristics for all Korean words in bilingual dictionary, we obtain a preliminary version of the Korean WordNet containing 21654 senses of 17696 Korean nouns with an accuracy of 93.59% (±0.84% with 99% confidence). 5 Related works Several attempts have been performed to automatically produce multilingual ontologies. (Knight &amp; Luk 1994) focuses on the construction of Sensus, a large knowledge base for supporting the Pangloss Machine Translation system, merging ontologies (ONTOS and UpperModel) and WordNet with monolingual and bilingual dictionaries. (Okumura &amp; Hovy 1994) describes a semi-automatic method for associating a Japanese lexicon to an ontology using a Japanese/English bilingual dictionary as a &apos;bridge&apos;. Several lexical resources and techniques are combined in (Atserias et al., 1997) to map Spanish words from a bilingual dictionary to WordNet. In (Farreres et al., 1998), use of a taxonomic structure derived from a monolingual MRD is proposed as an aid to the mapping process. This research is contrasted that it utilized bilingual dictionary to build monolingual thesaurus based on the existing popular lexical resources and used the combination of multi</context>
</contexts>
<marker>Okumura, Hovy, 1994</marker>
<rawString>Okumura A. and Hovy E. (1994) Building Japanese-English Dictionary based on Ontology for Machine Translation. In Proceedings of ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C4.5: Programs For Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="10090" citStr="Quinlan, 1993" startWordPosition="1638" endWordPosition="1639">(x). 3 Combining heuristics with decision tree learning Given a candidate synset of a translation and 6 individual heuristic scores, our goal is to use all these 6 scores in combination to classify the synset as linking or discarding. The combination of heuristics is performed by decision tree learning for non-linear relationship. Each internal node of a decision tree is a choice point, dividing an individual method into ranges of possible values. Each leaf node is labeled with a classification Oinking or discarding). The most popular method of decision tree induction, employed here, is C4.5 (Quinlan, 1993). Figure 4 shows a training phase in decision ,tree based combination method. In the training phase, the candidate synset ws, of a Korean word is manually classified as linking or discarding and get assigned scores by each heuristic. A training data set is constructed by these scores and manual classification. The training data set is used to optimize a model for combining heuristics. Figure 4: &apos;framing phase Figure 5: Mapping phase Figure 5 shows a mapping phase. In the mapping phase, the new candidate synset ws, of a Korean word is rated using 6 heuristics, and then the decision tree, which </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan R.. (1993) C4.5: Programs For Machine Learning. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seungwoo Lee</author>
<author>Geunbae Lee</author>
</authors>
<title>Unsupervised Noun Sense Disambiguation Using Local Context and Co-occurrence.</title>
<date>2000</date>
<journal>In Journal of Korean Information Science Society.</journal>
<note>(in press)</note>
<contexts>
<context position="4114" citStr="Lee and Lee, 2000" startWordPosition="606" endWordPosition="609">nary. We will describe each individual WSD (word sense disambiguation) heuristic for Korean word mapping into corresponding English senses. Korean word English word WordNet synset wsi ew, kw,4 wsk ewm wsn Figure 1: Word-to-Concept Association Figure 1 shows the Korean word to WordNet synset association. The j-th sense of Korean word kw, has m translations in English and n WordNet synsets as candidate senses. Each heuristic is applied to the candidate senses (ws,, ,ws„) and provides scores for them. 2.1 Heuristic 1: Maximum Similarity This heuristic comes from our previous Korean WSD research (Lee and Lee, 2000) and assumes that all the translations in English for the same Korean word sense are semantically similar. So this heuristic provides the maximum score to the sense that is most similar to the senses of the other translations. This heuristic is applied when the number of translations for the same Korean word sense is more than 1. The following formula explains the idea. In this formula, H,(s) is a heuristic score of synset sp s, is a candidate synset, ew is a translation into English, n is the number of translations and synset(ew) is the set of synsets of the translation ew. So Ew, becomes the</context>
</contexts>
<marker>Lee, Lee, 2000</marker>
<rawString>Seungwoo Lee and Geunbae Lee. (2000) Unsupervised Noun Sense Disambiguation Using Local Context and Co-occurrence. In Journal of Korean Information Science Society. (in press)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>