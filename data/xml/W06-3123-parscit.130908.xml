<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.100632">
<title confidence="0.9532305">
Constraining the Phrase-Based, Joint Probability Statistical Translation
Model
</title>
<author confidence="0.991205">
Alexandra Birch Chris Callison-Burch Miles Osborne Philipp Koehn
</author>
<affiliation confidence="0.998197">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.849969">
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
</address>
<email confidence="0.999027">
a.c.birch-mayne@sms.ed.ac.uk
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999726875">
The joint probability model proposed by
Marcu and Wong (2002) provides a strong
probabilistic framework for phrase-based
statistical machine translation (SMT). The
model’s usefulness is, however, limited by
the computational complexity of estimat-
ing parameters at the phrase level. We
present the first model to use word align-
ments for constraining the space of phrasal
alignments searched during Expectation
Maximization (EM) training. Constrain-
ing the joint model improves performance,
showing results that are very close to state-
of-the-art phrase-based models. It also al-
lows it to scale up to larger corpora and
therefore be more widely applicable.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974730769231">
Machine translation is a hard problem because of
the highly complex, irregular and diverse nature
of natural languages. It is impossible to accurately
model all the linguistic rules that shape the trans-
lation process, and therefore a principled approach
uses statistical methods to make optimal decisions
given incomplete data.
The original IBM Models (Brown et al., 1993)
learn word-to-word alignment probabilities which
makes it computationally feasible to estimate
model parameters from large amounts of train-
ing data. Phrase-based SMT models, such as the
alignment template model (Och, 2003), improve
on word-based models because phrases provide
local context which leads to better lexical choice
and more reliable local reordering. However, most
phrase-based models extract their phrase pairs
from previously word-aligned corpora using ad-
hoc heuristics. These models perform no search
for optimal phrasal alignments. Even though this
is an efficient strategy, it is a departure from the
rigorous statistical framework of the IBM Models.
Marcu and Wong (2002) proposed the joint
probability model which directly estimates the
phrase translation probabilities from the corpus in
a theoretically governed way. This model neither
relies on potentially sub-optimal word alignments
nor on heuristics for phrase extraction. Instead, it
searches the phrasal alignment space, simultane-
ously learning translation lexicons for both words
and phrases. The joint model has been shown to
outperform standard models on restricted data sets
such as the small data track for Chinese-English in
the 2004 NIST MT Evaluation (Przybocki, 2004).
However, considering all possible phrases and
all their possible alignments vastly increases the
computational complexity of the joint model when
compared to its word-based counterpart. In this
paper, we propose a method of constraining the
search space of the joint model to areas where
most of the unpromising phrasal alignments are
eliminated and yet as many potentially useful
alignments as possible are still explored. The
joint model is constrained to phrasal alignments
which do not contradict a set high confidence word
alignments for each sentence. These high con-
fidence alignments could incorporate information
from both statistical and linguistic sources. In this
paper we use the points of high confidence from
the intersection of the bi-directional Viterbi word
alignments to constrain the model, increasing per-
formance and decreasing complexity.
</bodyText>
<page confidence="0.995923">
154
</page>
<subsectionHeader confidence="0.5088985">
Proceedings of the Workshop on Statistical Machine Translation, pages 154–157,
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.98525838961039">
2 Translation Models els, it does not constrain the alignments to being
2.1 Standard Phrase-based Model single words.
Most phrase-based translation models (Och, 2003; The joint model creates phrases from words and
Koehn et al., 2003; Vogel et al., 2003) rely on commonly occurring sequences of words. A con-
a pre-existing set of word-based alignments from cept, c2, is defined as a pair of aligned phrases
which they induce their parameters. In this project &lt; e2, f2 &gt;. A set of concepts which completely
we use the model described by Koehn et al. (2003) covers the sentence pair is denoted by C. Phrases
which extracts its phrase alignments from a corpus are restricted to being sequences of words which
that has been word aligned. From now on we re- occur above a certain frequency in the corpus.
fer to this phrase-based translation model as the Commonly occurring phrases are more likely to
standard model. The standard model decomposes lead to the creation of useful phrase pairs, and
the foreign input sentence F into a sequence of without this restriction the search space would be
I phrases f1, ... , fI. Each foreign phrase f2 is much larger.
translated to an English phrase e2 using the prob- The probability of a sentence and its translation
ability distribution 0(f2|e2). English phrases may is the sum of all possible alignments C, each of
be reordered using a relative distortion probability. which is defined as the product of the probability
This model performs no search for optimal of all individual concepts:
phrase pairs. Instead, it extracts phrase pairs p(F, E) _ � ri p(&lt; e2, f2 &gt;) (1)
(f2, e2) in the following manner. First, it uses the CEC &lt;ei,fi&gt;EC
IBM Models to learn the most likely word-level The model is trained by initializing the trans-
Viterbi alignments for English to Foreign and For- lation table using Stirling numbers of the second
eign to English. It then uses a heuristic to recon- kind to efficiently estimate p(&lt; e2, f2 &gt;) by cal-
cile the two alignments, starting from the points culating the proportion of alignments which con-
of high confidence in the intersection of the two tain p(&lt; e2, f2 &gt;) compared to the total number
Viterbi alignments and growing towards the points of alignments in the sentence (Marcu and Wong,
in the union. Points from the union are selected if 2002). EM is then performed by first discovering
they are adjacent to points from the intersection an initial phrasal alignments using a greedy algo-
and their words are previously unaligned. rithm similar to the competitive linking algorithm
Phrases are then extracted by selecting phrase (Melamed, 1997). The highest probability phrase
pairs which are ‘consistent’ with the symmetrized pairs are iteratively selected until all phrases are
alignment, which means that all words within the are linked. Then hill-climbing is performed by
source language phrase are only aligned to the searching once for each iteration for all merges,
words of the target language phrase and vice versa. splits, moves and swaps that improve the proba-
Finally the phrase translation probability distribu- bility of the initial phrasal alignment. Fractional
tion is estimated using the relative frequencies of counts are collected for all alignments visited.
the extracted phrase pairs. Training the IBM models is computationally
This approach to phrase extraction means that challenging, but the joint model is much more de-
phrasal alignments are locked into the sym- manding. Considering all possible segmentations
metrized alignment. This is problematic because of phrases and all their possible alignments vastly
the symmetrization process will grow an align- increases the number of possible alignments that
ment based on arbitrary decisions about adjacent can be formed between two sentences. This num-
words and because word alignments inadequately ber is exponential with relation to the length of the
represent the real dependencies between transla- shorter sentence.
tions. 3 Constraining the Joint Model
2.2 Joint Probability Model The joint model requires a strategy for restricting
The joint model (Marcu and Wong, 2002), does the search for phrasal alignments to areas of the
not rely on a pre-existing set of word-level align- alignment space which contain most of the proba-
ments. Like the IBM Models, it uses EM to align bility mass. We propose a method which examines
and estimate the probabilities for sub-sentential
units in a parallel corpus. Unlike the IBM Mod-
155
phrase pairs that are consistent with a set of high
confidence word alignments defined for the sen-
tence. The set of alignments are taken from the in-
tersection of the bi-directional Viterbi alignments.
This strategy for extracting phrase pairs is simi-
lar to that of the standard phrase-based model and
the definition of ‘consistent’ is the same. How-
ever, the constrained joint model does not lock
the search into a heuristically derived symmetrized
alignment. Joint model phrases must also occur
above a certain frequency in the corpus to be con-
sidered.
The constraints on the model are binding during
the initialization phase of training. During EM,
inconsistent phrase pairs are given a small, non-
zero probability and are thus not considered un-
less unaligned words remain after linking together
high probability phrase pairs. All words must be
aligned, there is no NULL alignment like in the
IBM models.
By using the IBM Models to constrain the joint
model, we are searching areas in the phrasal align-
ment space where both models overlap. We com-
bine the advantage of prior knowledge about likely
word alignments with the ability to perform a
probabilistic search around them.
</bodyText>
<sectionHeader confidence="0.998806" genericHeader="introduction">
4 Experiments
</sectionHeader>
<bodyText confidence="0.795588333333333">
All data and software used was from the NAACL
2006 Statistical Machine Translation workshop
unless otherwise indicated.
</bodyText>
<subsectionHeader confidence="0.964034">
4.1 Constraints
</subsectionHeader>
<bodyText confidence="0.99996475">
The unconstrained joint model becomes in-
tractable with very small amounts of training data.
On a machine with 2 Gb of memory, we were
only able to train 10,000 sentences of the German-
English Europarl corpora. Beyond this, pruning is
required to keep the model in memory during EM.
Table 1 shows that the application of the word con-
straints considerably reduces the size of the space
of phrasal alignments that is searched. It also im-
proves the BLEU score of the model, by guiding it
to explore the more promising areas of the search
space.
</bodyText>
<subsectionHeader confidence="0.976388">
4.2 Scalability
</subsectionHeader>
<bodyText confidence="0.9995225">
Even though the constrained joint model reduces
complexity, pruning is still needed in order to scale
up to larger corpora. After the initialization phase
of the training, all phrase pairs with counts less
</bodyText>
<table confidence="0.98349">
Unconstrained Constrained
No. Concepts 6,178k 1,457k
BLEU 19.93 22.13
Time(min) 299 169
</table>
<tableCaption confidence="0.985547">
Table 1. The impact of constraining the joint model
trained on 10,000 sentences of the German-English
Europarl corpora and tested with the Europarl test set
used in Koehn et al. (2003)
</tableCaption>
<bodyText confidence="0.999302">
than 10 million times that of the phrase pair with
the highest count, are pruned from the phrase ta-
ble. The model is also parallelized in order to
speed up training.
The translation models are included within a
log-linear model (Och and Ney, 2002) which al-
lows a weighted combination of features func-
tions. For the comparison of the basic systems
in Table 2 only three features were used for both
the joint and the standard model: p(e|f), p(f|e)
and the language model, and they were given equal
weights.
The results in Table 2 show that the joint model
is capable of training on large data sets, with a
reasonable performance compared to the standard
model. However, here it seems that the standard
model has a slight advantage. This is almost cer-
tainly related to the fact that the joint model results
in a much smaller phrase table. Pruning eliminates
many phrase pairs, but further investigations indi-
cate that this has little impact on BLEU scores.
</bodyText>
<table confidence="0.998365666666667">
BLEU Size
Joint Model 25.49 2.28
Standard Model 26.15 19.04
</table>
<tableCaption confidence="0.94285">
Table 2. Basic system comparisons: BLEU scores
and model size in millions of phrase pairs for Spanish-
English
</tableCaption>
<bodyText confidence="0.999495071428571">
The results in Table 3 compare the joint and the
standard model with more features. Apart from
including all Pharaoh’s default features, we use
two new features for both the standard and joint
models: a 5-gram language model and a lexical-
ized reordering model as described in Koehn et al.
(2005). The weights of the feature functions, or
model components, are set by minimum error rate
training provided by David Chiang from the Uni-
versity of Maryland.
On smaller data sets (Koehn et al., 2003) the
joint model shows performance comparable to the
standard model, however the joint model does
not reach the level of performance of the stan-
</bodyText>
<page confidence="0.995625">
156
</page>
<table confidence="0.999231375">
EN-ES ES-EN
Joint
3-gram, dl4 20.51 26.64
5-gram, dl6 26.34 27.17
+ lex. reordering 26.82 27.80
Standard Model
5-gram, dl6 31.18 31.86
+ lex. reordering
</table>
<tableCaption confidence="0.8528824">
Table 3. Bleu scores for the joint model and the stan-
dard model showing the effect of the 5-gram language
model, distortion length of 6 (dl) and the addition of
lexical reordering for the English-Spanish and Spanish-
English tasks.
</tableCaption>
<bodyText confidence="0.9998455">
dard model for this larger data set. This could
be due to the fact that the joint model results in
a much smaller phrase table. During EM only
phrase pairs that occur in an alignment visited dur-
ing hill-climbing are retained. Only a very small
proportion of the alignment space can be searched
and this reduces the chances of finding optimum
parameters. The small number of alignments vis-
ited would lead to data sparseness and over-fitting.
Another factor could be efficiency trade-offs like
the fast but not optimal competitive linking search
for phrasal alignments.
</bodyText>
<subsectionHeader confidence="0.986255">
4.3 German-English submission
</subsectionHeader>
<bodyText confidence="0.999988285714286">
We also submitted a German-English system using
the standard approach to phrase extraction. The
purpose of this submission was to validate the syn-
tactic reordering method that we previously pro-
posed (Collins et al., 2005). We parse the Ger-
man training and test corpus and reorder it accord-
ing to a set of manually devised rules. Then, we
use our phrase-based system with standard phrase-
extraction, lexicalized reordering, lexical scoring,
5-gram LM, and the Pharaoh decoder.
On the development test set, the syntactic re-
ordering improved performance from 26.86 to
27.70. The best submission in last year’s shared
task achieved a score of 24.77 on this set.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999820111111111">
We presented the first attempt at creating a system-
atic framework which uses word alignment con-
straints to guide phrase-based EM training. This
shows competitive results, to within 0.66 BLEU
points for the basic systems, suggesting that a
rigorous probabilistic framework is preferable to
heuristics for extracting phrase pairs and their
probabilities.
By introducing constraints to the alignment
space we can reduce the complexity of the joint
model and increase its performance, allowing it to
train on larger corpora and making the model more
widely applicable.
For the future, the joint model would benefit
from lexical weighting like that used in the stan-
dard model (Koehn et al., 2003). Using IBM
Model 1 to extract a lexical alignment weight for
each phrase pair would decrease the impact of data
sparseness, and other kinds smoothing techniques
will be investigated. Better search algorithms for
Viterbi phrasal alignments during EM would in-
crease the number and quality of model parame-
ters.
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99964784375">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005.
Clause restructuring for statistical machine translation. In
Proceedings of ACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL, pages 127–133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne,
and Chris Callison-Burch. 2005. Edinburgh system de-
scription. In IWSLT Speech Translation Evaluation.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine translation.
In Proceedings of EMNLP.
Dan Melamed. 1997. A word-to-word model of translational
equivalence. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In ACL.
Franz Josef Och. 2003. Statistical Machine Translation:
From Single-Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen Department of Computer Science,
Aachen, Germany.
Mark Przybocki. 2004. NIST 2004 machine translation eval-
uation results. Confidential e-mail to workshop partici-
pants, May.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,
Ashish Venugopal, Bing Zhao, and Alex Waibel. 2003.
The CMU statistical machine translation system. In Ma-
chine Translation Summit.
</reference>
<page confidence="0.997736">
157
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.794933">
<title confidence="0.9828795">Constraining the Phrase-Based, Joint Probability Statistical Translation Model</title>
<author confidence="0.999862">Alexandra Birch Chris Callison-Burch Miles Osborne Philipp</author>
<affiliation confidence="0.988438">School of University of 2 Buccleuch</affiliation>
<address confidence="0.89652">Edinburgh, EH8 9LW,</address>
<email confidence="0.997871">a.c.birch-mayne@sms.ed.ac.uk</email>
<abstract confidence="0.996908705882353">The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). The model’s usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to stateof-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1309" citStr="Brown et al., 1993" startWordPosition="183" endWordPosition="186"> (EM) training. Constraining the joint model improves performance, showing results that are very close to stateof-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable. 1 Introduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the alignment template model (Och, 2003), improve on word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering. However, most phrase-based models extract their phrase pairs from previously word-aligned corpora using adhoc heuristics. These models perform no search for optimal phrasal alignments. Even though this is an efficient strategy, it is a</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13386" citStr="Collins et al., 2005" startWordPosition="2132" endWordPosition="2135">hill-climbing are retained. Only a very small proportion of the alignment space can be searched and this reduces the chances of finding optimum parameters. The small number of alignments visited would lead to data sparseness and over-fitting. Another factor could be efficiency trade-offs like the fast but not optimal competitive linking search for phrasal alignments. 4.3 German-English submission We also submitted a German-English system using the standard approach to phrase extraction. The purpose of this submission was to validate the syntactic reordering method that we previously proposed (Collins et al., 2005). We parse the German training and test corpus and reorder it according to a set of manually devised rules. Then, we use our phrase-based system with standard phraseextraction, lexicalized reordering, lexical scoring, 5-gram LM, and the Pharaoh decoder. On the development test set, the syntactic reordering improved performance from 26.86 to 27.70. The best submission in last year’s shared task achieved a score of 24.77 on this set. 5 Conclusion We presented the first attempt at creating a systematic framework which uses word alignment constraints to guide phrase-based EM training. This shows c</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="3816" citStr="Koehn et al., 2003" startWordPosition="550" endWordPosition="553">tatistical and linguistic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. 154 Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, New York City, June 2006. c�2006 Association for Computational Linguistics 2 Translation Models els, it does not constrain the alignments to being 2.1 Standard Phrase-based Model single words. Most phrase-based translation models (Och, 2003; The joint model creates phrases from words and Koehn et al., 2003; Vogel et al., 2003) rely on commonly occurring sequences of words. A cona pre-existing set of word-based alignments from cept, c2, is defined as a pair of aligned phrases which they induce their parameters. In this project &lt; e2, f2 &gt;. A set of concepts which completely we use the model described by Koehn et al. (2003) covers the sentence pair is denoted by C. Phrases which extracts its phrase alignments from a corpus are restricted to being sequences of words which that has been word aligned. From now on we re- occur above a certain frequency in the corpus. fer to this phrase-based translati</context>
<context position="10418" citStr="Koehn et al. (2003)" startWordPosition="1632" endWordPosition="1635">arched. It also improves the BLEU score of the model, by guiding it to explore the more promising areas of the search space. 4.2 Scalability Even though the constrained joint model reduces complexity, pruning is still needed in order to scale up to larger corpora. After the initialization phase of the training, all phrase pairs with counts less Unconstrained Constrained No. Concepts 6,178k 1,457k BLEU 19.93 22.13 Time(min) 299 169 Table 1. The impact of constraining the joint model trained on 10,000 sentences of the German-English Europarl corpora and tested with the Europarl test set used in Koehn et al. (2003) than 10 million times that of the phrase pair with the highest count, are pruned from the phrase table. The model is also parallelized in order to speed up training. The translation models are included within a log-linear model (Och and Ney, 2002) which allows a weighted combination of features functions. For the comparison of the basic systems in Table 2 only three features were used for both the joint and the standard model: p(e|f), p(f|e) and the language model, and they were given equal weights. The results in Table 2 show that the joint model is capable of training on large data sets, wi</context>
<context position="12036" citStr="Koehn et al., 2003" startWordPosition="1911" endWordPosition="1914">del 26.15 19.04 Table 2. Basic system comparisons: BLEU scores and model size in millions of phrase pairs for SpanishEnglish The results in Table 3 compare the joint and the standard model with more features. Apart from including all Pharaoh’s default features, we use two new features for both the standard and joint models: a 5-gram language model and a lexicalized reordering model as described in Koehn et al. (2005). The weights of the feature functions, or model components, are set by minimum error rate training provided by David Chiang from the University of Maryland. On smaller data sets (Koehn et al., 2003) the joint model shows performance comparable to the standard model, however the joint model does not reach the level of performance of the stan156 EN-ES ES-EN Joint 3-gram, dl4 20.51 26.64 5-gram, dl6 26.34 27.17 + lex. reordering 26.82 27.80 Standard Model 5-gram, dl6 31.18 31.86 + lex. reordering Table 3. Bleu scores for the joint model and the standard model showing the effect of the 5-gram language model, distortion length of 6 (dl) and the addition of lexical reordering for the English-Spanish and SpanishEnglish tasks. dard model for this larger data set. This could be due to the fact th</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT/NAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Edinburgh system description.</title>
<date>2005</date>
<booktitle>In IWSLT Speech Translation Evaluation.</booktitle>
<contexts>
<context position="11837" citStr="Koehn et al. (2005)" startWordPosition="1877" endWordPosition="1880">ults in a much smaller phrase table. Pruning eliminates many phrase pairs, but further investigations indicate that this has little impact on BLEU scores. BLEU Size Joint Model 25.49 2.28 Standard Model 26.15 19.04 Table 2. Basic system comparisons: BLEU scores and model size in millions of phrase pairs for SpanishEnglish The results in Table 3 compare the joint and the standard model with more features. Apart from including all Pharaoh’s default features, we use two new features for both the standard and joint models: a 5-gram language model and a lexicalized reordering model as described in Koehn et al. (2005). The weights of the feature functions, or model components, are set by minimum error rate training provided by David Chiang from the University of Maryland. On smaller data sets (Koehn et al., 2003) the joint model shows performance comparable to the standard model, however the joint model does not reach the level of performance of the stan156 EN-ES ES-EN Joint 3-gram, dl4 20.51 26.64 5-gram, dl6 26.34 27.17 + lex. reordering 26.82 27.80 Standard Model 5-gram, dl6 31.18 31.86 + lex. reordering Table 3. Bleu scores for the joint model and the standard model showing the effect of the 5-gram lan</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, and Chris Callison-Burch. 2005. Edinburgh system description. In IWSLT Speech Translation Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2000" citStr="Marcu and Wong (2002)" startWordPosition="283" endWordPosition="286">ally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the alignment template model (Och, 2003), improve on word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering. However, most phrase-based models extract their phrase pairs from previously word-aligned corpora using adhoc heuristics. These models perform no search for optimal phrasal alignments. Even though this is an efficient strategy, it is a departure from the rigorous statistical framework of the IBM Models. Marcu and Wong (2002) proposed the joint probability model which directly estimates the phrase translation probabilities from the corpus in a theoretically governed way. This model neither relies on potentially sub-optimal word alignments nor on heuristics for phrase extraction. Instead, it searches the phrasal alignment space, simultaneously learning translation lexicons for both words and phrases. The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). However, considering all possible </context>
<context position="7705" citStr="Marcu and Wong, 2002" startWordPosition="1182" endWordPosition="1185">ations metrized alignment. This is problematic because of phrases and all their possible alignments vastly the symmetrization process will grow an align- increases the number of possible alignments that ment based on arbitrary decisions about adjacent can be formed between two sentences. This numwords and because word alignments inadequately ber is exponential with relation to the length of the represent the real dependencies between transla- shorter sentence. tions. 3 Constraining the Joint Model 2.2 Joint Probability Model The joint model requires a strategy for restricting The joint model (Marcu and Wong, 2002), does the search for phrasal alignments to areas of the not rely on a pre-existing set of word-level align- alignment space which contain most of the probaments. Like the IBM Models, it uses EM to align bility mass. We propose a method which examines and estimate the probabilities for sub-sentential units in a parallel corpus. Unlike the IBM Mod155 phrase pairs that are consistent with a set of high confidence word alignments defined for the sentence. The set of alignments are taken from the intersection of the bi-directional Viterbi alignments. This strategy for extracting phrase pairs is si</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>A word-to-word model of translational equivalence.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6201" citStr="Melamed, 1997" startWordPosition="959" endWordPosition="960"> from the points culating the proportion of alignments which conof high confidence in the intersection of the two tain p(&lt; e2, f2 &gt;) compared to the total number Viterbi alignments and growing towards the points of alignments in the sentence (Marcu and Wong, in the union. Points from the union are selected if 2002). EM is then performed by first discovering they are adjacent to points from the intersection an initial phrasal alignments using a greedy algoand their words are previously unaligned. rithm similar to the competitive linking algorithm Phrases are then extracted by selecting phrase (Melamed, 1997). The highest probability phrase pairs which are ‘consistent’ with the symmetrized pairs are iteratively selected until all phrases are alignment, which means that all words within the are linked. Then hill-climbing is performed by source language phrase are only aligned to the searching once for each iteration for all merges, words of the target language phrase and vice versa. splits, moves and swaps that improve the probaFinally the phrase translation probability distribu- bility of the initial phrasal alignment. Fractional tion is estimated using the relative frequencies of counts are colle</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>Dan Melamed. 1997. A word-to-word model of translational equivalence. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="10666" citStr="Och and Ney, 2002" startWordPosition="1676" endWordPosition="1679">larger corpora. After the initialization phase of the training, all phrase pairs with counts less Unconstrained Constrained No. Concepts 6,178k 1,457k BLEU 19.93 22.13 Time(min) 299 169 Table 1. The impact of constraining the joint model trained on 10,000 sentences of the German-English Europarl corpora and tested with the Europarl test set used in Koehn et al. (2003) than 10 million times that of the phrase pair with the highest count, are pruned from the phrase table. The model is also parallelized in order to speed up training. The translation models are included within a log-linear model (Och and Ney, 2002) which allows a weighted combination of features functions. For the comparison of the basic systems in Table 2 only three features were used for both the joint and the standard model: p(e|f), p(f|e) and the language model, and they were given equal weights. The results in Table 2 show that the joint model is capable of training on large data sets, with a reasonable performance compared to the standard model. However, here it seems that the standard model has a slight advantage. This is almost certainly related to the fact that the joint model results in a much smaller phrase table. Pruning eli</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen Department of Computer Science,</institution>
<location>Aachen, Germany.</location>
<contexts>
<context position="1532" citStr="Och, 2003" startWordPosition="216" endWordPosition="217">1 Introduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the alignment template model (Och, 2003), improve on word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering. However, most phrase-based models extract their phrase pairs from previously word-aligned corpora using adhoc heuristics. These models perform no search for optimal phrasal alignments. Even though this is an efficient strategy, it is a departure from the rigorous statistical framework of the IBM Models. Marcu and Wong (2002) proposed the joint probability model which directly estimates the phrase translation probabilities from the corpus in a theoretical</context>
<context position="3749" citStr="Och, 2003" startWordPosition="540" endWordPosition="541">dence alignments could incorporate information from both statistical and linguistic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. 154 Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, New York City, June 2006. c�2006 Association for Computational Linguistics 2 Translation Models els, it does not constrain the alignments to being 2.1 Standard Phrase-based Model single words. Most phrase-based translation models (Och, 2003; The joint model creates phrases from words and Koehn et al., 2003; Vogel et al., 2003) rely on commonly occurring sequences of words. A cona pre-existing set of word-based alignments from cept, c2, is defined as a pair of aligned phrases which they induce their parameters. In this project &lt; e2, f2 &gt;. A set of concepts which completely we use the model described by Koehn et al. (2003) covers the sentence pair is denoted by C. Phrases which extracts its phrase alignments from a corpus are restricted to being sequences of words which that has been word aligned. From now on we re- occur above a </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen Department of Computer Science, Aachen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Przybocki</author>
</authors>
<title>machine translation evaluation results. Confidential e-mail to workshop participants,</title>
<date>2004</date>
<contexts>
<context position="2564" citStr="Przybocki, 2004" startWordPosition="367" endWordPosition="368">l framework of the IBM Models. Marcu and Wong (2002) proposed the joint probability model which directly estimates the phrase translation probabilities from the corpus in a theoretically governed way. This model neither relies on potentially sub-optimal word alignments nor on heuristics for phrase extraction. Instead, it searches the phrasal alignment space, simultaneously learning translation lexicons for both words and phrases. The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). However, considering all possible phrases and all their possible alignments vastly increases the computational complexity of the joint model when compared to its word-based counterpart. In this paper, we propose a method of constraining the search space of the joint model to areas where most of the unpromising phrasal alignments are eliminated and yet as many potentially useful alignments as possible are still explored. The joint model is constrained to phrasal alignments which do not contradict a set high confidence word alignments for each sentence. These high confidence alignments could i</context>
</contexts>
<marker>Przybocki, 2004</marker>
<rawString>Mark Przybocki. 2004. NIST 2004 machine translation evaluation results. Confidential e-mail to workshop participants, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Ying Zhang</author>
<author>Fei Huang</author>
<author>Alicia Tribble</author>
<author>Ashish Venugopal</author>
<author>Bing Zhao</author>
<author>Alex Waibel</author>
</authors>
<title>The CMU statistical machine translation system.</title>
<date>2003</date>
<booktitle>In Machine Translation Summit.</booktitle>
<contexts>
<context position="3837" citStr="Vogel et al., 2003" startWordPosition="554" endWordPosition="557">istic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. 154 Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, New York City, June 2006. c�2006 Association for Computational Linguistics 2 Translation Models els, it does not constrain the alignments to being 2.1 Standard Phrase-based Model single words. Most phrase-based translation models (Och, 2003; The joint model creates phrases from words and Koehn et al., 2003; Vogel et al., 2003) rely on commonly occurring sequences of words. A cona pre-existing set of word-based alignments from cept, c2, is defined as a pair of aligned phrases which they induce their parameters. In this project &lt; e2, f2 &gt;. A set of concepts which completely we use the model described by Koehn et al. (2003) covers the sentence pair is denoted by C. Phrases which extracts its phrase alignments from a corpus are restricted to being sequences of words which that has been word aligned. From now on we re- occur above a certain frequency in the corpus. fer to this phrase-based translation model as the Commo</context>
</contexts>
<marker>Vogel, Zhang, Huang, Tribble, Venugopal, Zhao, Waibel, 2003</marker>
<rawString>Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble, Ashish Venugopal, Bing Zhao, and Alex Waibel. 2003. The CMU statistical machine translation system. In Machine Translation Summit.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>