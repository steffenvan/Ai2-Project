<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.042010">
<title confidence="0.999229">
A Dataset of Syntactic-Ngrams over Time
from a Very Large Corpus of English Books
</title>
<author confidence="0.982551">
Yoav Goldberg Jon Orwant
</author>
<affiliation confidence="0.964785">
Bar Ilan University* Google Inc.
</affiliation>
<email confidence="0.998326">
yoav.goldberg@gmail.com orwant@google.com
</email>
<sectionHeader confidence="0.993883" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999216">
We created a dataset of syntactic-ngrams
(counted dependency-tree fragments) based
on a corpus of 3.5 million English books. The
dataset includes over 10 billion distinct items
covering a wide range of syntactic configura-
tions. It also includes temporal information,
facilitating new kinds of research into lexical
semantics over time. This paper describes the
dataset, the syntactic representation, and the
kinds of information provided.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999228153846154">
The distributional hypothesis of Harris (1954) states
that properties of words can be captured based on
their contexts. The consequences of this hypoth-
esis have been leveraged to a great effect by the
NLP community, resulting in algorithms for in-
ferring syntactic as well as semantic properties of
words (see e.g. (Turney and Pantel, 2010; Baroni
and Lenci, 2010) and the references therein).
In this paper, we describe a very large dataset
of syntactic-ngrams, that is, structures in which the
contexts of words are based on their respective po-
sition in a syntactic parse tree, and not on their se-
quential order in the sentence: the different words in
the ngram may be far apart from each other in the
sentence, yet close to each other syntactically. See
Figure 1 for an example of a syntactic-ngram.
The utility of syntactic contexts of words for con-
structing vector-space models of word meanings is
well established (Lin, 1998; Lin and Pantel, 2001;
Pad´o and Lapata, 2007; Baroni and Lenci, 2010).
Syntactic relations are successfully used for mod-
eling selectional preferences (Erk and Pad´o, 2008;
*Work performed while at Google.
Erk et al., 2010; Ritter et al., 2010; S´eaghdha,
2010), and dependency paths are also used to in-
fer binary relations between words (Lin and Pantel,
2001; Wu and Weld, 2010). The use of syntactic-
ngrams holds promise also for improving the accu-
racy of core NLP tasks such as syntactic language-
modeling (Shen et al., 2008) and syntactic-parsing
(Chen et al., 2009; Sagae and Gordon, 2009; Co-
hen et al., 2012), though most successful attempts
to improve syntactic parsing by using counts from
large corpora are based on sequential rather than
syntactic information (Koo et al., 2008; Bansal and
Klein, 2011; Pitler, 2012), we believe this is be-
cause large-scale datasets of syntactic counts are not
readily available. Unfortunately, most work utiliz-
ing counts from large textual corpora does not use a
standardized corpora for constructing their models,
making it very hard to reproduce results and chal-
lenging to compare results across different studies.
Our aim in this work is not to present new meth-
ods or results, but rather to provide a new kind of a
large-scale (based on corpora about 100 times larger
than previous efforts) high-quality and standard re-
source for researchers to build upon. Instead of fo-
cusing on a specific task, we aim to provide a flexi-
ble resource that could be adapted to many possible
tasks.
Specifically, the contribution of this work is in
creating a dataset of syntactic-ngrams which is:
</bodyText>
<listItem confidence="0.999758">
• Derived from a very large (345 billion words)
corpus spanning a long time period.
• Covers a wide range of syntactic phenomena
and is adaptable to many use cases.
• Based on state-of-the-art syntactic processing
in a modern syntactic representation.
• Broken down by year of occurrence, as well
</listItem>
<page confidence="0.948885">
241
</page>
<note confidence="0.408452">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 241–247, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.942150166666667">
Figure 1: A syntactic ngram appearing 112 times in
the extended-biarcs set, which include structures contain-
ing three content words (see Section 4). Grayed items
are non-content words and are not included in the word
count. The dashed auxiliary “have” is a functional marker
(see Section 3), appearing only in the extended-* sets.
</figureCaption>
<listItem confidence="0.877351">
as some coarse-grained regional and genre dis-
tinctions (British, American, Fiction).
• Freely available for non-commercial use. 1
</listItem>
<bodyText confidence="0.999695466666667">
After describing the underlying syntactic represen-
tation, we will present our definition of a syntactic-
ngram, and detail the kinds of syntactic-ngrams we
chose to include in the dataset. Then, we present de-
tails of the corpus and the syntactic processing we
performed.
With respect to previous efforts, the dataset has the
following distinguishing characteristics:
Temporal Dimension A unique aspect of our
dataset is the temporal dimension, allowing inspec-
tion of how the contexts of different words vary
over time. For example, one could examine how the
meaning of a word evolves over time by looking at
the contexts it appears in within different time peri-
ods. Figure 2 shows the cosine similarity between
the word “rock” and the words “stone” and “jazz”
from year 1930 to 2000, showing that rock acquired
a new meaning around 1968.
Large syntactic contexts Previous efforts of provid-
ing syntactic counts from large scale corpora (Ba-
roni and Lenci, 2010) focus on relations between
two content words. Our dataset include structures
covering much larger tree fragments, some of them
including 5 or more content words. By including
such structures we hope to encourage research ex-
ploring higher orders of interactions, for example
modeling the relation between adjectives of two con-
joined nouns, the interactions between subjects and
objects of verbs, or fine-grained selectional prefer-
ences of verbs and nouns.
</bodyText>
<footnote confidence="0.99837675">
1The dataset is made publicly available under the Cre-
ative Commons Attribution-Non Commercial ShareAlike 3.0
Unported License: http://creativecommons.org/licenses/by-nc-
sa/3.0/legalcode.
</footnote>
<figureCaption confidence="0.995544666666667">
Figure 2: Word-similarity over time: The word “rock” starts
to become similar to “jazz” around 1968. The plot shows the
cosine similarity between the immediate syntactic contexts of
of the word “rock” in each year, to the immediate syntactic con-
texts of the words “jazz” (in red) and “stone” (in blue) aggre-
gated over all years.
</figureCaption>
<bodyText confidence="0.99953775">
A closely related effort to add syntactic anno-
tation to the books corpus is described in Lin et
al. (2012). That effort emphasize an interactive
query interface covering several languages, in which
the underlying syntactic representations are linear-
ngrams enriched with universal part-of-speech tags,
as well as first order unlabeled dependencies. In
contrast, our emphasis is not on an easy-to-use query
interface but instead a useful and flexible resource
for computational-minded researchers. We focus
on English and use finer-grained English-specific
POS-tags. The syntactic analysis is done using a
more accurate parser, and we provide counts over la-
beled tree fragments, covering a diverse set of tree-
fragments many of which include more than two
content words.
Counted Fragments instead of complete trees
While some efforts provide complete parse trees
from large corpora (Charniak, 2000; Baroni et al.,
2009; Napoles et al., 2012), we instead provide
counted tree fragments. We believe that our form of
aggregate information is of more immediate use than
the raw parse trees. While access to the parse trees
may allow for somewhat greater flexibility in the
kinds of questions one could ask, it also comes with
a very hefty price tag in terms of the required com-
putational resources: while counting seems trivial,
it is, in fact, quite demanding computationally when
done on such a scale, and requires a massive infras-
tructure. By lifting this burden of NLP researchers,
we hope to free them to tackle interesting research
questions.
</bodyText>
<page confidence="0.992719">
242
</page>
<sectionHeader confidence="0.863578" genericHeader="method">
2 Underlying Syntactic Representation
</sectionHeader>
<bodyText confidence="0.999984">
We assume the part-of-speech tagset of the Penn
Treebank (Marcus et al., 1993). The syntactic rep-
resentation we work with is based on dependency-
grammar. Specifically, we use labeled dependency
trees following the “basic” variant of the Stanford-
dependencies scheme (de Marneffe and Manning,
2008b; de Marneffe and Manning, 2008a).
Dependency grammar is a natural choice, as it
emphasizes individual words and explicitly mod-
els the connections between them. Stanford de-
pendencies are appealing because they model rela-
tions between content words directly, without in-
tervening functional markers (so in a construction
such as “wanted to know” there is a direct rela-
tion (wanted, know) instead of two relation
(wanted, to) and (to, know). This facil-
itates focusing on meaning-bearing content words
and including the maximal amount of information
in an ngram.
</bodyText>
<sectionHeader confidence="0.958971" genericHeader="method">
3 Syntactic-ngrams
</sectionHeader>
<bodyText confidence="0.999974087719298">
We define a syntactic-ngram to be a rooted con-
nected dependency tree over k words, which is a
subtree of a dependency tree over an entire sentence.
For each of the k words in the ngram, we provide in-
formation about the word-form, its part-of-speech,
and its dependency relation to its head. The ngram
also indicates the relative ordering between the dif-
ferent words (the order of the words in the syntactic-
ngram is the same as the order in which the words
appear in the underlying sentence) but not the dis-
tance between them, nor an indication whether there
is a missing material between the nodes. Examples
of syntactic-ngrams are provided in Figures 1 and 3.
Content-words and Functional-markers We dis-
tinguish between content-words which are mean-
ing bearing elements and functional-markers, which
serve to add polarity, modality or definiteness in-
formation to the meaning bearing elements, but do
not carry semantic meaning of their own, such as
the auxiliary verb “have” in Figure 1. Specifi-
cally, we treat words with a dependency-label of
det, poss, neg, aux, auxpass, ps, mark,
complm and prt as functional-markers. With the
exception of poss, these are all closed-class cat-
egories. All other words except for prepositions
and conjunctions are treated as content-words. A
syntactic-ngram of order n includes exactly n con-
tent words. It may optionally include all of the
functional-markers that modify the content-words.
Conjunctions and Prepositions Conjunctions and
Prepositions receive a special treatment. When a co-
ordinating word (“and”, “or”, “but”) appears as part
of a conjunctive structure (e.g. “X, Y, and Z”), it
is treated as a non-content word. Instead, it is al-
ways included in the syntactic-ngrams that include
the conjunctive relation it is a part of, allowing to
differentiate between the various kinds of conjunc-
tions. An example is seen in Figure 3d, in which
the relation conj(efficient, effective)
is enriched with the coordinating word “or”. When
a coordinating word does not explicitly take part in
a conjunction relation (e.g. “But, ... ”) it is treated
as a content word.
When a preposition is part of a prepositional mod-
ification (i.e. in the middle of the pair (prep,
pcomp) or (prep, pobj)), such as the word
“of” in Figures 1 and 3h and the word “as” in Figure
3e, it is treated as a non-content word, and is always
included in a syntactic-ngram whenever the words it
connects are included. In cases of ellipsis or other
cases where there is no overt pobj or pcomp (“he
is hard to deal with”) the preposition is treated as a
content word.2
Multiword Expressions Some multiword expres-
sions are recognized by the parser. Whenever a con-
tent word in an ngram has modifiers with the mwe
relation, they are included in the ngram.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="method">
4 The Provided Ngram Types
</sectionHeader>
<bodyText confidence="0.978055933333333">
We aimed to include a diverse set of relations, with
maximal emphasis on relations between content-
bearing words, while still retaining access to defi-
2This treatment of prepositions and conjunction is similar to
the “collapsed” variant of Stanford Dependencies (de Marneffe
and Manning, 2008a), in which preposition- and conjunction-
words do not appear as nodes in the tree but are instead anno-
tated on the dependency label between the content words they
connect, e.g. prep with(saw, telescope). However,
we chose to represent the preposition or conjunction as a node
in the tree rather than moving it to the dependency label as it
retains the information about the location of the function word
with respect to the other words in the structure, is consistent
with cases in which one of the content words is not present, and
does not blow up the label-set size.
</bodyText>
<page confidence="0.993534">
243
</page>
<bodyText confidence="0.999880820512821">
niteness, modality and polarity if they are desired.
The dataset includes the following types of syntactic
structures:
nodes (47M items) consist of a single content word,
and capture the syntactic role of that word (as in Fig-
ure 3a). For example, we can learn that the pro-
noun “he” is predominantly used as a subject, and
that “help” as a noun is over 4 times more likely to
appear in object than in subject position.
arcs (919M items) consist of two content words, and
capture direct dependency relations such as “subject
of”, “adverbial modifier of” and so on (see Figure
3c,3d for examples). These correspond to “depen-
dency triplets” as used in Lin (1998) and most other
work on syntax-based semantic similarity.
biarcs (1.78B items) consist of three content words
(either a word and its two daughters, or a child-
parent-grandparent chain) and capture relations such
as “subject verb object”, “a noun and two adjectivial
modifiers”, “verb, object and adjectivial modifier of
the object” and many others.
triarcs (1.87B items) consist of four content words
(example in Figure 3f). The locality of the depen-
dency representation causes this set of three-arcs
structures to be large, sparse and noisy – many of
the relations may appear random because some arcs
are in many cases almost independent given the oth-
ers. However, some of the relations are known to be
of interest, and we hope more of them will prove to
be of interest in the future. Some of the interesting
relations include:
- modifiers of the head noun of the subject or object
in an SVO construction: ((small,boy), ate, cookies),
(boy, ate, (tasty, cookies)), and with abstraction: ad-
jectives that a boy likes to eat: (boy, ate, (tasty, *) )
- arguments of an embeded verb (said, (boy, ate,
cookie) ), (said, ((small, boy), ate) )
- modifiers of conjoined elements ( (small, boy)
(young, girl) ) , ( (small, *) (young, *) )
- relative clause constructions ( boy, (girl, with-
cookies, saw) )
quadarcs (187M items) consist of 5 content words
(example in Figure 3h). In contrast to the previous
datasets, this set includes only a subset of the pos-
sible relations involving 5 content words. We chose
to focus on relations which are attested in the liter-
ature (Pad´o and Lapata 2007; Appendix A), namely
structures consisting of two chains of length 2 with a
single head, e.g. ( (small, boy), ate, (tasty, cookie) ).
extended-nodes, extended-arcs, extended-biarcs,
extended-triarcs, extended-quadarcs (80M,
1.08B, 1.62B, 1.71B, and 180M items) Like the
above, but the functional markers of each content
words are included as well (see examples in Figures
3b, 3e, 3g). These structures retain information
regarding aspects such as modality, polarity and
definiteness, distinguishing, e.g. “his red car” from
“her red car”, “will go” from “should go” and “a
best time” from “the best time”.
verbargs (130M items) This set of ngrams consist
of verbs with all their immediate arguments, and
can be used to study interactions between modi-
fiers of a verb, as well as subcategorization frames.
These structures are also useful for syntactic lan-
guage modeling, as all the daughters of a verb are
guaranteed to be present.
nounargs (275M items) This set of ngrams consist
of nouns with all their immediate arguments.
verbargs-unlex, nounargs-unlex (114M, 195M
items) Like the above, but only the head word and
the top-1000 occurring words in the English-1M
subcorpus are lexicalized – other words are replaced
with a *W* symbol. By abstracting away from non-
frequent words, we include many of the larger syn-
tactic configurations that will otherwise be pruned
away by our frequency threshold. These could be
useful for inspecting fine-grained syntactic subcate-
gorization frames.
</bodyText>
<sectionHeader confidence="0.976403" genericHeader="evaluation">
5 Corpora and Syntactic Processing
</sectionHeader>
<bodyText confidence="0.997687">
The dataset is based on the English Google Books
corpus. This is the same corpus used to derive the
Google Books Ngrams, and is described in detail in
Michel et al. (2011). The corpus consists of the text
of 3,473,595 English books which were published
between 1520 and 2008, with the majority of the
content published after 1800. We provide counts
based on the entire corpus, as well as on several sub-
sets of it:
English 1M Uniformly sampled 1 million books.
Fiction Works of Fiction.
American English Books published in the US.
British English Books published in Britain.
The sizes of the different corpora are detailed in Ta-
ble 1.
</bodyText>
<page confidence="0.997976">
244
</page>
<figureCaption confidence="0.717745333333333">
Figure 3: Syntactic-ngram examples. Non-content words are
grayed, functional markers appearing only in the extended-*
collections are dashed. (a) node (b) extended-node (c) arcs (d)
arcs, including the coordinating word (e) extended-arcs, includ-
ing a preposition (f) triarcs (g) extended-triarcs (h) quadarcs,
including a preposition.
</figureCaption>
<bodyText confidence="0.997225333333333">
Counts Each syntactic ngram in each of the sub-
corpora is coupled with a corpus-level count as well
as counts from each individual year. To keep the
</bodyText>
<table confidence="0.9887415">
Corpus # Books # Pages # Sentences # Tokens
All 3.5M 925.7M 17.6B 345.1B
1M 1M 291.1M 5.1B 101.3B
Fiction 817K 231.3M 4.7B 86.1B
American 1.4M 387.6M 7.9B 146.2B
British 423K 124.9M 2.4B 46.1B
</table>
<tableCaption confidence="0.999935">
Table 1: Corpora sizes.
</tableCaption>
<bodyText confidence="0.991245">
data manageable, we employ a frequency threshold
of 10 on the corpus-level count.
Data Processing We ignored pages with over 600
white-spaces (which are indicative of OCR errors or
non-textual content), as well as sentences of over 60
tokens. Table 1 details the sizes of the various cor-
pora.
After OCR, sentence splitting and tokenization,
the corpus went through several stages of syntactic
processing: part-of-speech tagging, syntactic pars-
ing, and syntactic-ngrams extraction.
Part-of-speech tagging was performed using a
first order CRF tagger, which was trained on a union
of the Penn WSJ Corpus (Marcus et al., 1993), the
Brown corpus (Kucera and Francis, 1967) and the
Questions Treebank (Judge et al., 2006). In addition
to the diverse training material, the tagger makes use
of features based on word-clusters derived from tri-
grams of the Books corpus. These cluster-features
make the tagger more robust on the books domain.
For further details regarding the tagger, see Lin et al.
(2012).
Syntactic parsing was performed using a re-
implementation of a beam-search shift-reduce de-
pendency parser (Zhang and Clark, 2008) with a
beam of size 8 and the feature-set described in
Zhang and Nivre (2011). The parser was trained
on the same training data as the tagger after 4-way
jack-knifing so that the parser is trained on data with
predicted part-of-speech tags. The parser provides
state-of-the-art syntactic annotations for English.3
3Evaluating the quality of syntactic annotation on such a var-
ied dataset is a challenging task on its own right – the underly-
ing corpus includes many different genres spanning different
time periods, as well as varying levels of digitization and OCR
quality. It is extremely difficult to choose a representative sam-
ple to manually annotate and evaluate on, and we believe no
single number will do justice to describing the annotation qual-
ity across the entire dataset. On top of that, we then aggregate
fragments and filter based on counts, further changing the data
distribution. We feel that it is better not to provide any numbers
than to provide inaccurate, misleading or uninformative num-
</bodyText>
<page confidence="0.99843">
245
</page>
<sectionHeader confidence="0.995046" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998888">
We created a dataset of syntactic-ngrams based on
a very large literary corpus. The dataset contains
over 10 billion unique items covering a wide range
of syntactic structures, and includes a temporal di-
mension.
The dataset is available for download at
http://storage.googleapis.com/
books/syntactic-ngrams/index.html
</bodyText>
<sectionHeader confidence="0.997388" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999866357142857">
We would like to thank the members of Google’s
extended syntactic-parsing team (Ryan McDonald,
Keith Hall, Slav Petrov, Dipanjan Das, Hao Zhang,
Kuzman Ganchev, Terry Koo, Michael Ringgaard
and, at the time, Joakim Nivre) for many discus-
sions, support, and of course the creation and main-
tenance of an extremely robust parsing infrastruc-
ture. We further thank Fernando Pereira for sup-
porting the project, and Andrea Held and Supreet
Chinnan for their hard work in making this possible.
Sebastian Pad´o, Marco Baroni, Alessandro Lenci,
Jonathan Berant and Dan Klein provided valuable
input that helped shape the final form of this re-
source.
</bodyText>
<sectionHeader confidence="0.99838" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.966581176470588">
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In ACL, pages 693–702.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673–
721.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Eugene Charniak. 2000. Bllip 1987-89 wsj corpus re-
lease 1. In Linguistic Data Consortium, Philadelphia.
Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data. In
EMNLP, pages 570–579.
</reference>
<bodyText confidence="0.85973075">
bers. We therefore chose not to provide a numeric estimation
of syntactic-annotation quality, but note that we used a state-
of-the-art parser, and believe most of its output to be correct,
although we do expect a fair share of annotation errors as well.
</bodyText>
<reference confidence="0.985461230769231">
Raphael Cohen, Yoav Goldberg, and Michael Elhadad.
2012. Domain adaptation of a dependency parser with
a class-class selectional preference model. In Pro-
ceedings of ACL 2012 Student Research Workshop,
pages 43–48, Jeju Island, Korea, July. Association for
Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008a. Stanford dependencies manual. Techni-
cal report, Stanford University.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008b. The stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, CrossParser ’08, pages 1–8, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Katrin Erk and Sebastian Pad´o. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP, Honolulu, HI. To appear.
Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723–763.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In Proc. of ACL, pages 497–504. Associa-
tion for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL, pages 595–603.
Henry Kucera and W. Nelson Francis. 1967. Compu-
tational Analysis of Present-Day American English.
Brown University Press.
Dekang Lin and Patrick Pantel. 2001. Dirt: discovery of
inference rules from text. In KDD, pages 323–328.
Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the google books ngram cor-
pus. In ACL (System Demonstrations), pages 169–
174.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ’98, pages 768–
774, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313–330.
</reference>
<page confidence="0.978715">
246
</page>
<reference confidence="0.999584953488372">
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hoiberg,
Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,
Martin A. Nowak, and Erez Lieberman Aiden. 2011.
Quantitative analysis of culture using millions of digi-
tized books. Science, 331(6014):176–182.
Courtney Napoles, Matthew Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.
Sebastian Pad´o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161–199.
Emily Pitler. 2012. Attacking parsing bottlenecks with
unlabeled data and relevant factorizations. In ACL,
pages 768–776.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet allocation method for selectional preferences.
In ACL, pages 424–434.
Kenji Sagae and Andrew S. Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In IWPT,
pages 192–201.
Diarmuid O´ S´eaghdha. 2010. Latent variable models of
selectional preference. In ACL, pages 435–444.
Libin Shen, Jinxi Xu, and Ralph M. Weischedel. 2008.
A new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
ACL, pages 577–585.
P.D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37(1):141–188.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In ACL, pages 118–127.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proc. of
EMNLP, pages 562–571.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188–193.
</reference>
<page confidence="0.997975">
247
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.976986">
<title confidence="0.9939135">A Dataset of Syntactic-Ngrams over from a Very Large Corpus of English Books</title>
<author confidence="0.997214">Yoav Goldberg Jon Orwant</author>
<affiliation confidence="0.999953">Ilan Inc.</affiliation>
<email confidence="0.99735">yoav.goldberg@gmail.comorwant@google.com</email>
<abstract confidence="0.999382">We created a dataset of syntactic-ngrams (counted dependency-tree fragments) based on a corpus of 3.5 million English books. The dataset includes over 10 billion distinct items covering a wide range of syntactic configurations. It also includes temporal information, facilitating new kinds of research into lexical semantics over time. This paper describes the dataset, the syntactic representation, and the kinds of information provided.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Web-scale features for full-scale parsing. In</title>
<date>2011</date>
<booktitle>ACL,</booktitle>
<pages>693--702</pages>
<contexts>
<context position="2384" citStr="Bansal and Klein, 2011" startWordPosition="376" endWordPosition="379">d while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim in this work is not to present new methods or results, but rather to provide a new kind of a large-scale (based on corpora about 100 times larger than previous efforts) high-quality and standard resource for researchers to build upon. Instead</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>Mohit Bansal and Dan Klein. 2011. Web-scale features for full-scale parsing. In ACL, pages 693–702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<pages>721</pages>
<contexts>
<context position="1008" citStr="Baroni and Lenci, 2010" startWordPosition="146" endWordPosition="149">ange of syntactic configurations. It also includes temporal information, facilitating new kinds of research into lexical semantics over time. This paper describes the dataset, the syntactic representation, and the kinds of information provided. 1 Introduction The distributional hypothesis of Harris (1954) states that properties of words can be captured based on their contexts. The consequences of this hypothesis have been leveraged to a great effect by the NLP community, resulting in algorithms for inferring syntactic as well as semantic properties of words (see e.g. (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein). In this paper, we describe a very large dataset of syntactic-ngrams, that is, structures in which the contexts of words are based on their respective position in a syntactic parse tree, and not on their sequential order in the sentence: the different words in the ngram may be far apart from each other in the sentence, yet close to each other syntactically. See Figure 1 for an example of a syntactic-ngram. The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and </context>
<context position="5156" citStr="Baroni and Lenci, 2010" startWordPosition="821" endWordPosition="825">istinguishing characteristics: Temporal Dimension A unique aspect of our dataset is the temporal dimension, allowing inspection of how the contexts of different words vary over time. For example, one could examine how the meaning of a word evolves over time by looking at the contexts it appears in within different time periods. Figure 2 shows the cosine similarity between the word “rock” and the words “stone” and “jazz” from year 1930 to 2000, showing that rock acquired a new meaning around 1968. Large syntactic contexts Previous efforts of providing syntactic counts from large scale corpora (Baroni and Lenci, 2010) focus on relations between two content words. Our dataset include structures covering much larger tree fragments, some of them including 5 or more content words. By including such structures we hope to encourage research exploring higher orders of interactions, for example modeling the relation between adjectives of two conjoined nouns, the interactions between subjects and objects of verbs, or fine-grained selectional preferences of verbs and nouns. 1The dataset is made publicly available under the Creative Commons Attribution-Non Commercial ShareAlike 3.0 Unported License: http://creativeco</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673– 721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="7042" citStr="Baroni et al., 2009" startWordPosition="1104" endWordPosition="1107">ags, as well as first order unlabeled dependencies. In contrast, our emphasis is not on an easy-to-use query interface but instead a useful and flexible resource for computational-minded researchers. We focus on English and use finer-grained English-specific POS-tags. The syntactic analysis is done using a more accurate parser, and we provide counts over labeled tree fragments, covering a diverse set of treefragments many of which include more than two content words. Counted Fragments instead of complete trees While some efforts provide complete parse trees from large corpora (Charniak, 2000; Baroni et al., 2009; Napoles et al., 2012), we instead provide counted tree fragments. We believe that our form of aggregate information is of more immediate use than the raw parse trees. While access to the parse trees may allow for somewhat greater flexibility in the kinds of questions one could ask, it also comes with a very hefty price tag in terms of the required computational resources: while counting seems trivial, it is, in fact, quite demanding computationally when done on such a scale, and requires a massive infrastructure. By lifting this burden of NLP researchers, we hope to free them to tackle inter</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Bllip 1987-89 wsj corpus release 1.</title>
<date>2000</date>
<booktitle>In Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="7021" citStr="Charniak, 2000" startWordPosition="1102" endWordPosition="1103">part-of-speech tags, as well as first order unlabeled dependencies. In contrast, our emphasis is not on an easy-to-use query interface but instead a useful and flexible resource for computational-minded researchers. We focus on English and use finer-grained English-specific POS-tags. The syntactic analysis is done using a more accurate parser, and we provide counts over labeled tree fragments, covering a diverse set of treefragments many of which include more than two content words. Counted Fragments instead of complete trees While some efforts provide complete parse trees from large corpora (Charniak, 2000; Baroni et al., 2009; Napoles et al., 2012), we instead provide counted tree fragments. We believe that our form of aggregate information is of more immediate use than the raw parse trees. While access to the parse trees may allow for somewhat greater flexibility in the kinds of questions one could ask, it also comes with a very hefty price tag in terms of the required computational resources: while counting seems trivial, it is, in fact, quite demanding computationally when done on such a scale, and requires a massive infrastructure. By lifting this burden of NLP researchers, we hope to free</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. Bllip 1987-89 wsj corpus release 1. In Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving dependency parsing with subtrees from auto-parsed data. In</title>
<date>2009</date>
<booktitle>EMNLP,</booktitle>
<pages>570--579</pages>
<contexts>
<context position="2142" citStr="Chen et al., 2009" startWordPosition="337" endWordPosition="340">s of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; *Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim </context>
</contexts>
<marker>Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving dependency parsing with subtrees from auto-parsed data. In EMNLP, pages 570–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Cohen</author>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>Domain adaptation of a dependency parser with a class-class selectional preference model.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012 Student Research Workshop,</booktitle>
<pages>43--48</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2187" citStr="Cohen et al., 2012" startWordPosition="345" endWordPosition="349">, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; *Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim in this work is not to present new methods or</context>
</contexts>
<marker>Cohen, Goldberg, Elhadad, 2012</marker>
<rawString>Raphael Cohen, Yoav Goldberg, and Michael Elhadad. 2012. Domain adaptation of a dependency parser with a class-class selectional preference model. In Proceedings of ACL 2012 Student Research Workshop, pages 43–48, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford dependencies manual.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008a. Stanford dependencies manual. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, CrossParser ’08,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008b. The stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, CrossParser ’08, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Honolulu, HI.</location>
<note>To appear.</note>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP, Honolulu, HI. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
<author>Ulrike Pad´o</author>
</authors>
<title>A flexible, corpus-driven model of regular and inverse selectional preferences.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<marker>Erk, Pad´o, Pad´o, 2010</marker>
<rawString>Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010. A flexible, corpus-driven model of regular and inverse selectional preferences. Computational Linguistics, 36(4):723–763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="691" citStr="Harris (1954)" startWordPosition="95" endWordPosition="96">oks Yoav Goldberg Jon Orwant Bar Ilan University* Google Inc. yoav.goldberg@gmail.com orwant@google.com Abstract We created a dataset of syntactic-ngrams (counted dependency-tree fragments) based on a corpus of 3.5 million English books. The dataset includes over 10 billion distinct items covering a wide range of syntactic configurations. It also includes temporal information, facilitating new kinds of research into lexical semantics over time. This paper describes the dataset, the syntactic representation, and the kinds of information provided. 1 Introduction The distributional hypothesis of Harris (1954) states that properties of words can be captured based on their contexts. The consequences of this hypothesis have been leveraged to a great effect by the NLP community, resulting in algorithms for inferring syntactic as well as semantic properties of words (see e.g. (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein). In this paper, we describe a very large dataset of syntactic-ngrams, that is, structures in which the contexts of words are based on their respective position in a syntactic parse tree, and not on their sequential order in the sentence: the different wo</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Judge</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Questionbank: Creating a corpus of parse-annotated questions.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>497--504</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Judge, Cahill, van Genabith, 2006</marker>
<rawString>John Judge, Aoife Cahill, and Josef van Genabith. 2006. Questionbank: Creating a corpus of parse-annotated questions. In Proc. of ACL, pages 497–504. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="2360" citStr="Koo et al., 2008" startWordPosition="372" endWordPosition="375">08; *Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim in this work is not to present new methods or results, but rather to provide a new kind of a large-scale (based on corpora about 100 times larger than previous efforts) high-quality and standard resource for researcher</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proc. of ACL, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Kucera</author>
<author>W Nelson Francis</author>
</authors>
<title>Computational Analysis of Present-Day American English.</title>
<date>1967</date>
<publisher>Brown University Press.</publisher>
<contexts>
<context position="17999" citStr="Kucera and Francis, 1967" startWordPosition="2906" endWordPosition="2909">shold of 10 on the corpus-level count. Data Processing We ignored pages with over 600 white-spaces (which are indicative of OCR errors or non-textual content), as well as sentences of over 60 tokens. Table 1 details the sizes of the various corpora. After OCR, sentence splitting and tokenization, the corpus went through several stages of syntactic processing: part-of-speech tagging, syntactic parsing, and syntactic-ngrams extraction. Part-of-speech tagging was performed using a first order CRF tagger, which was trained on a union of the Penn WSJ Corpus (Marcus et al., 1993), the Brown corpus (Kucera and Francis, 1967) and the Questions Treebank (Judge et al., 2006). In addition to the diverse training material, the tagger makes use of features based on word-clusters derived from trigrams of the Books corpus. These cluster-features make the tagger more robust on the books domain. For further details regarding the tagger, see Lin et al. (2012). Syntactic parsing was performed using a reimplementation of a beam-search shift-reduce dependency parser (Zhang and Clark, 2008) with a beam of size 8 and the feature-set described in Zhang and Nivre (2011). The parser was trained on the same training data as the tagg</context>
</contexts>
<marker>Kucera, Francis, 1967</marker>
<rawString>Henry Kucera and W. Nelson Francis. 1967. Computational Analysis of Present-Day American English. Brown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt: discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In KDD,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="1596" citStr="Lin and Pantel, 2001" startWordPosition="247" endWordPosition="250">l, 2010; Baroni and Lenci, 2010) and the references therein). In this paper, we describe a very large dataset of syntactic-ngrams, that is, structures in which the contexts of words are based on their respective position in a syntactic parse tree, and not on their sequential order in the sentence: the different words in the ngram may be far apart from each other in the sentence, yet close to each other syntactically. See Figure 1 for an example of a syntactic-ngram. The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; *Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt: discovery of inference rules from text. In KDD, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Lin</author>
<author>Jean-Baptiste Michel</author>
<author>Erez Aiden Lieberman</author>
<author>Jon Orwant</author>
<author>Will Brockman</author>
<author>Slav Petrov</author>
</authors>
<title>Syntactic annotations for the google books ngram corpus.</title>
<date>2012</date>
<booktitle>In ACL (System Demonstrations),</booktitle>
<pages>169--174</pages>
<contexts>
<context position="6233" citStr="Lin et al. (2012)" startWordPosition="986" endWordPosition="989"> dataset is made publicly available under the Creative Commons Attribution-Non Commercial ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-ncsa/3.0/legalcode. Figure 2: Word-similarity over time: The word “rock” starts to become similar to “jazz” around 1968. The plot shows the cosine similarity between the immediate syntactic contexts of of the word “rock” in each year, to the immediate syntactic contexts of the words “jazz” (in red) and “stone” (in blue) aggregated over all years. A closely related effort to add syntactic annotation to the books corpus is described in Lin et al. (2012). That effort emphasize an interactive query interface covering several languages, in which the underlying syntactic representations are linearngrams enriched with universal part-of-speech tags, as well as first order unlabeled dependencies. In contrast, our emphasis is not on an easy-to-use query interface but instead a useful and flexible resource for computational-minded researchers. We focus on English and use finer-grained English-specific POS-tags. The syntactic analysis is done using a more accurate parser, and we provide counts over labeled tree fragments, covering a diverse set of tre</context>
<context position="18329" citStr="Lin et al. (2012)" startWordPosition="2960" endWordPosition="2963">f syntactic processing: part-of-speech tagging, syntactic parsing, and syntactic-ngrams extraction. Part-of-speech tagging was performed using a first order CRF tagger, which was trained on a union of the Penn WSJ Corpus (Marcus et al., 1993), the Brown corpus (Kucera and Francis, 1967) and the Questions Treebank (Judge et al., 2006). In addition to the diverse training material, the tagger makes use of features based on word-clusters derived from trigrams of the Books corpus. These cluster-features make the tagger more robust on the books domain. For further details regarding the tagger, see Lin et al. (2012). Syntactic parsing was performed using a reimplementation of a beam-search shift-reduce dependency parser (Zhang and Clark, 2008) with a beam of size 8 and the feature-set described in Zhang and Nivre (2011). The parser was trained on the same training data as the tagger after 4-way jack-knifing so that the parser is trained on data with predicted part-of-speech tags. The parser provides state-of-the-art syntactic annotations for English.3 3Evaluating the quality of syntactic annotation on such a varied dataset is a challenging task on its own right – the underlying corpus includes many diffe</context>
</contexts>
<marker>Lin, Michel, Lieberman, Orwant, Brockman, Petrov, 2012</marker>
<rawString>Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman, Jon Orwant, Will Brockman, and Slav Petrov. 2012. Syntactic annotations for the google books ngram corpus. In ACL (System Demonstrations), pages 169– 174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1574" citStr="Lin, 1998" startWordPosition="245" endWordPosition="246">y and Pantel, 2010; Baroni and Lenci, 2010) and the references therein). In this paper, we describe a very large dataset of syntactic-ngrams, that is, structures in which the contexts of words are based on their respective position in a syntactic parse tree, and not on their sequential order in the sentence: the different words in the ngram may be far apart from each other in the sentence, yet close to each other syntactically. See Figure 1 for an example of a syntactic-ngram. The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; *Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen </context>
<context position="12891" citStr="Lin (1998)" startWordPosition="2076" endWordPosition="2077">red. The dataset includes the following types of syntactic structures: nodes (47M items) consist of a single content word, and capture the syntactic role of that word (as in Figure 3a). For example, we can learn that the pronoun “he” is predominantly used as a subject, and that “help” as a noun is over 4 times more likely to appear in object than in subject position. arcs (919M items) consist of two content words, and capture direct dependency relations such as “subject of”, “adverbial modifier of” and so on (see Figure 3c,3d for examples). These correspond to “dependency triplets” as used in Lin (1998) and most other work on syntax-based semantic similarity. biarcs (1.78B items) consist of three content words (either a word and its two daughters, or a childparent-grandparent chain) and capture relations such as “subject verb object”, “a noun and two adjectivial modifiers”, “verb, object and adjectivial modifier of the object” and many others. triarcs (1.87B items) consist of four content words (example in Figure 3f). The locality of the dependency representation causes this set of three-arcs structures to be large, sparse and noisy – many of the relations may appear random because some arcs</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98, pages 768– 774, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="7789" citStr="Marcus et al., 1993" startWordPosition="1227" endWordPosition="1230">e immediate use than the raw parse trees. While access to the parse trees may allow for somewhat greater flexibility in the kinds of questions one could ask, it also comes with a very hefty price tag in terms of the required computational resources: while counting seems trivial, it is, in fact, quite demanding computationally when done on such a scale, and requires a massive infrastructure. By lifting this burden of NLP researchers, we hope to free them to tackle interesting research questions. 242 2 Underlying Syntactic Representation We assume the part-of-speech tagset of the Penn Treebank (Marcus et al., 1993). The syntactic representation we work with is based on dependencygrammar. Specifically, we use labeled dependency trees following the “basic” variant of the Stanforddependencies scheme (de Marneffe and Manning, 2008b; de Marneffe and Manning, 2008a). Dependency grammar is a natural choice, as it emphasizes individual words and explicitly models the connections between them. Stanford dependencies are appealing because they model relations between content words directly, without intervening functional markers (so in a construction such as “wanted to know” there is a direct relation (wanted, kno</context>
<context position="17954" citStr="Marcus et al., 1993" startWordPosition="2899" endWordPosition="2902">a manageable, we employ a frequency threshold of 10 on the corpus-level count. Data Processing We ignored pages with over 600 white-spaces (which are indicative of OCR errors or non-textual content), as well as sentences of over 60 tokens. Table 1 details the sizes of the various corpora. After OCR, sentence splitting and tokenization, the corpus went through several stages of syntactic processing: part-of-speech tagging, syntactic parsing, and syntactic-ngrams extraction. Part-of-speech tagging was performed using a first order CRF tagger, which was trained on a union of the Penn WSJ Corpus (Marcus et al., 1993), the Brown corpus (Kucera and Francis, 1967) and the Questions Treebank (Judge et al., 2006). In addition to the diverse training material, the tagger makes use of features based on word-clusters derived from trigrams of the Books corpus. These cluster-features make the tagger more robust on the books domain. For further details regarding the tagger, see Lin et al. (2012). Syntactic parsing was performed using a reimplementation of a beam-search shift-reduce dependency parser (Zhang and Clark, 2008) with a beam of size 8 and the feature-set described in Zhang and Nivre (2011). The parser was </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Baptiste Michel</author>
<author>Yuan Kui Shen</author>
<author>Aviva Presser Aiden</author>
<author>Adrian Veres</author>
<author>Matthew K Gray</author>
</authors>
<title>The Google Books Team,</title>
<date>2011</date>
<journal>Joseph</journal>
<volume>331</volume>
<issue>6014</issue>
<contexts>
<context position="16164" citStr="Michel et al. (2011)" startWordPosition="2612" endWordPosition="2615">ms) Like the above, but only the head word and the top-1000 occurring words in the English-1M subcorpus are lexicalized – other words are replaced with a *W* symbol. By abstracting away from nonfrequent words, we include many of the larger syntactic configurations that will otherwise be pruned away by our frequency threshold. These could be useful for inspecting fine-grained syntactic subcategorization frames. 5 Corpora and Syntactic Processing The dataset is based on the English Google Books corpus. This is the same corpus used to derive the Google Books Ngrams, and is described in detail in Michel et al. (2011). The corpus consists of the text of 3,473,595 English books which were published between 1520 and 2008, with the majority of the content published after 1800. We provide counts based on the entire corpus, as well as on several subsets of it: English 1M Uniformly sampled 1 million books. Fiction Works of Fiction. American English Books published in the US. British English Books published in Britain. The sizes of the different corpora are detailed in Table 1. 244 Figure 3: Syntactic-ngram examples. Non-content words are grayed, functional markers appearing only in the extended-* collections are</context>
</contexts>
<marker>Michel, Shen, Aiden, Veres, Gray, 2011</marker>
<rawString>Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak, and Erez Lieberman Aiden. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Matthew Gormley</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Annotated gigaword.</title>
<date>2012</date>
<booktitle>In AKBCWEKEX Workshop at NAACL 2012,</booktitle>
<marker>Napoles, Gormley, Van Durme, 2012</marker>
<rawString>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated gigaword. In AKBCWEKEX Workshop at NAACL 2012, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
</authors>
<title>Attacking parsing bottlenecks with unlabeled data and relevant factorizations.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>768--776</pages>
<contexts>
<context position="2399" citStr="Pitler, 2012" startWordPosition="380" endWordPosition="381">t al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim in this work is not to present new methods or results, but rather to provide a new kind of a large-scale (based on corpora about 100 times larger than previous efforts) high-quality and standard resource for researchers to build upon. Instead of focusing on</context>
</contexts>
<marker>Pitler, 2012</marker>
<rawString>Emily Pitler. 2012. Attacking parsing bottlenecks with unlabeled data and relevant factorizations. In ACL, pages 768–776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences. In</title>
<date>2010</date>
<booktitle>ACL,</booktitle>
<pages>424--434</pages>
<contexts>
<context position="1818" citStr="Ritter et al., 2010" startWordPosition="282" endWordPosition="285"> a syntactic parse tree, and not on their sequential order in the sentence: the different words in the ngram may be far apart from each other in the sentence, yet close to each other syntactically. See Figure 1 for an example of a syntactic-ngram. The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; *Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this i</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In ACL, pages 424–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Andrew S Gordon</author>
</authors>
<title>Clustering words by syntactic similarity improves dependency parsing of predicate-argument structures.</title>
<date>2009</date>
<booktitle>In IWPT,</booktitle>
<pages>192--201</pages>
<contexts>
<context position="2166" citStr="Sagae and Gordon, 2009" startWordPosition="341" endWordPosition="344">is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; *Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim in this work is not to p</context>
</contexts>
<marker>Sagae, Gordon, 2009</marker>
<rawString>Kenji Sagae and Andrew S. Gordon. 2009. Clustering words by syntactic similarity improves dependency parsing of predicate-argument structures. In IWPT, pages 192–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>435--444</pages>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In ACL, pages 435–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph M Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="2101" citStr="Shen et al., 2008" startWordPosition="331" endWordPosition="334"> words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; *Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare r</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph M. Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In ACL, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="983" citStr="Turney and Pantel, 2010" startWordPosition="142" endWordPosition="145">t items covering a wide range of syntactic configurations. It also includes temporal information, facilitating new kinds of research into lexical semantics over time. This paper describes the dataset, the syntactic representation, and the kinds of information provided. 1 Introduction The distributional hypothesis of Harris (1954) states that properties of words can be captured based on their contexts. The consequences of this hypothesis have been leveraged to a great effect by the NLP community, resulting in algorithms for inferring syntactic as well as semantic properties of words (see e.g. (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein). In this paper, we describe a very large dataset of syntactic-ngrams, that is, structures in which the contexts of words are based on their respective position in a syntactic parse tree, and not on their sequential order in the sentence: the different words in the ngram may be far apart from each other in the sentence, yet close to each other syntactically. See Figure 1 for an example of a syntactic-ngram. The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P.D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using wikipedia.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>118--127</pages>
<contexts>
<context position="1954" citStr="Wu and Weld, 2010" startWordPosition="305" endWordPosition="308">ther in the sentence, yet close to each other syntactically. See Figure 1 for an example of a syntactic-ngram. The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; *Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textu</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using wikipedia. In ACL, pages 118–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>562--571</pages>
<contexts>
<context position="18459" citStr="Zhang and Clark, 2008" startWordPosition="2979" endWordPosition="2982">s performed using a first order CRF tagger, which was trained on a union of the Penn WSJ Corpus (Marcus et al., 1993), the Brown corpus (Kucera and Francis, 1967) and the Questions Treebank (Judge et al., 2006). In addition to the diverse training material, the tagger makes use of features based on word-clusters derived from trigrams of the Books corpus. These cluster-features make the tagger more robust on the books domain. For further details regarding the tagger, see Lin et al. (2012). Syntactic parsing was performed using a reimplementation of a beam-search shift-reduce dependency parser (Zhang and Clark, 2008) with a beam of size 8 and the feature-set described in Zhang and Nivre (2011). The parser was trained on the same training data as the tagger after 4-way jack-knifing so that the parser is trained on data with predicted part-of-speech tags. The parser provides state-of-the-art syntactic annotations for English.3 3Evaluating the quality of syntactic annotation on such a varied dataset is a challenging task on its own right – the underlying corpus includes many different genres spanning different time periods, as well as varying levels of digitization and OCR quality. It is extremely difficult </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proc. of EMNLP, pages 562–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="18537" citStr="Zhang and Nivre (2011)" startWordPosition="2994" endWordPosition="2997">e Penn WSJ Corpus (Marcus et al., 1993), the Brown corpus (Kucera and Francis, 1967) and the Questions Treebank (Judge et al., 2006). In addition to the diverse training material, the tagger makes use of features based on word-clusters derived from trigrams of the Books corpus. These cluster-features make the tagger more robust on the books domain. For further details regarding the tagger, see Lin et al. (2012). Syntactic parsing was performed using a reimplementation of a beam-search shift-reduce dependency parser (Zhang and Clark, 2008) with a beam of size 8 and the feature-set described in Zhang and Nivre (2011). The parser was trained on the same training data as the tagger after 4-way jack-knifing so that the parser is trained on data with predicted part-of-speech tags. The parser provides state-of-the-art syntactic annotations for English.3 3Evaluating the quality of syntactic annotation on such a varied dataset is a challenging task on its own right – the underlying corpus includes many different genres spanning different time periods, as well as varying levels of digitization and OCR quality. It is extremely difficult to choose a representative sample to manually annotate and evaluate on, and we</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>