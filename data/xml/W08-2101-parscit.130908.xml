<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.996767">
Semantic Parsing for High-Precision Semantic Role Labelling
</title>
<author confidence="0.939569">
Paola Merlo
</author>
<affiliation confidence="0.930671">
Linguistics Department
University of Geneva
</affiliation>
<address confidence="0.914617">
5 rue de Candolle
1211 Gen`eve 4 Switzerland
</address>
<email confidence="0.998173">
merlo@lettres.unige.ch
</email>
<author confidence="0.994922">
Gabriele Musillo
</author>
<affiliation confidence="0.9977665">
Depts of Linguistics and Computer Science
University of Geneva
</affiliation>
<address confidence="0.8485965">
5 Rue de Candolle
1211 Gen`eve 4 Switzerland
</address>
<email confidence="0.997609">
musillo4@etu.unige.ch
</email>
<sectionHeader confidence="0.993829" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998979375">
In this paper, we report experiments that
explore learning of syntactic and seman-
tic representations. First, we extend a
state-of-the-art statistical parser to pro-
duce a richly annotated tree that identi-
fies and labels nodes with semantic role la-
bels as well as syntactic labels. Secondly,
we explore rule-based and learning tech-
niques to extract predicate-argument struc-
tures from this enriched output. The learn-
ing method is competitive with previous
single-system proposals for semantic role
labelling, yields the best reported preci-
sion, and produces a rich output. In com-
bination with other high recall systems it
yields an F-measure of 81%.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975764705882">
In statistical natural language processing, consid-
erable ingenuity and insight have been devoted to
developing models of syntactic information, such
as statistical parsers and taggers. Successes in
these syntactic tasks have recently paved the way
to applying novel statistical learning techniques
to levels of semantic representation, such as re-
covering the logical form of a sentence for in-
formation extraction and question-answering ap-
plications (Miller et al., 2000; Ge and Mooney,
2005; Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007).
In this paper, we also focus our interest on learn-
ing semantic information. Differently from other
work that has focussed on logical form, however,
we explore the problem of recovering the syn-
tactic structure of the sentence, the propositional
</bodyText>
<footnote confidence="0.90283225">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999970575">
argument-structure of its main predicates, and the
substantive labels assigned to the arguments in the
propositional structure, the semantic roles. This
rich output can be useful for information extrac-
tion and question-answering, but also for anaphora
resolution and other tasks for which the structural
information provided by full syntactic parsing is
necessary.
The task of semantic role labelling (SRL), as has
been defined by previous researchers (Gildea and
Jurafsky, 2002), requires collecting all the argu-
ments that together with a verb form a predicate-
argument structure. In most previous work, the
task has been decomposed into the argument iden-
tification and argument labelling subtasks: first the
arguments of each specific verb in the sentence are
identified by classifying constituents in the sen-
tence as arguments or not arguments. The argu-
ments are then labelled in a second step.
We propose to produce the rich syntactic-
semantic output in two steps, which are different
from the argument identification and argument la-
belling subtasks. First, we generate trees that bear
both syntactic and semantic annotation, such as
those in Figure 1. The parse tree, however, does
not explicitly encode information about predicate-
argument structure, because it does not explicitly
associate each semantic role to the verb that gov-
erns it. So, our second step consists in recovering
the predicate-argument structure of each verb by
gleaning this information in an already richly dec-
orated tree.
There are linguistic and computational reasons
to think that we can solve the joint problem of
recovering the constituent structure of a sentence
and its lexical semantics. From a linguistic point
of view, the assumption that syntactic distributions
will be predictive of semantic role assignments is
based on linking theory (Levin, 1986). Linking
theory assumes the existence of a hierarchy of se-
</bodyText>
<page confidence="0.820151">
1
</page>
<note confidence="0.878844">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 1–8
Manchester, August 2008
</note>
<bodyText confidence="0.999923323529412">
mantic roles which are mapped by default on a
hierarchy of grammatical functions and syntactic
positions, and it attempts to predict the mapping
of the underlying semantic component of a predi-
cate’s meaning onto the syntactic structure. For ex-
ample, Agents are always mapped in syntactically
higher positions than Themes. From a computa-
tional point of view, if the internal semantics of a
predicate determines the syntactic expressions of
constituents bearing a semantic role, it is then rea-
sonable to expect that knowledge about semantic
roles in a sentence will be informative of its syn-
tactic structure. It follows rather naturally that se-
mantic and syntactic parsing can be integrated into
a single complex task.
Our proposal also addresses the problem of se-
mantic role labelling from a slightly different per-
spective. We identify and label argument nodes
first, while parsing, and we group them in a
predicate-argument structure in a second step. Our
experiments investigate some of the effects that re-
sult from organising the task of semantic role la-
belling in this way, and the usefulness of some
novel features defined on syntactic trees.
In the remainder of the paper, we first illustrate
the data and the graphical model that formalise the
architecture used and its extension for semantic
parsing. We then report on two kinds of exper-
iments: we first evaluate the architecture on the
joint task of syntactic and semantic parsing and
then evaluate the joint approach on the task of se-
mantic role labelling. We conclude with a discus-
sion which highlights the practical and theoretical
contribution of this work.
</bodyText>
<sectionHeader confidence="0.941587" genericHeader="introduction">
2 The Data
</sectionHeader>
<bodyText confidence="0.999842769230769">
Our experiments on joint syntactic and semantic
parsing use data that is produced automatically by
merging the Penn Treebank (PTB) with PropBank
(PRBK) (Marcus et al., 1993; Palmer et al., 2005),
as shown in Figure 1. PropBank encodes proposi-
tional information by adding a layer of argument
structure annotation to the syntactic structures of
the Penn Treebank.1 Verbal predicates in the Penn
Treebank (PTB) receive a label REL and their ar-
guments are annotated with abstract semantic role
labels, such as A0, A1, or AA for those comple-
ments of the predicative verb that are considered
arguments. Those complements of the verb la-
</bodyText>
<footnote confidence="0.990266">
1We use PRBK data as they appear in the CONLL 2005
shared task.
</footnote>
<figureCaption confidence="0.982304">
Figure 1: A sample syntactic structure with seman-
tic role labels.
</figureCaption>
<bodyText confidence="0.996001125">
belled with a semantic functional label in the orig-
inal PTB receive the composite semantic role label
AM-X, where X stands for labels such as LOC,
TMP or ADV, for locative, temporal and adverbial
modifiers respectively. A tree structure with Prop-
Bank labels is shown in Figure 1. (The bold labels
are not relevant for the moment and they will be
explained later.)
</bodyText>
<sectionHeader confidence="0.9144065" genericHeader="method">
3 The Syntactic and Semantic Parser
Architecture
</sectionHeader>
<bodyText confidence="0.99968075">
To achieve the complex task of joint syntactic and
semantic parsing, we extend a current state-of-the-
art statistical parser (Titov and Henderson, 2007)
to learn semantic role annotation as well as syntac-
tic structure. The parser uses a form of left-corner
parsing strategy to map parse trees to sequences of
derivation steps.
We choose this parser because it exhibits the
best performance for a single generative parser,
and does not impose hard independence assump-
tions. It is therefore promising for extensions
to new tasks. Following (Titov and Henderson,
2007), we describe the original parsing architec-
ture and our modifications to it as a Dynamic
Bayesian network. Our description is brief and
limited to the few aspects of interest here. For
more detail, explanations and experiments see
(Titov and Henderson, 2007). A Bayesian network
is a directed acyclic graph that illustrates the statis-
tical dependencies between the random variables
describing a set of events (Jensen, 2001). Dy-
namic networks are Bayesian networks applied to
unboundedly long sequences. They are an appro-
priate model for sequences of derivation steps in
</bodyText>
<figure confidence="0.997707888888889">
NP-A0
VP
VBD-REL
PP-TMP
PP-DIR
S
NP
TO(DIR)
NN
at
to
midnight
NP
QP
$ 2.80 trillion
IN(TMP)
dropped
the authority
</figure>
<page confidence="0.963158">
2
</page>
<figureCaption confidence="0.868066666666667">
Figure 2: The pattern on connectivity and the latent
vectors of variables in an Incremental Bayesian
Network.
parsing (Titov and Henderson, 2007).
Figure 2 illustrates visually the main properties
that are of relevance for us in this parsing architec-
</figureCaption>
<bodyText confidence="0.86244">
ture. Let T be a parse tree and D1, ... , Dm be the
sequence of parsing decisions that has led to the
building of this parse tree. Let also each parsing
decision be composed of smaller parsing decisions
d11, ... , d1k, and let all these decisions be indepen-
dent. Then,
</bodyText>
<equation confidence="0.675736">
P(T) = P(D1, ... , Dm)
= r1t P(Dt  |D1, ... , Dt−1) (1)
= Ht Hk P(dtk|h(t, k))
</equation>
<bodyText confidence="0.998072039473684">
where h(t, k) denotes the parse history for sub-
decision dtk.
The figure represents a small portion of the ob-
served sequence of decisions that constitute the re-
covery of a parse tree, indicated by the observed
states Di. Specifically, it illustrates the pattern of
connectivity for decision dtk. As can be seen the re-
lationship between different probabilistic parsing
decisions are not Markovian, nor do the decisions
influence each other directly. Past decisions can in-
fluence the current decision through state vectors
of independent latent variables, referred to as Si.
These state vectors encode the probability distri-
butions of features of the history of parsing steps
(the features are indicated by sti in Figure 2).
As can be seen from the picture, the pattern
of inter-connectivity allows previous non-adjacent
states to influence future states. Not all states
in the history are relevant, however. The inter-
connectivity is defined dynamically based on the
topological structure and the labels of the tree that
is being developed. This inter-connectivity de-
pends on a notion of structural locality (Hender-
son, 2003; Musillo and Merlo, 2006).2
2Specifically, the conditioning states are based on the
In order to extend this model to learn decisions
concerning a joint syntactic-semantic representa-
tion, the semantic information needs to be high-
lighted in the model in several ways. We modify
the network connectivity, and bias the learner.
First, we take advantage of the network’s dy-
namic connectivity to highlight the portion of the
tree that bears semantic information. We augment
the nodes that can influence parsing decisions at
the current state by explicitly adding the vectors
of latent variables related to the most recent child
bearing a semantic role label of either type (REL,
A0 to A5 or AM-X) to the connectivity of the
current decision. These additions yield a model
that is sensitive to regularities in structurally de-
fined sequences of nodes bearing semantic role la-
bels, within and across constituents. These exten-
sions enlarge the locality domain over which de-
pendencies between predicates bearing the REL
label, arguments bearing an A0-A5 label, and ad-
juncts bearing an AM-X role can be specified, and
capture both linear and hierarchical constraints be-
tween predicates, arguments and adjuncts. Enlarg-
ing the locality domain this way ensures for in-
stance that the derivation of the role DIR in Figure
1 is not independent of the derivations of the roles
TMP, REL (the predicate) and A0.
Second, this version of the Bayesian network
tags its sentences internally. Following (Musillo
and Merlo, 2005), we split some part-of-speech
tags into tags marked with semantic role labels.
The semantic role labels attached to a non-terminal
directly projected by a preterminal and belonging
to a few selected categories (DIR, EXT, LOC, MNR,
PRP, CAUS or TMP) are propagated down to the
pre-terminal part-of-speech tag of its head.3 This
third extension biases the parser to learn the rela-
tionship between lexical items, semantic roles and
the constituents in which they occur. This tech-
nique is illustrated by the bold labels in Figure 1.
We compare this augmented model to a sim-
ple baseline parser, that does not present any of
the task-specific enhancements described above,
stack configuration of the left-corner parser and the derivation
tree built so far. The nodes in the partially built tree and stack
configuration that are selected to determine the relevant states
are the following: top, the node on top of the pushdown stack
before the current derivation move; the left-corner ancestor of
top (that is, the second top-most node on the parser stack);
the leftmost child of top; and the most recent child of top, if
any.
</bodyText>
<footnote confidence="0.470147">
3Exploratory data analysis indicates that these tags are the
most useful to disambiguate parsing decisions.
</footnote>
<figure confidence="0.981359125">
t−c St−1 St
S
s�
t−c Dt−1
D
Dt
dt
k
</figure>
<page confidence="0.861432">
3
</page>
<table confidence="0.9478652">
PTB/PRBK 24
P R F
Baseline 79.6 78.6 79.1
ST 80.5 79.4 79.9
ST+ EC 81.6 80.3 81.0
</table>
<tableCaption confidence="0.581095">
Table 1: Percentage F-measure (F), recall (R), and
precision (P) of our joint syntactic and semantic
parser on merged development PTB/PRBK data
(section 24). Legend of models: ST=Split Tags;
EC=enhanced connectivity.
</tableCaption>
<bodyText confidence="0.9997935">
other than being able to use the complex syntactic-
semantic labels. Our augmented model has a to-
tal of 613 non-terminals to represent both the PTB
and PropBank labels of constituents, instead of the
33 of the original syntactic parser. The 580 newly
introduced labels consist of a standard PTB label
followed by a set of one or more PropBank seman-
tic role such as PP-AM-TMP or NP-A0-A1. As a
result of lowering the six AM-X semantic role la-
bels, 240 new part-of-speech tags were introduced
to partition the original tag set which consisted
of 45 tags. As already mentioned, argumental la-
bels A0-A5 are specific to a given verb or a given
verb sense, thus their distribution is highly vari-
able. To reduce variability, we add the tag-verb
pairs licensing these argumental labels to the vo-
cabulary of our model. We reach a total of 4970
tag-word pairs. These pairs include, among oth-
ers, all the tag-verb pairs occuring at least 10 times
in the training data. In this very limited form of
lexicalisation, all other words are considered un-
known.
</bodyText>
<sectionHeader confidence="0.982936" genericHeader="method">
4 Parsing Experiments
</sectionHeader>
<bodyText confidence="0.999492584615385">
Our extended joint syntactic and semantic parser
was trained on sections 2-21 and validated on sec-
tion 24 from the merged PTB/PropBank. To eval-
uate the joint syntactic and semantic parsing task,
we compute the standard Parseval measures of la-
belled recall and precision of constituents, taking
into account not only the original PTB labels, but
also the newly introduced PropBank labels. This
evaluation gives us an indication of how accurately
and exhaustively we can recover this richer set of
syntactic and semantic labels. The results, com-
puted on the development data set from section 24
of the PTB with added PropBank annotation, are
shown in Table 1. As the table indicates, both the
enhancements based on semantic roles yield an im-
provement on the baseline.
This task enables us to compare, albeit indi-
rectly, our integrated method to other methods
where semantic role labels are learnt separately
from syntactic structure. (Musillo and Merlo,
2006) report results of a merging technique where
the output of the semantic role annotation pro-
duced by the best semantic role labellers in the
2005 CONLL shared task is merged with the out-
put of Charniak’s parser. Results range between
between 82.7% and 83.4% F-measure. Our inte-
grated method almost reaches this level of perfor-
mance.
The performance of the parser on the syntactic
labels only (note reported in Table 1) is slightly de-
graded in comparison to the original syntax-only
architecture (Henderson, 2003), which reported
an F-measure of 89.1% since we reach 88.4% F-
measure for the best syntactic-semantic model (last
line of Table 1). This level of performance is still
comparable to other syntactic parsers often used
for extraction of semantic role features (88.2% F-
measure) (Collins, 1999).
These results indicate that the extended parser is
able to recover both syntactic and semantic labels
in a fully connected parse tree. While it is true that
the full fine-grained interpretation of the semantic
label is verb-specific, the PropBank labels (A0,A1,
etc) do respect some general trends. A0 labels are
assigned to the most agentive of the arguments,
while A1 labels tend to be assigned to arguments
bearing a Theme role, and A2, A3, A4 and A5 la-
bels are assigned to indirect object roles, while all
the AM-X labels tend to be assigned to adjuncts.
The fact that the parser learns these labels with-
out explicit annotation of the link between the ar-
guments and the predicate to which they are as-
signed, but based on the smoothed representation
of the derivation of the parse tree and only very
limited lexicalisation, appears to confirm linking
theory, which assumes a correlation between the
syntactic configuration of a sentence and the lexi-
cal semantic labels.
We need to show now that the quality of the
output produced by the joint syntactic and seman-
tic parsing is such that it can be used to perform
other tasks where semantic role information is cru-
cial. The most directly related task is semantic role
labelling (SRL) as defined in the shared task of
CoNLL 2005.
</bodyText>
<page confidence="0.995856">
4
</page>
<sectionHeader confidence="0.902241" genericHeader="method">
5 Extraction of Predicate-Argument
Structures
</sectionHeader>
<bodyText confidence="0.999975272727273">
Although there is reason to think that the good
performance reported in the previous section is
due to implicit learning of the relationship of the
syntactic representation and the semantic role as-
signments, the output produced by the parser does
not explicitly encode the predicate-argument struc-
tures. Collecting these associations is required to
solve the semantic role labelling task as usually de-
fined. We experimented with two methods: a sim-
ple rule-based method and a more complex learn-
ing method.
</bodyText>
<subsectionHeader confidence="0.985088">
5.1 The rule-based method
</subsectionHeader>
<bodyText confidence="0.999981911764706">
The rule-based extraction method is the natural
second step to solve the complete semantic role
labelling task, after we identify and label seman-
tic roles while parsing. Since in our proposal, we
solve most of the problem in the first step, then we
should be able to collect the predicate-argument
pairs by simple, deterministic rules. The simplic-
ity of the method also provides a useful compari-
son for more complex learning methods, which can
be justified only if they perform better than simple
rule-based predicate-argument extraction.
Our rule-based method automatically compiles
finite-state automatata defining the paths that con-
nect the first node dominating a predicate to its
semantic roles from parse trees enriched with se-
mantic role labels.4 Such paths can then be used to
traverse parse trees returned by the parsing model
and collect argument structures. More specifically,
a sample of sentences are randomly selected from
the training section of the PTB/PRBK. For each
predicate, then, all the arguments left and right of
the predicate and all the adjuncts left and right
respectively are collected and filtered by simple
global constraints, thereby guaranteeing that only
one type of obligatory argument label (A0 to A5)
is assigned in each proposition.
When evaluated on gold data, this rule-based ex-
traction method reaches 94.9% precision, 96.9%
recall, for an F-measure of 95.9%. These results
provide an upper bound as well as indicating that,
while not perfect, the simple extraction rules reach
a very good level of correctness if the input from
the first step, syntactic and semantic parsing, is
correct. The performance is much lower when
</bodyText>
<footnote confidence="0.9756415">
4It uses VanNoord’s finite-state-toolkit
http://www.let.rug.nl/ vannoord/Fsa/.
</footnote>
<bodyText confidence="0.9995161">
parses are not entirely correct, and semantic role
labels are missing, as indicated by the results of
72.9% precision, 66.7% (F-measure 69.7%), ob-
tained when using the best automatic parse tree.
The fact that performance depends on the qual-
ity of the output of the first step, indicates that
the extraction rules are sensitive to errors in the
parse trees, as well as errors in the labelling. This
indicates that a learning method might be more
adapted to recover from these mistakes.
</bodyText>
<subsectionHeader confidence="0.997391">
5.2 The SVM learning method
</subsectionHeader>
<bodyText confidence="0.9999684">
In a different approach to extract predicate argu-
ment structures from the parsing output, the sec-
ond step learns to associate the right verb to each
semantically annotated node (srn) in the tree pro-
duced in the first step. Each individual (verb, srn)
pair in the tree is either a positive example (the srn
is a member of the verb’s argument structure) or a
negative example (the argument either should not
have been labelled as an argument or it is not as-
sociated to the verb). The training examples are
produced by parsing section 2-21 of the merged
PTB/PRBK data with the joint syntactic-semantic
parser and producing the training examples by
comparison with the CONLL 2005 gold proposi-
tions. There are approximately 800’000 training
examples in total. These examples are used by
an SVM classifier (Joachims, 1999).5. Once the
predicate-argument structures are built, they are
evaluated with the CONLL 2005 shared task cri-
teria.
</bodyText>
<subsectionHeader confidence="0.980998">
5.3 The learning features
</subsectionHeader>
<bodyText confidence="0.9999704">
The features used for the extraction of the
predicate-argument structure reflect the syntactic
properties that are useful to identify the arguments
of a given verb. We use syntactic and semantic
node label, the path between the verb and the argu-
ment, and the part-of-speech tag of the verb, which
provides useful information about the tense of the
verb. We also use novel features that encode min-
imality conditions and locality constraints (Rizzi,
1990). Minimality is a typical property of natu-
ral languages that is attested in several domains.
In recovering predicate-argument structures, mini-
mality guarantees that the arguments are related to
the closest verb in a predicate domain, which is not
always the verb to which they are connected by the
</bodyText>
<footnote confidence="0.764352333333333">
5We use a radial basis function kernel, where parameters
c and -y were determined by a grid search on a small subset of
2000 training examples. They are set at c=8 and -y = 0.03125.
</footnote>
<page confidence="0.993858">
5
</page>
<bodyText confidence="0.999502166666666">
shortest path. For example, the subject of an em-
bedded clause can be closer to the verb of the main
clause than to the predicate to which it should be
attached. Minimality is encoded as a binary feature
that indicates whether a verb w intervenes between
the verb v and the candidate argument srn. Mini-
mality is defined both in terms of linear precedence
(indicated below as �) and of dominance within
the same VP group. A VP group is a stack of VPs
covering the same compound verb group, such as
[V P should [V P have [V P [V come ]]]]. Formal
definitions are given below:
</bodyText>
<equation confidence="0.932166375">
minimal(v, srn, w) =df
false if (v � w � srn or srn � w � v) and
VPG-dominates(v, srn, w)
true otherwise
VPG-dominates(v, srn, w) =df
true if VP E path(v, srn) and
VP E VP-group directly dominating w
false otherwise
</equation>
<bodyText confidence="0.9999738">
In addition to the minimality conditions, which
resolve ambiguity when two predicates compete to
govern an argument, we use locality constraints to
capture distinct local relationships between a verb
and the syntactic position occupied by a candidate
argument. In particular, we distinguish between in-
ternal arguments occupying a position dominated
by a VP node, external arguments occupying a
position dominated by an S node, and extracted
arguments occupying a position dominated by an
SBAR node. To approximate such structural dis-
tinctions, we introduce two binary features indicat-
ing, respectively, whether there is a a node labelled
S or SBAR on the path connecting the verb and the
candidate argument.
</bodyText>
<sectionHeader confidence="0.999151" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999764666666667">
Table 2 illustrates our results on semantic role la-
belling. Notice how much more precise the learn-
ing method is than the rule-based method, when
the minimality constraint is added. The second and
third line indicate that this contribution is mostly
due to the minimality feature. The fifth and sixth
line however illustrate that these features together
are more useful than the widely used feature path.
Recall however, suffers in the learnt method. Over-
all, the learnt method is better than a rule-based
method only if path and either minimality or lo-
cality constraints are added, thus suggesting that
</bodyText>
<table confidence="0.9999325">
Prec Rec F
Learning all features 87.4 63.6 73.7
Learning all −min 75.4 66.2 70.5
Learning all −loc 87.4 63.6 73.6
Rule-based 72.9 66.7 69.7
Learning all −path 80.6 60.9 69.4
Learning all −min −loc 74.3 63.8 68.6
Baseline 57.4 53.9 55.6
</table>
<tableCaption confidence="0.67657375">
Table 2: Results on the development section (24),
rule-based, and learning, (with all features, and
without path, minimality and locality constraints)
compared to a closest verb baseline.
</tableCaption>
<bodyText confidence="0.99997775862069">
the choice of features is crucial to reach a level
of performance that justifies the added complex-
ity of a learning method. Both methods are much
better than a baseline that attaches each role to
a verb by the shortest path.6 Notice that both
these approaches are not lexicalised, they apply to
all verbs. Learning experiments where the actual
verbs were used showed a little degradation as well
as a very considerable increase in training times
(precision: 87.0%; recall: 61.0%; F: 71.7%).7
Some comments are in order to compare prop-
erly our best results – the learning method with
all features – to other methods. Most of the best
performing SRL systems are ensemble learners or
rerankers, or they use external sources of infor-
mation such as the PropBank frames files. While
these techniques are effective to improve classifi-
cation accuracy, we might want to compare the sin-
gle systems, thus teasing apart the contribution of
the features and the model from the contribution
of the ensemble technique. Table 3 reports the sin-
gle systems’ performance on the test set. These re-
sults seem to indicate that methods like ours, based
on a first step of PropBank parsing, are compara-
ble to other methods when learning regimes are
factored out, contrary to pessimistic conclusions
in previous work (Yi and Palmer, 2005). (Yi and
Palmer, 2005) share the motivation of our work.
They observe that the distributions of semantic la-
</bodyText>
<footnote confidence="0.5095045">
6In case of tie, the following verb is chosen for an A0 label
and the preceding verb is chosen for all the other labels.
7We should notice that all these models encode the feature
path as syntactic path, because in exploratory data analysis we
found that this feature performed quite a bit better than path
encoded taking into account the semantic roles assigned to the
nodes on the path. Concerning the learning model, we notice
that a simpler, and much faster to train, linear SVM classifier
performs almost as well as the more complex RBF classifier.
It is therefore preferable if speed is important.
</footnote>
<equation confidence="0.6561665">
I
I
</equation>
<page confidence="0.992229">
6
</page>
<table confidence="0.99176825">
Model CONLL 23 Comments
P R F
(Surdeanu and Turmo, 2005) 80.3 73.0 76.5 Propbank frames to filter output, boosting
(Liu et al., 2005) 80.5 72.8 76.4 Single system + simple post-processing
(Moschitti et al., 2005) 76.6 75.2 75.9 Specialised kernels for each kind of role
This paper 87.6 65.8 75.1 Single system and model, locality features
(Ozgencil and McCracken, 2005) 74.7 74.2 74.4 Simple system, no external knowledge
(Johansson and Nugues, 2005) 75.5 73.2 74.3 Uses only 3 sections for training
</table>
<tableCaption confidence="0.866367">
Table 3: Final Semantic Role Labelling results on test section 23 of Propbank as encoded in the CONLL
shared task for those CONLL 2005 participants not using ensemble learning or external resources.
</tableCaption>
<bodyText confidence="0.998645264705882">
bels could potentially interact with the distribu-
tions of syntactic labels and redefine the bound-
aries of constituents, thus yielding trees that reflect
generalisations over both these sources of infor-
mation. They also attempt to assign SRL while
parsing, by merging only the first two steps of
the standard pipeline architecture, pruning and ar-
gument identification. Their parser outputs a bi-
nary argument-nonargument distinction. The ac-
tual fine-grained labelling is performed, as in other
methods, by an ensemble classifier. Results are
not among the best and Yi and Palmer conclude
that PropBank parsing is too difficult and suffers
from differences between chunk annotation and
tree structure. We think instead that the method is
promising, as shown by the results reported here,
once the different factors that affect performance
are teased apart.
Some qualitative observations on the errors are
useful. On the one hand, as can be noticed in Table
3, our learning method yields the best precision,
but often the worse recall and it has the most ex-
treme difference between these two scores.8 This
is very likely to be a consequence of the method.
Since the assignment of the semantic role labels
proper is performed during parsing, the number
of nodes that require a semantic role is only 20%
of the total. Therefore the parser develops a bias
against assigning these roles in general, and recall
suffers.9 On the other hand, precision is very good,
thanks to the rich context in which the roles are as-
signed.
This property of our method suggests that com-
bining our results with those of other existing se-
</bodyText>
<footnote confidence="0.845998333333333">
8This observation applies also in a comparison to the other
systems that participated in the CONLL shared task.
9The SVM classifier, on the other hand, exceeds 94% in
</footnote>
<bodyText confidence="0.980381575">
accuracy and its F measures are situated around 87–88% de-
pending on the feature sets.
mantic role labellers might be beneficial, since the
errors it performs are quite different. We tested
this hypothesis by combining our outputs, which
are the most precise, with the outputs of the sys-
tem that reported the best recall (Haghighi et al.,
2005). The combination, performed on sections
24 and 23, gives priority to our system when it
outputs a non-null label (because of its high pre-
cision) and uses the other system’s label when our
system outputs a null label. This combination pro-
duces a result of 79.0% precision, 80.4% recall,
and 79.7% F-measure for section 24, and 80.5%
precision, 81.4% recall, and 81.0% F-measure for
section 23. We conclude that the combination is in-
deed able to exploit the positive aspects of both ap-
proaches, as the F-measure of the combined result
is better than each individual result. It is also the
best compared to the other systems of the CoNLL
shared task. Comparatively, we find that applying
the same combination technique to the output of
the system by (Haghighi et al., 2005) with the out-
put of the best system in the CoNLL 2005 shared
task (Punyakanok et al., 2005) yields combined
outputs that are not as good as the better of the
two systems (P:76.3%; R:78.6%; F:77.4% for sec-
tion 24; P:78.5%; R:80.0%; F:79.3% for section
23). This result confirms our initial hypothesis,
that combination of systems with different perfor-
mance characteristics yields greater improvement.
Another direct consequence of assigning roles
in a rich context is that in collecting arguments for
a given verb we hardly need to verify global con-
straints. Differently from previous work that had
found that global coherence constraints consider-
ably improved performance (Punyakanok et al.,
2005), using global filtering contraints showed no
improvement in our learning model. Thus, these
results confirm the observations that a verb does
</bodyText>
<page confidence="0.997699">
7
</page>
<bodyText confidence="0.9997779375">
not assign its semantic roles independently of each
other (Haghighi et al., 2005). Our method too can
be seen as a way of formulating the SRL problem
in a way that is not simply classification of each in-
stance independently. Because identification of ar-
guments and their labelling is done while parsing,
the parsing history, both syntactic and semantic,
is taken into account in identifying and labelling
an argument. Semantic role labelling is integrated
in structured sequence prediction. Further integra-
tion of semantic role labelling in structured prob-
abilistic models related to the one described here
has recently been shown to result in accurate syn-
chronous parsers that derive both syntactic and se-
mantic dependency representations (Henderson et
al., 2008).
</bodyText>
<sectionHeader confidence="0.999282" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999727">
Overall our experiments indicate that an inte-
grated approach to identification and labelling fol-
lowed by predicate-argument recovery can solve
the problem of semantic role labelling at a level
of performance comparable to other approaches,
as well as yielding a richly decorated syntactic-
semantic parse tree. The high precision of our
method yields very good results in combination
with other high-recall systems. Its shortcomings
indicates that future work lies in improving recall.
</bodyText>
<sectionHeader confidence="0.998227" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997714166666667">
We thank the Swiss NSF for supporting this research under
grant number 101411-105286/1, James Henderson for shar-
ing the SSN software, and Xavier Carreras for providing the
CoNLL-2005 data. Part of this work was completed while
the second author was visiting MIT/CSAIL, hosted by Prof.
Michael Collins.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999034375">
Collins, Michael John. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Ge, Ruifang and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics. In
Procs of CONLL-05, Ann Arbor, Michigan.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics,
28(3):245–288.
Haghighi, Aria, Kristina Toutanova, and Christopher Man-
ning. 2005. A joint model for semantic role labeling. In
Procs of CoNLL-2005, pages 173–176, Ann Arbor, Michi-
gan.
Henderson, Jamie. 2003. Inducing history representations
for broad-coverage statistical parsing. In Procs of NAACL-
HLT’03, pages 103–110, Edmonton, Canada.
Henderson, James, Paola Merlo, Gabriele Musillo and Ivan
Titov. 2008. A latent variable model of synchronous pars-
ing for syntactic and semantic dependencies. In Procs of
CoNLL’08 Shared Task, Manchester, UK.
Jensen, Finn V. 2001. Bayesian networks and decision
graphs. Springer Verlag.
Joachims, Thorsten. 1999. Making large-scale svm learning
practical. In Schlkopf, B., C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector Learn-
ing. MIT Press.
Johansson, Richard and Pierre Nugues. 2005. Sparse
bayesian classification of predicate arguments. In Procs
of CoNLL-2005, pages 177–180, Ann Arbor, Michigan.
Levin, Lori. 1986. Operations on lexical form: unaccusative
rules in Germanic languages. Ph.D. thesis, Massachus-
setts Institute of Technology.
Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and Huaijun
Liu. 2005. Semantic role labeling system using maximum
entropy classifier. In Procs of CoNLL-2005, pages 189–
192, Ann Arbor, Michigan.
Marcus, Mitch, Beatrice Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English: the
Penn Treebank. Computational Linguistics, 19:313–330.
Miller, S., H. Fox, L. Ramshaw, and R. Weischedel. 2000. A
novel use of statistical parsing to extract information from
text. In Procs of NAACL 2000.
Moschitti, Alessandro, Ana-Maria Giuglea, Bonaventura
Coppola, and Roberto Basili. 2005. Hierarchical semantic
role labeling. In Procs of CoNLL-2005, pages 201–204,
Ann Arbor, Michigan.
Musillo, Gabriele and Paola Merlo. 2005. Lexical and struc-
tural biases for function parsing. In Procs of IWPT’05,
pages 83–92, Vancouver, British Columbia, October.
Musillo, Gabriele and Paola Merlo. 2006. Accurate semantic
parsing of the proposition bank. In Procs of NAACL’06,
New York, NY.
Ozgencil, Necati Ercan and Nancy McCracken. 2005. Se-
mantic role labeling using libSVM. In Procs of CoNLL-
2005, pages 205–208, Ann Arbor, Michigan, June.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31:71–105.
Punyakanok, Vasin, Peter Koomen, Dan Roth, and Wen tau
Yih. 2005. Generalized inference with multiple seman-
tic role labeling systems. In Procs of CoNLL-2005, Ann
Arbor, MI USA.
Rizzi, Luigi. 1990. Relativized minimality. MIT Press, Cam-
bridge, MA.
Surdeanu, Mihai and Jordi Turmo. 2005. Semantic role
labeling using complete syntactic analysis. In Procs of
CoNLL’05, Ann Arbor, Michigan.
Titov, Ivan and James Henderson. 2007. Constituent parsing
with Incremental Sigmoid Belief Networks. In Procs of
ACL’07, pages 632–639, Prague, Czech Republic.
Wong, Yuk Wah and Raymond Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda cal-
culus. In Procs of ACL’07, pages 960–967, Prague, Czech
Republic.
Yi, Szu-ting and Martha Palmer. 2005. The integration of
semantic parsing and semantic role labelling. In Procs of
CoNLL’05, Ann Arbor, Michigan.
Zettlemoyer, Luke and Michael Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical form.
In Procs of EMNLP-CoNLL’07, pages 678–687.
</reference>
<page confidence="0.998492">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.060373">
<title confidence="0.999339">Semantic Parsing for High-Precision Semantic Role Labelling</title>
<author confidence="0.921455">Paola</author>
<affiliation confidence="0.91125">Linguistics University of</affiliation>
<abstract confidence="0.587519333333333">5 rue de 1211 Gen`eve 4 merlo@lettres.unige.ch</abstract>
<author confidence="0.811418">Gabriele</author>
<affiliation confidence="0.996051">Depts of Linguistics and Computer University of</affiliation>
<address confidence="0.4620765">5 Rue de 1211 Gen`eve 4</address>
<email confidence="0.459922">musillo4@etu.unige.ch</email>
<abstract confidence="0.996484470588235">In this paper, we report experiments that explore learning of syntactic and semantic representations. First, we extend a state-of-the-art statistical parser to produce a richly annotated tree that identifies and labels nodes with semantic role labels as well as syntactic labels. Secondly, we explore rule-based and learning techniques to extract predicate-argument structures from this enriched output. The learning method is competitive with previous single-system proposals for semantic role labelling, yields the best reported precision, and produces a rich output. In combination with other high recall systems it yields an F-measure of 81%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="15652" citStr="Collins, 1999" startWordPosition="2519" endWordPosition="2520">put of Charniak’s parser. Results range between between 82.7% and 83.4% F-measure. Our integrated method almost reaches this level of performance. The performance of the parser on the syntactic labels only (note reported in Table 1) is slightly degraded in comparison to the original syntax-only architecture (Henderson, 2003), which reported an F-measure of 89.1% since we reach 88.4% Fmeasure for the best syntactic-semantic model (last line of Table 1). This level of performance is still comparable to other syntactic parsers often used for extraction of semantic role features (88.2% Fmeasure) (Collins, 1999). These results indicate that the extended parser is able to recover both syntactic and semantic labels in a fully connected parse tree. While it is true that the full fine-grained interpretation of the semantic label is verb-specific, the PropBank labels (A0,A1, etc) do respect some general trends. A0 labels are assigned to the most agentive of the arguments, while A1 labels tend to be assigned to arguments bearing a Theme role, and A2, A3, A4 and A5 labels are assigned to indirect object roles, while all the AM-X labels tend to be assigned to adjuncts. The fact that the parser learns these l</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, Michael John. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Procs of CONLL-05,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1492" citStr="Ge and Mooney, 2005" startWordPosition="215" endWordPosition="218">ecision, and produces a rich output. In combination with other high recall systems it yields an F-measure of 81%. 1 Introduction In statistical natural language processing, considerable ingenuity and insight have been devoted to developing models of syntactic information, such as statistical parsers and taggers. Successes in these syntactic tasks have recently paved the way to applying novel statistical learning techniques to levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (Miller et al., 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007). In this paper, we also focus our interest on learning semantic information. Differently from other work that has focussed on logical form, however, we explore the problem of recovering the syntactic structure of the sentence, the propositional © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. argument-structure of its main predicates, and the substantive labels assigned to the arguments in the propositional stru</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ge, Ruifang and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Procs of CONLL-05, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="2450" citStr="Gildea and Jurafsky, 2002" startWordPosition="349" endWordPosition="352">tive Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. argument-structure of its main predicates, and the substantive labels assigned to the arguments in the propositional structure, the semantic roles. This rich output can be useful for information extraction and question-answering, but also for anaphora resolution and other tasks for which the structural information provided by full syntactic parsing is necessary. The task of semantic role labelling (SRL), as has been defined by previous researchers (Gildea and Jurafsky, 2002), requires collecting all the arguments that together with a verb form a predicateargument structure. In most previous work, the task has been decomposed into the argument identification and argument labelling subtasks: first the arguments of each specific verb in the sentence are identified by classifying constituents in the sentence as arguments or not arguments. The arguments are then labelled in a second step. We propose to produce the rich syntacticsemantic output in two steps, which are different from the argument identification and argument labelling subtasks. First, we generate trees t</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Kristina Toutanova</author>
<author>Christopher Manning</author>
</authors>
<title>A joint model for semantic role labeling.</title>
<date>2005</date>
<booktitle>In Procs of CoNLL-2005,</booktitle>
<pages>173--176</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="29060" citStr="Haghighi et al., 2005" startWordPosition="4742" endWordPosition="4745">es are assigned. This property of our method suggests that combining our results with those of other existing se8This observation applies also in a comparison to the other systems that participated in the CONLL shared task. 9The SVM classifier, on the other hand, exceeds 94% in accuracy and its F measures are situated around 87–88% depending on the feature sets. mantic role labellers might be beneficial, since the errors it performs are quite different. We tested this hypothesis by combining our outputs, which are the most precise, with the outputs of the system that reported the best recall (Haghighi et al., 2005). The combination, performed on sections 24 and 23, gives priority to our system when it outputs a non-null label (because of its high precision) and uses the other system’s label when our system outputs a null label. This combination produces a result of 79.0% precision, 80.4% recall, and 79.7% F-measure for section 24, and 80.5% precision, 81.4% recall, and 81.0% F-measure for section 23. We conclude that the combination is indeed able to exploit the positive aspects of both approaches, as the F-measure of the combined result is better than each individual result. It is also the best compare</context>
<context position="30754" citStr="Haghighi et al., 2005" startWordPosition="5019" endWordPosition="5022">that combination of systems with different performance characteristics yields greater improvement. Another direct consequence of assigning roles in a rich context is that in collecting arguments for a given verb we hardly need to verify global constraints. Differently from previous work that had found that global coherence constraints considerably improved performance (Punyakanok et al., 2005), using global filtering contraints showed no improvement in our learning model. Thus, these results confirm the observations that a verb does 7 not assign its semantic roles independently of each other (Haghighi et al., 2005). Our method too can be seen as a way of formulating the SRL problem in a way that is not simply classification of each instance independently. Because identification of arguments and their labelling is done while parsing, the parsing history, both syntactic and semantic, is taken into account in identifying and labelling an argument. Semantic role labelling is integrated in structured sequence prediction. Further integration of semantic role labelling in structured probabilistic models related to the one described here has recently been shown to result in accurate synchronous parsers that der</context>
</contexts>
<marker>Haghighi, Toutanova, Manning, 2005</marker>
<rawString>Haghighi, Aria, Kristina Toutanova, and Christopher Manning. 2005. A joint model for semantic role labeling. In Procs of CoNLL-2005, pages 173–176, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Henderson</author>
</authors>
<title>Inducing history representations for broad-coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Procs of NAACLHLT’03,</booktitle>
<pages>103--110</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="9764" citStr="Henderson, 2003" startWordPosition="1545" endWordPosition="1547">state vectors of independent latent variables, referred to as Si. These state vectors encode the probability distributions of features of the history of parsing steps (the features are indicated by sti in Figure 2). As can be seen from the picture, the pattern of inter-connectivity allows previous non-adjacent states to influence future states. Not all states in the history are relevant, however. The interconnectivity is defined dynamically based on the topological structure and the labels of the tree that is being developed. This inter-connectivity depends on a notion of structural locality (Henderson, 2003; Musillo and Merlo, 2006).2 2Specifically, the conditioning states are based on the In order to extend this model to learn decisions concerning a joint syntactic-semantic representation, the semantic information needs to be highlighted in the model in several ways. We modify the network connectivity, and bias the learner. First, we take advantage of the network’s dynamic connectivity to highlight the portion of the tree that bears semantic information. We augment the nodes that can influence parsing decisions at the current state by explicitly adding the vectors of latent variables related to</context>
<context position="15364" citStr="Henderson, 2003" startWordPosition="2473" endWordPosition="2474">hods where semantic role labels are learnt separately from syntactic structure. (Musillo and Merlo, 2006) report results of a merging technique where the output of the semantic role annotation produced by the best semantic role labellers in the 2005 CONLL shared task is merged with the output of Charniak’s parser. Results range between between 82.7% and 83.4% F-measure. Our integrated method almost reaches this level of performance. The performance of the parser on the syntactic labels only (note reported in Table 1) is slightly degraded in comparison to the original syntax-only architecture (Henderson, 2003), which reported an F-measure of 89.1% since we reach 88.4% Fmeasure for the best syntactic-semantic model (last line of Table 1). This level of performance is still comparable to other syntactic parsers often used for extraction of semantic role features (88.2% Fmeasure) (Collins, 1999). These results indicate that the extended parser is able to recover both syntactic and semantic labels in a fully connected parse tree. While it is true that the full fine-grained interpretation of the semantic label is verb-specific, the PropBank labels (A0,A1, etc) do respect some general trends. A0 labels a</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>Henderson, Jamie. 2003. Inducing history representations for broad-coverage statistical parsing. In Procs of NAACLHLT’03, pages 103–110, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous parsing for syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Procs of CoNLL’08 Shared Task,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="31437" citStr="Henderson et al., 2008" startWordPosition="5126" endWordPosition="5129"> problem in a way that is not simply classification of each instance independently. Because identification of arguments and their labelling is done while parsing, the parsing history, both syntactic and semantic, is taken into account in identifying and labelling an argument. Semantic role labelling is integrated in structured sequence prediction. Further integration of semantic role labelling in structured probabilistic models related to the one described here has recently been shown to result in accurate synchronous parsers that derive both syntactic and semantic dependency representations (Henderson et al., 2008). 7 Conclusion Overall our experiments indicate that an integrated approach to identification and labelling followed by predicate-argument recovery can solve the problem of semantic role labelling at a level of performance comparable to other approaches, as well as yielding a richly decorated syntacticsemantic parse tree. The high precision of our method yields very good results in combination with other high-recall systems. Its shortcomings indicates that future work lies in improving recall. Acknowledgments We thank the Swiss NSF for supporting this research under grant number 101411-105286/</context>
</contexts>
<marker>Henderson, Merlo, Musillo, Titov, 2008</marker>
<rawString>Henderson, James, Paola Merlo, Gabriele Musillo and Ivan Titov. 2008. A latent variable model of synchronous parsing for syntactic and semantic dependencies. In Procs of CoNLL’08 Shared Task, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn V Jensen</author>
</authors>
<title>Bayesian networks and decision graphs.</title>
<date>2001</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="7783" citStr="Jensen, 2001" startWordPosition="1213" endWordPosition="1214">st performance for a single generative parser, and does not impose hard independence assumptions. It is therefore promising for extensions to new tasks. Following (Titov and Henderson, 2007), we describe the original parsing architecture and our modifications to it as a Dynamic Bayesian network. Our description is brief and limited to the few aspects of interest here. For more detail, explanations and experiments see (Titov and Henderson, 2007). A Bayesian network is a directed acyclic graph that illustrates the statistical dependencies between the random variables describing a set of events (Jensen, 2001). Dynamic networks are Bayesian networks applied to unboundedly long sequences. They are an appropriate model for sequences of derivation steps in NP-A0 VP VBD-REL PP-TMP PP-DIR S NP TO(DIR) NN at to midnight NP QP $ 2.80 trillion IN(TMP) dropped the authority 2 Figure 2: The pattern on connectivity and the latent vectors of variables in an Incremental Bayesian Network. parsing (Titov and Henderson, 2007). Figure 2 illustrates visually the main properties that are of relevance for us in this parsing architecture. Let T be a parse tree and D1, ... , Dm be the sequence of parsing decisions that </context>
</contexts>
<marker>Jensen, 2001</marker>
<rawString>Jensen, Finn V. 2001. Bayesian networks and decision graphs. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In Schlkopf, B., C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="20585" citStr="Joachims, 1999" startWordPosition="3324" endWordPosition="3325">ed in the first step. Each individual (verb, srn) pair in the tree is either a positive example (the srn is a member of the verb’s argument structure) or a negative example (the argument either should not have been labelled as an argument or it is not associated to the verb). The training examples are produced by parsing section 2-21 of the merged PTB/PRBK data with the joint syntactic-semantic parser and producing the training examples by comparison with the CONLL 2005 gold propositions. There are approximately 800’000 training examples in total. These examples are used by an SVM classifier (Joachims, 1999).5. Once the predicate-argument structures are built, they are evaluated with the CONLL 2005 shared task criteria. 5.3 The learning features The features used for the extraction of the predicate-argument structure reflect the syntactic properties that are useful to identify the arguments of a given verb. We use syntactic and semantic node label, the path between the verb and the argument, and the part-of-speech tag of the verb, which provides useful information about the tense of the verb. We also use novel features that encode minimality conditions and locality constraints (Rizzi, 1990). Mini</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, Thorsten. 1999. Making large-scale svm learning practical. In Schlkopf, B., C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Sparse bayesian classification of predicate arguments.</title>
<date>2005</date>
<booktitle>In Procs of CoNLL-2005,</booktitle>
<pages>177--180</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="26691" citStr="Johansson and Nugues, 2005" startWordPosition="4350" endWordPosition="4353">t a simpler, and much faster to train, linear SVM classifier performs almost as well as the more complex RBF classifier. It is therefore preferable if speed is important. I I 6 Model CONLL 23 Comments P R F (Surdeanu and Turmo, 2005) 80.3 73.0 76.5 Propbank frames to filter output, boosting (Liu et al., 2005) 80.5 72.8 76.4 Single system + simple post-processing (Moschitti et al., 2005) 76.6 75.2 75.9 Specialised kernels for each kind of role This paper 87.6 65.8 75.1 Single system and model, locality features (Ozgencil and McCracken, 2005) 74.7 74.2 74.4 Simple system, no external knowledge (Johansson and Nugues, 2005) 75.5 73.2 74.3 Uses only 3 sections for training Table 3: Final Semantic Role Labelling results on test section 23 of Propbank as encoded in the CONLL shared task for those CONLL 2005 participants not using ensemble learning or external resources. bels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. They also attempt to assign SRL while parsing, by merging only the first two steps of the standard pipeline architecture, pruning and argument</context>
</contexts>
<marker>Johansson, Nugues, 2005</marker>
<rawString>Johansson, Richard and Pierre Nugues. 2005. Sparse bayesian classification of predicate arguments. In Procs of CoNLL-2005, pages 177–180, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori Levin</author>
</authors>
<title>Operations on lexical form: unaccusative rules in Germanic languages.</title>
<date>1986</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachussetts Institute of Technology.</institution>
<contexts>
<context position="3807" citStr="Levin, 1986" startWordPosition="565" endWordPosition="566"> predicateargument structure, because it does not explicitly associate each semantic role to the verb that governs it. So, our second step consists in recovering the predicate-argument structure of each verb by gleaning this information in an already richly decorated tree. There are linguistic and computational reasons to think that we can solve the joint problem of recovering the constituent structure of a sentence and its lexical semantics. From a linguistic point of view, the assumption that syntactic distributions will be predictive of semantic role assignments is based on linking theory (Levin, 1986). Linking theory assumes the existence of a hierarchy of se1 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 1–8 Manchester, August 2008 mantic roles which are mapped by default on a hierarchy of grammatical functions and syntactic positions, and it attempts to predict the mapping of the underlying semantic component of a predicate’s meaning onto the syntactic structure. For example, Agents are always mapped in syntactically higher positions than Themes. From a computational point of view, if the internal semantics of a predicate determines the </context>
</contexts>
<marker>Levin, 1986</marker>
<rawString>Levin, Lori. 1986. Operations on lexical form: unaccusative rules in Germanic languages. Ph.D. thesis, Massachussetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Wanxiang Che</author>
<author>Sheng Li</author>
<author>Yuxuan Hu</author>
<author>Huaijun Liu</author>
</authors>
<title>Semantic role labeling system using maximum entropy classifier.</title>
<date>2005</date>
<booktitle>In Procs of CoNLL-2005,</booktitle>
<pages>189--192</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="26374" citStr="Liu et al., 2005" startWordPosition="4301" endWordPosition="4304">hould notice that all these models encode the feature path as syntactic path, because in exploratory data analysis we found that this feature performed quite a bit better than path encoded taking into account the semantic roles assigned to the nodes on the path. Concerning the learning model, we notice that a simpler, and much faster to train, linear SVM classifier performs almost as well as the more complex RBF classifier. It is therefore preferable if speed is important. I I 6 Model CONLL 23 Comments P R F (Surdeanu and Turmo, 2005) 80.3 73.0 76.5 Propbank frames to filter output, boosting (Liu et al., 2005) 80.5 72.8 76.4 Single system + simple post-processing (Moschitti et al., 2005) 76.6 75.2 75.9 Specialised kernels for each kind of role This paper 87.6 65.8 75.1 Single system and model, locality features (Ozgencil and McCracken, 2005) 74.7 74.2 74.4 Simple system, no external knowledge (Johansson and Nugues, 2005) 75.5 73.2 74.3 Uses only 3 sections for training Table 3: Final Semantic Role Labelling results on test section 23 of Propbank as encoded in the CONLL shared task for those CONLL 2005 participants not using ensemble learning or external resources. bels could potentially interact wi</context>
</contexts>
<marker>Liu, Che, Li, Hu, Liu, 2005</marker>
<rawString>Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and Huaijun Liu. 2005. Semantic role labeling system using maximum entropy classifier. In Procs of CoNLL-2005, pages 189– 192, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch Marcus</author>
<author>Beatrice Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5796" citStr="Marcus et al., 1993" startWordPosition="886" endWordPosition="889">st illustrate the data and the graphical model that formalise the architecture used and its extension for semantic parsing. We then report on two kinds of experiments: we first evaluate the architecture on the joint task of syntactic and semantic parsing and then evaluate the joint approach on the task of semantic role labelling. We conclude with a discussion which highlights the practical and theoretical contribution of this work. 2 The Data Our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the Penn Treebank (PTB) with PropBank (PRBK) (Marcus et al., 1993; Palmer et al., 2005), as shown in Figure 1. PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank.1 Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels, such as A0, A1, or AA for those complements of the predicative verb that are considered arguments. Those complements of the verb la1We use PRBK data as they appear in the CONLL 2005 shared task. Figure 1: A sample syntactic structure with semantic role labels. belled with a</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitch, Beatrice Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>H Fox</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In Procs of NAACL</booktitle>
<contexts>
<context position="1471" citStr="Miller et al., 2000" startWordPosition="211" endWordPosition="214"> the best reported precision, and produces a rich output. In combination with other high recall systems it yields an F-measure of 81%. 1 Introduction In statistical natural language processing, considerable ingenuity and insight have been devoted to developing models of syntactic information, such as statistical parsers and taggers. Successes in these syntactic tasks have recently paved the way to applying novel statistical learning techniques to levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (Miller et al., 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007). In this paper, we also focus our interest on learning semantic information. Differently from other work that has focussed on logical form, however, we explore the problem of recovering the syntactic structure of the sentence, the propositional © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. argument-structure of its main predicates, and the substantive labels assigned to the arguments in t</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>Miller, S., H. Fox, L. Ramshaw, and R. Weischedel. 2000. A novel use of statistical parsing to extract information from text. In Procs of NAACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Ana-Maria Giuglea</author>
<author>Bonaventura Coppola</author>
<author>Roberto Basili</author>
</authors>
<title>Hierarchical semantic role labeling.</title>
<date>2005</date>
<booktitle>In Procs of CoNLL-2005,</booktitle>
<pages>201--204</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="26453" citStr="Moschitti et al., 2005" startWordPosition="4313" endWordPosition="4316">ath, because in exploratory data analysis we found that this feature performed quite a bit better than path encoded taking into account the semantic roles assigned to the nodes on the path. Concerning the learning model, we notice that a simpler, and much faster to train, linear SVM classifier performs almost as well as the more complex RBF classifier. It is therefore preferable if speed is important. I I 6 Model CONLL 23 Comments P R F (Surdeanu and Turmo, 2005) 80.3 73.0 76.5 Propbank frames to filter output, boosting (Liu et al., 2005) 80.5 72.8 76.4 Single system + simple post-processing (Moschitti et al., 2005) 76.6 75.2 75.9 Specialised kernels for each kind of role This paper 87.6 65.8 75.1 Single system and model, locality features (Ozgencil and McCracken, 2005) 74.7 74.2 74.4 Simple system, no external knowledge (Johansson and Nugues, 2005) 75.5 73.2 74.3 Uses only 3 sections for training Table 3: Final Semantic Role Labelling results on test section 23 of Propbank as encoded in the CONLL shared task for those CONLL 2005 participants not using ensemble learning or external resources. bels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constit</context>
</contexts>
<marker>Moschitti, Giuglea, Coppola, Basili, 2005</marker>
<rawString>Moschitti, Alessandro, Ana-Maria Giuglea, Bonaventura Coppola, and Roberto Basili. 2005. Hierarchical semantic role labeling. In Procs of CoNLL-2005, pages 201–204, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele Musillo</author>
<author>Paola Merlo</author>
</authors>
<title>Lexical and structural biases for function parsing.</title>
<date>2005</date>
<booktitle>In Procs of IWPT’05,</booktitle>
<pages>83--92</pages>
<location>Vancouver, British Columbia,</location>
<contexts>
<context position="11267" citStr="Musillo and Merlo, 2005" startWordPosition="1786" endWordPosition="1789">n and across constituents. These extensions enlarge the locality domain over which dependencies between predicates bearing the REL label, arguments bearing an A0-A5 label, and adjuncts bearing an AM-X role can be specified, and capture both linear and hierarchical constraints between predicates, arguments and adjuncts. Enlarging the locality domain this way ensures for instance that the derivation of the role DIR in Figure 1 is not independent of the derivations of the roles TMP, REL (the predicate) and A0. Second, this version of the Bayesian network tags its sentences internally. Following (Musillo and Merlo, 2005), we split some part-of-speech tags into tags marked with semantic role labels. The semantic role labels attached to a non-terminal directly projected by a preterminal and belonging to a few selected categories (DIR, EXT, LOC, MNR, PRP, CAUS or TMP) are propagated down to the pre-terminal part-of-speech tag of its head.3 This third extension biases the parser to learn the relationship between lexical items, semantic roles and the constituents in which they occur. This technique is illustrated by the bold labels in Figure 1. We compare this augmented model to a simple baseline parser, that does</context>
</contexts>
<marker>Musillo, Merlo, 2005</marker>
<rawString>Musillo, Gabriele and Paola Merlo. 2005. Lexical and structural biases for function parsing. In Procs of IWPT’05, pages 83–92, Vancouver, British Columbia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele Musillo</author>
<author>Paola Merlo</author>
</authors>
<title>Accurate semantic parsing of the proposition bank.</title>
<date>2006</date>
<booktitle>In Procs of NAACL’06,</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="9790" citStr="Musillo and Merlo, 2006" startWordPosition="1548" endWordPosition="1551">independent latent variables, referred to as Si. These state vectors encode the probability distributions of features of the history of parsing steps (the features are indicated by sti in Figure 2). As can be seen from the picture, the pattern of inter-connectivity allows previous non-adjacent states to influence future states. Not all states in the history are relevant, however. The interconnectivity is defined dynamically based on the topological structure and the labels of the tree that is being developed. This inter-connectivity depends on a notion of structural locality (Henderson, 2003; Musillo and Merlo, 2006).2 2Specifically, the conditioning states are based on the In order to extend this model to learn decisions concerning a joint syntactic-semantic representation, the semantic information needs to be highlighted in the model in several ways. We modify the network connectivity, and bias the learner. First, we take advantage of the network’s dynamic connectivity to highlight the portion of the tree that bears semantic information. We augment the nodes that can influence parsing decisions at the current state by explicitly adding the vectors of latent variables related to the most recent child bea</context>
<context position="14853" citStr="Musillo and Merlo, 2006" startWordPosition="2387" endWordPosition="2390">labels, but also the newly introduced PropBank labels. This evaluation gives us an indication of how accurately and exhaustively we can recover this richer set of syntactic and semantic labels. The results, computed on the development data set from section 24 of the PTB with added PropBank annotation, are shown in Table 1. As the table indicates, both the enhancements based on semantic roles yield an improvement on the baseline. This task enables us to compare, albeit indirectly, our integrated method to other methods where semantic role labels are learnt separately from syntactic structure. (Musillo and Merlo, 2006) report results of a merging technique where the output of the semantic role annotation produced by the best semantic role labellers in the 2005 CONLL shared task is merged with the output of Charniak’s parser. Results range between between 82.7% and 83.4% F-measure. Our integrated method almost reaches this level of performance. The performance of the parser on the syntactic labels only (note reported in Table 1) is slightly degraded in comparison to the original syntax-only architecture (Henderson, 2003), which reported an F-measure of 89.1% since we reach 88.4% Fmeasure for the best syntact</context>
</contexts>
<marker>Musillo, Merlo, 2006</marker>
<rawString>Musillo, Gabriele and Paola Merlo. 2006. Accurate semantic parsing of the proposition bank. In Procs of NAACL’06, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Necati Ercan Ozgencil</author>
<author>Nancy McCracken</author>
</authors>
<title>Semantic role labeling using libSVM.</title>
<date>2005</date>
<booktitle>In Procs of CoNLL2005,</booktitle>
<pages>205--208</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="26610" citStr="Ozgencil and McCracken, 2005" startWordPosition="4338" endWordPosition="4341">les assigned to the nodes on the path. Concerning the learning model, we notice that a simpler, and much faster to train, linear SVM classifier performs almost as well as the more complex RBF classifier. It is therefore preferable if speed is important. I I 6 Model CONLL 23 Comments P R F (Surdeanu and Turmo, 2005) 80.3 73.0 76.5 Propbank frames to filter output, boosting (Liu et al., 2005) 80.5 72.8 76.4 Single system + simple post-processing (Moschitti et al., 2005) 76.6 75.2 75.9 Specialised kernels for each kind of role This paper 87.6 65.8 75.1 Single system and model, locality features (Ozgencil and McCracken, 2005) 74.7 74.2 74.4 Simple system, no external knowledge (Johansson and Nugues, 2005) 75.5 73.2 74.3 Uses only 3 sections for training Table 3: Final Semantic Role Labelling results on test section 23 of Propbank as encoded in the CONLL shared task for those CONLL 2005 participants not using ensemble learning or external resources. bels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. They also attempt to assign SRL while parsing, by merging onl</context>
</contexts>
<marker>Ozgencil, McCracken, 2005</marker>
<rawString>Ozgencil, Necati Ercan and Nancy McCracken. 2005. Semantic role labeling using libSVM. In Procs of CoNLL2005, pages 205–208, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--71</pages>
<contexts>
<context position="5818" citStr="Palmer et al., 2005" startWordPosition="890" endWordPosition="893">a and the graphical model that formalise the architecture used and its extension for semantic parsing. We then report on two kinds of experiments: we first evaluate the architecture on the joint task of syntactic and semantic parsing and then evaluate the joint approach on the task of semantic role labelling. We conclude with a discussion which highlights the practical and theoretical contribution of this work. 2 The Data Our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the Penn Treebank (PTB) with PropBank (PRBK) (Marcus et al., 1993; Palmer et al., 2005), as shown in Figure 1. PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank.1 Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels, such as A0, A1, or AA for those complements of the predicative verb that are considered arguments. Those complements of the verb la1We use PRBK data as they appear in the CONLL 2005 shared task. Figure 1: A sample syntactic structure with semantic role labels. belled with a semantic functional l</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Peter Koomen</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>Generalized inference with multiple semantic role labeling systems.</title>
<date>2005</date>
<booktitle>In Procs of CoNLL-2005,</booktitle>
<location>Ann Arbor, MI USA.</location>
<contexts>
<context position="29922" citStr="Punyakanok et al., 2005" startWordPosition="4892" endWordPosition="4895">duces a result of 79.0% precision, 80.4% recall, and 79.7% F-measure for section 24, and 80.5% precision, 81.4% recall, and 81.0% F-measure for section 23. We conclude that the combination is indeed able to exploit the positive aspects of both approaches, as the F-measure of the combined result is better than each individual result. It is also the best compared to the other systems of the CoNLL shared task. Comparatively, we find that applying the same combination technique to the output of the system by (Haghighi et al., 2005) with the output of the best system in the CoNLL 2005 shared task (Punyakanok et al., 2005) yields combined outputs that are not as good as the better of the two systems (P:76.3%; R:78.6%; F:77.4% for section 24; P:78.5%; R:80.0%; F:79.3% for section 23). This result confirms our initial hypothesis, that combination of systems with different performance characteristics yields greater improvement. Another direct consequence of assigning roles in a rich context is that in collecting arguments for a given verb we hardly need to verify global constraints. Differently from previous work that had found that global coherence constraints considerably improved performance (Punyakanok et al.,</context>
</contexts>
<marker>Punyakanok, Koomen, Roth, Yih, 2005</marker>
<rawString>Punyakanok, Vasin, Peter Koomen, Dan Roth, and Wen tau Yih. 2005. Generalized inference with multiple semantic role labeling systems. In Procs of CoNLL-2005, Ann Arbor, MI USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luigi Rizzi</author>
</authors>
<title>Relativized minimality.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21179" citStr="Rizzi, 1990" startWordPosition="3418" endWordPosition="3419">r (Joachims, 1999).5. Once the predicate-argument structures are built, they are evaluated with the CONLL 2005 shared task criteria. 5.3 The learning features The features used for the extraction of the predicate-argument structure reflect the syntactic properties that are useful to identify the arguments of a given verb. We use syntactic and semantic node label, the path between the verb and the argument, and the part-of-speech tag of the verb, which provides useful information about the tense of the verb. We also use novel features that encode minimality conditions and locality constraints (Rizzi, 1990). Minimality is a typical property of natural languages that is attested in several domains. In recovering predicate-argument structures, minimality guarantees that the arguments are related to the closest verb in a predicate domain, which is not always the verb to which they are connected by the 5We use a radial basis function kernel, where parameters c and -y were determined by a grid search on a small subset of 2000 training examples. They are set at c=8 and -y = 0.03125. 5 shortest path. For example, the subject of an embedded clause can be closer to the verb of the main clause than to the</context>
</contexts>
<marker>Rizzi, 1990</marker>
<rawString>Rizzi, Luigi. 1990. Relativized minimality. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
</authors>
<title>Semantic role labeling using complete syntactic analysis.</title>
<date>2005</date>
<booktitle>In Procs of CoNLL’05,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="26297" citStr="Surdeanu and Turmo, 2005" startWordPosition="4288" endWordPosition="4291">osen for an A0 label and the preceding verb is chosen for all the other labels. 7We should notice that all these models encode the feature path as syntactic path, because in exploratory data analysis we found that this feature performed quite a bit better than path encoded taking into account the semantic roles assigned to the nodes on the path. Concerning the learning model, we notice that a simpler, and much faster to train, linear SVM classifier performs almost as well as the more complex RBF classifier. It is therefore preferable if speed is important. I I 6 Model CONLL 23 Comments P R F (Surdeanu and Turmo, 2005) 80.3 73.0 76.5 Propbank frames to filter output, boosting (Liu et al., 2005) 80.5 72.8 76.4 Single system + simple post-processing (Moschitti et al., 2005) 76.6 75.2 75.9 Specialised kernels for each kind of role This paper 87.6 65.8 75.1 Single system and model, locality features (Ozgencil and McCracken, 2005) 74.7 74.2 74.4 Simple system, no external knowledge (Johansson and Nugues, 2005) 75.5 73.2 74.3 Uses only 3 sections for training Table 3: Final Semantic Role Labelling results on test section 23 of Propbank as encoded in the CONLL shared task for those CONLL 2005 participants not usin</context>
</contexts>
<marker>Surdeanu, Turmo, 2005</marker>
<rawString>Surdeanu, Mihai and Jordi Turmo. 2005. Semantic role labeling using complete syntactic analysis. In Procs of CoNLL’05, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Constituent parsing with Incremental Sigmoid Belief Networks.</title>
<date>2007</date>
<booktitle>In Procs of ACL’07,</booktitle>
<pages>632--639</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6947" citStr="Titov and Henderson, 2007" startWordPosition="1079" endWordPosition="1082">re 1: A sample syntactic structure with semantic role labels. belled with a semantic functional label in the original PTB receive the composite semantic role label AM-X, where X stands for labels such as LOC, TMP or ADV, for locative, temporal and adverbial modifiers respectively. A tree structure with PropBank labels is shown in Figure 1. (The bold labels are not relevant for the moment and they will be explained later.) 3 The Syntactic and Semantic Parser Architecture To achieve the complex task of joint syntactic and semantic parsing, we extend a current state-of-theart statistical parser (Titov and Henderson, 2007) to learn semantic role annotation as well as syntactic structure. The parser uses a form of left-corner parsing strategy to map parse trees to sequences of derivation steps. We choose this parser because it exhibits the best performance for a single generative parser, and does not impose hard independence assumptions. It is therefore promising for extensions to new tasks. Following (Titov and Henderson, 2007), we describe the original parsing architecture and our modifications to it as a Dynamic Bayesian network. Our description is brief and limited to the few aspects of interest here. For mo</context>
<context position="8191" citStr="Titov and Henderson, 2007" startWordPosition="1278" endWordPosition="1281">tions and experiments see (Titov and Henderson, 2007). A Bayesian network is a directed acyclic graph that illustrates the statistical dependencies between the random variables describing a set of events (Jensen, 2001). Dynamic networks are Bayesian networks applied to unboundedly long sequences. They are an appropriate model for sequences of derivation steps in NP-A0 VP VBD-REL PP-TMP PP-DIR S NP TO(DIR) NN at to midnight NP QP $ 2.80 trillion IN(TMP) dropped the authority 2 Figure 2: The pattern on connectivity and the latent vectors of variables in an Incremental Bayesian Network. parsing (Titov and Henderson, 2007). Figure 2 illustrates visually the main properties that are of relevance for us in this parsing architecture. Let T be a parse tree and D1, ... , Dm be the sequence of parsing decisions that has led to the building of this parse tree. Let also each parsing decision be composed of smaller parsing decisions d11, ... , d1k, and let all these decisions be independent. Then, P(T) = P(D1, ... , Dm) = r1t P(Dt |D1, ... , Dt−1) (1) = Ht Hk P(dtk|h(t, k)) where h(t, k) denotes the parse history for subdecision dtk. The figure represents a small portion of the observed sequence of decisions that consti</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Titov, Ivan and James Henderson. 2007. Constituent parsing with Incremental Sigmoid Belief Networks. In Procs of ACL’07, pages 632–639, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Procs of ACL’07,</booktitle>
<pages>960--967</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1547" citStr="Wong and Mooney, 2007" startWordPosition="223" endWordPosition="226">with other high recall systems it yields an F-measure of 81%. 1 Introduction In statistical natural language processing, considerable ingenuity and insight have been devoted to developing models of syntactic information, such as statistical parsers and taggers. Successes in these syntactic tasks have recently paved the way to applying novel statistical learning techniques to levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (Miller et al., 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007). In this paper, we also focus our interest on learning semantic information. Differently from other work that has focussed on logical form, however, we explore the problem of recovering the syntactic structure of the sentence, the propositional © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. argument-structure of its main predicates, and the substantive labels assigned to the arguments in the propositional structure, the semantic roles. This rich output can be usef</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Wong, Yuk Wah and Raymond Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Procs of ACL’07, pages 960–967, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Szu-ting Yi</author>
<author>Martha Palmer</author>
</authors>
<title>The integration of semantic parsing and semantic role labelling.</title>
<date>2005</date>
<booktitle>In Procs of CoNLL’05,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="25523" citStr="Yi and Palmer, 2005" startWordPosition="4152" endWordPosition="4155">r they use external sources of information such as the PropBank frames files. While these techniques are effective to improve classification accuracy, we might want to compare the single systems, thus teasing apart the contribution of the features and the model from the contribution of the ensemble technique. Table 3 reports the single systems’ performance on the test set. These results seem to indicate that methods like ours, based on a first step of PropBank parsing, are comparable to other methods when learning regimes are factored out, contrary to pessimistic conclusions in previous work (Yi and Palmer, 2005). (Yi and Palmer, 2005) share the motivation of our work. They observe that the distributions of semantic la6In case of tie, the following verb is chosen for an A0 label and the preceding verb is chosen for all the other labels. 7We should notice that all these models encode the feature path as syntactic path, because in exploratory data analysis we found that this feature performed quite a bit better than path encoded taking into account the semantic roles assigned to the nodes on the path. Concerning the learning model, we notice that a simpler, and much faster to train, linear SVM classifie</context>
</contexts>
<marker>Yi, Palmer, 2005</marker>
<rawString>Yi, Szu-ting and Martha Palmer. 2005. The integration of semantic parsing and semantic role labelling. In Procs of CoNLL’05, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Procs of EMNLP-CoNLL’07,</booktitle>
<pages>678--687</pages>
<contexts>
<context position="1523" citStr="Zettlemoyer and Collins, 2007" startWordPosition="219" endWordPosition="222"> a rich output. In combination with other high recall systems it yields an F-measure of 81%. 1 Introduction In statistical natural language processing, considerable ingenuity and insight have been devoted to developing models of syntactic information, such as statistical parsers and taggers. Successes in these syntactic tasks have recently paved the way to applying novel statistical learning techniques to levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (Miller et al., 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007). In this paper, we also focus our interest on learning semantic information. Differently from other work that has focussed on logical form, however, we explore the problem of recovering the syntactic structure of the sentence, the propositional © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. argument-structure of its main predicates, and the substantive labels assigned to the arguments in the propositional structure, the semantic roles. This</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Zettlemoyer, Luke and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Procs of EMNLP-CoNLL’07, pages 678–687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>