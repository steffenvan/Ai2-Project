<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.9986355">
Incremental Semantic Construction Using
Normal Form CCG Derivation
</title>
<author confidence="0.99457">
Yoshihide Kato1 and Shigeki Matsubara2
</author>
<affiliation confidence="0.9162185">
1Information &amp; Communications, Nagoya University
2Graduate School of Information Science, Nagoya University
</affiliation>
<address confidence="0.851218">
Furo-cho, Chikusa-ku, Nagoya, 464-8601 Japan
</address>
<email confidence="0.999177">
yoshihide@icts.nagoya-u.ac.jp
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9989899375">
This paper proposes a method of incremen-
tally constructing semantic representations.
Our method is based on Steedman’s Combina-
tory Categorial Grammar (CCG), which has a
transparent correspondence between the syn-
tax and semantics. In our method, a derivation
for a sentence is constructed in an incremen-
tal fashion and the corresponding semantic
representation is derived synchronously. Our
method uses normal form CCG derivation.
This is the difference between our approach
and previous ones. Previous approaches use
most left-branching derivation called incre-
mental derivation, but they cannot process co-
ordinate structures incrementally. Our method
overcomes this problem.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999885083333333">
By incremental interpretation, we mean that a sen-
tence is analyzed from left to right, and a semantic
representation is assigned to each initial fragment of
the sentence. These properties enable NLP systems
to analyze unfinished sentences. Moreover, incre-
mental interpretation is useful for incremental dia-
logue systems (Allen et al., 2001; Aist et al., 2007;
Purver et al., 2011; Peldszus and Schlangen, 2012).
Furthermore, in the field of psycholinguistics, incre-
mental interpretation has been explored as a human
sentence processing model.
This paper proposes a method of constructing a
semantic representation for each initial fragment of
a sentence in an incremental fashion. The proposed
method is based on Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000). CCG represents the
syntactic process as a derivation which is a tree
structure. Our method constructs a CCG derivation
by applying operations used in incremental phrase
structure parsing. Each intermediate data structure
constructed by the operations represents partial in-
formation of some derivation. Our method obtains a
semantic representation from the intermediate struc-
ture. Since the obtained semantic representations
conform to the CCG semantic construction, we can
expect that incremental semantic interpretation is re-
alized by applying a CCG-based semantic analysis
such as (Bos, 2008).
This paper is organized as follows: Section
2 briefly explains Combinatory Categorial Gram-
mar. Section 3 gives an overview of previous work
of CCG-based incremental parsing and discusses
its problem. Section 4 proposes our CCG-based
method of incrementally constructing semantic rep-
resentations. Section 5 reviews related work and
Section 6 concludes this paper.
</bodyText>
<sectionHeader confidence="0.994207" genericHeader="method">
2 Combinatory Categorial Grammar
</sectionHeader>
<bodyText confidence="0.99833575">
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a grammar formalism which has a
transparent correspondence between the syntax and
semantics. Syntactic information is represented us-
ing basic categories (e.g., S, NP) and complex cate-
gories. Complex categories are in the form of X/Y
or X\Y , where X and Y are categories. Intuitively,
each category in the form of X/Y means that it re-
ceives a category Y from its right and returns a cat-
egory X. In the case of the form X\Y , the direc-
tion is to left. For example, the category of a transi-
tive verb is (S\NP)/NP, which receives an object NP
</bodyText>
<page confidence="0.977146">
269
</page>
<note confidence="0.9969936">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 269–278,
Denver, Colorado, June 4–5, 2015.
Forward function application: X/Y : f Y : a ⇒&gt; X : fa
Backward function application: Y : a X\Y : f ⇒&lt; X : fa
Forward function composition: X/Y : f Y/Z : g ⇒&gt;B X/Z : Ax.f(gx)
Backward function composition: Y \Z : g X\Y : f ⇒&lt;B X\Z : Ax.f(gx)
Backward crossed substitution: Y/Z : g (X\Y )/Z : f ⇒&lt;S× X/Z : Ax.fx(gx)
Forward type-raising: X : a ⇒&gt;T T/(T\X) : Af.fa
Backward type-raising: X : a ⇒&lt;T T\(T/X) : Af.fa
Coordination: X : f CONJ : b X : g ⇒&lt;�&gt; X : A ... b(g ...)(f ...)
</note>
<figureCaption confidence="0.9998">
Figure 1: CCG rules
</figureCaption>
<figure confidence="0.9985448">
5
and&apos;(might&apos;(marry&apos;manny&apos;)anna&apos;)(meet&apos;manny&apos;anna&apos;)
&lt;
NP 5\NP
anna&apos; Xz.and&apos;(might&apos;(marry&apos;manny&apos;)z)(meet&apos;manny&apos;z)
Anna &gt;
(5\NP)/NP NP
Xyz.and&apos;(might&apos;(marry&apos;y)z)(meet&apos;yz) manny&apos;
&lt;Φ&gt; Manny
(5\NP)/NP CONJ (5\NP)/NP
meet&apos; and&apos; Xx.might&apos;(marry&apos;x)
met and &gt;B
(5\NP)/(5\NP) (5\NP)/NP
might&apos; marry&apos;
might marry
</figure>
<figureCaption confidence="0.999323">
Figure 2: An example of CCG derivation.
</figureCaption>
<bodyText confidence="0.9993290625">
from its right and returns a category SNP. The cat-
egory SNP corresponds to a verb phrase. It receives
a subject NP from its left and the result is a sentence
S. Formally, categories are combined using CCG
rules such as the ones shown in Figure 1. Each rule
means that, when the elements of the left-hand side
of the arrow are combined in this order, the result
is the right-hand side. The symbol with which the
arrow is subscripted designates its rule type. Each
element consists of a syntactic category and a se-
mantic representation which is separated by a colon.
A semantic representation is a A-term. Each com-
bination of syntactic categories has a corresponding
semantic composition of their semantic representa-
tions. Figure 2 shows an example of CCG deriva-
tion, which is taken from (Steedman, 2000).1 Here,
</bodyText>
<footnote confidence="0.961416333333333">
&apos;For simplicity, we use a symbol for a semantic repre-
sentation of a word. Note that it is allowed to use com-
plex semantic representations. For example, by assigning
Apx.O(px) (O is possibility operator.) and Apq.p∧q to “might”
and “and” respectively, we can obtain a modal logic formula
O(marry′manny′anna′) ∧ meet′manny′anna′.
</footnote>
<construct confidence="0.507083">
we write Ax1x2 · · · x,,,.M and M1M2M3 · · · M,,, to
abbreviate A-terms (Ax1.(Ax2.(· · · (Ax,,,.M) · · · )))
and ((· · · ((M1M2)M3) · · · )M,,,), respectively. In
</construct>
<bodyText confidence="0.90199725">
this example, each node has three labels: a syntac-
tic category, a semantic representation and the rule
type which is used to derive this node. For each leaf
node, a word is assigned instead of a rule type.
</bodyText>
<sectionHeader confidence="0.997227" genericHeader="method">
3 Incremental Parsing Based on CCG
</sectionHeader>
<bodyText confidence="0.999773724137931">
Incremental parsing methods based on CCG have
been proposed so far (Reitter et al., 2006; Hassan
et al., 2008; Hefny et al., 2011). By using the prop-
erty that CCG allows non-standard constituents, pre-
vious CCG-based incremental parsers assign a syn-
tactic category to each initial fragment of an input
sentence. The obtained derivations are most left-
branching ones which are called incremental deriva-
tions. Figure 3 shows two examples of incremental
derivations. In Figure 3(a), the fragment “Anna met”
is a non-phrase, but it has a syntactic category S/NP.
However, Demberg (2012) has demonstrated that
some kinds of sentences cannot have strictly left-
branching derivations. This means that previous ap-
proaches have the case where the parser cannot as-
sign any syntactic categories to an initial fragment.
This also means that such initial fragments do not
have any semantic representations.
A typical example is coordinate structure. In
CCG, a coordinate structure is derived by combin-
ing conjuncts and a conjunction using coordination
rule. This prevents the first conjunct from combin-
ing with its left constituent. As an example, let us
consider the incremental derivation shown in Fig-
ure 3(b). Here, the word “met” is the first con-
junct of “met and might marry” and cannot be com-
bined with “Anna”. If we assign the category S/NP
to initial fragment “Anna met” as shown in Figure
3(a), the word “met” cannot be treated as a con-
</bodyText>
<page confidence="0.980988">
270
</page>
<figure confidence="0.863054933333333">
(a) s (b) s
meet&apos;manny&apos;anna&apos; and&apos;(might&apos;(marry&apos;manny&apos;)anna&apos;)(meet&apos;manny&apos;anna&apos;)
&gt; &lt;
s/NP NP s/NP NP
λx.meet&apos;xanna&apos; manny&apos; λx. and&apos;(might&apos;(marry&apos;x)anna&apos;)(meet&apos;xanna&apos;) manny&apos;
&gt;B Manny &gt;B Manny
s/(s\NP) (s\NP)/NP s/(s\NP) (s\NP)/NP
λf.fanna&apos; meet&apos; λf.fanna&apos; λyz.and&apos;(might&apos;(marry&apos;y)z)(meet&apos;yz)
&gt;T met &gt;T &lt;Φ&gt;
NP NP (s\NP)/NP CONJ (s\NP)/NP
anna&apos; anna&apos; meet&apos; and&apos; λx.might&apos;(marry&apos;x)
Anna Anna met and &gt;B
(s\NP)/(s\NP) (s\NP)/NP
might&apos; marry&apos;
might marry
</figure>
<figureCaption confidence="0.999953">
Figure 3: Incremental derivations.
</figureCaption>
<bodyText confidence="0.9983022">
junct. This example demonstrates that sentences in-
cluding coordinate structures cannot be represented
by any strictly left-branching derivations. That is,
incremental derivation approaches cannot achieve a
word-by-word incremental interpretation.
</bodyText>
<sectionHeader confidence="0.958931" genericHeader="method">
4 Incremental Semantic Construction
Based on CCG
</sectionHeader>
<bodyText confidence="0.995880535714286">
This section proposes a method of constructing se-
mantic representations in an incremental fashion. To
overcome the problem described in the previous sec-
tion, our method adapts a different approach. Our
method needs not to use incremental derivations.
For each initial fragment of a sentence, our proposed
method obtains a semantic representation from the
normal form derivation. A normal form derivation
is defined as the one which uses type-raising and
function composition only if they are required.2 We
consider a derivation as a parse tree and construct
it based on incremental phrase structure parsing.
For each initial fragment of a sentence, incremen-
tal parsing can construct a partial parse tree which
connects all words in the fragment. Our method ob-
tains a semantic representation from the partial parse
tree. In the constructed partial parse tree, some parts
of the derivation are underspecified. Our method in-
troduces variables to denote underspecified parts of
the semantic representation. These variables are re-
2Several variants of normal form have been presented. For
example, see (Eisner, 1996) and (Hockenmaier and Bisk, 2010).
placed with semantic representations as soon as they
are determined. In the rest of this section, we first
describe incremental parsing which is the basis of
our method. Next, we explain how to obtain a se-
mantic representation from a partial parse tree con-
structed by incremental parsing.
</bodyText>
<subsectionHeader confidence="0.728354">
4.1 Incremental Construction of CCG
Derivation
</subsectionHeader>
<bodyText confidence="0.9999956875">
Our method considers a CCG derivation as a tree
structure. We call this parse tree. Our method
constructs a parse tree according to an incremental
parsing formalism proposed in (Kato and Matsub-
ara, 2009). This formalism extends the incremental
parsing of (Collins and Roark, 2004) by introducing
adjoining operation used in Tree Adjoining Gram-
mar (Joshi, 1985). The incremental parsing assigns
partial parse trees for any initial fragments of a sen-
tence. Adjoining operation reduces local ambiguity
caused by left-recursive structure, and improves the
parsing accuracy (Kato and Matsubara, 2009). Fur-
thermore, in the field of psycholinguistics, adjoining
operation is introduced to a human sentence process-
ing model (e.g., (Sturt and Lombardo, 2005; Mazzei
et al., 2007; Demberg et al., 2013)).
</bodyText>
<subsectionHeader confidence="0.8280705">
4.1.1 A Formal Description of Incremental
Parsing
</subsectionHeader>
<bodyText confidence="0.9998885">
This section gives a formal description of incre-
mental parsing of (Kato and Matsubara, 2009). The
</bodyText>
<page confidence="0.983164">
271
</page>
<bodyText confidence="0.997887578947368">
parsing grammar consists of three types of elements:
allowable tuples, allowable chains and auxiliary
trees. Each allowable tuple is a 3-tuple (X, Y, Z)
which means that the grammar allows a node la-
belled with Z to follow a node labelled with Y un-
der its parent labelled with X. Each allowable chain
is a sequence of labels. This corresponds to a se-
quence of labels on a path from a node to its leftmost
descendant leaf in a parse tree. Each auxiliary tree
consists of two nodes: a root and a foot. The label
of a root is the same as that of its foot.
A parse tree is constructed by applying two opera-
tions: attaching and adjoining. Attaching operation
combines a partial parse tree and an allowable chain.
The operation is defined as follows:
attaching: Let Q be a partial parse tree and c be an
allowable chain. Let q be the attachment site of
Q. attach(Q, c) is the result of attaching c to q
as the rightmost child (see Figure 4(a)).
Let X, Y and Z be the label of q, the label of the
rightmost child of q and the label of the root of c. If
a grammar does not have allowable tuple (X, Y, Z),
attach(Q, c) is not allowed by the grammar. Next,
we give the definition of adjoining operation. Ad-
joining operation inserts an auxiliary tree into a par-
tial parse tree. The operation is defined as follows:
adjoining: Let Q be a partial parse tree and a be an
auxiliary tree. Let q be the adjunction site of Q.
adjoin(Q, a) is the result of splitting Q at q and
combining the upper tree of Q with the root of
a and the lower tree of Q with the foot of a (see
Figure 4(b)). If the label of q is not the same as
that of the foot of a, adjoin(Q, a) is undefined.
Here, we give the definitions of attachment site
and adjunction site. These sites are defined in order
to construct a parse tree from left to right. We say
that a node q is complete if q satisfies the following
conditions:
</bodyText>
<listItem confidence="0.810546888888889">
• All children of q are instantiated and com-
plete.3
3In incremental phrase structure parsing, to identify whether
or not all children are instantiated, (Collins and Roark, 2004)
and (Kato and Matsubara, 2009) use a special symbol which
means end of constituent. All children of rl are instantiated if
and only if the rightmost child of rl is labelled with this special
• Adjoining operation is not applicable to q. By
the term “applicable”, we mean that the gram-
</listItem>
<bodyText confidence="0.9511982">
mar has an auxiliary tree whose foot label is
identical to that of q and adjoining operation
has not been applied to q yet.
The attachment site of Q is defined as the node q
satisfying the following conditions:
</bodyText>
<listItem confidence="0.999886">
• Not all children of q are instantiated.
• All instantiated children of q are complete.
</listItem>
<bodyText confidence="0.991116">
The adjunction site of Q is defined as the node q sat-
isfying the following conditions:
</bodyText>
<listItem confidence="0.999951">
• All children of q are instantiated and complete.
• Adjoining operation is applicable to q.
</listItem>
<bodyText confidence="0.999759857142857">
Finally, we introduce nil-adjoining operation
which changes not a partial parse tree, but node
states. When the operation is applied to a node, we
deem that adjoining operation is applied to the node.
This affects whether or not each node in the partial
parse tree is complete. The symbol nil designates the
operation.
</bodyText>
<subsectionHeader confidence="0.70447">
4.1.2 Constructing CCG Derivations
</subsectionHeader>
<bodyText confidence="0.954451454545455">
First of all, we show an example of incremental
constructing process of CCG derivations in our pro-
posed method. See Figure 5. Attaching operation
is represented as a solid arrow labelled with an al-
lowable chain. Adjoining operation is represented as
a dotted arrow labelled with an auxiliary tree. The
subscript i of a node indicates that the node is instan-
tiated at the point when i-th word wi is consumed.
The solid boxes mean that the nodes are complete.
The dotted box represents that adjoining operation
is applicable to the node. The symbol ‘*’ means that
the annotated node is introduced by adjoining oper-
ation (This node corresponds to the root of the aux-
iliary tree.). We call it adjoined node. Each node in
a partial parse tree is labelled with a syntactic cate-
gory and a rule type (or a word). No semantic repre-
sentations are assigned. This is because each partial
parse tree includes underspecified parts and it is im-
possible to determine their contents. This example
symbol. In CCG derivation, it can be identified by counting the
number of children, since the number is uniquely determined by
the rule type of 77.
</bodyText>
<page confidence="0.988438">
272
</page>
<figureCaption confidence="0.999439">
Figure 5: Incremental constructing process of CCG derivations.
</figureCaption>
<figure confidence="0.9989848">
CONJ3
and
(5\NP)/NP2
met
(b)
attach(σ,c)
σ
X
Z2
Zn
</figure>
<figureCaption confidence="0.931459">
Figure 4: Attaching operation and adjoining operation.
</figureCaption>
<figure confidence="0.999754">
5
&lt;
η
5\NP
&gt;
Y Z
Z1
X
η
a
c
Y
σ
X
(a)
Z
Z1
Z2
Zn
adjoin (σ,a)
X
X
X
X
(5\NP)/NP2
met
51
&lt;
(5\NP)/NP NP, 5\NP2 NP,
:word Anna &gt; Anna
(5\NP)/NP3*
&lt;Φ&gt;
NP
Anna
NP,
Anna
51
&lt;
(5\NP)/NP
met
NP,
Anna
(5\NP)/NP2
met
51
&lt;
5\NP2
&gt;
51 NP
nil &lt; Manny
NP,
Anna
(5\NP)/NP2
met
5\NP2
&gt;
NP,
Anna
(5\NP)/NP2
met
51
&lt;
5\NP2
&gt;
NP,
Manny
(5\NP)/NP
&lt;Φ&gt;
CONJ
and
51
&lt;
5\NP2
&gt;
(5\NP)/NP3*
&lt;Φ&gt;
</figure>
<bodyText confidence="0.976856217391304">
demonstrates that each initial fragment has a partial
parse tree, which connects all the words in the frag-
ment.
Next, we consider the parsing grammar for CCG
derivation. We do not need any allowable tuples,
since the CCG rules determine the syntactic cate-
gory of the node which follows a node. For example,
when a parent node is labelled with category S and
rule type &lt;, and its leftmost child is labelled with
category NP, the following node must be labelled
with S\NP. The rule type is arbitrary. Of course, we
can also define allowable tuples to restrict the rule
type.
Each node of the allowable chains and the aux-
iliary trees is also labelled with a category and a
rule type as shown in Figure 5. When an auxil-
iary tree a is adjoined to a partial parse tree at a
node q, the label of q must be the same as that of
the foot of a. That is, cat(η) = cat(foot(a)) and
rule(η) = rule(foot(a)) hold. Here, we write
cat(η) and rule(η) for the category and the rule type
of a node 77, respectively. foot(a) is the foot node of
an auxiliary tree a.
</bodyText>
<subsectionHeader confidence="0.973444">
4.2 Incremental Semantic Construction
</subsectionHeader>
<bodyText confidence="0.999763833333333">
This section presents our incremental semantic con-
struction procedure. For each initial fragment, our
method derives a semantic representation from the
partial parse tree obtained by the incremental con-
structing process. The semantic representation is
composed as follows:
</bodyText>
<listItem confidence="0.550612">
• Construct a function ti which adds the informa-
tion about the word wi to the semantic repre-
sentation si−1 for w1 · · · w,1. The function is
</listItem>
<page confidence="0.997203">
273
</page>
<bodyText confidence="0.856653">
obtained from the nodes which are instantiated By applying semantic transition functions, our
at the point when the word wi is consumed. method realizes incremental semantic construction.
• Apply the function ti to the semantic represen- All semantic representations for initial fragments are
tation si_1. That is, the semantic representation in the form of Axα′.M′ where xα′ is a sequence of
for w1 · · · wi is si = ti(si_1). variables designating underspecified parts in a se-
We call the function ti semantic transition function mantic representation M′ (x is the first variable.).
(or transition function for short). The key point is By applying semantic transition function Asα.sM,
how to construct the semantic transition function for we obtain the following semantic representation:
a word. In the following, we explain it. (Asα.sM)(Axα′.M′) ↠,3 Aαα′.M′[x := M]
To construct a semantic transition function ti, our The result is in the same form. The underspecified
method assigns a pair (α, M) to each node q E part designated by the variable x is replaced with M
Ni(a) where Ni(a) is the set of the nodes in a partial which is specified by the word wi.
</bodyText>
<figureCaption confidence="0.749325941176471">
parse tree a which are instantiated at the point when As an example of our incremental semantic con-
i-th word wi is consumed. Here, α is a sequence of struction, let us consider a sentence “Anna met
variables and M is a semantic representation. The Manny.” Figure 6 shows examples of semantic tran-
variables in α occur in M and represent underspeci- sition functions. The initial semantic representa-
fied parts of the semantic representation M. The se- tion is the identity function Ax.x. For the word
mantic representation M conveys information about “Anna”, the transition function shown in Figure 6(a)
the word wi. The variables are expected to be spec- is constructed. By applying this function to the ini-
ified in the order of α. A transition function is ob- tial semantic representation, we obtain the follow-
tained from a pair. ing semantic representation for the initial fragment
4.2.1 Semantic Construction without “Anna”:
Adjoining Operation (Asy.s(yanna′))(Ax.x) ↠,a Ay.yanna′ (1)
For ease of explanation, we first describe the con- Next, by applying the semantic transition function
struction of transition function in the case where ad- for “met” which is shown in Figure 6(b) to the se-
joining operation is not used. Below, arity(R) is mantic representation (1), the following one is ob-
the number of the elements of the left-hand side of tained for the initial fragment “Anna met”:
rule R. CR[M1, ... , Mn] is the result of combining (Asy.s(meet′y))(Ay.yanna′) ↠,a Ay.meet′yanna′
semantic representations M1, ..., Mn using rule R (2)
</figureCaption>
<bodyText confidence="0.919876466666667">
where n must be equal to arity(R). The procedure This semantic representation captures the predicate-
of constructing a transition function is as follows: argument relation between anna′ and meet′. Fi-
1. For the leaf node q E Ni(a), if cat(q) : M is a nally, by applying the semantic transition function
lexical entry for wi, assign (E, M) to q. As.smanny′ to the semantic representation (2), we
2. Let q be an inner node in Ni(a). Let (α, M) can obtain the following one:
be the pair assigned to the child of q. As- meet′manny′anna′ (3)
sign (αx2 ··· xn, Crule(77)[M, x2, ... , xn]) to This semantic representation is the same as that of
q, where n = arity(rule(q)) and x2,...,xn are the normal form derivation.
fresh variables. 4.2.2 Semantic Construction Using Adjoining
3. Let (α, M) be the pair assigned to the highest Operation
node in Ni(a). The semantic transition func- In this section, we extend the transition function
tion ti is defined as follows: construction procedure to allow adjoining operation.
Asα.sM For q E Ni(a) which is a node of an allowable
where s is a fresh variable. chain, we modify steps 1 and 2 in the transition func-
274 tion construction procedure as follows:
</bodyText>
<figure confidence="0.991612708333333">
(c)
λszy.s(zmeet&apos;y)
(s\NP) &lt;zy, zmeet&apos;y&gt;
&gt;
(s\NP)/NP &lt;z, zmeet&apos;&gt;
met
(a)
λsy.s(yanna&apos;)
s &lt;y, yanna&apos;&gt;
&lt;
NP &lt;ε, anna&apos;&gt;
Anna
(b)
λsy.s(meet&apos;y)
s\NP &lt;y, meet&apos;y&gt;
&gt;
(s\NP)/NP &lt;ε, meet&apos;&gt;
met
(d)
λsy.s(λxab.and&apos;(yab)(xab))
(s\NP)/NP* &lt;y, λxab.and&apos;(yab)(xab)&gt;
&lt;Φ&gt;
CONJ &lt;ε,and&apos;&gt;
and
</figure>
<figureCaption confidence="0.99968">
Figure 6: Examples of semantic transition function construction.
</figureCaption>
<listItem confidence="0.971094">
• Let (a, M) be the pair assigned to q in the
version without adjoining operation. If adjoin-
ing operation is applicable to q, assign the pair
(az, zM) to q instead of (a, M) where z is a
fresh variable.
</listItem>
<bodyText confidence="0.999472666666667">
The variable z is utilized for updating a semantic
representation when adjoining operation is applied
to q. When nil-adjoining operation is applied to q,
the variable z is replaced with the identity function
Ax.x. That is, after applying As.s(Ax.x) to the se-
mantic representation si−1, the semantic transition
function ti is applied.
For an adjoined node q E Ni(a), the modified
procedure assigns a pair to q in the following way:
</bodyText>
<listItem confidence="0.987318">
• Let (a, M) be the pair assigned to the root node
of the allowable chain which is attached under
q. Let R be rule(q) and n be arity(R). If
adjoining operation is applicable to q, assign
the following pair to q:
</listItem>
<equation confidence="0.549088">
(ay3 ... ynz, Ax.zCR[x, M, y3, ... yn])
</equation>
<bodyText confidence="0.998457692307692">
Otherwise, assign the following pair to q:
(ay3 ... yn, Ax.CR[x, M, y3, ... yn])
Here, x, y3,...,yn and z are fresh variables.
The pair assignment for a node to which adjoining
operation is applicable and the one for an adjoined
node work cooperatively (see Figure 7). If adjoin-
ing operation is applicable to a node, a fresh vari-
able z is introduced to the semantic representation.
When adjoining operation is applied to the node, this
variable is replaced with a function in the form of
Ax.CR[x, M2, ...] which receives a semantic repre-
sentation of the first child and returns the result of se-
mantic composition. Figure 6(c) shows an example
</bodyText>
<tableCaption confidence="0.9789885">
Table 1: Incremental semantic construction of “Anna met
and might marry Manny.”
</tableCaption>
<figure confidence="0.939159428571429">
semantic representation
Ay.yanna′
Azy.zmeet′yanna′
Ayx.and′(yxanna′)(meet′xanna′)
Ayx.and′(might′(yx)anna′)(meet′xanna′)
Ax.and′(might′(marry′x)anna′)(meet′xanna′)
and′(might′(marry′manny′)anna′)(meet′manny′anna′)
</figure>
<bodyText confidence="0.983702482758621">
of constructing the transition function where adjoin-
ing operation is applicable to the node (SNP)/NP.
Figure 6(d) shows an example of constructing the
transition function where the node (SNP)/NP is an
adjoined node.
The transition function is applied in the same way
as the version without adjoining operation. Table 1
shows an example of the semantic representations
constructed by our method.
As an example, let us consider the initial fragment
“Anna met...” By applying the transition function
shown in Figure 6(c) to the semantic representation
(1), we obtain the semantic representation #2 shown
in Table 1.
In the case where the next word is “Manny”,
nil-adjoining operation is applied to the node
(SNP)/NP, that is, the function As.s(Ax.x) is ap-
plied to #2. The result is identical to the semantic
representation (2), therefore, we obtain the semantic
representation (3) for “Anna met Manny”.
Next, let us consider the case where the word
“and” follows the initial fragment “Anna met.” In
this case, the derivation is constructed as shown in
the lower side of Figure 5. The semantic transi-
tion function for the word “and” is constructed as
shown in Figure 6(d). By applying the function to
the semantic representation #2, we obtain the se-
mantic representation #3. Furthermore, if the word
sequence “might marry Manny” follows this initial
</bodyText>
<figure confidence="0.994510142857143">
word
Anna
met
and
might
marry
Manny
#
1
2
3
4
5
6
</figure>
<page confidence="0.83066">
275
</page>
<figureCaption confidence="0.986553">
Figure 7: Updating a semantic representation by adjoining operation.
</figureCaption>
<figure confidence="0.935661941176471">
λαα&apos;...(CR[M1, M2,...])...
CR[M1, M2,...]
M1 M2 ...
(λsα.s(λx.CR[x, M2,...]))(λzα&apos;...(zM1)...)
→β λα.(λzα&apos;...(zM1)...)(λx.CR[x, M2,...])
→β λαα&apos;...((λx.CR[x, M2,...])M1)...
→β λαα&apos;...(CR[M1, M2,...])...
λzα&apos;...(zM1)...
λsα.s(λx.CR[x, M2,...])
zM1
λs.s(λx.x)
λα&apos;...M1...
M1
(λs.s(λx.x))(λzα&apos;...(zM1)...)
→β (λzα&apos;...(zM1)...)(λx.x)
→β λα&apos;...((λx.x)M1)...
→β λα&apos;...(M1)...
</figure>
<tableCaption confidence="0.944927">
Table 2: Semantic representations assigned by incremen-
tal derivations.
</tableCaption>
<equation confidence="0.886048142857143">
semantic representation
anna′
−
−
−
Ax.and′(might′(marry′x)anna′)(meet′xanna′)
and′(might′(marry′manny′)anna′)(meet′manny′anna′)
</equation>
<bodyText confidence="0.999956">
fragment, the semantic representations #4, #5 and
#6 are obtained in this order. This example demon-
strates that our method can incrementally construct
semantic representations for sentences including co-
ordinate structures. In comparison with our incre-
mental semantic construction, incremental deriva-
tion approaches have the case where no semantic
representations are assigned to initial fragments. Ta-
ble 2 shows semantic representations which are as-
signed using incremental derivations. There exist
initial fragments which have no semantic represen-
tations as discussed in Section 3.4
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.969368974358974">
Our incremental semantic construction is based on
the A-calculus. There have been several meth-
ods of incremental semantic construction using the
A-calculus. Pulman (1985) has developed an in-
cremental parser which uses context-free rules an-
4The initial fragment “Anna met” can have the semantic rep-
resentation Ax.meet′xanna′ as shown in Figure 3(a). However,
the derivation which has this semantic representation is not a
partial structure of incremental derivation shown in Figure 3(b).
That is, the derivation is not consistent with that of “Anna met
and might marry Manny.”
notated with semantic representations. The pars-
ing process proceeds on a word-by-word basis, but
its intermediate structure is a stack, that is, the
parser does not assign a fully-connected seman-
tic representation to each initial fragment. Mil-
ward (1995) has proposed an incremental semantic
construction method based on Categorial Grammar.
The method uses two types of transition functions:
state-application and state-prediction. Our seman-
tic transition function is similar to these functions.
However, our method is more general than that of
Milward. Milward’s method cannot produce CCG
derivations, since it can deal with only function ap-
plication.
There are other approaches to incremental se-
mantic construction, which use different formalism.
Purver et al. (2011) have developed a dialogue sys-
tem based on Dynamic Syntax (DS) (Kempson et al.,
2001), which provides an incremental framework
of constructing semantic representations. Peldszus
and Schlangen (2012) have proposed incremental
semantic construction based on Robust Minimal Re-
cursion Semantics (RMRS) (Copestake, 2007). Say-
eed and Demberg (2012) have proposed incremental
semantic construction for PLTAG (Demberg et al.,
2013). It is unclear how to construct a wide cov-
erage grammar (with semantic annotation) in these
frameworks.5 On the other hand, our method can use
</bodyText>
<footnote confidence="0.973222333333333">
5DS grammar induction method (Eshghi et al., 2013) was
only applied to a small artificial corpus (200 sentences, max
sentence length is 6.). Peldszus and Schlangen (2012) manu-
ally assigned semantic annotations to a small set of context-free
rules (30 rules). Sayeed and Demberg (2012) only provided
small examples.
</footnote>
<figure confidence="0.991458571428572">
word
Anna
met
and
might
marry
Manny
</figure>
<page confidence="0.989955">
276
</page>
<bodyText confidence="0.984589954545455">
CCG-based lexicon (e.g., (Bos, 2009)) directly. Al- of the 42nd Meeting of the Association for Computa-
though our method requires a set of allowable chains tional Linguistics (ACL’04), Main Volume, pages 111–
and auxiliary trees in addition to such a lexicon, we 118, Barcelona, Spain, July.
can easily extract it from CCGbank (Hockenmaier Ann Copestake. 2007. Semantic composition with (ro-
and Steedman, 2007) by using the method proposed bust) minimal recursion semantics. In Proceedings of
in (Kato and Matsubara, 2009). the ACL 2007 Workshop on Deep Linguistic Process-
6 Conclusion ing, pages 73–80, Prague, Czech Republic, June.
This paper proposed a CCG-based method of incre- Vera Demberg, Frank Keller, and Alexander Koller.
mentally constructing semantic representations. Our 2013. Incremental, predictive parsing with psycholin-
approach is based on normal form derivations unlike guistically motivated tree-adjoining grammar. Com-
previous ones. In this paper, we focused on the for- putational Linguistics, 39(4):1025–1066.
mal aspect of our method. We defined semantic tran- Vera Demberg. 2012. Incremental derivations in CCG.
sition function to obtain semantic representations for In Proceedings of the 11th International Workshop
each initial fragment of an input sentence. on Tree Adjoining Grammars and Related Formalisms
Another important issue is how to interpret in- (TAG+ 11), pages 198–206.
termediate semantic representations for initial frag- Jason Eisner. 1996. Efficient normal-form parsing for
ments. To our knowledge, there is little work to this combinatory categorial grammar. In Proceedings of
direction. In future work, we will explore a model- the 34th Annual Meeting of the Association for Com-
theoretic approach to this problem. putational Linguistics, pages 79–86, Santa Cruz, Cali-
Acknowledgements fornia, USA, June.
This research was partially supported by the Grant- Arash Eshghi, Matthew Purver, and Julian Hough. 2013.
in-Aid for Scientific Research (B) (No.26280082) of Probabilistic induction for an incremental semantic
</bodyText>
<reference confidence="0.958653888888889">
JSPS. grammar. In Proceedings of the 10th International
References Conference on Computational Semantics, pages 107–
Gregory Aist, James Allen, Ellen Campana, Carlos G. 118.
Gallo, Scott Stoness, Mary Swift, and Michael K. Hany Hassan, Khalil Sima’an, and Andy Way. 2008. A
Tanenhaus. 2007. Incremental understanding in syntactic language model based on incremental CCG
human-computer dialogue and experimental evidence parsing. In Spoken Language Technology Workshop,
for advantages over nonincremental methods. In Ron 2008. SLT 2008. IEEE, pages 205–208.
Artstein and Laure View, editors, Proceedings of the Ahmed Hefny, Hany Hassan, and Mohamed Bahgat.
11th Workshop on the Semantics and Pragmatics of 2011. Incremental combinatory categorial grammar
Dialogue, pages 149–154, Trento, Italy, June. and its derivations. In Computational Linguistics and
James Allen, George Ferguson, and Amanda Stent. 2001. Intelligent Text Processing, pages 96–108. Springer.
An architecture for more realistic conversational sys- Julia Hockenmaier and Yonatan Bisk. 2010. Normal-
tems. In Proceedings of International Conference of form parsing for combinatory categorial grammars
Intelligent User Interfaces, pages 1–8, Santa Fe, New with generalized composition and type-raising. In
Mexico, USA, January. Proceedings of the 23rd International Conference on
Johan Bos. 2008. Wide-coverage semantic analysis with Computational Linguistics (Coling 2010), pages 465–
Boxer. In Semantics in Text Processing. STEP 2008 473, Beijing, China, August.
Conference Proceedings, pages 277–286. Julia Hockenmaier and Mark Steedman. 2007. CCG-
Johan Bos. 2009. Towards a large-scale formal semantic bank: A corpus of CCG derivations and dependency
lexicon for text processing. In Proceedings of the Bi- structures extracted from the Penn Treebank. Compu-
ennal GSCL Conference From Form to Meaning: Pro- tational Linguistics, 33(3):355–396.
cessing Texts Automatically, pages 3–14. Aravind K. Joshi. 1985. Tree adjoining grammars: How
Michael Collins and Brian Roark. 2004. Incremental much context sensitivity is required to provide a rea-
parsing with the perceptron algorithm. In Proceedings sonable structural description? In David R. Dowty,
277 Lauri Karttunen, and Arnold M. Zwicky, editors, Nat-
ural Language Parsing, pages 206–250. Cambridge
University Press.
Yoshihide Kato and Shigeki Matsubara. 2009. In-
cremental parsing with adjoining operation. IE-
ICE Transactions on Information and Systems, E92-
D(12):2306–2312.
Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: the Flow of Language Under-
standing. Blackwell.
Alessandro Mazzei, Vincenzo Lombardo, and Patrick
Sturt. 2007. Dynamic TAG and lexical dependencies.
Research on Language and Computation, 5(3):309–
332, September.
David Milward. 1995. Incremental interpretation of cat-
egorial grammar. In Proceedings of the Seventh Con-
ference on European Chapter of the Association for
Computational Linguistics, pages 119–126.
Andreas Peldszus and David Schlangen. 2012. Incre-
mental construction of robust but deep semantic rep-
resentations for use in responsive dialogue systems.
In Proceedings of the Workshop on Advances in Dis-
course Analysis and its Computational Aspects, pages
56–76.
Stephen G. Pulman. 1985. A parser that doesn’t. In
Proceedings of the Second Conference on European
Chapter of the Association for Computational Linguis-
tics, pages 128–135.
Matthew Purver, Arash Eshghi, and Julian Hough. 2011.
Incremental semantic construction in a dialogue sys-
tem. In Proceedings of the Ninth International Con-
ference on Computational Semantics, pages 365–369.
Association for Computational Linguistics.
David Reitter, Julia Hockenmaier, and Frank Keller.
2006. Priming effects in combinatory categorial gram-
mar. In Proceedings of the 2006 Conference on Empir-
ical Methods in Natural Language Processing, pages
308–316.
Asad Sayeed and Vera Demberg. 2012. Incremental
neo-davidsonian semantic construction for TAG. In
Proceedings of 11th International Workshop on Tree-
Adjoining Grammars and Related Formalisms, pages
64–72.
Mark Steedman. 2000. The Syntactic Process. The MIT
press.
Patrick Sturt and Vincenzo Lombardo. 2005. Processing
coordinated structures: Incrementality and connected-
ness. Cognitive Science, 2(29):291–305.
</reference>
<page confidence="0.996937">
278
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.579760">
<title confidence="0.830129">Incremental Semantic Construction Normal Form CCG Derivation</title>
<affiliation confidence="0.971436">amp; Communications, Nagoya School of Information Science, Nagoya</affiliation>
<address confidence="0.997551">Furo-cho, Chikusa-ku, Nagoya, 464-8601</address>
<email confidence="0.98204">yoshihide@icts.nagoya-u.ac.jp</email>
<abstract confidence="0.998833117647059">This paper proposes a method of incrementally constructing semantic representations. Our method is based on Steedman’s Combinatory Categorial Grammar (CCG), which has a transparent correspondence between the syntax and semantics. In our method, a derivation for a sentence is constructed in an incremental fashion and the corresponding semantic representation is derived synchronously. Our method uses normal form CCG derivation. This is the difference between our approach and previous ones. Previous approaches use most left-branching derivation called incremental derivation, but they cannot process coordinate structures incrementally. Our method overcomes this problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>