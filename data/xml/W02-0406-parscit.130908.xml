<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002656">
<note confidence="0.991748">
Proceedings of the Workshop on Automatic Summarization (including DUC 2002),
Philadelphia, July 2002, pp. 45-51. Association for Computational Linguistics.
</note>
<title confidence="0.793792">
Manual and Automatic Evaluation of Summaries
</title>
<author confidence="0.721445">
Chin-Yew Lin and Eduard Hovy
</author>
<affiliation confidence="0.714449">
USC Information Sciences Institute
</affiliation>
<address confidence="0.852995666666667">
4676 Admiralty Way
Marina del Rey, CA 90292
+1-310-448-8711/8731
</address>
<email confidence="0.997828">
{cyl,hovy}@isi.edu
</email>
<sectionHeader confidence="0.993882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999546210526316">
In this paper we discuss manual and
automatic evaluations of summaries using
data from the Document Understanding
Conference 2001 (DUC-2001). We first
show the instability of the manual
evaluation. Specifically, the low inter-
human agreement indicates that more
reference summaries are needed. To
investigate the feasibility of automated
summary evaluation based on the recent
BLEU method from machine translation, we
use accumulative n-gram overlap scores
between system and human summaries. The
initial results provide encouraging
correlations with human judgments, based
on the Spearman rank-order correlation
coefficient. However, relative ranking of
systems needs to take into account the
instability.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999734393939394">
Previous efforts in large-scale evaluation of text
summarization include TIPSTER SUMMAC
(Mani et al. 1998) and the Document
Understanding Conference (DUC) sponsored by
the National Institute of Standards and
Technology (NIST). DUC aims to compile
standard training and test collections that can be
shared among researchers and to provide
common and large scale evaluations in single
and multiple document summarization for their
participants.
In this paper we discuss manual and automatic
evaluations of summaries using data from the
Document Understanding Conference 2001
(DUC-2001). Section 2 gives a brief overview
of the evaluation procedure used in DUC-2001
and the Summary Evaluation Environment
(SEE) interface used to support the DUC-2001
human evaluation protocol. Section 3 discusses
evaluation metrics. Section 4 shows the
instability of manual evaluations. Section 5
outlines a method of automatic summary
evaluation using accumulative n-gram matching
score (NAMS) and proposes a view that casts
summary evaluation as a decision making
process. It shows that the NAMS method is
bounded and in most cases not usable, given
only a single reference summary to compare
with. Section 6 discusses why this is so,
illustrating various forms of mismatching
between human and system summaries. We
conclude with lessons learned and future
directions.
</bodyText>
<sectionHeader confidence="0.9914105" genericHeader="method">
2 Document Understanding
Conference (DUC)
</sectionHeader>
<bodyText confidence="0.685931">
DUC2001 included three tasks:
</bodyText>
<listItem confidence="0.985017090909091">
• Fully automatic single-document
summarization: given a document,
participants were required to create a
generic 100-word summary. The training
set comprised 30 sets of approximately 10
documents each, together with their 100-
word human written summaries. The test
set comprised 30 unseen documents.
• Fully automatic multi-document
summarization: given a set of documents
about a single subject, participants were
required to create 4 generic summaries of
the entire set, containing 50, 100, 200, and
400 words respectively. The document sets
were of four types: a single natural disaster
event; a single event; multiple instances of a
type of event; and information about an
individual. The training set comprised 30
sets of approximately 10 documents, each
provided with their 50, 100, 200, and 400-
word human written summaries. The test
set comprised 30 unseen sets.
</listItem>
<figureCaption confidence="0.992135">
Figure 1. SEE in an evaluation session.
</figureCaption>
<listItem confidence="0.98690575">
• Exploratory summarization: participants
were encouraged to investigate alternative
approaches to evaluating summarization and
report their results.
</listItem>
<bodyText confidence="0.999766571428571">
A total of 11 systems participated in the single-
document summarization task and 12 systems
participated in the multi-document task.
The training data were distributed in early
March of 2001 and the test data were distributed
in mid-June of 2001. Results were submitted to
NIST for evaluation by July 1st 2001.
</bodyText>
<subsectionHeader confidence="0.991225">
2.1 Evaluation Materials
</subsectionHeader>
<bodyText confidence="0.9998915625">
For each document or document set, one human
summary was created as the ‘ideal’ model
summary at each specified length. Two other
human summaries were also created at each
length. In addition, baseline summaries were
created automatically for each length as
reference points. For the multi-document
summarization task, one baseline, lead baseline,
took the first 50, 100, 200, and 400 words in the
last document in the collection. A second
baseline, coverage baseline, took the first
sentence in the first document, the first sentence
in the second document and so on until it had a
summary of 50, 100, 200, or 400 words. Only
one baseline (baseline1) was created for the
single document summarization task.
</bodyText>
<subsectionHeader confidence="0.998617">
2.2 Summary Evaluation Environment
</subsectionHeader>
<bodyText confidence="0.999973137931034">
NIST assessors who created the ‘ideal’ written
summaries did pairwise comparisons of their
summaries to the system-generated summaries,
other assessors’ summaries, and baseline
summaries. They used the Summary Evaluation
Environment (SEE) 2.0 developed by one of the
authors (Lin 2001) to support the process.
Using SEE, the assessors compared the system’s
text (the peer text) to the ideal (the model text).
As shown in Figure 1, each text was
decomposed into a list of units and displayed in
separate windows. In DUC-2001 the sentence
was used as the smallest unit of evaluation.
SEE 2.0 provides interfaces for assessors to
judge both the content and the quality of
summaries. To measure content, assessors step
through each model unit, mark all system units
sharing content with the current model unit
(shown in green highlight in the model summary
window), and specify that the marked system
units express all, most, some or hardly any of
the content of the current model unit. To
measure quality, assessors rate grammaticality1,
cohesion2, and coherence3 at five different
levels: all, most, some, hardly any, or none.
For example, as shown in Figure 1, an assessor
marked system units 1.1 and 10.4 (shown in red
underlines) as sharing some content with the
current model unit 2.2 (highlighted green).
</bodyText>
<sectionHeader confidence="0.997096" genericHeader="method">
3 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.999962625">
One goal of DUC-2001 was to debug the
evaluation procedures and identify stable
metrics that could serve as common reference
points. NIST did not define any official
performance metric in DUC-2001. It released
the raw evaluation results to DUC-2001
participants and encouraged them to propose
metrics that would help progress the field.
</bodyText>
<subsectionHeader confidence="0.999017">
3.1 Recall, Coverage, Retention and
Weighted Retention
</subsectionHeader>
<bodyText confidence="0.9865838">
Recall at different compression ratios has been
used in summarization research to measure how
well an automatic system retains important
content of original documents (Mani and
Maybury 1999). Assume we have a system
summary Ss and a model summary Sm. The
number of sentences occurring in Ss is Ns, the
number of sentences in Sm is Nm, and the number
in both Ss and Sm is Na. Recall is defined as
Na/Nm. The Compression Ratio is defined as the
length of a summary (by words or sentences)
divided by the length of its original document.
Applying this direct all-or-nothing recall in
DUC-2001 without modification is not
appropriate because:
</bodyText>
<footnote confidence="0.818364833333333">
1 Does the summary observe English grammatical
rules independent of its content?
2 Do sentences in the summary fit in with their
surrounding sentences?
3 Is the content of the summary expressed and
organized in an effective way?
</footnote>
<listItem confidence="0.999555">
1. Multiple system units contribute to multiple
model units.
2. Exact overlap between Ss and Sm rarely
occurs.
3. Overlap judgment is not binary.
</listItem>
<bodyText confidence="0.985434022727273">
For example in Figure 1, an assessor judged
system units 1.1 and 10.4 sharing some content
with model unit 2.2. Unit 1.1 says “Thousands
of people are feared dead” and unit 2.2 says
“3,000 and perhaps ... 5,000 people have been
killed”. Are “thousands” equivalent to “3,000 to
5,000” or not? Unit 10.4 indicates it was an
“earthquake of magnitude 6.9” and unit 2.2 says
it was “an earthquake measuring 6.9 on the
Richter scale”. Both of them report a “6.9”
earthquake. But the second part of system unit
10.4, “in an area so isolated...”, seems to share
some content with model unit 4.4 “the quake
was centered in a remote mountainous area”.
Are these two equivalent? This example
highlights the difficulty of judging the content
coverage of system summaries against model
summaries and the inadequacy of using simple
recall as defined.
For this reason, NIST assessors not only marked
the segments shared between system units (SU)
and model units (MU), they also indicated the
degree of match, i.e., all, most, some, hardly
any, or none. This enables us to compute
weighted recall.
Different versions of weighted recall were
proposed by DUC-2001 participants. (McKeown
et al. 2001) treated the completeness of coverage
as a threshold: 4 for all, 3 for most and above, 2
for some and above, and 1 for hardly any and
above. They then proceeded to compare system
performances at different threshold levels. They
defined recall at threshold t, Recallt, as follows:
Number of MUs marked at or above t
Total number of MUs in the model summary
Instead of thresholds, we use here as coverage
score the ratio of completeness of coverage C: 1
for all, 3/4 for most, 1/2 for some, 1/4 for hardly
any, and 0 for none. To avoid confusion with
the recall used in information retrieval, we call
our metric weighted retention, Retentionw, and
define it as follows:
(Number of MUs marked) •C
Total number of MUs in the model summary
</bodyText>
<figure confidence="0.998288962962963">
Retention
45.00
40.00
35.00
30.00
25.00
20.00
39.67
40.90
32.24
29.89
3021
29.61
34.78
26.38
34.44
28.25
28.99
27.02
MAJORITY
ORIGINAL
MAX
MIN
AVG
29.76
25.12
Systems
</figure>
<figureCaption confidence="0.999967">
Figure 2. DUC 2001 single document retention score distribution.
</figureCaption>
<bodyText confidence="0.999989">
If we ignore C (set it to 1), we obtain an
unweighted retention, Retention,. We used
Retention, in our evaluation to illustrate that
relative system performance (i.e., system
ranking) changes when different evaluation
metrics are chosen. Therefore, it is important to
have common and agreed upon metrics to
facilitate large scale evaluation efforts.
</bodyText>
<sectionHeader confidence="0.983535" genericHeader="method">
4 Instability of Manual Judgments
</sectionHeader>
<bodyText confidence="0.999458555555555">
In the human evaluation protocol described in
Section 2, nothing prevents an assessor from
assigning different coverage scores to the same
system units produced by different systems
against the same model unit. (Since most
systems produce extracts, the same sentence
may appear in many summaries, especially for
single-document summaries.) Analyzing the
DUC-2001 results, we found the following:
</bodyText>
<listItem confidence="0.988128">
• Single document task
o A total of 5,921 judgments
o Among them, 1,076 (18%) contain
multiple judgments for the same units
o 143 (2.4%) of them have three different
coverage scores
• Multi-document task
o A total of 6,963 judgments
</listItem>
<bodyText confidence="0.929066">
o Among them 528 (7.6%) contain multiple
judgments
o 27 (0.4%) of them have three different
coverage scores
Intuitively this is disturbing; the same phrase
compared to the same model unit should always
have the same score regardless of which system
produced it. The large percentage of multiple
judgments found in the single document
evaluation are test-retest errors that need to be
addressed in computing performance metrics.
Figure 2 and Figure 3 show the retention scores
for systems participating in the single- and
multi-document tasks respectively. The error
bars are bounded at the top by choosing the
maximum coverage score (MAX) assigned by
an assessor in the case of multiple judgment
scores and at the bottom by taking the minimum
assignment (MIN). We also compute system
</bodyText>
<figure confidence="0.998229166666667">
Retention
30.00
25.00
20.00
15.00
10.00
5.00
2
8.55
29.03
7.38
4.76
15.38
11.26
17.92
11.19
15.80
11.22
13.02
18.47
6.81
6.54
MAJORITY
ORIGINAL
MAX
MIN
AVG
17.80
9.02
Systems
</figure>
<figureCaption confidence="0.999899">
Figure 3. DUC 2001 multi-document retention score distribution.
</figureCaption>
<bodyText confidence="0.9994415">
retentions using the majority (MAJORITY) and
average (AVG) of assigned coverage scores.
The original (ORIGINAL) does not consider the
instability in the data.
Analyzing all systems’ results, we made the
following observations.
</bodyText>
<listItem confidence="0.93912452173913">
(1) Inter-human agreement is low in the single-
document task (~40%) and even lower in
multi-documents task (~29%). This
indicates that using a single model as
reference summary is not adequate.
(2) Despite the low inter-human agreement,
human summaries are still much better than
the best performing systems.
(3) The relative performance (rankings) of
systems changes when the instability of
human judgment is considered. However,
the rerankings remain local; systems remain
within performance groups. For example,
we have the following groups in the multi-
document summarization task (Figure 3,
considering 0.5% error):
a. {Human1, Human2}
b. {N, T, Y}
c. {Baseline2, L, P}
d. {S}
e. {M, O, R}
f. {Z}
g. {Baseline1, U, W}
</listItem>
<bodyText confidence="0.99555275">
The existence of stable performance regions is
encouraging. Still, given the large error bars,
one can produce 162 different rankings of these
16 systems. Groups are less obvious in the
single document summarization task due to
close performance among systems.
Table 1 shows relative performance between
systems x and y in the single document
</bodyText>
<equation confidence="0.990553133333333">
H1 H2 B1 O P Q R S T V W X Y Z
H1 = - + + + + + + + + + + + +
H2 + = + + + + + + + + + + + +
B1 - - = ~ ~ ~ ~ + - + ~ + ~ +
- ~ = ~ ~ - + - ~ ~ + ~ +
- ~ ~ = ~ - + - ~ ~ + ~ +
- ~ ~ ~ = - ~ - ~ ~ ~ ~ +
R - - ~ + + + = + ~ + + + + +
S~ - = - ~ ~ ~ - ~
T - - + + + + ~ + = + + + + +
- - ~ ~ ~ - ~ - = ~ ~ ~ ~
- ~ ~ ~ ~ - ~ - ~ = ~ ~ +
X~ - ~ - ~ ~ = - ~
Y- - ~ ~ ~ ~ - + - ~ ~ + = +
Z~ - ~ - ~ - =
</equation>
<tableCaption confidence="0.9955015">
Table 1. Pairwise relative system performance
(single document summarization task).
</tableCaption>
<equation confidence="0.91824685">
H1 H2 B1 B2 L M N O P R S T U W Y Z
H1 = - + + + + + + + + + + + + +
+
H2 + = + + + + + + + + + + + + +
+
B1 - - = + + - -
B2 - - + = ~ + - + ~ + + - + + - +
L - - + ~ = + - + ~ + + - + + - +
M - - + - - = - ~ - ~ - - + + - +
N - - + + + + = + + + + - + + ~ +
O - - + - - ~ - = - ~ - - + + - +
P - - - ~ ~ + - + = + + - + + - +
R - - + - - ~ - ~ - = - - + + - +
S - - + - - + - + - + = - + + - +
T - - + + + + + + + + = + + + +
+
U = - - -
W + = - -
Y - - + + + + ~ + + + + - + + = +
Z - - + + + - =
</equation>
<tableCaption confidence="0.9629145">
Table 2. Pairwise relative system performance
(multi-document summarization task).
</tableCaption>
<bodyText confidence="0.997464461538461">
summarization task. A ‘+’ indicates the
minimum retention score of x (row) is higher
than the maximum retention score of y
(column), a ‘-’ indicates the maximum retention
score of x is lower than the minimum retention
score of y, and a ‘~’ means x and y are
indistinguishable. Table 2 shows relative
system performance in the multi-document
summarization task.
Despite the instability of the manual evaluation,
we discuss automatic summary evaluation in an
attempt to approximate the human evaluation
results in the next section.
</bodyText>
<sectionHeader confidence="0.996312" genericHeader="method">
5 Automatic Summary Evaluation
</sectionHeader>
<bodyText confidence="0.972875625">
Inspired by recent progress in automatic
evaluation of machine translation (BLEU;
Papineni et al. 2001), we would like to apply the
same idea in the evaluation of summaries.
Following BLEU, we used the automatically
computed accumulative n-gram matching scores
(NAMS) between a model unit (MU) and a
system summary (S)4 as performance indicator,
considering multi-document summaries. Only
content words were used in forming n-grams.
NAMS is defined as follows:
a1·NAM1 + a2·NAM2 + a3·NAM3 + a4·NAM4
NAMn is n-gram hit ratio defined as:
# of matched n - grams between MU and S
total # of n - grams in MU
We tested three different configurations of ai:
</bodyText>
<footnote confidence="0.8288045">
4 The whole system summary was used to compute
NAMS against a model unit.
</footnote>
<equation confidence="0.917782666666667">
C1: a1 = 1 and a2 = a3 = a4 = 0;
C2: a1 = 1/3, a2 = 2/3, and a3 = a4 = 0;
C3: a1 = 1/6, a2 = 2/6, a3 = 3/6, and a4 = 0;
</equation>
<bodyText confidence="0.999594923076923">
C1 is simply unigram matching. C2 and C3
give more credit to longer n-gram matches. To
examine the effect of stemmers in helping the n-
gram matching, we also tested all configurations
with two different stemmers (Lovin’s and
Porter’s). Figure 4 shows the results with and
without using stemmers and their Spearman
rank-order correlation coefficients (rho)
compared against the original retention ranking
from Figure 4. X-nG is configuration n without
using any stemmer, L-nG with the Lovin
stemmer, and P-nG with the Porter stemmer.
The results in Figure 4 indicate that unigram
matching provides a good approximation, but
the best correlation is achieved using C2 with
the Porter stemmer. Using stemmers did
improve correlation. Notice that rank inversion
remains within the performance groups
identified in Section 4. For example, the
retention ranking of Baseline1, U, and W is 14,
16, and 15 respectively. The P-2G ranking of
these three systems is 15, 14, and 16. The only
system crossing performance groups is Y. Y
should be grouped with N and T but the
automatic evaluations place it lower, in the
group with Baseline2, L, and P. The primary
reason for Y’s behavior may be that its
summaries consist mainly of headlines, whose
abbreviated style differs from the language
models derived from normal newspaper text.
For comparison, we also ran IBM’s BLEU
evaluation script5 over the same model and
system summary set. The Spearman rank-order
correlation coefficient (ρ) for the single
document task is 0.66 using one reference
summary and 0.82 using three reference
summaries; while Spearman ρ for the multi-
document task is 0.67 using one reference and
0.70 using three.
</bodyText>
<sectionHeader confidence="0.999664" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.998705666666667">
We described manual and automatic evaluation
of single and multi-document summarization in
DUC-2001. We showed the instability of
</bodyText>
<footnote confidence="0.6015765">
5 We thank Kishore Papineni for sending us BLEU
1.0.
</footnote>
<table confidence="0.98842895">
Original No stemmer Lovin stemmer Porter stemmer
SYSCODE Retention X-1G X-2G X-3G L-1G L-2G L-3G P-1G P-2G P-3G
ranking (unigram) (unigram) (unigram)
Human1 1 2 1 1 1 1 1 1 1 1
Human2 2 1 2 2 2 2 2 2 2 2
Baseline1 14 15 15 15 16 15 14 16 15 14
Baseline2 8 8 7 6 8 8 6 8 8 6
L 7 7 6 7 7 7 7 7 7 7
M 10 10 10 10 10 11 11 9 10 11
N 4 4 4 4 4 4 4 4 4 4
O 11 12 12 12 12 12 12 11 12 12
P 6 5 5 5 5 5 5 5 5 5
R 11 11 11 11 11 10 10 12 11 10
S 9 9 9 9 9 9 9 9 9 9
T 3 3 3 3 3 3 3 3 3 3
U 16 14 14 14 14 14 15 14 14 15
W 15 16 16 16 15 16 16 15 16 16
Y 5 6 8 8 6 6 8 6 6 8
Z 13 13 13 13 13 13 13 13 13 13
Spearman ρ 1.00000 0.98382 0.97206 0.96912 0.98382 0.98382 0.97206 0.98235 0.98676 0.97206
</table>
<figureCaption confidence="0.998716">
Figure 4. Manual and automatic ranking comparisons.
</figureCaption>
<bodyText confidence="0.951966413793104">
human evaluations and the need to consider this
factor when comparing system performances.
As we factored in the instability, systems tended
to form separate performance groups. One
should treat with caution any interpretation of
performance figures that ignores this instability.
Automatic evaluation of summaries using
accumulative n-gram matching scores seems
promising. System rankings using NAMS and
retention ranking had a Spearman rank-order
correlation coefficient above 97%. Using
stemmers improved the correlation. However,
satisfactory correlation is still elusive. The main
problem we ascribe to automated summary
evaluation is the large expressive range of
English since human summarizers tend to create
fresh text. No n-gram matching evaluation
procedure can overcome the paraphrase or
synonym problem unless (many) model
summaries are available.
We conclude the following:
(1) We need more than one model summary
although we cannot estimate how many
model summaries are required to achieve
reliable automated summary evaluation.
(2) We need more than one evaluation for each
summary against each model summary.
(3) We need to ensure a single rating for each
system unit.
</bodyText>
<sectionHeader confidence="0.998036" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998770928571429">
DUC. 2001. The Document Understanding
Conference 2001. http://www-nlpir.nist.gov/
projects/duc/2001.html.
Lin, C.-Y. 2001. Summary Evaluation
Environment. http://www.isi.edu/~cyl/SEE.
Mani, I., D. House, G. Klein, L. Hirschman, L.
Obrst, T. Firmin, M. Chrzanowski, and B.
Sundheim. 1998. The TIPSTER SUMMAC
Text Summarization Evaluation: Final
Report. MITRE Corp. Tech. Report.
Papineni K., S. Roukos, T. Ward, W.-J. Zhu.
2001. BLEU: a Method for Automatic
Evaluation of Machine Translation. IBM
Research Report RC22176(W0109-022).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.8772265">Proceedings of the Workshop on Automatic Summarization (including DUC 2002), Philadelphia, July 2002, pp. 45-51. Association for Computational Linguistics.</note>
<title confidence="0.972473">Manual and Automatic Evaluation of Summaries</title>
<author confidence="0.998721">Chin-Yew Lin</author>
<author confidence="0.998721">Eduard Hovy</author>
<affiliation confidence="0.998877">USC Information Sciences</affiliation>
<address confidence="0.989695">4676 Admiralty</address>
<author confidence="0.94684">Marina del Rey</author>
<author confidence="0.94684">CA</author>
<email confidence="0.998983">cyl@isi.edu</email>
<email confidence="0.998983">hovy@isi.edu</email>
<abstract confidence="0.99197302293578">In this paper we discuss manual and automatic evaluations of summaries using data from the Document Understanding Conference 2001 (DUC-2001). We first show the instability of the manual evaluation. Specifically, the low interhuman agreement indicates that more reference summaries are needed. To investigate the feasibility of automated summary evaluation based on the recent BLEU method from machine translation, we use accumulative n-gram overlap scores between system and human summaries. The initial results provide encouraging correlations with human judgments, based on the Spearman rank-order correlation coefficient. However, relative ranking of systems needs to take into account the instability. Previous efforts in large-scale evaluation of text summarization include TIPSTER SUMMAC (Mani et al. 1998) and the Document Understanding Conference (DUC) sponsored by the National Institute of Standards and Technology (NIST). DUC aims to compile standard training and test collections that can be shared among researchers and to provide common and large scale evaluations in single and multiple document summarization for their participants. In this paper we discuss manual and automatic evaluations of summaries using data from the Document Understanding Conference 2001 (DUC-2001). Section 2 gives a brief overview of the evaluation procedure used in DUC-2001 and the Summary Evaluation Environment (SEE) interface used to support the DUC-2001 human evaluation protocol. Section 3 discusses evaluation metrics. Section 4 shows the instability of manual evaluations. Section 5 outlines a method of automatic summary evaluation using accumulative n-gram matching and proposes a view that casts summary evaluation as a decision making It shows that the is bounded and in most cases not usable, given only a single reference summary to compare with. Section 6 discusses why this is so, illustrating various forms of mismatching between human and system summaries. We conclude with lessons learned and future directions. Understanding Conference (DUC) DUC2001 included three tasks: • Fully automatic summarization: given a document, participants were required to create a generic 100-word summary. The training set comprised 30 sets of approximately 10 documents each, together with their 100word human written summaries. The test set comprised 30 unseen documents. • Fully automatic summarization: given a set of documents about a single subject, participants were required to create 4 generic summaries of the entire set, containing 50, 100, 200, and 400 words respectively. The document sets were of four types: a single natural disaster event; a single event; multiple instances of a type of event; and information about an individual. The training set comprised 30 sets of approximately 10 documents, each provided with their 50, 100, 200, and 400word human written summaries. The test set comprised 30 unseen sets. 1. in an evaluation session. • Exploratory summarization: participants were encouraged to investigate alternative approaches to evaluating summarization and report their results. A total of 11 systems participated in the singledocument summarization task and 12 systems participated in the multi-document task. The training data were distributed in early March of 2001 and the test data were distributed in mid-June of 2001. Results were submitted to for evaluation by July 2001. 2.1 Evaluation Materials For each document or document set, one human summary was created as the ‘ideal’ model summary at each specified length. Two other human summaries were also created at each length. In addition, baseline summaries were created automatically for each length as reference points. For the task, one baseline, took the first 50, 100, 200, and 400 words in the last document in the collection. A second took the first sentence in the first document, the first sentence in the second document and so on until it had a summary of 50, 100, 200, or 400 words. Only one baseline (baseline1) was created for the single document summarization task. 2.2 Summary Evaluation Environment NIST assessors who created the ‘ideal’ written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors’ summaries, and baseline summaries. They used the Summary Evaluation Environment (SEE) 2.0 developed by one of the authors (Lin 2001) to support the process. Using SEE, the assessors compared the system’s (the to the ideal (the As shown in Figure 1, each text was decomposed into a list of units and displayed in separate windows. In DUC-2001 the sentence was used as the smallest unit of evaluation. SEE 2.0 provides interfaces for assessors to judge both the content and the quality of summaries. To measure content, assessors step through each model unit, mark all system units sharing content with the current model unit (shown in green highlight in the model summary window), and specify that the marked system express most, some any the content of the current model unit. To quality, assessors rate and at five different most, some, hardly or For example, as shown in Figure 1, an assessor marked system units 1.1 and 10.4 (shown in red as sharing with the current model unit 2.2 (highlighted green). Metrics One goal of DUC-2001 was to debug the evaluation procedures and identify stable metrics that could serve as common reference points. NIST did not define any official performance metric in DUC-2001. It released the raw evaluation results to DUC-2001 participants and encouraged them to propose metrics that would help progress the field. 3.1 Recall, Coverage, Retention and Weighted Retention Recall at different compression ratios has been used in summarization research to measure how well an automatic system retains important content of original documents (Mani and Maybury 1999). Assume we have a system and a model summary The of sentences occurring in is the of sentences in is and the number both and is Recall is defined as The Compression Ratio is defined as the length of a summary (by words or sentences) divided by the length of its original document. Applying this direct all-or-nothing recall in DUC-2001 without modification is not appropriate because: the summary observe English grammatical rules independent of its content? sentences in the summary fit in with their surrounding sentences? the content of the summary expressed and organized in an effective way? 1. Multiple system units contribute to multiple model units. Exact overlap between and rarely occurs. 3. Overlap judgment is not binary. For example in Figure 1, an assessor judged units 1.1 and 10.4 sharing model unit 2.2. Unit 1.1 says people are feared and unit 2.2 says and perhaps ... 5,000 people have been Are “thousands” equivalent to “3,000 to 5,000” or not? Unit 10.4 indicates it was an of magnitude and unit 2.2 says was earthquake measuring 6.9 on the Both of them report a “6.9” earthquake. But the second part of system unit an area so seems to share content with model unit 4.4 quake centered in a remote mountainous Are these two equivalent? This example highlights the difficulty of judging the content coverage of system summaries against model summaries and the inadequacy of using simple recall as defined. For this reason, NIST assessors not only marked the segments shared between system units (SU) and model units (MU), they also indicated the of match, i.e., or This enables us to compute Different versions of weighted recall were proposed by DUC-2001 participants. (McKeown et al. 2001) treated the completeness of coverage a threshold: 4 for 3 for above, 2 above, and 1 for any above. They then proceeded to compare system performances at different threshold levels. They recall at threshold follows: of MUs marked at or above Total number of MUs in the model summary Instead of thresholds, we use here as coverage the ratio of completeness of coverage 1 3/4 for 1/2 for 1/4 for and 0 for To avoid confusion with the recall used in information retrieval, we call metric weighted retention, and define it as follows: of MUs marked) Total number of MUs in the model summary Retention</abstract>
<address confidence="0.912255555555556">45.00 40.00 35.00 30.00 25.00 20.00 39.67 40.90 32.24 29.89 3021 29.61 34.78 26.38 34.44 28.25 28.99 27.02</address>
<email confidence="0.483637">MAJORITY</email>
<title confidence="0.758652">ORIGINAL</title>
<author confidence="0.868894">MAX MIN</author>
<affiliation confidence="0.796373">AVG</affiliation>
<address confidence="0.5670885">29.76 25.12</address>
<abstract confidence="0.984196145833334">Systems 2. 2001 single document retention score distribution. we ignore it to 1), we obtain an retention, We used our evaluation to illustrate that relative system performance (i.e., system ranking) changes when different evaluation metrics are chosen. Therefore, it is important to have common and agreed upon metrics to facilitate large scale evaluation efforts. of Manual Judgments In the human evaluation protocol described in Section 2, nothing prevents an assessor from assigning different coverage scores to the same system units produced by different systems against the same model unit. (Since most systems produce extracts, the same sentence may appear in many summaries, especially for single-document summaries.) Analyzing the DUC-2001 results, we found the following: • Single document task o A total of 5,921 judgments o Among them, 1,076 (18%) contain multiple judgments for the same units o 143 (2.4%) of them have three different coverage scores • Multi-document task o A total of 6,963 judgments o Among them 528 (7.6%) contain multiple judgments o 27 (0.4%) of them have three different coverage scores Intuitively this is disturbing; the same phrase compared to the same model unit should always have the same score regardless of which system produced it. The large percentage of multiple judgments found in the single document evaluation are test-retest errors that need to be addressed in computing performance metrics. Figure 2 and Figure 3 show the retention scores for systems participating in the singleand multi-document tasks respectively. The error bars are bounded at the top by choosing the maximum coverage score (MAX) assigned by an assessor in the case of multiple judgment scores and at the bottom by taking the minimum assignment (MIN). We also compute system Retention</abstract>
<address confidence="0.8496205">30.00 25.00</address>
<date confidence="0.403446">20.00 15.00</date>
<note confidence="0.659012142857143">10.00 5.00 2 8.55 29.03 7.38 4.76</note>
<date confidence="0.80025325">15.38 11.26 17.92 11.19 15.80 11.22 13.02 18.47</date>
<note confidence="0.660661">6.81 6.54</note>
<title confidence="0.8062585">MAJORITY ORIGINAL</title>
<author confidence="0.8440835">MAX MIN</author>
<abstract confidence="0.947392727272727">AVG 17.80 9.02 Systems 3. 2001 multi-document retention score distribution. retentions using the majority (MAJORITY) and average (AVG) of assigned coverage scores. The original (ORIGINAL) does not consider the instability in the data. Analyzing all systems’ results, we made the following observations. (1) Inter-human agreement is low in the singledocument task (~40%) and even lower in multi-documents task (~29%). This indicates that using a single model as reference summary is not adequate. (2) Despite the low inter-human agreement, human summaries are still much better than the best performing systems. (3) The relative performance (rankings) of systems changes when the instability of human judgment is considered. However, the rerankings remain local; systems remain within performance groups. For example, we have the following groups in the multidocument summarization task (Figure 3, considering 0.5% error): a. {Human1, Human2} b. {N, T, Y} c. {Baseline2, L, P} d. {S} e. {M, O, R} f. {Z} g. {Baseline1, U, W} The existence of stable performance regions is encouraging. Still, given the large error bars, one can produce 162 different rankings of these 16 systems. Groups are less obvious in the single document summarization task due to close performance among systems. Table 1 shows relative performance between the single document H1 H2 B1 O P Q R S T V W X Y Z H1 = - + + + + + + + + + + + + H2 + = + + + + + + + + + + + + B1 - - = ~ ~ ~ ~ + - + ~ + ~ + - ~ = ~ ~ - + - ~ ~ + ~ + - ~ ~ = ~ - + - ~ ~ + ~ + - ~ ~ ~ = - ~ - ~ ~ ~ ~ + R - - ~ + + + = + ~ + + + + + - = - ~ ~ ~ - ~ T - - + + + + ~ + = + + + + + - - ~ ~ ~ - ~ - = ~ ~ ~ ~ - ~ ~ ~ ~ - ~ - ~ = ~ ~ + - ~ - ~ ~ = - ~ - ~ ~ ~ ~ - + - ~ ~ + = + - ~ - ~ - = 1. relative system performance (single document summarization task). H1 H2 B1 B2 L M N O P R S T U W Y Z H1 = - + + + + + + + + + + + + + + H2 + = + + + + + + + + + + + + + + B1 - - = + + - - B2 - - + = ~ + - + ~ + + - + + - +</abstract>
<title confidence="0.85081575">L - - + ~ = + - + ~ + + - + + - + M - - + - - = - ~ - ~ - - + + - + N - - + + + + = + + + + - + + ~ + O - - + - - ~ - = - ~ - - + + - + P - - - ~ ~ + - + = + + - + + - + R - - + - - ~ - ~ - = - - + + - + S - - + - - + - + - + = - + + - + T - - + + + + + + + + = + + + + + U = - - - W + = - - Y - - + + + + ~ + + + + - + + = +</title>
<abstract confidence="0.986316927710843">Z - - + + + - = 2. relative system performance (multi-document summarization task). summarization task. A ‘+’ indicates the retention score of is higher the maximum retention score of (column), a ‘-’ indicates the maximum retention of lower than the minimum retention of and a ‘~’ means indistinguishable. Table 2 shows system performance in the multi-document summarization task. Despite the instability of the manual evaluation, we discuss automatic summary evaluation in an attempt to approximate the human evaluation results in the next section. Summary Evaluation Inspired by recent progress in automatic evaluation of machine translation (BLEU; Papineni et al. 2001), we would like to apply the same idea in the evaluation of summaries. Following BLEU, we used the automatically computed accumulative n-gram matching scores between a model unit (MU) and a summary as performance indicator, considering multi-document summaries. Only content words were used in forming n-grams. defined as follows: is n-gram hit ratio defined as: total # of n grams in MU tested three different configurations of whole system summary was used to compute a model unit. C1: 1 and 0; C2: 1/3, 2/3, and 0; C3: 1/6, 2/6, 3/6, and 0; C1 is simply unigram matching. C2 and C3 give more credit to longer n-gram matches. To examine the effect of stemmers in helping the ngram matching, we also tested all configurations with two different stemmers (Lovin’s and Porter’s). Figure 4 shows the results with and without using stemmers and their Spearman rank-order correlation coefficients (rho) compared against the original retention ranking Figure 4. is configuration any stemmer, with the Lovin and with the Porter stemmer. The results in Figure 4 indicate that unigram matching provides a good approximation, but the best correlation is achieved using C2 with the Porter stemmer. Using stemmers did improve correlation. Notice that rank inversion remains within the performance groups identified in Section 4. For example, the retention ranking of Baseline1, U, and W is 14, 16, and 15 respectively. The P-2G ranking of these three systems is 15, 14, and 16. The only system crossing performance groups is Y. Y should be grouped with N and T but the automatic evaluations place it lower, in the group with Baseline2, L, and P. The primary reason for Y’s behavior may be that its summaries consist mainly of headlines, whose abbreviated style differs from the language models derived from normal newspaper text. For comparison, we also ran IBM’s BLEU over the same model and system summary set. The Spearman rank-order coefficient for the single document task is 0.66 using one reference summary and 0.82 using three reference while Spearman the multidocument task is 0.67 using one reference and 0.70 using three. We described manual and automatic evaluation of single and multi-document summarization in DUC-2001. We showed the instability of thank Kishore Papineni for sending us 1.0. Original No stemmer Lovin stemmer Porter stemmer SYSCODE Retention X-1G X-2G X-3G L-1G L-2G L-3G P-1G P-2G P-3G ranking (unigram) (unigram) (unigram)</abstract>
<phone confidence="0.5484132">Human1 1 2 1 1 1 1 1 1 1 1 Human2 2 1 2 2 2 2 2 2 2 2 Baseline1 14 15 15 15 16 15 14 16 15 14 Baseline2 8 8 7 6 8 8 6 8 8 6 L 7 7 6 7 7 7 7 7 7 7 M 10 10 10 10 10 11 11 9 10 11 N 4 4 4 4 4 4 4 4 4 4 O 11 12 12 12 12 12 12 11 12 12 P 6 5 5 5 5 5 5 5 5 5 R 11 11 11 11 11 10 10 12 11 10 S 9 9 9 9 9 9 9 9 9 9 T 3 3 3 3 3 3 3 3 3 3 U 16 14 14 14 14 14 15 14 14 15 W 15 16 16 16 15 16 16 15 16 16 Y 5 6 8 8 6 6 8 6 6 8</phone>
<abstract confidence="0.9789095">Z 13 13 13 13 13 13 13 13 13 13 Spearman ρ 1.00000 0.98382 0.97206 0.96912 0.98382 0.98382 0.97206 0.98235 0.98676 0.97206 4. and automatic ranking comparisons. human evaluations and the need to consider this factor when comparing system performances. As we factored in the instability, systems tended to form separate performance groups. One should treat with caution any interpretation of performance figures that ignores this instability. Automatic evaluation of summaries using accumulative n-gram matching scores seems System rankings using retention ranking had a Spearman rank-order correlation coefficient above 97%. stemmers improved the correlation. However, satisfactory correlation is still elusive. The main problem we ascribe to automated summary evaluation is the large expressive range of English since human summarizers tend to create fresh text. No n-gram matching evaluation procedure can overcome the paraphrase or synonym problem unless (many) model summaries are available. We conclude the following: (1) We need more than one model summary although we cannot estimate how many model summaries are required to achieve reliable automated summary evaluation. (2) We need more than one evaluation for each summary against each model summary. (3) We need to ensure a single rating for each system unit.</abstract>
<note confidence="0.545646866666667">References DUC. 2001. The Document Understanding Conference 2001. http://www-nlpir.nist.gov/ projects/duc/2001.html. C.-Y. 2001. Evaluation http://www.isi.edu/~cyl/SEE. Mani, I., D. House, G. Klein, L. Hirschman, L. Obrst, T. Firmin, M. Chrzanowski, and B. 1998. TIPSTER SUMMAC Text Summarization Evaluation: Final MITRE Corp. Tech. Report. Papineni K., S. Roukos, T. Ward, W.-J. Zhu. a Method for Automatic of Machine Translation. Research Report RC22176(W0109-022).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<title>The Document Understanding Conference</title>
<date>2001</date>
<note>http://www-nlpir.nist.gov/ projects/duc/2001.html.</note>
<contexts>
<context position="9482" citStr="DUC 2001" startWordPosition="1485" endWordPosition="1486"> t Total number of MUs in the model summary Instead of thresholds, we use here as coverage score the ratio of completeness of coverage C: 1 for all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 for none. To avoid confusion with the recall used in information retrieval, we call our metric weighted retention, Retentionw, and define it as follows: (Number of MUs marked) •C Total number of MUs in the model summary Retention 45.00 40.00 35.00 30.00 25.00 20.00 39.67 40.90 32.24 29.89 3021 29.61 34.78 26.38 34.44 28.25 28.99 27.02 MAJORITY ORIGINAL MAX MIN AVG 29.76 25.12 Systems Figure 2. DUC 2001 single document retention score distribution. If we ignore C (set it to 1), we obtain an unweighted retention, Retention,. We used Retention, in our evaluation to illustrate that relative system performance (i.e., system ranking) changes when different evaluation metrics are chosen. Therefore, it is important to have common and agreed upon metrics to facilitate large scale evaluation efforts. 4 Instability of Manual Judgments In the human evaluation protocol described in Section 2, nothing prevents an assessor from assigning different coverage scores to the same system units produced by diffe</context>
<context position="11519" citStr="DUC 2001" startWordPosition="1806" endWordPosition="1807">hat need to be addressed in computing performance metrics. Figure 2 and Figure 3 show the retention scores for systems participating in the single- and multi-document tasks respectively. The error bars are bounded at the top by choosing the maximum coverage score (MAX) assigned by an assessor in the case of multiple judgment scores and at the bottom by taking the minimum assignment (MIN). We also compute system Retention 30.00 25.00 20.00 15.00 10.00 5.00 2 8.55 29.03 7.38 4.76 15.38 11.26 17.92 11.19 15.80 11.22 13.02 18.47 6.81 6.54 MAJORITY ORIGINAL MAX MIN AVG 17.80 9.02 Systems Figure 3. DUC 2001 multi-document retention score distribution. retentions using the majority (MAJORITY) and average (AVG) of assigned coverage scores. The original (ORIGINAL) does not consider the instability in the data. Analyzing all systems’ results, we made the following observations. (1) Inter-human agreement is low in the singledocument task (~40%) and even lower in multi-documents task (~29%). This indicates that using a single model as reference summary is not adequate. (2) Despite the low inter-human agreement, human summaries are still much better than the best performing systems. (3) The relative pe</context>
</contexts>
<marker>DUC, 2001</marker>
<rawString>DUC. 2001. The Document Understanding Conference 2001. http://www-nlpir.nist.gov/ projects/duc/2001.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>Summary Evaluation Environment.</title>
<date>2001</date>
<note>http://www.isi.edu/~cyl/SEE.</note>
<contexts>
<context position="4931" citStr="Lin 2001" startWordPosition="723" endWordPosition="724">collection. A second baseline, coverage baseline, took the first sentence in the first document, the first sentence in the second document and so on until it had a summary of 50, 100, 200, or 400 words. Only one baseline (baseline1) was created for the single document summarization task. 2.2 Summary Evaluation Environment NIST assessors who created the ‘ideal’ written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors’ summaries, and baseline summaries. They used the Summary Evaluation Environment (SEE) 2.0 developed by one of the authors (Lin 2001) to support the process. Using SEE, the assessors compared the system’s text (the peer text) to the ideal (the model text). As shown in Figure 1, each text was decomposed into a list of units and displayed in separate windows. In DUC-2001 the sentence was used as the smallest unit of evaluation. SEE 2.0 provides interfaces for assessors to judge both the content and the quality of summaries. To measure content, assessors step through each model unit, mark all system units sharing content with the current model unit (shown in green highlight in the model summary window), and specify that the ma</context>
</contexts>
<marker>Lin, 2001</marker>
<rawString>Lin, C.-Y. 2001. Summary Evaluation Environment. http://www.isi.edu/~cyl/SEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>D House</author>
<author>G Klein</author>
<author>L Hirschman</author>
<author>L Obrst</author>
<author>T Firmin</author>
<author>M Chrzanowski</author>
<author>B Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC Text Summarization Evaluation: Final Report.</title>
<date>1998</date>
<tech>MITRE Corp. Tech. Report.</tech>
<contexts>
<context position="1185" citStr="Mani et al. 1998" startWordPosition="156" endWordPosition="159">, the low interhuman agreement indicates that more reference summaries are needed. To investigate the feasibility of automated summary evaluation based on the recent BLEU method from machine translation, we use accumulative n-gram overlap scores between system and human summaries. The initial results provide encouraging correlations with human judgments, based on the Spearman rank-order correlation coefficient. However, relative ranking of systems needs to take into account the instability. 1 Introduction Previous efforts in large-scale evaluation of text summarization include TIPSTER SUMMAC (Mani et al. 1998) and the Document Understanding Conference (DUC) sponsored by the National Institute of Standards and Technology (NIST). DUC aims to compile standard training and test collections that can be shared among researchers and to provide common and large scale evaluations in single and multiple document summarization for their participants. In this paper we discuss manual and automatic evaluations of summaries using data from the Document Understanding Conference 2001 (DUC-2001). Section 2 gives a brief overview of the evaluation procedure used in DUC-2001 and the Summary Evaluation Environment (SEE</context>
</contexts>
<marker>Mani, House, Klein, Hirschman, Obrst, Firmin, Chrzanowski, Sundheim, 1998</marker>
<rawString>Mani, I., D. House, G. Klein, L. Hirschman, L. Obrst, T. Firmin, M. Chrzanowski, and B. Sundheim. 1998. The TIPSTER SUMMAC Text Summarization Evaluation: Final Report. MITRE Corp. Tech. Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<journal>IBM Research Report</journal>
<pages>22176--0109</pages>
<contexts>
<context position="14583" citStr="Papineni et al. 2001" startWordPosition="2571" endWordPosition="2574">imum retention score of x (row) is higher than the maximum retention score of y (column), a ‘-’ indicates the maximum retention score of x is lower than the minimum retention score of y, and a ‘~’ means x and y are indistinguishable. Table 2 shows relative system performance in the multi-document summarization task. Despite the instability of the manual evaluation, we discuss automatic summary evaluation in an attempt to approximate the human evaluation results in the next section. 5 Automatic Summary Evaluation Inspired by recent progress in automatic evaluation of machine translation (BLEU; Papineni et al. 2001), we would like to apply the same idea in the evaluation of summaries. Following BLEU, we used the automatically computed accumulative n-gram matching scores (NAMS) between a model unit (MU) and a system summary (S)4 as performance indicator, considering multi-document summaries. Only content words were used in forming n-grams. NAMS is defined as follows: a1·NAM1 + a2·NAM2 + a3·NAM3 + a4·NAM4 NAMn is n-gram hit ratio defined as: # of matched n - grams between MU and S total # of n - grams in MU We tested three different configurations of ai: 4 The whole system summary was used to compute NAMS </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni K., S. Roukos, T. Ward, W.-J. Zhu. 2001. BLEU: a Method for Automatic Evaluation of Machine Translation. IBM Research Report RC22176(W0109-022).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>