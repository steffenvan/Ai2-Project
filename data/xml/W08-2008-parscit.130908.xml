<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010660">
<title confidence="0.9928635">
Concept-graph based Biomedical Automatic Summarization using
Ontologies
</title>
<author confidence="0.707808">
Laura Plaza Morales
Alberto Diaz Esteban
Pablo Gerv´as
</author>
<affiliation confidence="0.6295325">
Universidad Complutense de Madrid
C/Profesor Jos´e Garc´ıa Santesmases, s/n, Madrid 28040, Spain
</affiliation>
<email confidence="0.994077">
lplazam@pas.ucm.es, albertodiaz@fdi.ucm.es,pgervas@sip.ucm.es
</email>
<sectionHeader confidence="0.993796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997958">
One of the main problems in research on
automatic summarization is the inaccu-
rate semantic interpretation of the source.
Using specific domain knowledge can con-
siderably alleviate the problem. In this pa-
per, we introduce an ontology-based ex-
tractive method for summarization. It is
based on mapping the text to concepts
and representing the document and its sen-
tences as graphs. We have applied our
approach to summarize biomedical litera-
ture, taking advantages of free resources as
UMLS. Preliminary empirical results are
presented and pending problems are iden-
tified.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973111111111">
In recent years, the amount of electronic biomedi-
cal literature has increased explosively. Physicians
and researchers constantly have to consult up-to
date information according to their needs, but the
process is time-consuming. In order to tackle this
overload of information, text summarization can
undoubtedly play a role.
Simultaneously, a big deal of resources, such
as biomedical terminologies and ontologies, have
emerged. They can significantly benefit the deve-
lopment of NLP systems, and in particular, when
used in automatic summarization, they can in-
crease the quality of summaries.
In this paper, we present an ontology-based ex-
tractive method for the summarization of biomed-
ical literature, based on mapping the text to con-
cepts in UMLS and representing the document and
its sentences as graphs. To assess the importance
</bodyText>
<footnote confidence="0.97491775">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.982431">
of the sentences, we compute the centrality of their
concepts in the document graph.
</bodyText>
<sectionHeader confidence="0.995623" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999960638888889">
Traditionally, automatic summarization methods
have been classified in those which generate ex-
tracts and those which generate abstracts. Al-
though human summaries are typically abstracts,
most of existing systems produce extracts.
Extractive methods build summaries on a super-
ficial analysis of the source. Early summariza-
tion systems are based on simple heuristic fea-
tures, as the position of sentences in the docu-
ment (Brandow et al., 1995), the frequency of
the words they contain (Luhn, 1958; Edmundson,
1969), or the presence of certain cue words or in-
dicative phrases (Edmundson, 1969). Some ad-
vanced approaches also employ machine learning
techniques to determine the best set of attributes
for extraction (Kupiec et al., 1995). Recently,
several graph-based methods have been proposed
to rank sentences for extraction. LexRank (Erkan
and Radev, 2004) is an example of a centroid-
based method to multi-document summarization
that assess sentence importance based on the con-
cept of eigenvector centrality. It represents the
sentences in each document by its tf*idf vectors
and computes sentence connectivity using the co-
sine similarity. Even if results are promising, most
of these approaches exhibit important deficiencies
which are consequences of not capturing the se-
mantic relations between terms (synonymy, hyper-
onymy, homonymy, and co-occurs and associated-
with relations).
We present an extractive method for summariza-
tion which attempts to solve this deficiencies. Un-
like researches conducted by (Yoo et al., 2007;
Erkan and Radev, 2004), which cluster sentences
to identify shared topics in multiple documents, in
this work we apply clustering to identify groups
</bodyText>
<page confidence="0.966747">
53
</page>
<note confidence="0.572018">
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 53–56
</note>
<bodyText confidence="0.911177">
Manchester, August 2008
of concepts closely related. We hypothesize that
each cluster represents a theme or topic in the do-
cument, and we evaluate three different heuristics
to ranking sentences.
</bodyText>
<sectionHeader confidence="0.967528" genericHeader="method">
3 Biomedical Ontologies. UMLS
</sectionHeader>
<bodyText confidence="0.999643944444444">
Biomedical ontologies organize domain concepts
and knowledge in a system of hierarchical and as-
sociative relations. One of the most widespread
in NLP applications is UMLS1 (Unified Medi-
cal Language System). UMLS consists of three
components: the Metathesaurus, a collection of
concepts and terms from various vocabularies and
their relationships; the Semantic Network, a set of
categories and relations used to classify and relate
the entries in the Metathesaurus; and the Special-
ist Lexicon, a database of lexicographic informa-
tion for use in NLP. In this work, we have se-
lected UMLS for several reasons. First, it pro-
vides a mapping structure between different ter-
minologies, including MeSH or SNOMED, and
thus allows to translate between them. Secondly, it
contains vocabularies in various languages, which
allows to process multilingual information.
</bodyText>
<sectionHeader confidence="0.980587" genericHeader="method">
4 Summarization Method
</sectionHeader>
<bodyText confidence="0.9981305">
The method proposed consists of three steps. Each
step is discussed in detail below. A preliminary
system has been implemented and tested on several
documents from the corpus developed by BioMed
Central2.
As the preprocessing, text is split into sentences
using GATE3, and generic words and high fre-
quency terms are removed, as they are not useful
in discriminating between relevant and irrelevant
sentences.
</bodyText>
<subsectionHeader confidence="0.995861">
4.1 Graph-based Document Representation
</subsectionHeader>
<bodyText confidence="0.999986111111111">
This step consists in representing each document
as a graph, where the vertices are the concepts in
UMLS associated to the terms, and the edges indi-
cate the relations between them. Firstly, each sen-
tence is mapped to the UMLS Metathesaurus using
MetaMap (Aronson, 2001). MetaMap allows
to map terms to UMLS concepts, using n-grams
for indexing in the ULMS Metathesaurus, and
performing disambiguation to identify the correct
</bodyText>
<footnote confidence="0.9986126">
1NLM Unified Medical Language System (UMLS). URL:
http://www.nlm.nih.gov/research/umls
2BioMed Central: http://www.biomedcentral.com/
3GATE (Generic Architecture for Text Engineering):
http://gate.ac.uk/
</footnote>
<bodyText confidence="0.996036153846154">
concept for a term. Secondly, the UMLS concepts
are extended with their hyperonyms. Figure 1
shows the graph for sentence ”The goal of the trial
was to assess cardiovascular mortality and mor-
bidity for stroke, coronary heart disease and con-
gestive heart failure, as an evidence-based guide
for clinicians who treat hypertension.”
Next, each edge is assigned a weight, which is
directly proportional to the deep in the hierarchy at
which the concepts lies (Figure 1). That is to say,
the more specific the concepts connected are, the
more weight is assigned to them. Expression (1)
shows how these values are computed.
</bodyText>
<equation confidence="0.543803">
|α n β ||β |(1)
|α U β ||α|
</equation>
<bodyText confidence="0.9997914">
where α is the set of all the parents of a con-
cept, including the concept, and β is the set of all
the parents of its immediate higher-level concept,
including the concept.
Finally, the sentence graphs are merged into
a document graph, enriched with the associated-
with relations between the semantic types in
UMLS corresponding to the concepts (Figure 1).
Weights for the new edges are computed using ex-
pression (1).
</bodyText>
<subsectionHeader confidence="0.914877">
4.2 Concept Clustering and Theme
Recognition
</subsectionHeader>
<bodyText confidence="0.999971307692308">
The second step consists of clustering concepts in
the document graph, using a degree-based method
(Erkan and Radev, 2004). Each cluster is com-
posed by a set of concepts that are closely related
in meaning, and can be seen as a theme in the do-
cument. The most central concepts in the cluster
give the sufficient and necessary information re-
lated to its theme. We hypothesize that the docu-
ment graph is an instance of a scale-free network
(Barabasi, 1999). Following (Yoo et al., 2007),
we introduce the salience of vertices. Mathemati-
cally, the salience of a vertex (vi) is calculated as
follows.
</bodyText>
<equation confidence="0.695084">
salience(vi) = weight(ej)
(2)
ej|∃vk∧ejconecta(vi,vk)
</equation>
<bodyText confidence="0.999753">
Within the set of vertices, we select the n
that present the higher salience and iteratively
group them in Hub Vertex Sets (HVS). A HVS
represents a group of vertices that are strongly
related to each other. The remaining vertices are
</bodyText>
<page confidence="0.998321">
54
</page>
<figureCaption confidence="0.999918">
Figure 1: Sentence graph
</figureCaption>
<bodyText confidence="0.990416777777778">
assigned to that cluster to which they are more
connected.
Finally, we assign each sentence to a cluster. To
measure the similarity between a cluster and a sen-
tence graph, we use a vote mechanism (Yoo et al.,
2007). Each vertex (vk) of a sentence (Oj) gives to
each cluster (Ci) a different number of votes (pi,j)
depending on whether the vertex belongs to HVS
or non-HVS (3).
</bodyText>
<equation confidence="0.99632475">
similarity(Ci, Oj) = � wk,j (3)
vk|vkEOj
�
wk,j=0 si vk�ECi
</equation>
<bodyText confidence="0.848127">
where
</bodyText>
<equation confidence="0.755111">
wk,j=1.0,si vk∈HV S(Ci)
wk,j=0.5,si vkOHV S(Ci)
</equation>
<subsectionHeader confidence="0.999252">
4.3 Sentence Selection
</subsectionHeader>
<bodyText confidence="0.99865525">
The last step consists of selecting significant sen-
tences for the summary, based on the similarity
between sentences and clusters. We investigated
three alternatives for this step.
</bodyText>
<listItem confidence="0.979820888888889">
• Heuristic 1: For each cluster, the top ni sen-
tences are selected, where ni is proportional
to its size.
• Heuristic 2: We accept the hypothesis that
the cluster with more concepts represents the
main theme in the document, and select the
top N sentences from this cluster.
• Heuristic 3: We compute a single score for
each sentence, as the sum of the votes as-
</listItem>
<bodyText confidence="0.9972255">
signed to each cluster adjusted to their sizes,
and select the N sentences with higher scores.
</bodyText>
<sectionHeader confidence="0.998613" genericHeader="evaluation">
5 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.9999534">
In order to evaluate the method, we analyze the
summaries generated by the three heuristics over
a document4 from the BioMed Central Corpus,
using a compression rate of 20%. Table 1 shows
the sentences selected along with their scores.
Although results are not statistically significant,
they show some aspects in which our method be-
haves satisfactorily. Heuristics 1 and 3 extract sen-
tence 0, and assign to it the higher score. This
supports the positional criterion of selecting the
first sentence in the document, as the one that con-
tains the most significant information. Sentence 58
represents an example of sentence, situated at the
end, which gathers the conclusions of the author.
In general, these sentences are highly informative.
Sentence 19, in turn, evidences how the method
systematically gives preference to long sentences.
Moreover, while summaries by heuristics 1 and 3
have a lot of sentences in common (9 out of 12),
heuristic 2 generates a summary considerably dif-
ferent and ignores important topics in the docu-
ment. Finally, we have compared these summaries
with the author’s abstract. It can be observed that
heuristics 1 and 3 cover all topics in the author’s
abstract (see sentences 0, 4, 15, 17, 19, 20 and 25).
</bodyText>
<footnote confidence="0.99497">
4BioMed Central: www.biomedcentral.com/content/
download/xml/cvm-2-6-254.xml
</footnote>
<page confidence="0.988136">
55
</page>
<table confidence="0.99811725">
Sentences 0 4 19 58 7 28 25 20 21 8 43 15
Heuristic 1 99.0 20.0 19.0 18.5 17.0 16.5 16.0 15.5 15.5 13.5 13.5 12.0
Heuristic 2 19.0 16.5 15.5 12.5 12.0 10.5 9.0 9.0 7.5 7.0 7.0 7.0
Heuristic 3 98.8 18.7 17.9 16.3 15.3 14.5 13.4 13.0 13.0 12.7 12.7 12.2
</table>
<tableCaption confidence="0.999884">
Table 1: Results
</tableCaption>
<bodyText confidence="0.9955905">
As far as heuristic 2 is concerned, it does not cover
adequately the information in the abstract.
</bodyText>
<sectionHeader confidence="0.998467" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999490814814815">
In this paper we introduce a method for summa-
rizing biomedical literature. We represent the do-
cument as an ontology-enriched scale-free graph,
using UMLS concepts and relations. This way we
get a richer representation than the one provided by
a vector space model. In section 5 we have evalu-
ated several heuristics for sentence extraction. We
have determined that heuristic 2 does not cover all
relevant topics and selects sentences with a low rel-
ative significance. Conversely, heuristics 1 and 3,
present very similar results and cover all important
topics.
Nonetheless, we have identified several prob-
lems and some possible improvements. Firstly, as
our method extracts whole sentences, long ones
have higher probability to be selected, because
they contain more concepts. The alternative could
be to normalise the sentences scores by the number
of concepts. Secondly, concepts associated with
general semantic types in UMLS, as functional
concept, temporal concept, entity and language,
could be ignored, since they do not contribute to
distinguish what sentences are significant.
Finally, in order to formally evaluate the method
and the different heuristics, a large-scale evalua-
tion on the BioMed Corpus is under way, based on
computing the ROUGE measures (Lin, 2004).
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999132">
This research is funded by the Ministerio de Edu-
caci´on y Ciencia (TIN2006-14433-C02-01), Uni-
versidad Complutense de Madrid and Direcci´on
General de Universidades e Investigaci´on de la Co-
munidad de Madrid (CCG07-UCM/TIC 2803).
</bodyText>
<sectionHeader confidence="0.999253" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994589648648648">
Aronson A. R. Effective Mapping of Biomedical Text
to the UMLS Metathesaurus: The MetaMap Pro-
gram. 2001. In Proceedings of American Medical
Informatics Association.
Barabasi A.L. and Albert R. Emergence of scaling in
random networks. 1999. Science,286–509.
Brandow R. and Mitze K. and Rau L. F. Automatic
Condensation of Electronic Publications by Sen-
tence Selection. 1995. Information Processing and
Management,5(31):675–685.
Edmundson H.P. New Methods in Automatic Extract-
ing. 1969. Journal of the Association for Computing
Machinery,2(16):264–285.
Erkan G. and Radev D. R. LexRank: Graph-based
Lexical Centrality as Salience in Text Summariza-
tion. 2004. Journal of Artificial Intelligence Re-
search (JAIR),22:457–479.
Kupiec J. and Pedersen J.O. and Chen F. A Trainable
Document Summarizer. 1995. In Proceedings of
the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval,68–73.
Lin C-Y. ROUGE: A Package for Automatic Eval-
uation of Summaries. 2004. In Proceedings of
Workshop on Text Summarization Branches Out,
Post-Conference Workshop ofACL 2004, Barcelona,
Spain.
Luhn H.P. The Automatic Creation of Literature
Abstracts. 1958. IBM Journal of Research
Development,2(2):159–165.
Sparck-Jones K. Automatic Summarizing: Factors and
Directions. 1999. I. Mani y M.T. Maybury, Advances
in Automatic Text Summarization. The MIT Press.
Yoo I. and Hu X. and Song I.Y. A coherent graph-based
semantic clustering and summarization approach for
biomedical literature and a new summarization eval-
uation method. 2007. BMC Bioinformatics,8(9).
</reference>
<page confidence="0.99833">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.363260">
<title confidence="0.9983405">Concept-graph based Biomedical Automatic Summarization using Ontologies</title>
<author confidence="0.844091">Laura Plaza Alberto Diaz Pablo</author>
<affiliation confidence="0.9805">Universidad Complutense de</affiliation>
<address confidence="0.863313">C/Profesor Jos´e Garc´ıa Santesmases, s/n, Madrid 28040,</address>
<email confidence="0.94141">lplazam@pas.ucm.es,albertodiaz@fdi.ucm.es,pgervas@sip.ucm.es</email>
<abstract confidence="0.9887189375">One of the main problems in research on automatic summarization is the inaccurate semantic interpretation of the source. Using specific domain knowledge can considerably alleviate the problem. In this paper, we introduce an ontology-based extractive method for summarization. It is based on mapping the text to concepts and representing the document and its sentences as graphs. We have applied our approach to summarize biomedical literature, taking advantages of free resources as UMLS. Preliminary empirical results are presented and pending problems are identified.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A R Aronson</author>
</authors>
<title>Effective Mapping of Biomedical Text to the UMLS Metathesaurus: The MetaMap Program.</title>
<date>2001</date>
<booktitle>In Proceedings of American Medical Informatics Association.</booktitle>
<contexts>
<context position="5614" citStr="Aronson, 2001" startWordPosition="830" endWordPosition="831">m has been implemented and tested on several documents from the corpus developed by BioMed Central2. As the preprocessing, text is split into sentences using GATE3, and generic words and high frequency terms are removed, as they are not useful in discriminating between relevant and irrelevant sentences. 4.1 Graph-based Document Representation This step consists in representing each document as a graph, where the vertices are the concepts in UMLS associated to the terms, and the edges indicate the relations between them. Firstly, each sentence is mapped to the UMLS Metathesaurus using MetaMap (Aronson, 2001). MetaMap allows to map terms to UMLS concepts, using n-grams for indexing in the ULMS Metathesaurus, and performing disambiguation to identify the correct 1NLM Unified Medical Language System (UMLS). URL: http://www.nlm.nih.gov/research/umls 2BioMed Central: http://www.biomedcentral.com/ 3GATE (Generic Architecture for Text Engineering): http://gate.ac.uk/ concept for a term. Secondly, the UMLS concepts are extended with their hyperonyms. Figure 1 shows the graph for sentence ”The goal of the trial was to assess cardiovascular mortality and morbidity for stroke, coronary heart disease and con</context>
</contexts>
<marker>Aronson, 2001</marker>
<rawString>Aronson A. R. Effective Mapping of Biomedical Text to the UMLS Metathesaurus: The MetaMap Program. 2001. In Proceedings of American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Barabasi</author>
<author>R Albert</author>
</authors>
<title>Emergence of scaling in random networks.</title>
<date>1999</date>
<note>Science,286–509.</note>
<marker>Barabasi, Albert, 1999</marker>
<rawString>Barabasi A.L. and Albert R. Emergence of scaling in random networks. 1999. Science,286–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brandow</author>
<author>K Mitze</author>
<author>L F Rau</author>
</authors>
<title>Automatic Condensation of Electronic Publications by Sentence Selection.</title>
<date>1995</date>
<booktitle>Information Processing and Management,5(31):675–685.</booktitle>
<contexts>
<context position="2434" citStr="Brandow et al., 1995" startWordPosition="345" endWordPosition="348">nse (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. of the sentences, we compute the centrality of their concepts in the document graph. 2 Previous Work Traditionally, automatic summarization methods have been classified in those which generate extracts and those which generate abstracts. Although human summaries are typically abstracts, most of existing systems produce extracts. Extractive methods build summaries on a superficial analysis of the source. Early summarization systems are based on simple heuristic features, as the position of sentences in the document (Brandow et al., 1995), the frequency of the words they contain (Luhn, 1958; Edmundson, 1969), or the presence of certain cue words or indicative phrases (Edmundson, 1969). Some advanced approaches also employ machine learning techniques to determine the best set of attributes for extraction (Kupiec et al., 1995). Recently, several graph-based methods have been proposed to rank sentences for extraction. LexRank (Erkan and Radev, 2004) is an example of a centroidbased method to multi-document summarization that assess sentence importance based on the concept of eigenvector centrality. It represents the sentences in </context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>Brandow R. and Mitze K. and Rau L. F. Automatic Condensation of Electronic Publications by Sentence Selection. 1995. Information Processing and Management,5(31):675–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New Methods in Automatic Extracting.</title>
<date>1969</date>
<journal>Journal of the Association for Computing</journal>
<volume>2</volume>
<issue>16</issue>
<contexts>
<context position="2505" citStr="Edmundson, 1969" startWordPosition="358" endWordPosition="359">d. of the sentences, we compute the centrality of their concepts in the document graph. 2 Previous Work Traditionally, automatic summarization methods have been classified in those which generate extracts and those which generate abstracts. Although human summaries are typically abstracts, most of existing systems produce extracts. Extractive methods build summaries on a superficial analysis of the source. Early summarization systems are based on simple heuristic features, as the position of sentences in the document (Brandow et al., 1995), the frequency of the words they contain (Luhn, 1958; Edmundson, 1969), or the presence of certain cue words or indicative phrases (Edmundson, 1969). Some advanced approaches also employ machine learning techniques to determine the best set of attributes for extraction (Kupiec et al., 1995). Recently, several graph-based methods have been proposed to rank sentences for extraction. LexRank (Erkan and Radev, 2004) is an example of a centroidbased method to multi-document summarization that assess sentence importance based on the concept of eigenvector centrality. It represents the sentences in each document by its tf*idf vectors and computes sentence connectivity </context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson H.P. New Methods in Automatic Extracting. 1969. Journal of the Association for Computing Machinery,2(16):264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>Radev D R LexRank</author>
</authors>
<title>Graph-based Lexical Centrality as Salience in Text Summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research</journal>
<marker>Erkan, LexRank, 2004</marker>
<rawString>Erkan G. and Radev D. R. LexRank: Graph-based Lexical Centrality as Salience in Text Summarization. 2004. Journal of Artificial Intelligence Research (JAIR),22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J O Pedersen</author>
<author>F Chen</author>
</authors>
<title>A Trainable Document Summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,68–73.</booktitle>
<contexts>
<context position="2726" citStr="Kupiec et al., 1995" startWordPosition="391" endWordPosition="394">ich generate abstracts. Although human summaries are typically abstracts, most of existing systems produce extracts. Extractive methods build summaries on a superficial analysis of the source. Early summarization systems are based on simple heuristic features, as the position of sentences in the document (Brandow et al., 1995), the frequency of the words they contain (Luhn, 1958; Edmundson, 1969), or the presence of certain cue words or indicative phrases (Edmundson, 1969). Some advanced approaches also employ machine learning techniques to determine the best set of attributes for extraction (Kupiec et al., 1995). Recently, several graph-based methods have been proposed to rank sentences for extraction. LexRank (Erkan and Radev, 2004) is an example of a centroidbased method to multi-document summarization that assess sentence importance based on the concept of eigenvector centrality. It represents the sentences in each document by its tf*idf vectors and computes sentence connectivity using the cosine similarity. Even if results are promising, most of these approaches exhibit important deficiencies which are consequences of not capturing the semantic relations between terms (synonymy, hyperonymy, homon</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Kupiec J. and Pedersen J.O. and Chen F. A Trainable Document Summarizer. 1995. In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin C-Y ROUGE</author>
</authors>
<title>A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop ofACL 2004,</booktitle>
<location>Barcelona,</location>
<marker>ROUGE, 2004</marker>
<rawString>Lin C-Y. ROUGE: A Package for Automatic Evaluation of Summaries. 2004. In Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop ofACL 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The Automatic Creation of Literature Abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="2487" citStr="Luhn, 1958" startWordPosition="356" endWordPosition="357">ghts reserved. of the sentences, we compute the centrality of their concepts in the document graph. 2 Previous Work Traditionally, automatic summarization methods have been classified in those which generate extracts and those which generate abstracts. Although human summaries are typically abstracts, most of existing systems produce extracts. Extractive methods build summaries on a superficial analysis of the source. Early summarization systems are based on simple heuristic features, as the position of sentences in the document (Brandow et al., 1995), the frequency of the words they contain (Luhn, 1958; Edmundson, 1969), or the presence of certain cue words or indicative phrases (Edmundson, 1969). Some advanced approaches also employ machine learning techniques to determine the best set of attributes for extraction (Kupiec et al., 1995). Recently, several graph-based methods have been proposed to rank sentences for extraction. LexRank (Erkan and Radev, 2004) is an example of a centroidbased method to multi-document summarization that assess sentence importance based on the concept of eigenvector centrality. It represents the sentences in each document by its tf*idf vectors and computes sent</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Luhn H.P. The Automatic Creation of Literature Abstracts. 1958. IBM Journal of Research Development,2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck-Jones</author>
</authors>
<title>Automatic Summarizing: Factors and Directions.</title>
<date>1999</date>
<journal>I. Mani</journal>
<publisher>The MIT Press.</publisher>
<marker>Sparck-Jones, 1999</marker>
<rawString>Sparck-Jones K. Automatic Summarizing: Factors and Directions. 1999. I. Mani y M.T. Maybury, Advances in Automatic Text Summarization. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Yoo</author>
<author>X Hu</author>
<author>I Y Song</author>
</authors>
<title>A coherent graph-based semantic clustering and summarization approach for biomedical literature and a new summarization evaluation method.</title>
<date>2007</date>
<journal>BMC Bioinformatics,8(9).</journal>
<contexts>
<context position="3517" citStr="Yoo et al., 2007" startWordPosition="508" endWordPosition="511">ocument summarization that assess sentence importance based on the concept of eigenvector centrality. It represents the sentences in each document by its tf*idf vectors and computes sentence connectivity using the cosine similarity. Even if results are promising, most of these approaches exhibit important deficiencies which are consequences of not capturing the semantic relations between terms (synonymy, hyperonymy, homonymy, and co-occurs and associatedwith relations). We present an extractive method for summarization which attempts to solve this deficiencies. Unlike researches conducted by (Yoo et al., 2007; Erkan and Radev, 2004), which cluster sentences to identify shared topics in multiple documents, in this work we apply clustering to identify groups 53 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 53–56 Manchester, August 2008 of concepts closely related. We hypothesize that each cluster represents a theme or topic in the document, and we evaluate three different heuristics to ranking sentences. 3 Biomedical Ontologies. UMLS Biomedical ontologies organize domain concepts and knowledge in a system of hierarchical and assoc</context>
<context position="7566" citStr="Yoo et al., 2007" startWordPosition="1143" endWordPosition="1146">o the concepts (Figure 1). Weights for the new edges are computed using expression (1). 4.2 Concept Clustering and Theme Recognition The second step consists of clustering concepts in the document graph, using a degree-based method (Erkan and Radev, 2004). Each cluster is composed by a set of concepts that are closely related in meaning, and can be seen as a theme in the document. The most central concepts in the cluster give the sufficient and necessary information related to its theme. We hypothesize that the document graph is an instance of a scale-free network (Barabasi, 1999). Following (Yoo et al., 2007), we introduce the salience of vertices. Mathematically, the salience of a vertex (vi) is calculated as follows. salience(vi) = weight(ej) (2) ej|∃vk∧ejconecta(vi,vk) Within the set of vertices, we select the n that present the higher salience and iteratively group them in Hub Vertex Sets (HVS). A HVS represents a group of vertices that are strongly related to each other. The remaining vertices are 54 Figure 1: Sentence graph assigned to that cluster to which they are more connected. Finally, we assign each sentence to a cluster. To measure the similarity between a cluster and a sentence graph</context>
</contexts>
<marker>Yoo, Hu, Song, 2007</marker>
<rawString>Yoo I. and Hu X. and Song I.Y. A coherent graph-based semantic clustering and summarization approach for biomedical literature and a new summarization evaluation method. 2007. BMC Bioinformatics,8(9).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>