<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994667">
Improving Machine Translation Quality with Automatic Named
Entity Recognition
</title>
<author confidence="0.976704">
Bogdan Babych Anthony Hartley
</author>
<affiliation confidence="0.9936775">
Centre for Translation Studies Centre for Translation Studies
University of Leeds, UK University of Leeds, UK
Department of Computer Science a.hartley@leeds.ac.uk
University of Sheffield, UK
</affiliation>
<email confidence="0.992163">
bogdan@comp.leeds.ac.uk
</email>
<sectionHeader confidence="0.995566" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998280625">
Named entities create serious problems
for state-of-the-art commercial machine
translation (MT) systems and often cause
translation failures beyond the local
context, affecting both the overall
morphosyntactic well-formedness of
sentences and word sense disambiguation
in the source text. We report on the
results of an experiment in which MT
input was processed using output from
the named entity recognition module of
Sheffield&apos;s GATE information extraction
(IE) system. The gain in MT quality
indicates that specific components of IE
technology could boost the performance
of current MT systems.
</bodyText>
<sectionHeader confidence="0.998352" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999639818181818">
Correct identification of named entities (NEs) is
an important problem for machine translation
(MT) research and for the development of
commercial MT systems. In the first place,
translation of proper names often requires
different approaches and methods than
translation of other types of words (Newmark,
1982: 70-83). Mistakenly translating NEs as
common nouns often leads to
incomprehensibility or necessitates extensive
post-editing. In many cases failure to correctly
identify NEs has an effect not only on a local and
immediate context, but also on the global
syntactic and lexical structure of the translation,
since proper segmentation of a source text might
be seriously distorted.
However, the developers of commercial MT
systems often pay insufficient attention to correct
automatic identification of certain types of NE,
e.g., organisation names. This is due partly to the
greater complexity of this problem (the set of
proper names is open and highly dynamic), and
partly to the lack of time and other development
resources.
On the other hand, the problem of correct
identification of NE is specifically addressed and
benchmarked by the developers of Information
Extraction (IE) systems, such as the GATE
system, created at the University of Sheffield and
distributed under GPL (Cunningham et al., 1996,
2002). The quality of automatic NE identification
has been evaluated at several message-
understanding conferences (MUC) sponsored by
DARPA. Accuracy scores for leading systems
are relatively high (in comparison to other IE
tasks, such as co-reference resolution, template
element filling or scenario template filling). The
default settings of NE recognition module of the
GATE system produces between 80-90%
Precision &amp; Recall on news texts (Cunningham
et al., 2002).
In this paper we describe the effect of using
the GATE NE recognition module as a pre-
processor for commercial state-of-the-art MT
</bodyText>
<page confidence="0.975643">
1
</page>
<bodyText confidence="0.999965392857143">
systems. The idea of our experiment is that high-
quality automatic NE recognition, produced by
GATE, could be used to create do-not-translate
(DNT) lists of organisation names, a specific type
of NE which in human translation practice is
often left untranslated. (Newmark, 1982: 70-83).
In our experiment we systematically analysed
the effect of incorrect NE recognition on the
surrounding lexical and morphosyntactic context
in MT output. We tried to establish how far NE
recognition (specifically recognition of
organisation names) influences grammatical
well-formedness and word sense choices in the
context of NEs. We compared the baseline
translations (produced without NE DNT-
processing) with translations produced using
DNT lists (created with the GATE-1 NE
recognition system), by systematically scoring
cases of improvement and decline in lexical and
morphosyntactic well-formedness. Texts with NE
DNT-processing showed consistent improvement
for all systems in comparison with baseline
translations. The improvement was not lower
than 20%.
This indicates that combining present-day MT
systems with specific TE modules (where certain
NLP problems are treated systematically) has
beneficial effect on the overall MT quality.
</bodyText>
<sectionHeader confidence="0.410066" genericHeader="method">
2. Problems of NEs for MT
</sectionHeader>
<bodyText confidence="0.991778609756097">
NEs usually require different approaches to
translation than do other types of words. For
example, foreign person names in Russian should
be transcribed and written in Cyrillic; names that
coincide with common nouns should not be
looked up in the general dictionary. In some
cases NEs (mostly organisation names) are not
translated and preserve Roman orthography
within Russian Cyrillic text. For example, in a
1000-word selection of 4 articles about the
international economy on the Russian BBC
World Service site, Roman-script NEs within the
Cyrillic text covered 6% of the selection. The
following NEs were neither translated, nor
transliterated into Cyrillic: &apos;Nestle&apos; (9
occurrences), &apos;AOL&apos; (8); &apos;Buffalo Grill&apos; (7);
&apos;Burger King&apos; (7); `Diageo&apos; (7); `Schweisfurth
(Group)&apos; (2). In general, the practice not to
translate organisation names is very common for
translations into Slavic languages.
Mistakes related to the failure to distinguish
between common nouns and proper nouns in MT
can be very serious. For example, in our
experiments an MT system translated the person
name Ray as Ilyu (beam of light). Translating
parts of compound NEs is also detrimental to MT
quality, since it often involves incorrect
segmentation of NEs: American Telephone and
Telegraph Corp. was translated as
Ameptucaucicuii TelleOwl u Kavinauus Ta7eepa0a
(&apos;an American telephone and a company of a
telegraph). Yet another problem for MT systems
is that failure to recognise NEs often has a
negative effect on well-formedness of
morphosyntactic and lexical context beyond the
NEs themselves. Certain morphological features
of neighbouring and syntactically related words,
word order, a choice of word senses in MT
output could be distorted if a NE is not correctly
recognised. For example, an English phrase (1)
was translated into Russian as (2):
</bodyText>
<listItem confidence="0.8700486">
(1) Original: Eastern Airlines executives
notified union leaders ...
(2) MT output: Boonouuble ucnaimumem
Antall111211.4 yeeoommzu npoOccno3usix
pyicoeodume.neu
</listItem>
<bodyText confidence="0.983639142857143">
(&apos;Oriental executives of the Airlines notified
This happened because the failure to identify
Eastern Airlines as a NE led to incorrect
syntactic segmentation of the sentence.
However, current MT systems allow the
processing of MT input with DNT lists. Making a
DNT of organisation names from the text in most
cases improves not only the acceptability of NE
translation, but also the overall well-formedness
of the morphosyntactic and lexical context. For
example, after the string Eastern Airlines was
entered into a DNT list for the English-Russian
MT system, the translation of (1) was
morphologically and syntactically well-formed:
</bodyText>
<listItem confidence="0.9455">
(3) DNT-processed MT output: Hcnannume.gu
Eastern Airlines yeeaamtou npoOcolombix
pproeodumemit
</listItem>
<bodyText confidence="0.9963286">
Creating DNT lists manually requires much
effort from the user of an MT system. However,
the high accuracy in NE tagging of current 1E
systems, including GATE, means that DNT lists
for MT can be created automatically.
</bodyText>
<page confidence="0.988286">
2
</page>
<bodyText confidence="0.995461333333333">
The performance results reported here are
based entirely on automatically created DNT lists
used to process NEs.
</bodyText>
<sectionHeader confidence="0.402832" genericHeader="method">
3. Description of the experiment
</sectionHeader>
<bodyText confidence="0.894484833333333">
In order to measure the effect of NE recognition
on MT quality, we took 30 texts (news articles)
from the DARPA MUC-6 evaluation set. These
texts were selected because they are relatively
rich in NEs, and because clean NE annotation is
available for them. We used the following
linguistic resources of the Sheffield NLP group:
DARPA &apos;keys&apos; — texts manually annotated
with NEs;
GATE &apos;responses&apos; — the output of the
automatic NE annotation of the GATE-1
system, which participated in MUC-6.
</bodyText>
<tableCaption confidence="0.6947262">
Table 1 summarises statistical parameters of
this corpus. The table indicates how frequently
NEs (organisation names) occur and shows that
GATE &apos;response&apos; figures are very close to the
DARPA &amp;quot;key&amp;quot; figures.
</tableCaption>
<table confidence="0.9980585">
Number of: For the Av. per Av. per Av. per
corpus doc. para. sent.
Paragraphs 283 9.4
Sentences 565 18.8 2.0
Word 11975 399.2 42.3 21.2
occurrences
Different 3944 235.7 36.3 19.7
words
NE 544/ 18.1/ 1.9/ 1.0/
occurrences 510 17.0 1.8 0.9
keys/ GATE
Different 201/ 7.6/ 1.5/ 0.9/
NEs: keys/ 174 6.7 1.4 0.8
GATE
</table>
<tableCaption confidence="0.999852">
Table 1: Statistical parameters of the corpus
</tableCaption>
<bodyText confidence="0.894749">
The density of NEs in the DARPA corpus is
also characterised by Table 2:
</bodyText>
<tableCaption confidence="0.983933">
Table 2: NE density in the corpus
</tableCaption>
<bodyText confidence="0.991197">
The accuracy of GATE-1 in the NE recognition
task at MUC-6 (Recall — 84%, Precision — 94%,
Precision &amp; Recall — 89.06 % (Gaizauskas et al.,
1995)) is such that we used the GATE output for
our MT experiment, rather than the cleaner
manually annotated data. Moreover, the
advantage of using automatic NE recognition is
that the results of the experiment should be
consistent with the results for other corpora on
which the NE recognition task has been
performed.
Having automatically generated DNT lists of
organisation names from GATE &apos;response&apos;
annotation, we translated the texts using three
commercial MT systems:
- English-Russian `ProMT 98&apos; v4.0, released
in 1998 (Softissimo)
English-French &apos;ProMT (Reverso) v5.01,
released in 2001 (Softissimo)
English-French Systran Professional
Premium&apos; v3 .Ob, released in 2000 (Systran)
Two translations were generated by each MT
system:
</bodyText>
<listItem confidence="0.960193">
- a baseline translation without a DNT list
a DNT-processed translation with the
automatically created DNT list of
organisation names
</listItem>
<bodyText confidence="0.99978625">
The baseline translations were then compared
with DNT-processed translations, with respect to
the morphosyntactic well-formedness of the
context surrounding the NEs.
</bodyText>
<subsectionHeader confidence="0.993121">
3.1. Segmentation
</subsectionHeader>
<bodyText confidence="0.993738466666667">
To speed-up the process of finding contextual
differences, we developed automatic tools, which
allowed us to make a formal distinction between
NE-internal and NE-external issues in MT.
Whereas Al-Onaizan and Knight (2002) focus on
the former issue, our primary interest is in NE-
external differences in context caused by
improved NE recognition after DNT-processing.
Thus, we automatically selected paragraphs with
contextual differences and highlighted different
strings in these paragraphs.
The example below illustrates the output of
these annotation tools:
Different strings found in two translations are
indicated by
</bodyText>
<listItem confidence="0.9966204">
- &apos;ORI&apos; indicates the original English string in
the DARPA corpus;
- `TWS&apos; (baseline translation) indicates a
String Translated Without the do-not-
translate list;
</listItem>
<figure confidence="0.9047444">
Manual keys GATE
Paragraphs 228 (80.6%) 218 (77.0%)
with NEs
Sentences 329 (58.2%) 315 (55.8%)
with NEs
</figure>
<page confidence="0.910672">
3
</page>
<listItem confidence="0.951223">
- &apos;TDS&apos; (DNT-processed translation) indicates
a String Translated with Do-not-translate list.
</listItem>
<construct confidence="0.86950475">
----&gt;40;TDSnotInTWS: 40# OTTtenbH0, B ero perocrparmo
----&gt;40;TDSnotInTWS: pacKpbui geTanu ero nnaHoB
4:114HaHCIIp0BaH1451 nplio6porem451
40;ORI=40#&lt;s&gt; Separately, in its &lt;ENAMEX
TYPE=&amp;quot;ORGANIZATION&amp;quot;&gt;SEC&lt;/ENAMEX&gt; filing,
&lt;ENAMEX
TYPE=&amp;quot;ORGANIZATION&amp;quot;&gt;USAir&lt;/ENAMEX&gt; disclosed
details of its plans for financing the &lt;ENAMEX
TYPE=&amp;quot;ORGANIZATION&amp;quot;&gt;Piedmont&lt;/ENAMEX&gt;
acquisition.
40;TWS= 40# aukernmo, B ee perocrparmll CEKYI-ELIM,
USAir paCKpbITbIe Beranri ee IlllaHOB cillulaHcopoBaorm
llpeBropooro npoo6peTera4B.
40;TDS= 40# OTABAbH0, B ero per ncrpaunn SEC, USAir
pacicpbui peraan ero ILTIBHOB 4114HBHCIVOBBHIBI
npuooperenun Piedmont.
</construct>
<bodyText confidence="0.999937315789474">
Since the amount of manual annotation was
relatively small, no complex alignment for the
two translated texts was implemented. Instead,
we implemented a simple segmentation
algorithm for paragraphs, using NE annotation in
the corpus.
The segmentation was done in two stages.
First, tagged NEs from the &apos;ORI&apos; paragraph were
identified and searched for in the &apos;TDS&apos;
paragraph. Then they were used as separators for
the TDS: parts of the TDS between (untranslated)
NEs were identified and searched for in the
`TWS&apos; paragraph. If any sub-string was not
found in TWS, it was printed and also
highlighted in bold in TDS. This shows that
strings in the context of the NE are different in
the DNT-processed translation and in the
baseline translation. This difference was then
manually scored.
</bodyText>
<subsectionHeader confidence="0.989318">
3.2. Scoring
</subsectionHeader>
<bodyText confidence="0.999332133333333">
Contextual differences between the baseline
translation and the DNT-processed translation
were manually scored using the scale in Table 3.
The terms &apos;well-formed&apos; and &apos;not well-
formed&apos; refer to the local morphosyntactic or
lexical context within a segment where
differences occur. It remains possible that well-
formed structures require post-editing at a higher
level in the translated text.
The term &apos;features&apos; refers to morphosyntactic
or lexical features of certain words in the context
of the NE. By &apos;more correct&apos;, we mean that the
features considered in the context are correct, but
the corresponding features in the compared text
are wrong.
</bodyText>
<table confidence="0.999756545454546">
Score Baseline translation DNT-processed
translation
+ 1 not well-formed well-formed
+ 0.5 not well-formed; not well-formed;
some features are
more correct
= 0 equally (not) well-formed
— 0.5 not well-formed; not well-formed
some features are
more correct
— 1 well-formed not well-formed
</table>
<tableCaption confidence="0.987301">
Table 3: Scoring scheme
</tableCaption>
<table confidence="0.94297422">
Here are some example strings to illustrate
each score:
+1 Original:
(It) represents 4,400 Western Union employees
around the country.
Baseline translation:
(OH) npeAcTairmeT 4,400 3anammix capnaulnx
Com3a no Beeil crpaue.
(&apos;It represents 4,400 Western employees of the
Union around the country)
DNT-processed translation:
(0o) npeAcTamBeT 4,400 eaywaulux Western
Union no Beal crpaue.
(&apos;(It) represents 4,400 employees of Western
Union around the country&apos;)
+0.5 Original:
Western Union Corp. said its subsidiary, Western
Union Telegraph Co....
Baseline translation:
3ana4mB KopnopauBB ColoBa cKa3a.na ee
BenomoraTe.ru.nym, 3anammo Komnallmo
Temerpacim
(&apos;Western Corporation of a Union said its
auxiliary (case.acc.), Western Company of
Telegraph of a Union ...&apos;)
DNT-processed translation:
Western Union Corp. CKa3auni.di ero dflumaa,
Western Union Telegraph Co. ...
(&apos;Western Union Corp. Its branch (case.nom) is
said, Western Union Telegraph Co....&apos;)
4
figure in row 2. These figures show the likely
=0 Original: reliability of the results for manual scoring
American Airlines Calls for Mediation presented in the next section.
Baseline translation:
Amernixaxciclie ABIlaJ111111114 ilimminamT K
nocpe,arnmecamy
(American Airlines Call(num.plur.) for Mediation)
DNT-processed translation:
American Airlines lip1131.maeT K
nocpeaungecmy
(American Airlines Calls(num.sing.) for Mediation)
—0.5 Original:
USAir said that William R. Howard, chairman and
chief executive of Piedmont, will be elected
president of USAir
Baseline translation:
USAir cicama TOT Y1111b5IM P. Fosap,a,
npe,ace,laremb H pyKOBOAHTeJlb IIKEITOpHbIX,
6y,LIyT 1436parim ripeallaeirrom USAIR
</table>
<tableCaption confidence="0.200158333333333">
USAir said that (particular) (demonstr.pron,nom.)
William R. Howard, chairman and chief executive
of piedmont people, will be elected president of
</tableCaption>
<figure confidence="0.5097542">
USAir
DNT-processed translation:
USAir exaaaa TOTO Yitabsrma Pa. Fosap,a,
npe,acegareni, 14 pyxosoariTe.ab Piedmont, 6yayr
1436parna rrpenraeirrom USAir
</figure>
<figureCaption confidence="0.936032666666667">
USAir said of that (particular) (demonstr.pron,gen.)
William Ra. Howard, chairman and chief executive
of Piedmont, will be elected president of USAir
</figureCaption>
<figure confidence="0.797135454545455">
—1 Original:
to discuss the benefits of combining TWA and
USAir
Baseline translation:
rrro6m 06Cy4VITb BEIrogm OT o613eminenna TWA
a USAIR
(&apos;to discuss the benefits of the merge (noun) (of)
TWA and USAir&apos;)
DNT-processed translation:
rITO6b1 06C314111Tb BbITO,L1b1 OT o6i.eminsuomerocsi
TWA 14 USAir
</figure>
<bodyText confidence="0.980886928571429">
(&apos;to discuss the benefits of the combining
(participle, sing.) TWA and (of) USAir&apos;)
For each MT system, we scored 50 strings
showing differences. Table 4 summarises the
number of paragraphs with contextual differences
between the baseline and DNT-processed
translations.
The figures in row 2 — Paragraphs with
contextual differences — show to what extent
DNT-processing affects the NE context for each
system, showing also the percentage of these
paragraphs in relation to the corresponding figure
in row 1. Row 3 represents the percentage of
manually scored paragraphs in relation to the
</bodyText>
<table confidence="0.998749722222222">
Number of: Original MT E-R MT E-F MT E-F
— GATE ProMT ProMT Systran
Paras. with 218 225 225 239
NE
Paras. with 139 132 207
contextual (61.8%) (58.7%) (86.6%)
differences
Paras. 31 28 30
manually (22.3%) (21.2%) (14.5%)
scored
Strings with 211 212 411
differences
Strings 50 50 50
scored (23.7%) (23.6%) (12.2%)
Diff. strings 7.0 7.0 13.7
per text
Dll:i paras. 4.6 4.4 6.9
per text
</table>
<tableCaption confidence="0.999958">
Table 4: Paragraphs with contextual differences
</tableCaption>
<bodyText confidence="0.999949833333333">
Note that in row 1 there is a mismatch between
the number of paragraphs with NEs in the
original GATE-annotated English texts (218) and
in the translations produced by the three MT
systems (225, 225 and 239 paragraphs with NEs).
This is because the results of NE pre-processing
could be submitted to the proprietary MT
systems only in the form of a DNT list, which
has its limitations. The most serious potential
problem is over-generation: ambiguous items,
which could be either NEs or common words in
different contexts, are treated as NEs in every
context, once they are written to the DNT list.
For example, the word Labour could be either an
organisation name (&apos;the party&apos;), a part of a larger
NE, often of a type other than organisation name
(Federal Railway Labour Act), or a common
noun (&apos;work&apos;, as in the phrase: rise in labour
costs). As a result, in the translated corpus there
are more NEs than in the original English corpus,
annotated with GATE. This is reflected in the
figures presented in row 1 of Table 2.
Nevertheless, the difference is relatively low
(less then 10% for the worst case). Given that
there are (on average) only about 2 NE
occurrences per paragraph in the corpus, over-
generation does not greatly affect our evaluation
results.
The segmentation method described above
provided us with a clear formal distinction
</bodyText>
<page confidence="0.984993">
5
</page>
<bodyText confidence="0.9999004375">
between NE-internal and NE-external problems
for MT. However, we made one exception to this
distinction: in the DNT-processed English-
French, Systran often incorrectly inserts definite
articles for organisation names which are present
in DNT list, but does not do so in the baseline
translation. Our segmentation method treats these
articles as part of the morphosyntactic context of
NEs, and considerably increases the contextual
degradation figures for Systran. But,
linguistically, it is more correct to treat French
articles as inner parts of NEs. Therefore, for the
evaluation of contextual changes for Systran, we
ignored strings where the inserted article was the
only difference. As a result, Systran showed a net
contextual improvement.
</bodyText>
<sectionHeader confidence="0.974433" genericHeader="evaluation">
4. Results of the experiment
</sectionHeader>
<bodyText confidence="0.999206833333333">
Table 5 summarises the results of the manual
annotation of 50 strings containing differences
for each MT system. (There are 61 scored
differences for Systran, because in some strings
there was more then one morphosyntactic or
lexical difference).
</bodyText>
<table confidence="0.9988742">
ProMT 1998 ProMT 2001 Systran 2000
E-R E-F E-F
Mark N Score N Score N Score
+1* 28— +28.0 23— +23.0 18— +18.0
+0.5* 2 = +1.0 5 = + 2.5 24 = + 12.0
0* 4= 0 7= 0 8= 0
—0.5* 3 = —1.5 1 = —0.5 1 = —0.5
—1* 13 = —13.0 14 = — 14.0 10 = — 10.0
1 50 +14.5 50 + 11.0 61 +19.5
Gain +29% +22% +32%
</table>
<tableCaption confidence="0.998053">
Table 5: Manual annotation results
</tableCaption>
<bodyText confidence="0.999792666666667">
N is the number of differences, annotated with
that particular score. To compute the overall
score for the system we multiplied the scores by
the number of strings with this particular score,
and added the results. The improvement was then
computed by dividing the overall score by the
number of scored differences: Lscore / N.
In order to see how the resulting scores change
when more data is analysed, we continued
scoring the English Russian ProMT 98 system,
until 100 paragraphs with differences had been
annotated. The results are presented in Table 6.
</bodyText>
<table confidence="0.9947911">
ProMT 1998
E-R
Mark N Score
+1* 59= +59.0
+0.5* 8= +4.0
0* 6= 0
—0.5* 7 = —3.5
—1* 31 = —31.0
1 111 +28.5
Gain +26%
</table>
<tableCaption confidence="0.996064">
Table 6: Results for additional E-R data
</tableCaption>
<bodyText confidence="0.9977716">
We give an example of a sentence where
improvement has been achieved in the DNT-
processed translation for all three MT systems on
several levels: morphological, syntactic and
lexical.
</bodyText>
<table confidence="0.540156181818182">
Original:
The agreement was reached by a coalition of four
of Pan Am&apos;s five unions.
Baseline translation:
Cornameume 6bum Aoci-HrHyTo Koannumeil
geTbipex KaCTp101114 1131Tb com3oB Ama.
(&apos;The agreement was reached by a coalition of
four of a Saucepan five unions of Am.&apos;)
DNT-processed translation:
Cornamame 6bum apcmrHyTo Koannumeil
BeThipex 113 1151T11 C01030B Pan Am.
(&apos;The agreement was reached by a coalition of
four out of five unions of Pan Am&apos;)
Baseline translation:
L&apos;accord a ete atteint par une coalition de quatre
de casserole cinq unions d&apos;Am.
(`The agreement was reached by a coalition of
four of saucepan five unions of Am.&apos;)
DNT-processed translation:
L&apos;accord a ete atteint par tine coalition de quatre
de cinq unions de Pan Am.
(`The agreement was reached by a coalition of
four of five unions of Pan Am.&apos;)
Baseline translation:
L&apos;accord a ete conclu par une coalition de quatre
de la casserole etais cinq syndicats.
(`The agreement was reached by a coalition of
four of the saucepan was five trades-unions.&apos;)
DNT-processed translation:
L&apos;accord a ete conclu par une coalition de quatre
de Pan Am&apos;s cinq syndicats.
(`The agreement was reached by a coalition of
four of Pan Am&apos;s five trades-unions.&apos;)
</table>
<bodyText confidence="0.993174">
Here are further typical cases of morphosyntactic
improvement in the translated material:
</bodyText>
<figure confidence="0.573974909090909">
E-R
ProMT
E-F
ProMT
E-F
Systran
6
Improved syntactic segmentation:
Original:
Representatives for the 5,400-member Allied
Pilots Association didn&apos;t return phone calls.
E-R Baseline translation:
ProMT IlpeAcTaHHTerm ,E1.1151 C07037111qeCKTIX HICIOMOG C
5,400 gilettavai Accoyucalust He Ho3Hpaulanli
ofipauleHHH no Tenec])oHy.
(&apos;Representatives for the Allied Pilots with 5,400
members Association didn&apos;t return phone calls.&apos;)
DNT-processed translation:
IlpeacTaHrrrenri all51 Allied Pilots Association c
5,400 4neHam14 He Ho3Hpauranui o6paureHH5I no
TenecboHy.
Representatives for the Allied Pilots Association
with 5,400-members didn&apos;t return phone calls.
Improved proper / common disambiguation:
Original:
A spokesman for the company said American
officials &apos;felt that ...&apos;
E-F Baseline translation:
ProMT Un porte-parole de la societe a dit que les
fonctionnaires americains `ont estime que
CA spokesman for the company said that the
American [US] officials &apos;felt that ... &amp;quot;)
DNT-processed translation:
</figure>
<bodyText confidence="0.96568975">
Un porte-parole de la societe a dit que les
fonctionnaires d&apos;American `ont estime que
CA spokesman for the company said that the
officials of American &apos;felt that ...&amp;quot;)
</bodyText>
<figure confidence="0.876290708333333">
Improved word order:
Original:
USAir disclosed details of its plans for financing
E-F Baseline translation:
ProMT USAir les details revel&amp; de ses plans pour
financer
(&apos;USAir the details revealed (Past participle) of its
plans for financing ...&apos;)
DNT-processed translation:
USAir a revele les details de ses plans pour
financer
(&apos;USAir revealed (Verb) the details of its plans for
financing ...&apos;)
Improved lexical or syntactic disambiguation:
Original:
TWA stock closed at $28 ...
E-F Baseline translation:
Systran Ferme courant de TWA a $28 ...
(&apos;Closed (Past participle) current (Noun/Present
participle) of TWA at $28 ...&apos;)
DNT-processed translation:
L &apos;action de TWA s &apos;est fermee a $28 ...
(&apos;The stock of TWA closed (Verb) at $28 ...&apos;)
Original:
</figure>
<reference confidence="0.673181142857143">
National Mediation Board is expected to release
Pan Am Corp. and its Teamsters union from their
long-stalled contract negotiations.
E-R Baseline translation:
ProMT HarmoHarn,Hoe Ilparoemie FlocpeAmmecrna,
Kai( rnicH4aeTc51, estnycmum KacTpromo -
K0p110pari1451 H ee C01-03 BoAurreneii OT HX gariro-
OCTaHOBJleHHMX fleper0B0p0B KoHrnaxTa.
(National Mediation Board is expected to release
[put on the market] a Saucepan - Corporation and
its union of drivers from their long-stalled
contract negotiations.&apos;)
DNT-processed translation:
National Mediation Board, Kai&lt; MICH,LkaeTC51,
oceo6oaum Pan Am Corp. 14 ero C0103
Teamsters OT HX AaTIFO-OCTaHOBJ1eHHbIX
neperoHopos Korirnarcra.
`National Mediation Board is expected to release
[make free] Pan Am Corp. and its Teamsters
union from their long-stalled contract
negotiations.&apos;)
</reference>
<sectionHeader confidence="0.996153" genericHeader="conclusions">
5. Conclusions
</sectionHeader>
<bodyText confidence="0.999891333333333">
The results indicate that combining IE
technology with MT has a great potential for
improving the state-of-the art in output quality.
Taking advantage of efforts to resolve specific
linguistic problems — as has happened with NE
recognition within the IE framework — improves
not only the treatment of that phenomenon by
MT, but also morphosyntactic and lexical well-
formedness more generally in the wider context
of the target, thus boosting the overall quality of
MT. Our results show that modern MT systems
still leave room to achieve a considerable
improvement. Further gains in performance may
be anticipated by harnessing other focussed
technologies, such as word sense disambiguation,
to MT.
We noted also that the scale of the
improvement for particular MT systems
correlates with the baseline quality of MT: it is
more difficult to achieve improvement for a
system which produces high-quality well-formed
structures without DNT-processing. The
improvement which is possible with NE DNT-
processing is lowest for the English-French
ProMT (Reverso) system. This system was
ranked higher than English-French Systran by
human evaluators in an experiment conducted by
(Rajman and Hartley, 2001) using data from
DARPA&apos;s 1992-1994 series of MT evaluations
(White, et al, 1994). These human evaluations
</bodyText>
<page confidence="0.997203">
7
</page>
<bodyText confidence="0.999987302325582">
confirmed the ranking predictions of an
automatic evaluation algorithm which correlated
the fluency, adequacy and informativeness scores
awarded by human evaluators to the DARPA
corpus with syntactic and semantic attributes of
the corpus. In this respect, the measures of
contextual improvement after DNT-processing
with lists of NEs (organisation names) produced
by GATE could be seen as a possible evaluation
score for MT systems, which could lead to
establishing a reliable quality scale for MT
systems.
Future work will look at the sensitivity of the
performance gain to corpus size and variation.
Table 6 shows that the difference in the score for
50 annotated paragraphs and the score for 100
paragraphs for E-R ProMT98 is 3%. In general,
different occurrences of the same NE tend to
have a similar morpho syntactic context, so they
constantly tend to either improve or worsen the
quality. In a particular text, the same NEs tend to
re-occur. As a result, an improvement or a
decline in quality is usually not homogeneous
across corpora, but is more constant for a
particular text. The score changes in more or less
homogeneous chunks of text. For E-R ProMT 98
MT system the average size of such chunks is
about 7 differences (See Table 3, row 6
&apos;Different strings per text&apos;). For E-R ProMT 98,
the value of each `-F1&apos; or `-1&apos; score after 50
annotated differences is ±2%, so one text can
potentially change the score by about ±14%.
After checking 100 differences, the value of each
`-F1&apos; or &apos;A &apos; score becomes ±1%, so a new text
could change the score by ±7% on average. In the
case of E-R ProMT 98, scoring 50 additional new
strings (about 7 new texts) changed the overall
score by —3%. This indicates that, for our corpus,
there is a reliable improvement after NE DNT-
processing, but more work remains to be done.
Other future work will consider the well-
formedness or acceptability of the NEs
themselves.
</bodyText>
<sectionHeader confidence="0.99645" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999376972972973">
Al-Onaizan, Y. and K. Knight. 2002. Translating
Named Entities Using Monolingual and Bilingual
Resources. Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics.
Philadelphia, July 2002. pp. 400-408.
Cunningham, H., D. Maynard, K. Bontcheva,
V. Tablan. 2002. GATE: A Framework and
Graphical Development Environment for robust
NLP Tools and Applications. Proceedings of the
40th Anniversary Meeting of the Association for
Computational Linguistics (ACL &apos;02). Philadelphia,
July 2002.
Cunningham, H., Y. Wilks and R. Gaizauskas. 1996.
GATE -- a General Architecture for Text
Engineering. Proceedings of the 16th Conference
on Computational Linguistics (COLING-96),
Copenhagen, Aug, 1996.
Gaizauskas, R., T. Wakao, K. Humphreys,
H. Cunningham, Y. Wilks. 1995. University of
Sheffield: Description of the LaSIE system as used
for MUC-6. Proceedings of the 6th Message
Understanding Conference (MUC-6). Morgan
Kaufmann, pp. 207-220.
Newmark, P. 1982. Approaches to translation.
Pergamon Press, Oxford, NY.
Rajman, M. and T. Hartley. 2001. Automatically
predicting MT systems ranking compatible with
Fluency, Adequacy and Informativeness scores.
Proceedings of the 4th ISLE Workshop on MT
Evaluation, MT Summit VIII. Santiago de
Compostela, September 2001. pp. 29-34.
White, J., T. O&apos;Connell and F. O&apos;Mara. 1994. The
ARPA MT evaluation methodologies: evolution,
lessons and future approaches. Proceedings of the
1St Conference of the Association for Machine
Translation in the Americas. Columbia, MD,
October 1994. pp. 193-205.
</reference>
<page confidence="0.998477">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911614">
<title confidence="0.9991945">Improving Machine Translation Quality with Automatic Entity Recognition</title>
<author confidence="0.99944">Bogdan Anthony</author>
<affiliation confidence="0.98067175">Centre for Translation Centre for Translation University of Leeds, University of Leeds, UK Department of Computer a.hartley@leeds.ac.uk University of Sheffield, UK</affiliation>
<email confidence="0.995857">bogdan@comp.leeds.ac.uk</email>
<abstract confidence="0.999588764705883">Named entities create serious problems for state-of-the-art commercial machine translation (MT) systems and often cause translation failures beyond the local context, affecting both the overall morphosyntactic well-formedness of sentences and word sense disambiguation in the source text. We report on the results of an experiment in which MT input was processed using output from the named entity recognition module of Sheffield&apos;s GATE information extraction (IE) system. The gain in MT quality indicates that specific components of IE technology could boost the performance of current MT systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Board is expected to release Pan Am Corp. and its Teamsters union from their long-stalled contract negotiations. E-R Baseline translation:</title>
<institution>National Mediation</institution>
<marker></marker>
<rawString>National Mediation Board is expected to release Pan Am Corp. and its Teamsters union from their long-stalled contract negotiations. E-R Baseline translation:</rawString>
</citation>
<citation valid="false">
<title>ProMT HarmoHarn,Hoe Ilparoemie FlocpeAmmecrna, Kai( rnicH4aeTc51, estnycmum KacTpromo -K0p110pari1451 H ee C01-03 BoAurreneii OT HX gariroOCTaHOBJleHHMX fleper0B0p0B KoHrnaxTa. (National Mediation Board is expected to release [put on the market] a Saucepan - Corporation and its union of drivers from their long-stalled contract negotiations.&apos;) DNT-processed translation: National Mediation Board, Kai&lt; MICH,LkaeTC51, oceo6oaum Pan Am Corp.</title>
<note>14 ero C0103</note>
<marker></marker>
<rawString>ProMT HarmoHarn,Hoe Ilparoemie FlocpeAmmecrna, Kai( rnicH4aeTc51, estnycmum KacTpromo -K0p110pari1451 H ee C01-03 BoAurreneii OT HX gariroOCTaHOBJleHHMX fleper0B0p0B KoHrnaxTa. (National Mediation Board is expected to release [put on the market] a Saucepan - Corporation and its union of drivers from their long-stalled contract negotiations.&apos;) DNT-processed translation: National Mediation Board, Kai&lt; MICH,LkaeTC51, oceo6oaum Pan Am Corp. 14 ero C0103</rawString>
</citation>
<citation valid="false">
<booktitle>Teamsters OT HX AaTIFO-OCTaHOBJ1eHHbIX neperoHopos Korirnarcra.</booktitle>
<marker></marker>
<rawString>Teamsters OT HX AaTIFO-OCTaHOBJ1eHHbIX neperoHopos Korirnarcra.</rawString>
</citation>
<citation valid="false">
<title>National Mediation Board is expected to release [make free] Pan Am Corp. and its Teamsters union from their long-stalled contract negotiations.&apos;</title>
<marker></marker>
<rawString>`National Mediation Board is expected to release [make free] Pan Am Corp. and its Teamsters union from their long-stalled contract negotiations.&apos;)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Translating Named Entities Using Monolingual and Bilingual Resources.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>400--408</pages>
<location>Philadelphia,</location>
<contexts>
<context position="9789" citStr="Al-Onaizan and Knight (2002)" startWordPosition="1470" endWordPosition="1473">m&apos; v3 .Ob, released in 2000 (Systran) Two translations were generated by each MT system: - a baseline translation without a DNT list a DNT-processed translation with the automatically created DNT list of organisation names The baseline translations were then compared with DNT-processed translations, with respect to the morphosyntactic well-formedness of the context surrounding the NEs. 3.1. Segmentation To speed-up the process of finding contextual differences, we developed automatic tools, which allowed us to make a formal distinction between NE-internal and NE-external issues in MT. Whereas Al-Onaizan and Knight (2002) focus on the former issue, our primary interest is in NEexternal differences in context caused by improved NE recognition after DNT-processing. Thus, we automatically selected paragraphs with contextual differences and highlighted different strings in these paragraphs. The example below illustrates the output of these annotation tools: Different strings found in two translations are indicated by - &apos;ORI&apos; indicates the original English string in the DARPA corpus; - `TWS&apos; (baseline translation) indicates a String Translated Without the do-nottranslate list; Manual keys GATE Paragraphs 228 (80.6%</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan, Y. and K. Knight. 2002. Translating Named Entities Using Monolingual and Bilingual Resources. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Philadelphia, July 2002. pp. 400-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A Framework and Graphical Development Environment for robust NLP Tools and Applications.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL &apos;02).</booktitle>
<location>Philadelphia,</location>
<contexts>
<context position="2729" citStr="Cunningham et al., 2002" startWordPosition="393" endWordPosition="396">rs of Information Extraction (IE) systems, such as the GATE system, created at the University of Sheffield and distributed under GPL (Cunningham et al., 1996, 2002). The quality of automatic NE identification has been evaluated at several messageunderstanding conferences (MUC) sponsored by DARPA. Accuracy scores for leading systems are relatively high (in comparison to other IE tasks, such as co-reference resolution, template element filling or scenario template filling). The default settings of NE recognition module of the GATE system produces between 80-90% Precision &amp; Recall on news texts (Cunningham et al., 2002). In this paper we describe the effect of using the GATE NE recognition module as a preprocessor for commercial state-of-the-art MT 1 systems. The idea of our experiment is that highquality automatic NE recognition, produced by GATE, could be used to create do-not-translate (DNT) lists of organisation names, a specific type of NE which in human translation practice is often left untranslated. (Newmark, 1982: 70-83). In our experiment we systematically analysed the effect of incorrect NE recognition on the surrounding lexical and morphosyntactic context in MT output. We tried to establish how f</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Cunningham, H., D. Maynard, K. Bontcheva, V. Tablan. 2002. GATE: A Framework and Graphical Development Environment for robust NLP Tools and Applications. Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL &apos;02). Philadelphia, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>Y Wilks</author>
<author>R Gaizauskas</author>
</authors>
<title>GATE -- a General Architecture for Text Engineering.</title>
<date>1996</date>
<booktitle>Proceedings of the 16th Conference on Computational Linguistics (COLING-96),</booktitle>
<location>Copenhagen,</location>
<contexts>
<context position="2262" citStr="Cunningham et al., 1996" startWordPosition="325" endWordPosition="328">owever, the developers of commercial MT systems often pay insufficient attention to correct automatic identification of certain types of NE, e.g., organisation names. This is due partly to the greater complexity of this problem (the set of proper names is open and highly dynamic), and partly to the lack of time and other development resources. On the other hand, the problem of correct identification of NE is specifically addressed and benchmarked by the developers of Information Extraction (IE) systems, such as the GATE system, created at the University of Sheffield and distributed under GPL (Cunningham et al., 1996, 2002). The quality of automatic NE identification has been evaluated at several messageunderstanding conferences (MUC) sponsored by DARPA. Accuracy scores for leading systems are relatively high (in comparison to other IE tasks, such as co-reference resolution, template element filling or scenario template filling). The default settings of NE recognition module of the GATE system produces between 80-90% Precision &amp; Recall on news texts (Cunningham et al., 2002). In this paper we describe the effect of using the GATE NE recognition module as a preprocessor for commercial state-of-the-art MT 1</context>
</contexts>
<marker>Cunningham, Wilks, Gaizauskas, 1996</marker>
<rawString>Cunningham, H., Y. Wilks and R. Gaizauskas. 1996. GATE -- a General Architecture for Text Engineering. Proceedings of the 16th Conference on Computational Linguistics (COLING-96), Copenhagen, Aug, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gaizauskas</author>
<author>T Wakao</author>
<author>K Humphreys</author>
<author>H Cunningham</author>
<author>Y Wilks</author>
</authors>
<title>University of Sheffield: Description of the LaSIE system as used for MUC-6.</title>
<date>1995</date>
<booktitle>Proceedings of the 6th Message Understanding Conference (MUC-6).</booktitle>
<pages>207--220</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="8514" citStr="Gaizauskas et al., 1995" startWordPosition="1287" endWordPosition="1290">&amp;quot;key&amp;quot; figures. Number of: For the Av. per Av. per Av. per corpus doc. para. sent. Paragraphs 283 9.4 Sentences 565 18.8 2.0 Word 11975 399.2 42.3 21.2 occurrences Different 3944 235.7 36.3 19.7 words NE 544/ 18.1/ 1.9/ 1.0/ occurrences 510 17.0 1.8 0.9 keys/ GATE Different 201/ 7.6/ 1.5/ 0.9/ NEs: keys/ 174 6.7 1.4 0.8 GATE Table 1: Statistical parameters of the corpus The density of NEs in the DARPA corpus is also characterised by Table 2: Table 2: NE density in the corpus The accuracy of GATE-1 in the NE recognition task at MUC-6 (Recall — 84%, Precision — 94%, Precision &amp; Recall — 89.06 % (Gaizauskas et al., 1995)) is such that we used the GATE output for our MT experiment, rather than the cleaner manually annotated data. Moreover, the advantage of using automatic NE recognition is that the results of the experiment should be consistent with the results for other corpora on which the NE recognition task has been performed. Having automatically generated DNT lists of organisation names from GATE &apos;response&apos; annotation, we translated the texts using three commercial MT systems: - English-Russian `ProMT 98&apos; v4.0, released in 1998 (Softissimo) English-French &apos;ProMT (Reverso) v5.01, released in 2001 (Softiss</context>
</contexts>
<marker>Gaizauskas, Wakao, Humphreys, Cunningham, Wilks, 1995</marker>
<rawString>Gaizauskas, R., T. Wakao, K. Humphreys, H. Cunningham, Y. Wilks. 1995. University of Sheffield: Description of the LaSIE system as used for MUC-6. Proceedings of the 6th Message Understanding Conference (MUC-6). Morgan Kaufmann, pp. 207-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Newmark</author>
</authors>
<title>Approaches to translation.</title>
<date>1982</date>
<publisher>Pergamon Press,</publisher>
<location>Oxford, NY.</location>
<contexts>
<context position="1257" citStr="Newmark, 1982" startWordPosition="172" endWordPosition="173">s of an experiment in which MT input was processed using output from the named entity recognition module of Sheffield&apos;s GATE information extraction (IE) system. The gain in MT quality indicates that specific components of IE technology could boost the performance of current MT systems. 1. Introduction Correct identification of named entities (NEs) is an important problem for machine translation (MT) research and for the development of commercial MT systems. In the first place, translation of proper names often requires different approaches and methods than translation of other types of words (Newmark, 1982: 70-83). Mistakenly translating NEs as common nouns often leads to incomprehensibility or necessitates extensive post-editing. In many cases failure to correctly identify NEs has an effect not only on a local and immediate context, but also on the global syntactic and lexical structure of the translation, since proper segmentation of a source text might be seriously distorted. However, the developers of commercial MT systems often pay insufficient attention to correct automatic identification of certain types of NE, e.g., organisation names. This is due partly to the greater complexity of thi</context>
<context position="3139" citStr="Newmark, 1982" startWordPosition="461" endWordPosition="462">template element filling or scenario template filling). The default settings of NE recognition module of the GATE system produces between 80-90% Precision &amp; Recall on news texts (Cunningham et al., 2002). In this paper we describe the effect of using the GATE NE recognition module as a preprocessor for commercial state-of-the-art MT 1 systems. The idea of our experiment is that highquality automatic NE recognition, produced by GATE, could be used to create do-not-translate (DNT) lists of organisation names, a specific type of NE which in human translation practice is often left untranslated. (Newmark, 1982: 70-83). In our experiment we systematically analysed the effect of incorrect NE recognition on the surrounding lexical and morphosyntactic context in MT output. We tried to establish how far NE recognition (specifically recognition of organisation names) influences grammatical well-formedness and word sense choices in the context of NEs. We compared the baseline translations (produced without NE DNTprocessing) with translations produced using DNT lists (created with the GATE-1 NE recognition system), by systematically scoring cases of improvement and decline in lexical and morphosyntactic we</context>
</contexts>
<marker>Newmark, 1982</marker>
<rawString>Newmark, P. 1982. Approaches to translation. Pergamon Press, Oxford, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rajman</author>
<author>T Hartley</author>
</authors>
<title>Automatically predicting MT systems ranking compatible with Fluency, Adequacy and Informativeness scores.</title>
<date>2001</date>
<booktitle>Proceedings of the 4th ISLE Workshop on MT Evaluation, MT Summit VIII. Santiago de Compostela,</booktitle>
<pages>29--34</pages>
<marker>Rajman, Hartley, 2001</marker>
<rawString>Rajman, M. and T. Hartley. 2001. Automatically predicting MT systems ranking compatible with Fluency, Adequacy and Informativeness scores. Proceedings of the 4th ISLE Workshop on MT Evaluation, MT Summit VIII. Santiago de Compostela, September 2001. pp. 29-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J White</author>
<author>T O&apos;Connell</author>
<author>F O&apos;Mara</author>
</authors>
<title>The ARPA MT evaluation methodologies: evolution, lessons and future approaches.</title>
<date>1994</date>
<booktitle>Proceedings of the 1St Conference of the Association for Machine Translation in the Americas.</booktitle>
<pages>193--205</pages>
<location>Columbia, MD,</location>
<marker>White, O&apos;Connell, O&apos;Mara, 1994</marker>
<rawString>White, J., T. O&apos;Connell and F. O&apos;Mara. 1994. The ARPA MT evaluation methodologies: evolution, lessons and future approaches. Proceedings of the 1St Conference of the Association for Machine Translation in the Americas. Columbia, MD, October 1994. pp. 193-205.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>