<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.892584">
Plug and Play Speech Understanding
</title>
<author confidence="0.8179705">
Manny Rayner, Ian Lewin
&amp; Genevieve Gorrell
</author>
<affiliation confidence="0.715426">
netdecisions Ltd
</affiliation>
<address confidence="0.6945585">
Wellington House,
East Road, Cambridge C131 113H, UK
</address>
<email confidence="0.769962">
manny.raynerlian.lewinlgenevieve.gorrell
@netdecisions.com
</email>
<note confidence="0.893526333333333">
Johan Boye
Telia Research
S-123 86 Farsta, Sweden
</note>
<email confidence="0.969551">
johan.boye@trab.se
</email>
<sectionHeader confidence="0.978197" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876117647059">
Plug and Play is an increasingly im-
portant concept in system and network
architectures. We introduce and de-
scribe a spoken language dialogue sys-
tem architecture which supports Plug
and Playable networks of objects in its
domain. Each device in the network car-
ries the linguistic and dialogue manage-
ment information which is pertinent to it
and uploads it dynamically to the rele-
vant language processing components in
the spoken language interface. We de-
scribe the current state of our plug and
play demonstrator and discuss theoreti-
cal issues that arise from our work. Plug
and Play forms a central topic for the
DHomme project.
</bodyText>
<sectionHeader confidence="0.996302" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994865">
The notion of Plug and Play finds its most natu-
ral home in the world of networked home devices,
where it offers at least the following two important
properties
</bodyText>
<listItem confidence="0.993079214285714">
• the network of devices is dynamically recon-
figurable as devices are brought online or dis-
appear offline
• zero re-configuration by the user is required
Frameworks for achieving Plug and Play gener-
ally address this by including at least the following
• devices announce themselves on the network
when they are plugged into it (and also dis-
cover the existence of others)
• devices describe their own capabilities, pro-
vide a means for accessing them and can
query and access the capabilities of others
• devices should support, where possible, seam-
less interaction with other devices.
</listItem>
<bodyText confidence="0.999779513513514">
Plug and Play is, not surprisingly, viewed as
a pre-requisite for the commercial success of net-
worked devices in the home. There are already
several promising candidate platforms for achiev-
ing the necessary functionality, including Univer-
sal Plug and Play (UPnP) (Microsoft, 2000) and
Jini (Oaks and Wong, 2000). In this paper, we
address the requirements on spoken dialogue in-
terfaces that arise from a plug and play domain.
We also present the current state of our English
language plug and play demonstrator for control-
ling lamps, dimmers and sensors, previously de-
scribed in (Rayner et al., 2001b). (There is also a
Swedish instantiation).
First, however, we need briefly to distinguish
our notion from other notions of plug and play
and reconfigurability.
The notion of Plug and Play has been used
for dialogue system toolkits in which the various
different language processing components them-
selves (e.g. recognition, parsing, generation and
dialogue management) can be plugged in and
out. The most prominent instance of this is
the Darpa Communicator architecture (Goldschen
and Loehr, 1999), which defines interoperability
standards for language processing components.
The intention is simply that researchers and de-
velopers can experiment with systems containing
different instantiations of the language process-
ing components. The Communicator Architec-
ture is not designed to address the special require-
ments of a plug and play domain. In fact, the
Communicator architecture does not support the
dynamic re-configuration of language processing
components while the system is running.
At a more general level, simple re-configuration
of spoken language dialogue systems has of course
long been a goal of language engineering. But
such re-configuration is nearly always viewed as
the problem of cross-domain or possibly cross-
language porting, e.g. (Glass, 1999). Once one
has a cinema ticket booking service, for example,
one may examine the effort required for book-
ing train tickets, or for e-shopping in general or
even the &amp;quot;database access&amp;quot; scenario. There are
various toolkits, architectures and methodologies
for rapidly and/or semi-expertly generating new
instances of dialogue systems, e.g. by abstract-
ing away from domain or application dependent
features of particular systems, e.g. (Fraser and
Thornton, 1995; Kolzer, 1999), or `bottom-up&apos; by
aggregation of useful re-configurable components
, e.g. (Sutton et al, 1998; Larsson and Traum,
2000). The automated within-domain reconfigu-
ration required for a plug and play domain, has
not, to our knowledge, been described previously.
Pursuit of plug and play functionality (and its
realization in strong and weak forms - discussed in
section 3) forms a central theme of the DHomme
project. 1
In the rest of this paper, we begin by detail-
ing our concrete Plug and Play scenario - device
control in the home - with an example dialogue
from our demonstrator and an outline of the main
dialogue processing elements. In section 3, we dis-
tinguish strong and weak notions of Plug and Play
and their applicability to spoken language inter-
faces. In section 4, we discuss the strong plug
and play capability we have built into the recogni-
tion, parsing and (context independent) semantic
interpretation system components of our demon-
strator. In section 5, we discuss some future work
for Plug and Play dialogue management. Section
6 contains our conclusions.
</bodyText>
<sectionHeader confidence="0.575285" genericHeader="introduction">
2 A Plug and Play Scenario
</sectionHeader>
<bodyText confidence="0.999859166666667">
In this section we present example dialogues from
our current demonstrator and briefly outline the
main processing elements. The domain is net-
worked home devices, an area where plug and play
is already in a reasonably advanced state of devel-
opment and speech control looks highly attractive.
</bodyText>
<subsectionHeader confidence="0.990749">
2.1 Plug and Play Examples
</subsectionHeader>
<bodyText confidence="0.996010363636364">
Figure 1 displays an example dialogue from our
current demonstrator.
In U1, the user asks for the TV to be switched
on. The system reports (S1) that it simply does
not understand the user. (If it possessed &amp;quot;TV&amp;quot;
in its recognition vocabulary and knew something
about the concept of TVs it could have reported
I don&apos;t know of any Tl�. When a TV is plugged
into the network (following U2), the system is able
to understand, and act on the user&apos;s repeated re-
quest to switch on the TV. The system reports on
</bodyText>
<footnote confidence="0.957779">
&apos;This work is supported by EU 5th Framework
project IST-2000-26280 - see Acknowledgments
</footnote>
<bodyText confidence="0.9996945">
its action (S3). S4 illustrates another type of &amp;quot;er-
ror&amp;quot; message. When another TV is plugged into
the network, the system must now engage in dis-
ambiguation behaviour whereas previously it had
no need to (S7).
S10 illustrates that, in the absence of dimmable
lights, &amp;quot;Dim&amp;quot; is not understood, and, possibly,
not even recognized. When a dimmable light is
plugged in (or, at least, knowledge of dimmable
lights is plugged in), then a more helpful error
message can be given in S12. Finally, when the
grammar is increased to cover new commands, the
system may begin to make mistakes that it did not
make originally (S13).
</bodyText>
<subsectionHeader confidence="0.98829">
2.2 The current demonstrator
</subsectionHeader>
<bodyText confidence="0.999096839285714">
Our demonstrator expects devices of three main
types: switchable, dimmable and sensors. Switch-
able devices are binary state devices that can be
set or queried. Dimmable devices have a state
varying on a single scalar dimension which can be
set, changed, or queried. Sensors are similar but
can only be queried.
Formally, these commands and queries are en-
coded by a 4-ary slot-value structure and De-
vice Grammars must generate semantic values
containing these slots. (Not all slots are re-
quired for every utterance, of course.) The
four slots are: op (filled by command or query);
level; change and dir (on or off). In or-
der to identify devices, there are 4 other slots:
device (light, tv ... ), loc (bathroom, kitchen ... ),
device-spec (all, the ... ) and pronoun (it, them
... ). For example, Is the light in the hall on?
translates to h op=query dir=on device=light
device-spec=the loc=hall i. Dim everything by
ten percent translates to h op=command dir=off
device-spec=everything change=10 i. Switch
the hall and kitchen lights off translates to h
op=command dir=off level=0 device=light h
loc=kitchen loc=hall ii.
Dialogue interpretation contains three stages.
First, conjunctions in the input (which are treated
as just packed representations) are unpacked into
a set of (7-ary) slot-structures. Secondly, a form of
ellipsis resolution, &amp;quot;sticky defaults&amp;quot;, takes place in
which missing slots are filled in from their previ-
ous values. A fragmentary semantic value is sim-
ply pasted over the corresponding parts of the last
one. Thus And in the bathroom translates to h
loc=bathroomi but the other required slots (e.g.
device) are supplied from the previous represen-
tation. Finally reference resolution tries to deter-
mine device identifiers for pronouns and definitely
described devices. Currently, devices are identi-
fied only by their location and type so a simple
matching procedure can be used.
Following contextual interpretation, either a
program (a command or sequence of such) or an
`error&apos; condition (e.g. resolution failed to identify
a device) will have been generated. The system
must then execute the program and/or generate
feedback. Knowledge of how to execute these pro-
grams, e.g. that it is an `error&apos; to try to switch on
a light that is already on, and possible feedback
messages are simply hardcoded into the Dialogue
Manager. There is a pre-defined set of feedback
message structures associated with each underly-
ing action and their possible results. Some exam-
ple paraphrases of message structures are &amp;quot;The X
is now on&amp;quot;, &amp;quot;The X is already on&amp;quot;, &amp;quot;The X is now
at Y percent&amp;quot;, &amp;quot;There is no X in the Y&amp;quot;.
</bodyText>
<sectionHeader confidence="0.829953" genericHeader="method">
3 Strong and Weak Plug and Play
</sectionHeader>
<bodyText confidence="0.999937048076924">
In its weakest form, Plug and Play refers only to
the ability to add a device to a network with-
out manual configuration. Knowledge distribu-
tion is not included. Standard Plug and Play for
PC peripherals simply automates the matching up
of physical devices with software device-specific
drivers in the PC. Communication links between
them are established by reserving resources such
as shared memory and interrupt request numbers.
The weak sense is very useful. Users need not
configure their hardware via jumper switches or
software drivers by entering `magic&apos; numbers in
configuration files.
In the strong sense, Plug and Play can refer
also to modular, distributed knowledge. Devices
not only set up network communications but pub-
lish information about themselves over it. Other
devices can obtain and use it. In Jini, for exam-
ple, a new printer device can register its printing
service (and java code for invoking methods on it)
on the network. Then, a word-processing applica-
tion can find it and configure itself to use it. In
UPnP, devices publish XML descriptions of their
interfaces.
The strong-weak contrast is not a sharp or bi-
nary one. The word-processor might know the
industry agreed printer interface and so display
a greyed-out print button if no printer is net-
worked. When a new type of printer is net-
worked, it might supply additional print options
(e.g. &amp;quot;print colour&amp;quot;) that the processor knows
nothing about.
One desirable Plug and Play property in both
strong and weak forms is commutativity, i.e. the
system understands the same commands in the
same way no matter which device is connected
first. It is less obvious whether disconnecting de-
vice X should be the inverse operation of connect-
ing device X. This seems reasonable in a weak plug
and play system, but in the strong case it would
mean that the recognizer would cease to under-
stand the word &amp;quot;TV&amp;quot; as soon as the TV were dis-
connected. This might be confusing for the user.
The strong and weak senses of plug and play
apply to spoken language dialogue interfaces. In
the weakest sense, the dialogue system might be
entirely pre-configured to deal with all possible
devices and device-combinations. The required
knowledge is already present in the network. Plug
and Play then consists of identifying which partic-
ular devices are currently networked and estab-
lishing communication channels with them. In
the stronger sense, the components of the spoken
language dialogue interface acquire the knowledge
pertinent to particular devices from those devices.
So, as in example S1 above, the speech recognizer
may not have the word &amp;quot;TV&amp;quot; in its vocabulary
until a TV is plugged into the network. The di-
alogue manager may not be capable of uttering
&amp;quot;That device is not dimmable&amp;quot; until a dimmable
device is plugged into the network. A strongly
Plug and Play system may therefore be distin-
guishable from a weaker one by its behaviour in
the absence of certain device specific knowledge.
If the relevant knowledge is present, one cannot be
certain whether it was pre-configured or uploaded
&amp;quot;on demand&amp;quot;.
Plug and Play also enforces a certain sort of
modularity on the system. Since devices must de-
clare the information required to update the dia-
logue components, a clear interface is provided for
re-configuring the system for new types of device
as well as a clearer picture of the internal structure
of those dialogue components. Indeed, it is really
just a design choice whether device knowledge is in
fact installed only when the device is plugged in.
One may, for example, choose to optimize recog-
nition performance on the set of devices actually
installed by not loading information about other
devices. Alternatively, one might prefer to rec-
ognize the names of devices not installed so that
helpful error messages can be delivered.
Potentially, each component in a spoken lan-
guage interface (recognizer, parser, interpreter, di-
alogue manager etc.) can be updated by informa-
tion from a device in a Plug and Play domain. Dif-
ferent components might support different degrees
of strength of the Plug and Play notion. Further-
more, different instantiations of these components
may require very different sorts of update. To
take a very simple example, if recognition is car-
ried out by a statistically trained language model,
then updating this with information pertinent to a
particular device will evidently be a significantly
different task from updating a recognizer which
uses a grammar-based language model.
Our current demonstrator program instantiates
a Plug and Play capability for recognition, parsing
and context independent semantic analysis and is
built on top of the Nuance toolkit (Nuance Com-
munications, 1999). The next section discusses
the capability in detail. Section 5 discusses and
makes some proposals for Plug and Play Dialogue
Management.
</bodyText>
<sectionHeader confidence="0.987515" genericHeader="method">
4 Distributed Grammar
</sectionHeader>
<subsectionHeader confidence="0.65322">
4.1 Introduction
</subsectionHeader>
<bodyText confidence="0.984638863157896">
In this section, we will describe how we have ad-
dressed the issues that arise when we attempt to
apply the (strong) Plug and Play scenario to the
tasks of speech recognition and language process-
ing. Each device will provide the knowledge that
the speech interface needs in order to recognise the
new types of utterance relevant to the device in
question, and convert these utterances into well-
formed semantic representations.
Let&apos;s start by considering what this means in
practice. There are in fact a whole range of pos-
sible scenarios to consider, depending on how the
speech understanding module is configured. If the
module&apos;s construction is simple enough, there may
be no significant problems involved in extending
it to offer Plug and Play functionality. For ex-
ample, the command vocabulary offered by the
speech interface may just consist of a list of fixed
phrases. In this case, Plug and Play speech recog-
nition becomes trivial: each device contributes the
phrases it needs, after which they can be com-
bined into a single grammar. An approach of
this kind fails however to scale up to an interface
which supports complex commands, in particular
commands which combine within the same utter-
ance language referring to two or more different
devices. For example, a command may address
several devices at once (&amp;quot;turn on the radio and the
living room light&amp;quot;); alternately, several commands
may be combined into a single utterance (&amp;quot;switch
on the cooker and switch off the microwave&amp;quot;). Our
experience with practical spoken device interfaces
suggests that examples like these are by no means
uncommon.
Another architecture relatively easy to com-
bine with Plug and Play is doing recognition
through a general large-vocabulary recogniser,
and language-processing through device-specific
phrase-spotting (Milward, 2000). The recogniser
stays the same irrespective of how many devices
are connected, so there are by definition no prob-
lems at the level of speech recognition, and it is in
principle possible to support complex commands.
The main drawback, however, is that recognition
quality is markedly inferior compared to a system
in which recognition coverage is limited to the do-
main defined by the current set of devices.
Modern speech interfaces supporting complex
commands are typically specified using a rule-
based grammar formalism defined by a platform
like Nuance (Nuance Communications, 1999) or
SpeechWorks (Inc, 2001). The type of grammar
supported is some subset of full CFG, extended to
include semantic annotations. Grammar rules de-
fine the language model that constrains the recog-
nition process, tuning it to the domain in order to
achieve high performance. (They also supply the
semantic rules that define the output representa-
tion; we will return to this point later). If we want
to implement an ambitious Plug and Play speech
recognition module within this kind of framework,
we have two top-level goals. On the one hand, we
want to achieve high-quality speech recognition.
At the same time, standard software engineering
considerations suggest that we want to minimize
the overlap between the rule-sets contributed by
each device: ideally, the device will only upload
the specific lexical items relevant to it.
It turns out that our software engineering ob-
jectives conflict to some extent with our initial
goal of achieving high-quality speech recognition.
Consider a straightforward solution, in which the
grammatical information contributed by each de-
vice consists purely of lexical entries, i.e. entries
of the form
&lt;Nonterminal&gt; --&gt; &lt;Terminal&gt;
In a CFG-based framework, this implies that we
have a central device-independent CFG grammar,
which defines the other rules which link together
the nonterminals that appear on the left-hand-
sides of the lexical rules. The crucial question is
what these lexical non-terminal symbols will be.
Suppose, for concreteness, that we want our set
of devices to include lights with dimmer switches,
which will among other things accept commands
like &amp;quot;dim the light&amp;quot;. We might achieve this by
making the device upload lexical rules of the rough
form
TRANSITIVE�VERB --&gt; dim
NOUN --&gt; light
where the LHSs are conventional grammatical cat-
egories. (We will for the moment skip over the
question of how to represent semantics). The lex-
ical rules might combine with general grammar
rules of the form
</bodyText>
<equation confidence="0.337164666666667">
COMMAND --&gt; TRANSITIVE-VERB NP
NP --&gt; DET NOUN
DET --&gt; the
</equation>
<bodyText confidence="0.999568294117647">
This kind of solution is easy to understand, but ex-
perience shows that it leads to poor speech recog-
nition. The problem is that the language model
produced by the grammar is underconstrained: it
will in particular allow any transitive verb to com-
bine with any NP. However, a verb like &amp;quot;dim&amp;quot; will
only combine with a restricted range of possible
NPs, and ideally we would like to capture this
fact. What we really want to do is parameterise
the language model. In the present case, we want
to parameterise the TRANSITIVE VERB &amp;quot;dim&amp;quot; with
the information that it only combines with object
NPs that can be used to refer to dimmable de-
vices. We will parameterise the NP and NOUN
non-terminals similarly. The obvious way to do
this within the bounds of CFG is to specialise the
rules approximately as follows:
</bodyText>
<sectionHeader confidence="0.527823" genericHeader="method">
COMMAND --&gt; TRANS-DIM-VERB DIMMABLE-NP
DIMMABLE-NP --&gt; DET DIMMABLE-NOUN
TRANS-DIM-VERB --&gt; dim
DIMMABLE-NOUN --&gt; light
DET --&gt; the
</sectionHeader>
<bodyText confidence="0.998616">
Unfortunately, however, this defeats the original
object of the exercise, since the &amp;quot;general&amp;quot; rules
now make reference to the device-specific concept
of dimming. What we want instead is a more
generic treatment, like the following:
</bodyText>
<equation confidence="0.467275222222222">
COMMAND --&gt;
TRANSITIVE-VERB:Esem-obj-type=T]
NP:Esem-type=T]
NP:Esem-type=T] --&gt;
DET NOUN:Esem-type=T]
DET --&gt; the
TRANSITIVE-VERB:Esem-obj-type=dimmable]
--&gt; dim
NOUN:Esem-type=dimmable] --&gt; light
</equation>
<bodyText confidence="0.999809733333333">
This kind of parameterisation of a CFG is not
in any way new: it is simply unification gram-
mar (Pullum and Gazdar, 1982; Gazdar et al.,
1985). Thus our first main idea is to raise the
level of abstraction, formulating the device gram-
mar at the level of unification grammars, and
compiling these down into the underlying CFG
representation. There are now a number of sys-
tems which can perform this type of compilation
(Moore, 1998; Kiefer and Krieger, 2000); the ba-
sic methods we use in our system are described
in detail elsewhere (Rayner et al., 2001a). Here,
we focus on the aspects that are required for &amp;quot;dis-
tributed&amp;quot; unification grammars needed for Plug
and Play.
</bodyText>
<subsectionHeader confidence="0.77465">
4.2 &amp;quot;Unification grammars meet
</subsectionHeader>
<bodyText confidence="0.981014142857143">
object-oriented programming&amp;quot;.
Our basic idea is to start with a general device-
independent unification grammar, which imple-
ments the core grammar rules. In our prototype,
there are 34 core rules. Typical examples are
the NP conjunction and PP modifications rules,
schematically
</bodyText>
<equation confidence="0.9176655">
NP --&gt; NP CONJ NP
NP --&gt; NP PP
</equation>
<bodyText confidence="0.999950105263158">
which are likely to occur in connection with any
kind of device. These rules are parameterised by
various features. For example, the set of features
associated with the NP category includes gram-
matical number (singular or plural), WH (plus or
minus) and sortal type (multiple options).
Each individual type of device can extend the
core grammar in one of three possible ways:
New lexical entries A device may add lexical
entries for device-specific words and phrases;
e.g., a device will generally contribute at least
one noun used to refer to it.
New grammar rules A device may add device-
specific rules; e.g., a dimmer switch may in-
clude rules for dimming and brightening, like
&amp;quot;another X percent&amp;quot; or &amp;quot;a bit brighter&amp;quot;.
New feature values Least obviously, a device
may extend the range of values that a gram-
matical feature can take (see further below).
For usual software engineering reasons, we find it
convenient to divide the distributed grammar into
modules; the grammatical knowledge associated
with a device may reside in more than one module.
The grammar in our current demonstrator con-
tains 21 modules, including the &amp;quot;core&amp;quot; grammar
described above. Each device typically requires
between two and five modules. For example, an
on/off light switch loads three modules: the core
grammar, the general grammar for on/off switch-
able devices, and the grammar specifically for
on/off switchable lights. The core grammar, as al-
ready explained, consists of linguistically oriented
device-independent grammar rules. The mod-
ule for on/off switchable devices contains gram-
mar rules specific to on/off switchable behaviour,
which in general make use of the framework es-
tablished by the general grammar. For example,
there are rules of the schematic form
</bodyText>
<equation confidence="0.820559333333333">
QUESTION --&gt;
is
NP:Esem-type=device]
ON-OFF-PHRASE
PARTICLE-VERB:[particle-type=onoff]
--&gt; switch
</equation>
<bodyText confidence="0.99996537037037">
Finally, the module for on/off switchable lights is
very small, and just consists of a handful of lexi-
cal entries for nouns like &amp;quot;light&amp;quot;, defining these as
nouns referring to on/off switchable devices. The
way in which nouns of this kind can combine is
however defined entirely by the on/off switchable
device grammar and core grammar.
The pattern here turns out to be the usual one:
the grammar appropriate to a device is composed
of a chain of modules, each one depending on the
previous link in the chain and in some way special-
ising it. Structurally, this is similar to the organ-
isation of a piece of normal object-oriented soft-
ware, and we have been interested to discover that
many of the standard concepts of object-oriented
programming carry over naturally to distributed
unification grammars. In the remainder of the sec-
tion, we will expand on this analogy.
If we think in terms of Java or a similar main-
stream OO language, a major grammatical con-
stituent like S, NP or PP has many of the prop-
erties of an OO interface. Grammar rules in one
module can make reference to these constituents,
letting rules in other modules implement their
definition. For example, the temperature sen-
sor grammar module contains a small number of
highly specialised rules, e.g.
</bodyText>
<sectionHeader confidence="0.397969" genericHeader="method">
QUESTION --&gt;
</sectionHeader>
<bodyText confidence="0.963432561643836">
what is the temperature
PP:[pp-type=location]
QUESTION --&gt;
how many degrees is it
PP:[pp-type=location]
The point to note here is that the temperature
sensor grammar module does not define the loca-
tive PP construction; this is handled elsewhere,
currently in the core grammar module. The up-
shot is that the temperature sensor module is able
to define its constructions without worrying about
the exact nature of the locative PP construction.
As a result, we were for instance able to upgrade
the PP rules to include conjoined PPs (thus allow-
ing e.g. &amp;quot;what is the temperature in the kitchen
and the living room&amp;quot;) without in any way alter-
ing the grammar rules in the temparature sensor
module
2 A ambitious treatment of conjunction might ar-
guably also necessitate changes in the dialogue man-
agement component specific to the temperature sen-
sor device. In the implemented system, conjunction
is uniformly treated as distributive, so &amp;quot;what is the
temperature in the kitchen and the living room&amp;quot; is au-
In order for the scheme to work, the &amp;quot;interfaces&amp;quot;
- the major categories - naturally need to be well-
defined. In practice, this implies restrictions on
the way we handle three things: the set of syntac-
tic features associated with a category, the range
of possible values (the domain) associated with
each feature, and the semantics of the category.
We consider each of these in turn.
Most obviously, we need to standardise the
feature-set for the category. At present, we de-
fine most major categories in the core grammar
module, to the extent of specifying there the full
range of features associated with each category.
It turns out, however, that it is sometimes desir-
able not to fix the domain of a feature in the core
grammar, but rather to allow this domain to be
extended as new modules are added. The issues
that arise here are interesting, and we will discuss
them in some detail.
The problems occur primarily in connection
with features mediating sortal constraints. As
we have already seen in examples above, most
constituents will have at least one sortal fea-
ture, encoding the sortal type of the constituent;
there may also be further features encoding the
sortal types of possible complements and ad-
juncts. For example, the V category has a fea-
ture vtype encoding the sortal type of the V it-
self, a feature obj sem np type encoding the sor-
tal type of a possible direct object, and a feature
vp modifiers type encoding the sortal type of a
possible postverbal modifier.
Features like these pose two interrelated prob-
lems. First, the plug and play scenario implies
that we cannot know ahead of time the whole do-
main of a sortal feature. It is always possible that
we will connect a device whose associated gram-
mar module requires definition of a new sortal
type, in order to enforce appropriate constraints in
the language model. The second problem is that
it is still often necessary to define grammar rules
referring to sortal features before the domains of
these features are known: in particular, the core
module will contain many such rules. Even before
knowing the identity of any specific devices, gen-
eral grammar rules may well want to distinguish
between &amp;quot;device&amp;quot; NPs and &amp;quot;location&amp;quot; NPs. For
example, the general &amp;quot;where-question&amp;quot; rule has
the form
</bodyText>
<equation confidence="0.274914">
QUESTION --&gt; where is NP
</equation>
<bodyText confidence="0.9965699">
Here, we prefer to constrain the NP so as to make
it refer only to devices, since the system currently
tomatically interpreted as equivalent to &amp;quot;what is the
temperature in the kitchen and what is the tempera-
ture in the living room&apos;.
has no way to interpret a where question referring
to a room, e.g. &amp;quot;where is the bathroom&amp;quot;.
We have addressed these issues in a natural way
by adapting the OO-oriented idea of inheritance:
specifically, we define a hierarchy of possible fea-
ture values, allowing one feature value to inherit
from another. In the context of the &amp;quot;where is
NP&amp;quot; rule above, we define the rule in the core
module; in this module, the sortal NP feature
sem np type may only take the two values device
and location, which we specify with the declara-
tion3
domain(sem-np-type, [location, device])
This allows us to write the constrained &amp;quot;where is&amp;quot;
rule as
</bodyText>
<sectionHeader confidence="0.399657" genericHeader="method">
QUESTION --&gt;
</sectionHeader>
<bodyText confidence="0.9923501">
where is NP:[sem-np-type=device]
Suppose now that we add modules for both on/off
switchable and dimmable devices; we would like
to make these into distinct sortal types, called
switchable device and dimmable device. We
do this by including the following declarations in
the &amp;quot;switchable&amp;quot; module:
domain(sem-np-type,
[location,
device,
switchable-device])
specialises(switchable-device, device)
and correspondingly in the &amp;quot;dimmable&amp;quot; module:
domain(sem-np-type,
[location,
device,
dimmable-device])
specialises(dimmable-device, device)
When all these declarations are combined at
compile-time, the effect is as follows. The do-
main of the sem np type feature is now the
union of the domains specified by each compo-
nent, and is thus the set {location, device,
switchable device, dimmable device}. Since
switchable device and dimmable device are
the precise values specialising device, the com-
piler systematically replaces the original feature
value device with the disjunction
switchable-device \/ dimmable-device
Thus the &amp;quot;where is&amp;quot; rule now becomes
</bodyText>
<footnote confidence="0.7294825">
QUESTION --&gt;
where is
NP:[sem-np-type=switchable-device \/
dimmable-device]
3We have slightly simplified the form of the decla-
ration for expository purposes.
</footnote>
<bodyText confidence="0.999942571428571">
If new modules are added which further specialise
switchable device, then the rule will again be
adjusted by the compiler so as to include appropri-
ate new elements in the disjunction. The impor-
tant point to notice here is that no change is made
to the original rule definition; in line with nor-
mal OO thinking, the feature domain information
is distributed across several independent modules,
and the changes occur invisibly at compile-time4.
We have so far said nothing about how we deal
with semantics, and we conclude the section by
sketching our treatment. In fact, it is not clear
to us that the demands of supporting Plug and
Play greatly affect semantics. If they do, the
most important practical consideration is proba-
bly that plug and play becomes easier to realise
if the semantics are kept simple. We have at any
rate adopted a minimal semantic representation
scheme, and the lack of problems we have experi-
enced with regard to semantics may partly be due
to this.
The annotated CFG grammars produced by our
compiler are in normal Nuance Grammar Specifi-
cation Language (GSL) notation, which includes
semantics; unification grammar rules encode se-
mantics using the distinguished feature sem, which
translates into the GSL return construction. So
for example the unification grammar rules
</bodyText>
<equation confidence="0.7990245">
DEVICE-NOUN:[sem=light] --&gt; light
DEVICE-NOUN:[sem=heater] --&gt; heater
</equation>
<bodyText confidence="0.863470357142857">
translates into the GSL rule
DEVICE-NOUN
[ light {return(light)}
heater {return(heater)}]
Unification grammar rules may contain variables,
translating down into GSL variables; so for exam-
ple,
NP:[sem=[D, N]] --&gt;
DET:[sem=D]
NOUN:[sem=N]
translates into the GSL rule
NP (DET:d NOUN:n) {return(($d $n))}
Our basic semantic representation is a form of fea-
ture/value notation, extended to allow handling
</bodyText>
<footnote confidence="0.9293928">
4Readers familiar with OO methodology may
be disturbed by the fact that the rule appears
to have been attached to the daughter nodes
(switchable device dimmable device, etc), rather
than to the mother device node. We would argue that
the rule is still conceptually attached to the device
node, but that the necessity of eventually realising it
in CFG form implies that it must be compiled in this
way, so that it can later be expanded into a separate
CFG rule for each daughter.
</footnote>
<listItem confidence="0.945960285714286">
of conjunction. We allow four types of semantic
construction:
• Simple values, e.g. light, heater. Typically
associated with lexical entries.
• Feature/value pairs expressed in list no-
tation, e.g. [device, light], [location,
kitchen]. These are associated with nouns,
adjectives and similar constituents.
• Lists of feature/value pairs, e.g. [[device,
light], [location, kitchen]]. These are
associated with major constituents such as
NP, PP, VP and S.
• Conjunctions of lists of feature/value pairs,
e.g. [and, [[device, light]], [[device,
</listItem>
<bodyText confidence="0.964888857142857">
heater]]] These represent conjoined con-
stituents, e.g. conjoined NPs, PPs and Ss.
This scheme makes it straightforward to write the
semantic parts of grammar rules. Most often, the
rule just concatenates the semantic contributions
of its daughters: thus for example the semantic
features of the nominal PP rule are simply
</bodyText>
<equation confidence="0.676146666666667">
NP:[sem=concat(Np, Pp)] --&gt;
NP:[sem=Np]
PP:[sem=Pp]
</equation>
<bodyText confidence="0.848297428571429">
The semantic output of a conjunction rule is typ-
ically the conjunction of its daughters excluding
the conjunction itself, e.g.
NP:[sem=[and, Np1, Np2]] --&gt;
NP:[sem=Np1]
and
NP:[sem=Np2]
</bodyText>
<sectionHeader confidence="0.785192" genericHeader="method">
5 Future Plug &amp; Play work
</sectionHeader>
<bodyText confidence="0.999965430555556">
In the future, we intend to move to a system
in which all dialogue components can be recon-
figured by devices. For example, in a complete
Plug and Play scenario, the possible device ac-
tions themselves should be declared by devices
perhaps following UPNP standards in which de-
vices publish all interface commands in the form
actionname(arg1 ...arg2) plus an internal state
model of a simple vector of values. In this section
we start with some very general observations on
Plug and Play dialogue management and the role
of inference. Then we outline a proposal for a rule
based formalism.
At a very general level of course, indirections
between executable actions and linguistic contents
can arise at several levels: the speech act level
(&amp;quot;It&apos;s too warm in here&amp;quot;), the content level (&amp;quot;How
warm is it?&amp;quot;), as well through underdetermination
of contents either through pronominal or ellipti-
cal constructions. At the moment, our pronom-
inal and elliptical resolution methods depend on
very simple `matching&apos; algorithms. In general, one
might at least want some sort of consistency check
between the linguistic properties expressed in an
utterance and those of candidate objects referred
to. One might expect that inferential elements in
contextual interpretation should be strongly Plug
and Play - they will depend, for correctness and
efficiency, on tailoring to the current objects in
the domain. The research project of uploading
relevant axioms and meaning postulates from a
device to a general purpose inference engine that
can be invoked in contextual resolution looks very
exciting.
Evidently, higher pragmatic relations between
what the user &amp;quot;strictly says&amp;quot; and possible device
operations are also very heavily inference based.
At the moment, we simply encode any neces-
sary inferences directly into the device grammars
and this suffices to deal with certain simple be-
haviours. However, the requirement to encapsu-
late all device behaviour in a Plug and Play man-
ner imposes a significant requirement. For ex-
ample, the most natural interaction with a ther-
mometer is, for example, &amp;quot;How warm is it?&amp;quot; or
&amp;quot;What is the temperature?&amp;quot; and not &amp;quot;Query the
thermometer&amp;quot;. In our demonstrator, the (gram-
mar derived) semantic values simply reflect di-
rectly the relevant device operations: ( op=query
device=thermometer). The strategy supports the
simple natural interactions it is designed to. It
even interacts tolerably well with our ellipsis and
reference resolution methods. &amp;quot;What is the tem-
perature in the hall? And in the living room?&amp;quot;
and &amp;quot;What is the temperature in the hall? What
is it in the living room?&amp;quot; can both be correctly
interpreted. Other interactions are less natural.
The default output when resolution cannot iden-
tify a device N is &amp;quot;I don&apos;t know which N you
mean&amp;quot;. However, asking for the temperature in a
room with several thermometers should probably
not result in &amp;quot;I don&apos;t know which temperature you
mean&amp;quot;. It follows that prescribing all behaviour in
a Plug and Play fashion is a significant constraint.
Indeed, a more general point can be made here.
A problem has arisen because the inference from
service required to service provider has become in-
secure in the presence of other service providers.
In the highly networked homes of the future, more
sophisticated inference may be required just be-
cause service level concepts will predominate over
device level concepts.
</bodyText>
<subsectionHeader confidence="0.957651">
5.1 A Rule based formalism
</subsectionHeader>
<bodyText confidence="0.999984366666667">
In this section we assume that semantic
values consist of 5-ary slot-structure with
slots device class (dimmable light, TV ... ),
device attributes (kitchen, blue ... ),pronoun
(as before) and device-specifier (as before)
and operation. An operation is the action
the user wants carried out, e.g. switch on,
set level(X), where X is a real number (for
dimmable lights), set program(Y) etc. (for Hi-
Fis, TVs), and so on. As in the grammar, device
classes are ordered in a hierarchy in a standard
object-oriented way. Thus &amp;quot;dimmable light&amp;quot; is a
subclass of &amp;quot;dimmable device&amp;quot; and inherits from
it.
For strong plug and play, at least the follow-
ing information must be loaded by a device into
the dialogue manager: the device interface (e.g.
that the &amp;quot;switchable light&amp;quot; class has a switch on
method; the feedback to the user; the update to
the system&apos;s device model generated by executing
the command. Clearly, behaviour can also depend
on the current state. Reaction to &amp;quot;switch on the
kitchen light&amp;quot; depends on whether the lamp is off,
on, and whether there is a kitchen light. We write
a rule as command(A,B,C,D) where A is a com-
mand, B is a class of devices for which A is appli-
cable, C is a list of device attributes whose values
must be known in order to execute A, D is a list of
items describing how the system should react on
A. Each item has the following components:
</bodyText>
<equation confidence="0.913576">
precondition(X) X is an executable proce-
</equation>
<bodyText confidence="0.998059">
dure that tests the network state and returns
true or false. If true, the item can `fire&apos;.
</bodyText>
<equation confidence="0.998176">
action(Y) Y is the procedure to execute
feedback(Z,R) Z is the feedback for the user
</equation>
<bodyText confidence="0.870619714285714">
and can depend on R, the return value from
the device operation
upd(W,R) W describes how the system&apos;s
model of the network state should be up-
dated. Also W can depend on R.
For example switching on
a light might be encoded as
command(switch, switchable light, [id = ID], D)
where D is a list of items in the above form of
which one describes behaviour when the light is
already off thus:
[ precondition(light off(ID)),
action(switch on light(ID)),
feedback(light now on(ID), success),
feedback(could not switch(ID),error),
upd([dev update(ID, status = 1)], success),
upd([], failure) ]
light off and switch on light are procedures
provided by the lamp. The feedback to the user
and the update rules depend on the result of the
switch on light procedure.
</bodyText>
<sectionHeader confidence="0.996839" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995846153846">
Applying the idea of plug and play to spoken di-
alogue interfaces poses a number of interesting
and important problems. Since the linguistic and
dialogue management information is distributed
throughout the network, a plug and play system
must update its speech interface whenever a new
device is connected. In this paper, we have fo-
cussed in particular on distributed grammars for
plug and play speech recognition which we have
integrated into our demonstrator system. We have
also examined some issues and described a possi-
ble approach to distributed dialogue management
which we plan to undertake in further work.
</bodyText>
<sectionHeader confidence="0.996565" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999297857142857">
We are very grateful to our partners in the
DHomme project for discussion of the above ideas
- especially on the importance and role of differ-
ing strengths of Plug and Play. The DHomme
project partners include netdecisions Ltd, SRI In-
ternational, Telia Research AB, and the Universi-
ties of Edinburgh, Gothenburg and Seville.
</bodyText>
<sectionHeader confidence="0.990056" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.663102">
N.M. Fraser and J.H.S. Thornton. 1995. Vocalist:
A robust, portable spoken language dialogue
system for telephone applications. In Proc. of
Eurospeech &apos;95, pages 1947{1950, Madrid.
</bodyText>
<reference confidence="0.930351777777778">
Gerald Gazdar, Ewan Klein, Geoffrey Pullum,
and Ivan Sag. 1985. Generalized Phrase Struc-
ture Grammar. Harvard University Press, Cam-
bridge, MA.
J.R Glass. 1999. Challenges for spoken dialogue
systems. In Proc. IEEE ASRU Workshop, Key-
stone, CO.
A. Goldschen and D Loehr. 1999. The role of the
darpa communicator architecture as a human
computer interface for distributed simulations.
In 1999 SISO Spring Simulation Interoperabil-
ity Workshop, Orlando, Florida, March 1999.
SpeechWorks Int Inc, 2001. SpeechWorks.
http://www.speechworks.com. As at 31/01/01.
B. Kiefer and H. Krieger. 2000. A context-free
approximation of head-driven phrase structure
grammar. In Proceedings of 6th Int. Workshop
on Parsing Technologies, pages 135{146.
</reference>
<figureCaption confidence="0.992472">
Figure 1 - Example Dialogue
</figureCaption>
<bodyText confidence="0.976319166666667">
Network status: There is no TV. There is a VCR
in the TV room.
A. Kolzer. 1999. Universal dialogue specification
for conversational systems. In Proceedings of
IJCAI&apos;99 Workshop on Knowledge &amp; Reason-
ing In Practical Dialogue Systems, Stockholm.
</bodyText>
<reference confidence="0.973094257142857">
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the trindi dialogue
move engine toolkit. Nat.Lang. Engineering, 6.
Microsoft, 2000. Universal Plug and Play Device
Architecture. http://www.upnp.org. Version
1.0, 8 June 2000.
D. Milward. 2000. Distributing representation for
robust interpretation of dialogue utterances. In
Proc. of 38th ACL, Hong Kong, pages 133{141.
R. Moore. 1998. Using natural language knowl-
edge sources in speech recognition. In Proceed-
ings of the NATO Advanced Studies Institute.
Nuance Communications, 1999. Nuance Speech
Recognition System Developer&apos;s Manual version
6.2. 1380 Willow Road, Menlo Park, CA 94025.
S. Oaks and H. Wong. 2000. Jini in a Nutshell.
O&apos;Reilly.
Geoffrey K. Pullum and Gerald Gazdar. 1982.
natural languages and context-free languages.
Llinguistics and Philosophy, 4:471{504.
M. Rayner, J. Dowding, and B.A. Hockey. 2001a.
A baseline method for compiling typed unifica-
tion grammars into context free language mod-
els. Proc. Eurospeech 2001, Aalborg.
M. Rayner, G. Gorrell, B.A. Hockey, J. Dowding,
and J. Boye. 2001b. Do cfg based language
models need agreement constraints? In Pro-
ceedings of 2nd NAACL, Pittsburgh.
S. Sutton et al. 1998. Universal speech tools: The
cslu toolkit. In Proc. ICSLP-98, pages 3221{
3224.
U1 Switch on the television
S1 Sorry, try again
U2 Add new component (this must precede
device installation)
</reference>
<bodyText confidence="0.9961846">
Now, a TV is plugged into the living room. Cur-
rently we simulate this in a simple text interface,
where devices and rooms are chosen from prede-
fined lists. The speech interface is then automati-
cally reconfigured (this currently takes 29 seconds)
</bodyText>
<equation confidence="0.472414333333333">
U3 Switch on the television
S3 The TV is now on
U4 Switch on the television
</equation>
<bodyText confidence="0.62822334375">
S4 The TV is already on
U5a Add new component
Now, another TV is plugged into the kitchen. No
speech interface reconfiguration required this time
U5 Switch on the television
S5 The living-room TV is already on
U6 Switch on the VCR
S6 The VCR is now on
U7 Switch on the television
S7 Sorry, I don&apos;t know which TV you mean
U8 The television in the kitchen
S8 The kitchen TV is now on
U9a Add new component
Now, on-off lights are installed in the hall and
bathroom. Reconfiguration time is 27 seconds
U9 Switch on the light in the hall
S9 The hall light is now on
U10 Dim the light to fifty percent
S10 Sorry, try again.
U11a Add new component
Now, a dimmable light is installed in the kitchen.
Reconfiguration time is 39 seconds
U11 Dim the kitchen light to 50 percent
S11 The light in the kitchen is now on at fifty
percent
U12 Dim the light in the hall
S12 I&apos;m sorry, the hall light can only be on or
off
U13 Turn on the light in the kitchen (misheard
as &amp;quot;increase the light in the kitchen&amp;quot;)
S13 The kitchen light is now on at sixty per-
cent
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.236022">
<title confidence="0.999857">Plug and Play Speech Understanding</title>
<author confidence="0.999887">Manny Rayner</author>
<author confidence="0.999887">Ian</author>
<affiliation confidence="0.866529">amp; Genevieve</affiliation>
<email confidence="0.511013">netdecisions</email>
<affiliation confidence="0.778099">Wellington</affiliation>
<address confidence="0.951446">East Road, Cambridge C131 113H, UK</address>
<email confidence="0.998853">@netdecisions.com</email>
<author confidence="0.99966">Johan Boye</author>
<affiliation confidence="0.740068">Telia</affiliation>
<address confidence="0.976668">S-123 86 Farsta, Sweden</address>
<email confidence="0.989941">johan.boye@trab.se</email>
<abstract confidence="0.998047611111111">Plug and Play is an increasingly important concept in system and network architectures. We introduce and describe a spoken language dialogue system architecture which supports Plug and Playable networks of objects in its domain. Each device in the network carries the linguistic and dialogue management information which is pertinent to it and uploads it dynamically to the relevant language processing components in the spoken language interface. We describe the current state of our plug and play demonstrator and discuss theoretical issues that arise from our work. Plug and Play forms a central topic for the DHomme project.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="20104" citStr="Gazdar et al., 1985" startWordPosition="3237" endWordPosition="3240">VERB --&gt; dim DIMMABLE-NOUN --&gt; light DET --&gt; the Unfortunately, however, this defeats the original object of the exercise, since the &amp;quot;general&amp;quot; rules now make reference to the device-specific concept of dimming. What we want instead is a more generic treatment, like the following: COMMAND --&gt; TRANSITIVE-VERB:Esem-obj-type=T] NP:Esem-type=T] NP:Esem-type=T] --&gt; DET NOUN:Esem-type=T] DET --&gt; the TRANSITIVE-VERB:Esem-obj-type=dimmable] --&gt; dim NOUN:Esem-type=dimmable] --&gt; light This kind of parameterisation of a CFG is not in any way new: it is simply unification grammar (Pullum and Gazdar, 1982; Gazdar et al., 1985). Thus our first main idea is to raise the level of abstraction, formulating the device grammar at the level of unification grammars, and compiling these down into the underlying CFG representation. There are now a number of systems which can perform this type of compilation (Moore, 1998; Kiefer and Krieger, 2000); the basic methods we use in our system are described in detail elsewhere (Rayner et al., 2001a). Here, we focus on the aspects that are required for &amp;quot;distributed&amp;quot; unification grammars needed for Plug and Play. 4.2 &amp;quot;Unification grammars meet object-oriented programming&amp;quot;. Our basic id</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Glass</author>
</authors>
<title>Challenges for spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In Proc. IEEE ASRU Workshop,</booktitle>
<location>Keystone, CO.</location>
<contexts>
<context position="3542" citStr="Glass, 1999" startWordPosition="549" endWordPosition="550">tems containing different instantiations of the language processing components. The Communicator Architecture is not designed to address the special requirements of a plug and play domain. In fact, the Communicator architecture does not support the dynamic re-configuration of language processing components while the system is running. At a more general level, simple re-configuration of spoken language dialogue systems has of course long been a goal of language engineering. But such re-configuration is nearly always viewed as the problem of cross-domain or possibly crosslanguage porting, e.g. (Glass, 1999). Once one has a cinema ticket booking service, for example, one may examine the effort required for booking train tickets, or for e-shopping in general or even the &amp;quot;database access&amp;quot; scenario. There are various toolkits, architectures and methodologies for rapidly and/or semi-expertly generating new instances of dialogue systems, e.g. by abstracting away from domain or application dependent features of particular systems, e.g. (Fraser and Thornton, 1995; Kolzer, 1999), or `bottom-up&apos; by aggregation of useful re-configurable components , e.g. (Sutton et al, 1998; Larsson and Traum, 2000). The a</context>
</contexts>
<marker>Glass, 1999</marker>
<rawString>J.R Glass. 1999. Challenges for spoken dialogue systems. In Proc. IEEE ASRU Workshop, Keystone, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Goldschen</author>
<author>D Loehr</author>
</authors>
<title>The role of the darpa communicator architecture as a human computer interface for distributed simulations.</title>
<date>1999</date>
<booktitle>In 1999 SISO Spring Simulation Interoperability Workshop,</booktitle>
<location>Orlando, Florida,</location>
<contexts>
<context position="2772" citStr="Goldschen and Loehr, 1999" startWordPosition="436" endWordPosition="439">sh language plug and play demonstrator for controlling lamps, dimmers and sensors, previously described in (Rayner et al., 2001b). (There is also a Swedish instantiation). First, however, we need briefly to distinguish our notion from other notions of plug and play and reconfigurability. The notion of Plug and Play has been used for dialogue system toolkits in which the various different language processing components themselves (e.g. recognition, parsing, generation and dialogue management) can be plugged in and out. The most prominent instance of this is the Darpa Communicator architecture (Goldschen and Loehr, 1999), which defines interoperability standards for language processing components. The intention is simply that researchers and developers can experiment with systems containing different instantiations of the language processing components. The Communicator Architecture is not designed to address the special requirements of a plug and play domain. In fact, the Communicator architecture does not support the dynamic re-configuration of language processing components while the system is running. At a more general level, simple re-configuration of spoken language dialogue systems has of course long b</context>
</contexts>
<marker>Goldschen, Loehr, 1999</marker>
<rawString>A. Goldschen and D Loehr. 1999. The role of the darpa communicator architecture as a human computer interface for distributed simulations. In 1999 SISO Spring Simulation Interoperability Workshop, Orlando, Florida, March 1999.</rawString>
</citation>
<citation valid="false">
<date>2001</date>
<institution>SpeechWorks Int Inc,</institution>
<marker>2001</marker>
<rawString>SpeechWorks Int Inc, 2001. SpeechWorks. http://www.speechworks.com. As at 31/01/01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kiefer</author>
<author>H Krieger</author>
</authors>
<title>A context-free approximation of head-driven phrase structure grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of 6th Int. Workshop on Parsing Technologies,</booktitle>
<pages>135--146</pages>
<contexts>
<context position="20419" citStr="Kiefer and Krieger, 2000" startWordPosition="3290" endWordPosition="3293">sem-obj-type=T] NP:Esem-type=T] NP:Esem-type=T] --&gt; DET NOUN:Esem-type=T] DET --&gt; the TRANSITIVE-VERB:Esem-obj-type=dimmable] --&gt; dim NOUN:Esem-type=dimmable] --&gt; light This kind of parameterisation of a CFG is not in any way new: it is simply unification grammar (Pullum and Gazdar, 1982; Gazdar et al., 1985). Thus our first main idea is to raise the level of abstraction, formulating the device grammar at the level of unification grammars, and compiling these down into the underlying CFG representation. There are now a number of systems which can perform this type of compilation (Moore, 1998; Kiefer and Krieger, 2000); the basic methods we use in our system are described in detail elsewhere (Rayner et al., 2001a). Here, we focus on the aspects that are required for &amp;quot;distributed&amp;quot; unification grammars needed for Plug and Play. 4.2 &amp;quot;Unification grammars meet object-oriented programming&amp;quot;. Our basic idea is to start with a general deviceindependent unification grammar, which implements the core grammar rules. In our prototype, there are 34 core rules. Typical examples are the NP conjunction and PP modifications rules, schematically NP --&gt; NP CONJ NP NP --&gt; NP PP which are likely to occur in connection with any </context>
</contexts>
<marker>Kiefer, Krieger, 2000</marker>
<rawString>B. Kiefer and H. Krieger. 2000. A context-free approximation of head-driven phrase structure grammar. In Proceedings of 6th Int. Workshop on Parsing Technologies, pages 135{146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Larsson</author>
<author>D Traum</author>
</authors>
<title>Information state and dialogue management in the trindi dialogue move engine toolkit.</title>
<date>2000</date>
<journal>Nat.Lang. Engineering,</journal>
<volume>6</volume>
<contexts>
<context position="4135" citStr="Larsson and Traum, 2000" startWordPosition="635" endWordPosition="638">age porting, e.g. (Glass, 1999). Once one has a cinema ticket booking service, for example, one may examine the effort required for booking train tickets, or for e-shopping in general or even the &amp;quot;database access&amp;quot; scenario. There are various toolkits, architectures and methodologies for rapidly and/or semi-expertly generating new instances of dialogue systems, e.g. by abstracting away from domain or application dependent features of particular systems, e.g. (Fraser and Thornton, 1995; Kolzer, 1999), or `bottom-up&apos; by aggregation of useful re-configurable components , e.g. (Sutton et al, 1998; Larsson and Traum, 2000). The automated within-domain reconfiguration required for a plug and play domain, has not, to our knowledge, been described previously. Pursuit of plug and play functionality (and its realization in strong and weak forms - discussed in section 3) forms a central theme of the DHomme project. 1 In the rest of this paper, we begin by detailing our concrete Plug and Play scenario - device control in the home - with an example dialogue from our demonstrator and an outline of the main dialogue processing elements. In section 3, we distinguish strong and weak notions of Plug and Play and their appli</context>
</contexts>
<marker>Larsson, Traum, 2000</marker>
<rawString>S. Larsson and D. Traum. 2000. Information state and dialogue management in the trindi dialogue move engine toolkit. Nat.Lang. Engineering, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Microsoft</author>
</authors>
<title>Universal Plug and Play Device Architecture.</title>
<date>2000</date>
<journal>http://www.upnp.org. Version</journal>
<volume>1</volume>
<contexts>
<context position="1954" citStr="Microsoft, 2000" startWordPosition="308" endWordPosition="309">ing • devices announce themselves on the network when they are plugged into it (and also discover the existence of others) • devices describe their own capabilities, provide a means for accessing them and can query and access the capabilities of others • devices should support, where possible, seamless interaction with other devices. Plug and Play is, not surprisingly, viewed as a pre-requisite for the commercial success of networked devices in the home. There are already several promising candidate platforms for achieving the necessary functionality, including Universal Plug and Play (UPnP) (Microsoft, 2000) and Jini (Oaks and Wong, 2000). In this paper, we address the requirements on spoken dialogue interfaces that arise from a plug and play domain. We also present the current state of our English language plug and play demonstrator for controlling lamps, dimmers and sensors, previously described in (Rayner et al., 2001b). (There is also a Swedish instantiation). First, however, we need briefly to distinguish our notion from other notions of plug and play and reconfigurability. The notion of Plug and Play has been used for dialogue system toolkits in which the various different language processi</context>
</contexts>
<marker>Microsoft, 2000</marker>
<rawString>Microsoft, 2000. Universal Plug and Play Device Architecture. http://www.upnp.org. Version 1.0, 8 June 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
</authors>
<title>Distributing representation for robust interpretation of dialogue utterances.</title>
<date>2000</date>
<booktitle>In Proc. of 38th ACL, Hong Kong,</booktitle>
<pages>133--141</pages>
<contexts>
<context position="16004" citStr="Milward, 2000" startWordPosition="2588" endWordPosition="2589">anguage referring to two or more different devices. For example, a command may address several devices at once (&amp;quot;turn on the radio and the living room light&amp;quot;); alternately, several commands may be combined into a single utterance (&amp;quot;switch on the cooker and switch off the microwave&amp;quot;). Our experience with practical spoken device interfaces suggests that examples like these are by no means uncommon. Another architecture relatively easy to combine with Plug and Play is doing recognition through a general large-vocabulary recogniser, and language-processing through device-specific phrase-spotting (Milward, 2000). The recogniser stays the same irrespective of how many devices are connected, so there are by definition no problems at the level of speech recognition, and it is in principle possible to support complex commands. The main drawback, however, is that recognition quality is markedly inferior compared to a system in which recognition coverage is limited to the domain defined by the current set of devices. Modern speech interfaces supporting complex commands are typically specified using a rulebased grammar formalism defined by a platform like Nuance (Nuance Communications, 1999) or SpeechWorks </context>
</contexts>
<marker>Milward, 2000</marker>
<rawString>D. Milward. 2000. Distributing representation for robust interpretation of dialogue utterances. In Proc. of 38th ACL, Hong Kong, pages 133{141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
</authors>
<title>Using natural language knowledge sources in speech recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of the NATO Advanced Studies Institute.</booktitle>
<contexts>
<context position="20392" citStr="Moore, 1998" startWordPosition="3288" endWordPosition="3289">SITIVE-VERB:Esem-obj-type=T] NP:Esem-type=T] NP:Esem-type=T] --&gt; DET NOUN:Esem-type=T] DET --&gt; the TRANSITIVE-VERB:Esem-obj-type=dimmable] --&gt; dim NOUN:Esem-type=dimmable] --&gt; light This kind of parameterisation of a CFG is not in any way new: it is simply unification grammar (Pullum and Gazdar, 1982; Gazdar et al., 1985). Thus our first main idea is to raise the level of abstraction, formulating the device grammar at the level of unification grammars, and compiling these down into the underlying CFG representation. There are now a number of systems which can perform this type of compilation (Moore, 1998; Kiefer and Krieger, 2000); the basic methods we use in our system are described in detail elsewhere (Rayner et al., 2001a). Here, we focus on the aspects that are required for &amp;quot;distributed&amp;quot; unification grammars needed for Plug and Play. 4.2 &amp;quot;Unification grammars meet object-oriented programming&amp;quot;. Our basic idea is to start with a general deviceindependent unification grammar, which implements the core grammar rules. In our prototype, there are 34 core rules. Typical examples are the NP conjunction and PP modifications rules, schematically NP --&gt; NP CONJ NP NP --&gt; NP PP which are likely to oc</context>
</contexts>
<marker>Moore, 1998</marker>
<rawString>R. Moore. 1998. Using natural language knowledge sources in speech recognition. In Proceedings of the NATO Advanced Studies Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuance Communications</author>
</authors>
<title>Nuance Speech Recognition System Developer&apos;s Manual version 6.2. 1380 Willow Road,</title>
<date>1999</date>
<pages>94025</pages>
<location>Menlo Park, CA</location>
<contexts>
<context position="14009" citStr="Communications, 1999" startWordPosition="2271" endWordPosition="2273">nd Play notion. Furthermore, different instantiations of these components may require very different sorts of update. To take a very simple example, if recognition is carried out by a statistically trained language model, then updating this with information pertinent to a particular device will evidently be a significantly different task from updating a recognizer which uses a grammar-based language model. Our current demonstrator program instantiates a Plug and Play capability for recognition, parsing and context independent semantic analysis and is built on top of the Nuance toolkit (Nuance Communications, 1999). The next section discusses the capability in detail. Section 5 discusses and makes some proposals for Plug and Play Dialogue Management. 4 Distributed Grammar 4.1 Introduction In this section, we will describe how we have addressed the issues that arise when we attempt to apply the (strong) Plug and Play scenario to the tasks of speech recognition and language processing. Each device will provide the knowledge that the speech interface needs in order to recognise the new types of utterance relevant to the device in question, and convert these utterances into wellformed semantic representatio</context>
<context position="16588" citStr="Communications, 1999" startWordPosition="2680" endWordPosition="2681">cific phrase-spotting (Milward, 2000). The recogniser stays the same irrespective of how many devices are connected, so there are by definition no problems at the level of speech recognition, and it is in principle possible to support complex commands. The main drawback, however, is that recognition quality is markedly inferior compared to a system in which recognition coverage is limited to the domain defined by the current set of devices. Modern speech interfaces supporting complex commands are typically specified using a rulebased grammar formalism defined by a platform like Nuance (Nuance Communications, 1999) or SpeechWorks (Inc, 2001). The type of grammar supported is some subset of full CFG, extended to include semantic annotations. Grammar rules define the language model that constrains the recognition process, tuning it to the domain in order to achieve high performance. (They also supply the semantic rules that define the output representation; we will return to this point later). If we want to implement an ambitious Plug and Play speech recognition module within this kind of framework, we have two top-level goals. On the one hand, we want to achieve high-quality speech recognition. At the sa</context>
</contexts>
<marker>Communications, 1999</marker>
<rawString>Nuance Communications, 1999. Nuance Speech Recognition System Developer&apos;s Manual version 6.2. 1380 Willow Road, Menlo Park, CA 94025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oaks</author>
<author>H Wong</author>
</authors>
<date>2000</date>
<note>Jini in a Nutshell. O&apos;Reilly.</note>
<contexts>
<context position="1985" citStr="Oaks and Wong, 2000" startWordPosition="312" endWordPosition="315">selves on the network when they are plugged into it (and also discover the existence of others) • devices describe their own capabilities, provide a means for accessing them and can query and access the capabilities of others • devices should support, where possible, seamless interaction with other devices. Plug and Play is, not surprisingly, viewed as a pre-requisite for the commercial success of networked devices in the home. There are already several promising candidate platforms for achieving the necessary functionality, including Universal Plug and Play (UPnP) (Microsoft, 2000) and Jini (Oaks and Wong, 2000). In this paper, we address the requirements on spoken dialogue interfaces that arise from a plug and play domain. We also present the current state of our English language plug and play demonstrator for controlling lamps, dimmers and sensors, previously described in (Rayner et al., 2001b). (There is also a Swedish instantiation). First, however, we need briefly to distinguish our notion from other notions of plug and play and reconfigurability. The notion of Plug and Play has been used for dialogue system toolkits in which the various different language processing components themselves (e.g. </context>
</contexts>
<marker>Oaks, Wong, 2000</marker>
<rawString>S. Oaks and H. Wong. 2000. Jini in a Nutshell. O&apos;Reilly.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
<author>Gerald Gazdar</author>
</authors>
<title>natural languages and context-free languages. Llinguistics and Philosophy,</title>
<date>1982</date>
<pages>4--471</pages>
<contexts>
<context position="20082" citStr="Pullum and Gazdar, 1982" startWordPosition="3233" endWordPosition="3236"> DIMMABLE-NOUN TRANS-DIM-VERB --&gt; dim DIMMABLE-NOUN --&gt; light DET --&gt; the Unfortunately, however, this defeats the original object of the exercise, since the &amp;quot;general&amp;quot; rules now make reference to the device-specific concept of dimming. What we want instead is a more generic treatment, like the following: COMMAND --&gt; TRANSITIVE-VERB:Esem-obj-type=T] NP:Esem-type=T] NP:Esem-type=T] --&gt; DET NOUN:Esem-type=T] DET --&gt; the TRANSITIVE-VERB:Esem-obj-type=dimmable] --&gt; dim NOUN:Esem-type=dimmable] --&gt; light This kind of parameterisation of a CFG is not in any way new: it is simply unification grammar (Pullum and Gazdar, 1982; Gazdar et al., 1985). Thus our first main idea is to raise the level of abstraction, formulating the device grammar at the level of unification grammars, and compiling these down into the underlying CFG representation. There are now a number of systems which can perform this type of compilation (Moore, 1998; Kiefer and Krieger, 2000); the basic methods we use in our system are described in detail elsewhere (Rayner et al., 2001a). Here, we focus on the aspects that are required for &amp;quot;distributed&amp;quot; unification grammars needed for Plug and Play. 4.2 &amp;quot;Unification grammars meet object-oriented prog</context>
</contexts>
<marker>Pullum, Gazdar, 1982</marker>
<rawString>Geoffrey K. Pullum and Gerald Gazdar. 1982. natural languages and context-free languages. Llinguistics and Philosophy, 4:471{504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>J Dowding</author>
<author>B A Hockey</author>
</authors>
<title>A baseline method for compiling typed unification grammars into context free language models.</title>
<date>2001</date>
<booktitle>Proc. Eurospeech</booktitle>
<location>Aalborg.</location>
<contexts>
<context position="2273" citStr="Rayner et al., 2001" startWordPosition="361" endWordPosition="364">with other devices. Plug and Play is, not surprisingly, viewed as a pre-requisite for the commercial success of networked devices in the home. There are already several promising candidate platforms for achieving the necessary functionality, including Universal Plug and Play (UPnP) (Microsoft, 2000) and Jini (Oaks and Wong, 2000). In this paper, we address the requirements on spoken dialogue interfaces that arise from a plug and play domain. We also present the current state of our English language plug and play demonstrator for controlling lamps, dimmers and sensors, previously described in (Rayner et al., 2001b). (There is also a Swedish instantiation). First, however, we need briefly to distinguish our notion from other notions of plug and play and reconfigurability. The notion of Plug and Play has been used for dialogue system toolkits in which the various different language processing components themselves (e.g. recognition, parsing, generation and dialogue management) can be plugged in and out. The most prominent instance of this is the Darpa Communicator architecture (Goldschen and Loehr, 1999), which defines interoperability standards for language processing components. The intention is simpl</context>
<context position="20514" citStr="Rayner et al., 2001" startWordPosition="3308" endWordPosition="3311">:Esem-obj-type=dimmable] --&gt; dim NOUN:Esem-type=dimmable] --&gt; light This kind of parameterisation of a CFG is not in any way new: it is simply unification grammar (Pullum and Gazdar, 1982; Gazdar et al., 1985). Thus our first main idea is to raise the level of abstraction, formulating the device grammar at the level of unification grammars, and compiling these down into the underlying CFG representation. There are now a number of systems which can perform this type of compilation (Moore, 1998; Kiefer and Krieger, 2000); the basic methods we use in our system are described in detail elsewhere (Rayner et al., 2001a). Here, we focus on the aspects that are required for &amp;quot;distributed&amp;quot; unification grammars needed for Plug and Play. 4.2 &amp;quot;Unification grammars meet object-oriented programming&amp;quot;. Our basic idea is to start with a general deviceindependent unification grammar, which implements the core grammar rules. In our prototype, there are 34 core rules. Typical examples are the NP conjunction and PP modifications rules, schematically NP --&gt; NP CONJ NP NP --&gt; NP PP which are likely to occur in connection with any kind of device. These rules are parameterised by various features. For example, the set of feat</context>
</contexts>
<marker>Rayner, Dowding, Hockey, 2001</marker>
<rawString>M. Rayner, J. Dowding, and B.A. Hockey. 2001a. A baseline method for compiling typed unification grammars into context free language models. Proc. Eurospeech 2001, Aalborg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>G Gorrell</author>
<author>B A Hockey</author>
<author>J Dowding</author>
<author>J Boye</author>
</authors>
<title>Do cfg based language models need agreement constraints?</title>
<date>2001</date>
<booktitle>In Proceedings of 2nd NAACL,</booktitle>
<location>Pittsburgh.</location>
<contexts>
<context position="2273" citStr="Rayner et al., 2001" startWordPosition="361" endWordPosition="364">with other devices. Plug and Play is, not surprisingly, viewed as a pre-requisite for the commercial success of networked devices in the home. There are already several promising candidate platforms for achieving the necessary functionality, including Universal Plug and Play (UPnP) (Microsoft, 2000) and Jini (Oaks and Wong, 2000). In this paper, we address the requirements on spoken dialogue interfaces that arise from a plug and play domain. We also present the current state of our English language plug and play demonstrator for controlling lamps, dimmers and sensors, previously described in (Rayner et al., 2001b). (There is also a Swedish instantiation). First, however, we need briefly to distinguish our notion from other notions of plug and play and reconfigurability. The notion of Plug and Play has been used for dialogue system toolkits in which the various different language processing components themselves (e.g. recognition, parsing, generation and dialogue management) can be plugged in and out. The most prominent instance of this is the Darpa Communicator architecture (Goldschen and Loehr, 1999), which defines interoperability standards for language processing components. The intention is simpl</context>
<context position="20514" citStr="Rayner et al., 2001" startWordPosition="3308" endWordPosition="3311">:Esem-obj-type=dimmable] --&gt; dim NOUN:Esem-type=dimmable] --&gt; light This kind of parameterisation of a CFG is not in any way new: it is simply unification grammar (Pullum and Gazdar, 1982; Gazdar et al., 1985). Thus our first main idea is to raise the level of abstraction, formulating the device grammar at the level of unification grammars, and compiling these down into the underlying CFG representation. There are now a number of systems which can perform this type of compilation (Moore, 1998; Kiefer and Krieger, 2000); the basic methods we use in our system are described in detail elsewhere (Rayner et al., 2001a). Here, we focus on the aspects that are required for &amp;quot;distributed&amp;quot; unification grammars needed for Plug and Play. 4.2 &amp;quot;Unification grammars meet object-oriented programming&amp;quot;. Our basic idea is to start with a general deviceindependent unification grammar, which implements the core grammar rules. In our prototype, there are 34 core rules. Typical examples are the NP conjunction and PP modifications rules, schematically NP --&gt; NP CONJ NP NP --&gt; NP PP which are likely to occur in connection with any kind of device. These rules are parameterised by various features. For example, the set of feat</context>
</contexts>
<marker>Rayner, Gorrell, Hockey, Dowding, Boye, 2001</marker>
<rawString>M. Rayner, G. Gorrell, B.A. Hockey, J. Dowding, and J. Boye. 2001b. Do cfg based language models need agreement constraints? In Proceedings of 2nd NAACL, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sutton</author>
</authors>
<title>Universal speech tools: The cslu toolkit.</title>
<date>1998</date>
<booktitle>In Proc. ICSLP-98,</booktitle>
<pages>3221--3224</pages>
<marker>Sutton, 1998</marker>
<rawString>S. Sutton et al. 1998. Universal speech tools: The cslu toolkit. In Proc. ICSLP-98, pages 3221{ 3224.</rawString>
</citation>
<citation valid="false">
<booktitle>U1 Switch on the television S1 Sorry, try again U2 Add new component (this must precede device installation)</booktitle>
<marker></marker>
<rawString>U1 Switch on the television S1 Sorry, try again U2 Add new component (this must precede device installation)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>