<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015504">
<title confidence="0.928485">
UT-DB: An Experimental Study on Sentiment Analysis in Twitter
</title>
<author confidence="0.993909">
Zhemin Zhu Djoerd Hiemstra Peter Apers Andreas Wombacher
</author>
<affiliation confidence="0.988008">
CTIT Database Group, University of Twente
</affiliation>
<address confidence="0.810748">
Drienerlolaan 5, 7500 AE, Enschede, The Netherlands
</address>
<email confidence="0.984197">
{z.zhu, d.hiemstra, p.m.g.apers, A.Wombacher}@utwente.nl
</email>
<sectionHeader confidence="0.995542" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998371421052632">
This paper describes our system for participat-
ing SemEval2013 Task2-B (Kozareva et al.,
2013): Sentiment Analysis in Twitter. Given
a message, our system classifies whether the
message is positive, negative or neutral senti-
ment. It uses a co-occurrence rate model. The
training data are constrained to the data pro-
vided by the task organizers (No other tweet
data are used). We consider 9 types of fea-
tures and use a subset of them in our submitted
system. To see the contribution of each type of
features, we do experimental study on features
by leaving one type of features out each time.
Results suggest that unigrams are the most im-
portant features, bigrams and POS tags seem
not helpful, and stopwords should be retained
to achieve the best results. The overall results
of our system are promising regarding the con-
strained features and data we use.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999093063829787">
The past years have witnessed the emergence and
popularity of short messages such as tweets and
SMS messages. Comparing with the traditional gen-
res such as newswire data, tweets are very short and
use informal grammar and expressions. The short-
ness and informality make them a new genre and
bring new challenges to sentiment analysis (Pang et
al., 2002) as well as other NLP applications such
named entity recognition (Habib et al., 2013).
Recently a wide range of methods and features
have been applied to sentimental analysis over
tweets. Go et al. (2009) train sentiment classi-
fiers using machine learning methods, such as Naive
Bayes, Maximum Entropy and SVMs, with different
combinations of features such as unigrams, bigrams
and Part-of-Speech (POS) tags. Microblogging fea-
tures such as hashtags, emoticons, abbreviations, all-
caps and character repetitions are also found help-
ful (Kouloumpis et al., 2011). Saif et al. (2012)
train Naive Bayes models with semantic features.
Also the lexicon prior polarities have been proved
very useful (Agarwal et al., 2011). Davidov et al.
(2010) utilize hashtags and smileys to build a large-
scale annotated tweet dataset automatically. This
avoids the need for labour intensive manual anno-
tation. Due to the fact that tweets are generated con-
stantly, sentiment analysis over tweets has some in-
teresting applications, such as predicting stock mar-
ket movement (Bollen et al., 2011) and predicting
election results (Tumasjan et al., 2010; O’Connor et
al., 2010).
But there are still some unclear parts in the lit-
erature. For example, it is unclear whether using
POS tags improves the sentiment analysis perfor-
mance or not. Conflicting results are reported (Pak
and Paroubek, 2010; Go et al., 2009). It is also
a little surprising that not removing stopwords in-
creases performance (Saif et al., 2012). In this pa-
per, we build a system based on the concept of co-
occurrence rate. 9 different types of features are con-
sidered. We find that using a subset of these features
achieves the best results in our system, so we use
this subset of features rather than all the 9 types of
features in our submitted system. To see the contri-
bution of each type of features, we perform experi-
ments by leaving one type of features out each time.
Results show that unigrams are the most important
</bodyText>
<page confidence="0.98375">
384
</page>
<bodyText confidence="0.9956035">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 384–389, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
tags and emoticons are also considered as un-
igrams. Some of the unigrams are stopwords
which will be discussed in the next section.
features, bigrams and POS tags seem not helpful,
and retaining stopwords makes the results better.
The overall results of our system are also promis-
ing regarding the constrained features and data we
use.
</bodyText>
<listItem confidence="0.693961">
2. Bigrams. We consider two adjacent lemmas as
bigrams.
</listItem>
<sectionHeader confidence="0.831597" genericHeader="method">
2 System Description
</sectionHeader>
<subsectionHeader confidence="0.994239">
2.1 Method
</subsectionHeader>
<bodyText confidence="0.99998075">
We use a supervised method which is similar to the
Naive Bayes classifier. The score of a tweet, denoted
by t, and a sentiment category, denoted by c, is cal-
culated according to the following formula:
</bodyText>
<equation confidence="0.997375666666667">
n
Score(t, c) = [ log CR(fi, c)] + log P(c),
i=1
</equation>
<bodyText confidence="0.9996275625">
where fi is a feature extracted from t. The sentiment
category c can be positive, negative or neutral. And
CR(fi, c) is Co-occurrence Rate (CR) of fi and c
which can be obtained as follows:
where #(*) is the number of times that the pattern
* appears in the training dataset. Then the category
of the highest score arg max, Score(t, c) is the pre-
diction.
This method assumes all the features are inde-
pendent which is also the assumption of the Naive
Bayes model. But our model excludes P(fi) be-
cause they are observations. Hence comparing with
Naive Bayes, our model saves the effort to model
feature distributions P(fi). Also this method can
be trained efficiently because it only depends on the
empirical distributions.
</bodyText>
<subsectionHeader confidence="0.982506">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.9996482">
To make our system general, we constrain to the text
features. That is we do not use the features outside
the tweet texts such as features related to the user
profiles, discourse information or links. The follow-
ing 9 types of features are considered:
</bodyText>
<footnote confidence="0.43506375">
1. Unigrams. We use lemmas as the form of un-
igrams. The lemmas are obtained by the Stan-
ford CoreNLP1 (Toutanova et al., 2003). Hash-
1http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<listItem confidence="0.999284">
3. Named entities. We use the CMU Twitter Tag-
ger (Gimpel et al., 2011; Owoputi et al., 2013)2
to recognize named entities. The tokens cov-
ered by a named entity are not considered as
unigrams any more. Instead a named entity as
a whole is treated as a single feature.
4. Dependency relations. Dependency relations
</listItem>
<bodyText confidence="0.976174838709677">
are helpful to the sentiment prediction. Here we
give an example to explain this type of features.
In the tweet “I may not be able to vote from
Britain but I COMPLETLEY support you!!!!”,
the dependency relation between the word ‘not’
and ‘able’ is ‘NEG’ which stands for nega-
tion, and the dependency relation between the
word ‘COMPLETELY’ and ‘support’ is ‘ADV-
MOD’ which means adverb modifier. For this
example, we add ‘NEG able’ and ‘completely
support’ as dependency features to our system.
We use Stanford CoreNLP (Klein and Man-
ning, 2003a; Klein and Manning, 2003b) to ob-
tain dependencies. And we only consider two
types of dependencies ‘NEG’ and ‘ADVMOD’.
Other dependency relations are not helpful.
5. Lexicon prior polarity. The prior polarity of
lexicons have been proved very useful to sen-
timent analysis. Many lexicon resources have
been developed. But for a single lexicon re-
source, the coverage is limited. To achieve
better coverage, we merge three lexicon re-
sources. The first one is SentiStrength3 (Ku-
cuktunc et al., 2012). SentiStrength provides a
fine-granularity system for grading lexicon po-
larity which ranges from −5 (most negative) to
+5 (most positive). Our grading system con-
sists of three categories: negative, neutral and
positive. So we map the words ranging from
−5 to −1 in SentiStrength to negative in our
grading system, and the words ranging from
</bodyText>
<footnote confidence="0.993823">
2http://www.ark.cs.cmu.edu/TweetNLP/
3http://sentistrength.wlv.ac.uk/
</footnote>
<equation confidence="0.9973404">
P(fi, c) #(fi, c)
CR(f, c) =
P (f #(f
i)P (c) oc i)#(c)
,
</equation>
<page confidence="0.987573">
385
</page>
<bodyText confidence="0.6372936">
+1 to +5 to positive. The rest are mapped
to neutral. We do the same for the other two
lexicon resources: OpinionFinder4 (Wiebe et
al., 2005) and SentiWordNet5 (Esuli and Sebas-
tiani, 2006; Baccianella and Sebastiani, 2010).
</bodyText>
<listItem confidence="0.956596088235294">
6. Intensifiers. The tweets containing intensifiers
are more likely to be non-neutral. In the sub-
mitted system, we merge the boosters in Sen-
tiStrength and the intensifiers in OpinionFinder
to form a list of intensifiers. Some of these in-
tensifiers strengthen emotion (e.g. ‘definitely’),
but others weaken emotion (e.g. ‘slightly’).
They are distinguished and assigned with dif-
ferentlabels {intensifier strengthen,
intensifier weaken}.
7. All-caps and repeat characters. All-caps6 and
repeat characters are common expressions in
tweets to make emphasis on the applied tokens.
They can be considered as implicit intensifiers.
In our system, we first normalize the repeat
characters. For example, happyyyy is nor-
malized to happy as there are &gt; 3 consequent
y. Then they are treated in the same way as
intensifier features discussed above.
8. Interrogative sentence. Interrogative sentences
are more likely to be neutral. So we add if a
tweet includes interrogative sentences as a fea-
ture to our system. The sentences ending with
a question mark ‘?’ are considered as inter-
rogative sentences. We first use the Stanford
CoreNLP to find the sentence boundaries in a
tweet, then check the ending mark of each sen-
tence.
9. Imperative sentence. Intuitively, imperative
sentences are more likely to be negative. So
if a tweet contains imperative sentences can be
a feature. We consider the sentences start with
a verb as imperative sentences. The verbs are
identified by the CMU Twitter Tagger.
</listItem>
<bodyText confidence="0.9991415">
We further filter out the low-frequency features
which have been observed less than 3 times in the
</bodyText>
<footnote confidence="0.994874333333333">
4https://code.google.com/p/opinionfinder/
5http://sentiwordnet.isti.cnr.it/
6All characters of a token are in upper case.
</footnote>
<bodyText confidence="0.9998175">
training data. Because these features are not stable
indicators of sentiment. Our experiments show that
removing these low-frequency features increases the
accuracy.
</bodyText>
<subsectionHeader confidence="0.99976">
2.3 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999527857142857">
The pre-processing of our system includes two steps.
In the first step, we replace the abbreviations as de-
scribed in Section 2.3.1. In the second step, we use
the CMU Twitter Tagger to extract the features of
emoticons (e.g. :)), hashtags (e.g. #Friday), re-
ciepts (e.g. @Peter) and URLs, and remove these
symbols from tweet texts for further processing.
</bodyText>
<subsectionHeader confidence="0.79582">
2.3.1 Replacing Abbreviations
</subsectionHeader>
<bodyText confidence="0.9999455">
Abbreviations are replaced by their original ex-
pressions. We use the Internet Lingo Dictionary
(Wasden, 2010) to obtain the original expressions
of abbreviations. This dictionary originally contains
748 acronyms. But we do not use the acronyms in
which all characters are digits. Because we find they
are more likely to be numbers than acronyms. This
results in 735 acronyms.
</bodyText>
<sectionHeader confidence="0.999704" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999944833333333">
Our system is implemented in Java and organized
as a pipeline consisting of a sequence of annotators
and extractors. This architecture is very similar to
the framework of UIMA (Ferrucci and Lally, 2004).
With such an architecture, we can easily vary the
configurations of our system.
</bodyText>
<subsectionHeader confidence="0.98845">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999968142857143">
We use the standard dataset provided by Se-
mEval2013 Task2-B (Kozareva et al., 2013) for
training and testing. The training and develop-
ment data provided are merged together to train our
model. Originally, the training and development
data contain 9,684 and 1,654 instances, respectively.
But due to the policy of Twitter, only the tweet IDs
can be released publicly. So we need to fetch the ac-
tual tweets by their IDs. Some of the tweets are no
longer existing after they were downloaded for an-
notation. So the number of tweets used for training
is less than the original tweets provided by the orga-
nizers. In our case, we obtained 10,370 tweets for
training our model.
</bodyText>
<page confidence="0.997342">
386
</page>
<table confidence="0.999838">
Class Precision Recall F-Score
Positive 74.86 60.05 66.64
Negative 47.80 59.73 53.11
Neutral 67.02 73.60 70.15
Avg (Pos &amp; Neg) 61.33 59.89 59.87
</table>
<tableCaption confidence="0.986664">
Table 1: Submitted System on Twitter Data
</tableCaption>
<table confidence="0.99994">
Class Precision Recall F-Score
Positive 54.81 57.93 56.32
Negative 37.87 67.77 48.59
Neutral 80.78 58.11 67.60
Avg (Pos &amp; Neg) 46.34 62.85 52.46
</table>
<tableCaption confidence="0.99893">
Table 2: Submitted System on SMS Data
</tableCaption>
<bodyText confidence="0.999931333333333">
There are two test datasets: Twitter and SMS. The
first dataset consists of 3,813 twitter messages and
the second dataset contains 2,094 SMS messages.
The purpose of having a separate test set of SMS
messages is to see how well systems trained on twit-
ter data will generalize to other types of data.
</bodyText>
<subsectionHeader confidence="0.997337">
3.2 Results of Our Submitted System
</subsectionHeader>
<bodyText confidence="0.99998625">
We use a subset of features described in Section 2.2
in our submitted system: unigrams, named entities,
dependency relations, lexicon prior polarity, inten-
sifiers, all-caps and repeat characters, interrogative
and imperative sentences. The official results on the
two datasets are given in Table (1, 2). Our system is
ranked as #14/51 on the Twitter dataset and #18/44
on the SMS dataset.
</bodyText>
<subsectionHeader confidence="0.990058">
3.3 Feature Contribution Analysis
</subsectionHeader>
<bodyText confidence="0.999533785714286">
To see the contribution of each type of features, we
vary the configuration of our system by leaving one
type of features out each time. The results are listed
in Table 3.
In Table 3, ‘Y(T)’ means the corresponding fea-
ture is used and the test dataset is the Twitter Data,
and ‘N(sms)’ means the corresponding feature is left
out and the test dataset is SMS Data.
From Table 3, we can see that unigrams are the
most important features. Leaving out unigrams
leads to a radical decrease of F-scores. On the Twit-
ter dataset, the F-score drops from 59.87 to 41.44,
and on the SMS dataset, the F-score drops from
52.64 to 35.09. And also filtering out the low-
</bodyText>
<table confidence="0.999744">
Feature Y(T) N(T) Y(sms) N(sms)
Stopword 59.87 58.19 52.64 51.00
POS Tag 58.68 59.87 51.87 52.64
Bigram 58.47 59.87 51.94 52.64
Unigram 59.87 41.22 52.64 35.09
3 &lt; 59.87 57.66 52.64 51.20
Intensifier 59.87 59.47 52.64 52.39
Lexicon 59.87 58.33 52.64 51.26
Named Ent. 59.87 59.71 52.64 51.80
Interrogative 59.87 59.67 52.64 52.93
Imperative 59.87 59.54 52.64 52.14
Dependence 59.87 59.37 52.64 52.08
</table>
<tableCaption confidence="0.999797">
Table 3: Avg (Pos &amp; Neg) of Leave-one-out Experiments
</tableCaption>
<bodyText confidence="0.999893789473684">
frequency features which happens less than 3 times
increases the F-scores on Twitter data from 57.66 to
59.87, and on SMS data from 51.20 to 52.64. Re-
moving stopwords decreases the scores by 1.66 per-
cent. This result is consistent with that reported by
Saif et al. (2012). By taking a close look at the
stopwords we use, we find that some of the stop-
words are highly related to the sentiment polarity,
such as ‘can’, ‘no’, ‘very’ and ‘want’, but others
are not, such as ‘the’, ‘him’ and ‘on’. Removing
the stopwords which are related to the sentiment
is obviously harmful. This means the stopwords
which originally developed for the purpose of in-
formation retrieval are not suitable for sentimental
analysis. Dependency relations are also helpful fea-
tures which increase F-scores by about 0.5 percent.
The POS tags and bigrams seem not helpful in our
experiments, which is consistent with the results re-
ported by (Kouloumpis et al., 2011).
</bodyText>
<sectionHeader confidence="0.997111" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999895142857143">
We described the method and features used in our
system. We also did analysis on feautre contribu-
tion. Experiment results suggest that unigrams are
the most important features, POS tags and bigrams
seem not helpful, filtering out the low-frequency fea-
tures is helpful and retaining stopwords makes the
results better.
</bodyText>
<sectionHeader confidence="0.979885" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.80049">
This work has been supported by the Dutch national
program COMMIT.
</bodyText>
<page confidence="0.997162">
387
</page>
<sectionHeader confidence="0.960832" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999417903846154">
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, LSM ’11, pages 30–38,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC’10), Valletta, Malta, May. European Language
Resources Association (ELRA).
J. Bollen, H. Mao, and X. Zeng. 2011. Twitter mood
predicts the stock market. Journal of Computational
Science.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ’10, pages 241–249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC06, pages
417–422.
David Ferrucci and Adam Lally. 2004. Uima: an archi-
tectural approach to unstructured information process-
ing in the corporate research environment. Nat. Lang.
Eng., 10(3-4):327–348, September.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
’11, pages 42–47, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report, Stanford University.
M. B. Habib, M. van Keulen, and Z. Zhu. 2013. Con-
cept extraction challenge: University of twente at
#msm2013. In Proceedings of the 3rd workshop on
’Making Sense of Microposts’ (#MSM2013), Rio de
Janeiro, Brazil, Brazil, May. CEUR.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. In Advances in Neural Information
Processing Systems, volume 15. MIT Press.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In Lada A. Adamic, Ricardo A.
Baeza-Yates, and Scott Counts, editors, ICWSM. The
AAAI Press.
Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara
Rosenthal, Veselin Stoyonov, and Theresa Wilson.
2013. Sentiment analysis in twitter. In Proceedings
of the 7th International Workshop on Semantic Evalu-
ation. Association for Computation Linguistics.
Onur Kucuktunc, B. Barla Cambazoglu, Ingmar Weber,
and Hakan Ferhatosmanoglu. 2012. A large-scale
sentiment analysis for yahoo! answers. In Proceed-
ings of the fifth ACM international conference on Web
search and data mining, WSDM ’12, pages 633–642,
New York, NY, USA. ACM.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proceedings of the Fourth
International Conference on Weblogs and Social
Media, ICWSM 2010, Washington, DC, USA, May
23-26, 2010.
Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Hello, who is calling?: Can words reveal the social
nature of conversations? In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, June.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC’10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing - Volume 10, EMNLP ’02, pages 79–
</reference>
<page confidence="0.983163">
388
</page>
<reference confidence="0.99966075">
86, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hassan Saif, Yulan He, and Harith Alani. 2012. Seman-
tic sentiment analysis of twitter. In Proceedings of the
11th international conference on The Semantic Web -
Volume Part I, ISWC’12, pages 508–524, Berlin, Hei-
delberg. Springer-Verlag.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, pages 173–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M.
Welpe. 2010. Predicting elections with twitter: What
140 characters reveal about political sentiment. In
Proceedings of the Fourth International AAAI Confer-
ence on Weblogs and Social Media, pages 178–185.
Lawrence Wasden. 2010. Internet lingo dictionary: A
parents guide to codes used in chat rooms, instant mes-
saging, text messaging, and blogs. Technical report,
Attorney General.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
1(2):0.
</reference>
<page confidence="0.999146">
389
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903709">
<title confidence="0.999843">UT-DB: An Experimental Study on Sentiment Analysis in Twitter</title>
<author confidence="0.974095">Zhemin Zhu Djoerd Hiemstra Peter Apers Andreas</author>
<affiliation confidence="0.987847">CTIT Database Group, University of</affiliation>
<address confidence="0.976182">Drienerlolaan 5, 7500 AE, Enschede, The Netherlands</address>
<email confidence="0.986831">d.hiemstra,p.m.g.apers,</email>
<abstract confidence="0.9978434">This paper describes our system for participating SemEval2013 Task2-B (Kozareva et al., 2013): Sentiment Analysis in Twitter. Given a message, our system classifies whether the is sentiment. It uses a co-occurrence rate model. The training data are constrained to the data provided by the task organizers (No other tweet data are used). We consider 9 types of features and use a subset of them in our submitted system. To see the contribution of each type of features, we do experimental study on features by leaving one type of features out each time. Results suggest that unigrams are the most important features, bigrams and POS tags seem not helpful, and stopwords should be retained to achieve the best results. The overall results of our system are promising regarding the constrained features and data we use.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media, LSM ’11,</booktitle>
<pages>30--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2215" citStr="Agarwal et al., 2011" startWordPosition="347" endWordPosition="350">tly a wide range of methods and features have been applied to sentimental analysis over tweets. Go et al. (2009) train sentiment classifiers using machine learning methods, such as Naive Bayes, Maximum Entropy and SVMs, with different combinations of features such as unigrams, bigrams and Part-of-Speech (POS) tags. Microblogging features such as hashtags, emoticons, abbreviations, allcaps and character repetitions are also found helpful (Kouloumpis et al., 2011). Saif et al. (2012) train Naive Bayes models with semantic features. Also the lexicon prior polarities have been proved very useful (Agarwal et al., 2011). Davidov et al. (2010) utilize hashtags and smileys to build a largescale annotated tweet dataset automatically. This avoids the need for labour intensive manual annotation. Due to the fact that tweets are generated constantly, sentiment analysis over tweets has some interesting applications, such as predicting stock market movement (Bollen et al., 2011) and predicting election results (Tumasjan et al., 2010; O’Connor et al., 2010). But there are still some unclear parts in the literature. For example, it is unclear whether using POS tags improves the sentiment analysis performance or not. Co</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, LSM ’11, pages 30–38, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli Stefano Baccianella</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="7609" citStr="Baccianella and Sebastiani, 2010" startWordPosition="1247" endWordPosition="1250">lexicon polarity which ranges from −5 (most negative) to +5 (most positive). Our grading system consists of three categories: negative, neutral and positive. So we map the words ranging from −5 to −1 in SentiStrength to negative in our grading system, and the words ranging from 2http://www.ark.cs.cmu.edu/TweetNLP/ 3http://sentistrength.wlv.ac.uk/ P(fi, c) #(fi, c) CR(f, c) = P (f #(f i)P (c) oc i)#(c) , 385 +1 to +5 to positive. The rest are mapped to neutral. We do the same for the other two lexicon resources: OpinionFinder4 (Wiebe et al., 2005) and SentiWordNet5 (Esuli and Sebastiani, 2006; Baccianella and Sebastiani, 2010). 6. Intensifiers. The tweets containing intensifiers are more likely to be non-neutral. In the submitted system, we merge the boosters in SentiStrength and the intensifiers in OpinionFinder to form a list of intensifiers. Some of these intensifiers strengthen emotion (e.g. ‘definitely’), but others weaken emotion (e.g. ‘slightly’). They are distinguished and assigned with differentlabels {intensifier strengthen, intensifier weaken}. 7. All-caps and repeat characters. All-caps6 and repeat characters are common expressions in tweets to make emphasis on the applied tokens. They can be considered</context>
</contexts>
<marker>Baccianella, Sebastiani, 2010</marker>
<rawString>Andrea Esuli Stefano Baccianella and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bollen</author>
<author>H Mao</author>
<author>X Zeng</author>
</authors>
<title>Twitter mood predicts the stock market.</title>
<date>2011</date>
<journal>Journal of Computational Science.</journal>
<contexts>
<context position="2572" citStr="Bollen et al., 2011" startWordPosition="404" endWordPosition="407">emoticons, abbreviations, allcaps and character repetitions are also found helpful (Kouloumpis et al., 2011). Saif et al. (2012) train Naive Bayes models with semantic features. Also the lexicon prior polarities have been proved very useful (Agarwal et al., 2011). Davidov et al. (2010) utilize hashtags and smileys to build a largescale annotated tweet dataset automatically. This avoids the need for labour intensive manual annotation. Due to the fact that tweets are generated constantly, sentiment analysis over tweets has some interesting applications, such as predicting stock market movement (Bollen et al., 2011) and predicting election results (Tumasjan et al., 2010; O’Connor et al., 2010). But there are still some unclear parts in the literature. For example, it is unclear whether using POS tags improves the sentiment analysis performance or not. Conflicting results are reported (Pak and Paroubek, 2010; Go et al., 2009). It is also a little surprising that not removing stopwords increases performance (Saif et al., 2012). In this paper, we build a system based on the concept of cooccurrence rate. 9 different types of features are considered. We find that using a subset of these features achieves the </context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>J. Bollen, H. Mao, and X. Zeng. 2011. Twitter mood predicts the stock market. Journal of Computational Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2238" citStr="Davidov et al. (2010)" startWordPosition="351" endWordPosition="354">hods and features have been applied to sentimental analysis over tweets. Go et al. (2009) train sentiment classifiers using machine learning methods, such as Naive Bayes, Maximum Entropy and SVMs, with different combinations of features such as unigrams, bigrams and Part-of-Speech (POS) tags. Microblogging features such as hashtags, emoticons, abbreviations, allcaps and character repetitions are also found helpful (Kouloumpis et al., 2011). Saif et al. (2012) train Naive Bayes models with semantic features. Also the lexicon prior polarities have been proved very useful (Agarwal et al., 2011). Davidov et al. (2010) utilize hashtags and smileys to build a largescale annotated tweet dataset automatically. This avoids the need for labour intensive manual annotation. Due to the fact that tweets are generated constantly, sentiment analysis over tweets has some interesting applications, such as predicting stock market movement (Bollen et al., 2011) and predicting election results (Tumasjan et al., 2010; O’Connor et al., 2010). But there are still some unclear parts in the literature. For example, it is unclear whether using POS tags improves the sentiment analysis performance or not. Conflicting results are r</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 241–249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="7574" citStr="Esuli and Sebastiani, 2006" startWordPosition="1242" endWordPosition="1246">nularity system for grading lexicon polarity which ranges from −5 (most negative) to +5 (most positive). Our grading system consists of three categories: negative, neutral and positive. So we map the words ranging from −5 to −1 in SentiStrength to negative in our grading system, and the words ranging from 2http://www.ark.cs.cmu.edu/TweetNLP/ 3http://sentistrength.wlv.ac.uk/ P(fi, c) #(fi, c) CR(f, c) = P (f #(f i)P (c) oc i)#(c) , 385 +1 to +5 to positive. The rest are mapped to neutral. We do the same for the other two lexicon resources: OpinionFinder4 (Wiebe et al., 2005) and SentiWordNet5 (Esuli and Sebastiani, 2006; Baccianella and Sebastiani, 2010). 6. Intensifiers. The tweets containing intensifiers are more likely to be non-neutral. In the submitted system, we merge the boosters in SentiStrength and the intensifiers in OpinionFinder to form a list of intensifiers. Some of these intensifiers strengthen emotion (e.g. ‘definitely’), but others weaken emotion (e.g. ‘slightly’). They are distinguished and assigned with differentlabels {intensifier strengthen, intensifier weaken}. 7. All-caps and repeat characters. All-caps6 and repeat characters are common expressions in tweets to make emphasis on the app</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>Uima: an architectural approach to unstructured information processing in the corporate research environment.</title>
<date>2004</date>
<journal>Nat. Lang. Eng.,</journal>
<pages>10--3</pages>
<contexts>
<context position="10479" citStr="Ferrucci and Lally, 2004" startWordPosition="1689" endWordPosition="1692">g. 2.3.1 Replacing Abbreviations Abbreviations are replaced by their original expressions. We use the Internet Lingo Dictionary (Wasden, 2010) to obtain the original expressions of abbreviations. This dictionary originally contains 748 acronyms. But we do not use the acronyms in which all characters are digits. Because we find they are more likely to be numbers than acronyms. This results in 735 acronyms. 3 Experiments Our system is implemented in Java and organized as a pipeline consisting of a sequence of annotators and extractors. This architecture is very similar to the framework of UIMA (Ferrucci and Lally, 2004). With such an architecture, we can easily vary the configurations of our system. 3.1 Datasets We use the standard dataset provided by SemEval2013 Task2-B (Kozareva et al., 2013) for training and testing. The training and development data provided are merged together to train our model. Originally, the training and development data contain 9,684 and 1,654 instances, respectively. But due to the policy of Twitter, only the tweet IDs can be released publicly. So we need to fetch the actual tweets by their IDs. Some of the tweets are no longer existing after they were downloaded for annotation. S</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. Uima: an architectural approach to unstructured information processing in the corporate research environment. Nat. Lang. Eng., 10(3-4):327–348, September.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 42–47, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="1706" citStr="Go et al. (2009)" startWordPosition="271" endWordPosition="274">g the constrained features and data we use. 1 Introduction The past years have witnessed the emergence and popularity of short messages such as tweets and SMS messages. Comparing with the traditional genres such as newswire data, tweets are very short and use informal grammar and expressions. The shortness and informality make them a new genre and bring new challenges to sentiment analysis (Pang et al., 2002) as well as other NLP applications such named entity recognition (Habib et al., 2013). Recently a wide range of methods and features have been applied to sentimental analysis over tweets. Go et al. (2009) train sentiment classifiers using machine learning methods, such as Naive Bayes, Maximum Entropy and SVMs, with different combinations of features such as unigrams, bigrams and Part-of-Speech (POS) tags. Microblogging features such as hashtags, emoticons, abbreviations, allcaps and character repetitions are also found helpful (Kouloumpis et al., 2011). Saif et al. (2012) train Naive Bayes models with semantic features. Also the lexicon prior polarities have been proved very useful (Agarwal et al., 2011). Davidov et al. (2010) utilize hashtags and smileys to build a largescale annotated tweet </context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Habib</author>
<author>M van Keulen</author>
<author>Z Zhu</author>
</authors>
<title>Concept extraction challenge: University of twente at #msm2013.</title>
<date>2013</date>
<booktitle>In Proceedings of the 3rd workshop on ’Making Sense of Microposts’ (#MSM2013), Rio de Janeiro,</booktitle>
<publisher>CEUR.</publisher>
<location>Brazil, Brazil,</location>
<marker>Habib, van Keulen, Zhu, 2013</marker>
<rawString>M. B. Habib, M. van Keulen, and Z. Zhu. 2013. Concept extraction challenge: University of twente at #msm2013. In Proceedings of the 3rd workshop on ’Making Sense of Microposts’ (#MSM2013), Rio de Janeiro, Brazil, Brazil, May. CEUR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6417" citStr="Klein and Manning, 2003" startWordPosition="1056" endWordPosition="1060">ole is treated as a single feature. 4. Dependency relations. Dependency relations are helpful to the sentiment prediction. Here we give an example to explain this type of features. In the tweet “I may not be able to vote from Britain but I COMPLETLEY support you!!!!”, the dependency relation between the word ‘not’ and ‘able’ is ‘NEG’ which stands for negation, and the dependency relation between the word ‘COMPLETELY’ and ‘support’ is ‘ADVMOD’ which means adverb modifier. For this example, we add ‘NEG able’ and ‘completely support’ as dependency features to our system. We use Stanford CoreNLP (Klein and Manning, 2003a; Klein and Manning, 2003b) to obtain dependencies. And we only consider two types of dependencies ‘NEG’ and ‘ADVMOD’. Other dependency relations are not helpful. 5. Lexicon prior polarity. The prior polarity of lexicons have been proved very useful to sentiment analysis. Many lexicon resources have been developed. But for a single lexicon resource, the coverage is limited. To achieve better coverage, we merge three lexicon resources. The first one is SentiStrength3 (Kucuktunc et al., 2012). SentiStrength provides a fine-granularity system for grading lexicon polarity which ranges from −5 (mo</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003a. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 423– 430, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>15</volume>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6417" citStr="Klein and Manning, 2003" startWordPosition="1056" endWordPosition="1060">ole is treated as a single feature. 4. Dependency relations. Dependency relations are helpful to the sentiment prediction. Here we give an example to explain this type of features. In the tweet “I may not be able to vote from Britain but I COMPLETLEY support you!!!!”, the dependency relation between the word ‘not’ and ‘able’ is ‘NEG’ which stands for negation, and the dependency relation between the word ‘COMPLETELY’ and ‘support’ is ‘ADVMOD’ which means adverb modifier. For this example, we add ‘NEG able’ and ‘completely support’ as dependency features to our system. We use Stanford CoreNLP (Klein and Manning, 2003a; Klein and Manning, 2003b) to obtain dependencies. And we only consider two types of dependencies ‘NEG’ and ‘ADVMOD’. Other dependency relations are not helpful. 5. Lexicon prior polarity. The prior polarity of lexicons have been proved very useful to sentiment analysis. Many lexicon resources have been developed. But for a single lexicon resource, the coverage is limited. To achieve better coverage, we merge three lexicon resources. The first one is SentiStrength3 (Kucuktunc et al., 2012). SentiStrength provides a fine-granularity system for grading lexicon polarity which ranges from −5 (mo</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003b. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems, volume 15. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the omg!</title>
<date>2011</date>
<editor>In Lada A. Adamic, Ricardo A. Baeza-Yates, and Scott Counts, editors, ICWSM.</editor>
<publisher>The AAAI Press.</publisher>
<contexts>
<context position="2060" citStr="Kouloumpis et al., 2011" startWordPosition="322" endWordPosition="325">and bring new challenges to sentiment analysis (Pang et al., 2002) as well as other NLP applications such named entity recognition (Habib et al., 2013). Recently a wide range of methods and features have been applied to sentimental analysis over tweets. Go et al. (2009) train sentiment classifiers using machine learning methods, such as Naive Bayes, Maximum Entropy and SVMs, with different combinations of features such as unigrams, bigrams and Part-of-Speech (POS) tags. Microblogging features such as hashtags, emoticons, abbreviations, allcaps and character repetitions are also found helpful (Kouloumpis et al., 2011). Saif et al. (2012) train Naive Bayes models with semantic features. Also the lexicon prior polarities have been proved very useful (Agarwal et al., 2011). Davidov et al. (2010) utilize hashtags and smileys to build a largescale annotated tweet dataset automatically. This avoids the need for labour intensive manual annotation. Due to the fact that tweets are generated constantly, sentiment analysis over tweets has some interesting applications, such as predicting stock market movement (Bollen et al., 2011) and predicting election results (Tumasjan et al., 2010; O’Connor et al., 2010). But the</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the omg! In Lada A. Adamic, Ricardo A. Baeza-Yates, and Scott Counts, editors, ICWSM. The AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyonov</author>
<author>Theresa Wilson</author>
</authors>
<title>Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computation Linguistics.</booktitle>
<contexts>
<context position="10657" citStr="Kozareva et al., 2013" startWordPosition="1718" endWordPosition="1721">abbreviations. This dictionary originally contains 748 acronyms. But we do not use the acronyms in which all characters are digits. Because we find they are more likely to be numbers than acronyms. This results in 735 acronyms. 3 Experiments Our system is implemented in Java and organized as a pipeline consisting of a sequence of annotators and extractors. This architecture is very similar to the framework of UIMA (Ferrucci and Lally, 2004). With such an architecture, we can easily vary the configurations of our system. 3.1 Datasets We use the standard dataset provided by SemEval2013 Task2-B (Kozareva et al., 2013) for training and testing. The training and development data provided are merged together to train our model. Originally, the training and development data contain 9,684 and 1,654 instances, respectively. But due to the policy of Twitter, only the tweet IDs can be released publicly. So we need to fetch the actual tweets by their IDs. Some of the tweets are no longer existing after they were downloaded for annotation. So the number of tweets used for training is less than the original tweets provided by the organizers. In our case, we obtained 10,370 tweets for training our model. 386 Class Pre</context>
</contexts>
<marker>Kozareva, Nakov, Ritter, Rosenthal, Stoyonov, Wilson, 2013</marker>
<rawString>Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara Rosenthal, Veselin Stoyonov, and Theresa Wilson. 2013. Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computation Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Onur Kucuktunc</author>
<author>B Barla Cambazoglu</author>
<author>Ingmar Weber</author>
<author>Hakan Ferhatosmanoglu</author>
</authors>
<title>A large-scale sentiment analysis for yahoo! answers.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12,</booktitle>
<pages>633--642</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6913" citStr="Kucuktunc et al., 2012" startWordPosition="1136" endWordPosition="1140"> add ‘NEG able’ and ‘completely support’ as dependency features to our system. We use Stanford CoreNLP (Klein and Manning, 2003a; Klein and Manning, 2003b) to obtain dependencies. And we only consider two types of dependencies ‘NEG’ and ‘ADVMOD’. Other dependency relations are not helpful. 5. Lexicon prior polarity. The prior polarity of lexicons have been proved very useful to sentiment analysis. Many lexicon resources have been developed. But for a single lexicon resource, the coverage is limited. To achieve better coverage, we merge three lexicon resources. The first one is SentiStrength3 (Kucuktunc et al., 2012). SentiStrength provides a fine-granularity system for grading lexicon polarity which ranges from −5 (most negative) to +5 (most positive). Our grading system consists of three categories: negative, neutral and positive. So we map the words ranging from −5 to −1 in SentiStrength to negative in our grading system, and the words ranging from 2http://www.ark.cs.cmu.edu/TweetNLP/ 3http://sentistrength.wlv.ac.uk/ P(fi, c) #(fi, c) CR(f, c) = P (f #(f i)P (c) oc i)#(c) , 385 +1 to +5 to positive. The rest are mapped to neutral. We do the same for the other two lexicon resources: OpinionFinder4 (Wieb</context>
</contexts>
<marker>Kucuktunc, Cambazoglu, Weber, Ferhatosmanoglu, 2012</marker>
<rawString>Onur Kucuktunc, B. Barla Cambazoglu, Ingmar Weber, and Hakan Ferhatosmanoglu. 2012. A large-scale sentiment analysis for yahoo! answers. In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12, pages 633–642, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth International Conference on Weblogs and Social Media, ICWSM 2010,</booktitle>
<location>Washington, DC, USA,</location>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of the Fourth International Conference on Weblogs and Social Media, ICWSM 2010, Washington, DC, USA, May 23-26, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan OConnor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Hello, who is calling?: Can words reveal the social nature of conversations?</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association</booktitle>
<contexts>
<context position="5655" citStr="Owoputi et al., 2013" startWordPosition="928" endWordPosition="931">od can be trained efficiently because it only depends on the empirical distributions. 2.2 Features To make our system general, we constrain to the text features. That is we do not use the features outside the tweet texts such as features related to the user profiles, discourse information or links. The following 9 types of features are considered: 1. Unigrams. We use lemmas as the form of unigrams. The lemmas are obtained by the Stanford CoreNLP1 (Toutanova et al., 2003). Hash1http://nlp.stanford.edu/software/corenlp.shtml 3. Named entities. We use the CMU Twitter Tagger (Gimpel et al., 2011; Owoputi et al., 2013)2 to recognize named entities. The tokens covered by a named entity are not considered as unigrams any more. Instead a named entity as a whole is treated as a single feature. 4. Dependency relations. Dependency relations are helpful to the sentiment prediction. Here we give an example to explain this type of features. In the tweet “I may not be able to vote from Britain but I COMPLETLEY support you!!!!”, the dependency relation between the word ‘not’ and ‘able’ is ‘NEG’ which stands for negation, and the dependency relation between the word ‘COMPLETELY’ and ‘support’ is ‘ADVMOD’ which means ad</context>
</contexts>
<marker>Owoputi, OConnor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Hello, who is calling?: Can words reveal the social nature of conversations? In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="2869" citStr="Pak and Paroubek, 2010" startWordPosition="452" endWordPosition="455">hashtags and smileys to build a largescale annotated tweet dataset automatically. This avoids the need for labour intensive manual annotation. Due to the fact that tweets are generated constantly, sentiment analysis over tweets has some interesting applications, such as predicting stock market movement (Bollen et al., 2011) and predicting election results (Tumasjan et al., 2010; O’Connor et al., 2010). But there are still some unclear parts in the literature. For example, it is unclear whether using POS tags improves the sentiment analysis performance or not. Conflicting results are reported (Pak and Paroubek, 2010; Go et al., 2009). It is also a little surprising that not removing stopwords increases performance (Saif et al., 2012). In this paper, we build a system based on the concept of cooccurrence rate. 9 different types of features are considered. We find that using a subset of these features achieves the best results in our system, so we use this subset of features rather than all the 9 types of features in our submitted system. To see the contribution of each type of features, we perform experiments by leaving one type of features out each time. Results show that unigrams are the most important </context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1502" citStr="Pang et al., 2002" startWordPosition="237" endWordPosition="240">t that unigrams are the most important features, bigrams and POS tags seem not helpful, and stopwords should be retained to achieve the best results. The overall results of our system are promising regarding the constrained features and data we use. 1 Introduction The past years have witnessed the emergence and popularity of short messages such as tweets and SMS messages. Comparing with the traditional genres such as newswire data, tweets are very short and use informal grammar and expressions. The shortness and informality make them a new genre and bring new challenges to sentiment analysis (Pang et al., 2002) as well as other NLP applications such named entity recognition (Habib et al., 2013). Recently a wide range of methods and features have been applied to sentimental analysis over tweets. Go et al. (2009) train sentiment classifiers using machine learning methods, such as Naive Bayes, Maximum Entropy and SVMs, with different combinations of features such as unigrams, bigrams and Part-of-Speech (POS) tags. Microblogging features such as hashtags, emoticons, abbreviations, allcaps and character repetitions are also found helpful (Kouloumpis et al., 2011). Saif et al. (2012) train Naive Bayes mod</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02, pages 79– 86, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Saif</author>
<author>Yulan He</author>
<author>Harith Alani</author>
</authors>
<title>Semantic sentiment analysis of twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th international conference on The Semantic Web -Volume Part I, ISWC’12,</booktitle>
<pages>508--524</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2080" citStr="Saif et al. (2012)" startWordPosition="326" endWordPosition="329">o sentiment analysis (Pang et al., 2002) as well as other NLP applications such named entity recognition (Habib et al., 2013). Recently a wide range of methods and features have been applied to sentimental analysis over tweets. Go et al. (2009) train sentiment classifiers using machine learning methods, such as Naive Bayes, Maximum Entropy and SVMs, with different combinations of features such as unigrams, bigrams and Part-of-Speech (POS) tags. Microblogging features such as hashtags, emoticons, abbreviations, allcaps and character repetitions are also found helpful (Kouloumpis et al., 2011). Saif et al. (2012) train Naive Bayes models with semantic features. Also the lexicon prior polarities have been proved very useful (Agarwal et al., 2011). Davidov et al. (2010) utilize hashtags and smileys to build a largescale annotated tweet dataset automatically. This avoids the need for labour intensive manual annotation. Due to the fact that tweets are generated constantly, sentiment analysis over tweets has some interesting applications, such as predicting stock market movement (Bollen et al., 2011) and predicting election results (Tumasjan et al., 2010; O’Connor et al., 2010). But there are still some un</context>
<context position="13755" citStr="Saif et al. (2012)" startWordPosition="2245" endWordPosition="2248">.47 59.87 51.94 52.64 Unigram 59.87 41.22 52.64 35.09 3 &lt; 59.87 57.66 52.64 51.20 Intensifier 59.87 59.47 52.64 52.39 Lexicon 59.87 58.33 52.64 51.26 Named Ent. 59.87 59.71 52.64 51.80 Interrogative 59.87 59.67 52.64 52.93 Imperative 59.87 59.54 52.64 52.14 Dependence 59.87 59.37 52.64 52.08 Table 3: Avg (Pos &amp; Neg) of Leave-one-out Experiments frequency features which happens less than 3 times increases the F-scores on Twitter data from 57.66 to 59.87, and on SMS data from 51.20 to 52.64. Removing stopwords decreases the scores by 1.66 percent. This result is consistent with that reported by Saif et al. (2012). By taking a close look at the stopwords we use, we find that some of the stopwords are highly related to the sentiment polarity, such as ‘can’, ‘no’, ‘very’ and ‘want’, but others are not, such as ‘the’, ‘him’ and ‘on’. Removing the stopwords which are related to the sentiment is obviously harmful. This means the stopwords which originally developed for the purpose of information retrieval are not suitable for sentimental analysis. Dependency relations are also helpful features which increase F-scores by about 0.5 percent. The POS tags and bigrams seem not helpful in our experiments, which i</context>
</contexts>
<marker>Saif, He, Alani, 2012</marker>
<rawString>Hassan Saif, Yulan He, and Harith Alani. 2012. Semantic sentiment analysis of twitter. In Proceedings of the 11th international conference on The Semantic Web -Volume Part I, ISWC’12, pages 508–524, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5509" citStr="Toutanova et al., 2003" startWordPosition="908" endWordPosition="911">fi) because they are observations. Hence comparing with Naive Bayes, our model saves the effort to model feature distributions P(fi). Also this method can be trained efficiently because it only depends on the empirical distributions. 2.2 Features To make our system general, we constrain to the text features. That is we do not use the features outside the tweet texts such as features related to the user profiles, discourse information or links. The following 9 types of features are considered: 1. Unigrams. We use lemmas as the form of unigrams. The lemmas are obtained by the Stanford CoreNLP1 (Toutanova et al., 2003). Hash1http://nlp.stanford.edu/software/corenlp.shtml 3. Named entities. We use the CMU Twitter Tagger (Gimpel et al., 2011; Owoputi et al., 2013)2 to recognize named entities. The tokens covered by a named entity are not considered as unigrams any more. Instead a named entity as a whole is treated as a single feature. 4. Dependency relations. Dependency relations are helpful to the sentiment prediction. Here we give an example to explain this type of features. In the tweet “I may not be able to vote from Britain but I COMPLETLEY support you!!!!”, the dependency relation between the word ‘not’</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tumasjan</author>
<author>T O Sprenger</author>
<author>P G Sandner</author>
<author>I M Welpe</author>
</authors>
<title>Predicting elections with twitter: What 140 characters reveal about political sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="2627" citStr="Tumasjan et al., 2010" startWordPosition="412" endWordPosition="415">tions are also found helpful (Kouloumpis et al., 2011). Saif et al. (2012) train Naive Bayes models with semantic features. Also the lexicon prior polarities have been proved very useful (Agarwal et al., 2011). Davidov et al. (2010) utilize hashtags and smileys to build a largescale annotated tweet dataset automatically. This avoids the need for labour intensive manual annotation. Due to the fact that tweets are generated constantly, sentiment analysis over tweets has some interesting applications, such as predicting stock market movement (Bollen et al., 2011) and predicting election results (Tumasjan et al., 2010; O’Connor et al., 2010). But there are still some unclear parts in the literature. For example, it is unclear whether using POS tags improves the sentiment analysis performance or not. Conflicting results are reported (Pak and Paroubek, 2010; Go et al., 2009). It is also a little surprising that not removing stopwords increases performance (Saif et al., 2012). In this paper, we build a system based on the concept of cooccurrence rate. 9 different types of features are considered. We find that using a subset of these features achieves the best results in our system, so we use this subset of fe</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M. Welpe. 2010. Predicting elections with twitter: What 140 characters reveal about political sentiment. In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Wasden</author>
</authors>
<title>Internet lingo dictionary: A parents guide to codes used in chat rooms, instant messaging, text messaging, and blogs.</title>
<date>2010</date>
<tech>Technical report, Attorney General.</tech>
<contexts>
<context position="9996" citStr="Wasden, 2010" startWordPosition="1613" endWordPosition="1614">f sentiment. Our experiments show that removing these low-frequency features increases the accuracy. 2.3 Pre-processing The pre-processing of our system includes two steps. In the first step, we replace the abbreviations as described in Section 2.3.1. In the second step, we use the CMU Twitter Tagger to extract the features of emoticons (e.g. :)), hashtags (e.g. #Friday), reciepts (e.g. @Peter) and URLs, and remove these symbols from tweet texts for further processing. 2.3.1 Replacing Abbreviations Abbreviations are replaced by their original expressions. We use the Internet Lingo Dictionary (Wasden, 2010) to obtain the original expressions of abbreviations. This dictionary originally contains 748 acronyms. But we do not use the acronyms in which all characters are digits. Because we find they are more likely to be numbers than acronyms. This results in 735 acronyms. 3 Experiments Our system is implemented in Java and organized as a pipeline consisting of a sequence of annotators and extractors. This architecture is very similar to the framework of UIMA (Ferrucci and Lally, 2004). With such an architecture, we can easily vary the configurations of our system. 3.1 Datasets We use the standard da</context>
</contexts>
<marker>Wasden, 2010</marker>
<rawString>Lawrence Wasden. 2010. Internet lingo dictionary: A parents guide to codes used in chat rooms, instant messaging, text messaging, and blogs. Technical report, Attorney General.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="7528" citStr="Wiebe et al., 2005" startWordPosition="1236" endWordPosition="1239">012). SentiStrength provides a fine-granularity system for grading lexicon polarity which ranges from −5 (most negative) to +5 (most positive). Our grading system consists of three categories: negative, neutral and positive. So we map the words ranging from −5 to −1 in SentiStrength to negative in our grading system, and the words ranging from 2http://www.ark.cs.cmu.edu/TweetNLP/ 3http://sentistrength.wlv.ac.uk/ P(fi, c) #(fi, c) CR(f, c) = P (f #(f i)P (c) oc i)#(c) , 385 +1 to +5 to positive. The rest are mapped to neutral. We do the same for the other two lexicon resources: OpinionFinder4 (Wiebe et al., 2005) and SentiWordNet5 (Esuli and Sebastiani, 2006; Baccianella and Sebastiani, 2010). 6. Intensifiers. The tweets containing intensifiers are more likely to be non-neutral. In the submitted system, we merge the boosters in SentiStrength and the intensifiers in OpinionFinder to form a list of intensifiers. Some of these intensifiers strengthen emotion (e.g. ‘definitely’), but others weaken emotion (e.g. ‘slightly’). They are distinguished and assigned with differentlabels {intensifier strengthen, intensifier weaken}. 7. All-caps and repeat characters. All-caps6 and repeat characters are common exp</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 1(2):0.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>