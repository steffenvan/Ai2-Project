<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9987455">
Learning Word Representations by Jointly Modeling
Syntagmatic and Paradigmatic Relations
</title>
<author confidence="0.983563">
Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, and Xueqi Cheng
</author>
<affiliation confidence="0.977042">
CAS Key Lab of Network Data Science and Technology
Institute of Computing Technology
Chinese Academy of Sciences, China
</affiliation>
<email confidence="0.9253785">
ofey.sunfei@gmail.com
{guojiafeng,lanyanyan,junxu,cxq}@ict.ac.cn
</email>
<sectionHeader confidence="0.994717" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946192307692">
Vector space representation of words has
been widely used to capture fine-grained
linguistic regularities, and proven to be
successful in various natural language pro-
cessing tasks in recent years. However,
existing models for learning word repre-
sentations focus on either syntagmatic or
paradigmatic relations alone. In this pa-
per, we argue that it is beneficial to jointly
modeling both relations so that we can not
only encode different types of linguistic
properties in a unified way, but also boost
the representation learning due to the mu-
tual enhancement between these two types
of relations. We propose two novel dis-
tributional models for word representation
using both syntagmatic and paradigmatic
relations via a joint training objective. The
proposed models are trained on a public
Wikipedia corpus, and the learned rep-
resentations are evaluated on word anal-
ogy and word similarity tasks. The re-
sults demonstrate that the proposed mod-
els can perform significantly better than
all the state-of-the-art baseline methods on
both tasks.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941916666667">
Vector space models of language represent each
word with a real-valued vector that captures both
semantic and syntactic information of the word.
The representations can be used as basic features
in a variety of applications, such as information re-
trieval (Manning et al., 2008), named entity recog-
nition (Collobert et al., 2011), question answer-
ing (Tellex et al., 2003), disambiguation (Sch¨utze,
1998), and parsing (Socher et al., 2011).
A common paradigm for acquiring such repre-
sentations is based on the distributional hypothe-
sis (Harris, 1954; Firth, 1957), which states that
</bodyText>
<equation confidence="0.5800414">
syntagmatic
The wolf is a fierce animal.
paradigmatic
The tiger is a fierce animal.
syntagmatic
</equation>
<figureCaption confidence="0.993797">
Figure 1: Example for syntagmatic and paradig-
matic relations.
</figureCaption>
<bodyText confidence="0.997715310344828">
words occurring in similar contexts tend to have
similar meanings. Based on this hypothesis, vari-
ous models on learning word representations have
been proposed during the last two decades.
According to the leveraged distributional infor-
mation, existing models can be grouped into two
categories (Sahlgren, 2008). The first category
mainly concerns the syntagmatic relations among
the words, which relate the words that co-occur
in the same text region. For example, “wolf” is
close to “fierce” since they often co-occur in a sen-
tence, as shown in Figure 1. This type of models
learn the distributional representations of words
based on the text region that the words occur in, as
exemplified by Latent Semantic Analysis (LSA)
model (Deerwester et al., 1990) and Non-negative
Matrix Factorization (NMF) model (Lee and Se-
ung, 1999). The second category mainly cap-
tures paradigmatic relations, which relate words
that occur with similar contexts but may not co-
occur in the text. For example, “wolf” is close
to “tiger” since they often have similar context
words. This type of models learn the word rep-
resentations based on the surrounding words, as
exemplified by the Hyperspace Analogue to Lan-
guage (HAL) model (Lund et al., 1995), Con-
tinuous Bag-of-Words (CBOW) model and Skip-
Gram (SG) model (Mikolov et al., 2013a).
In this work, we argue that it is important to
</bodyText>
<page confidence="0.971326">
136
</page>
<note confidence="0.978188">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 136–145,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999624885714285">
take both syntagmatic and paradigmatic relations
into account to build a good distributional model.
Firstly, in distributional meaning acquisition, it
is expected that a good representation should be
able to encode a bunch of linguistic properties.
For example, it can put semantically related words
close (e.g., “microsoft” and “office”), and also be
able to capture syntactic regularities like “big is
to bigger as deep is to deeper”. Obviously, these
linguistic properties are related to both syntag-
matic and paradigmatic relations, and cannot be
well modeled by either alone. Secondly, syntag-
matic and paradigmatic relations are complimen-
tary rather than conflicted in representation learn-
ing. That is relating the words that co-occur within
the same text region (e.g., “wolf” and “fierce” as
well as “tiger” and “fierce”) can better relate words
that occur with similar contexts (e.g., “wolf” and
“tiger”), and vice versa.
Based on the above analysis, we propose two
new distributional models for word representa-
tion using both syntagmatic and paradigmatic re-
lations. Specifically, we learn the distributional
representations of words based on the text region
(i.e., the document) that the words occur in as well
as the surrounding words (i.e., word sequences
within some window size). By combining these
two types of relations either in a parallel or a hier-
archical way, we obtain two different joint training
objectives for word representation learning. We
evaluate our new models in two tasks, i.e., word
analogy and word similarity. The experimental
results demonstrate that the proposed models can
perform significantly better than all of the state-of-
the-art baseline methods in both of the tasks.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999686">
The distributional hypothesis has provided the
foundation for a class of statistical methods
for word representation learning. According to
the leveraged distributional information, existing
models can be grouped into two categories, i.e.,
syntagmatic models and paradigmatic models.
Syntagmatic models concern combinatorial re-
lations between words (i.e., syntagmatic rela-
tions), which relate words that co-occur within the
same text region (e.g., sentence, paragraph or doc-
ument).
For example, sentences have been used as the
text region to acquire co-occurrence information
by (Rubenstein and Goodenough, 1965; Miller
and Charles, 1991). However, as pointed our by
Picard (1999), the smaller the context regions are
that we use to collect syntagmatic information,
the worse the sparse-data problem will be for the
resulting representation. Therefore, syntagmatic
models tend to favor the use of larger text regions
as context. Specifically, a document is often taken
as a natural context of a word following the liter-
ature of information retrieval. In these methods, a
words-by-documents co-occurrence matrix is built
to collect the distributional information, where the
entry indicates the (normalized) frequency of a
word in a document. A low-rank decomposition
is then conducted to learn the distributional word
representations. For example, LSA (Deerwester et
al., 1990) employs singular value decomposition
by assuming the decomposed matrices to be or-
thogonal. In (Lee and Seung, 1999), non-negative
matrix factorization is conducted over the words-
by-documents matrix to learn the word represen-
tations.
Paradigmatic models concern substitutional
relations between words (i.e., paradigmatic rela-
tions), which relate words that occur in the same
context but may not at the same time. Unlike
syntagmatic model, paradigmatic models typically
collect distributional information in a words-by-
words co-occurrence matrix, where entries indi-
cate how many times words occur together within
a context window of some size.
For example, the Hyperspace Analogue to Lan-
guage (HAL) model (Lund et al., 1995) con-
structed a high-dimensional vector for words
based on the word co-occurrence matrix from a
large corpus of text. However, a major problem
with HAL is that the similarity measure will be
dominated by the most frequent words due to its
weight scheme. Various methods have been pro-
posed to address the drawback of HAL. For exam-
ple, the Correlated Occurrence Analogue to Lexi-
cal Semantic (COALS) (Rohde et al., 2006) trans-
formed the co-occurrence matrix by an entropy or
correlation based normalization. Bullinaria and
Levy (2007), and Levy and Goldberg (2014b) sug-
gested that positive pointwise mutual information
(PPMI) is a good transformation. More recently,
Lebret and Collobert (2014) obtained the word
representations through a Hellinger PCA (HPCA)
of the words-by-words co-occurrence matrix. Pen-
nington et al. (2014) explicitly factorizes the
words-by-words co-occurrence matrix to obtain
</bodyText>
<page confidence="0.996644">
137
</page>
<bodyText confidence="0.999522947368421">
the Global Vectors (GloVe) for word representa-
tion.
Alternatively, neural probabilistic language
models (NPLMs) (Bengio et al., 2003) learn word
representations by predicting the next word given
previously seen words. Unfortunately, the training
of NPLMs is quite time consuming, since com-
puting probabilities in such model requires nor-
malizing over the entire vocabulary. Recently,
Mnih and Teh (2012) applied Noise Contrastive
Estimation (NCE) to approximately maximize the
probability of the softmax in NPLM. Mikolov
et al. (2013a) further proposed continuous bag-
of-words (CBOW) and skip-gram (SG) models,
which use a simple single-layer architecture based
on inner product between two word vectors. Both
models can be learned efficiently via a simple vari-
ant of Noise Contrastive Estimation, i.e., Negative
sampling (NS) (Mikolov et al., 2013b).
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="method">
3 Our Models
</sectionHeader>
<bodyText confidence="0.9999475">
In this paper, we argue that it is important to jointly
model both syntagmatic and paradigmatic rela-
tions to learn good word representations. In this
way, we not only encode different types of linguis-
tic properties in a unified way, but also boost the
representation learning due to the mutual enhance-
ment between these two types of relations.
We propose two joint models that learn the dis-
tributional representations of words based on both
the text region that the words occur in (i.e., syntag-
matic relations) and the surrounding words (i.e.,
paradigmatic relations). To model syntagmatic re-
lations, we follow the previous work (Deerwester
et al., 1990; Lee and Seung, 1999) to take docu-
ment as a nature text region of a word. To model
paradigmatic relations, we are inspired by the re-
cent work from Mikolov et al. (Mikolov et al.,
2013a; Mikolov et al., 2013b), where simple mod-
els over word sequences are introduced for effi-
cient and effective word representation learning.
In the following, we introduce the notations
used in this paper, followed by detailed model de-
scriptions, ending with some discussions of the
proposed models.
</bodyText>
<subsectionHeader confidence="0.988509">
3.1 Notation
</subsectionHeader>
<bodyText confidence="0.998913">
Before presenting our models, we first list the no-
tations used in this paper. Let D={d1, ... , dN}
denote a corpus of N documents over the
word vocabulary W. The contexts for word
</bodyText>
<equation confidence="0.8803955">
cni_1
cni+1
cni+2
dn
</equation>
<figureCaption confidence="0.5413148">
Figure 2: The framework for PDC model. Four
words (“the”, “cat”, “on” and “the”) are used to
predict the center word (“sat”). Besides, the doc-
ument in which the word sequence occurs is also
used to predict the center word (“sat”).
</figureCaption>
<bodyText confidence="0.998648555555556">
wni ∈W (i.e. i-th word in document dn) are
the words surrounding it in an L-sized window
(cni_L, . . . , cni_1, cni+1, . . . , cni+L) ∈ H, where cjn ∈
W, j∈{i−L, . . . , i−�, i��, . . . , i�L}.Each doc-
ument d ∈ D, each word w ∈ W and each con-
text c ∈ W is associated with a vector d⃗∈ RK,
w⃗ ∈ RK and c⃗ ∈ RK, respectively, where K is
the embedding dimensionality. The entries in the
vectors are treated as parameters to be learned.
</bodyText>
<subsectionHeader confidence="0.998509">
3.2 Parallel Document Context Model
</subsectionHeader>
<bodyText confidence="0.998386235294118">
The first proposed model architecture is shown in
Figure 2. In this model, a target word is predicted
by its surrounding context, as well as the docu-
ment it occurs in. The former prediction task cap-
tures the paradigmatic relations, since words with
similar context will tend to have similar represen-
tations. While the latter prediction task models the
syntagmatic relations, since words co-occur in the
same document will tend to have similar represen-
tations. More detailed analysis on this will be pre-
sented in Section 3.4. The model can be viewed
as an extension of CBOW model (Mikolov et
al., 2013a), by adding an extra document branch.
Since both the context and document are parallel
in predicting the target word, we call this model
the Parallel Document Context (PDC) model.
More formally, the objective function of PDC
</bodyText>
<figure confidence="0.941118">
...
cni_2
the
cat
on
the
...
. . .
the cat
sat
on the
. . .
Projection
sat wni
138
model is the log likelihood of all words cni−2 cn cn cn
i−1 i+1 i+2
(log p(wni |hni )+log p(wni |dn))
</figure>
<bodyText confidence="0.863019">
where hni denotes the projection of wni ’s contexts,
defined as
</bodyText>
<figure confidence="0.829772545454545">
N
n=1
ℓ =
�
wni Edn
· · · the
Projection
cat on
· · ·
the
sat
</figure>
<bodyText confidence="0.978355333333333">
hin =f(cni−L, ... , cni−1, cni+1, ... , cni+L)
where f(·) can be sum, average, concatenate or
max pooling of context vectors1. In this paper, we
use average, as that of word2vec tool.
We use softmax function to define the probabil-
ities p(wni |hni ) and p(wni |dn) as follows:
</bodyText>
<equation confidence="0.9979426">
p  |) =
exp ( ⃗wni· hZ) (1)
EwEW exp( 2.
⃗hni )
exp(⃗wni · ⃗dn)
</equation>
<bodyText confidence="0.99998925">
where ⃗hni denotes projected vector of wni ’s con-
texts.
To learn the model, we adopt the negative sam-
pling technique (Mikolov et al., 2013b) for effi-
cient learning since the original objective is in-
tractable for direct optimization. The negative
sampling actually defines an alternate training ob-
jective function as follows
</bodyText>
<equation confidence="0.967989">
(log σ( ⃗wni · ⃗hni )+ log σ( ⃗wn i · ⃗dn)
+ k · Ew′_P., log σ( ⃗w′ · ⃗hni ) (3)
+ k · Ew′_P., log σ(⃗w′ · ⃗dn))
</equation>
<bodyText confidence="0.998869666666667">
where σ(x) = 1/(1 + exp(−x)), k is the num-
ber of “negative” samples, w′ denotes the sampled
word, and Pnw denotes the distribution of negative
word samples. We use stochastic gradient descent
(SGD) for optimization, and the gradient is calcu-
lated via back-propagation algorithm.
</bodyText>
<subsectionHeader confidence="0.998229">
3.3 Hierarchical Document Context Model
</subsectionHeader>
<bodyText confidence="0.925031625">
Since the above PDC model can be viewed as an
extension of CBOW model, it is natural to in-
troduce the same document-word prediction layer
into the SG model. This becomes our second
1Note that the context window size L can be a function of
the target word wni . In this paper, we use the same strategy
as word2vec tools which uniformly samples from the set
11, 2, · · · , L}.
</bodyText>
<figure confidence="0.492356857142857">
wni
Projection
. . .
the cat
sat
on the
. . .
</figure>
<figureCaption confidence="0.779359">
Figure 3: The framework for HDC model. The
document is used to predict the target word (“sat”).
</figureCaption>
<bodyText confidence="0.995236333333333">
Then, the word (“sat”) is used to predict the sur-
rounding words (“the”, “cat”, “on” and “the”).
model architecture as shown in Figure 3. Specif-
ically, the document is used to predict a target
word, and the target word is further used to pre-
dict its surrounding context words. Since the pre-
diction is conducted in a hierarchical manner, we
name this model the Hierarchical Document Con-
text (HDC) model. Similar as the PDC model,
the syntagmatic relation in HDC is modeled by
the document-word prediction layer and the word-
context prediction layer models the paradigmatic
relation.
Formally, the objective function of HDC model
is the log likelihood of all words:
</bodyText>
<equation confidence="0.9742676">
�
� �i+L
log p(cjn |wni )+ log p(wni|dn)
wni Edn j=i−L
j�i
</equation>
<bodyText confidence="0.999913">
where p(wni |dn) is defined the same as in Equa-
tion (2), and p(cjn |wni ) is also defined by a softmax
function as follows:
</bodyText>
<equation confidence="0.4821275">
exp(⃗cn j · ⃗wni )
EcEW exp(⃗c · ⃗wni )
</equation>
<bodyText confidence="0.996325">
Similarly, we adopt the negative sampling tech-
nique for learning, which defines the following
</bodyText>
<equation confidence="0.9080094375">
p(wn i |dn) =
EwEW exp(⃗w ·
(2)
⃗dn)
ℓ= N �
n=1 wni Edn
dn
N
ℓ=
n=1
p(cnj |wni ) =
139
training objective function
(log σ(⃗cn j · ⃗wni )
+ k · Ec′—Pnc log σ(⃗c′ · �⃗wn i )
�+ log σ( ⃗wn i · ⃗dn) + k·Ew′—Pnwlog σ( ⃗w′· ⃗dn)
</equation>
<bodyText confidence="0.999887142857143">
where k is the number of the negative samples, c′
and w′ denotes the sampled context and word re-
spectively, and Pnc and Pnw denotes the distribu-
tion of negative context and word samples respec-
tively2. We also employ SGD for optimization,
and calculate the gradient via back-propagation al-
gorithm.
</bodyText>
<subsectionHeader confidence="0.759708">
3.4 Discussions
</subsectionHeader>
<bodyText confidence="0.999969227272727">
In this section we first show how PDC and HDC
models capture the syntagmatic and paradigmatic
relations from the viewpoint of matrix factoriza-
tion. We then talk about the relationship of our
models with previous work.
As pointed out in (Sahlgren, 2008), to capture
syntagmatic relations, the implementational basis
is to collect text data in a words-by-documents co-
occurrence matrix in which the entry indicates the
(normalized) frequency of occurrence of a word
in a document (or, some other type of text region,
e.g., a sentence). While the implementational ba-
sis for paradigmatic relations is to collect text data
in a words-by-words co-occurrence matrix that is
populated by counting how many times words oc-
cur together within the context window. We now
take the proposed PDC model as an example to
show how it achieves these goals, and similar re-
sults can be shown for HDC model.
The objective function of PDC with negative
sampling in Equation (3) can be decomposed into
the following two parts:
</bodyText>
<equation confidence="0.9456418">
(#(w, h)· log σ(⃗w · ⃗h)
H (4)
+k·#(h)·pnw(w)log σ(−⃗w· ⃗h))
(5)
+k·|d|·pnw(w)log σ(−⃗w· ⃗d))
</equation>
<bodyText confidence="0.9977335">
where #(·, ·) denotes the number of times the pair
(·, ·) appears in D, #(h)= EwEW #(w, h), |d|
</bodyText>
<footnote confidence="0.804955">
2Pnc is not necessary to be the same as Pnw.
</footnote>
<bodyText confidence="0.999067142857143">
denotes the length of document d, the objective
function ℓ1 corresponds to the context-word pre-
diction task and ℓ2 corresponds to the document-
word prediction task.
Following the idea introduced by (Levy and
Goldberg, 2014a), it is easy to show that the so-
lution of the objective function ℓ1 follows that
</bodyText>
<equation confidence="0.9943175">
w⃗ · h⃗ = log( #(w, h) ) − log k
#(h) · pnw(w)
</equation>
<bodyText confidence="0.9877455">
and the solution of the objective function ℓ2 fol-
lows that
</bodyText>
<equation confidence="0.8445395">
w⃗ · d⃗ = log( #(w, d) ) − log k
|d|· pnw(w)
</equation>
<bodyText confidence="0.999722944444444">
It reveals that the PDC model with negative sam-
pling is actually factorizing both a words-by-
contexts co-occurrence matrix and a words-by-
documents co-occurrence matrix simultaneously.
In this way, we can see that the implementational
basis of the PDC model is consistent with that of
syntagmatic and paradigmatic models. In other
words, PDC can indeed capture both syntagmatic
and paradigmatic relations by processing the right
distributional information. Please notice that the
PDC model is not equivalent to direct combina-
tion of existing matrix factorization methods, due
to the fact that the matrix entries defined in PDC
model are more complicated than the simple co-
occurrence frequency (Lee and Seung, 1999).
When considering existing models, one may
connect our models to the Distributed Memory
model of Paragraph Vectors (PV-DM) and the Dis-
tributed Bag of Words version of Paragraph Vec-
tors (PV-DBOW) (Le and Mikolov, 2014). How-
ever, both of them are quite different from our
models. In PV-DM, the paragraph vector and con-
text vectors are averaged or concatenated to pre-
dict the next word. Therefore, the objective func-
tion of PV-DM can no longer decomposed as the
PDC model as shown in Equation (4) and (5).
In other words, although PV-DM leverages both
paragraph and context information, it is unclear
how these information is collected and used in
this model. As for PV-DBOW, it simply lever-
ages paragraph vector to predict words in the para-
graph. It is easy to show that it only uses the
words-by-documents co-occurrence matrix, and
thus only captures syntagmatic relations.
Another close work is the Global Context-
Aware Neural Language Model (GCANLM for
</bodyText>
<equation confidence="0.727221375">
i+L
C�
j
N
ℓ =
n=1
E
wni Edn =i�L
j0i
�ℓ1=
wEW
hE
�ℓ2=
dED
� (#(w, d)· log σ(⃗w · ⃗d)
wEW
</equation>
<page confidence="0.966591">
140
</page>
<bodyText confidence="0.999775">
short) (Huang et al., 2012). The model defines
two scoring components that contribute to the fi-
nal score of a (word sequence, document) pair.
The architecture of GCANLM seems similar to
our PDC model, but exhibits lots of differences
as follows: (1) GCANLM employs neural net-
works as components while PDC resorts to simple
model structure without non-linear hidden layers;
(2) GCANLM uses weighted average of all word
vectors to represent the document, which turns
out to model words-by-words co-occurrence (i.e.,
paradigmatic relations) again rather than words-
by-documents co-occurrence (i.e., syntagmatic re-
lations); (3) GCANLM is a language model which
predicts the next word given the preceding words,
while PDC model leverages both preceding and
succeeding contexts for prediction.
</bodyText>
<sectionHeader confidence="0.999676" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999967333333333">
In this section, we first describe our experimen-
tal settings including the corpus, hyper-parameter
selections, and baseline methods. Then we com-
pare our models with baseline methods on two
tasks, i.e., word analogy and word similarity. Af-
ter that, we conduct some case studies to show
that our model can better capture both syntagmatic
and paradigmatic relations and how it improves
the performances on semantic tasks.
</bodyText>
<subsectionHeader confidence="0.981629">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999964352941177">
We select Wikipedia, the largest online knowl-
edge base, to train our models. We adopt the
publicly available April 2010 dump3 (Shaoul and
Westbury, 2010), which is also used by (Huang et
al., 2012; Luong et al., 2013; Neelakantan et al.,
2014). The corpus in total has 3, 035, 070 articles
and about 1 billion tokens. In preprocessing, we
lowercase the corpus, remove pure digit words and
non-English characters4.
Following the practice in (Pennington et al.,
2014), we set context window size as 10 and use
10 negative samples. The noise distributions for
context and words are set as the same as used
in (Mikolov et al., 2013a), pnR,(w) a #(w)0.75.
We also adopt the same linear learning rate strat-
egy described in (Mikolov et al., 2013a), where
the initial learning rate of PDC model is 0.05, and
</bodyText>
<footnote confidence="0.97468475">
3http://www.psych.ualberta.ca/—westburylab/downloads/
westburylab.wikicorp.download.html
4We ignore the words less than 20 occurrences during
training.
</footnote>
<bodyText confidence="0.999895826086956">
HDC is 0.025. No additional regularization is used
in our models5.
We compare our models with various state-of-
the-art models including C&amp;W (Collobert et al.,
2011), GCANLM (Huang et al., 2012), CBOW,
SG (Mikolov et al., 2013a), GloVe (Pennington et
al., 2014), PV-DM, PV-DBOW (Le and Mikolov,
2014) and HPCA (Lebret and Collobert, 2014).
For C&amp;W, GCANLM6, GloVe and HPCA, we use
the word embeddings they provided. For CBOW
and SG model, we reimplement these two mod-
els since the original word2vec tool uses SGD
but cannot shuffle the data. Besides, we also im-
plement PV-DM and PV-DBOW models due to
(Le and Mikolov, 2014) has not released source
codes. We train these four models on the same
dataset with the same hyper-parameter settings as
our models for fair comparison. The statistics of
the corpora used in baseline models are shown
in Table 1. Moreover, since different papers re-
port different dimensionality, to be fair, we con-
duct evaluations on three dimensions (i.e., 50, 100,
300) to cover the publicly available results7.
</bodyText>
<subsectionHeader confidence="0.994573">
4.2 Word Analogy
</subsectionHeader>
<bodyText confidence="0.813417">
The word analogy task is introduced by Mikolov et
al. (2013a) to quantitatively evaluate the linguistic
regularities between pairs of word representations.
The task consists of questions like “a is to b as c is
to ”, where is missing and must be guessed
from the entire vocabulary. To answer such ques-
tions, we need to find a word vector i, which is
the closest to b − a + c according to the cosine
similarity:
arg max
xEW,x̸�a
x̸�b, x̸�c
The question is judged as correctly answered only
if x is exactly the answer word in the evaluation
</bodyText>
<footnote confidence="0.87056275">
5Codes avaiable at http://www.bigdatalab.ac.cn/benchma
rk/bm/bd?code=PDC, http://www.bigdatalab.ac.cn/benchma
rk/bm/bd?code=HDC.
6Here, we use GCANLM’s single-prototype embedding.
</footnote>
<tableCaption confidence="0.71106025">
7C&amp;W and GCANLM only released the vectors with 50
dimensions, and HPCA released vectors with 50 and 100 di-
mensions.
Table 1: Corpora used in baseline models.
</tableCaption>
<figure confidence="0.558255375">
model corpus size
C&amp;W Wikipedia 2007 + Reuters RCV1 0.85B
HPCA Wikipedia 2012 1.6B
GloVe Wikipedia 2014+ Gigaword5 6B
GCANLM, CBOW, SG
Wikipedia 2010 1B
PV-DBOW, PV-DM
(b +c−a) - x�
</figure>
<page confidence="0.984314">
141
</page>
<tableCaption confidence="0.946716">
Table 2: Results on the word analogy task. Un-
</tableCaption>
<bodyText confidence="0.616010333333333">
derlined scores are the best within groups of the
same dimensionality, while bold scores are the
best overall.
</bodyText>
<table confidence="0.999367461538462">
model size dim semantic syntactic total
C&amp;W 0.85B 50 9.33 11.33 10.98
GCANLM 1B 50 2.6 10.7 7.34
HPCA 1.6B 50 3.36 9.89 7.2
GloVe 6B 50 48.46 45.24 46.22
CBOW 1B 50 54.38 49.64 52.01
SG 1B 50 53.73 46.12 49.04
PV-DBOW 1B 50 55.02 44.17 49.34
PV-DM 1B 50 45.08 43.22 44.25
PDC 1B 50 61.21 54.55 57.88
HDC 1B 50 57.8 49.74 53.41
HPCA 1.6B 100 4.16 15.73 10.79
GloVe 6B 100 65.34 61.51 63.11
CBOW 1B 100 70.73 63.01 66.87
SG 1B 100 67.66 59.72 63.45
PV-DBOW 1B 100 67.49 56.29 61.51
PV-DM 1B 100 57.72 58.81 58.45
PDC 1B 100 72.77 67.68 70.35
HDC 1B 100 69.57 63.75 66.67
GloVe 6B 300 77.44 67.75 71.7
CBOW 1B 300 76.2 68.44 72.39
SG 1B 300 78.9 65.72 71.88
PV-DBOW 1B 300 66.85 58.5 62.08
PV-DM 1B 300 56.88 68.35 63.39
PDC 1B 300 79.55 69.71 74.76
HDC 1B 300 79.67 67.1 73.13
</table>
<bodyText confidence="0.99892335">
set. The evaluation metric for this task is the per-
centage of questions answered correctly.
The dataset contains 5 types of semantic analo-
gies and 9 types of syntactic analogies8. The se-
mantic analogy contains 8,869 questions, typi-
cally about people and place like “Beijing is to
China as Paris is to France”, while the syntac-
tic analogy contains 10, 675 questions, mostly on
forms of adjectives or verb tense, such as “good is
to better as bad to worse”.
Result Table 2 shows the results on word
analogy task. As we can see that CBOW, SG
and GloVe are much stronger baselines as com-
pare with C&amp;W, GCANLM and HPCA. Even so,
our PDC model still performs significantly bet-
ter than these state-of-the-art methods (p-value
&lt; 0.01), especially with smaller vector dimen-
sionality. More interestingly, by only training
on 1 billion words, our models can outperform
the GloVe model which is trained on 6 billion
</bodyText>
<footnote confidence="0.5340345">
8http://code.google.com/p/word2vec/source/browse/trunk
/questions-words.txt
</footnote>
<bodyText confidence="0.9998114">
words. The results demonstrate that by model-
ing both syntagmatic and paradigmatic relations,
we can learn better word representations capturing
linguistic regularities.
Besides, CBOW, SG and PV-DBOW can be
viewed as sub-models of our proposed models,
since they use either context (i.e., paradigmatic re-
lations) or document (i.e., syntagmatic relations)
alone to predict the target word. By comparing
with these sub-models, we can see that the PDC
and HDC models can perform significantly better
on both syntactic and semantic subtasks. It shows
that by jointly modeling the two relations, one can
boost the representation learning and better cap-
ture both semantic and syntactic regularities.
</bodyText>
<subsectionHeader confidence="0.999452">
4.3 Word Similarity
</subsectionHeader>
<bodyText confidence="0.999978">
Besides the word analogy task, we also evalu-
ate our models on three different word similar-
ity tasks, including WordSim-353 (Finkelstein et
al., 2002), Stanford’s Contextual Word Similari-
ties (SCWS) (Huang et al., 2012) and rare word
(RW) (Luong et al., 2013). These datasets contain
word paris together with human assigned similar-
ity scores. We compute the Spearman rank corre-
lation between similarity scores based on learned
word representations and the human judgements.
In all experiments, we removed the word pairs that
cannot be found in the vocabulary.
Results Figure 4 shows results on three differ-
ent word similarity datasets. First of all, our pro-
posed PDC model always achieves the best per-
formances on the three tasks. Besides, if we com-
pare the PDC and HDC models with their cor-
responding sub-models (i.e., CBOW and SG) re-
spectively, we can see performance gain by adding
syntagmatic information via document. This gain
becomes even larger for rare words with low di-
mensionality as shown on RW dataset. More-
over, on the SCWS dataset, our PDC model us-
ing the single-prototype representations under di-
mensionality 50 can achieve a comparable result
(65.63) to the state-of-the-art GCANLM (65.7 as
the best performance reported in (Huang et al.,
2012)) which uses multi-prototype vectors9.
</bodyText>
<subsectionHeader confidence="0.999856">
4.4 Case Study
</subsectionHeader>
<bodyText confidence="0.940561">
Here we conduct some case studies to (1) gain
some intuition on how these two relations affect
9Note, in Figure 4, the performance of GCANLM is com-
puted based on their released single-prototype vectors.
</bodyText>
<page confidence="0.983272">
142
</page>
<figure confidence="0.9964476">
C&amp;W GCANLM HPCA GloVe PV-DM PV-DBOW SG HDC CBOW PDC
px100
40
80
60
20
50 100 300
px100
40
70
60
50
50 100 300
px100
40
60
20
0
50 100 300
WordSim 353 SCWS RW
</figure>
<figureCaption confidence="0.998857">
Figure 4: Spearman rank correlation on three datasets. Results are grouped by dimensionality.
</figureCaption>
<tableCaption confidence="0.996549">
Table 3: Target words and their 5 most similar
words under different representations. Words in
italic often co-occur with the target words, while
words in bold are substitutable to the target words.
</tableCaption>
<table confidence="0.678227380952381">
feynman
einstein, schwinger, bohm, bethe
CBOW
relativity
schwinger, quantum, bethe, einstein
SG
semiclassical
geometrodynamics, bethe, semiclassical
PDC
schwinger, perturbative
schwinger, electrodynamics, bethe
HDC
semiclassical, quantum
PV-DBOW physicists, spacetime, geometrodynamics
tachyons, einstein
moon
CBOW earth, moons, pluto, sun, nebula
SG earth, sun, mars, planet, aquarius
PDC sun, moons, lunar, heavens, earth
HDC earth, sun, mars, planet, heavens
PV-DBOW lunar, moons, celestial, sun, ecliptic
</table>
<bodyText confidence="0.9995118125">
the representation learning, and (2) analyze why
the joint model can perform better.
To show how syntagmatic and paradigmatic
relations affect the learned representations, we
present the 5 most similar words (by cosine simi-
larity with 50-dimensional vectors) to a given tar-
get word under the PDC and HDC models, as well
as three sub-models, i.e., CBOW, SG, and PV-
DBOW. The results are shown in table 3, where
words in italic are those often co-occurred with
the target word (i.e., syntagmatic relations), while
words in bold are whose substitutable to the target
word (i.e., paradigmatic relation).
Clearly, top words from CBOW and SG mod-
els are more under paradigmatic relations, while
those from PV-DBOW model are more under syn-
</bodyText>
<figureCaption confidence="0.580156333333333">
Figure 5: The 3-D embedding of learned word
vectors of “deep”, “deeper” and “crevasses” under
CBOW and PDC models.
</figureCaption>
<bodyText confidence="0.999971208333333">
tagmatic relations, which is quite consistent with
the model design. By modeling both relations, the
top words from PDC and HDC models become
more diverse, i.e., more syntagmatic relations than
CBOW and SG models, and more paradigmatic re-
lations than PV-DBOW model. The results reveal
that the word representations learned by PDC and
HDC models are more balanced with respect to the
two relations as compared with sub-models.
The next question is why learning a joint model
can work better on previous tasks? We first take
one example from the word analogy task, which is
the question “big is to bigger as deep is to ”
with the correct answer as “deeper”. Our PDC
model produce the right answer but the CBOW
model fails with the answer “shallower”. We thus
embedding the learned word vectors from the two
models into a 3-D space to illustrate and analyze
the reason.
As shown in Figure 5, we can see that by jointly
modeling two relations, PDC model not only re-
quires that “deep” to be close to “deeper” (in co-
sine similarity), but also requires that “deep” and
“deeper” to be close to “crevasses”. The additional
</bodyText>
<figure confidence="0.9987345">
CBOW PDC
0 0
0
0
crevasses
deep
deeper
0
0
crevasses
deep
deeper
</figure>
<page confidence="0.996703">
143
</page>
<bodyText confidence="0.9999364">
requirements further drag these three words closer
as compared with those from the CBOW model,
and this make our model outperform the CBOW
model on this question. As for the word similarity
tasks, we find that the word pairs are either syntag-
matic (e.g., “bank” and “money”) or paradigmatic
(e.g., “left” and “abandon”). It is, therefore, not
surprising to see that a more balanced representa-
tion can achieve much better performance than a
biased representation.
</bodyText>
<sectionHeader confidence="0.998709" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999963333333334">
Existing work on word representations models ei-
ther syntagmatic or paradigmatic relations. In this
paper, we propose two novel distributional models
for word representation, using both syntagmatic
and paradigmatic relations via a joint training ob-
jective. The experimental results on both word
analogy and word similarity tasks show that the
proposed joint models can learn much better word
representations than the state-of-the-art methods.
Several directions remain to be explored. In
this paper, the syntagmatic and paradigmatic rela-
tions are equivalently important in both PDC and
HDC models. An interesting question would then
be whether and how we can add different weights
for syntagmatic and paradigmatic relations. Be-
sides, we may also try to learn the multi-prototype
word representations for polysemous words based
on our proposed models.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999164">
This work was funded by 973 Program of
China under Grants No. 2014CB340401 and
2012CB316303, and the National Natural Sci-
ence Foundation of China (NSFC) under Grants
No. 61232010, 61433014, 61425016, 61472401
and 61203298. We thank Ronan Collobert, Eric
H. Huang, R´emi Lebret, Jeffrey Pennington and
Tomas Mikolov for their kindness in sharing codes
and word vectors. We also thank the anonymous
reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.998486" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999730548387097">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155,
March.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510–526.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan andGadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Trans. Inf. Syst., 20(1):116–
131, January.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
55. Studies in Linguistic Analysis (special volume of
the Philological Society), 1952-59:1–32.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ’12, pages 873–
882, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Tony
Jebara and Eric P. Xing, editors, Proceedings of the
31st International Conference on Machine Learning
(ICML-14), pages 1188–1196. JMLR Workshop and
Conference Proceedings.
R´emi Lebret and Ronan Collobert. 2014. Word em-
beddings through hellinger pca. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, pages
482–490. Association for Computational Linguis-
tics.
Daniel D. Lee and H. Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788–791, october.
Omer Levy and Yoav Goldberg. 2014a. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems
27, pages 2177–2185. Curran Associates, Inc., Mon-
treal, Quebec, Canada.
Omer Levy and Yoav Goldberg, 2014b. Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning, chapter Linguistic Regular-
ities in Sparse and Explicit Word Representations,
pages 171–180. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.987007">
144
</page>
<reference confidence="0.999906952941176">
Kevin Lund, Curt Burgess, and Ruth Ann Atchley.
1995. Semantic and associative priming in a high-
dimensional semantic space. In Proceedings of the
17th Annual Conference of the Cognitive Science
Society, pages 660–665.
Minh-Thang Luong, Richard Socher, and Christo-
pher D. Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learning,
pages 104–113. Association for Computational Lin-
guistics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
of ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed repre-
sentations of words and phrases and their compo-
sitionality. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.
George A Miller and Walter G Charles. 1991. Contex-
tual correlates of semantic similarity. Language &amp;
Cognitive Processes, 6(1):1–28.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the 29th In-
ternational Conference on Machine Learning, pages
1751–1758.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient
non-parametric estimation of multiple embeddings
per word in vector space. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1059–
1069, Doha, Qatar, October. Association for Com-
putational Linguistics.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532–1543.
Justin Picard. 1999. Finding content-bearing terms us-
ing term similarities. In Proceedings of the Ninth
Conference on European Chapter of the Association
for Computational Linguistics, EACL ’99, pages
241–244, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2006. An improved model of
semantic similarity based on lexical co-occurence.
Communications of the ACM, 8:627–633.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627–633, October.
Magnus Sahlgren. 2008. The distributional hypothe-
sis. Italian Journal of Linguistics, 20(1):33–54.
Hinrich Sch¨utze. 1998. Automatic word sense
discrimination. Comput. Linguist., 24(1):97–123,
March.
Cyrus Shaoul and Chris Westbury. 2010. The westbury
lab wikipedia corpus. Edmonton, AB: University of
Alberta.
Richard Socher, Cliff C. Lin, Chris Manning, and An-
drew Y. Ng. 2011. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Lise Getoor and Tobias Scheffer, editors, Proceed-
ings of the 28th International Conference on Ma-
chine Learning (ICML-11), pages 129–136, New
York, NY, USA. ACM.
Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-
des, and Gregory Marton. 2003. Quantitative eval-
uation of passage retrieval algorithms for question
answering. In Proceedings of the 26th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Informaion Retrieval, SIGIR ’03,
pages 41–47, New York, NY, USA. ACM.
</reference>
<page confidence="0.998809">
145
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929954">
<title confidence="0.9989435">Learning Word Representations by Jointly Syntagmatic and Paradigmatic Relations</title>
<author confidence="0.999111">Jiafeng Guo Sun</author>
<author confidence="0.999111">Yanyan Lan</author>
<author confidence="0.999111">Jun Xu</author>
<affiliation confidence="0.977398">CAS Key Lab of Network Data Science and Institute of Computing Chinese Academy of Sciences,</affiliation>
<abstract confidence="0.999727481481481">Vector space representation of words has been widely used to capture fine-grained linguistic regularities, and proven to be successful in various natural language processing tasks in recent years. However, existing models for learning word representations focus on either syntagmatic or paradigmatic relations alone. In this paper, we argue that it is beneficial to jointly modeling both relations so that we can not only encode different types of linguistic properties in a unified way, but also boost the representation learning due to the mutual enhancement between these two types of relations. We propose two novel distributional models for word representation using both syntagmatic and paradigmatic relations via a joint training objective. The proposed models are trained on a public Wikipedia corpus, and the learned representations are evaluated on word analogy and word similarity tasks. The results demonstrate that the proposed models can perform significantly better than all the state-of-the-art baseline methods on both tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1137</pages>
<contexts>
<context position="8623" citStr="Bengio et al., 2003" startWordPosition="1302" endWordPosition="1305">6) transformed the co-occurrence matrix by an entropy or correlation based normalization. Bullinaria and Levy (2007), and Levy and Goldberg (2014b) suggested that positive pointwise mutual information (PPMI) is a good transformation. More recently, Lebret and Collobert (2014) obtained the word representations through a Hellinger PCA (HPCA) of the words-by-words co-occurrence matrix. Pennington et al. (2014) explicitly factorizes the words-by-words co-occurrence matrix to obtain 137 the Global Vectors (GloVe) for word representation. Alternatively, neural probabilistic language models (NPLMs) (Bengio et al., 2003) learn word representations by predicting the next word given previously seen words. Unfortunately, the training of NPLMs is quite time consuming, since computing probabilities in such model requires normalizing over the entire vocabulary. Recently, Mnih and Teh (2012) applied Noise Contrastive Estimation (NCE) to approximately maximize the probability of the softmax in NPLM. Mikolov et al. (2013a) further proposed continuous bagof-words (CBOW) and skip-gram (SG) models, which use a simple single-layer architecture based on inner product between two word vectors. Both models can be learned eff</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word cooccurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="8119" citStr="Bullinaria and Levy (2007)" startWordPosition="1233" endWordPosition="1236">w of some size. For example, the Hyperspace Analogue to Language (HAL) model (Lund et al., 1995) constructed a high-dimensional vector for words based on the word co-occurrence matrix from a large corpus of text. However, a major problem with HAL is that the similarity measure will be dominated by the most frequent words due to its weight scheme. Various methods have been proposed to address the drawback of HAL. For example, the Correlated Occurrence Analogue to Lexical Semantic (COALS) (Rohde et al., 2006) transformed the co-occurrence matrix by an entropy or correlation based normalization. Bullinaria and Levy (2007), and Levy and Goldberg (2014b) suggested that positive pointwise mutual information (PPMI) is a good transformation. More recently, Lebret and Collobert (2014) obtained the word representations through a Hellinger PCA (HPCA) of the words-by-words co-occurrence matrix. Pennington et al. (2014) explicitly factorizes the words-by-words co-occurrence matrix to obtain 137 the Global Vectors (GloVe) for word representation. Alternatively, neural probabilistic language models (NPLMs) (Bengio et al., 2003) learn word representations by predicting the next word given previously seen words. Unfortunate</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods, 39(3):510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1727" citStr="Collobert et al., 2011" startWordPosition="254" endWordPosition="257">dels are trained on a public Wikipedia corpus, and the learned representations are evaluated on word analogy and word similarity tasks. The results demonstrate that the proposed models can perform significantly better than all the state-of-the-art baseline methods on both tasks. 1 Introduction Vector space models of language represent each word with a real-valued vector that captures both semantic and syntactic information of the word. The representations can be used as basic features in a variety of applications, such as information retrieval (Manning et al., 2008), named entity recognition (Collobert et al., 2011), question answering (Tellex et al., 2003), disambiguation (Sch¨utze, 1998), and parsing (Socher et al., 2011). A common paradigm for acquiring such representations is based on the distributional hypothesis (Harris, 1954; Firth, 1957), which states that syntagmatic The wolf is a fierce animal. paradigmatic The tiger is a fierce animal. syntagmatic Figure 1: Example for syntagmatic and paradigmatic relations. words occurring in similar contexts tend to have similar meanings. Based on this hypothesis, various models on learning word representations have been proposed during the last two decades.</context>
<context position="21481" citStr="Collobert et al., 2011" startWordPosition="3538" endWordPosition="3541">e as 10 and use 10 negative samples. The noise distributions for context and words are set as the same as used in (Mikolov et al., 2013a), pnR,(w) a #(w)0.75. We also adopt the same linear learning rate strategy described in (Mikolov et al., 2013a), where the initial learning rate of PDC model is 0.05, and 3http://www.psych.ualberta.ca/—westburylab/downloads/ westburylab.wikicorp.download.html 4We ignore the words less than 20 occurrences during training. HDC is 0.025. No additional regularization is used in our models5. We compare our models with various state-ofthe-art models including C&amp;W (Collobert et al., 2011), GCANLM (Huang et al., 2012), CBOW, SG (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), PV-DM, PV-DBOW (Le and Mikolov, 2014) and HPCA (Lebret and Collobert, 2014). For C&amp;W, GCANLM6, GloVe and HPCA, we use the word embeddings they provided. For CBOW and SG model, we reimplement these two models since the original word2vec tool uses SGD but cannot shuffle the data. Besides, we also implement PV-DM and PV-DBOW models due to (Le and Mikolov, 2014) has not released source codes. We train these four models on the same dataset with the same hyper-parameter settings as our models for fair c</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="2896" citStr="Deerwester et al., 1990" startWordPosition="436" endWordPosition="439">tations have been proposed during the last two decades. According to the leveraged distributional information, existing models can be grouped into two categories (Sahlgren, 2008). The first category mainly concerns the syntagmatic relations among the words, which relate the words that co-occur in the same text region. For example, “wolf” is close to “fierce” since they often co-occur in a sentence, as shown in Figure 1. This type of models learn the distributional representations of words based on the text region that the words occur in, as exemplified by Latent Semantic Analysis (LSA) model (Deerwester et al., 1990) and Non-negative Matrix Factorization (NMF) model (Lee and Seung, 1999). The second category mainly captures paradigmatic relations, which relate words that occur with similar contexts but may not cooccur in the text. For example, “wolf” is close to “tiger” since they often have similar context words. This type of models learn the word representations based on the surrounding words, as exemplified by the Hyperspace Analogue to Language (HAL) model (Lund et al., 1995), Continuous Bag-of-Words (CBOW) model and SkipGram (SG) model (Mikolov et al., 2013a). In this work, we argue that it is import</context>
<context position="6867" citStr="Deerwester et al., 1990" startWordPosition="1039" endWordPosition="1042"> information, the worse the sparse-data problem will be for the resulting representation. Therefore, syntagmatic models tend to favor the use of larger text regions as context. Specifically, a document is often taken as a natural context of a word following the literature of information retrieval. In these methods, a words-by-documents co-occurrence matrix is built to collect the distributional information, where the entry indicates the (normalized) frequency of a word in a document. A low-rank decomposition is then conducted to learn the distributional word representations. For example, LSA (Deerwester et al., 1990) employs singular value decomposition by assuming the decomposed matrices to be orthogonal. In (Lee and Seung, 1999), non-negative matrix factorization is conducted over the wordsby-documents matrix to learn the word representations. Paradigmatic models concern substitutional relations between words (i.e., paradigmatic relations), which relate words that occur in the same context but may not at the same time. Unlike syntagmatic model, paradigmatic models typically collect distributional information in a words-bywords co-occurrence matrix, where entries indicate how many times words occur toget</context>
<context position="10006" citStr="Deerwester et al., 1990" startWordPosition="1518" endWordPosition="1521">t is important to jointly model both syntagmatic and paradigmatic relations to learn good word representations. In this way, we not only encode different types of linguistic properties in a unified way, but also boost the representation learning due to the mutual enhancement between these two types of relations. We propose two joint models that learn the distributional representations of words based on both the text region that the words occur in (i.e., syntagmatic relations) and the surrounding words (i.e., paradigmatic relations). To model syntagmatic relations, we follow the previous work (Deerwester et al., 1990; Lee and Seung, 1999) to take document as a nature text region of a word. To model paradigmatic relations, we are inspired by the recent work from Mikolov et al. (Mikolov et al., 2013a; Mikolov et al., 2013b), where simple models over word sequences are introduced for efficient and effective word representation learning. In the following, we introduce the notations used in this paper, followed by detailed model descriptions, ending with some discussions of the proposed models. 3.1 Notation Before presenting our models, we first list the notations used in this paper. Let D={d1, ... , dN} denot</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan andGadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>20</volume>
<issue>1</issue>
<pages>131</pages>
<contexts>
<context position="26205" citStr="Finkelstein et al., 2002" startWordPosition="4330" endWordPosition="4333">posed models, since they use either context (i.e., paradigmatic relations) or document (i.e., syntagmatic relations) alone to predict the target word. By comparing with these sub-models, we can see that the PDC and HDC models can perform significantly better on both syntactic and semantic subtasks. It shows that by jointly modeling the two relations, one can boost the representation learning and better capture both semantic and syntactic regularities. 4.3 Word Similarity Besides the word analogy task, we also evaluate our models on three different word similarity tasks, including WordSim-353 (Finkelstein et al., 2002), Stanford’s Contextual Word Similarities (SCWS) (Huang et al., 2012) and rare word (RW) (Luong et al., 2013). These datasets contain word paris together with human assigned similarity scores. We compute the Spearman rank correlation between similarity scores based on learned word representations and the human judgements. In all experiments, we removed the word pairs that cannot be found in the vocabulary. Results Figure 4 shows results on three different word similarity datasets. First of all, our proposed PDC model always achieves the best performances on the three tasks. Besides, if we comp</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan andGadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Trans. Inf. Syst., 20(1):116– 131, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-55.</title>
<date>1957</date>
<booktitle>Studies in Linguistic Analysis (special volume of the Philological Society),</booktitle>
<pages>1952--59</pages>
<contexts>
<context position="1961" citStr="Firth, 1957" startWordPosition="291" endWordPosition="292">rt baseline methods on both tasks. 1 Introduction Vector space models of language represent each word with a real-valued vector that captures both semantic and syntactic information of the word. The representations can be used as basic features in a variety of applications, such as information retrieval (Manning et al., 2008), named entity recognition (Collobert et al., 2011), question answering (Tellex et al., 2003), disambiguation (Sch¨utze, 1998), and parsing (Socher et al., 2011). A common paradigm for acquiring such representations is based on the distributional hypothesis (Harris, 1954; Firth, 1957), which states that syntagmatic The wolf is a fierce animal. paradigmatic The tiger is a fierce animal. syntagmatic Figure 1: Example for syntagmatic and paradigmatic relations. words occurring in similar contexts tend to have similar meanings. Based on this hypothesis, various models on learning word representations have been proposed during the last two decades. According to the leveraged distributional information, existing models can be grouped into two categories (Sahlgren, 2008). The first category mainly concerns the syntagmatic relations among the words, which relate the words that co-</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. R. Firth. 1957. A synopsis of linguistic theory 1930-55. Studies in Linguistic Analysis (special volume of the Philological Society), 1952-59:1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1947" citStr="Harris, 1954" startWordPosition="289" endWordPosition="290">state-of-the-art baseline methods on both tasks. 1 Introduction Vector space models of language represent each word with a real-valued vector that captures both semantic and syntactic information of the word. The representations can be used as basic features in a variety of applications, such as information retrieval (Manning et al., 2008), named entity recognition (Collobert et al., 2011), question answering (Tellex et al., 2003), disambiguation (Sch¨utze, 1998), and parsing (Socher et al., 2011). A common paradigm for acquiring such representations is based on the distributional hypothesis (Harris, 1954; Firth, 1957), which states that syntagmatic The wolf is a fierce animal. paradigmatic The tiger is a fierce animal. syntagmatic Figure 1: Example for syntagmatic and paradigmatic relations. words occurring in similar contexts tend to have similar meanings. Based on this hypothesis, various models on learning word representations have been proposed during the last two decades. According to the leveraged distributional information, existing models can be grouped into two categories (Sahlgren, 2008). The first category mainly concerns the syntagmatic relations among the words, which relate the </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="19147" citStr="Huang et al., 2012" startWordPosition="3177" endWordPosition="3180">the PDC model as shown in Equation (4) and (5). In other words, although PV-DM leverages both paragraph and context information, it is unclear how these information is collected and used in this model. As for PV-DBOW, it simply leverages paragraph vector to predict words in the paragraph. It is easy to show that it only uses the words-by-documents co-occurrence matrix, and thus only captures syntagmatic relations. Another close work is the Global ContextAware Neural Language Model (GCANLM for i+L C� j N ℓ = n=1 E wni Edn =i�L j0i �ℓ1= wEW hE �ℓ2= dED � (#(w, d)· log σ(⃗w · ⃗d) wEW 140 short) (Huang et al., 2012). The model defines two scoring components that contribute to the final score of a (word sequence, document) pair. The architecture of GCANLM seems similar to our PDC model, but exhibits lots of differences as follows: (1) GCANLM employs neural networks as components while PDC resorts to simple model structure without non-linear hidden layers; (2) GCANLM uses weighted average of all word vectors to represent the document, which turns out to model words-by-words co-occurrence (i.e., paradigmatic relations) again rather than wordsby-documents co-occurrence (i.e., syntagmatic relations); (3) GCAN</context>
<context position="20562" citStr="Huang et al., 2012" startWordPosition="3396" endWordPosition="3399">st describe our experimental settings including the corpus, hyper-parameter selections, and baseline methods. Then we compare our models with baseline methods on two tasks, i.e., word analogy and word similarity. After that, we conduct some case studies to show that our model can better capture both syntagmatic and paradigmatic relations and how it improves the performances on semantic tasks. 4.1 Experimental Settings We select Wikipedia, the largest online knowledge base, to train our models. We adopt the publicly available April 2010 dump3 (Shaoul and Westbury, 2010), which is also used by (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014). The corpus in total has 3, 035, 070 articles and about 1 billion tokens. In preprocessing, we lowercase the corpus, remove pure digit words and non-English characters4. Following the practice in (Pennington et al., 2014), we set context window size as 10 and use 10 negative samples. The noise distributions for context and words are set as the same as used in (Mikolov et al., 2013a), pnR,(w) a #(w)0.75. We also adopt the same linear learning rate strategy described in (Mikolov et al., 2013a), where the initial learning rate of PDC model is 0.05, </context>
<context position="26274" citStr="Huang et al., 2012" startWordPosition="4340" endWordPosition="4343">or document (i.e., syntagmatic relations) alone to predict the target word. By comparing with these sub-models, we can see that the PDC and HDC models can perform significantly better on both syntactic and semantic subtasks. It shows that by jointly modeling the two relations, one can boost the representation learning and better capture both semantic and syntactic regularities. 4.3 Word Similarity Besides the word analogy task, we also evaluate our models on three different word similarity tasks, including WordSim-353 (Finkelstein et al., 2002), Stanford’s Contextual Word Similarities (SCWS) (Huang et al., 2012) and rare word (RW) (Luong et al., 2013). These datasets contain word paris together with human assigned similarity scores. We compute the Spearman rank correlation between similarity scores based on learned word representations and the human judgements. In all experiments, we removed the word pairs that cannot be found in the vocabulary. Results Figure 4 shows results on three different word similarity datasets. First of all, our proposed PDC model always achieves the best performances on the three tasks. Besides, if we compare the PDC and HDC models with their corresponding sub-models (i.e.,</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 873– 882, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>Proceedings of the 31st International Conference on Machine Learning (ICML-14),</booktitle>
<pages>1188--1196</pages>
<editor>In Tony Jebara and Eric P. Xing, editors,</editor>
<contexts>
<context position="18290" citStr="Mikolov, 2014" startWordPosition="3024" endWordPosition="3025">ls. In other words, PDC can indeed capture both syntagmatic and paradigmatic relations by processing the right distributional information. Please notice that the PDC model is not equivalent to direct combination of existing matrix factorization methods, due to the fact that the matrix entries defined in PDC model are more complicated than the simple cooccurrence frequency (Lee and Seung, 1999). When considering existing models, one may connect our models to the Distributed Memory model of Paragraph Vectors (PV-DM) and the Distributed Bag of Words version of Paragraph Vectors (PV-DBOW) (Le and Mikolov, 2014). However, both of them are quite different from our models. In PV-DM, the paragraph vector and context vectors are averaged or concatenated to predict the next word. Therefore, the objective function of PV-DM can no longer decomposed as the PDC model as shown in Equation (4) and (5). In other words, although PV-DM leverages both paragraph and context information, it is unclear how these information is collected and used in this model. As for PV-DBOW, it simply leverages paragraph vector to predict words in the paragraph. It is easy to show that it only uses the words-by-documents co-occurrenc</context>
<context position="21616" citStr="Mikolov, 2014" startWordPosition="3562" endWordPosition="3563">w) a #(w)0.75. We also adopt the same linear learning rate strategy described in (Mikolov et al., 2013a), where the initial learning rate of PDC model is 0.05, and 3http://www.psych.ualberta.ca/—westburylab/downloads/ westburylab.wikicorp.download.html 4We ignore the words less than 20 occurrences during training. HDC is 0.025. No additional regularization is used in our models5. We compare our models with various state-ofthe-art models including C&amp;W (Collobert et al., 2011), GCANLM (Huang et al., 2012), CBOW, SG (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), PV-DM, PV-DBOW (Le and Mikolov, 2014) and HPCA (Lebret and Collobert, 2014). For C&amp;W, GCANLM6, GloVe and HPCA, we use the word embeddings they provided. For CBOW and SG model, we reimplement these two models since the original word2vec tool uses SGD but cannot shuffle the data. Besides, we also implement PV-DM and PV-DBOW models due to (Le and Mikolov, 2014) has not released source codes. We train these four models on the same dataset with the same hyper-parameter settings as our models for fair comparison. The statistics of the corpora used in baseline models are shown in Table 1. Moreover, since different papers report differen</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188–1196. JMLR Workshop and Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R´emi Lebret</author>
<author>Ronan Collobert</author>
</authors>
<title>Word embeddings through hellinger pca.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>482--490</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8279" citStr="Lebret and Collobert (2014)" startWordPosition="1256" endWordPosition="1259">ord co-occurrence matrix from a large corpus of text. However, a major problem with HAL is that the similarity measure will be dominated by the most frequent words due to its weight scheme. Various methods have been proposed to address the drawback of HAL. For example, the Correlated Occurrence Analogue to Lexical Semantic (COALS) (Rohde et al., 2006) transformed the co-occurrence matrix by an entropy or correlation based normalization. Bullinaria and Levy (2007), and Levy and Goldberg (2014b) suggested that positive pointwise mutual information (PPMI) is a good transformation. More recently, Lebret and Collobert (2014) obtained the word representations through a Hellinger PCA (HPCA) of the words-by-words co-occurrence matrix. Pennington et al. (2014) explicitly factorizes the words-by-words co-occurrence matrix to obtain 137 the Global Vectors (GloVe) for word representation. Alternatively, neural probabilistic language models (NPLMs) (Bengio et al., 2003) learn word representations by predicting the next word given previously seen words. Unfortunately, the training of NPLMs is quite time consuming, since computing probabilities in such model requires normalizing over the entire vocabulary. Recently, Mnih a</context>
<context position="21654" citStr="Lebret and Collobert, 2014" startWordPosition="3566" endWordPosition="3569">opt the same linear learning rate strategy described in (Mikolov et al., 2013a), where the initial learning rate of PDC model is 0.05, and 3http://www.psych.ualberta.ca/—westburylab/downloads/ westburylab.wikicorp.download.html 4We ignore the words less than 20 occurrences during training. HDC is 0.025. No additional regularization is used in our models5. We compare our models with various state-ofthe-art models including C&amp;W (Collobert et al., 2011), GCANLM (Huang et al., 2012), CBOW, SG (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), PV-DM, PV-DBOW (Le and Mikolov, 2014) and HPCA (Lebret and Collobert, 2014). For C&amp;W, GCANLM6, GloVe and HPCA, we use the word embeddings they provided. For CBOW and SG model, we reimplement these two models since the original word2vec tool uses SGD but cannot shuffle the data. Besides, we also implement PV-DM and PV-DBOW models due to (Le and Mikolov, 2014) has not released source codes. We train these four models on the same dataset with the same hyper-parameter settings as our models for fair comparison. The statistics of the corpora used in baseline models are shown in Table 1. Moreover, since different papers report different dimensionality, to be fair, we condu</context>
</contexts>
<marker>Lebret, Collobert, 2014</marker>
<rawString>R´emi Lebret and Ronan Collobert. 2014. Word embeddings through hellinger pca. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482–490. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Learning the parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature,</journal>
<volume>401</volume>
<issue>6755</issue>
<contexts>
<context position="2968" citStr="Lee and Seung, 1999" startWordPosition="446" endWordPosition="450">eraged distributional information, existing models can be grouped into two categories (Sahlgren, 2008). The first category mainly concerns the syntagmatic relations among the words, which relate the words that co-occur in the same text region. For example, “wolf” is close to “fierce” since they often co-occur in a sentence, as shown in Figure 1. This type of models learn the distributional representations of words based on the text region that the words occur in, as exemplified by Latent Semantic Analysis (LSA) model (Deerwester et al., 1990) and Non-negative Matrix Factorization (NMF) model (Lee and Seung, 1999). The second category mainly captures paradigmatic relations, which relate words that occur with similar contexts but may not cooccur in the text. For example, “wolf” is close to “tiger” since they often have similar context words. This type of models learn the word representations based on the surrounding words, as exemplified by the Hyperspace Analogue to Language (HAL) model (Lund et al., 1995), Continuous Bag-of-Words (CBOW) model and SkipGram (SG) model (Mikolov et al., 2013a). In this work, we argue that it is important to 136 Proceedings of the 53rd Annual Meeting of the Association for</context>
<context position="6983" citStr="Lee and Seung, 1999" startWordPosition="1057" endWordPosition="1060">tend to favor the use of larger text regions as context. Specifically, a document is often taken as a natural context of a word following the literature of information retrieval. In these methods, a words-by-documents co-occurrence matrix is built to collect the distributional information, where the entry indicates the (normalized) frequency of a word in a document. A low-rank decomposition is then conducted to learn the distributional word representations. For example, LSA (Deerwester et al., 1990) employs singular value decomposition by assuming the decomposed matrices to be orthogonal. In (Lee and Seung, 1999), non-negative matrix factorization is conducted over the wordsby-documents matrix to learn the word representations. Paradigmatic models concern substitutional relations between words (i.e., paradigmatic relations), which relate words that occur in the same context but may not at the same time. Unlike syntagmatic model, paradigmatic models typically collect distributional information in a words-bywords co-occurrence matrix, where entries indicate how many times words occur together within a context window of some size. For example, the Hyperspace Analogue to Language (HAL) model (Lund et al.,</context>
<context position="10028" citStr="Lee and Seung, 1999" startWordPosition="1522" endWordPosition="1525"> model both syntagmatic and paradigmatic relations to learn good word representations. In this way, we not only encode different types of linguistic properties in a unified way, but also boost the representation learning due to the mutual enhancement between these two types of relations. We propose two joint models that learn the distributional representations of words based on both the text region that the words occur in (i.e., syntagmatic relations) and the surrounding words (i.e., paradigmatic relations). To model syntagmatic relations, we follow the previous work (Deerwester et al., 1990; Lee and Seung, 1999) to take document as a nature text region of a word. To model paradigmatic relations, we are inspired by the recent work from Mikolov et al. (Mikolov et al., 2013a; Mikolov et al., 2013b), where simple models over word sequences are introduced for efficient and effective word representation learning. In the following, we introduce the notations used in this paper, followed by detailed model descriptions, ending with some discussions of the proposed models. 3.1 Notation Before presenting our models, we first list the notations used in this paper. Let D={d1, ... , dN} denote a corpus of N docume</context>
<context position="18072" citStr="Lee and Seung, 1999" startWordPosition="2987" endWordPosition="2990">contexts co-occurrence matrix and a words-bydocuments co-occurrence matrix simultaneously. In this way, we can see that the implementational basis of the PDC model is consistent with that of syntagmatic and paradigmatic models. In other words, PDC can indeed capture both syntagmatic and paradigmatic relations by processing the right distributional information. Please notice that the PDC model is not equivalent to direct combination of existing matrix factorization methods, due to the fact that the matrix entries defined in PDC model are more complicated than the simple cooccurrence frequency (Lee and Seung, 1999). When considering existing models, one may connect our models to the Distributed Memory model of Paragraph Vectors (PV-DM) and the Distributed Bag of Words version of Paragraph Vectors (PV-DBOW) (Le and Mikolov, 2014). However, both of them are quite different from our models. In PV-DM, the paragraph vector and context vectors are averaged or concatenated to predict the next word. Therefore, the objective function of PV-DM can no longer decomposed as the PDC model as shown in Equation (4) and (5). In other words, although PV-DM leverages both paragraph and context information, it is unclear h</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 1999. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–791, october.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27,</booktitle>
<pages>2177--2185</pages>
<publisher>Curran Associates, Inc.,</publisher>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="8148" citStr="Levy and Goldberg (2014" startWordPosition="1238" endWordPosition="1241"> Hyperspace Analogue to Language (HAL) model (Lund et al., 1995) constructed a high-dimensional vector for words based on the word co-occurrence matrix from a large corpus of text. However, a major problem with HAL is that the similarity measure will be dominated by the most frequent words due to its weight scheme. Various methods have been proposed to address the drawback of HAL. For example, the Correlated Occurrence Analogue to Lexical Semantic (COALS) (Rohde et al., 2006) transformed the co-occurrence matrix by an entropy or correlation based normalization. Bullinaria and Levy (2007), and Levy and Goldberg (2014b) suggested that positive pointwise mutual information (PPMI) is a good transformation. More recently, Lebret and Collobert (2014) obtained the word representations through a Hellinger PCA (HPCA) of the words-by-words co-occurrence matrix. Pennington et al. (2014) explicitly factorizes the words-by-words co-occurrence matrix to obtain 137 the Global Vectors (GloVe) for word representation. Alternatively, neural probabilistic language models (NPLMs) (Bengio et al., 2003) learn word representations by predicting the next word given previously seen words. Unfortunately, the training of NPLMs is </context>
<context position="17126" citStr="Levy and Goldberg, 2014" startWordPosition="2823" endWordPosition="2826">, and similar results can be shown for HDC model. The objective function of PDC with negative sampling in Equation (3) can be decomposed into the following two parts: (#(w, h)· log σ(⃗w · ⃗h) H (4) +k·#(h)·pnw(w)log σ(−⃗w· ⃗h)) (5) +k·|d|·pnw(w)log σ(−⃗w· ⃗d)) where #(·, ·) denotes the number of times the pair (·, ·) appears in D, #(h)= EwEW #(w, h), |d| 2Pnc is not necessary to be the same as Pnw. denotes the length of document d, the objective function ℓ1 corresponds to the context-word prediction task and ℓ2 corresponds to the documentword prediction task. Following the idea introduced by (Levy and Goldberg, 2014a), it is easy to show that the solution of the objective function ℓ1 follows that w⃗ · h⃗ = log( #(w, h) ) − log k #(h) · pnw(w) and the solution of the objective function ℓ2 follows that w⃗ · d⃗ = log( #(w, d) ) − log k |d|· pnw(w) It reveals that the PDC model with negative sampling is actually factorizing both a words-bycontexts co-occurrence matrix and a words-bydocuments co-occurrence matrix simultaneously. In this way, we can see that the implementational basis of the PDC model is consistent with that of syntagmatic and paradigmatic models. In other words, PDC can indeed capture both sy</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014a. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems 27, pages 2177–2185. Curran Associates, Inc., Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<date>2014</date>
<booktitle>Proceedings of the Eighteenth Conference on Computational Natural Language Learning, chapter Linguistic Regularities in Sparse and Explicit Word Representations,</booktitle>
<pages>171--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8148" citStr="Levy and Goldberg (2014" startWordPosition="1238" endWordPosition="1241"> Hyperspace Analogue to Language (HAL) model (Lund et al., 1995) constructed a high-dimensional vector for words based on the word co-occurrence matrix from a large corpus of text. However, a major problem with HAL is that the similarity measure will be dominated by the most frequent words due to its weight scheme. Various methods have been proposed to address the drawback of HAL. For example, the Correlated Occurrence Analogue to Lexical Semantic (COALS) (Rohde et al., 2006) transformed the co-occurrence matrix by an entropy or correlation based normalization. Bullinaria and Levy (2007), and Levy and Goldberg (2014b) suggested that positive pointwise mutual information (PPMI) is a good transformation. More recently, Lebret and Collobert (2014) obtained the word representations through a Hellinger PCA (HPCA) of the words-by-words co-occurrence matrix. Pennington et al. (2014) explicitly factorizes the words-by-words co-occurrence matrix to obtain 137 the Global Vectors (GloVe) for word representation. Alternatively, neural probabilistic language models (NPLMs) (Bengio et al., 2003) learn word representations by predicting the next word given previously seen words. Unfortunately, the training of NPLMs is </context>
<context position="17126" citStr="Levy and Goldberg, 2014" startWordPosition="2823" endWordPosition="2826">, and similar results can be shown for HDC model. The objective function of PDC with negative sampling in Equation (3) can be decomposed into the following two parts: (#(w, h)· log σ(⃗w · ⃗h) H (4) +k·#(h)·pnw(w)log σ(−⃗w· ⃗h)) (5) +k·|d|·pnw(w)log σ(−⃗w· ⃗d)) where #(·, ·) denotes the number of times the pair (·, ·) appears in D, #(h)= EwEW #(w, h), |d| 2Pnc is not necessary to be the same as Pnw. denotes the length of document d, the objective function ℓ1 corresponds to the context-word prediction task and ℓ2 corresponds to the documentword prediction task. Following the idea introduced by (Levy and Goldberg, 2014a), it is easy to show that the solution of the objective function ℓ1 follows that w⃗ · h⃗ = log( #(w, h) ) − log k #(h) · pnw(w) and the solution of the objective function ℓ2 follows that w⃗ · d⃗ = log( #(w, d) ) − log k |d|· pnw(w) It reveals that the PDC model with negative sampling is actually factorizing both a words-bycontexts co-occurrence matrix and a words-bydocuments co-occurrence matrix simultaneously. In this way, we can see that the implementational basis of the PDC model is consistent with that of syntagmatic and paradigmatic models. In other words, PDC can indeed capture both sy</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg, 2014b. Proceedings of the Eighteenth Conference on Computational Natural Language Learning, chapter Linguistic Regularities in Sparse and Explicit Word Representations, pages 171–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
<author>Ruth Ann Atchley</author>
</authors>
<title>Semantic and associative priming in a highdimensional semantic space.</title>
<date>1995</date>
<booktitle>In Proceedings of the 17th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>660--665</pages>
<contexts>
<context position="3368" citStr="Lund et al., 1995" startWordPosition="514" endWordPosition="517">ations of words based on the text region that the words occur in, as exemplified by Latent Semantic Analysis (LSA) model (Deerwester et al., 1990) and Non-negative Matrix Factorization (NMF) model (Lee and Seung, 1999). The second category mainly captures paradigmatic relations, which relate words that occur with similar contexts but may not cooccur in the text. For example, “wolf” is close to “tiger” since they often have similar context words. This type of models learn the word representations based on the surrounding words, as exemplified by the Hyperspace Analogue to Language (HAL) model (Lund et al., 1995), Continuous Bag-of-Words (CBOW) model and SkipGram (SG) model (Mikolov et al., 2013a). In this work, we argue that it is important to 136 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 136–145, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics take both syntagmatic and paradigmatic relations into account to build a good distributional model. Firstly, in distributional meaning acquisition, it is expected that a good representation should be </context>
<context position="7589" citStr="Lund et al., 1995" startWordPosition="1146" endWordPosition="1149">Seung, 1999), non-negative matrix factorization is conducted over the wordsby-documents matrix to learn the word representations. Paradigmatic models concern substitutional relations between words (i.e., paradigmatic relations), which relate words that occur in the same context but may not at the same time. Unlike syntagmatic model, paradigmatic models typically collect distributional information in a words-bywords co-occurrence matrix, where entries indicate how many times words occur together within a context window of some size. For example, the Hyperspace Analogue to Language (HAL) model (Lund et al., 1995) constructed a high-dimensional vector for words based on the word co-occurrence matrix from a large corpus of text. However, a major problem with HAL is that the similarity measure will be dominated by the most frequent words due to its weight scheme. Various methods have been proposed to address the drawback of HAL. For example, the Correlated Occurrence Analogue to Lexical Semantic (COALS) (Rohde et al., 2006) transformed the co-occurrence matrix by an entropy or correlation based normalization. Bullinaria and Levy (2007), and Levy and Goldberg (2014b) suggested that positive pointwise mutu</context>
</contexts>
<marker>Lund, Burgess, Atchley, 1995</marker>
<rawString>Kevin Lund, Curt Burgess, and Ruth Ann Atchley. 1995. Semantic and associative priming in a highdimensional semantic space. In Proceedings of the 17th Annual Conference of the Cognitive Science Society, pages 660–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>104--113</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20582" citStr="Luong et al., 2013" startWordPosition="3400" endWordPosition="3403">rimental settings including the corpus, hyper-parameter selections, and baseline methods. Then we compare our models with baseline methods on two tasks, i.e., word analogy and word similarity. After that, we conduct some case studies to show that our model can better capture both syntagmatic and paradigmatic relations and how it improves the performances on semantic tasks. 4.1 Experimental Settings We select Wikipedia, the largest online knowledge base, to train our models. We adopt the publicly available April 2010 dump3 (Shaoul and Westbury, 2010), which is also used by (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014). The corpus in total has 3, 035, 070 articles and about 1 billion tokens. In preprocessing, we lowercase the corpus, remove pure digit words and non-English characters4. Following the practice in (Pennington et al., 2014), we set context window size as 10 and use 10 negative samples. The noise distributions for context and words are set as the same as used in (Mikolov et al., 2013a), pnR,(w) a #(w)0.75. We also adopt the same linear learning rate strategy described in (Mikolov et al., 2013a), where the initial learning rate of PDC model is 0.05, and 3http://www.psyc</context>
<context position="26314" citStr="Luong et al., 2013" startWordPosition="4348" endWordPosition="4351">) alone to predict the target word. By comparing with these sub-models, we can see that the PDC and HDC models can perform significantly better on both syntactic and semantic subtasks. It shows that by jointly modeling the two relations, one can boost the representation learning and better capture both semantic and syntactic regularities. 4.3 Word Similarity Besides the word analogy task, we also evaluate our models on three different word similarity tasks, including WordSim-353 (Finkelstein et al., 2002), Stanford’s Contextual Word Similarities (SCWS) (Huang et al., 2012) and rare word (RW) (Luong et al., 2013). These datasets contain word paris together with human assigned similarity scores. We compute the Spearman rank correlation between similarity scores based on learned word representations and the human judgements. In all experiments, we removed the word pairs that cannot be found in the vocabulary. Results Figure 4 shows results on three different word similarity datasets. First of all, our proposed PDC model always achieves the best performances on the three tasks. Besides, if we compare the PDC and HDC models with their corresponding sub-models (i.e., CBOW and SG) respectively, we can see p</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recursive neural networks for morphology. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104–113. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop of ICLR.</booktitle>
<contexts>
<context position="3452" citStr="Mikolov et al., 2013" startWordPosition="528" endWordPosition="531">y Latent Semantic Analysis (LSA) model (Deerwester et al., 1990) and Non-negative Matrix Factorization (NMF) model (Lee and Seung, 1999). The second category mainly captures paradigmatic relations, which relate words that occur with similar contexts but may not cooccur in the text. For example, “wolf” is close to “tiger” since they often have similar context words. This type of models learn the word representations based on the surrounding words, as exemplified by the Hyperspace Analogue to Language (HAL) model (Lund et al., 1995), Continuous Bag-of-Words (CBOW) model and SkipGram (SG) model (Mikolov et al., 2013a). In this work, we argue that it is important to 136 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 136–145, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics take both syntagmatic and paradigmatic relations into account to build a good distributional model. Firstly, in distributional meaning acquisition, it is expected that a good representation should be able to encode a bunch of linguistic properties. For example, it can put semanticall</context>
<context position="9022" citStr="Mikolov et al. (2013" startWordPosition="1361" endWordPosition="1364"> al. (2014) explicitly factorizes the words-by-words co-occurrence matrix to obtain 137 the Global Vectors (GloVe) for word representation. Alternatively, neural probabilistic language models (NPLMs) (Bengio et al., 2003) learn word representations by predicting the next word given previously seen words. Unfortunately, the training of NPLMs is quite time consuming, since computing probabilities in such model requires normalizing over the entire vocabulary. Recently, Mnih and Teh (2012) applied Noise Contrastive Estimation (NCE) to approximately maximize the probability of the softmax in NPLM. Mikolov et al. (2013a) further proposed continuous bagof-words (CBOW) and skip-gram (SG) models, which use a simple single-layer architecture based on inner product between two word vectors. Both models can be learned efficiently via a simple variant of Noise Contrastive Estimation, i.e., Negative sampling (NS) (Mikolov et al., 2013b). 3 Our Models In this paper, we argue that it is important to jointly model both syntagmatic and paradigmatic relations to learn good word representations. In this way, we not only encode different types of linguistic properties in a unified way, but also boost the representation le</context>
<context position="12003" citStr="Mikolov et al., 2013" startWordPosition="1885" endWordPosition="1888">ument Context Model The first proposed model architecture is shown in Figure 2. In this model, a target word is predicted by its surrounding context, as well as the document it occurs in. The former prediction task captures the paradigmatic relations, since words with similar context will tend to have similar representations. While the latter prediction task models the syntagmatic relations, since words co-occur in the same document will tend to have similar representations. More detailed analysis on this will be presented in Section 3.4. The model can be viewed as an extension of CBOW model (Mikolov et al., 2013a), by adding an extra document branch. Since both the context and document are parallel in predicting the target word, we call this model the Parallel Document Context (PDC) model. More formally, the objective function of PDC ... cni_2 the cat on the ... . . . the cat sat on the . . . Projection sat wni 138 model is the log likelihood of all words cni−2 cn cn cn i−1 i+1 i+2 (log p(wni |hni )+log p(wni |dn)) where hni denotes the projection of wni ’s contexts, defined as N n=1 ℓ = � wni Edn · · · the Projection cat on · · · the sat hin =f(cni−L, ... , cni−1, cni+1, ... , cni+L) where f(·) can </context>
<context position="20993" citStr="Mikolov et al., 2013" startWordPosition="3470" endWordPosition="3473">ct Wikipedia, the largest online knowledge base, to train our models. We adopt the publicly available April 2010 dump3 (Shaoul and Westbury, 2010), which is also used by (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014). The corpus in total has 3, 035, 070 articles and about 1 billion tokens. In preprocessing, we lowercase the corpus, remove pure digit words and non-English characters4. Following the practice in (Pennington et al., 2014), we set context window size as 10 and use 10 negative samples. The noise distributions for context and words are set as the same as used in (Mikolov et al., 2013a), pnR,(w) a #(w)0.75. We also adopt the same linear learning rate strategy described in (Mikolov et al., 2013a), where the initial learning rate of PDC model is 0.05, and 3http://www.psych.ualberta.ca/—westburylab/downloads/ westburylab.wikicorp.download.html 4We ignore the words less than 20 occurrences during training. HDC is 0.025. No additional regularization is used in our models5. We compare our models with various state-ofthe-art models including C&amp;W (Collobert et al., 2011), GCANLM (Huang et al., 2012), CBOW, SG (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), PV-DM, PV-DBOW</context>
<context position="22428" citStr="Mikolov et al. (2013" startWordPosition="3698" endWordPosition="3701">rd2vec tool uses SGD but cannot shuffle the data. Besides, we also implement PV-DM and PV-DBOW models due to (Le and Mikolov, 2014) has not released source codes. We train these four models on the same dataset with the same hyper-parameter settings as our models for fair comparison. The statistics of the corpora used in baseline models are shown in Table 1. Moreover, since different papers report different dimensionality, to be fair, we conduct evaluations on three dimensions (i.e., 50, 100, 300) to cover the publicly available results7. 4.2 Word Analogy The word analogy task is introduced by Mikolov et al. (2013a) to quantitatively evaluate the linguistic regularities between pairs of word representations. The task consists of questions like “a is to b as c is to ”, where is missing and must be guessed from the entire vocabulary. To answer such questions, we need to find a word vector i, which is the closest to b − a + c according to the cosine similarity: arg max xEW,x̸�a x̸�b, x̸�c The question is judged as correctly answered only if x is exactly the answer word in the evaluation 5Codes avaiable at http://www.bigdatalab.ac.cn/benchma rk/bm/bd?code=PDC, http://www.bigdatalab.ac.cn/benchma rk/bm/bd?c</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<editor>In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="3452" citStr="Mikolov et al., 2013" startWordPosition="528" endWordPosition="531">y Latent Semantic Analysis (LSA) model (Deerwester et al., 1990) and Non-negative Matrix Factorization (NMF) model (Lee and Seung, 1999). The second category mainly captures paradigmatic relations, which relate words that occur with similar contexts but may not cooccur in the text. For example, “wolf” is close to “tiger” since they often have similar context words. This type of models learn the word representations based on the surrounding words, as exemplified by the Hyperspace Analogue to Language (HAL) model (Lund et al., 1995), Continuous Bag-of-Words (CBOW) model and SkipGram (SG) model (Mikolov et al., 2013a). In this work, we argue that it is important to 136 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 136–145, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics take both syntagmatic and paradigmatic relations into account to build a good distributional model. Firstly, in distributional meaning acquisition, it is expected that a good representation should be able to encode a bunch of linguistic properties. For example, it can put semanticall</context>
<context position="9022" citStr="Mikolov et al. (2013" startWordPosition="1361" endWordPosition="1364"> al. (2014) explicitly factorizes the words-by-words co-occurrence matrix to obtain 137 the Global Vectors (GloVe) for word representation. Alternatively, neural probabilistic language models (NPLMs) (Bengio et al., 2003) learn word representations by predicting the next word given previously seen words. Unfortunately, the training of NPLMs is quite time consuming, since computing probabilities in such model requires normalizing over the entire vocabulary. Recently, Mnih and Teh (2012) applied Noise Contrastive Estimation (NCE) to approximately maximize the probability of the softmax in NPLM. Mikolov et al. (2013a) further proposed continuous bagof-words (CBOW) and skip-gram (SG) models, which use a simple single-layer architecture based on inner product between two word vectors. Both models can be learned efficiently via a simple variant of Noise Contrastive Estimation, i.e., Negative sampling (NS) (Mikolov et al., 2013b). 3 Our Models In this paper, we argue that it is important to jointly model both syntagmatic and paradigmatic relations to learn good word representations. In this way, we not only encode different types of linguistic properties in a unified way, but also boost the representation le</context>
<context position="12003" citStr="Mikolov et al., 2013" startWordPosition="1885" endWordPosition="1888">ument Context Model The first proposed model architecture is shown in Figure 2. In this model, a target word is predicted by its surrounding context, as well as the document it occurs in. The former prediction task captures the paradigmatic relations, since words with similar context will tend to have similar representations. While the latter prediction task models the syntagmatic relations, since words co-occur in the same document will tend to have similar representations. More detailed analysis on this will be presented in Section 3.4. The model can be viewed as an extension of CBOW model (Mikolov et al., 2013a), by adding an extra document branch. Since both the context and document are parallel in predicting the target word, we call this model the Parallel Document Context (PDC) model. More formally, the objective function of PDC ... cni_2 the cat on the ... . . . the cat sat on the . . . Projection sat wni 138 model is the log likelihood of all words cni−2 cn cn cn i−1 i+1 i+2 (log p(wni |hni )+log p(wni |dn)) where hni denotes the projection of wni ’s contexts, defined as N n=1 ℓ = � wni Edn · · · the Projection cat on · · · the sat hin =f(cni−L, ... , cni−1, cni+1, ... , cni+L) where f(·) can </context>
<context position="20993" citStr="Mikolov et al., 2013" startWordPosition="3470" endWordPosition="3473">ct Wikipedia, the largest online knowledge base, to train our models. We adopt the publicly available April 2010 dump3 (Shaoul and Westbury, 2010), which is also used by (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014). The corpus in total has 3, 035, 070 articles and about 1 billion tokens. In preprocessing, we lowercase the corpus, remove pure digit words and non-English characters4. Following the practice in (Pennington et al., 2014), we set context window size as 10 and use 10 negative samples. The noise distributions for context and words are set as the same as used in (Mikolov et al., 2013a), pnR,(w) a #(w)0.75. We also adopt the same linear learning rate strategy described in (Mikolov et al., 2013a), where the initial learning rate of PDC model is 0.05, and 3http://www.psych.ualberta.ca/—westburylab/downloads/ westburylab.wikicorp.download.html 4We ignore the words less than 20 occurrences during training. HDC is 0.025. No additional regularization is used in our models5. We compare our models with various state-ofthe-art models including C&amp;W (Collobert et al., 2011), GCANLM (Huang et al., 2012), CBOW, SG (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), PV-DM, PV-DBOW</context>
<context position="22428" citStr="Mikolov et al. (2013" startWordPosition="3698" endWordPosition="3701">rd2vec tool uses SGD but cannot shuffle the data. Besides, we also implement PV-DM and PV-DBOW models due to (Le and Mikolov, 2014) has not released source codes. We train these four models on the same dataset with the same hyper-parameter settings as our models for fair comparison. The statistics of the corpora used in baseline models are shown in Table 1. Moreover, since different papers report different dimensionality, to be fair, we conduct evaluations on three dimensions (i.e., 50, 100, 300) to cover the publicly available results7. 4.2 Word Analogy The word analogy task is introduced by Mikolov et al. (2013a) to quantitatively evaluate the linguistic regularities between pairs of word representations. The task consists of questions like “a is to b as c is to ”, where is missing and must be guessed from the entire vocabulary. To answer such questions, we need to find a word vector i, which is the closest to b − a + c according to the cosine similarity: arg max xEW,x̸�a x̸�b, x̸�c The question is judged as correctly answered only if x is exactly the answer word in the evaluation 5Codes avaiable at http://www.bigdatalab.ac.cn/benchma rk/bm/bd?code=PDC, http://www.bigdatalab.ac.cn/benchma rk/bm/bd?c</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<journal>Language &amp; Cognitive Processes,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="6129" citStr="Miller and Charles, 1991" startWordPosition="928" endWordPosition="931">ional hypothesis has provided the foundation for a class of statistical methods for word representation learning. According to the leveraged distributional information, existing models can be grouped into two categories, i.e., syntagmatic models and paradigmatic models. Syntagmatic models concern combinatorial relations between words (i.e., syntagmatic relations), which relate words that co-occur within the same text region (e.g., sentence, paragraph or document). For example, sentences have been used as the text region to acquire co-occurrence information by (Rubenstein and Goodenough, 1965; Miller and Charles, 1991). However, as pointed our by Picard (1999), the smaller the context regions are that we use to collect syntagmatic information, the worse the sparse-data problem will be for the resulting representation. Therefore, syntagmatic models tend to favor the use of larger text regions as context. Specifically, a document is often taken as a natural context of a word following the literature of information retrieval. In these methods, a words-by-documents co-occurrence matrix is built to collect the distributional information, where the entry indicates the (normalized) frequency of a word in a documen</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language &amp; Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning,</booktitle>
<pages>1751--1758</pages>
<contexts>
<context position="8892" citStr="Mnih and Teh (2012)" startWordPosition="1342" endWordPosition="1345">(2014) obtained the word representations through a Hellinger PCA (HPCA) of the words-by-words co-occurrence matrix. Pennington et al. (2014) explicitly factorizes the words-by-words co-occurrence matrix to obtain 137 the Global Vectors (GloVe) for word representation. Alternatively, neural probabilistic language models (NPLMs) (Bengio et al., 2003) learn word representations by predicting the next word given previously seen words. Unfortunately, the training of NPLMs is quite time consuming, since computing probabilities in such model requires normalizing over the entire vocabulary. Recently, Mnih and Teh (2012) applied Noise Contrastive Estimation (NCE) to approximately maximize the probability of the softmax in NPLM. Mikolov et al. (2013a) further proposed continuous bagof-words (CBOW) and skip-gram (SG) models, which use a simple single-layer architecture based on inner product between two word vectors. Both models can be learned efficiently via a simple variant of Noise Contrastive Estimation, i.e., Negative sampling (NS) (Mikolov et al., 2013b). 3 Our Models In this paper, we argue that it is important to jointly model both syntagmatic and paradigmatic relations to learn good word representation</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of the 29th International Conference on Machine Learning, pages 1751–1758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Jeevan Shankar</author>
<author>Alexandre Passos</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient non-parametric estimation of multiple embeddings per word in vector space.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1059--1069</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="20609" citStr="Neelakantan et al., 2014" startWordPosition="3404" endWordPosition="3407">cluding the corpus, hyper-parameter selections, and baseline methods. Then we compare our models with baseline methods on two tasks, i.e., word analogy and word similarity. After that, we conduct some case studies to show that our model can better capture both syntagmatic and paradigmatic relations and how it improves the performances on semantic tasks. 4.1 Experimental Settings We select Wikipedia, the largest online knowledge base, to train our models. We adopt the publicly available April 2010 dump3 (Shaoul and Westbury, 2010), which is also used by (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014). The corpus in total has 3, 035, 070 articles and about 1 billion tokens. In preprocessing, we lowercase the corpus, remove pure digit words and non-English characters4. Following the practice in (Pennington et al., 2014), we set context window size as 10 and use 10 negative samples. The noise distributions for context and words are set as the same as used in (Mikolov et al., 2013a), pnR,(w) a #(w)0.75. We also adopt the same linear learning rate strategy described in (Mikolov et al., 2013a), where the initial learning rate of PDC model is 0.05, and 3http://www.psych.ualberta.ca/—westburylab/</context>
</contexts>
<marker>Neelakantan, Shankar, Passos, McCallum, 2014</marker>
<rawString>Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient non-parametric estimation of multiple embeddings per word in vector space. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059– 1069, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>1532--1543</pages>
<location>Doha,</location>
<contexts>
<context position="8413" citStr="Pennington et al. (2014)" startWordPosition="1274" endWordPosition="1278">by the most frequent words due to its weight scheme. Various methods have been proposed to address the drawback of HAL. For example, the Correlated Occurrence Analogue to Lexical Semantic (COALS) (Rohde et al., 2006) transformed the co-occurrence matrix by an entropy or correlation based normalization. Bullinaria and Levy (2007), and Levy and Goldberg (2014b) suggested that positive pointwise mutual information (PPMI) is a good transformation. More recently, Lebret and Collobert (2014) obtained the word representations through a Hellinger PCA (HPCA) of the words-by-words co-occurrence matrix. Pennington et al. (2014) explicitly factorizes the words-by-words co-occurrence matrix to obtain 137 the Global Vectors (GloVe) for word representation. Alternatively, neural probabilistic language models (NPLMs) (Bengio et al., 2003) learn word representations by predicting the next word given previously seen words. Unfortunately, the training of NPLMs is quite time consuming, since computing probabilities in such model requires normalizing over the entire vocabulary. Recently, Mnih and Teh (2012) applied Noise Contrastive Estimation (NCE) to approximately maximize the probability of the softmax in NPLM. Mikolov et </context>
<context position="20831" citStr="Pennington et al., 2014" startWordPosition="3439" endWordPosition="3442">hat our model can better capture both syntagmatic and paradigmatic relations and how it improves the performances on semantic tasks. 4.1 Experimental Settings We select Wikipedia, the largest online knowledge base, to train our models. We adopt the publicly available April 2010 dump3 (Shaoul and Westbury, 2010), which is also used by (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014). The corpus in total has 3, 035, 070 articles and about 1 billion tokens. In preprocessing, we lowercase the corpus, remove pure digit words and non-English characters4. Following the practice in (Pennington et al., 2014), we set context window size as 10 and use 10 negative samples. The noise distributions for context and words are set as the same as used in (Mikolov et al., 2013a), pnR,(w) a #(w)0.75. We also adopt the same linear learning rate strategy described in (Mikolov et al., 2013a), where the initial learning rate of PDC model is 0.05, and 3http://www.psych.ualberta.ca/—westburylab/downloads/ westburylab.wikicorp.download.html 4We ignore the words less than 20 occurrences during training. HDC is 0.025. No additional regularization is used in our models5. We compare our models with various state-ofthe</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532–1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Picard</author>
</authors>
<title>Finding content-bearing terms using term similarities.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL ’99,</booktitle>
<pages>241--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6171" citStr="Picard (1999)" startWordPosition="937" endWordPosition="938">ss of statistical methods for word representation learning. According to the leveraged distributional information, existing models can be grouped into two categories, i.e., syntagmatic models and paradigmatic models. Syntagmatic models concern combinatorial relations between words (i.e., syntagmatic relations), which relate words that co-occur within the same text region (e.g., sentence, paragraph or document). For example, sentences have been used as the text region to acquire co-occurrence information by (Rubenstein and Goodenough, 1965; Miller and Charles, 1991). However, as pointed our by Picard (1999), the smaller the context regions are that we use to collect syntagmatic information, the worse the sparse-data problem will be for the resulting representation. Therefore, syntagmatic models tend to favor the use of larger text regions as context. Specifically, a document is often taken as a natural context of a word following the literature of information retrieval. In these methods, a words-by-documents co-occurrence matrix is built to collect the distributional information, where the entry indicates the (normalized) frequency of a word in a document. A low-rank decomposition is then conduc</context>
</contexts>
<marker>Picard, 1999</marker>
<rawString>Justin Picard. 1999. Finding content-bearing terms using term similarities. In Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL ’99, pages 241–244, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L T Rohde</author>
<author>Laura M Gonnerman</author>
<author>David C Plaut</author>
</authors>
<title>An improved model of semantic similarity based on lexical co-occurence.</title>
<date>2006</date>
<journal>Communications of the ACM,</journal>
<pages>8--627</pages>
<contexts>
<context position="8005" citStr="Rohde et al., 2006" startWordPosition="1217" endWordPosition="1220">rds co-occurrence matrix, where entries indicate how many times words occur together within a context window of some size. For example, the Hyperspace Analogue to Language (HAL) model (Lund et al., 1995) constructed a high-dimensional vector for words based on the word co-occurrence matrix from a large corpus of text. However, a major problem with HAL is that the similarity measure will be dominated by the most frequent words due to its weight scheme. Various methods have been proposed to address the drawback of HAL. For example, the Correlated Occurrence Analogue to Lexical Semantic (COALS) (Rohde et al., 2006) transformed the co-occurrence matrix by an entropy or correlation based normalization. Bullinaria and Levy (2007), and Levy and Goldberg (2014b) suggested that positive pointwise mutual information (PPMI) is a good transformation. More recently, Lebret and Collobert (2014) obtained the word representations through a Hellinger PCA (HPCA) of the words-by-words co-occurrence matrix. Pennington et al. (2014) explicitly factorizes the words-by-words co-occurrence matrix to obtain 137 the Global Vectors (GloVe) for word representation. Alternatively, neural probabilistic language models (NPLMs) (Be</context>
</contexts>
<marker>Rohde, Gonnerman, Plaut, 2006</marker>
<rawString>Douglas L. T. Rohde, Laura M. Gonnerman, and David C. Plaut. 2006. An improved model of semantic similarity based on lexical co-occurence. Communications of the ACM, 8:627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Commun. ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="6102" citStr="Rubenstein and Goodenough, 1965" startWordPosition="924" endWordPosition="927">sks. 2 Related Work The distributional hypothesis has provided the foundation for a class of statistical methods for word representation learning. According to the leveraged distributional information, existing models can be grouped into two categories, i.e., syntagmatic models and paradigmatic models. Syntagmatic models concern combinatorial relations between words (i.e., syntagmatic relations), which relate words that co-occur within the same text region (e.g., sentence, paragraph or document). For example, sentences have been used as the text region to acquire co-occurrence information by (Rubenstein and Goodenough, 1965; Miller and Charles, 1991). However, as pointed our by Picard (1999), the smaller the context regions are that we use to collect syntagmatic information, the worse the sparse-data problem will be for the resulting representation. Therefore, syntagmatic models tend to favor the use of larger text regions as context. Specifically, a document is often taken as a natural context of a word following the literature of information retrieval. In these methods, a words-by-documents co-occurrence matrix is built to collect the distributional information, where the entry indicates the (normalized) frequ</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM, 8(10):627–633, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The distributional hypothesis.</title>
<date>2008</date>
<journal>Italian Journal of Linguistics,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="2450" citStr="Sahlgren, 2008" startWordPosition="364" endWordPosition="365">1). A common paradigm for acquiring such representations is based on the distributional hypothesis (Harris, 1954; Firth, 1957), which states that syntagmatic The wolf is a fierce animal. paradigmatic The tiger is a fierce animal. syntagmatic Figure 1: Example for syntagmatic and paradigmatic relations. words occurring in similar contexts tend to have similar meanings. Based on this hypothesis, various models on learning word representations have been proposed during the last two decades. According to the leveraged distributional information, existing models can be grouped into two categories (Sahlgren, 2008). The first category mainly concerns the syntagmatic relations among the words, which relate the words that co-occur in the same text region. For example, “wolf” is close to “fierce” since they often co-occur in a sentence, as shown in Figure 1. This type of models learn the distributional representations of words based on the text region that the words occur in, as exemplified by Latent Semantic Analysis (LSA) model (Deerwester et al., 1990) and Non-negative Matrix Factorization (NMF) model (Lee and Seung, 1999). The second category mainly captures paradigmatic relations, which relate words t</context>
<context position="15920" citStr="Sahlgren, 2008" startWordPosition="2621" endWordPosition="2622">+ log σ( ⃗wn i · ⃗dn) + k·Ew′—Pnwlog σ( ⃗w′· ⃗dn) where k is the number of the negative samples, c′ and w′ denotes the sampled context and word respectively, and Pnc and Pnw denotes the distribution of negative context and word samples respectively2. We also employ SGD for optimization, and calculate the gradient via back-propagation algorithm. 3.4 Discussions In this section we first show how PDC and HDC models capture the syntagmatic and paradigmatic relations from the viewpoint of matrix factorization. We then talk about the relationship of our models with previous work. As pointed out in (Sahlgren, 2008), to capture syntagmatic relations, the implementational basis is to collect text data in a words-by-documents cooccurrence matrix in which the entry indicates the (normalized) frequency of occurrence of a word in a document (or, some other type of text region, e.g., a sentence). While the implementational basis for paradigmatic relations is to collect text data in a words-by-words co-occurrence matrix that is populated by counting how many times words occur together within the context window. We now take the proposed PDC model as an example to show how it achieves these goals, and similar res</context>
</contexts>
<marker>Sahlgren, 2008</marker>
<rawString>Magnus Sahlgren. 2008. The distributional hypothesis. Italian Journal of Linguistics, 20(1):33–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Comput. Linguist.,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Comput. Linguist., 24(1):97–123, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Shaoul</author>
<author>Chris Westbury</author>
</authors>
<title>The westbury lab wikipedia corpus.</title>
<date>2010</date>
<institution>University of Alberta.</institution>
<location>Edmonton, AB:</location>
<contexts>
<context position="20519" citStr="Shaoul and Westbury, 2010" startWordPosition="3387" endWordPosition="3390">r prediction. 4 Experiments In this section, we first describe our experimental settings including the corpus, hyper-parameter selections, and baseline methods. Then we compare our models with baseline methods on two tasks, i.e., word analogy and word similarity. After that, we conduct some case studies to show that our model can better capture both syntagmatic and paradigmatic relations and how it improves the performances on semantic tasks. 4.1 Experimental Settings We select Wikipedia, the largest online knowledge base, to train our models. We adopt the publicly available April 2010 dump3 (Shaoul and Westbury, 2010), which is also used by (Huang et al., 2012; Luong et al., 2013; Neelakantan et al., 2014). The corpus in total has 3, 035, 070 articles and about 1 billion tokens. In preprocessing, we lowercase the corpus, remove pure digit words and non-English characters4. Following the practice in (Pennington et al., 2014), we set context window size as 10 and use 10 negative samples. The noise distributions for context and words are set as the same as used in (Mikolov et al., 2013a), pnR,(w) a #(w)0.75. We also adopt the same linear learning rate strategy described in (Mikolov et al., 2013a), where the i</context>
</contexts>
<marker>Shaoul, Westbury, 2010</marker>
<rawString>Cyrus Shaoul and Chris Westbury. 2010. The westbury lab wikipedia corpus. Edmonton, AB: University of Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>129--136</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1837" citStr="Socher et al., 2011" startWordPosition="270" endWordPosition="273">word similarity tasks. The results demonstrate that the proposed models can perform significantly better than all the state-of-the-art baseline methods on both tasks. 1 Introduction Vector space models of language represent each word with a real-valued vector that captures both semantic and syntactic information of the word. The representations can be used as basic features in a variety of applications, such as information retrieval (Manning et al., 2008), named entity recognition (Collobert et al., 2011), question answering (Tellex et al., 2003), disambiguation (Sch¨utze, 1998), and parsing (Socher et al., 2011). A common paradigm for acquiring such representations is based on the distributional hypothesis (Harris, 1954; Firth, 1957), which states that syntagmatic The wolf is a fierce animal. paradigmatic The tiger is a fierce animal. syntagmatic Figure 1: Example for syntagmatic and paradigmatic relations. words occurring in similar contexts tend to have similar meanings. Based on this hypothesis, various models on learning word representations have been proposed during the last two decades. According to the leveraged distributional information, existing models can be grouped into two categories (Sa</context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C. Lin, Chris Manning, and Andrew Y. Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129–136, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
<author>Aaron Fernandes</author>
<author>Gregory Marton</author>
</authors>
<title>Quantitative evaluation of passage retrieval algorithms for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR ’03,</booktitle>
<pages>41--47</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1769" citStr="Tellex et al., 2003" startWordPosition="261" endWordPosition="264">, and the learned representations are evaluated on word analogy and word similarity tasks. The results demonstrate that the proposed models can perform significantly better than all the state-of-the-art baseline methods on both tasks. 1 Introduction Vector space models of language represent each word with a real-valued vector that captures both semantic and syntactic information of the word. The representations can be used as basic features in a variety of applications, such as information retrieval (Manning et al., 2008), named entity recognition (Collobert et al., 2011), question answering (Tellex et al., 2003), disambiguation (Sch¨utze, 1998), and parsing (Socher et al., 2011). A common paradigm for acquiring such representations is based on the distributional hypothesis (Harris, 1954; Firth, 1957), which states that syntagmatic The wolf is a fierce animal. paradigmatic The tiger is a fierce animal. syntagmatic Figure 1: Example for syntagmatic and paradigmatic relations. words occurring in similar contexts tend to have similar meanings. Based on this hypothesis, various models on learning word representations have been proposed during the last two decades. According to the leveraged distributional</context>
</contexts>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR ’03, pages 41–47, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>