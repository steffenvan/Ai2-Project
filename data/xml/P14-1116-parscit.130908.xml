<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000757">
<title confidence="0.9990925">
Comparing Multi-label Classification with Reinforcement Learning for
Summarisation of Time-series Data
</title>
<author confidence="0.991869">
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
</author>
<affiliation confidence="0.975668">
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
</affiliation>
<email confidence="0.98798">
{dg106, h.hastie, o.lemon}@hw.ac.uk
</email>
<sectionHeader confidence="0.993696" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999995">
We present a novel approach for automatic
report generation from time-series data, in
the context of student feedback genera-
tion. Our proposed methodology treats
content selection as a multi-label (ML)
classification problem, which takes as in-
put time-series data and outputs a set of
templates, while capturing the dependen-
cies between selected templates. We show
that this method generates output closer to
the feedback that lecturers actually gener-
ated, achieving 3.5% higher accuracy and
15% higher F-score than multiple simple
classifiers that keep a history of selected
templates. Furthermore, we compare a
ML classifier with a Reinforcement Learn-
ing (RL) approach in simulation and using
ratings from real student users. We show
that the different methods have different
benefits, with ML being more accurate for
predicting what was seen in the training
data, whereas RL is more exploratory and
slightly preferred by the students.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999338">
Summarisation of time-series data refers to the
task of automatically generating text from vari-
ables whose values change over time. We con-
sider the task of automatically generating feed-
back summaries for students describing their per-
formance during the lab of a Computer Science
module over the semester. Students’ learning can
be influenced by many variables, such as difficulty
of the material (Person et al., 1995), other dead-
lines (Craig et al., 2004), attendance in lectures
(Ames, 1992), etc. These variables have two im-
portant qualities. Firstly, they change over time,
and secondly they can be dependent on or inde-
pendent of each other. Therefore, when generating
feedback, we need to take into account all vari-
ables simultaneously in order to capture potential
dependencies and provide more effective and use-
ful feedback that is relevant to the students.
In this work, we concentrate on content selec-
tion which is the task of choosing what to say,
i.e. what information is to be included in a report
(Reiter and Dale, 2000). Content selection deci-
sions based on trends in time-series data determine
the selection of the useful and important variables,
which we refer to here as factors, that should be
conveyed in a summary. The decisions of factor
selection can be influenced by other factors that
their values are correlated with; can be based on
the appearance or absence of other factors in the
summary; and can be based on the factors’ be-
haviour over time. Moreover, some factors may
have to be discussed together in order to achieve
some communicative goal, for instance, a teacher
might want to refer to student’s marks as a moti-
vation for increasing the number of hours studied.
We frame content selection as a simple classifi-
cation task: given a set of time-series data, decide
for each template whether it should be included
in a summary or not. In this paper, with the term
‘template’ we refer to a quadruple consisting of an
id, a factor (bottom left of Table 1), a reference
type (trend, weeks, average, other) and surface
text. However, simple classification assumes that
the templates are independent of each other, thus
the decision for each template is taken in isolation
from the others, which is not appropriate for our
domain. In order to capture the dependencies in
the context, multiple simple classifiers can make
the decisions for each template iteratively. After
each iteration, the feature space grows by 1 fea-
ture, in order to include the history of the previous
template decisions. Here, we propose an alterna-
tive method that tackles the challenge of interde-
pendent data by using multi-label (ML) classifica-
tion, which is efficient in taking data dependencies
</bodyText>
<page confidence="0.923711">
1231
</page>
<note confidence="0.8743585">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1231–1240,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.376998285714286">
Raw Data
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Trends from Data
factors trend
</figure>
<listItem confidence="0.989870333333333">
(1) marks (M) trend other
(2) hours studied (HS) trend increasing
(3) understandability (Und) trend decreasing
(4) difficulty (Diff) trend decreasing
(5) deadlines (DL) trend increasing
(6) health issues (HI) trend other
(7) personal issues (PI) trend decreasing
(8) lectures attended (LA) trend other
(9) revision (R) trend decreasing
</listItem>
<sectionHeader confidence="0.611422" genericHeader="introduction">
Summary
</sectionHeader>
<bodyText confidence="0.989993175">
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think whether revis-
ing has improved your performance.
Table 1: The table on the top left shows an example of the time-series raw data for feedback generation.
The table on the bottom left shows an example of described trends. The box on the right presents a target
summary (target summaries have been constructed by teaching staff).
into account and generating a set of labels (in our
case templates) simultaneously (Tsoumakas et al.,
2010). ML classification requires no history, i.e.
does not keep track of previous decisions, and thus
has a smaller feature space.
Our contributions to the field are as follows: we
present a novel and efficient method for tackling
the challenge of content selection using a ML clas-
sification approach; we applied this method to the
domain of feedback summarisation; we present a
comparison with an optimisation technique (Rein-
forcement Learning), and we discuss the similari-
ties and differences between the two methods.
In the next section, we refer to the related work
on Natural Language Generation from time-series
data and on Content Selection. In Section 4.2, we
describe our approach and we carry out a compar-
ison with simple classification methods. In Sec-
tion 5, we present the evaluation setup and in Sec-
tion 6 we discuss the results, obtained in simula-
tion and with real students. Finally, in Section 8,
directions for future work are discussed.
</bodyText>
<sectionHeader confidence="0.999805" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997427794117647">
Natural Language Generation from time-series
data has been investigated for various tasks such
as weather forecast generation (Belz and Kow,
2010; Angeli et al., 2010; Sripada et al., 2004),
report generation from clinical data (Hunter et al.,
2011; Gatt et al., 2009), narrative to assist children
with communication needs (Black et al., 2010) and
audiovisual debrief generation from sensor data
from Autonomous Underwater Vehicles missions
(Johnson and Lane, 2011).
The important tasks of time-series data sum-
marisation systems are content selection (what to
say), surface realisation (how to say it) and infor-
mation presentation (Document Planning, Order-
ing, etc.). In this work, we concentrate on content
selection. Previous methods for content selection
include Reinforcement Learning (Rieser et al.,
2010); multi-objective optimisation (Gkatzia et
al., 2014); Gricean Maxims (Sripada et al., 2003);
Integer Linear Programming (Lampouras and An-
droutsopoulos, 2013); collective content selection
(Barzilay and Lapata, 2004); interest scores as-
signed to content (Androutsopoulos et al., 2013); a
combination of statistical and template-based ap-
proaches to NLG (Kondadadi et al., 2013); statis-
tical acquisition of rules (Duboue and McKeown,
2003) and the Hidden Markov model approach for
Content Selection and ordering (Barzilay and Lee,
2004).
Collective content selection (Barzilay and La-
pata, 2004) is similar to our proposed method in
that it is a classification task that predicts the tem-
plates from the same instance simultaneously. The
difference between the two methods lies in that the
</bodyText>
<page confidence="0.989206">
1232
</page>
<bodyText confidence="0.999838604166667">
collective content selection requires the considera-
tion of an individual preference score (which is de-
fined as the preference of the entity to be selected
or omitted, and it is based on the values of entity
attributes and is computed using a boosting algo-
rithm) and the identification of links between the
entities with similar labels. In contrast, ML clas-
sification does not need the computation of links
between the data and the templates. ML classi-
fication can also apply to other problems whose
features are correlated, such as text classification
(Madjarov et al., 2012), when an aligned dataset is
provided.
ML classification algorithms have been divided
into three categories: algorithm adaptation meth-
ods, problem transformation and ensemble meth-
ods (Tsoumakas and Katakis, 2007; Madjarov
et al., 2012). Algorithm adaptation approaches
(Tsoumakas et al., 2010) extend simple classifi-
cation methods to handle ML data. For exam-
ple, the k-nearest neighbour algorithm is extended
to ML-kNN by Zhang and Zhou (2007). ML-
kNN identifies for each new instance its k nearest
neighbours in the training set and then it predicts
the label set by utilising the maximum a posteri-
ori principle according to statistical information
derived from the label sets of the k neighbours.
Problem transformation approaches (Tsoumakas
and Katakis, 2007) transform the ML classifica-
tion task into one or more simple classification
tasks. Ensemble methods (Tsoumakas et al., 2010)
are algorithms that use ensembles to perform ML
learning and they are based on problem transfor-
mation or algorithm adaptation methods. In this
paper, we applied RAkEL (Random k-labelsets)
(Tsoumakas et al., 2010): an ensemble problem
transformation method, which constructs an en-
semble of simple-label classifiers, where each one
deals with a random subset of the labels.
Finally, our domain for feedback generation is
motivated by previous studies (Law et al., 2005;
van den Meulen et al., 2010) who show that text
summaries are more effective in decision making
than graphs therefore it is advantageous to provide
a summary over showing users the raw data graph-
ically. In addition, feedback summarisation from
time-series data can be applied to the field of In-
telligent Tutoring Systems (Gross et al., 2012).
</bodyText>
<sectionHeader confidence="0.993949" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999057166666667">
The dataset consists of 37 instances referring to
the activities of 26 students. For a few students
there is more than 1 instance. An example of one
such instance is presented in Table 1. Each in-
stance includes time-series information about the
student’s learning habits and the selected tem-
plates that lecturers used to provide feedback to
this student. The time-series information includes
for each week of the semester: (1) the marks
achieved at the lab; (2) the hours that the stu-
dent spent studying; (3) the understandability of
the material; (4) the difficulty of the lab exercises
as assessed by the student; (5) the number of other
deadlines that the student had that week; (6) health
issues; (7) personal issues; (8) the number of lec-
tures attended; and (9) the amount of revision that
the student had performed. The templates describe
these factors in four different ways:
</bodyText>
<listItem confidence="0.992020083333333">
1. &lt;trend&gt;: referring to the trend of a fac-
tor over the semester (e.g. “Your performance
was increasing...”),
2. &lt;weeks&gt;: explicitly describing the factor
value at specific weeks (e.g. “In weeks 2, 3
and 9...”),
3. &lt;average&gt;: considering the average of a
factor value (e.g. “You dedicated 1.5 hours
studying on average...”), and
4. &lt;other&gt;: mentioning other relevant infor-
mation (e.g. “Revising material will improve
your performance”).
</listItem>
<bodyText confidence="0.9999203">
For the corpus creation, 11 lecturers selected the
content to be conveyed in a summary, given the
set of raw data (Gkatzia et al., 2013). As a result,
for the same student there are various summaries
provided by the different experts. This character-
istic of the dataset, that each instance is associated
with more than one solution, additionally moti-
vates the use of multi-label classification, which
is concerned with learning from examples, where
each example is associated with multiple labels.
Our analysis of the dataset showed that there
are significant correlations between the factors, for
example, the number of lectures attended (LA)
correlates with the student’s understanding of the
material (Und), see Table 2. As we will discuss
further in Section 5.1, content decisions are in-
fluenced by the previously generated content, for
example, if the lecturer has previously mentioned
health issues, mentioning hours studied has a high
probability of also being mentioned.
</bodyText>
<page confidence="0.879864">
1233
</page>
<table confidence="0.8386364">
Factor (1) M (2) HS (3) Und (4) Diff (5) DL (6) HI (7) PI (8) LA (9) R
(1) M 1* 0.52* 0.44* -0.53* -0.31 -0.30 -0.36* 0.44* 0.16
(2) HS 0.52* 1* 0.23 -0.09 -0.11 0.11 -0.29 0.32 0.47*
(3) Und 0.44* 0.23 1* -0.54* 0.03 -0.26 0.12 0.60* 0.32
(4) Diff -0.53* -0.09 -0.54* 1* 0.16 -0.06 0.03 -0.19 0.14
(5) DL -0.31 -0.11 0.03 0.16 1* 0.26 0.24 -0.44* 0.14
(6) HI -0.30 -0.11 -0.26 -0.06 0.26 1* 0.27 -0.50* 0.15
(7) PI -0.36* -0.29 0.12 0.03 0.24 0.27 1* -0.46* 0.34*
(8) LA 0.44* 0.32 0.60* -0.19 -0.44* -0.50* -0.46* 1* -0.12
(9) R 0.16 0.47* 0.03 0.14 0.14 0.15 0.34* -0.12 1*
</table>
<tableCaption confidence="0.99799">
Table 2: The table presents the Pearson’s correlation coefficients of the factors (* means p&lt;0.05).
</tableCaption>
<sectionHeader confidence="0.994842" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.9999228">
In this section, the content selection task and the
suggested multi-label classification approach are
presented. The development and evaluation of the
time-series generation system follows the follow-
ing pipeline (Gkatzia et al., 2013):
</bodyText>
<listItem confidence="0.999391454545455">
1. Time-Series data collection from students
2. Template construction by Learning and
Teaching (L&amp;T) expert
3. Feedback summaries constructed by lectur-
ers; random summaries rated by lecturers
4. Development of time-series generation sys-
tems (Section 4.2, Section 5.3): ML system,
RL system, Rule-based and Random system
5. Evaluation: (Section 5)
- Offline evaluation (Accuracy and Reward)
- Online evaluation (Subjective Ratings)
</listItem>
<subsectionHeader confidence="0.990743">
4.1 The Content Selection Task
</subsectionHeader>
<bodyText confidence="0.999993411764706">
Our learning task is formed as follows: given a
set of 9 time-series factors, select the content that
is most appropriate to be included in a summary.
Content is regarded as labels (each template rep-
resents a label) and thus the task can be thought of
as a classification problem. As mentioned, there
are 4 ways to refer to a factor: (1) describing the
trend, (2) describing what happened in every time
stamp, (3) mentioning the average and (4) making
another general statement. Overall, for all factors
there are 29 different templates1. An example of
the input data is shown in Table 1. There are two
decisions that need to be made: (1) whether to talk
about a factor and (2) in which way to refer to it.
Instead of dealing with this task in a hierarchical
way, where the algorithm will first learn whether
to talk about a factor and then to decide how to
</bodyText>
<footnote confidence="0.5664445">
1There are fewer than 36 templates, because for some fac-
tors there are less than 4 possible ways of referring to them.
</footnote>
<bodyText confidence="0.996741">
refer to it, we transformed the task in order to re-
duce the learning steps. Therefore, classification
can reduce the decision workload by deciding ei-
ther in which way to talk about it, or not to talk
about a factor at all.
</bodyText>
<subsectionHeader confidence="0.957134">
4.2 The Multi-label Classification Approach
</subsectionHeader>
<bodyText confidence="0.99972446875">
Traditional single-label classification is the task of
identifying which label one new observation is as-
sociated with, by choosing from a set of labels L
(Tsoumakas et al., 2010). Multi-label classifica-
tion is the task of associating an observation with
a set of labels Y C_ L (Tsoumakas et al., 2010).
One set of factor values can result in various
sets of templates as interpreted by the different
experts. A ML classifier is able to make deci-
sions for all templates simultaneously and cap-
ture these differences. The RAndom k-labELsets
(RAkEL) (Tsoumakas et al., 2010) was applied
in order to perform ML classification. RAkEL is
based on Label Powerset (LP), a problem transfor-
mation method (Tsoumakas et al., 2010). LP ben-
efits from taking into consideration label correla-
tions, but does not perform well when trained with
few examples as in our case (Tsoumakas et al.,
2010). RAkEL overcomes this limitation by con-
structing a set of LP classifiers, which are trained
with different random subsets of the set of labels
(Tsoumakas et al., 2010).
The LP method transforms the ML task, into
one single-label multi-class classification task,
where the possible set of predicted variables for
the transformed class is the powerset of labels
present in the original dataset. For instance, the set
of labels L = {temp0, temp1, ...temp281 could be
transformed to {temp0,1,2, temp28,3,17,...1. This
algorithm does not perform well when consider-
ing a large number of labels, due to the fact that
the label space grows exponentially (Tsoumakas
</bodyText>
<page confidence="0.914337">
1234
</page>
<table confidence="0.999723571428571">
Classifier Accuracy Precision Recall F score
(10-fold)
Decision Tree (no history) *75.95% 67.56 75.96 67.87
Decision Tree (with predicted history) **73.43% 65.49 72.05 70.95
Decision Tree (with real history) **78.09% 74.51 78.11 75.54
Majority-class (single label) **72.02% 61.73 77.37 68.21
RAkEL (multi-label) (no history) 76.95% 85.08 85.94 85.50
</table>
<tableCaption confidence="0.888877">
Table 3: Average, precision, recall and F-score of the different classification methods (T-test, * denotes
significance with p&lt;0.05 and ** significance with p&lt;0.01, when comparing each result to RAkEL).
</tableCaption>
<bodyText confidence="0.9949015">
et al., 2010). RAkEL tackles this problem by con-
structing an ensemble of LP classifiers and train-
ing each one on a different random subset of the
set of labels (Tsoumakas et al., 2010).
</bodyText>
<subsubsectionHeader confidence="0.973007">
4.2.1 The Production Phase of RAkEL
</subsubsectionHeader>
<bodyText confidence="0.99364575">
The algorithm was implemented using the MU-
LAN Open Source Java library (Tsoumakas et
al., 2011), which is based on WEKA (Witten and
Frank, 2005). The algorithm works in two phases:
</bodyText>
<listItem confidence="0.925268571428571">
1. the production of an ensemble of LP algo-
rithms, and
2. the combination of the LP algorithms.
RAkEL takes as input the following parameters:
(1) the numbers of iterations m (which is devel-
oper specified and denotes the number of models
that the algorithm will produce), (2) the size of la-
</listItem>
<bodyText confidence="0.892768333333333">
belset k (which is also developer specified), (3) the
set of labels L, and (4) the training set D. During
the initial phase it outputs an ensemble of LP clas-
sifiers and the corresponding k-labelsets. A pseu-
docode for the production phase is shown below:
Algorithm 1 RAkEL production phase
</bodyText>
<listItem confidence="0.985799555555556">
1: Input: iterations m, k labelsets ,
labels L, training data D
2: for i =0 to m
3: Select random k−l a b e l s e t from L
4: Train an LP on D
5: Add LP to ensemble
6: end for
7: Output: the ensemble of LPs
with corresponding k−l a b e l s e t s
</listItem>
<subsubsectionHeader confidence="0.90458">
4.2.2 The Combination Phase
</subsubsectionHeader>
<bodyText confidence="0.999963642857143">
During the combination phase, the algorithm takes
as input the results of the production phase, i.e.
the ensemble of LPs with the corresponding k-
labelsets, the set of labels L, and the new instance
x and it outputs the result vector of predicted la-
bels for instance x. During run time, RAkEL es-
timates the average decision for each label in L
and if the average is greater than a threshold t (de-
termined by the developer) it includes the label in
the predicted labelset. We used the standard pa-
rameter values of t, k and m (t = 0.5, k = 3 and
m equals to 58 (2*29 templates)). In future, we
could perform parameter optimisation by using a
technique similar to (Gabsdil and Lemon, 2004).
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999935285714286">
Firstly, we performed a preliminary evaluation on
classification methods, comparing our proposed
ML classification with multiple iterated classifica-
tion approaches. The summaries generated by the
ML classification system are then compared with
the output of a RL system and two baseline sys-
tems in simulation and with real students.
</bodyText>
<subsectionHeader confidence="0.998313">
5.1 Comparison with Simple Classification
</subsectionHeader>
<bodyText confidence="0.999910619047619">
We compared the RAkEL algorithm with single-
label (SL) classification. Different SL classifiers
were trained using WEKA: JRip, Decision Trees,
Naive Bayes, k-nearest neighbour, logistic regres-
sion, multi-layer perceptron and support vector
machines. It was found out that Decision Trees
achieved on average 3% higher accuracy. We,
therefore, went on to use Decision Trees that use
generation history in three ways.
Firstly, for Decision Tree (no history), 29
decision-tree classifiers were trained, one for each
template. The input of these classifiers were the
9 factors and each classifier was trained in order
to decide whether to include a specific template or
not. This method did not take into account other
selected templates – it was only based on the time-
series data.
Secondly, for Decision Tree (with predicted
history), 29 classifiers were also trained, but this
time the input included the previous decisions
made by the previous classifiers (i.e. the history)
</bodyText>
<page confidence="0.977805">
1235
</page>
<bodyText confidence="0.99981802631579">
as well as the set of time-series data in order to
emulate the dependencies in the dataset. For in-
stance, classifier n was trained using the data from
the 9 factors and the template decisions for tem-
plates 0 to n − 1.
Thirdly, for Decision Tree (with real his-
tory), the real, expert values were used rather
than the predicted ones in the history. The
above-mentioned classifiers are compared with,
the Majority-class (single label) baseline, which
labels each instance with the most frequent tem-
plate.
The accuracy, the weighted precision, the
weighted recall, and the weighted F-score of the
classifiers are shown in Table 3. It was found that
in 10-fold cross validation RAkEL performs sig-
nificantly better in all these automatic measures
(accuracy = 76.95%, F-score = 85.50%). Remark-
ably, ML achieves more than 10% higher F-score
than the other methods (Table 3). The average
accuracy of the single-label classifiers is 75.95%
(10-fold validation), compared to 73.43% of clas-
sification with history. The reduced accuracy of
the classification with predicted history is due to
the error in the predicted values. In this method,
at every step, the predicted outcome was used in-
cluding the incorrect decisions that the classifier
made. The upper-bound accuracy is 78.09% cal-
culated by using the expert previous decisions and
not the potentially erroneous predicted decisions.
This result is indicative of the significance of the
relations between the factors showing that the pre-
dicted decisions are dependent due to existing cor-
relations as discussed in Section 1, therefore the
system should not take these decisions indepen-
dently. ML classification performs better because
it does take into account these correlations and de-
pendencies in the data.
</bodyText>
<subsectionHeader confidence="0.998602">
5.2 The Reinforcement Learning System
</subsectionHeader>
<bodyText confidence="0.999320904761905">
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns to
take optimal actions so as to maximise a cumu-
lative reward (Sutton and Barto, 1998). Content
selection is seen as a Markov Decision problem
and the goal of the agent is to learn to take the se-
quence of actions that leads to optimal content se-
lection. The Temporal Difference learning method
was used to train an agent for content selection.
Actions and States: The state consists of the
time-series data and the selected templates. In or-
der to explore the state space the agent selects a
factor (e.g. marks, deadlines etc.) and then decides
whether to talk about it or not.
Reward Function: The reward function reflects
the lecturers’ preferences on summaries and is
derived through linear regression analysis of a
dataset containing lecturer constructed summaries
and ratings of randomly generated summaries.
Specifically, it is the following cumulative multi-
variate function:
</bodyText>
<equation confidence="0.988105333333333">
n
Reward = a + bi * xi + c * length
i=1
</equation>
<bodyText confidence="0.996062444444444">
where X = {x1, x2, ..., xn} describes the com-
binations of the data trends observed in the time-
series data and a particular template. a, b and c are
the regression coefficients, and their values vary
from -99 to 221. The value of xi is given by the
function:
{ 1, the combination of a factor trend
and a template type is included
in a summary
0, if not.
The RL system differs from the classification
system in the way it performs content selection.
In the training phase, the agent selects a factor and
then decides whether to talk about it or not. If the
agent decides to refer to a factor, the template is
selected in a deterministic way, i.e. from the avail-
able templates it selects the template that results in
higher expected cumulative future reward.
</bodyText>
<subsectionHeader confidence="0.998092">
5.3 The Baseline Systems
</subsectionHeader>
<bodyText confidence="0.999248666666667">
We compared the ML system and the RL system
with two baselines described below by measuring
the accuracy of their outputs, the reward achieved
by the reward function used for the RL system,
and finally we also performed evaluation with stu-
dent users. In order to reduce the confounding
variables, we kept the ordering of content in all
systems the same, by adopting the ordering of the
rule-based system. The baselines are as follows:
</bodyText>
<listItem confidence="0.999692">
1. Rule-based System: generates summaries
based on Content Selection rules derived by work-
ing with a L&amp;T expert and a student (Gkatzia et
al., 2013).
2. Random System: initially, selects a factor
randomly and then selects a template randomly,
until it makes decisions for all factors.
</listItem>
<equation confidence="0.693199">
xi =
</equation>
<page confidence="0.943813">
1236
</page>
<table confidence="0.999466">
Time-Series Accuracy Reward Rating Mode (mean) Data Source
Summarisation Systems
Multi-label Classification 85% 65.4 7 (6.24) Lecturers’ constructed summaries
Reinforcement Learning **66% 243.82 8 (6.54) Lecturers’ ratings &amp; summaries
Rule-based **65% 107.77 7, 8 (5.86) L&amp;T expert
Random **45.2% 43.29 *2 (*4.37) Random
</table>
<tableCaption confidence="0.949550666666667">
Table 4: Accuracy, average rewards (based on lecturers’ preferences) and averages of the means of the
student ratings. Accuracy significance (Z-test) with RAkEL at p&lt;0.05 is indicated as * and at p&lt;0.01
as **. Student ratings significance (Mann Whitney U test) with RAkEL at p&lt;0.05 is indicated as *.
</tableCaption>
<sectionHeader confidence="0.999337" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9964625">
Each of the four systems described above gener-
ated 26 feedback summaries corresponding to the
26 student profiles. These summaries were evalu-
ated in simulation and with real student users.
</bodyText>
<subsectionHeader confidence="0.947521">
6.1 Results in Simulation
</subsectionHeader>
<bodyText confidence="0.999990551020408">
Table 4 presents the accuracy, reward, and mode
of student rating of each algorithm when used to
generate the 26 summaries. Accuracy was esti-
mated as the proportion of the correctly classified
templates to the population of templates. In or-
der to have a more objective view on the results,
the score achieved by each algorithm using the
reward function was also calculated. ML clas-
sification achieved significantly higher accuracy,
which was expected as it is a supervised learning
method. The rule-based system and the RL sys-
tem have lower accuracy compared to the ML sys-
tem. There is evidently a mismatch between the
rules and the test-set; the content selection rules
are based on heuristics provided by a L&amp;T Expert
rather than by the same pool of lecturers that cre-
ated the test-set. On the contrary, the RL is trained
to optimise the selected content and not to repli-
cate the existing lecturer summaries, hence there
is a difference in accuracy.
Accuracy measures how similar the generated
output is to the gold standard, whereas the reward
function calculates a score regarding how good
the output is, given an objective function. RL is
trained to optimise for this function, and therefore
it achieves higher reward, whereas ML is trained
to learn by examples, therefore it produces out-
put closer to the gold standard (lecturer’s produced
summaries). RL uses exploration and exploitation
to discover combinations of content that result in
higher reward. The reward represents predicted
ratings that lecturers would give to the summary.
The reward for the lecturers’ produced summaries
is 124.62 and for the ML method is 107.77. The
ML classification system performed worse than
this gold standard in terms of reward, which is ex-
pected given the error in predictions (supervised
methods learn to reproduce the gold standard).
Moreover, each decision is rewarded with a dif-
ferent value as some combinations of factors and
templates have greater or negative regression coef-
ficients. For instance, the combination of the fac-
tors “deadlines” and the template that corresponds
to &lt;weeks&gt; is rewarded with 57. On the other
hand, when mentioning the &lt;average&gt; difficulty
the summary is “punished” with -81 (see descrip-
tion of the reward function in Section 5.2). Conse-
quently, a single poor decision in the ML classifi-
cation can result in much less reward.
</bodyText>
<subsectionHeader confidence="0.99908">
6.2 Subjective Results with Students
</subsectionHeader>
<bodyText confidence="0.999969391304348">
37 first year computer science students partici-
pated in the study. Each participant was shown
a graphical representation of the time-series data
of one student and four different summaries gen-
erated by the four systems (see Figure 1). The or-
der of the presented summaries was randomised.
They were asked to rate each feedback summary
on a 10-point rating scale in response to the fol-
lowing statement: “Imagine you are the following
student. How would you evaluate the following
feedback summaries from 1 to 10?”, where 10 cor-
responds to the most preferred summary and 1 to
the least preferred.
The difference in ratings between the ML clas-
sification system, the RL system and the Rule-
based system is not significant (see Mode (mean)
in Table 4, p&gt;0.05). However, there is a trend to-
wards the RL system. The classification method
reduces the generation steps, by making the de-
cision of the factor selection and the template se-
lection jointly. Moreover, the training time for the
classification method is faster (a couple of seconds
compared to over an hour). Finally, the student
</bodyText>
<page confidence="0.994757">
1237
</page>
<figureCaption confidence="0.995244">
Figure 1: The Figure show the evaluation setup. Students were presenting with the data in a graphical
</figureCaption>
<bodyText confidence="0.70065725">
way and then they were asked to evaluate each summary in a 10-point Rating scale. Summaries displayed
from left to right: ML system, RL, rule-based and random.
significantly prefer all the systems over the ran-
dom.
</bodyText>
<sectionHeader confidence="0.992046" genericHeader="evaluation">
7 Summary
</sectionHeader>
<bodyText confidence="0.99997315">
We have shown that ML classification for sum-
marisation of our time-series data has an accuracy
of 76.95% and that this approach significantly out-
performs other classification methods as it is able
to capture dependencies in the data when mak-
ing content selection decisions. ML classifica-
tion was also directly compared to a RL method.
It was found that although ML classification is
almost 20% more accurate than RL, both meth-
ods perform comparably when rated by humans.
This may be due to the fact that the RL optimi-
sation method is able to provide more varied re-
sponses over time rather than just emulating the
training data as with standard supervised learn-
ing approaches. Foster (2008) found similar re-
sults when performing a study on generation of
emphatic facial displays. A previous study by
Belz and Reiter (2006) has demonstrated that au-
tomatic metrics can correlate highly with human
ratings if the training dataset is of high quality.
In our study, the human ratings correlate well to
the average scores achieved by the reward func-
tion. However, the human ratings do not correlate
well to the accuracy scores. It is interesting that
the two methods that score differently on various
automatic metrics, such as accuracy, reward, pre-
cision, recall and F-score, are evaluated similarly
by users.
The comparison shows that each method can
serve different goals. Multi-label classification
generates output closer to gold standard whereas
RL can optimise the output according to a reward
function. ML classification could be used when
the goal of the generation is to replicate phenom-
ena seen in the dataset, because it achieves high
accuracy, precision and recall. However, opti-
misation methods can be more flexible, provide
more varied output and can be trained for different
goals, e.g. for capturing preferences of different
users.
</bodyText>
<page confidence="0.989735">
1238
</page>
<sectionHeader confidence="0.998468" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999994666666667">
For this initial experiment, we evaluated with stu-
dents and not with lecturers, since the students are
the recipients of feedback. In future, we plan to
evaluate with students’ own data under real cir-
cumstances as well as with ratings from lecturers.
Moreover, we plan to utilise the results from this
student evaluation in order to train an optimisation
algorithm to perform summarisation according to
students’ preferences. In this case, optimisation
would be the preferred method as it would not be
appropriate to collect gold standard data from stu-
dents. In fact, it would be of interest to investi-
gate multi-objective optimisation techniques that
can balance the needs of the lecturers to convey
important content to the satisfaction of students.
</bodyText>
<sectionHeader confidence="0.997543" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997255">
The research leading to this work has re-
ceived funding from the EC’s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
</bodyText>
<sectionHeader confidence="0.997973" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998908292682927">
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):261–71.
Ion Androutsopoulos, Gerasimos Lampouras, and
Dimitrios Galanis. 2013. Generating natural lan-
guage descriptions from owl ontologies: the nat-
ural owl system. Atrificial Intelligence Research,
48:671–715.
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Regina Barzilay and Mirella Lapata. 2004. Collec-
tive content selection for concept-to-text generation.
In Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT-NAACL).
Anja Belz and Eric Kow. 2010. Extracting parallel
fragments from comparable corpora for data-to-text
generation. In 6th International Natural Language
Generation Conference (INLG).
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of nlg systems. In 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (ACL).
Rolf Black, Joe Reddington, Ehud Reiter, Nava
Tintarev, and Annalu Waller. 2010. Using NLG and
sensors to support personal narrative for children
with complex communication needs. In NAACL
HLT 2010 Workshop on Speech and Language Pro-
cessing for Assistive Technologies.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning:
an exploratory look into the role of affect in learn-
ing with autotutor. Journal of Educational Media,
29:241–250.
Pable Duboue and K.R. McKeown. 2003. Statistical
acquisition of content selection rules for natural lan-
guage generation. In Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing (EMNLP).
Mary Ellen Foster. 2008. Automated metrics that
agree with human judgements on generated output
for an embodied conversational agent. In 5th Inter-
national Natural Language Generation Conference
(INLG).
Malte Gabsdil and Oliver Lemon. 2004. Combining
acoustic and pragmatic features to predict recogni-
tion performance in spoken dialogue systems. In
42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
AI Communications, 22: 153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation (ENLG).
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.
2014. Finding Middle Ground? Multi-objective
Natural Language Generation from time-series data.
In 14th Conference of the European Chapter of the
Association for Computational Linguistics (EACL)
(to appear).
Sebastian Gross, Bassam Mokbel, Barbara Hammer,
and Niels Pinkwart. 2012. Feedback provision
strategies in intelligent tutoring systems based on
clustered solution spaces. In J. Desel, J. M. Haake,
and C. Spannagel, editors, Tagungsband der 10. e-
Learning Fachtagung Informatik (DeLFI), number
P-207 in GI Lecture Notes in Informatics, pages 27–
38. GI.
</reference>
<page confidence="0.861321">
1239
</page>
<reference confidence="0.9996231375">
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. Bt-nurse:
Computer generation of natural language shift sum-
maries from complex heterogeneous medical data.
American Medical Informatics Association, 18:621-
624.
Nicholas Johnson and David Lane. 2011. Narrative
monologue as a first step towards advanced mis-
sion debrief for AUV operator situational aware-
ness. In 15th International Conference on Advanced
Robotics.
Ravi Kondadadi, Blake Howald, and Frank Schilder.
2013. A statistical nlg framework for aggregated
planning and realization. In 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Gerasimos Lampouras and Ion Androutsopoulos.
2013. Using integer linear programming in concept-
to-text generation to produce more compact texts. In
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Anna S. Law, Yvonne Freer, Jim Hunter, Robert H.
Logie, Neil McIntosh, and John Quinn. 2005. A
comparison of graphical and textual presentations of
time series data to support medical decision making
in the neonatal intensive care unit. Journal of Clini-
cal Monitoring and Computing, pages 19: 183–194.
Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, and
Saso Dzeroski. 2012. An extensive experimen-
tal comparison of methods for multi-label learning.
Pattern Recognition, 45(9):3084–3104.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. Journal of Cognition
and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natu-
ral language generation systems. Cambridge Uni-
versity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Generating english summaries of time se-
ries data using the gricean maxims. In 9th ACM in-
ternational conference on Knowledge discovery and
data mining (SIGKDD).
Somayajulu Sripada, Ehud Reiter, I Davy, and
K Nilssen. 2004. Lessons from deploying NLG
technology for marine weather forecast text gener-
ation. In PAIS session of ECAI-2004:760-764.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. MIT Press.
Grigorios Tsoumakas and Ioannis Katakis. 2007.
Multi-label classification: An overview. Inter-
national Journal Data Warehousing and Mining,
3(3):1–13.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis
Vlahavas. 2010. Random k-labelsets for multi-
label classification. IEEE Transactions on Knowl-
edge and Data Engineering, 99(1):1079–1089.
Grigorios Tsoumakas, Eleftherios Spyromitros-
Xioufis, Josef Vilcek, and Ioannis Vlahavas.
2011. Mulan: A java library for multi-label
learning. Journal of Machine Learning Research,
12(1):2411–2414.
Marian van den Meulen, Robert Logie, Yvonne Freer,
Cindy Sykes, Neil McIntosh, and Jim Hunter. 2010.
When a graph is poorer than 100 words: A com-
parison of computerised natural language genera-
tion, human generated descriptions and graphical
displays in neonatal intensive care. In Applied Cog-
nitive Psychology, 24: 77-89.
Ian Witten and Eibe Frank. 2005. Data mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann Publishers.
Min-Ling Zhang and Zhi-Hua Zhou. 2007. Ml-knn: A
lazy learning approach to multi-label learning. Pat-
tern Recognition, 40(7):2038–2048.
</reference>
<page confidence="0.987421">
1240
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.943839">
<title confidence="0.9985">Comparing Multi-label Classification with Reinforcement Learning Summarisation of Time-series Data</title>
<author confidence="0.988759">Dimitra Gkatzia</author>
<author confidence="0.988759">Helen Hastie</author>
<author confidence="0.988759">Oliver Lemon</author>
<affiliation confidence="0.999993">School of Mathematical and Computer Sciences, Heriot-Watt University,</affiliation>
<email confidence="0.965507">h.hastie,</email>
<abstract confidence="0.999551791666667">We present a novel approach for automatic report generation from time-series data, in the context of student feedback generation. Our proposed methodology treats content selection as a multi-label (ML) classification problem, which takes as input time-series data and outputs a set of templates, while capturing the dependencies between selected templates. We show that this method generates output closer to the feedback that lecturers actually generated, achieving 3.5% higher accuracy and 15% higher F-score than multiple simple classifiers that keep a history of selected templates. Furthermore, we compare a ML classifier with a Reinforcement Learning (RL) approach in simulation and using ratings from real student users. We show that the different methods have different benefits, with ML being more accurate for predicting what was seen in the training data, whereas RL is more exploratory and slightly preferred by the students.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carole Ames</author>
</authors>
<title>Classrooms: Goals, structures, and student motivation.</title>
<date>1992</date>
<journal>Journal of Educational Psychology,</journal>
<volume>84</volume>
<issue>3</issue>
<contexts>
<context position="1721" citStr="Ames, 1992" startWordPosition="253" endWordPosition="254">t was seen in the training data, whereas RL is more exploratory and slightly preferred by the students. 1 Introduction Summarisation of time-series data refers to the task of automatically generating text from variables whose values change over time. We consider the task of automatically generating feedback summaries for students describing their performance during the lab of a Computer Science module over the semester. Students’ learning can be influenced by many variables, such as difficulty of the material (Person et al., 1995), other deadlines (Craig et al., 2004), attendance in lectures (Ames, 1992), etc. These variables have two important qualities. Firstly, they change over time, and secondly they can be dependent on or independent of each other. Therefore, when generating feedback, we need to take into account all variables simultaneously in order to capture potential dependencies and provide more effective and useful feedback that is relevant to the students. In this work, we concentrate on content selection which is the task of choosing what to say, i.e. what information is to be included in a report (Reiter and Dale, 2000). Content selection decisions based on trends in time-series</context>
</contexts>
<marker>Ames, 1992</marker>
<rawString>Carole Ames. 1992. Classrooms: Goals, structures, and student motivation. Journal of Educational Psychology, 84(3):261–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Gerasimos Lampouras</author>
<author>Dimitrios Galanis</author>
</authors>
<title>Generating natural language descriptions from owl ontologies: the natural owl system.</title>
<date>2013</date>
<journal>Atrificial Intelligence Research,</journal>
<pages>48--671</pages>
<contexts>
<context position="7645" citStr="Androutsopoulos et al., 2013" startWordPosition="1210" endWordPosition="1213">. The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (wh</context>
</contexts>
<marker>Androutsopoulos, Lampouras, Galanis, 2013</marker>
<rawString>Ion Androutsopoulos, Gerasimos Lampouras, and Dimitrios Galanis. 2013. Generating natural language descriptions from owl ontologies: the natural owl system. Atrificial Intelligence Research, 48:671–715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A simple domain-independent probabilistic approach to generation.</title>
<date>2010</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="6716" citStr="Angeli et al., 2010" startWordPosition="1079" endWordPosition="1082">s. In the next section, we refer to the related work on Natural Language Generation from time-series data and on Content Selection. In Section 4.2, we describe our approach and we carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include</context>
</contexts>
<marker>Angeli, Liang, Klein, 2010</marker>
<rawString>Gabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Collective content selection for concept-to-text generation.</title>
<date>2004</date>
<booktitle>In Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP).</booktitle>
<contexts>
<context position="7577" citStr="Barzilay and Lapata, 2004" startWordPosition="1200" endWordPosition="1203"> Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content select</context>
</contexts>
<marker>Barzilay, Lapata, 2004</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2004. Collective content selection for concept-to-text generation. In Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="7895" citStr="Barzilay and Lee, 2004" startWordPosition="1248" endWordPosition="1251">evious methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boosting algorithm) and the identification of links between the entities with similar labels. In cont</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>Extracting parallel fragments from comparable corpora for data-to-text generation.</title>
<date>2010</date>
<booktitle>In 6th International Natural Language Generation Conference (INLG).</booktitle>
<contexts>
<context position="6695" citStr="Belz and Kow, 2010" startWordPosition="1075" endWordPosition="1078">tween the two methods. In the next section, we refer to the related work on Natural Language Generation from time-series data and on Content Selection. In Section 4.2, we describe our approach and we carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for cont</context>
</contexts>
<marker>Belz, Kow, 2010</marker>
<rawString>Anja Belz and Eric Kow. 2010. Extracting parallel fragments from comparable corpora for data-to-text generation. In 6th International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Ehud Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of nlg systems.</title>
<date>2006</date>
<booktitle>In 11th Conference of the European Chapter of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="30556" citStr="Belz and Reiter (2006)" startWordPosition="5000" endWordPosition="5003"> capture dependencies in the data when making content selection decisions. ML classification was also directly compared to a RL method. It was found that although ML classification is almost 20% more accurate than RL, both methods perform comparably when rated by humans. This may be due to the fact that the RL optimisation method is able to provide more varied responses over time rather than just emulating the training data as with standard supervised learning approaches. Foster (2008) found similar results when performing a study on generation of emphatic facial displays. A previous study by Belz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. In our study, the human ratings correlate well to the average scores achieved by the reward function. However, the human ratings do not correlate well to the accuracy scores. It is interesting that the two methods that score differently on various automatic metrics, such as accuracy, reward, precision, recall and F-score, are evaluated similarly by users. The comparison shows that each method can serve different goals. Multi-label classification generates output closer t</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of nlg systems. In 11th Conference of the European Chapter of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rolf Black</author>
<author>Joe Reddington</author>
<author>Ehud Reiter</author>
<author>Nava Tintarev</author>
<author>Annalu Waller</author>
</authors>
<title>Using NLG and sensors to support personal narrative for children with complex communication needs.</title>
<date>2010</date>
<booktitle>In NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies.</booktitle>
<contexts>
<context position="6894" citStr="Black et al., 2010" startWordPosition="1107" endWordPosition="1110">e carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras a</context>
</contexts>
<marker>Black, Reddington, Reiter, Tintarev, Waller, 2010</marker>
<rawString>Rolf Black, Joe Reddington, Ehud Reiter, Nava Tintarev, and Annalu Waller. 2010. Using NLG and sensors to support personal narrative for children with complex communication needs. In NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scotty D Craig</author>
<author>Arthur C Graesser</author>
<author>Jeremiah Sullins</author>
<author>Barry Gholson</author>
</authors>
<title>Affect and learning: an exploratory look into the role of affect in learning with autotutor.</title>
<date>2004</date>
<journal>Journal of Educational Media,</journal>
<pages>29--241</pages>
<contexts>
<context position="1684" citStr="Craig et al., 2004" startWordPosition="246" endWordPosition="249">ith ML being more accurate for predicting what was seen in the training data, whereas RL is more exploratory and slightly preferred by the students. 1 Introduction Summarisation of time-series data refers to the task of automatically generating text from variables whose values change over time. We consider the task of automatically generating feedback summaries for students describing their performance during the lab of a Computer Science module over the semester. Students’ learning can be influenced by many variables, such as difficulty of the material (Person et al., 1995), other deadlines (Craig et al., 2004), attendance in lectures (Ames, 1992), etc. These variables have two important qualities. Firstly, they change over time, and secondly they can be dependent on or independent of each other. Therefore, when generating feedback, we need to take into account all variables simultaneously in order to capture potential dependencies and provide more effective and useful feedback that is relevant to the students. In this work, we concentrate on content selection which is the task of choosing what to say, i.e. what information is to be included in a report (Reiter and Dale, 2000). Content selection dec</context>
</contexts>
<marker>Craig, Graesser, Sullins, Gholson, 2004</marker>
<rawString>Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins, and Barry Gholson. 2004. Affect and learning: an exploratory look into the role of affect in learning with autotutor. Journal of Educational Media, 29:241–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pable Duboue</author>
<author>K R McKeown</author>
</authors>
<title>Statistical acquisition of content selection rules for natural language generation.</title>
<date>2003</date>
<booktitle>In Conference on Human Language Technology and Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="7798" citStr="Duboue and McKeown, 2003" startWordPosition="1233" endWordPosition="1236">entation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boos</context>
</contexts>
<marker>Duboue, McKeown, 2003</marker>
<rawString>Pable Duboue and K.R. McKeown. 2003. Statistical acquisition of content selection rules for natural language generation. In Conference on Human Language Technology and Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Ellen Foster</author>
</authors>
<title>Automated metrics that agree with human judgements on generated output for an embodied conversational agent.</title>
<date>2008</date>
<booktitle>In 5th International Natural Language Generation Conference (INLG).</booktitle>
<contexts>
<context position="30424" citStr="Foster (2008)" startWordPosition="4980" endWordPosition="4981">ta has an accuracy of 76.95% and that this approach significantly outperforms other classification methods as it is able to capture dependencies in the data when making content selection decisions. ML classification was also directly compared to a RL method. It was found that although ML classification is almost 20% more accurate than RL, both methods perform comparably when rated by humans. This may be due to the fact that the RL optimisation method is able to provide more varied responses over time rather than just emulating the training data as with standard supervised learning approaches. Foster (2008) found similar results when performing a study on generation of emphatic facial displays. A previous study by Belz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. In our study, the human ratings correlate well to the average scores achieved by the reward function. However, the human ratings do not correlate well to the accuracy scores. It is interesting that the two methods that score differently on various automatic metrics, such as accuracy, reward, precision, recall and F-score, are evaluated simil</context>
</contexts>
<marker>Foster, 2008</marker>
<rawString>Mary Ellen Foster. 2008. Automated metrics that agree with human judgements on generated output for an embodied conversational agent. In 5th International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Gabsdil</author>
<author>Oliver Lemon</author>
</authors>
<title>Combining acoustic and pragmatic features to predict recognition performance in spoken dialogue systems.</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="19404" citStr="Gabsdil and Lemon, 2004" startWordPosition="3173" endWordPosition="3176">e results of the production phase, i.e. the ensemble of LPs with the corresponding klabelsets, the set of labels L, and the new instance x and it outputs the result vector of predicted labels for instance x. During run time, RAkEL estimates the average decision for each label in L and if the average is greater than a threshold t (determined by the developer) it includes the label in the predicted labelset. We used the standard parameter values of t, k and m (t = 0.5, k = 3 and m equals to 58 (2*29 templates)). In future, we could perform parameter optimisation by using a technique similar to (Gabsdil and Lemon, 2004). 5 Evaluation Firstly, we performed a preliminary evaluation on classification methods, comparing our proposed ML classification with multiple iterated classification approaches. The summaries generated by the ML classification system are then compared with the output of a RL system and two baseline systems in simulation and with real students. 5.1 Comparison with Simple Classification We compared the RAkEL algorithm with singlelabel (SL) classification. Different SL classifiers were trained using WEKA: JRip, Decision Trees, Naive Bayes, k-nearest neighbour, logistic regression, multi-layer p</context>
</contexts>
<marker>Gabsdil, Lemon, 2004</marker>
<rawString>Malte Gabsdil and Oliver Lemon. 2004. Combining acoustic and pragmatic features to predict recognition performance in spoken dialogue systems. In 42nd Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Francois Portet</author>
<author>Ehud Reiter</author>
<author>James Hunter</author>
<author>Saad Mahamood</author>
<author>Wendy Moncur</author>
<author>Somayajulu Sripada</author>
</authors>
<title>From data to text in the neonatal intensive care unit: Using NLG technology for decision support and information management.</title>
<date>2009</date>
<journal>AI Communications,</journal>
<volume>22</volume>
<pages>153--186</pages>
<contexts>
<context position="6818" citStr="Gatt et al., 2009" startWordPosition="1096" endWordPosition="1099">ta and on Content Selection. In Section 4.2, we describe our approach and we carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gr</context>
</contexts>
<marker>Gatt, Portet, Reiter, Hunter, Mahamood, Moncur, Sripada, 2009</marker>
<rawString>Albert Gatt, Francois Portet, Ehud Reiter, James Hunter, Saad Mahamood, Wendy Moncur, and Somayajulu Sripada. 2009. From data to text in the neonatal intensive care unit: Using NLG technology for decision support and information management. AI Communications, 22: 153-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitra Gkatzia</author>
<author>Helen Hastie</author>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>Generating student feedback from time-series data using Reinforcement Learning.</title>
<date>2013</date>
<booktitle>In 14th European Workshop in Natural Language Generation (ENLG).</booktitle>
<contexts>
<context position="11888" citStr="Gkatzia et al., 2013" startWordPosition="1890" endWordPosition="1893"> describe these factors in four different ways: 1. &lt;trend&gt;: referring to the trend of a factor over the semester (e.g. “Your performance was increasing...”), 2. &lt;weeks&gt;: explicitly describing the factor value at specific weeks (e.g. “In weeks 2, 3 and 9...”), 3. &lt;average&gt;: considering the average of a factor value (e.g. “You dedicated 1.5 hours studying on average...”), and 4. &lt;other&gt;: mentioning other relevant information (e.g. “Revising material will improve your performance”). For the corpus creation, 11 lecturers selected the content to be conveyed in a summary, given the set of raw data (Gkatzia et al., 2013). As a result, for the same student there are various summaries provided by the different experts. This characteristic of the dataset, that each instance is associated with more than one solution, additionally motivates the use of multi-label classification, which is concerned with learning from examples, where each example is associated with multiple labels. Our analysis of the dataset showed that there are significant correlations between the factors, for example, the number of lectures attended (LA) correlates with the student’s understanding of the material (Und), see Table 2. As we will d</context>
<context position="13661" citStr="Gkatzia et al., 2013" startWordPosition="2187" endWordPosition="2190">L -0.31 -0.11 0.03 0.16 1* 0.26 0.24 -0.44* 0.14 (6) HI -0.30 -0.11 -0.26 -0.06 0.26 1* 0.27 -0.50* 0.15 (7) PI -0.36* -0.29 0.12 0.03 0.24 0.27 1* -0.46* 0.34* (8) LA 0.44* 0.32 0.60* -0.19 -0.44* -0.50* -0.46* 1* -0.12 (9) R 0.16 0.47* 0.03 0.14 0.14 0.15 0.34* -0.12 1* Table 2: The table presents the Pearson’s correlation coefficients of the factors (* means p&lt;0.05). 4 Methodology In this section, the content selection task and the suggested multi-label classification approach are presented. The development and evaluation of the time-series generation system follows the following pipeline (Gkatzia et al., 2013): 1. Time-Series data collection from students 2. Template construction by Learning and Teaching (L&amp;T) expert 3. Feedback summaries constructed by lecturers; random summaries rated by lecturers 4. Development of time-series generation systems (Section 4.2, Section 5.3): ML system, RL system, Rule-based and Random system 5. Evaluation: (Section 5) - Offline evaluation (Accuracy and Reward) - Online evaluation (Subjective Ratings) 4.1 The Content Selection Task Our learning task is formed as follows: given a set of 9 time-series factors, select the content that is most appropriate to be included</context>
<context position="24938" citStr="Gkatzia et al., 2013" startWordPosition="4080" endWordPosition="4083">ture reward. 5.3 The Baseline Systems We compared the ML system and the RL system with two baselines described below by measuring the accuracy of their outputs, the reward achieved by the reward function used for the RL system, and finally we also performed evaluation with student users. In order to reduce the confounding variables, we kept the ordering of content in all systems the same, by adopting the ordering of the rule-based system. The baselines are as follows: 1. Rule-based System: generates summaries based on Content Selection rules derived by working with a L&amp;T expert and a student (Gkatzia et al., 2013). 2. Random System: initially, selects a factor randomly and then selects a template randomly, until it makes decisions for all factors. xi = 1236 Time-Series Accuracy Reward Rating Mode (mean) Data Source Summarisation Systems Multi-label Classification 85% 65.4 7 (6.24) Lecturers’ constructed summaries Reinforcement Learning **66% 243.82 8 (6.54) Lecturers’ ratings &amp; summaries Rule-based **65% 107.77 7, 8 (5.86) L&amp;T expert Random **45.2% 43.29 *2 (*4.37) Random Table 4: Accuracy, average rewards (based on lecturers’ preferences) and averages of the means of the student ratings. Accuracy sign</context>
</contexts>
<marker>Gkatzia, Hastie, Janarthanam, Lemon, 2013</marker>
<rawString>Dimitra Gkatzia, Helen Hastie, Srinivasan Janarthanam, and Oliver Lemon. 2013. Generating student feedback from time-series data using Reinforcement Learning. In 14th European Workshop in Natural Language Generation (ENLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitra Gkatzia</author>
<author>Helen Hastie</author>
<author>Oliver Lemon</author>
</authors>
<title>Finding Middle Ground? Multi-objective Natural Language Generation from time-series data.</title>
<date>2014</date>
<booktitle>In 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</booktitle>
<note>(to appear).</note>
<contexts>
<context position="7414" citStr="Gkatzia et al., 2014" startWordPosition="1179" endWordPosition="1182"> 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classifica</context>
</contexts>
<marker>Gkatzia, Hastie, Lemon, 2014</marker>
<rawString>Dimitra Gkatzia, Helen Hastie, and Oliver Lemon. 2014. Finding Middle Ground? Multi-objective Natural Language Generation from time-series data. In 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL) (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Gross</author>
<author>Bassam Mokbel</author>
<author>Barbara Hammer</author>
<author>Niels Pinkwart</author>
</authors>
<title>Feedback provision strategies in intelligent tutoring systems based on clustered solution spaces. In</title>
<date>2012</date>
<booktitle>Tagungsband der 10. eLearning Fachtagung Informatik (DeLFI), number P-207 in GI Lecture Notes in Informatics,</booktitle>
<pages>27--38</pages>
<editor>J. Desel, J. M. Haake, and C. Spannagel, editors,</editor>
<publisher>GI.</publisher>
<contexts>
<context position="10423" citStr="Gross et al., 2012" startWordPosition="1648" endWordPosition="1651">soumakas et al., 2010): an ensemble problem transformation method, which constructs an ensemble of simple-label classifiers, where each one deals with a random subset of the labels. Finally, our domain for feedback generation is motivated by previous studies (Law et al., 2005; van den Meulen et al., 2010) who show that text summaries are more effective in decision making than graphs therefore it is advantageous to provide a summary over showing users the raw data graphically. In addition, feedback summarisation from time-series data can be applied to the field of Intelligent Tutoring Systems (Gross et al., 2012). 3 Data The dataset consists of 37 instances referring to the activities of 26 students. For a few students there is more than 1 instance. An example of one such instance is presented in Table 1. Each instance includes time-series information about the student’s learning habits and the selected templates that lecturers used to provide feedback to this student. The time-series information includes for each week of the semester: (1) the marks achieved at the lab; (2) the hours that the student spent studying; (3) the understandability of the material; (4) the difficulty of the lab exercises as </context>
</contexts>
<marker>Gross, Mokbel, Hammer, Pinkwart, 2012</marker>
<rawString>Sebastian Gross, Bassam Mokbel, Barbara Hammer, and Niels Pinkwart. 2012. Feedback provision strategies in intelligent tutoring systems based on clustered solution spaces. In J. Desel, J. M. Haake, and C. Spannagel, editors, Tagungsband der 10. eLearning Fachtagung Informatik (DeLFI), number P-207 in GI Lecture Notes in Informatics, pages 27– 38. GI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Hunter</author>
<author>Yvonne Freer</author>
<author>Albert Gatt</author>
<author>Yaji Sripada</author>
<author>Cindy Sykes</author>
<author>D Westwater</author>
</authors>
<title>Bt-nurse: Computer generation of natural language shift summaries from complex heterogeneous medical data.</title>
<date>2011</date>
<journal>American Medical Informatics Association,</journal>
<pages>18--621</pages>
<contexts>
<context position="6798" citStr="Hunter et al., 2011" startWordPosition="1092" endWordPosition="1095">n from time-series data and on Content Selection. In Section 4.2, we describe our approach and we carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatz</context>
</contexts>
<marker>Hunter, Freer, Gatt, Sripada, Sykes, Westwater, 2011</marker>
<rawString>Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada, Cindy Sykes, and D Westwater. 2011. Bt-nurse: Computer generation of natural language shift summaries from complex heterogeneous medical data. American Medical Informatics Association, 18:621-624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Johnson</author>
<author>David Lane</author>
</authors>
<title>Narrative monologue as a first step towards advanced mission debrief for AUV operator situational awareness.</title>
<date>2011</date>
<booktitle>In 15th International Conference on Advanced Robotics.</booktitle>
<contexts>
<context position="7016" citStr="Johnson and Lane, 2011" startWordPosition="1123" endWordPosition="1126">ion 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (</context>
</contexts>
<marker>Johnson, Lane, 2011</marker>
<rawString>Nicholas Johnson and David Lane. 2011. Narrative monologue as a first step towards advanced mission debrief for AUV operator situational awareness. In 15th International Conference on Advanced Robotics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Kondadadi</author>
<author>Blake Howald</author>
<author>Frank Schilder</author>
</authors>
<title>A statistical nlg framework for aggregated planning and realization.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7737" citStr="Kondadadi et al., 2013" startWordPosition="1224" endWordPosition="1227">), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on</context>
</contexts>
<marker>Kondadadi, Howald, Schilder, 2013</marker>
<rawString>Ravi Kondadadi, Blake Howald, and Frank Schilder. 2013. A statistical nlg framework for aggregated planning and realization. In 51st Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerasimos Lampouras</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Using integer linear programming in conceptto-text generation to produce more compact texts.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7519" citStr="Lampouras and Androutsopoulos, 2013" startWordPosition="1192" endWordPosition="1196"> al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the t</context>
</contexts>
<marker>Lampouras, Androutsopoulos, 2013</marker>
<rawString>Gerasimos Lampouras and Ion Androutsopoulos. 2013. Using integer linear programming in conceptto-text generation to produce more compact texts. In 51st Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna S Law</author>
<author>Yvonne Freer</author>
<author>Jim Hunter</author>
<author>Robert H Logie</author>
<author>Neil McIntosh</author>
<author>John Quinn</author>
</authors>
<title>A comparison of graphical and textual presentations of time series data to support medical decision making in the neonatal intensive care unit.</title>
<date>2005</date>
<journal>Journal of Clinical Monitoring and Computing,</journal>
<pages>19--183</pages>
<contexts>
<context position="10080" citStr="Law et al., 2005" startWordPosition="1591" endWordPosition="1594">makas and Katakis, 2007) transform the ML classification task into one or more simple classification tasks. Ensemble methods (Tsoumakas et al., 2010) are algorithms that use ensembles to perform ML learning and they are based on problem transformation or algorithm adaptation methods. In this paper, we applied RAkEL (Random k-labelsets) (Tsoumakas et al., 2010): an ensemble problem transformation method, which constructs an ensemble of simple-label classifiers, where each one deals with a random subset of the labels. Finally, our domain for feedback generation is motivated by previous studies (Law et al., 2005; van den Meulen et al., 2010) who show that text summaries are more effective in decision making than graphs therefore it is advantageous to provide a summary over showing users the raw data graphically. In addition, feedback summarisation from time-series data can be applied to the field of Intelligent Tutoring Systems (Gross et al., 2012). 3 Data The dataset consists of 37 instances referring to the activities of 26 students. For a few students there is more than 1 instance. An example of one such instance is presented in Table 1. Each instance includes time-series information about the stu</context>
</contexts>
<marker>Law, Freer, Hunter, Logie, McIntosh, Quinn, 2005</marker>
<rawString>Anna S. Law, Yvonne Freer, Jim Hunter, Robert H. Logie, Neil McIntosh, and John Quinn. 2005. A comparison of graphical and textual presentations of time series data to support medical decision making in the neonatal intensive care unit. Journal of Clinical Monitoring and Computing, pages 19: 183–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gjorgji Madjarov</author>
<author>Dragi Kocev</author>
<author>Dejan Gjorgjevikj</author>
<author>Saso Dzeroski</author>
</authors>
<title>An extensive experimental comparison of methods for multi-label learning.</title>
<date>2012</date>
<journal>Pattern Recognition,</journal>
<volume>45</volume>
<issue>9</issue>
<contexts>
<context position="8727" citStr="Madjarov et al., 2012" startWordPosition="1385" endWordPosition="1388"> between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boosting algorithm) and the identification of links between the entities with similar labels. In contrast, ML classification does not need the computation of links between the data and the templates. ML classification can also apply to other problems whose features are correlated, such as text classification (Madjarov et al., 2012), when an aligned dataset is provided. ML classification algorithms have been divided into three categories: algorithm adaptation methods, problem transformation and ensemble methods (Tsoumakas and Katakis, 2007; Madjarov et al., 2012). Algorithm adaptation approaches (Tsoumakas et al., 2010) extend simple classification methods to handle ML data. For example, the k-nearest neighbour algorithm is extended to ML-kNN by Zhang and Zhou (2007). MLkNN identifies for each new instance its k nearest neighbours in the training set and then it predicts the label set by utilising the maximum a posterior</context>
</contexts>
<marker>Madjarov, Kocev, Gjorgjevikj, Dzeroski, 2012</marker>
<rawString>Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, and Saso Dzeroski. 2012. An extensive experimental comparison of methods for multi-label learning. Pattern Recognition, 45(9):3084–3104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalie K Person</author>
<author>Roger J Kreuz</author>
<author>Rolf A Zwaan</author>
<author>Arthur C Graesser</author>
</authors>
<title>Pragmatics and pedagogy: Conversational rules and politeness strategies may inhibit effective tutoring.</title>
<date>1995</date>
<journal>Journal of Cognition and Instruction,</journal>
<pages>13--2</pages>
<contexts>
<context position="1646" citStr="Person et al., 1995" startWordPosition="239" endWordPosition="242">rent methods have different benefits, with ML being more accurate for predicting what was seen in the training data, whereas RL is more exploratory and slightly preferred by the students. 1 Introduction Summarisation of time-series data refers to the task of automatically generating text from variables whose values change over time. We consider the task of automatically generating feedback summaries for students describing their performance during the lab of a Computer Science module over the semester. Students’ learning can be influenced by many variables, such as difficulty of the material (Person et al., 1995), other deadlines (Craig et al., 2004), attendance in lectures (Ames, 1992), etc. These variables have two important qualities. Firstly, they change over time, and secondly they can be dependent on or independent of each other. Therefore, when generating feedback, we need to take into account all variables simultaneously in order to capture potential dependencies and provide more effective and useful feedback that is relevant to the students. In this work, we concentrate on content selection which is the task of choosing what to say, i.e. what information is to be included in a report (Reiter </context>
</contexts>
<marker>Person, Kreuz, Zwaan, Graesser, 1995</marker>
<rawString>Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and Arthur C. Graesser. 1995. Pragmatics and pedagogy: Conversational rules and politeness strategies may inhibit effective tutoring. Journal of Cognition and Instruction, 13(2):161-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building natural language generation systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2261" citStr="Reiter and Dale, 2000" startWordPosition="343" endWordPosition="346">, 1995), other deadlines (Craig et al., 2004), attendance in lectures (Ames, 1992), etc. These variables have two important qualities. Firstly, they change over time, and secondly they can be dependent on or independent of each other. Therefore, when generating feedback, we need to take into account all variables simultaneously in order to capture potential dependencies and provide more effective and useful feedback that is relevant to the students. In this work, we concentrate on content selection which is the task of choosing what to say, i.e. what information is to be included in a report (Reiter and Dale, 2000). Content selection decisions based on trends in time-series data determine the selection of the useful and important variables, which we refer to here as factors, that should be conveyed in a summary. The decisions of factor selection can be influenced by other factors that their values are correlated with; can be based on the appearance or absence of other factors in the summary; and can be based on the factors’ behaviour over time. Moreover, some factors may have to be discussed together in order to achieve some communicative goal, for instance, a teacher might want to refer to student’s ma</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
<author>Xingkun Liu</author>
</authors>
<title>Optimising information presentation for spoken dialogue systems.</title>
<date>2010</date>
<booktitle>In 48th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7361" citStr="Rieser et al., 2010" startWordPosition="1173" endWordPosition="1176">report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is simi</context>
</contexts>
<marker>Rieser, Lemon, Liu, 2010</marker>
<rawString>Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010. Optimising information presentation for spoken dialogue systems. In 48th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu Sripada</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>Generating english summaries of time series data using the gricean maxims.</title>
<date>2003</date>
<booktitle>In 9th ACM international conference on Knowledge discovery and data mining (SIGKDD).</booktitle>
<contexts>
<context position="7453" citStr="Sripada et al., 2003" startWordPosition="1185" endWordPosition="1188"> assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates f</context>
</contexts>
<marker>Sripada, Reiter, Hunter, Yu, 2003</marker>
<rawString>Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2003. Generating english summaries of time series data using the gricean maxims. In 9th ACM international conference on Knowledge discovery and data mining (SIGKDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu Sripada</author>
<author>Ehud Reiter</author>
<author>I Davy</author>
<author>K Nilssen</author>
</authors>
<title>Lessons from deploying NLG technology for marine weather forecast text generation.</title>
<date>2004</date>
<booktitle>In PAIS session of</booktitle>
<pages>2004--760</pages>
<contexts>
<context position="6739" citStr="Sripada et al., 2004" startWordPosition="1083" endWordPosition="1086">n, we refer to the related work on Natural Language Generation from time-series data and on Content Selection. In Section 4.2, we describe our approach and we carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning</context>
</contexts>
<marker>Sripada, Reiter, Davy, Nilssen, 2004</marker>
<rawString>Somayajulu Sripada, Ehud Reiter, I Davy, and K Nilssen. 2004. Lessons from deploying NLG technology for marine weather forecast text generation. In PAIS session of ECAI-2004:760-764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richart Sutton</author>
<author>Andrew Barto</author>
</authors>
<title>Reinforcement learning.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="22739" citStr="Sutton and Barto, 1998" startWordPosition="3698" endWordPosition="3701">erroneous predicted decisions. This result is indicative of the significance of the relations between the factors showing that the predicted decisions are dependent due to existing correlations as discussed in Section 1, therefore the system should not take these decisions independently. ML classification performs better because it does take into account these correlations and dependencies in the data. 5.2 The Reinforcement Learning System Reinforcement Learning (RL) is a machine learning technique that defines how an agent learns to take optimal actions so as to maximise a cumulative reward (Sutton and Barto, 1998). Content selection is seen as a Markov Decision problem and the goal of the agent is to learn to take the sequence of actions that leads to optimal content selection. The Temporal Difference learning method was used to train an agent for content selection. Actions and States: The state consists of the time-series data and the selected templates. In order to explore the state space the agent selects a factor (e.g. marks, deadlines etc.) and then decides whether to talk about it or not. Reward Function: The reward function reflects the lecturers’ preferences on summaries and is derived through </context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richart Sutton and Andrew Barto. 1998. Reinforcement learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigorios Tsoumakas</author>
<author>Ioannis Katakis</author>
</authors>
<title>Multi-label classification: An overview.</title>
<date>2007</date>
<journal>International Journal Data Warehousing and Mining,</journal>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="8938" citStr="Tsoumakas and Katakis, 2007" startWordPosition="1414" endWordPosition="1417">mitted, and it is based on the values of entity attributes and is computed using a boosting algorithm) and the identification of links between the entities with similar labels. In contrast, ML classification does not need the computation of links between the data and the templates. ML classification can also apply to other problems whose features are correlated, such as text classification (Madjarov et al., 2012), when an aligned dataset is provided. ML classification algorithms have been divided into three categories: algorithm adaptation methods, problem transformation and ensemble methods (Tsoumakas and Katakis, 2007; Madjarov et al., 2012). Algorithm adaptation approaches (Tsoumakas et al., 2010) extend simple classification methods to handle ML data. For example, the k-nearest neighbour algorithm is extended to ML-kNN by Zhang and Zhou (2007). MLkNN identifies for each new instance its k nearest neighbours in the training set and then it predicts the label set by utilising the maximum a posteriori principle according to statistical information derived from the label sets of the k neighbours. Problem transformation approaches (Tsoumakas and Katakis, 2007) transform the ML classification task into one or </context>
</contexts>
<marker>Tsoumakas, Katakis, 2007</marker>
<rawString>Grigorios Tsoumakas and Ioannis Katakis. 2007. Multi-label classification: An overview. International Journal Data Warehousing and Mining, 3(3):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigorios Tsoumakas</author>
</authors>
<title>Ioannis Katakis, and Ioannis Vlahavas.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>99</volume>
<issue>1</issue>
<marker>Tsoumakas, 2010</marker>
<rawString>Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. 2010. Random k-labelsets for multilabel classification. IEEE Transactions on Knowledge and Data Engineering, 99(1):1079–1089.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigorios Tsoumakas</author>
</authors>
<title>Eleftherios SpyromitrosXioufis, Josef Vilcek, and Ioannis Vlahavas.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<issue>1</issue>
<marker>Tsoumakas, 2011</marker>
<rawString>Grigorios Tsoumakas, Eleftherios SpyromitrosXioufis, Josef Vilcek, and Ioannis Vlahavas. 2011. Mulan: A java library for multi-label learning. Journal of Machine Learning Research, 12(1):2411–2414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marian van den Meulen</author>
<author>Robert Logie</author>
<author>Yvonne Freer</author>
<author>Cindy Sykes</author>
<author>Neil McIntosh</author>
<author>Jim Hunter</author>
</authors>
<title>When a graph is poorer than 100 words: A comparison of computerised natural language generation, human generated descriptions and graphical displays in neonatal intensive care.</title>
<date>2010</date>
<journal>In Applied Cognitive Psychology,</journal>
<volume>24</volume>
<pages>77--89</pages>
<marker>van den Meulen, Logie, Freer, Sykes, McIntosh, Hunter, 2010</marker>
<rawString>Marian van den Meulen, Robert Logie, Yvonne Freer, Cindy Sykes, Neil McIntosh, and Jim Hunter. 2010. When a graph is poorer than 100 words: A comparison of computerised natural language generation, human generated descriptions and graphical displays in neonatal intensive care. In Applied Cognitive Psychology, 24: 77-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="17829" citStr="Witten and Frank, 2005" startWordPosition="2872" endWordPosition="2875">L (multi-label) (no history) 76.95% 85.08 85.94 85.50 Table 3: Average, precision, recall and F-score of the different classification methods (T-test, * denotes significance with p&lt;0.05 and ** significance with p&lt;0.01, when comparing each result to RAkEL). et al., 2010). RAkEL tackles this problem by constructing an ensemble of LP classifiers and training each one on a different random subset of the set of labels (Tsoumakas et al., 2010). 4.2.1 The Production Phase of RAkEL The algorithm was implemented using the MULAN Open Source Java library (Tsoumakas et al., 2011), which is based on WEKA (Witten and Frank, 2005). The algorithm works in two phases: 1. the production of an ensemble of LP algorithms, and 2. the combination of the LP algorithms. RAkEL takes as input the following parameters: (1) the numbers of iterations m (which is developer specified and denotes the number of models that the algorithm will produce), (2) the size of labelset k (which is also developer specified), (3) the set of labels L, and (4) the training set D. During the initial phase it outputs an ensemble of LP classifiers and the corresponding k-labelsets. A pseudocode for the production phase is shown below: Algorithm 1 RAkEL p</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian Witten and Eibe Frank. 2005. Data mining: Practical machine learning tools and techniques. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Ling Zhang</author>
<author>Zhi-Hua Zhou</author>
</authors>
<title>Ml-knn: A lazy learning approach to multi-label learning.</title>
<date>2007</date>
<journal>Pattern Recognition,</journal>
<volume>40</volume>
<issue>7</issue>
<contexts>
<context position="9170" citStr="Zhang and Zhou (2007)" startWordPosition="1450" endWordPosition="1453">of links between the data and the templates. ML classification can also apply to other problems whose features are correlated, such as text classification (Madjarov et al., 2012), when an aligned dataset is provided. ML classification algorithms have been divided into three categories: algorithm adaptation methods, problem transformation and ensemble methods (Tsoumakas and Katakis, 2007; Madjarov et al., 2012). Algorithm adaptation approaches (Tsoumakas et al., 2010) extend simple classification methods to handle ML data. For example, the k-nearest neighbour algorithm is extended to ML-kNN by Zhang and Zhou (2007). MLkNN identifies for each new instance its k nearest neighbours in the training set and then it predicts the label set by utilising the maximum a posteriori principle according to statistical information derived from the label sets of the k neighbours. Problem transformation approaches (Tsoumakas and Katakis, 2007) transform the ML classification task into one or more simple classification tasks. Ensemble methods (Tsoumakas et al., 2010) are algorithms that use ensembles to perform ML learning and they are based on problem transformation or algorithm adaptation methods. In this paper, we app</context>
</contexts>
<marker>Zhang, Zhou, 2007</marker>
<rawString>Min-Ling Zhang and Zhi-Hua Zhou. 2007. Ml-knn: A lazy learning approach to multi-label learning. Pattern Recognition, 40(7):2038–2048.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>