<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014670">
<title confidence="0.984916">
Semantic Transcoding:
Making the World Wide Web More Understandable and Usable
with External Annotations
</title>
<author confidence="0.996893">
Katashi Nagao Shingo Hosoya Yoshinari Shirai Kevin Squire
</author>
<affiliation confidence="0.9729505">
IBM Research, Keio University NTT Communication Univ. of Illinois
Tokyo Research Lab. punpun@sfc.wide.ad.jp Science Laboratories at Urbana-Champaign
</affiliation>
<email confidence="0.996986">
nagao@trl.ibm.co.jp way@cslab.kecl.ntt.co.jp k-squire@uiuc.edu
</email>
<sectionHeader confidence="0.980009" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992346153846">
This paper proposes an easy and simple method for
constructing a super-structure on the Web which
provides current Web contents with new value and
new means of use. The super-structure is based on
external annotations to Web documents. We have
developed a system for any user to annotate any
element of any Web document with additional in-
formation. We have also developed a proxy that
transcodes requested contents by considering anno-
tations assigned to them. In this paper, we classify
annotations into three categories. One is linguistic
annotation which helps the transcoder understand
the semantic structure of textual elements. The
second is commentary annotation which helps the
transcoder manipulate non-textual elements such as
images and sounds. The third is multimedia annota-
tion, which is a combination of the above two types.
All types of annotation are described using XML,
and correspondence between annotations and docu-
ment elements is defined using URLs and XPaths.
We call the entire process “semantic transcoding”
because we deal with the deep semantic content of
documents with annotations. The current semantic
transcoding process mainly handles text and video
summarization, language translation, and speech
synthesis of documents including images.
</bodyText>
<sectionHeader confidence="0.997331" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982602466666667">
The conventional Web structure can be considered
as a graph on a plane. In this paper, we pro-
pose a method for extending such planar graph to a
three-dimensional structure that consisting of multi-
ple planar layers. Such metalevel structure is based
on external annotations on documents on the Web.
Figure 1 represents the concept of our approach.
A super-structure on the Web consists of layers
of content and metacontent. The first layer cor-
rensponds to the set of metacontents of base doc-
uments. The second layer corresponds to the set of
metacontents of the first layer, and so on. We gen-
erally consider such metacontent an external anno-
tation. A famous example of external annotations is
external links that can be defined outside of the set
</bodyText>
<figureCaption confidence="0.999365">
Figure 1: Super-structure on the Web
</figureCaption>
<bodyText confidence="0.999953352941177">
of link-connected documents. These external links
have been discussed in the XML community but
they have not yet been implemented in the current
Web architecture (W3C XML, 2000).
Another popular example of external annotation
is comments or notes on Web documents created by
people other than the author. This kind of annota-
tions is helpful for readers evaluating the documents.
For example, images without alternative descrip-
tions are not understandable for visually-challenged
people. If there are comments on these images, these
people will understand the image contents by listen-
ing to them via speech transcoding. This example is
explained later in more detail.
We can easily imagine that an open platform for
creating and sharing annotaions would greatly ex-
tend the expressive power and value of the Web.
</bodyText>
<subsectionHeader confidence="0.994447">
1.1 Content Adaptation
</subsectionHeader>
<bodyText confidence="0.999898055555556">
Annotations do not just increase the expressive
power of the Web but also play an important role
in content reuse. An example of content reuse is, for
example, the transformation of content depending
on user preferences.
Content adaptation is a type of transcoding which
considers a users’ environment such as devices, net-
work bandwidth, profiles, and so on. Such adapta-
tion sometimes also involves a deep understanding
of the original document contents. If the transcoder
fails to analyse the semantic structure of a docu-
ment, then the results may cause user misunder-
standing.
Our technology assumes that external annota-
tions help machines to understand document con-
tents so that transcoding can have higher quality.
We call such transcoding based on annotation “se-
mantic transcoding.”
</bodyText>
<subsectionHeader confidence="0.987937">
1.2 Knowledge Discovery
</subsectionHeader>
<bodyText confidence="0.999910588235294">
Another use of annotations is in knowledge discov-
ery, where huge amounts of Web contents are au-
tomatically mined for some essential points. Unlike
conventional search engines that retrieve Web pages
using user specified keywords, knowledge miners cre-
ate a single document that satisfies a user’s request.
For example, the knowledge miner may generate a
summary document on a certain company’s product
strategy for the year from many kinds of information
resources of its products on the Web.
Currently, we are developing an information col-
lector that gathers documents related to a topic and
generates a document containing a summary of each
document.
There are many unresolved issues before we can
realize true knowledge discovery, but we can say that
annotations facilitate this activity.
</bodyText>
<sectionHeader confidence="0.974899" genericHeader="method">
2 External Annotation
</sectionHeader>
<bodyText confidence="0.999993846153846">
We have developed a simple method to associate ex-
ternal annotations with any element of any HTML
document. We use URLs, XPaths, and document
hash codes (digest values) to identify HTML ele-
ments in documents. We have also developed an
annotation server that maintains the relationship
between contents and annotations and transfers re-
quested annotations to a transcoder.
Our annotations are represented as XML format-
ted data and divided into three categories: linguistic,
commentary, and multimedia annotation. Multime-
dia (especially video) annotation is a combination of
the other two types of annotation.
</bodyText>
<subsectionHeader confidence="0.995145">
2.1 Annotation Environment
</subsectionHeader>
<bodyText confidence="0.998683666666667">
Our annotation environment consists of a client side
editor for the creation of annotations and a server
for the management of annotations.
The annotation environment is shown in Figure 2.
The process flows as follows (in this example case,
HTML files are processed):
</bodyText>
<listItem confidence="0.997674">
1. The user runs the annotation editor and re-
quests an URL as a target of annotation.
2. The annotation server accepts the request and
sends it to the Web server.
</listItem>
<figureCaption confidence="0.620748">
Figure 2: Annotation environment
</figureCaption>
<listItem confidence="0.969440384615385">
3. The annotation server receives the Web docu-
ment.
4. The server calculates the document hash code
(digest value) and registers the URL with the
code to its database.
5. The server returns the Web document to the
editor.
6. The user annotates the requested document and
sends the result to the server with some personal
data (name, professional areas, etc.).
7. The server receives the annotation data and re-
lates it with its URL in the database.
8. The server also updates the annotator profiles.
</listItem>
<subsectionHeader confidence="0.992087">
2.2 Annotation Editor
</subsectionHeader>
<bodyText confidence="0.99965225">
Our annotation editor, implemented as a Java appli-
cation, can communicate with the annotation server
explained below.
The annotation editor has the following functions:
</bodyText>
<listItem confidence="0.995367125">
1. To register targets of annotation to the annota-
tion server by sending URLs
2. To specify any element in the document using
the Web browser
3. To generate and send annotation data to the
annotation server
4. To reuse previously-created annotations when
the target contents are updated
</listItem>
<bodyText confidence="0.994874111111111">
An example screen of our annotation editor is
shown in Figure 3.
The left window of the editor shows the document
object structure of the HTML document. The right
window shows some text that was selected on the
Web browser (shown on the lower right corner). The
selected area is automatically assigned an XPath
(i.e., a location identifier in the document) (W3C
XPath, 2000).
</bodyText>
<figureCaption confidence="0.998899">
Figure 3: Annotation editor
</figureCaption>
<bodyText confidence="0.97020825">
Using the editor, the user annotates text with lin-
guistic structure (grammatical and semantic struc-
ture, described later) and adds a comment to an
element in the document. The editor is capable of
natural language processing and interactive disam-
biguation. The user will modify the result of the
automatically-analyzed sentence structure as shown
in Figure 4.
</bodyText>
<figureCaption confidence="0.704358">
Figure 4: Annotation editor with linguistic structure
editor
</figureCaption>
<subsectionHeader confidence="0.999816">
2.3 Annotation Server
</subsectionHeader>
<bodyText confidence="0.999988962962963">
Our annotation server receives annotation data from
any annotator and classifies it according to the an-
notator. The server retrieves documents from URLs
in annotation data and registers the document hash
codes with their URLs in its annotation database.
The hash codes are used to find di↵erences between
annotated documents and updated documents iden-
tified by the same URL. A hash code of document in-
ternal structure or DOM (Document Object Model)
enables the server to discover modified elements in
the annotated document (Maruyama et al., 1999).
The annotation server makes a table of annotator
names, URLs, XPaths, and document hash codes.
When the server accepts a URL as a request from
a transcoding proxy (described below), the server
returns a list of XPaths with associated annotation
files, their types (linguistic or commentary), and a
hash code. If the server receives an annotator’s name
as a request, it responds with the set of annotations
created by the specified annotator.
We are currently developing a mechanism for ac-
cess control between annotation servers and normal
Web servers. If authors of original documents do
not want to allow anyone to annotate their docu-
ments, they can add a statement about it in the
documents, and annotation servers will not retrieve
such contents for the annotation editors.
</bodyText>
<subsectionHeader confidence="0.971957">
2.4 Linguistic Annotation
</subsectionHeader>
<bodyText confidence="0.983727">
The purpose of linguistic annotation is to make
WWW texts machine-understandable (on the ba-
sis of a new tag set), and to develop content-based
presentation, retrieval, question-answering, summa-
rization, and translation systems with much higher
quality than is currently available. The new tag
set was proposed by the GDA (Global Document
Annotation) project (GDA, 2000). It is based on
XML (Extensible Markup Language), and designed
to be as compatible as possible with HTML, TEI
(TEI, 2000), CES (CES, 2000), EAGLES (EAGLES,
2000), and LAL (Watanabe, 1999). It specifies
modifier-modifiee relations, anaphor-referent rela-
tions, word senses, etc.
An example of a GDA-tagged sentence is as fol-
lows:
&lt;su&gt;&lt;np rel=&amp;quot;agt&amp;quot; sense=&amp;quot;time0&amp;quot;&gt;Time&lt;/np&gt;
&lt;v sense=&amp;quot;fly1&amp;quot;&gt;flies&lt;/v&gt;
&lt;adp rel=&amp;quot;eg&amp;quot;&gt;&lt;ad sense=&amp;quot;like0&amp;quot;&gt;like&lt;/ad&gt;
&lt;np&gt;an &lt;n sense=&amp;quot;arrow0&amp;quot;&gt;arrow&lt;/n&gt;&lt;/np&gt;
&lt;/adp&gt;.&lt;/su&gt;
&lt;su&gt; means sentential unit.
&lt;n&gt;, &lt;np&gt;, &lt;v&gt;, &lt;ad&gt; and &lt;adp&gt; mean noun, noun
phrase, verb, adnoun or adverb (including preposi-
tion and postposition), and adnominal or adverbial
phrase, respectively&apos; .
The rel attribute encodes a relationship in which
the current element stands with respect to the
element that it semantically depends on. Its
value is called a relational term. A relational
term denotes a binary relation, which may be a
thematic role such as agent, patient, recipient,
etc., or a rhetorical relation such as cause, con-
cession, etc. For instance, in the above sen-
tence, &lt;np rel=&amp;quot;agt&amp;quot; sense=&amp;quot;time0&amp;quot;&gt;Time&lt;/np&gt;
depends on the second
</bodyText>
<footnote confidence="0.967077">
1 A more detailed description of the GDA tag set can be
found at http://www.etl.go.jp/etl/nl/GDA/tagset.html.
</footnote>
<bodyText confidence="0.998837111111111">
element &lt;v sense=&amp;quot;fly1&amp;quot;&gt;flies&lt;/v&gt;. rel=&amp;quot;agt&amp;quot;
means that Time has the agent role with respect to
the event denoted by flies.
The sense attribute encodes a word sense.
Linguistic annotation is generated by automatic
morphological analysis, interactive sentence parsing,
and word sense disambiguation by selecting the most
appropriate paraphrase. Some research issues on lin-
guistic annotation are related to how the annota-
tion cost can be reduced within some feasible levels.
We have been developing some machine-guided an-
notation interfaces that conceal the complexity of
annotation. Machine learning mechanisms also con-
tribute to reducing the cost because they can gradu-
ally increase the accuracy of automatic annotation.
In principle, the tag set does not depend on lan-
guage, but as a first step we implemented a semi-
automatic tagging system for English and Japanese.
</bodyText>
<subsectionHeader confidence="0.922801">
2.5 Commentary Annotation
</subsectionHeader>
<bodyText confidence="0.999970777777778">
Commentary annotation is mainly used to annotate
non-textual elements like images and sounds with
some additional information. Each comment can in-
clude not only tagged texts but also other images
and links. Currently, this type of annotation ap-
pears in a subwindow that is overlayed on the orig-
inal document window when a user locates a mouse
pointer at the area of a comment-added element as
shown in Figure 5.
</bodyText>
<figureCaption confidence="0.988153">
Figure 5: Comment overlay on the document
</figureCaption>
<bodyText confidence="0.9993098">
Users can also annotate text elements with infor-
mation such as paraphrases, correctly-spelled words,
and underlines. This type of annotation is used for
text transcoding that combines such comments on
texts and original texts.
Commentary annotaion on hyperlinks is also avail-
able. This contributes to quick introduction of tar-
get documents before clicking the links. If there are
linguistic annotations on the target documents, the
transcoders can generate summaries of these docu-
ments and relate them with hyperlinks in the source
document.
There are some previous work on sharing com-
ments on the Web. For example, ComMentor is
a general meta-information architecture for anno-
tating documents on the Web (Roscheisen et al.,
1995). This architecture includes a basic client-
server protocol, meta-information description lan-
guage, a server system, and a remodeled NCSA Mo-
saic browser with interface augmentations to pro-
vide access to its extended functionality. ComMen-
tor provides a general mechanism for shared an-
notations, which enables people to annotate arbi-
trary documents at any position in-place, share com-
ments/pointers with other people (either publicly or
privately), and create shared “landmark” reference
points in the information space.
Such systems are often limited to particular doc-
uments or documents shared only among a few peo-
ple. Our annotation and transcoding system can
also handle multiple comments on any element of
any document on the Web. Also, a community
wide access control mechanism can be added to our
transcoding proxy. If a user is not a member of a
particular group, then the user cannot access the
transcoding proxy that is for group use only. In the
future, transcoding proxies and annotation servers
will communicate with some secured protocol that
prevents some other server or proxy from accessing
the annotation data.
Our main focus is adaptation of WWW contents
to users, and sharing comments in a community
is one of our additional features. We apply both
commentary and linguistic annotations to semantic
transcoding.
</bodyText>
<subsectionHeader confidence="0.957157">
2.6 Multimedia Annotation
</subsectionHeader>
<bodyText confidence="0.999827">
Our annotation technique can also be applied to
multimedia data such as digital video. Digital video
is becoming a necessary information source. Since
the size of these collections is growing to huge num-
bers of hours, summarization is required to e↵ec-
tively browse video segments in a short time with-
out losing the significant content. We have devel-
oped techniques for semi-automatic video annota-
tion using a text describing the content of the video.
Our techniques also use some video analysis methods
such as automatic cut detection, characterization of
frames in a cut, and scene recognition using similar-
ity between several cuts.
There is another approach to video annotation.
MPEG-7 is an e↵ort within the Moving Picture Ex-
perts Group (MPEG) of ISO/IEC that is dealing
with multimedia content description (MPEG, 2000).
Using content descriptions, video coded in MPEG-
7 is concerned with transcoding and delivery of
multimedia content to di↵erent devices. MPEG-7
will potentially allow greater input from the con-
tent publishers in guiding how multimedia content
is transcoded in different situations and for differ-
ent client devices. Also, MPEG-7 provides object-
level description of multimedia content which allows
a higher granularity of transcoding in which individ-
ual regions, segments, objects and events in image,
audio and video data can be differentially transcoded
depending on publisher and user preferences, net-
work bandwidth and client capabilities.
Our method will be integrated into tools for au-
thoring MPEG-7 data. However, we do not cur-
rently know when the MPEG-7 technology will be
widely available.
Our video annotation includes automatic segmen-
tation of video, semi-automatic linking of video seg-
ments with corresponding text segments, and inter-
active naming of people and objects in video frames.
Video annotation is performed through the follow-
ing three steps.
First, for each video clip, the annotation system
creates the text corresponding to its content. We
employed speech recognition for the automatic gen-
eration of a video transcript. The speech recognition
module also records correspondences between the
video frames and the words. The transcript is not
required to describe the whole video content. The
resolution of the description effects the final quality
of the transcoding (e.g., summarization).
Second, some video analysis techniques are ap-
plied to characterize scenes, segments (cuts and
shots), and individual frames in video. For exam-
ple, by detecting significant changes in the color his-
togram of successive frames, frame sequences can be
separated into cuts and shots.
Also, by searching and matching prepared tem-
plates to individual regions in the frame, the anno-
tation system identifies objects. The user can specify
significant objects in some scene in order to reduce
the time to identify target objects and to obtain a
higher recognition success ratio. The user can name
objects in a frame simply by selecting words in the
corresponding text.
Third, the user relates video segments to text seg-
ments such as paragraphs, sentences, and phrases,
based on scene structures and object-name corre-
spondences. The system helps the user to select
appropriate segments by prioritizing based on the
number of objects detected, camera movement, and
by showing a representative frame of each segment.
We developed a video annotation editor capable
of scene change detection, speech recognition, and
correlation of scenes and words. An example screen
of our video annotation editor is shown in Figure 6.
</bodyText>
<figureCaption confidence="0.995202">
Figure 6: Video annotation editor
</figureCaption>
<sectionHeader confidence="0.94154" genericHeader="method">
3 Semantic Transcoding
</sectionHeader>
<bodyText confidence="0.993487142857143">
Semantic transcoding is a transcoding technique
based on external annotations, used for content
adaptation according to user preferences. The
transcoders here are implemented as an extension
to an HTTP (HyperText Transfer Protocol) proxy
server. Such an HTTP proxy is called a transcoding
proxy.
</bodyText>
<figureCaption confidence="0.999383">
Figure 7 shows the environment of semantic
transcoding.
Figure 7: Transcoding environment
</figureCaption>
<bodyText confidence="0.996217">
The information flow in transcoding is as follows:
</bodyText>
<listItem confidence="0.979040333333333">
1. The transcoding proxy receives a request URL
with a client ID.
2. The proxy sends the request of the URL to the
Web server.
3. The proxy receives the document and calculates
its hash code.
4. The proxy also asks the annotation server for
annotation data related to the URL.
5. If the server finds the annotation data of the
URL in its database, it returns the data to the
proxy.
6. The proxy accepts the data and compares the
document hash code with that of the already
retrieved document.
7. The proxy also searches for the user preference
with the client ID. If there is no preference data,
the proxy uses a default setting until the user
gives the preference.
8. If the hash codes match, the proxy attempts
to transcode the document based on the an-
notation data by activating the appropriate
transcoders.
9. The proxy returns the transcoded document to
the client Web browser.
</listItem>
<subsectionHeader confidence="0.994215">
3.1 Transcoding Proxy
</subsectionHeader>
<bodyText confidence="0.998591222222222">
We employed IBM’s WBI (Web Intermediaries) as
a development platform to implement the seman-
tic transcoding system (WBI, 2000). WBI is a
customizable and extendable HTTP proxy server.
WBI provides APIs (Application Programming In-
terfaces) for user level access control and easy ma-
nipulation of input/output data of the proxy.
The transcoding proxy based on WBI has the fol-
lowing functionality:
</bodyText>
<listItem confidence="0.999654333333333">
1. Maintenance of personal preferences
2. Gathering and management of annotation data
3. Activation and integration of transcoders
</listItem>
<subsectionHeader confidence="0.997333">
3.2 Text Transcoding
</subsectionHeader>
<bodyText confidence="0.998930227272727">
Text transcoding is the transformation of text con-
tents based on linguistic annotations. As a first step,
we implemented text summarization.
Our text summarization method employs a
spreading activation technique to calculate the im-
portance values of elements in the text (Nagao and
Hasida, 1998). Since the method does not employ
any heuristics dependent on the domain and style
of documents, it is applicable to any linguistically-
annotated document. The method can also trim sen-
tences in the summary because importance scores
are assigned to elements smaller than sentences.
A linguistically-annotated document naturally de-
fines an intra-document network in which nodes
correspond to elements and links represent the
semantic relations. This network consists of
sentence trees (syntactic head-daughter hierar-
chies of subsentential elements such as words
or phrases), coreference/anaphora links, docu-
ment/subdivision/paragraph nodes, and rhetorical
relation links.
The summarization algorithm works as follows:
</bodyText>
<listItem confidence="0.950038857142857">
1. Spreading activation is performed in such a
way that two elements have the same activa-
tion value if they are coreferent or one of them
is the syntactic head of the other.
2. The unmarked element with the highest activa-
tion value is marked for inclusion in the sum-
mary.
3. When an element is marked, the following ele-
ments are recursively marked as well, until no
more elements are found:
• the marker’s head
• the marker’s antecedent
• the marker’s compulsory or a priori impor-
tant daughters, the values of whose rela-
tional attributes are agt (agent), pat (pa-
tient), rec (recipient), sbj (syntactic sub-
ject), obj (syntactic object), pos (posses-
sor), cnt (content), cau (cause), cnd (con-
dition), sbm (subject matter), etc.
• the antecedent of a zero anaphor in the
marker with some of the above values for
the relational attribute
4. All marked elements in the intra-document net-
work are generated preserving the order of their
positions in the original document.
5. If a size of the summary reaches the user-
specified value, then terminate; otherwise go
back to Step 2.
</listItem>
<bodyText confidence="0.999938809523809">
The size of the summary can be changed by sim-
ple user interaction. Thus the user can see the sum-
mary in a preferred size by using an ordinary Web
browser without any additional software. The user
can also input any words of interest. The corre-
sponding words in the document are assigned nu-
meric values that reflect degrees of interest. These
values are used during spreading activation for cal-
culating importance scores.
Figure 8 shows the summarization result on the
normal Web browser. The top document is the orig-
inal and the bottom one is the summarized version.
Another kind of text transcoding is language
translation. We can predict that translation based
on linguistic annotations will produce a much better
result than many existing systems. This is because
the major difficulties of present machine translation
come from syntactic and word sense ambiguities in
natural languages, which can be easily clarified in
annotation. An example of the result of English-to-
Japanese translation is shown in Figure 9.
</bodyText>
<subsectionHeader confidence="0.996114">
3.3 Image Transcoding
</subsectionHeader>
<bodyText confidence="0.995341727272727">
Image transcoding is to convert images into these of
di↵erent size, color (full color or grayscale), and res-
olution (e.g., compression ratio) depending on user’s
in half size of the original and whose images are re-
duced to one-third. In this figure, the preference
setting subwindow is shown on the right hand. The
window appears when the user double-clicks the icon
on the lower right corner (the transcoding proxy au-
tomatically inserts the icon). Using this window, the
user can easily modify the parameters for transcod-
ing.
</bodyText>
<figureCaption confidence="0.941596">
Figure 10: Image transcoding (and preference set-
ting window)
Figure 8: Original and summarized documents
Figure 9: Translated document
</figureCaption>
<bodyText confidence="0.991040222222222">
device and communication capability. Links to these
converted images are made from the original images.
Therefore, users will notice that the images they are
looking at are not original if there are links to similar
images.
Figure 10 shows the document that is summarized
By combining image and text transcodings, the
system can, for example, convert contents to just fit
the client screen size.
</bodyText>
<subsectionHeader confidence="0.994603">
3.4 Voice Mranscoding
</subsectionHeader>
<bodyText confidence="0.999581424242424">
Voice synthesis also works better if the content has
linguistic annotation. For example, a speech synthe-
sis markup language is being discussed in (SABLE,
2000). A typical example is processing proper nouns
and technical terms. Word level annotations on
proper nouns allow the transcoders to recognize not
only their meanings but also their readings.
Voice transcoding generates spoken language ver-
sion of documents. There are two types of voice
transcoding. One is when the transcoder synthesizes
sound data in audio formats such as MP3 (MPEG-
1 Audio Layer 3). This case is useful for devices
without voice synthesis capability such as cellular
phones and PDAs. The other is when the transcoder
converts documents into more appropriate style for
voice synthesis. This case requires that a voice syn-
thesis program is installed on the client side. Of
cource, the synthesizer uses the output of the voice
synthesizer. Therefore, the mechanism of document
conversion is a common part of both types of voice
transcoding.
Documents annotated for voice include some text
in commentary annotation for non-textual elements
and some word information in linguistic annota-
tion for the reading of proper nouns and unknown
words in the dictionary. The document also con-
tains phrase and sentence boundary information so
that pauses appear in appropriate positions.
Figure 11 shows an example of the voice-
transcoded document in which icons that represent
the speaker are inserted. When the user clicks the
speaker icon, the MP3 player software is invoked and
starts playing the synthesized voice data.
</bodyText>
<figureCaption confidence="0.961613">
Figure 11: Voice transcoding
</figureCaption>
<subsectionHeader confidence="0.744224">
3.5 Video Mranscoding
</subsectionHeader>
<bodyText confidence="0.997592754098361">
Video transcoding employs video annotation that
consists of linguistically-marked-up transcripts such
as closed captions, time stamps of scene changes,
representative images (key frames) of each scene,
and additional information such as program names,
etc. Our video transcoding has several variations,
including video summarization, video to document
transformation, video translation, etc.
Video summarization is performed as a by-
product of text summarization. Since a summa-
rized video transcript contains important informa-
tion, corresponding video sequences will produce a
collection of significant scenes in the video. Sum-
marized video is played by a player we developed.
An example screen of our video player is shown in
Figure 12.
There are some previous work on video sum-
marization such as Infomedia (Smith and Kanade,
1995) and CueVideo (Amir et al., 1999). They create
a video summary based on automatically extracted
features in video such as scene changes, speech, text
and human faces in frames, and closed captions.
They can transcode video data without annotations.
However, currently, an accuracy of their summariza-
tion is not practical because of the failure of auto-
matic video analysis. Our approach to video sum-
marization has sufficient quality for use if the data
has enough semantic annotation. As mentioned ear-
lier, we have developed a tool to help annotators
Figure 12: Video player with summarization func-
tion
to create semantic annotation data for multimedia
data. Since our annotation data is task-independent
and versatile, annotations on video are worth creat-
ing if the video will be used in di↵erent applications
such as automatic editing and information extrac-
tion from video.
Video to document transformation is another type
of video transcoding. If the client device does not
have video playing capability, the user cannot access
video contents. In this case, the video transcoder
creates a document including important images of
scenes and texts related to each scene. Also, the
resulting document can be summarized by the text
transcoder.
Video translation is a combination of text and
voice transcodings. First, a video transcript with
linguistic annotation is translated by the text
transcoder. Then, the result of translation is
converted into voice-suitable text by the voice
transcoder. Synchronization of video playing and
voice synthesis makes another language version of
the original video clip. This part has not yet been
implemented, but this function will be integrated
into our video player.
The above described transcodings are automat-
ically combined according to user demand, so the
transcoding proxy has a planning machanism to de-
termine the order of activation of each transcoder
necessary for the requested content and user prefer-
ences (including client device constraints).
</bodyText>
<sectionHeader confidence="0.996156" genericHeader="method">
4 future Plans
</sectionHeader>
<bodyText confidence="0.999990705882353">
We are planning to apply our technology to knowl-
edge discovery from huge online resources. Anno-
tations will be very useful to extract some essen-
tial points in documents. For example, an anno-
tator adds comments to several documents, and he
or she seems to be a specialist of some particular
field. Then, the machine automatically collects doc-
uments annotated by this annotator and generates
a single document including summaries of the anno-
tated documents.
Also, content-based retrieval of Web documents
including multimedia data is being pursued. Such
retrieval enables users to ask questions in natural
language (either spoken or written).
While our current prototype system is running lo-
cally, we are also planning to evaluate our system
with some open experiments.
</bodyText>
<sectionHeader confidence="0.995559" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999985375">
We have discussed a full architecture for creating
and utilizing external annotations. Using the anno-
tations, we realized semantic transcoding that au-
tomatically customizes Web contents depending on
user preferences.
This technology also contributes to commentary
information sharing and device dependent transfor-
mation for any device. One of our future goals is to
make contents of the WWW intelligent enough to
answer our questions asked using natural language.
We imagine that in the near future we will not use
search engines but will instead use knowledge discov-
ery engines that give us a personalized summary of
multiple documents instead of hyperlinks. The work
in this paper is one step toward a better solution of
dealing with the coming information deluge.
</bodyText>
<sectionHeader confidence="0.998876" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999959">
The authors would like to acknowledge the contrib-
utors to the project described in this paper. Koiti
Hasida gave the authors some helpful advices to ap-
ply the GDA tag set to our linguistic and multi-
media annotations. Hideo Watanabe developed the
prototype of English-to-Japanese language transla-
tor. Shinichi Torihara developed the prototype of
voice synthesizer.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999252755102041">
A. Amir, S. Srinivasan, D. Ponceleon, and D.
Petkovic. CueVideo: Automated indexing of
video for searching and browsing. In Proceedings
of SIGIR’99. 1999.
Corpus Encoding Standard (CES). Corpus Encod-
ing Standard. http://www.cs.vassar.edu/CES/.
Expert Advisory Group on Language Engineer-
ing Standards (EAGLES). EAGLES online.
http://www.ilc.pi.cnr.it/EAGLES/home.html.
Koiti Hasida. Global Document Annotation.
http://www.etl.go.jp/etl/nl/gda/.
IBM Almaden Research Center. Web Intermediaries
(WBI). http://www.almaden.ibm.com/cs/wbi/.
Hiroshi Maruyama, Kent Tamura, and Naohiko
Uramoto. XML and Java: Developing Web ap-
plications. Addison-Wesley, 1999.
Moving Picture Experts Group (MPEG).
MPEG-7 Context and Objectives.
http://drogo.cselt.stet.it/mpeg/standards/mpeg-
7/mpeg-7.htm.
Katashi Nagao and Koiti Hasida. Automatic text
summarization based on the Global Document
Annotation. In Proceedings of COLING-ACL’98.
1998.
Martin Roscheisen, Christian Mogensen, and Terry
Winograd. Shared Web annotations as a platform
for third-party value-added information providers:
Architecture, protocols, and usage examples.
Technical Report CSDTR/DLTR. Computer Sci-
ence Department, Stanford University, 1995.
The SABLE Consortium.
A Speech Synthesis Markup Language.
http://www.cstr.ed.ac.uk/projects/ssml.html.
Michael A. Smith and Takeo Kanade. Video skim-
ming for quick browsing based on audio and image
characterization. Technical Report CMU-CS-95-
186. School of Computer Science, Carnegie Mellon
University, 1995.
The Text Encoding Initiative (TEI). Text Encoding
Initiative. http://www.uic.edu:80/orgs/tei/.
Hideo Watanabe. Linguistic Annotation Language:
The markup langauge for assisting NLP programs.
TRL Research Report RT0334. IBM Tokyo Re-
search Laboratory, 1999.
World Wide Web Consortium. Extensible Markup
Language (XML). http://www.w3.org/XML/.
World Wide Web Consortium.
XML Path Language (XPath) Version 1.0.
http://www.w3.org/TR/xpath.html.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.842354">
<title confidence="0.998937">Semantic Transcoding: Making the World Wide Web More Understandable and with External Annotations</title>
<author confidence="0.99995">Katashi Nagao Shingo Hosoya Yoshinari Shirai Kevin Squire</author>
<affiliation confidence="0.9601175">IBM Research, Keio University NTT Communication Univ. of Tokyo Research Lab. punpun@sfc.wide.ad.jp Science Laboratories at Urbana-Champaign</affiliation>
<email confidence="0.870327">nagao@trl.ibm.co.jpway@cslab.kecl.ntt.co.jpk-squire@uiuc.edu</email>
<abstract confidence="0.999877444444444">This paper proposes an easy and simple method for constructing a super-structure on the Web which provides current Web contents with new value and new means of use. The super-structure is based on external annotations to Web documents. We have developed a system for any user to annotate any element of any Web document with additional information. We have also developed a proxy that transcodes requested contents by considering annotations assigned to them. In this paper, we classify annotations into three categories. One is linguistic annotation which helps the transcoder understand the semantic structure of textual elements. The second is commentary annotation which helps the transcoder manipulate non-textual elements such as images and sounds. The third is multimedia annotation, which is a combination of the above two types. All types of annotation are described using XML, and correspondence between annotations and document elements is defined using URLs and XPaths. We call the entire process “semantic transcoding” because we deal with the deep semantic content of documents with annotations. The current semantic transcoding process mainly handles text and video summarization, language translation, and speech synthesis of documents including images.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Amir</author>
<author>S Srinivasan</author>
<author>D Ponceleon</author>
<author>D Petkovic</author>
</authors>
<title>CueVideo: Automated indexing of video for searching and browsing.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR’99.</booktitle>
<contexts>
<context position="26415" citStr="Amir et al., 1999" startWordPosition="4134" endWordPosition="4137">ames, etc. Our video transcoding has several variations, including video summarization, video to document transformation, video translation, etc. Video summarization is performed as a byproduct of text summarization. Since a summarized video transcript contains important information, corresponding video sequences will produce a collection of significant scenes in the video. Summarized video is played by a player we developed. An example screen of our video player is shown in Figure 12. There are some previous work on video summarization such as Infomedia (Smith and Kanade, 1995) and CueVideo (Amir et al., 1999). They create a video summary based on automatically extracted features in video such as scene changes, speech, text and human faces in frames, and closed captions. They can transcode video data without annotations. However, currently, an accuracy of their summarization is not practical because of the failure of automatic video analysis. Our approach to video summarization has sufficient quality for use if the data has enough semantic annotation. As mentioned earlier, we have developed a tool to help annotators Figure 12: Video player with summarization function to create semantic annotation d</context>
</contexts>
<marker>Amir, Srinivasan, Ponceleon, Petkovic, 1999</marker>
<rawString>A. Amir, S. Srinivasan, D. Ponceleon, and D. Petkovic. CueVideo: Automated indexing of video for searching and browsing. In Proceedings of SIGIR’99. 1999.</rawString>
</citation>
<citation valid="false">
<title>Corpus Encoding Standard (CES). Corpus Encoding Standard.</title>
<note>http://www.cs.vassar.edu/CES/.</note>
<marker></marker>
<rawString>Corpus Encoding Standard (CES). Corpus Encoding Standard. http://www.cs.vassar.edu/CES/.</rawString>
</citation>
<citation valid="false">
<title>Expert Advisory Group on Language Engineering Standards (EAGLES).</title>
<note>EAGLES online. http://www.ilc.pi.cnr.it/EAGLES/home.html.</note>
<marker></marker>
<rawString>Expert Advisory Group on Language Engineering Standards (EAGLES). EAGLES online. http://www.ilc.pi.cnr.it/EAGLES/home.html.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Koiti Hasida</author>
</authors>
<title>Global Document Annotation.</title>
<note>http://www.etl.go.jp/etl/nl/gda/.</note>
<marker>Hasida, </marker>
<rawString>Koiti Hasida. Global Document Annotation. http://www.etl.go.jp/etl/nl/gda/.</rawString>
</citation>
<citation valid="false">
<institution>IBM Almaden Research Center. Web Intermediaries</institution>
<marker></marker>
<rawString>IBM Almaden Research Center. Web Intermediaries (WBI). http://www.almaden.ibm.com/cs/wbi/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Maruyama</author>
<author>Kent Tamura</author>
<author>Naohiko Uramoto</author>
</authors>
<title>XML and Java: Developing Web applications.</title>
<date>1999</date>
<publisher>Addison-Wesley,</publisher>
<contexts>
<context position="8378" citStr="Maruyama et al., 1999" startWordPosition="1307" endWordPosition="1310">: Annotation editor with linguistic structure editor 2.3 Annotation Server Our annotation server receives annotation data from any annotator and classifies it according to the annotator. The server retrieves documents from URLs in annotation data and registers the document hash codes with their URLs in its annotation database. The hash codes are used to find di↵erences between annotated documents and updated documents identified by the same URL. A hash code of document internal structure or DOM (Document Object Model) enables the server to discover modified elements in the annotated document (Maruyama et al., 1999). The annotation server makes a table of annotator names, URLs, XPaths, and document hash codes. When the server accepts a URL as a request from a transcoding proxy (described below), the server returns a list of XPaths with associated annotation files, their types (linguistic or commentary), and a hash code. If the server receives an annotator’s name as a request, it responds with the set of annotations created by the specified annotator. We are currently developing a mechanism for access control between annotation servers and normal Web servers. If authors of original documents do not want t</context>
</contexts>
<marker>Maruyama, Tamura, Uramoto, 1999</marker>
<rawString>Hiroshi Maruyama, Kent Tamura, and Naohiko Uramoto. XML and Java: Developing Web applications. Addison-Wesley, 1999.</rawString>
</citation>
<citation valid="false">
<title>Moving Picture Experts Group (MPEG). MPEG-7 Context and Objectives.</title>
<note>http://drogo.cselt.stet.it/mpeg/standards/mpeg7/mpeg-7.htm.</note>
<marker></marker>
<rawString>Moving Picture Experts Group (MPEG). MPEG-7 Context and Objectives. http://drogo.cselt.stet.it/mpeg/standards/mpeg7/mpeg-7.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katashi Nagao</author>
<author>Koiti Hasida</author>
</authors>
<title>Automatic text summarization based on the Global Document Annotation.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL’98.</booktitle>
<contexts>
<context position="20009" citStr="Nagao and Hasida, 1998" startWordPosition="3120" endWordPosition="3123">gramming Interfaces) for user level access control and easy manipulation of input/output data of the proxy. The transcoding proxy based on WBI has the following functionality: 1. Maintenance of personal preferences 2. Gathering and management of annotation data 3. Activation and integration of transcoders 3.2 Text Transcoding Text transcoding is the transformation of text contents based on linguistic annotations. As a first step, we implemented text summarization. Our text summarization method employs a spreading activation technique to calculate the importance values of elements in the text (Nagao and Hasida, 1998). Since the method does not employ any heuristics dependent on the domain and style of documents, it is applicable to any linguisticallyannotated document. The method can also trim sentences in the summary because importance scores are assigned to elements smaller than sentences. A linguistically-annotated document naturally defines an intra-document network in which nodes correspond to elements and links represent the semantic relations. This network consists of sentence trees (syntactic head-daughter hierarchies of subsentential elements such as words or phrases), coreference/anaphora links,</context>
</contexts>
<marker>Nagao, Hasida, 1998</marker>
<rawString>Katashi Nagao and Koiti Hasida. Automatic text summarization based on the Global Document Annotation. In Proceedings of COLING-ACL’98. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Roscheisen</author>
<author>Christian Mogensen</author>
<author>Terry Winograd</author>
</authors>
<title>Shared Web annotations as a platform for third-party value-added information providers: Architecture, protocols, and usage examples.</title>
<date>1995</date>
<tech>Technical Report CSDTR/DLTR.</tech>
<institution>Computer Science Department, Stanford University,</institution>
<contexts>
<context position="12877" citStr="Roscheisen et al., 1995" startWordPosition="1997" endWordPosition="2000">nderlines. This type of annotation is used for text transcoding that combines such comments on texts and original texts. Commentary annotaion on hyperlinks is also available. This contributes to quick introduction of target documents before clicking the links. If there are linguistic annotations on the target documents, the transcoders can generate summaries of these documents and relate them with hyperlinks in the source document. There are some previous work on sharing comments on the Web. For example, ComMentor is a general meta-information architecture for annotating documents on the Web (Roscheisen et al., 1995). This architecture includes a basic clientserver protocol, meta-information description language, a server system, and a remodeled NCSA Mosaic browser with interface augmentations to provide access to its extended functionality. ComMentor provides a general mechanism for shared annotations, which enables people to annotate arbitrary documents at any position in-place, share comments/pointers with other people (either publicly or privately), and create shared “landmark” reference points in the information space. Such systems are often limited to particular documents or documents shared only am</context>
</contexts>
<marker>Roscheisen, Mogensen, Winograd, 1995</marker>
<rawString>Martin Roscheisen, Christian Mogensen, and Terry Winograd. Shared Web annotations as a platform for third-party value-added information providers: Architecture, protocols, and usage examples. Technical Report CSDTR/DLTR. Computer Science Department, Stanford University, 1995.</rawString>
</citation>
<citation valid="false">
<title>The SABLE Consortium. A Speech Synthesis Markup Language.</title>
<note>http://www.cstr.ed.ac.uk/projects/ssml.html.</note>
<marker></marker>
<rawString>The SABLE Consortium. A Speech Synthesis Markup Language. http://www.cstr.ed.ac.uk/projects/ssml.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Smith</author>
<author>Takeo Kanade</author>
</authors>
<title>Video skimming for quick browsing based on audio and image characterization.</title>
<date>1995</date>
<tech>Technical Report CMU-CS-95-186.</tech>
<institution>School of Computer Science, Carnegie Mellon University,</institution>
<contexts>
<context position="26382" citStr="Smith and Kanade, 1995" startWordPosition="4128" endWordPosition="4131">ditional information such as program names, etc. Our video transcoding has several variations, including video summarization, video to document transformation, video translation, etc. Video summarization is performed as a byproduct of text summarization. Since a summarized video transcript contains important information, corresponding video sequences will produce a collection of significant scenes in the video. Summarized video is played by a player we developed. An example screen of our video player is shown in Figure 12. There are some previous work on video summarization such as Infomedia (Smith and Kanade, 1995) and CueVideo (Amir et al., 1999). They create a video summary based on automatically extracted features in video such as scene changes, speech, text and human faces in frames, and closed captions. They can transcode video data without annotations. However, currently, an accuracy of their summarization is not practical because of the failure of automatic video analysis. Our approach to video summarization has sufficient quality for use if the data has enough semantic annotation. As mentioned earlier, we have developed a tool to help annotators Figure 12: Video player with summarization functio</context>
</contexts>
<marker>Smith, Kanade, 1995</marker>
<rawString>Michael A. Smith and Takeo Kanade. Video skimming for quick browsing based on audio and image characterization. Technical Report CMU-CS-95-186. School of Computer Science, Carnegie Mellon University, 1995.</rawString>
</citation>
<citation valid="false">
<title>The Text Encoding Initiative (TEI). Text Encoding Initiative.</title>
<note>http://www.uic.edu:80/orgs/tei/.</note>
<marker></marker>
<rawString>The Text Encoding Initiative (TEI). Text Encoding Initiative. http://www.uic.edu:80/orgs/tei/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideo Watanabe</author>
</authors>
<title>Linguistic Annotation Language: The markup langauge for assisting NLP programs.</title>
<date>1999</date>
<booktitle>TRL Research Report RT0334. IBM Tokyo Research Laboratory,</booktitle>
<contexts>
<context position="9740" citStr="Watanabe, 1999" startWordPosition="1525" endWordPosition="1526">nts for the annotation editors. 2.4 Linguistic Annotation The purpose of linguistic annotation is to make WWW texts machine-understandable (on the basis of a new tag set), and to develop content-based presentation, retrieval, question-answering, summarization, and translation systems with much higher quality than is currently available. The new tag set was proposed by the GDA (Global Document Annotation) project (GDA, 2000). It is based on XML (Extensible Markup Language), and designed to be as compatible as possible with HTML, TEI (TEI, 2000), CES (CES, 2000), EAGLES (EAGLES, 2000), and LAL (Watanabe, 1999). It specifies modifier-modifiee relations, anaphor-referent relations, word senses, etc. An example of a GDA-tagged sentence is as follows: &lt;su&gt;&lt;np rel=&amp;quot;agt&amp;quot; sense=&amp;quot;time0&amp;quot;&gt;Time&lt;/np&gt; &lt;v sense=&amp;quot;fly1&amp;quot;&gt;flies&lt;/v&gt; &lt;adp rel=&amp;quot;eg&amp;quot;&gt;&lt;ad sense=&amp;quot;like0&amp;quot;&gt;like&lt;/ad&gt; &lt;np&gt;an &lt;n sense=&amp;quot;arrow0&amp;quot;&gt;arrow&lt;/n&gt;&lt;/np&gt; &lt;/adp&gt;.&lt;/su&gt; &lt;su&gt; means sentential unit. &lt;n&gt;, &lt;np&gt;, &lt;v&gt;, &lt;ad&gt; and &lt;adp&gt; mean noun, noun phrase, verb, adnoun or adverb (including preposition and postposition), and adnominal or adverbial phrase, respectively&apos; . The rel attribute encodes a relationship in which the current element stands with respect to the </context>
</contexts>
<marker>Watanabe, 1999</marker>
<rawString>Hideo Watanabe. Linguistic Annotation Language: The markup langauge for assisting NLP programs. TRL Research Report RT0334. IBM Tokyo Research Laboratory, 1999.</rawString>
</citation>
<citation valid="false">
<authors>
<author>World Wide</author>
</authors>
<title>Web Consortium. Extensible Markup Language (XML). http://www.w3.org/XML/. World Wide Web Consortium.</title>
<marker>Wide, </marker>
<rawString>World Wide Web Consortium. Extensible Markup Language (XML). http://www.w3.org/XML/. World Wide Web Consortium.</rawString>
</citation>
<citation valid="false">
<journal>XML Path Language (XPath) Version</journal>
<volume>1</volume>
<note>http://www.w3.org/TR/xpath.html.</note>
<marker></marker>
<rawString>XML Path Language (XPath) Version 1.0. http://www.w3.org/TR/xpath.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>