<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000820">
<title confidence="0.760427">
Measures of Distributional Similarity
</title>
<author confidence="0.991681">
Lillian Lee
</author>
<affiliation confidence="0.991465">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.506911">
Ithaca, NY 14853-7501
</address>
<email confidence="0.997478">
llee@cs.cornell.edu
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999711777777778">
We study distributional similarity measures for
the purpose of improving probability estima-
tion for unseen cooccurrences. Our contribu-
tions are three-fold: an empirical comparison
of a broad range of measures; a classification
of similarity functions based on the information
that they incorporate; and the introduction of
a novel function that is superior at evaluating
potential proxy distributions.
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987964">
An inherent problem for statistical methods in
natural language processing is that of sparse
data — the inaccurate representation in any
training corpus of the probability of low fre-
quency events. In particular, reasonable events
that happen to not occur in the training set may
mistakenly be assigned a probability of zero.
These unseen events generally make up a sub-
stantial portion of novel data; for example, Es-
sen and Steinbiss (1992) report that 12% of the
test-set bigrams in a 75%-25% split of one mil-
lion words did not occur in the training parti-
tion.
We consider here the question of how to es-
timate the conditional cooccurrence probability
P(v In) of an unseen word pair (n, v) drawn from
some finite set N x V. Two state-of-the-art
technologies are Katz&apos;s (1987) backoff method
and Jelinek and Mercer&apos;s (1980) interpolation
method. Both use P(v) to estimate P(v In)
when (n, v) is unseen, essentially ignoring the
identity of n.
An alternative approach is distance-weighted
averaging, which arrives at an estimate for un-
seen cooccurrences by combining estimates for
</bodyText>
<equation confidence="0.8408605">
cooccurrences involving similar words:1
sim(n, m)P(v1m)
P(0) = rriES(n)
EmEs(n) sim(n, m) , (1)
</equation>
<bodyText confidence="0.998727628571428">
where S(n) is a set of candidate similar words
and sim(n, m) is a function of the similarity
between n and m. We focus on distributional
rather than semantic similarity (e.g., Resnik
(1995)) because the goal of distance-weighted
averaging is to smooth probability distributions
— although the words &amp;quot;chance&amp;quot; and &amp;quot;probabil-
ity&amp;quot; are synonyms, the former may not be a
good model for predicting what cooccurrences
the latter is likely to participate in.
There are many plausible measures of distri-
butional similarity. In previous work (Dagan
et al., 1999), we compared the performance of
three different functions: the Jensen-Shannon
divergence (total divergence to the average), the
L1 norm, and the confusion probability. Our
experiments on a frequency-controlled pseu-
doword disambiguation task showed that using
any of the three in a distance-weighted aver-
aging scheme yielded large improvements over
Katz&apos;s backoff smoothing method in predicting
unseen coocurrences. Furthermore, by using a
restricted version of model (1) that stripped in-
comparable parameters, we were able to empir-
ically demonstrate that the confusion probabil-
ity is fundamentally worse at selecting useful
similar words. D. Lin also found that the choice
of similarity function can affect the quality of
automatically-constructed thesauri to a statis-
tically significant degree (1998a) and the ability
to determine common morphological roots by as
much as 49% in precision (1998b).
3-The term &amp;quot;similarity-based&amp;quot;, which we have used
previously, has been applied to describe other models
as well (L. Lee, 1997; Karov and Edelman, 1998).
</bodyText>
<page confidence="0.996356">
25
</page>
<bodyText confidence="0.999987666666667">
These empirical results indicate that investi-
gating different similarity measures can lead to
improved natural language processing. On the
other hand, while there have been many sim-
ilarity measures proposed and analyzed in the
information retrieval literature (Jones and Fur-
nas, 1987), there has been some doubt expressed
in that community that the choice of similarity
metric has any practical impact:
Several authors have pointed out that
the difference in retrieval performance
achieved by different measures of asso-
ciation is insignificant, providing that
these are appropriately normalised.
(van Rijsbergen, 1979, pg. 38)
But no contradiction arises because, as van Rijs-
bergen continues, &amp;quot;one would expect this since
most measures incorporate the same informa-
tion&amp;quot;. In the language-modeling domain, there
is currently no agreed-upon best similarity met-
ric because there is no agreement on what the
&amp;quot;same information&amp;quot; — the key data that a sim-
ilarity function should incorporate — is.
The overall goal of the work described here
was to discover these key characteristics. To
this end, we first compared a number of com-
mon similarity measures, evaluating them in a
parameter-free way on a decision task. When
grouped by average performance, they fell into
several coherent classes, which corresponded to
the extent to which the functions focused on
the intersection of the supports (regions of posi-
tive probability) of the distributions. Using this
insight, we developed an information-theoretic
metric, the skew divergence, which incorporates
the support-intersection data in an asymmetric
fashion. This function yielded the best perfor-
mance overall: an average error rate reduction
of 4% (significant at the .01 level) with respect
to the Jensen-Shannon divergence, the best pre-
dictor of unseen events in our earlier experi-
ments (Dagan et al., 1999).
Our contributions are thus three-fold: an em-
pirical comparison of a broad range of similarity
metrics using an evaluation methodology that
factors out inessential degrees of freedom; a pro-
posal, building on this comparison, of a charac-
teristic for classifying similarity functions; and
the introduction of a new similarity metric in-
corporating this characteristic that is superior
at evaluating potential proxy distributions.
</bodyText>
<sectionHeader confidence="0.955173" genericHeader="method">
2 Distributional Similarity Functions
</sectionHeader>
<bodyText confidence="0.998457107142857">
In this section, we describe the seven distri-
butional similarity functions we initally evalu-
ated.2 For concreteness, we choose N and V
to be the set of nouns and the set of transitive
verbs, respectively; a cooccurrence pair (n, v)
results when n appears as the head noun of the
direct object of v. We use P to denote probabil-
ities assigned by a base language model (in our
experiments, we simply used unsmoothed rel-
ative frequencies derived from training corpus
counts).
Let n and m be two nouns whose distribu-
tional similarity is to be determined; for nota-
tional simplicity, we write q(v) for P(v In) and
r (v) for P(v1m), their respective conditional
verb cooccurrence probabilities.
Figure 1 lists several familiar functions. The
cosine metric and Jaccard&apos;s coefficient are com-
monly used in information retrieval as measures
of association (Salton and McGill, 1983). Note
that Jaccard&apos;s coefficient differs from all the
other measures we consider in that it is essen-
tially combinatorial, being based only on the
sizes of the supports of q, r, and q • r rather
than the actual values of the distributions.
Previously, we found the Jensen-Shannon di-
vergence (Rao, 1982; J. Lin, 1991) to be a useful
measure of the distance between distributions:
</bodyText>
<equation confidence="0.985485">
JS(q,r) = [D (q
</equation>
<bodyText confidence="0.99704275">
The function D is the KL divergence, which
measures the (always nonnegative) average in-
efficiency in using one distribution to code for
another (Cover and Thomas, 1991):
</bodyText>
<listItem confidence="0.622921">
•
</listItem>
<bodyText confidence="0.987891">
The function avgq denotes the average distri-
bution avgq,r(v) = (q(v) + r(v))/2; observe that
its use ensures that the Jensen-Shannon diver-
gence is always defined. In contrast, D(q1 Ir) is
undefined if q is not absolutely continuous with
respect to r (i.e., the support of q is not a subset
of the support of r).
2Strictly speaking, some of these functions are dissim-
ilarity measures, but each such function f can be recast
as a similarity function via the simple transformation
C — f, where C is an appropriate constant. Whether we
mean f or C — f should be clear from context.
</bodyText>
<equation confidence="0.924130857142857">
lavgqm) + D avgq,,)] .
D(pi(V) II P2(V)) =E (v) log P2 Pi ((v)V)
26
Euclidean distance L2(q,r)
L
L1 norm i(q, r)
cosine cos (q, r)
Jaccard&apos;s coefficient Jac(q, r)
=
= E lq(v) _ r(v)I
q(v)r(v)
vE2, q(v)2 vEv r(v)2
1{v : q(v) &gt; 0 and r(v) &gt; 011
I q(v) &gt; 0 or r(v) &gt;0)1
</equation>
<figureCaption confidence="0.966694">
Figure 1: Well-known functions
</figureCaption>
<bodyText confidence="0.971671333333333">
1,
The confusion probability has been used by
several authors to smooth word cooccurrence
probabilities (Sugawara et al., 1985; Essen and
Steinbiss, 1992; Grishman and Sterling, 1993);
it measures the degree to which word m can
be substituted into the contexts in which n ap-
pears. If the base language model probabili-
ties obey certain Bayesian consistency condi-
tions (Dagan et al., 1999), as is the case for
relative frequencies, then we may write the con-
fusion probability as follows:
</bodyText>
<equation confidence="0.76328">
P(v)
</equation>
<bodyText confidence="0.993215113207548">
Note that it incorporates unigram probabilities
as well as the two distributions q and r.
Finally, Kendall&apos;s T which appears in work
on clustering similar adjectives (Hatzivassilo-
glou and McKeown, 1993; Hatzivassiloglou,
1996), is a nonparametric measure of the as-
sociation between random variables (Gibbons,
1993). In our context, it looks for correlation
between the behavior of q and r on pairs of
verbs. Three versions exist; we use the simplest,
Ta, here:
sign [(q(vi) — q(v2))(r(vi) — r(v2))
2(11;1)
where sign(x) is 1 for positive arguments, —1
for negative arguments, and 0 at 0. The intu-
ition behind Kendall&apos;s T is as follows. Assume
all verbs have distinct conditional probabilities.
If sorting the verbs by the likelihoods assigned
by q yields exactly the same ordering as that
which results from ranking them according to
r, then T (q, r) = 1; if it yields exactly the op-
posite ordering, then T(q,r) = —1. We treat a
value of —1 as indicating extreme dissimilarity.3
It is worth noting at this point that there
are several well-known measures from the NLP
literature that we have omitted from our ex-
periments. Arguably the most widely used is
the mutual information (Hindle, 1990; Church
and Hanks, 1990; Dagan et al., 1995; Luk,
1995; D. Lin, 1998a). It does not apply in
the present setting because it does not mea-
sure the similarity between two arbitrary prob-
ability distributions (in our case, P(Vin) and
P(V1m)), but rather the similarity between
a joint distribution P(Xi, X2) and the cor-
responding product distribution P(Xi)P(X2)•
Hamming-type metrics (Cardie, 1993; Zavrel
and Daelemans, 1997) are intended for data
with symbolic features, since they count feature
label mismatches, whereas we are dealing fea-
ture values that are probabilities. Variations of
the value difference metric (Stanfill and Waltz,
1986) have been employed for supervised disam-
biguation (Ng and H.B. Lee, 1996; Ng, 1997);
but it is not reasonable in language modeling to
expect training data tagged with correct prob-
abilities. The Dice coefficient (Smadja et al.,
1996; D. Lin, 1998a, 1998b) is monotonic in Jac-
card&apos;s coefficient (van Rijsbergen, 1979), so its
inclusion in our experiments would be redun-
dant. Finally, we did not use the KL divergence
because it requires a smoothed base language
model.
</bodyText>
<footnote confidence="0.94045975">
3Zero would also be a reasonable choice, since it in-
dicates zero correlation between q and r. However, it
would then not be clear how to average in the estimates
of negatively correlated words in equation (1).
</footnote>
<equation confidence="0.994413">
conf (q, r, P(m)) -= q(v)r(v)P(m)
r(q,r) = E
V1 ,V2
</equation>
<page confidence="0.993433">
27
</page>
<sectionHeader confidence="0.998589" genericHeader="method">
3 Empirical Comparison
</sectionHeader>
<bodyText confidence="0.99238041025641">
We evaluated the similarity functions intro-
duced in the previous section on a binary dec-
ision task, using the same experimental frame-
work as in our previous preliminary compari-
son (Dagan et al., 1999). That is, the data
consisted of the verb-object cooccurrence pairs
in the 1988 Associated Press newswire involv-
ing the 1000 most frequent nouns, extracted
via Church&apos;s (1988) and Yarowsky&apos;s process-
ing tools. 587,833 (80%) of the pairs served
as a training set from which to calculate base
probabilities. From the other 20%, we pre-
pared test sets as follows: after discarding pairs
occurring in the training data (after all, the
point of similarity-based estimation is to deal
with unseen pairs), we split the remaining pairs
into five partitions, and replaced each noun-
verb pair (n, v1) with a noun-verb-verb triple
(n, vi, v2) such that P(v2) P(vi). The task
for the language model under evaluation was
to reconstruct which of (n, vi) and (n, v2) was
the original cooccurrence. Note that by con-
struction, (n, vi) was always the correct answer,
and furthermore, methods relying solely on uni-
gram frequencies would perform no better than
chance. Test-set performance was measured by
the error rate, defined as
—1(# of incorrect choices + (# of ties)/2) ,
where T is the number of test triple tokens in
the set, and a tie results when both alternatives
are deemed equally likely by the language model
in question.
To perform the evaluation, we incorporated
each similarity function into a decision rule as
follows. For a given similarity measure f and
neighborhood size k, let Sf,k(n) denote the k
most similar words to n according to f. We
define the evidence according to f for the cooc-
currence (n, vi) as
</bodyText>
<equation confidence="0.845373">
Ef,k(n,vi) = frn E Sf,k(n) : P(vilm) &gt;
</equation>
<bodyText confidence="0.999912434782609">
Then, the decision rule was to choose the alter-
native with the greatest evidence.
The reason we used a restricted version of the
distance-weighted averaging model was that we
sought to discover fundamental differences in
behavior. Because we have a binary decision
task, Ef,k(n, vi) simply counts the number of k
nearest neighbors to n that make the right de-
cision. If we have two functions f and g such
that Ef,k(n,vi) &gt; Eg,k(n, v1), then the k most
similar words according to f are on the whole
better predictors than the k most similar words
according to g; hence, f induces an inherently
better similarity ranking for distance-weighted
averaging. The difficulty with using the full
model (Equation (1)) for comparison purposes
is that fundamental differences can be obscured
by issues of weighting. For example, suppose
the probability estimate E (2 — Li (q,r)) • r(v)
(suitably normalized) performed poorly. We
would not be able to tell whether the cause
was an inherent deficiency in the L1 norm or
just a poor choice of weight function — per-
haps (2 — Li (q, r))2 would have yielded better
estimates.
Figure 2 shows how the average error rate
varies with k for the seven similarity metrics
introduced above. As previously mentioned, a
steeper slope indicates a better similarity rank-
ing.
All the curves have a generally upward trend
but always lie far below backoff (51% error
rate). They meet at k = 1000 because Sf JD:x:1(n)
is always the set of all nouns. We see that the
functions fall into four groups: (1) the L2 norm;
(2) Kendall&apos;s T; (3) the confusion probability
and the cosine metric; and (4) the L1 norm,
Jensen-Shannon divergence, and Jaccard&apos;s co-
efficient.
We can account for the similar performance
of various metrics by analyzing how they incor-
porate information from the intersection of the
supports of q and r. (Recall that we are using
q and r for the conditional verb cooccurrrence
probabilities of two nouns n and m.) Consider
the following supports (illustrated in Figure 3):
</bodyText>
<equation confidence="0.96374">
Vq = {v E V : q(v) &gt; 0}
V, = {v E V : r(v) &gt; 0}
Vqr = {v €V : q(v)r(v) &gt; 01 = n vr
</equation>
<bodyText confidence="0.986233">
We can rewrite the similarity functions from
Section 2 in terms of these sets, making use of
the identities E vEVqWqr q(v) + EvEVqr q(V) =
EvEvrwqr r(v) + EvEVqr r(v) = 1. Table 1 lists
these alternative forms in order of performance.
</bodyText>
<page confidence="0.996638">
28
</page>
<figure confidence="0.999312">
0.4 Error rates (averages and ranges)
average error rate
0.36 -
0.34 -
0.32 -
tau
cont -a—
cos
L1
-w—
Jaccard -a-
0.3
0.28
0.26
100 200 300 400 500 600 700 800 900 1000
</figure>
<figureCaption confidence="0.9961465">
Figure 2: Similarity metric performance. Errorbars denote the range of error rates over the five
test sets. Backoff&apos;s average error rate was 51%.
</figureCaption>
<table confidence="0.999221235294118">
L2(q, r)
. \IEq(v)2 — 2Eq(v)r(v) + Er(v)2
vg Vqr Vr
r(q, r) • 2(I1) = 2 I Vqr I IV \ (Vg Li Vr )1 — 2 1 Vq \ Vqr 1 1 iir \ VV. 1
+ E E sign[(q(vi) — q(v2))(r(vi) — r(v2))]
viE(VqAVr) v2EVq,
+ E E sign[(q(vi) — q(v2))(r(vi) — r(v2))1
viEVqr v2EliqUVr
conf (q, r, P(m)) = P (m) E q(v)r(v)/P(v)
cos (q, r) voigr
= E q(v)r (v)( E q(v)2 E r(v)2)-1/2
vEvqr vEv, vEvr
Li(q, r) = 2 — E (Ig(v) - r(v)I — q(v) — r(v)) h(x) = —slog x
J S (q, r) vEvgr
Jac(q,r) -= log 2 + i E (h(q(v) + r (v)) — h(q(v)) — h(r(v))) ,
vEvq,
= Ivqr1/117qUVr1
</table>
<tableCaption confidence="0.9209415">
Table 1: Similarity functions, written in terms of sums over supports and grouped by average
performance. \ denotes set difference; A denotes symmetric set difference.
</tableCaption>
<bodyText confidence="0.9995924">
We see that for the non-combinatorial functions,
the groups correspond to the degree to which
the measures rely on the verbs in Vqr. The
Jensen-Shannon divergence and the L1 norm
can be computed simply by knowing the val-
ues of q and r on Vqr. For the cosine and the
confusion probability, the distribution values on
Vqr are key, but other information is also incor-
porated. The statistic Ta takes into account all
verbs, including those that occur neither with
</bodyText>
<page confidence="0.998171">
29
</page>
<figureCaption confidence="0.932443107142857">
We can think of a as a degree of confidence
in the empirical distribution q; or, equivalently,
(1 — a) can be thought of as controlling the
amount by which one smooths q by r. Thus,
we can view the skew divergence as an approx-
imation to the KL divergence to be used when
sparse data problems would cause the latter
measure to be undefined.
Figure 4 shows the performance of 8, for
a = .99. It performs better than all the other
functions; the difference with respect to Jac-
card&apos;s coefficient is statistically significant, ac-
cording to the paired t-test, at all k (except
k = 1000), with significance level .01 at all k
except 100, 400, and 1000.
5 Discussion
In this paper, we empirically evaluated a num-
ber of distributional similarity measures, includ-
ing the skew divergence, and analyzed their in-
formation sources. We observed that the ability
of a similarity function f (q, r) to select useful
nearest neighbors appears to be correlated with
its focus on the intersection Vv. of the supports
of q and r. This is of interest from a computa-
tional point of view because Vqr tends to be a
relatively small subset of V, the set of all verbs.
Furthermore, it suggests downplaying the role of
negative information, which is encoded by verbs
appearing with exactly one noun, although the
Jaccard coefficient does take this type of infor-
mation into account.
Our explicit division of V-space into vari-
ous support regions has been implicitly con-
sidered in other work. Smadja et al. (1996)
observe that for two potential mutual transla-
tions X and Y, the fact that X occurs with
translation Y indicates association; X&apos;s occur-
ring with a translation other than Y decreases
one&apos;s belief in their association; but the absence
of both X and Y yields no information. In
essence, Smadja et al. argue that information
from the union of supports, rather than the just
the intersection, is important. D. Lin (1997;
1998a) takes an axiomatic approach to deter-
mining the characteristics of a good similarity
measure. Starting with a formalization (based
on certain assumptions) of the intuition that the
similarity between two events depends on both
their commonality and their differences, he de-
rives a unique similarity function schema. The
Figure 3: Supports on V
n nor m. Finally, the Euclidean distance is
quadratic in verbs outside Vqr; indeed, Kaufman
and Rousseeuw (1990) note that it is &amp;quot;extremely
sensitive to the effect of one or more outliers&amp;quot;
(pg. 117).
</figureCaption>
<bodyText confidence="0.9673942">
The superior performance of Jac(q, r) seems
to underscore the importance of the set Vqr.
Jaccard&apos;s coefficient ignores the values of q and
r on Vqr; but we see that simply knowing the
size of Vqr relative to the supports of q and r
leads to good rankings.
4 The Skew Divergence
Based on the results just described, it appears
that it is desirable to have a similarity func-
tion that focuses on the verbs that cooccur with
both of the nouns being compared. However,
we can make a further observation: with the
exception of the confusion probability, all the
functions we compared are symmetric, that is,
f (q, r) = f (r, q). But the substitutability of
one word for another need not symmetric. For
instance, &amp;quot;fruit&amp;quot; may be the best possible ap-
proximation to &amp;quot;apple&amp;quot;, but the distribution of
&amp;quot;apple&amp;quot; may not be a suitable proxy for the dis-
tribution of &amp;quot;fruit&amp;quot; .4
In accordance with this insight, we developed
a novel asymmetric generalization of the KL di-
vergence, the a-skew divergence:
scr(q,r) = D (r Ia.q+(1 —a) r)
for 0 &lt; a &lt; 1. It can easily be shown that sc,
depends only on the verbs in Vqr. Note that at
a = 1, the skew divergence is exactly the KL di-
vergence, and s1/2 is twice one of the summands
of JS (note that it is still asymmetric).
40n a related note, an anonymous reviewer cited the 30
following example from the psychology literature: we can
say Smith&apos;s lecture is like a sleeping pill, but &amp;quot;not the
other way round&amp;quot;.
average error rate
Error rates (averages and ranges)
</bodyText>
<figure confidence="0.998003235294118">
0.4
0.38 -
0.36 -
0.34 -
0.32 -
-
cont -e—
a*
L1 -4—
Js -0—
Jaccard -4—
.99-skew
0.28
0.26&apos;
0.3
100 200 300 400 500 600 700 800 900 1000
k
</figure>
<figureCaption confidence="0.999989">
Figure 4: Performance of the skew divergence with respect to the best functions from Figure 2.
</figureCaption>
<bodyText confidence="0.99991855">
definition of commonality is left to the user (sev-
eral different definitions are proposed for differ-
ent tasks).
We view the empirical approach taken in this
paper as complementary to Lin&apos;s. That is, we
are working in the context of a particular appli-
cation, and, while we have no mathematical cer-
tainty of the importance of the &amp;quot;common sup-
port&amp;quot; information, we did not assume it a priori;
rather, we let the performance data guide our
thinking.
Finally, we observe that the skew metric
seems quite promising. We conjecture that ap-
propriate values for a may inversely correspond
to the degree of sparseness in the data, and
intend in the future to test this conjecture on
larger-scale prediction tasks. We also plan to
evaluate skewed versions of the Jensen-Shannon
divergence proposed by Rao (1982) and J. Lin
(1991).
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="method">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999839375">
Thanks to Claire Cardie, Jon Kleinberg, Fer-
nando Pereira, and Stuart Shieber for helpful
discussions, the anonymous reviewers for their
insightful comments, Fernando Pereira for ac-
cess to computational resources at AT&amp;T, and
Stuart Shieber for the opportunity to pursue
this work at Harvard University under NSF
Grant No. IRI9712068.
</bodyText>
<sectionHeader confidence="0.987761" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.8239707">
Claire Cardie. 1993. A case-based approach
to knowledge acquisition for domain-specific
sentence analysis. In 11th National Confer-
ence on Artifical Intelligence, pages 798-803.
Kenneth Ward Church and Patrick Hanks.
1990. Word association norms, mutual in-
formation, and lexicography. Computational
Linguistics, 16(1):22-29.
Kenneth W. Church. 1988. A stochastic parts
program and noun phrase parser for un-
restricted text. In Second Conference on
Applied Natural Language Processing, pages
136-143.
Thomas M. Cover and Joy A. Thomas. 1991.
Elements of Information Theory. John Wiley.
Ido Dagan, Shaul Marcus, and Shaul Marko-
vitch. 1995. Contextual word similarity
and estimation from sparse data. Computer
Speech and Language, 9:123-152.
Ido Dagan, Lillian Lee, and Fernando Pereira.
1999. Similarity-based models of cooccur-
rence probabilities. Machine Learning, 34(1-
3):43-69.
Ute Essen and Volker Steinbiss. 1992. Co-
occurrence smoothing for stochastic language
modeling. In ICASSP 92, volume 1, pages
161-164.
Jean Dickinson Gibbons. 1993. Nonparametric
Measures of Association. Sage University Pa-
per series on Quantitative Applications in the
</bodyText>
<page confidence="0.999753">
31
</page>
<bodyText confidence="0.9867068">
Social Sciences, 07-091. Sage Publications.
Ralph Grishman and John Sterling. 1993.
Smoothing of automatically generated selec-
tional constraints. In Human Language Tech-
nology: Proceedings of the ARPA Workshop,
pages 254-259.
Vasileios Hatzivassiloglou and Kathleen McKe-
own. 1993. Towards the automatic identifica-
tion of adjectival scales: Clustering of adjec-
tives according to meaning. In 31st Annual
Meeting of the ACL, pages 172-182.
Vasileios Hatzivassiloglou. 1996. Do we need
linguistics when we have statistics? A com-
parative analysis of the contributions of lin-
guistic cues to a statistical word grouping
</bodyText>
<reference confidence="0.99556511627907">
system. In Judith L. Klavans and Philip
Resnik, editors, The Balancing Act, pages 67-
94. MIT Press.
Don Hindle. 1990. Noun classification from
predicate-argument structures. In 28th An-
nual Meeting of the ACL, pages 268-275.
Frederick Jelinek and Robert L. Mercer. 1980.
Interpolated estimation of Markov source pa-
rameters from sparse data. In Proceedings
of the Workshop on Pattern Recognition in
Practice.
William P. Jones and George W. Furnas.
1987. Pictures of relevance. Journal of the
American Society for Information Science,
38(6):420-442.
Yael Karov and Shimon Edelman. 1998.
Similarity-based word sense disambiguation.
Computational Linguistics, 24(1):41-59.
Slava M. Katz. 1987. Estimation of probabili-
ties from sparse data for the language model
component of a speech recognizer. IEEE
Transactions on Acoustics, Speech and Signal
Processing, ASSP-35(3):400-401, March.
Leonard Kaufman and Peter J. Rousseeuw.
1990. Finding Groups in Data: An Intro-
duction to Cluster Analysis. John Wiley and
Sons.
Lillian Lee. 1997. Similarity-Based Approaches
to Natural Language Processing. Ph.D. the-
sis, Harvard University.
Dekang Lin. 1997. Using syntactic dependency
as local context to resolve word sense ambi-
guity. In 35th Annual Meeting of the ACL,
pages 64-71.
Dekang Lin. 1998a. Automatic retrieval and
clustering of similar words. In COLING-ACL
&apos;98, pages 768-773.
Dekang Lin. 1998b. An information theoretic
definition of similarity. In Machine Learn-
ing: Proceedings of the Fiftheenth Interna-
tional Conference (ICML &apos;98).
Jianhua Lin. 1991. Divergence measures based
on the Shannon entropy. IEEE Transactions
on Information Theory, 37(1):145-151.
Alpha K. Luk. 1995. Statistical sense disam-
biguation with relatively small corpora using
dictionary definitions. In 33rd Annual Meet-
ing of the ACL, pages 181-188.
Hwee Tou Ng and Hian Beng Lee. 1996. Inte-
grating multiple knowledge sources to disam-
biguate word sense: An exemplar-based ap-
proach. In 34th Annual Meeting of the ACL,
pages 40-47.
Hwee Tou Ng. 1997. Exemplar-based word
sense disambiguation: Some recent improve-
ments. In Second Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP-2), pages 208-213.
C. Ralhakrishna Rao. 1982. Diversity: Its
measurement, decomposition, apportionment
and analysis. Sankyha: The Indian Journal
of Statistics, 44(A):1-22.
Philip Resnik. 1995. Using information content
to evaluate semantic similarity in a taxonomy.
In Proceedings of IJCAI-95, pages 448-453.
Gerard Salton and Michael J. McGill. 1983. In-
troduction to Modern Information Retrieval.
McGraw-Hill.
Frank Smadja, Kathleen R. McKeown, and
Vasileios Hatzivassiloglou. 1996. Translat-
ing collocations for bilingual lexicons: A sta-
tistical approach. Computational Linguistics,
22(1):1-38.
Craig Stanfill and David Waltz. 1986. To-
ward memory-based reasoning. Communica-
tions of the ACM, 29(12):1213-1228.
K. Sugawara, M. Nishimura, K. Toshioka,
M. Okochi, and T. Kaneko. 1985. Isolated
word recognition using hidden Markov mod-
els. In ICASSP 85, pages 1-4.
C. J. van Rijsbergen. 1979. Information Re-
trieval. Butterworths, second edition.
Jakub Zavrel and Walter Daelemans. 1997.
Memory-based learning: Using similarity for
smoothing. In 35th Annual Meeting of the
ACL, pages 436-443.
</reference>
<page confidence="0.999299">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.787921">
<title confidence="0.999098">Measures of Distributional Similarity</title>
<author confidence="0.999991">Lillian Lee</author>
<affiliation confidence="0.999451">Department of Computer Science Cornell University</affiliation>
<address confidence="0.99975">Ithaca, NY 14853-7501</address>
<email confidence="0.99983">llee@cs.cornell.edu</email>
<abstract confidence="0.998782666666667">distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences. Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification</abstract>
<intro confidence="0.794305">of similarity functions based on the information</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>The Balancing Act,</booktitle>
<pages>67--94</pages>
<editor>system. In Judith L. Klavans and Philip Resnik, editors,</editor>
<publisher>MIT Press.</publisher>
<marker></marker>
<rawString>system. In Judith L. Klavans and Philip Resnik, editors, The Balancing Act, pages 67-94. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In 28th Annual Meeting of the ACL,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="9662" citStr="Hindle, 1990" startWordPosition="1551" endWordPosition="1552"> 0 at 0. The intuition behind Kendall&apos;s T is as follows. Assume all verbs have distinct conditional probabilities. If sorting the verbs by the likelihoods assigned by q yields exactly the same ordering as that which results from ranking them according to r, then T (q, r) = 1; if it yields exactly the opposite ordering, then T(q,r) = —1. We treat a value of —1 as indicating extreme dissimilarity.3 It is worth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments. Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a). It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities. Variations of the value di</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Don Hindle. 1990. Noun classification from predicate-argument structures. In 28th Annual Meeting of the ACL, pages 268-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Pattern Recognition in Practice.</booktitle>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Jones</author>
<author>George W Furnas</author>
</authors>
<title>Pictures of relevance.</title>
<date>1987</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>38--6</pages>
<contexts>
<context position="3632" citStr="Jones and Furnas, 1987" startWordPosition="551" endWordPosition="555">ty of automatically-constructed thesauri to a statistically significant degree (1998a) and the ability to determine common morphological roots by as much as 49% in precision (1998b). 3-The term &amp;quot;similarity-based&amp;quot;, which we have used previously, has been applied to describe other models as well (L. Lee, 1997; Karov and Edelman, 1998). 25 These empirical results indicate that investigating different similarity measures can lead to improved natural language processing. On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987), there has been some doubt expressed in that community that the choice of similarity metric has any practical impact: Several authors have pointed out that the difference in retrieval performance achieved by different measures of association is insignificant, providing that these are appropriately normalised. (van Rijsbergen, 1979, pg. 38) But no contradiction arises because, as van Rijsbergen continues, &amp;quot;one would expect this since most measures incorporate the same information&amp;quot;. In the language-modeling domain, there is currently no agreed-upon best similarity metric because there is no agr</context>
</contexts>
<marker>Jones, Furnas, 1987</marker>
<rawString>William P. Jones and George W. Furnas. 1987. Pictures of relevance. Journal of the American Society for Information Science, 38(6):420-442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yael Karov</author>
<author>Shimon Edelman</author>
</authors>
<title>Similarity-based word sense disambiguation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<contexts>
<context position="3343" citStr="Karov and Edelman, 1998" startWordPosition="509" endWordPosition="512">, by using a restricted version of model (1) that stripped incomparable parameters, we were able to empirically demonstrate that the confusion probability is fundamentally worse at selecting useful similar words. D. Lin also found that the choice of similarity function can affect the quality of automatically-constructed thesauri to a statistically significant degree (1998a) and the ability to determine common morphological roots by as much as 49% in precision (1998b). 3-The term &amp;quot;similarity-based&amp;quot;, which we have used previously, has been applied to describe other models as well (L. Lee, 1997; Karov and Edelman, 1998). 25 These empirical results indicate that investigating different similarity measures can lead to improved natural language processing. On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987), there has been some doubt expressed in that community that the choice of similarity metric has any practical impact: Several authors have pointed out that the difference in retrieval performance achieved by different measures of association is insignificant, providing that these are appropriately normalised.</context>
</contexts>
<marker>Karov, Edelman, 1998</marker>
<rawString>Yael Karov and Shimon Edelman. 1998. Similarity-based word sense disambiguation. Computational Linguistics, 24(1):41-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<pages>35--3</pages>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP-35(3):400-401, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Kaufman</author>
<author>Peter J Rousseeuw</author>
</authors>
<title>Finding Groups in Data: An Introduction to Cluster Analysis.</title>
<date>1990</date>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="18959" citStr="Kaufman and Rousseeuw (1990)" startWordPosition="3183" endWordPosition="3186">ields no information. In essence, Smadja et al. argue that information from the union of supports, rather than the just the intersection, is important. D. Lin (1997; 1998a) takes an axiomatic approach to determining the characteristics of a good similarity measure. Starting with a formalization (based on certain assumptions) of the intuition that the similarity between two events depends on both their commonality and their differences, he derives a unique similarity function schema. The Figure 3: Supports on V n nor m. Finally, the Euclidean distance is quadratic in verbs outside Vqr; indeed, Kaufman and Rousseeuw (1990) note that it is &amp;quot;extremely sensitive to the effect of one or more outliers&amp;quot; (pg. 117). The superior performance of Jac(q, r) seems to underscore the importance of the set Vqr. Jaccard&apos;s coefficient ignores the values of q and r on Vqr; but we see that simply knowing the size of Vqr relative to the supports of q and r leads to good rankings. 4 The Skew Divergence Based on the results just described, it appears that it is desirable to have a similarity function that focuses on the verbs that cooccur with both of the nouns being compared. However, we can make a further observation: with the exce</context>
</contexts>
<marker>Kaufman, Rousseeuw, 1990</marker>
<rawString>Leonard Kaufman and Peter J. Rousseeuw. 1990. Finding Groups in Data: An Introduction to Cluster Analysis. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Similarity-Based Approaches to Natural Language Processing.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="3317" citStr="Lee, 1997" startWordPosition="507" endWordPosition="508">Furthermore, by using a restricted version of model (1) that stripped incomparable parameters, we were able to empirically demonstrate that the confusion probability is fundamentally worse at selecting useful similar words. D. Lin also found that the choice of similarity function can affect the quality of automatically-constructed thesauri to a statistically significant degree (1998a) and the ability to determine common morphological roots by as much as 49% in precision (1998b). 3-The term &amp;quot;similarity-based&amp;quot;, which we have used previously, has been applied to describe other models as well (L. Lee, 1997; Karov and Edelman, 1998). 25 These empirical results indicate that investigating different similarity measures can lead to improved natural language processing. On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987), there has been some doubt expressed in that community that the choice of similarity metric has any practical impact: Several authors have pointed out that the difference in retrieval performance achieved by different measures of association is insignificant, providing that these are</context>
</contexts>
<marker>Lee, 1997</marker>
<rawString>Lillian Lee. 1997. Similarity-Based Approaches to Natural Language Processing. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity.</title>
<date>1997</date>
<booktitle>In 35th Annual Meeting of the ACL,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="18495" citStr="Lin (1997" startWordPosition="3113" endWordPosition="3114">ccard coefficient does take this type of information into account. Our explicit division of V-space into various support regions has been implicitly considered in other work. Smadja et al. (1996) observe that for two potential mutual translations X and Y, the fact that X occurs with translation Y indicates association; X&apos;s occurring with a translation other than Y decreases one&apos;s belief in their association; but the absence of both X and Y yields no information. In essence, Smadja et al. argue that information from the union of supports, rather than the just the intersection, is important. D. Lin (1997; 1998a) takes an axiomatic approach to determining the characteristics of a good similarity measure. Starting with a formalization (based on certain assumptions) of the intuition that the similarity between two events depends on both their commonality and their differences, he derives a unique similarity function schema. The Figure 3: Supports on V n nor m. Finally, the Euclidean distance is quadratic in verbs outside Vqr; indeed, Kaufman and Rousseeuw (1990) note that it is &amp;quot;extremely sensitive to the effect of one or more outliers&amp;quot; (pg. 117). The superior performance of Jac(q, r) seems to u</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>Dekang Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In 35th Annual Meeting of the ACL, pages 64-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL &apos;98,</booktitle>
<pages>768--773</pages>
<contexts>
<context position="9731" citStr="Lin, 1998" startWordPosition="1564" endWordPosition="1565">s have distinct conditional probabilities. If sorting the verbs by the likelihoods assigned by q yields exactly the same ordering as that which results from ranking them according to r, then T (q, r) = 1; if it yields exactly the opposite ordering, then T(q,r) = —1. We treat a value of —1 as indicating extreme dissimilarity.3 It is worth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments. Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a). It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for sup</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998a. Automatic retrieval and clustering of similar words. In COLING-ACL &apos;98, pages 768-773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Machine Learning: Proceedings of the Fiftheenth International Conference (ICML &apos;98).</booktitle>
<contexts>
<context position="9731" citStr="Lin, 1998" startWordPosition="1564" endWordPosition="1565">s have distinct conditional probabilities. If sorting the verbs by the likelihoods assigned by q yields exactly the same ordering as that which results from ranking them according to r, then T (q, r) = 1; if it yields exactly the opposite ordering, then T(q,r) = —1. We treat a value of —1 as indicating extreme dissimilarity.3 It is worth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments. Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a). It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for sup</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998b. An information theoretic definition of similarity. In Machine Learning: Proceedings of the Fiftheenth International Conference (ICML &apos;98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the Shannon entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>37--1</pages>
<contexts>
<context position="6853" citStr="Lin, 1991" startWordPosition="1062" endWordPosition="1063">, we write q(v) for P(v In) and r (v) for P(v1m), their respective conditional verb cooccurrence probabilities. Figure 1 lists several familiar functions. The cosine metric and Jaccard&apos;s coefficient are commonly used in information retrieval as measures of association (Salton and McGill, 1983). Note that Jaccard&apos;s coefficient differs from all the other measures we consider in that it is essentially combinatorial, being based only on the sizes of the supports of q, r, and q • r rather than the actual values of the distributions. Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: JS(q,r) = [D (q The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): • The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined. In contrast, D(q1 Ir) is undefined if q is not absolutely continuous with respect to r (i.e., the support of q is not a subset of the support of r). 2Strictly speaking, some of these fun</context>
<context position="21585" citStr="Lin (1991)" startWordPosition="3661" endWordPosition="3662">working in the context of a particular application, and, while we have no mathematical certainty of the importance of the &amp;quot;common support&amp;quot; information, we did not assume it a priori; rather, we let the performance data guide our thinking. Finally, we observe that the skew metric seems quite promising. We conjecture that appropriate values for a may inversely correspond to the degree of sparseness in the data, and intend in the future to test this conjecture on larger-scale prediction tasks. We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991). 6 Acknowledgements Thanks to Claire Cardie, Jon Kleinberg, Fernando Pereira, and Stuart Shieber for helpful discussions, the anonymous reviewers for their insightful comments, Fernando Pereira for access to computational resources at AT&amp;T, and Stuart Shieber for the opportunity to pursue this work at Harvard University under NSF Grant No. IRI9712068. References Claire Cardie. 1993. A case-based approach to knowledge acquisition for domain-specific sentence analysis. In 11th National Conference on Artifical Intelligence, pages 798-803. Kenneth Ward Church and Patrick Hanks. 1990. Word associa</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1):145-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alpha K Luk</author>
</authors>
<title>Statistical sense disambiguation with relatively small corpora using dictionary definitions.</title>
<date>1995</date>
<booktitle>In 33rd Annual Meeting of the ACL,</booktitle>
<pages>181--188</pages>
<contexts>
<context position="9717" citStr="Luk, 1995" startWordPosition="1561" endWordPosition="1562">ssume all verbs have distinct conditional probabilities. If sorting the verbs by the likelihoods assigned by q yields exactly the same ordering as that which results from ranking them according to r, then T (q, r) = 1; if it yields exactly the opposite ordering, then T(q,r) = —1. We treat a value of —1 as indicating extreme dissimilarity.3 It is worth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments. Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a). It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been em</context>
</contexts>
<marker>Luk, 1995</marker>
<rawString>Alpha K. Luk. 1995. Statistical sense disambiguation with relatively small corpora using dictionary definitions. In 33rd Annual Meeting of the ACL, pages 181-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In 34th Annual Meeting of the ACL,</booktitle>
<pages>40--47</pages>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In 34th Annual Meeting of the ACL, pages 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
</authors>
<title>Exemplar-based word sense disambiguation: Some recent improvements.</title>
<date>1997</date>
<booktitle>In Second Conference on Empirical Methods in Natural Language Processing (EMNLP-2),</booktitle>
<pages>208--213</pages>
<contexts>
<context position="10387" citStr="Ng, 1997" startWordPosition="1664" endWordPosition="1665">cause it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities. The Dice coefficient (Smadja et al., 1996; D. Lin, 1998a, 1998b) is monotonic in Jaccard&apos;s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant. Finally, we did not use the KL divergence because it requires a smoothed base language model. 3Zero would also be a reasonable choice, since it indicates zero correlation between q and r. However, it would then not be clear how to average in the estimates of negatively correlated words in equation (1). conf (</context>
</contexts>
<marker>Ng, 1997</marker>
<rawString>Hwee Tou Ng. 1997. Exemplar-based word sense disambiguation: Some recent improvements. In Second Conference on Empirical Methods in Natural Language Processing (EMNLP-2), pages 208-213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ralhakrishna Rao</author>
</authors>
<title>Diversity: Its measurement, decomposition, apportionment and analysis.</title>
<date>1982</date>
<journal>Sankyha: The Indian Journal of Statistics,</journal>
<pages>44--1</pages>
<contexts>
<context position="6838" citStr="Rao, 1982" startWordPosition="1059" endWordPosition="1060">nal simplicity, we write q(v) for P(v In) and r (v) for P(v1m), their respective conditional verb cooccurrence probabilities. Figure 1 lists several familiar functions. The cosine metric and Jaccard&apos;s coefficient are commonly used in information retrieval as measures of association (Salton and McGill, 1983). Note that Jaccard&apos;s coefficient differs from all the other measures we consider in that it is essentially combinatorial, being based only on the sizes of the supports of q, r, and q • r rather than the actual values of the distributions. Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: JS(q,r) = [D (q The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): • The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined. In contrast, D(q1 Ir) is undefined if q is not absolutely continuous with respect to r (i.e., the support of q is not a subset of the support of r). 2Strictly speaking, so</context>
<context position="21567" citStr="Rao (1982)" startWordPosition="3657" endWordPosition="3658">. That is, we are working in the context of a particular application, and, while we have no mathematical certainty of the importance of the &amp;quot;common support&amp;quot; information, we did not assume it a priori; rather, we let the performance data guide our thinking. Finally, we observe that the skew metric seems quite promising. We conjecture that appropriate values for a may inversely correspond to the degree of sparseness in the data, and intend in the future to test this conjecture on larger-scale prediction tasks. We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991). 6 Acknowledgements Thanks to Claire Cardie, Jon Kleinberg, Fernando Pereira, and Stuart Shieber for helpful discussions, the anonymous reviewers for their insightful comments, Fernando Pereira for access to computational resources at AT&amp;T, and Stuart Shieber for the opportunity to pursue this work at Harvard University under NSF Grant No. IRI9712068. References Claire Cardie. 1993. A case-based approach to knowledge acquisition for domain-specific sentence analysis. In 11th National Conference on Artifical Intelligence, pages 798-803. Kenneth Ward Church and Patrick Hanks. </context>
</contexts>
<marker>Rao, 1982</marker>
<rawString>C. Ralhakrishna Rao. 1982. Diversity: Its measurement, decomposition, apportionment and analysis. Sankyha: The Indian Journal of Statistics, 44(A):1-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="1931" citStr="Resnik (1995)" startWordPosition="300" endWordPosition="301">logies are Katz&apos;s (1987) backoff method and Jelinek and Mercer&apos;s (1980) interpolation method. Both use P(v) to estimate P(v In) when (n, v) is unseen, essentially ignoring the identity of n. An alternative approach is distance-weighted averaging, which arrives at an estimate for unseen cooccurrences by combining estimates for cooccurrences involving similar words:1 sim(n, m)P(v1m) P(0) = rriES(n) EmEs(n) sim(n, m) , (1) where S(n) is a set of candidate similar words and sim(n, m) is a function of the similarity between n and m. We focus on distributional rather than semantic similarity (e.g., Resnik (1995)) because the goal of distance-weighted averaging is to smooth probability distributions — although the words &amp;quot;chance&amp;quot; and &amp;quot;probability&amp;quot; are synonyms, the former may not be a good model for predicting what cooccurrences the latter is likely to participate in. There are many plausible measures of distributional similarity. In previous work (Dagan et al., 1999), we compared the performance of three different functions: the Jensen-Shannon divergence (total divergence to the average), the L1 norm, and the confusion probability. Our experiments on a frequency-controlled pseudoword disambiguation ta</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of IJCAI-95, pages 448-453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="6537" citStr="Salton and McGill, 1983" startWordPosition="1005" endWordPosition="1008">n appears as the head noun of the direct object of v. We use P to denote probabilities assigned by a base language model (in our experiments, we simply used unsmoothed relative frequencies derived from training corpus counts). Let n and m be two nouns whose distributional similarity is to be determined; for notational simplicity, we write q(v) for P(v In) and r (v) for P(v1m), their respective conditional verb cooccurrence probabilities. Figure 1 lists several familiar functions. The cosine metric and Jaccard&apos;s coefficient are commonly used in information retrieval as measures of association (Salton and McGill, 1983). Note that Jaccard&apos;s coefficient differs from all the other measures we consider in that it is essentially combinatorial, being based only on the sizes of the supports of q, r, and q • r rather than the actual values of the distributions. Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: JS(q,r) = [D (q The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): • The function avgq denotes the ave</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen R McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="10535" citStr="Smadja et al., 1996" startWordPosition="1686" endWordPosition="1689">similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities. The Dice coefficient (Smadja et al., 1996; D. Lin, 1998a, 1998b) is monotonic in Jaccard&apos;s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant. Finally, we did not use the KL divergence because it requires a smoothed base language model. 3Zero would also be a reasonable choice, since it indicates zero correlation between q and r. However, it would then not be clear how to average in the estimates of negatively correlated words in equation (1). conf (q, r, P(m)) -= q(v)r(v)P(m) r(q,r) = E V1 ,V2 27 3 Empirical Comparison We evaluated the similarity functions introduced in the previous section on </context>
<context position="18081" citStr="Smadja et al. (1996)" startWordPosition="3040" endWordPosition="3043">y function f (q, r) to select useful nearest neighbors appears to be correlated with its focus on the intersection Vv. of the supports of q and r. This is of interest from a computational point of view because Vqr tends to be a relatively small subset of V, the set of all verbs. Furthermore, it suggests downplaying the role of negative information, which is encoded by verbs appearing with exactly one noun, although the Jaccard coefficient does take this type of information into account. Our explicit division of V-space into various support regions has been implicitly considered in other work. Smadja et al. (1996) observe that for two potential mutual translations X and Y, the fact that X occurs with translation Y indicates association; X&apos;s occurring with a translation other than Y decreases one&apos;s belief in their association; but the absence of both X and Y yields no information. In essence, Smadja et al. argue that information from the union of supports, rather than the just the intersection, is important. D. Lin (1997; 1998a) takes an axiomatic approach to determining the characteristics of a good similarity measure. Starting with a formalization (based on certain assumptions) of the intuition that t</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Stanfill</author>
<author>David Waltz</author>
</authors>
<title>Toward memory-based reasoning.</title>
<date>1986</date>
<journal>Communications of the ACM,</journal>
<pages>29--12</pages>
<contexts>
<context position="10304" citStr="Stanfill and Waltz, 1986" startWordPosition="1648" endWordPosition="1651">s, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a). It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities. The Dice coefficient (Smadja et al., 1996; D. Lin, 1998a, 1998b) is monotonic in Jaccard&apos;s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant. Finally, we did not use the KL divergence because it requires a smoothed base language model. 3Zero would also be a reasonable choice, since it indicates zero correlation between q and r. However, it would then not be clear how</context>
</contexts>
<marker>Stanfill, Waltz, 1986</marker>
<rawString>Craig Stanfill and David Waltz. 1986. Toward memory-based reasoning. Communications of the ACM, 29(12):1213-1228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sugawara</author>
<author>M Nishimura</author>
<author>K Toshioka</author>
<author>M Okochi</author>
<author>T Kaneko</author>
</authors>
<title>Isolated word recognition using hidden Markov models.</title>
<date>1985</date>
<booktitle>In ICASSP 85,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="8106" citStr="Sugawara et al., 1985" startWordPosition="1287" endWordPosition="1290">s, but each such function f can be recast as a similarity function via the simple transformation C — f, where C is an appropriate constant. Whether we mean f or C — f should be clear from context. lavgqm) + D avgq,,)] . D(pi(V) II P2(V)) =E (v) log P2 Pi ((v)V) 26 Euclidean distance L2(q,r) L L1 norm i(q, r) cosine cos (q, r) Jaccard&apos;s coefficient Jac(q, r) = = E lq(v) _ r(v)I q(v)r(v) vE2, q(v)2 vEv r(v)2 1{v : q(v) &gt; 0 and r(v) &gt; 011 I q(v) &gt; 0 or r(v) &gt;0)1 Figure 1: Well-known functions 1, The confusion probability has been used by several authors to smooth word cooccurrence probabilities (Sugawara et al., 1985; Essen and Steinbiss, 1992; Grishman and Sterling, 1993); it measures the degree to which word m can be substituted into the contexts in which n appears. If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al., 1999), as is the case for relative frequencies, then we may write the confusion probability as follows: P(v) Note that it incorporates unigram probabilities as well as the two distributions q and r. Finally, Kendall&apos;s T which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a no</context>
</contexts>
<marker>Sugawara, Nishimura, Toshioka, Okochi, Kaneko, 1985</marker>
<rawString>K. Sugawara, M. Nishimura, K. Toshioka, M. Okochi, and T. Kaneko. 1985. Isolated word recognition using hidden Markov models. In ICASSP 85, pages 1-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<date>1979</date>
<note>Information Retrieval. Butterworths, second edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. Butterworths, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Memory-based learning: Using similarity for smoothing.</title>
<date>1997</date>
<booktitle>In 35th Annual Meeting of the ACL,</booktitle>
<pages>436--443</pages>
<contexts>
<context position="10084" citStr="Zavrel and Daelemans, 1997" startWordPosition="1615" endWordPosition="1618">orth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments. Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a). It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities. The Dice coefficient (Smadja et al., 1996; D. Lin, 1998a, 1998b) is monotonic in Jaccard&apos;s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant. Finally</context>
</contexts>
<marker>Zavrel, Daelemans, 1997</marker>
<rawString>Jakub Zavrel and Walter Daelemans. 1997. Memory-based learning: Using similarity for smoothing. In 35th Annual Meeting of the ACL, pages 436-443.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>