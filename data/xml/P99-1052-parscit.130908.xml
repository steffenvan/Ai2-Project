<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000306">
<title confidence="0.995014">
Charting the Depths of Robust Speech Parsing
</title>
<author confidence="0.998083">
W. Kaspert, B. Kiefert, H.-U. Kriegert, C. J. Ruppt, and K. L. Worint
</author>
<affiliation confidence="0.9683395">
tGerman Research Center for Artificial Intelligence (DFKI)
IComputational Linguistics Department, Universitat des Saarlandes
</affiliation>
<email confidence="0.89403">
fkasper,kiefer,kriegerlOdfki.de and {cj,worm}@coli.uni-sb.de
</email>
<sectionHeader confidence="0.992152" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998649">
We describe a novel method for coping with un-
grammatical input based on the use of chart-like
data structures, which permit anytime process-
ing. Priority is given to deep syntactic anal-
ysis. Should this fail, the best partial analy-
ses are selected, according to a shortest-paths
algorithm, and assembled in a robust process-
ing phase. The method has been applied in
a speech translation project with large HPSG
grammars.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996707666666667">
This paper describes a new method of deal-
ing robustly with deficient speech or text in-
put, which may be due to recognition errors,
spontaneous speech phenomena, or ungrammat-
ical constructions. Two key features of this ap-
proach are:
</bodyText>
<listItem confidence="0.9836805">
• the priority given to a deep and restrictive
grammatical analysis,
• and the use of chart-like data structures at
each level of processing.
</listItem>
<bodyText confidence="0.998872305555556">
The initial input is taken from a Word Hy-
pothesis Graph, or WHG, (Oerder and Ney,
1993) from which the best ranked paths are
successively selected until a result is found or
a time limit proportional to the length of the
utterancel is reached. Each path is parsed with
an incremental chart parser that uses a Head-
Driven Phrase Structure grammar (HPSG).
The parser is adapted to input from WHGs and
optimized to meet the needs of real-time speech
processing. Since the goal of the parsing compo-
nent is to process as many WHG paths as pos-
sible, in order to find a grammatical utterance
iThis is currently up to four times real time.
and analyze it with highest accuracy, neither
relaxation of the constraints imposed by the
grammar nor repair rules are used at this stage.
If the analysis of the current path is successful,
the parsing process is complete. However, in
most cases there is no spanning and syntacti-
cally correct analysis. So a sequence of partial
analyses is determined by incrementally eval-
uating the passive edges in the parser&apos;s chart.
These analyzed fragments are passed on to a
robust semantic processing component for fur-
ther treatment, while the next best WHO path
is analyzed by the parser2. Robust semantic
processing similarly builds up a chart-like data
structure including analyzed fragments and the
results of applying robustness rules at the se-
mantic level. After the first path of the WHG
has been (unsuccessfully) analyzed, processing
in both the restrictive parser and the robust-
ness component proceeds in parallel, with the
aid of a parallel virtual machine, until one of
the following conditions is fulfilled:
</bodyText>
<listItem confidence="0.999476">
1. a spanning grammatical analysis is found,
2. all the WHG paths have been explored, or
3. the time limit is reached.
</listItem>
<bodyText confidence="0.998611125">
In the case of either of the latter two condi-
tions, robust semantic processing is allowed a
limited time to complete processing and then
the best result or sequence of results is selected
from its chart.
Our approach has been implemented in
VERBMOBIL (Wahlster, 1993), a large scale re-
search project in the area of spoken language
</bodyText>
<footnote confidence="0.999258">
2This means that the maximal sequential delay be-
tween parsing and robust semantics processing is the
parse time for one path. Similarly, the limit on pars-
ing time, essentially, applies to both components
</footnote>
<page confidence="0.998405">
405
</page>
<bodyText confidence="0.998224444444445">
translation. Its goal is to develop a system that
translates negotiation dialogues between Ger-
man, English, and Japanese speakers in face-
to-face or video conferencing situations. This
application highlights the basic problem asso-
ciated with machine processing of spontaneous
speech, namely that the input to the natural
language processing component is perturbed by
two influences:
</bodyText>
<listItem confidence="0.990522428571429">
1. Speakers make mistakes, correct them-
selves during speaking, produce false starts
and use ungrammatical constructions.
2. The acoustic signal produced by a human
speaker is mapped by a speech recognizer
onto a written form; this mapping is rarely
completely correct.
</listItem>
<bodyText confidence="0.999981120689656">
This introduces two levels of uncertainty into
the processing of speech, which make the task
of linguistically analyzing a spoken utterance in
a speech processing system doubly hard. In ad-
dition, the dialogue context imposes strict time
constraints, as the overall system must attempt
to emulate real time performance.
The strategy we adopt responds to time con-
straints by universally incorporating an anytime
property (Dean and Boddy, 1988) into the se-
lection procedures. As will be seen, this prop-
erty derives from the way in which intermedi-
ate results are stored and the selections which
can be made from among these. However, the
overriding priority of this same strategy is to
maximize the chance that a truly grammatical
path will be found and analyzed, if one exists
in the WHG. This means that while we have
implemented extensive mechanisms to achieve
robustness, their design, and in particular the
separation of processing into a restrictive parser
and a robust postprocessor, are subservient to
the cases where a fully grammatical analysis is
possible, since these results are in any case bet-
ter. These decisions may be in conflict with
much of the literature on robust parsing (e.g.,
(Hindle, 1983; Hipp, 1993; Heinecke et al.,
1998)), but the alternative of relaxing the pars-
ing constraints would appear to be a dead end
in the context of the VERBMOBIL architecture.
In the first place, the chances of locating the
best grammatical path in the lattice would be
reduced, e.g., by the acceptance of a preceding
ungrammatical one. Secondly, a more liberal
parser would raise the spectre of an explosion
of edges in the parser&apos;s chart, so that in fact
less paths could be processed overall, regardless
of their quality. Either of these conditions could
prove fatal.
This paper focuses on the aspects of the
VERBMOBIL analysis component which ensure
that the most accurate results available are pro-
vided to the system as a whole. We first de-
scribe the basic inventory we need to explain
our approach: the unification-based bottom-up
chart parser, the HPSG grammar, and the in-
terface terms which are exchanged between the
parser and the robust semantic processing. Af-
ter that, we come to the basic algorithm which
determines best partial analyses. We also give
an example of how the evaluation function on
edges might look. In section 4, we focus on
the robust semantic processing whose task is
to store and combine the partial results, before
choosing a final result out of a set of possible
candidates. We end this paper by presenting
empirical results on the usefulness of our ap-
proach.
</bodyText>
<sectionHeader confidence="0.99852" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.999272">
2.1 The Chart Parser
</subsectionHeader>
<bodyText confidence="0.999646086956522">
The parser used in the system is a bottom-
up chart parser. Since the grammar is a pure
unification-based grammar, there is no context-
free backbone and the chart edges are labelled
with typed feature structures. At the moment,
there is no local ambiguity packing of chart
edges. Therefore, the worst case complexity of
parsing is potentially exponential, but since the
parser employs a best-first strategy, exponential
behavior is rarely found in practice.
The parser provides a flexible priority system
for guiding the parsing process, using parsing
tasks on an agenda. A parsing task represents
the combination of a passive chart edge and an
active chart edge or a rule. When such a com-
bination succeeds, new tasks are generated and
for each new task, a priority is assigned.
This priority system helps to obtain good par-
tial results, even in cases where the search space
cannot be fully explored due to parsing time re-
strictions. A higher time bound would allow
either the processing of more WHG paths or a
more elaborate analysis of the given input, both
</bodyText>
<page confidence="0.99539">
406
</page>
<bodyText confidence="0.999942235294118">
of which may lead to better results. The deci-
sion when to switch to the next best path of a
given WHG depends on the length of the input
and on the time already used. After the pars-
ing of one path is finished, the passive edges of
the chart form a directed acyclic graph which is
directly used as input to compute best partial
analyses.
We note here that the parser processes the n-
best paths of a WHG fully incrementally. I.e.,
when the analysis of a new input path begins,
only those input items are added to the chart
that have not been part of a previously treated
path. Everything else that has been computed
up to that point remains in the chart and can
be used to process the new input without being
recomputed.
</bodyText>
<subsectionHeader confidence="0.997976">
2.2 The HPSG Grammars
</subsectionHeader>
<bodyText confidence="0.999994">
The grammars for English, German, and
Japanese follow the paradigm of HPSG (Pol-
lard and Sag, 1994) which is the most advanced
unification-based grammatical theory based on
typed feature structures. The fundamental con-
cept is that of a sign, a structure incorporating
information from all levels of linguistic analysis,
such as phonology, morphology, syntax, and se-
mantics. This structure makes all information
simultaneously available and provides declara-
tive interfaces between these levels. The gram-
mars use Minimal Recursion Semantics (Copes-
take et al., 1996) as the semantic representation
formalism, allowing us to deal with ambiguity
by underspecification.
To give an impression of the size of gram-
mars, we present the numbers for the German
grammar. It consists of 2,389 types, 76 rule
schemata, 4,284 stems and an average of six
entries per stem. Morphological information is
computed online which further increases the lex-
ical ambiguity.
</bodyText>
<subsectionHeader confidence="0.998691">
2.3 Partial Analyses and the
Syntax-Semantics Interface
</subsectionHeader>
<bodyText confidence="0.9991807">
Our architecture requires that the linguistic
analysis module is capable of delivering not just
analyses of complete utterances, but also of
phrases and even of lexical items in the special
interface format of VITs (VERBMOBIL Interface
Terms) (Bos et al., 1998). There are three con-
siderations which the interface has to take into
account:
1. Only maximal projections, i.e., complete
phrases, are candidates for robust process-
ing. This qualifies, e.g., prepositional and
noun phrases. On the other hand, this
approach leaves gaps in the coverage of
the input string as not every word needs
to be dominated by a maximal projec-
tion. In particular, verbal projections be-
low the sentential level usually are incom-
plete phrases. The use of intermediate, in-
complete projections is avoided for several
reasons:
</bodyText>
<listItem confidence="0.922560227272727">
• intermediate projections are highly
grammar and language specific and
• there are too many of them.
2. Phrases must be distinguished from ellipti-
cal utterances. A major difference is that
elliptical utterances express a speech act.
E.g., a prepositional phrase can be a com-
plete utterance expressing an answer to a
question (On Monday.) or a question itself
(On Monday?). If the phrase occurs in a
sentence, it is not associated with a speech
act of its own. This distinction is dealt with
in the grammars by specifying special types
for these complete utterances, phrases, and
lexical items.
3. For robust processing, the interface must
export a certain amount of information
from syntax and morphology together with
the semantics of the phrase. In addition,
it is necessary to represent semantically
empty parts of speech, e.g., separable verb
prefixes in German.
</listItem>
<sectionHeader confidence="0.915176" genericHeader="method">
3 Computing Best Partial Analyses
</sectionHeader>
<bodyText confidence="0.999244923076923">
In contrast to a traditional parser which never
comes up with an analysis for input not cov-
ered by the grammar, our approach focuses on
partial analyses without giving up the correct-
ness of the overall deep grammar. These par-
tial analyses are combined in a later stage (see
Section 4) to form total analyses. But what
is a partial analysis? Obviously a derivation
(sub)tree licensed by the grammar which covers
a continuous part of the input (i.e., a passive
parser edge). But not every passive edge is a
good candidate, since otherwise we would end
up with perhaps thousands of them. Our ap-
</bodyText>
<page confidence="0.992919">
407
</page>
<bodyText confidence="0.99774476">
proach lies in between these two extremes: com-
puting a connected sequence of best partial anal-
yses which cover the whole input. The idea here
is to view the set of passive edges of a parser
as a directed graph which needs to be evaluated
according to a user-defined (and therefore gram-
mar and language specific) metric. Using this
graph, we then compute the shortest paths w.r.t.
the evaluation function, i.e., paths through this
graph with minimum cost.
Since this graph is acyclic and topologically
sorted (vertices are integers and edges always
connect a vertex to a larger vertex), we have
chosen the DAG-shortest-path algorithm (Cor-
men et al., 1990) which runs in e(v + E). This
fast algorithm is a solution to the single-source
shortest-paths problem. We modified and ex-
tended this algorithm to cope with the needs we
encountered in speech parsing: (i) one can use
several start and end vertices (e.g., in the case
of n-best chains or WHGs); (ii) all best shortest
paths are returned (i.e., we obtain a shortest-
path subgraph); and (iii) evaluation and selec-
tion of the best edges is done incrementally as is
the case for parsing the n-best chains (i.e., only
new passive edges entered into the chart are
evaluated and may be selected by our shortest-
path algorithm).
We now sketch the basic algorithm. Let
G = (V, E) denote the set of passive edges, $
the set of start vertices, E the set of end ver-
tices, and let n be the vertex with the high-
est number (remember, vertices are integers):
n = max(V). In the algorithm, we make use
of two global vectors of length n which store
information associated with each vertex: dist
keeps track of the distance of a vertex to one
of the start vertices (the so-called shortest-path
estimate), whereas pred records the predeces-
sors of a given vertex, weight defines the cost
of an edge and is assigned its value during the
evaluation stage of our algorithm according to
the user-defined function Estimate. Finally, Adj
consists of all vertices adjacent to a given vertex
(we use an adjacency-list representation).
Clearly, before computing the shortest path,
the distance of a vertex to one of the start ver-
tices is infinity, except for the start vertices,
and there is of course no shortest path subgraph
(pred(v) 4— 0).
</bodyText>
<equation confidence="0.866579666666667">
Initialise- Single- S ource(G , S) :&lt;=&gt;
global dist, pred;
for each v E V(G) do
dist(v) 4— oo;
pred(v) 4— 0
od;
for each s E S do
dist(s) 0
od.
</equation>
<bodyText confidence="0.999874916666667">
After initialization, we perform evaluation
and relaxation on every passive edge, taken in
topologically sorted order. Relaxing an edge
(u, v) means checking whether we can improve
the shortest path(s) to v via u. There are
two cases to consider: either we overwrite the
shortest-path estimate for v since the new one
is better (and so have a new predecessor for v,
viz., u), or the shortest-path estimate is as good
as the old one, hence we have to add v to the
predecessors of v. In case the shortest-path es-
timate is worse, there is clearly nothing to do.
</bodyText>
<figure confidence="0.943849923076923">
Relax(u,v) :&lt;=&gt;
global dist, pred;
if dist(v) &gt; dist(u) + weight (u, v)
then do
dist(v) dist(u) + weight(u,v);
pred(v) E- {u}
od
else do
when dist(v) = dist(u) + weight(u,v) do
pred(v) &lt;— pred(v)U ful
od
od
a.
</figure>
<bodyText confidence="0.998994833333333">
The shortest paths are then determined by es-
timating and relaxing edges, beginning with the
start vertices S. The shortest path subgraph is
stored in pred and can be extracted by walk-
ing from the end vertices E &apos;back&apos; to the start
vertices.
</bodyText>
<equation confidence="0.935785875">
D A G- Shortest- P aths (G , 8, e)
global pred;
Initialise-Single-Source (G, S);
for each u E V(G)\E taken in topologically
sorted order do
for each v E Adj(u) do
weight(u,v) Estimate(u,v);
Relax(u,v)
</equation>
<bodyText confidence="0.475922">
od
od;
return pred.
</bodyText>
<page confidence="0.996309">
408
</page>
<bodyText confidence="0.999687">
After we have determined the shortest-path
subgraph, the feature structures associated with
these edges are selected and transformed to the
corresponding VITs which are then sent to the
robust semantic processing component.
This approach has an important property:
even if certain parts of the input have not un-
dergone at least one rule application, there are
still lexical edges which help to form a best path
through the passive edges. Hence, this approach
shows anytime behavior which is a necessary re-
quirement in time-critical (speech) applications:
even if the parser is interrupted at a certain
point, we can always return a shortest path up
to that moment through our chart.
Let us now give an example to see what the
evaluation function on edges (i.e., derivation
trees) might look like3:
</bodyText>
<listItem confidence="0.9882845">
• n-ary trees (n &gt; 1) with utterance status
(e.g., NPs, PPs): value 1
• lexical items: value 2
• otherwise: value oo
</listItem>
<bodyText confidence="0.973649785714286">
If available, other properties, such as prosodic
information or probabilistic scores can also be
utilized in the evaluation function to determine
the best edges.
Figure 1: Computing best partial analyses.
Note that the paths PR and QR are chosen,
but not ST, although S is the longest edge. By
using uniform costs, all three paths would be
selected.
Depending on the evaluation, our method
does not necessarily favor paths with longest
edges as the example in Figure 1 shows — the
above strategy instead prefers paths contain-
ing no lexical edges (where this is possible) and
</bodyText>
<footnote confidence="0.998335">
3This is a slightly simplified form of the evaluation
that is actually used for the German grammar.
</footnote>
<bodyText confidence="0.9994225">
there might be several such paths having the
same cost. Longest (sub)paths, however, can
be obtained by employing an exponential func-
tions during the evaluation of an edge e E E:
</bodyText>
<equation confidence="0.542529">
Estimate(e) = —(max(E) _ min(s))length(e).
</equation>
<sectionHeader confidence="0.976421" genericHeader="method">
4 Robust Semantic Processing
</sectionHeader>
<bodyText confidence="0.999859454545455">
The second phase of processing, after produc-
ing a set of partial analyses, consists of assem-
bling and combining the fragments, where pos-
sible. We call this robust semantic processing
(Worm and Rupp, 1998), since the structures
being dealt with are semantic representations
(VITs) and the rules applied refer primarily to
the semantic content of fragments, though they
also consider syntactic and prosodic informa-
tion, e.g., about irregular boundaries.
This phase falls into three tasks:
</bodyText>
<listItem confidence="0.9943776">
1. storing the partial analyses from the
parser,
2. combining them on the basis of a set of
rules, and
3. selecting a result.
</listItem>
<bodyText confidence="0.99995368">
For storing of partial results, both delivered
from the parser or constructed later, we make
use of a chart-like data structure we call VIT
hypothesis graph (VHG), since it bears a resem-
blance to the WHG which is input to the parser.
It is organized according to WHG vertices. We
give an example in Figure 2, which will be ex-
plained in 4.1.
Combination of partial results takes place
using a set of rules which describe how frag-
mentary analyses can be combined. There are
language-independent rules, e.g., describing the
combination of a semantic functor with a possi-
ble argument, and language specific ones, such
as those for dealing with self-corrections in Ger-
man. Each operation carried out delivers a con-
fidence value which influences the score assigned
to an edge.
The overall mechanism behind the robust se-
mantic processing resembles that of a chart
parser. It runs in parallel with the HPSG
parser; each time the parser delivers partial re-
sults, they are handed over and processed, while
the parser may continue to look for a better
path in the WHG. The processing strategy is
</bodyText>
<page confidence="0.998908">
409
</page>
<figureCaption confidence="0.9973195">
Figure 2: The VHG for the first example. Only three VITs are delivered by the parser (the shortest
path), although the number of passive edges is 217.
</figureCaption>
<figure confidence="0.690885142857143">
42: past + Ihne (19998.3)13 21
43: past + Ihnen (19999.0)13,21
23: den halben Tag (80999.1) (11
81: oast + Ihnen + den halben Tao (109998.3)143.11
3 a&apos;st 9999. 2. lhnen 9999.0
41: Ihnen + den halben Tag (90998.9) [2231
1: den halben Ta 89999.0
</figure>
<bodyText confidence="0.998851333333333">
agenda-based, giving priority to new parser re-
sults.
Selection of a result means that the best edge
covering the whole input, or if that has not been
achieved, an optimal sequence of edges has to
be selected. We use a simple graph search al-
gorithm which finds the path with the highest
sum of individual scores.
Note that the robust semantic processing has
the anytime property as well: as soon as the first
partial result has been entered into the chart, a
result can be delivered on demand.
</bodyText>
<subsectionHeader confidence="0.999815">
4.1 An Example
</subsectionHeader>
<bodyText confidence="0.9998032">
Consider the utterance (1) where the case of the
NP den halben Tag (&apos;half the day&apos;) is accusative
and thus does not match the subcategorization
requirements of the verb passen (&apos;suit&apos;) which
would require nominative.
</bodyText>
<listItem confidence="0.976058">
(1) Past Ihnen den halben Tag?
&apos;Does half the day suit you?&apos;
</listItem>
<bodyText confidence="0.980117227272727">
According to the grammar, this string is ill-
formed, thus no complete analysis can be
achieved. However, the parser delivers frag-
ments for paflt, Ihnen, and den halben Tag.
(2) verb_arg_r
Utype(V1,verbal),missing_arg(V1)],
Etype(V2,term),possible_arg(V2,V1)]]
[apply_fun(V1,V2,V3),
assign_mood(V3,V4)1 &amp; V4.
When these results are stored, the rule in
(2) will combine the verb with its first argu-
ment, Ihnen. Each rule consists of three parts:
mnemonic rule name, tests on a sequence of in-
put VITs and the operations performed to con-
struct the ouput VIT. The first separator is : : ,
the second ---&gt;. A further application of the
same rule accounts for the second argument, den
halben Tag. However, the confidence value for
the second combination will reflect the viola-
tion of the case requirement. The resulting edge
spans the whole input and is selected as output.
The corresponding VHG is shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.99901">
4.2 Bridging
</subsectionHeader>
<bodyText confidence="0.9951482">
Not all cases can be handled as simply. Of-
ten, there are partial results in the input which
cannot be integrated into a spanning result. In
these cases, a mechanism called bridging is ap-
plied. Consider the self-correction in (3).
</bodyText>
<listItem confidence="0.99036825">
(3) Ich treffe habe einen Termin am
Montag.
&apos;I (will) meet ... have an appointment on
Monday.&apos;
</listItem>
<bodyText confidence="0.99968005882353">
Again, the parser will only find partial results.
Combinations of ich with treffe lead nowhere;
the combination of the second verb with the NP
does not lead to a complete analysis either (cf.
Figure 3). Note that if a nominal argument can
bind several argument roles, for each such read-
ing there is a passive edge in the VHG. Its score
reflects to what degree the selectional require-
ments of the verb, in terms of the required case
and sortal restrictions, have been met.
If no spanning result exists when all rules
have been applied, the bridging mechanism pro-
duces new active edges which extend edges al-
ready present. Here, it extends the active edge
aiming to combine ich with a verbal functor to
end after treffe, thus allowing for a combination
with the VP already built, habe einen Terrnin
</bodyText>
<page confidence="0.991671">
410
</page>
<table confidence="0.970501769230769">
264: ich + habe + einen Termin am Montan (179998.7) [18.4ffl
263: ich + habe + einen Termin am Montag (179998.0)118491
262: ich + habe + einen Termin am Montag (179998.7)118.481
261: ich + habe + einen Tenin am Monter] (179999,01118.481
18: Ich 9999.0 I 10&apos; treffe 9999.0 I 2&apos; habe 9999 I 1- einen Termin am Monta. 159999.0 I
. ,
76: ich + treffe (19998.3)118,101
77: ich + treffe (19999.0)118,101
258: ich + habe (19998.3) 18,21
259: ich + habe (19999.0) 18.21
260: ich + habe (19998.7 8.21
48: habe + einen Termin am Montag (169999.0) 12.11
49: habe + einen Tennin am Monter) (169998.7)12.11
</table>
<figureCaption confidence="0.999235">
Figure 3: The VHG for the second example.
</figureCaption>
<bodyText confidence="0.99777375">
am Montag. Extending the active edges from
left to right corresponds to the linear nature of
self-corrections, in which material to the right
replaces some to the left.
</bodyText>
<subsectionHeader confidence="0.999448">
4.3 Scoring and Result Selection
</subsectionHeader>
<bodyText confidence="0.999980428571429">
The scoring function for edges takes into ac-
count their length, the coverage of the edge, the
number of component edges it consists of, and
the confidence value for the operation which cre-
ated it. It has to satisfy the following property,
which is illustrated in Figure 4: If there are two
edges which together span an interval (edges a
and b) and another edge which has been built
from them (edge c), the latter should get a bet-
ter score than the sequence of the original two
edges. If there is another edge from the parser
which again spans the complete interval (edge
d), it should get a better score than the edge
built from the two components.
</bodyText>
<figureCaption confidence="0.990713">
Figure 4: Requirements for the scoring function.
</figureCaption>
<bodyText confidence="0.999965111111111">
The selection is done in two different ways.
If there is more than one spanning result, the
scores of the spanning results are weighted ac-
cording to a statistical model describing se-
quence probabilities based on semantic predi-
cates (Ruland et al., 1998) and the best is se-
lected. Otherwise, the best sequence, i.e., the
one with the highest score, is chosen in square
time, using a standard graph search algorithm.
</bodyText>
<sectionHeader confidence="0.995754" genericHeader="method">
5 Empirical Results
</sectionHeader>
<bodyText confidence="0.99987092">
For an intermediate evaluation of the robust
semantic processing phase, we ran our system
consisting of HPSG parser and robust semantic
processing on a dialogue from the VERBMOBIL
corpus of spontaneous appointment negotiation
dialogues, producing WHGs from the original
recorded audio data. The dialogue consists of
90 turns. These 90 turns were split into 130 seg-
ments according to pauses by the speech recog-
nizer. The segments received 213 segment anal-
yses, i.e., there are 1.6 analyses per segment on
average. 172 (80.8%) of these were generated
by the parser and 41(19.2%) were assembled
from parser results by robust semantic process-
ing. Of these 41 results, 34 (83%) were sensibly
improved, while 7 (17%) did not represent a real
improvement.
This evaluation is local in the sense that we
only consider the input-output behaviour of ro-
bust semantic processing. We do this in order to
exclude the effects of insufficiencies introduced
by other modules in the system, since they
would distort the picture. For this same rea-
son, the criterion we apply is whether the result
delivered is a sensible combination of the frag-
</bodyText>
<page confidence="0.996076">
411
</page>
<bodyText confidence="0.9998906">
ments received, without reference to the original
utterance or the translation produced. How-
ever, in the long run we plan to compare the
complete system&apos;s behaviour with and without
the robust processing strategy.
</bodyText>
<sectionHeader confidence="0.998207" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.846526571428572">
The approach to the robust analysis of spoken
language input, that we have described above,
exhibits three crucial properties.
1. The restrictive parser is given the maxi-
mum opportunity of finding a correct anal-
ysis for a grammatical sequence of word hy-
potheses, where this exists.
</bodyText>
<listItem confidence="0.548668">
2. The robustness component assembles par-
tial analyses as a fallback, if no grammati-
cal sequence can be found.
3. Almost arbitrary time constraints can be
</listItem>
<bodyText confidence="0.946319764705882">
supported. Though, obviously, more pro-
cessing time would usually improve the re-
sults.
The latter property depends directly on the
chart-like data structures used at each level of
processing. Whether it be the input WHG,
VHG for robust processing or, most signifi-
cantly, the parser&apos;s chart; each is formally a di-
rected acyclic graph and each permits a selec-
tion of the best intermediate result at, virtually,
any stage in processing, for a given evaluation
function.
The relatively efficient processing of WHG in-
put achieved by parsing and robustness compo-
nents working in parallel depends quite heav-
ily on the successive processing of ranked WHG
paths, effectively as alternative input strings.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999956">
We would like to thank the anonymous ACL
reviewers for their detailed comments. This
research was supported by the German Fed-
eral Ministry for Education and Research under
grants nos. 01 IV 701 R4 and 01 IV 701 VO.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.945109672727273">
Johan Bos, C.J. Rupp, Bianka Buschbeck-Wolf,
and Michael Dorna. 1998. Managing infor-
mation at linguistic interfaces. In Proc. of
the 17th COLING/36th ACL, pages 160-166,
Montréal, Canada.
Ann Copestake, Dan Flickinger, and Ivan A.
Sag. 1996. Minimal recursion semantics. an
introduction. Ms, Stanford.
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to Al-
gorithms. MIT Press, Cambridge, MA.
Thomas Dean and Mark Boddy. 1988. An anal-
ysis of time-dependent planning. In Proceed-
ings of the 7th National Conference on Arti-
ficial Intelligence, AAAI-88, pages 49-54.
Johannes Heinecke, Jürgen Kunze, Wolfgang
Menzel, and Ingo Schröder. 1998. Elimina-
tive parsing with graded constraints. In Proc.
of the 17th COLING/36th ACL, pages 526-
530, Montréal, Canada.
Donald Hindle. 1983. Deterministic parsing of
syntactic non-fluencies. In Proc. of the 21 th
ACL, pages 123-128, Cambridge, MA.
Dwayne Richard Hipp. 1993. Design and De-
velopment of Spoken Natural-Language Dia-
log Parsing Systems. Ph.D. thesis, Depart-
ment of Computer Science, Duke University,
Durham, NC.
Martin Oerder and Hermann Ney. 1993.
Word graphs: An efficient interface between
continuous-speech recognition and language
understanding. In Proc. Int. Conf. on Acous-
tics, Speech and Signal Processing (ICASSP),
Minneapolis, MN. IEEE Signal Processing
Society.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. Univer-
sity of Chicago Press, Chicago.
Tobias Ruland, C. J. Rupp, JOrg Spilker, Hans
Weber, and Karsten L. Worm. 1998. Mak-
ing the most of multiplicity: A multi-parser
multi-strategy architecture for the robust
processing of spoken language. In Proc. of
the 1998 International Conference on Spo-
ken Language Processing (ICSLP 98), Syd-
ney, Australia.
Wolfgang Wahlster. 1993. VERBMOBIL —
translation of face-to-face dialogs. In Proc.
MT Summit IV, pages 127-135, Kobe, Japan,
July.
Karsten L. Worm and C. J. Rupp. 1998. To-
wards robust understanding of speech by
combination of partial analyses. In Proc. of
the 13 th ECAI, pages 190-194, Brighton,
UK.
</reference>
<page confidence="0.997573">
412
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.272437">
<title confidence="0.985154">Charting the Depths of Robust Speech Parsing</title>
<author confidence="0.804356">B Kiefert Kaspert</author>
<author confidence="0.804356">H-U Kriegert</author>
<author confidence="0.804356">C J Ruppt</author>
<author confidence="0.804356">L Worint</author>
<affiliation confidence="0.51698">tGerman Research Center for Artificial Intelligence (DFKI) IComputational Linguistics Department, Universitat des Saarlandes</affiliation>
<abstract confidence="0.977101545454546">We describe a novel method for coping with ungrammatical input based on the use of chart-like data structures, which permit anytime processing. Priority is given to deep syntactic analysis. Should this fail, the best partial analyses are selected, according to a shortest-paths algorithm, and assembled in a robust processing phase. The method has been applied in a speech translation project with large HPSG grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>C J Rupp</author>
<author>Bianka Buschbeck-Wolf</author>
<author>Michael Dorna</author>
</authors>
<title>Managing information at linguistic interfaces.</title>
<date>1998</date>
<booktitle>In Proc. of the 17th COLING/36th ACL,</booktitle>
<pages>160--166</pages>
<location>Montréal, Canada.</location>
<contexts>
<context position="9768" citStr="Bos et al., 1998" startWordPosition="1604" endWordPosition="1607">rspecification. To give an impression of the size of grammars, we present the numbers for the German grammar. It consists of 2,389 types, 76 rule schemata, 4,284 stems and an average of six entries per stem. Morphological information is computed online which further increases the lexical ambiguity. 2.3 Partial Analyses and the Syntax-Semantics Interface Our architecture requires that the linguistic analysis module is capable of delivering not just analyses of complete utterances, but also of phrases and even of lexical items in the special interface format of VITs (VERBMOBIL Interface Terms) (Bos et al., 1998). There are three considerations which the interface has to take into account: 1. Only maximal projections, i.e., complete phrases, are candidates for robust processing. This qualifies, e.g., prepositional and noun phrases. On the other hand, this approach leaves gaps in the coverage of the input string as not every word needs to be dominated by a maximal projection. In particular, verbal projections below the sentential level usually are incomplete phrases. The use of intermediate, incomplete projections is avoided for several reasons: • intermediate projections are highly grammar and languag</context>
</contexts>
<marker>Bos, Rupp, Buschbeck-Wolf, Dorna, 1998</marker>
<rawString>Johan Bos, C.J. Rupp, Bianka Buschbeck-Wolf, and Michael Dorna. 1998. Managing information at linguistic interfaces. In Proc. of the 17th COLING/36th ACL, pages 160-166, Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal recursion semantics. an introduction.</title>
<date>1996</date>
<location>Ms, Stanford.</location>
<contexts>
<context position="9066" citStr="Copestake et al., 1996" startWordPosition="1494" endWordPosition="1498">he new input without being recomputed. 2.2 The HPSG Grammars The grammars for English, German, and Japanese follow the paradigm of HPSG (Pollard and Sag, 1994) which is the most advanced unification-based grammatical theory based on typed feature structures. The fundamental concept is that of a sign, a structure incorporating information from all levels of linguistic analysis, such as phonology, morphology, syntax, and semantics. This structure makes all information simultaneously available and provides declarative interfaces between these levels. The grammars use Minimal Recursion Semantics (Copestake et al., 1996) as the semantic representation formalism, allowing us to deal with ambiguity by underspecification. To give an impression of the size of grammars, we present the numbers for the German grammar. It consists of 2,389 types, 76 rule schemata, 4,284 stems and an average of six entries per stem. Morphological information is computed online which further increases the lexical ambiguity. 2.3 Partial Analyses and the Syntax-Semantics Interface Our architecture requires that the linguistic analysis module is capable of delivering not just analyses of complete utterances, but also of phrases and even o</context>
</contexts>
<marker>Copestake, Flickinger, Sag, 1996</marker>
<rawString>Ann Copestake, Dan Flickinger, and Ivan A. Sag. 1996. Minimal recursion semantics. an introduction. Ms, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="12462" citStr="Cormen et al., 1990" startWordPosition="2051" endWordPosition="2055">xtremes: computing a connected sequence of best partial analyses which cover the whole input. The idea here is to view the set of passive edges of a parser as a directed graph which needs to be evaluated according to a user-defined (and therefore grammar and language specific) metric. Using this graph, we then compute the shortest paths w.r.t. the evaluation function, i.e., paths through this graph with minimum cost. Since this graph is acyclic and topologically sorted (vertices are integers and edges always connect a vertex to a larger vertex), we have chosen the DAG-shortest-path algorithm (Cormen et al., 1990) which runs in e(v + E). This fast algorithm is a solution to the single-source shortest-paths problem. We modified and extended this algorithm to cope with the needs we encountered in speech parsing: (i) one can use several start and end vertices (e.g., in the case of n-best chains or WHGs); (ii) all best shortest paths are returned (i.e., we obtain a shortestpath subgraph); and (iii) evaluation and selection of the best edges is done incrementally as is the case for parsing the n-best chains (i.e., only new passive edges entered into the chart are evaluated and may be selected by our shortes</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. 1990. Introduction to Algorithms. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Dean</author>
<author>Mark Boddy</author>
</authors>
<title>An analysis of time-dependent planning.</title>
<date>1988</date>
<booktitle>In Proceedings of the 7th National Conference on Artificial Intelligence, AAAI-88,</booktitle>
<pages>49--54</pages>
<contexts>
<context position="4510" citStr="Dean and Boddy, 1988" startWordPosition="720" endWordPosition="723">se ungrammatical constructions. 2. The acoustic signal produced by a human speaker is mapped by a speech recognizer onto a written form; this mapping is rarely completely correct. This introduces two levels of uncertainty into the processing of speech, which make the task of linguistically analyzing a spoken utterance in a speech processing system doubly hard. In addition, the dialogue context imposes strict time constraints, as the overall system must attempt to emulate real time performance. The strategy we adopt responds to time constraints by universally incorporating an anytime property (Dean and Boddy, 1988) into the selection procedures. As will be seen, this property derives from the way in which intermediate results are stored and the selections which can be made from among these. However, the overriding priority of this same strategy is to maximize the chance that a truly grammatical path will be found and analyzed, if one exists in the WHG. This means that while we have implemented extensive mechanisms to achieve robustness, their design, and in particular the separation of processing into a restrictive parser and a robust postprocessor, are subservient to the cases where a fully grammatical</context>
</contexts>
<marker>Dean, Boddy, 1988</marker>
<rawString>Thomas Dean and Mark Boddy. 1988. An analysis of time-dependent planning. In Proceedings of the 7th National Conference on Artificial Intelligence, AAAI-88, pages 49-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Heinecke</author>
<author>Jürgen Kunze</author>
<author>Wolfgang Menzel</author>
<author>Ingo Schröder</author>
</authors>
<title>Eliminative parsing with graded constraints.</title>
<date>1998</date>
<booktitle>In Proc. of the 17th COLING/36th ACL,</booktitle>
<pages>526--530</pages>
<location>Montréal, Canada.</location>
<contexts>
<context position="5314" citStr="Heinecke et al., 1998" startWordPosition="855" endWordPosition="858">wever, the overriding priority of this same strategy is to maximize the chance that a truly grammatical path will be found and analyzed, if one exists in the WHG. This means that while we have implemented extensive mechanisms to achieve robustness, their design, and in particular the separation of processing into a restrictive parser and a robust postprocessor, are subservient to the cases where a fully grammatical analysis is possible, since these results are in any case better. These decisions may be in conflict with much of the literature on robust parsing (e.g., (Hindle, 1983; Hipp, 1993; Heinecke et al., 1998)), but the alternative of relaxing the parsing constraints would appear to be a dead end in the context of the VERBMOBIL architecture. In the first place, the chances of locating the best grammatical path in the lattice would be reduced, e.g., by the acceptance of a preceding ungrammatical one. Secondly, a more liberal parser would raise the spectre of an explosion of edges in the parser&apos;s chart, so that in fact less paths could be processed overall, regardless of their quality. Either of these conditions could prove fatal. This paper focuses on the aspects of the VERBMOBIL analysis component </context>
</contexts>
<marker>Heinecke, Kunze, Menzel, Schröder, 1998</marker>
<rawString>Johannes Heinecke, Jürgen Kunze, Wolfgang Menzel, and Ingo Schröder. 1998. Eliminative parsing with graded constraints. In Proc. of the 17th COLING/36th ACL, pages 526-530, Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Deterministic parsing of syntactic non-fluencies.</title>
<date>1983</date>
<booktitle>In Proc. of the 21 th ACL,</booktitle>
<pages>123--128</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="5278" citStr="Hindle, 1983" startWordPosition="851" endWordPosition="852"> made from among these. However, the overriding priority of this same strategy is to maximize the chance that a truly grammatical path will be found and analyzed, if one exists in the WHG. This means that while we have implemented extensive mechanisms to achieve robustness, their design, and in particular the separation of processing into a restrictive parser and a robust postprocessor, are subservient to the cases where a fully grammatical analysis is possible, since these results are in any case better. These decisions may be in conflict with much of the literature on robust parsing (e.g., (Hindle, 1983; Hipp, 1993; Heinecke et al., 1998)), but the alternative of relaxing the parsing constraints would appear to be a dead end in the context of the VERBMOBIL architecture. In the first place, the chances of locating the best grammatical path in the lattice would be reduced, e.g., by the acceptance of a preceding ungrammatical one. Secondly, a more liberal parser would raise the spectre of an explosion of edges in the parser&apos;s chart, so that in fact less paths could be processed overall, regardless of their quality. Either of these conditions could prove fatal. This paper focuses on the aspects </context>
</contexts>
<marker>Hindle, 1983</marker>
<rawString>Donald Hindle. 1983. Deterministic parsing of syntactic non-fluencies. In Proc. of the 21 th ACL, pages 123-128, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dwayne Richard Hipp</author>
</authors>
<title>Design and Development of Spoken Natural-Language Dialog Parsing Systems.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Duke University,</institution>
<location>Durham, NC.</location>
<contexts>
<context position="5290" citStr="Hipp, 1993" startWordPosition="853" endWordPosition="854">ng these. However, the overriding priority of this same strategy is to maximize the chance that a truly grammatical path will be found and analyzed, if one exists in the WHG. This means that while we have implemented extensive mechanisms to achieve robustness, their design, and in particular the separation of processing into a restrictive parser and a robust postprocessor, are subservient to the cases where a fully grammatical analysis is possible, since these results are in any case better. These decisions may be in conflict with much of the literature on robust parsing (e.g., (Hindle, 1983; Hipp, 1993; Heinecke et al., 1998)), but the alternative of relaxing the parsing constraints would appear to be a dead end in the context of the VERBMOBIL architecture. In the first place, the chances of locating the best grammatical path in the lattice would be reduced, e.g., by the acceptance of a preceding ungrammatical one. Secondly, a more liberal parser would raise the spectre of an explosion of edges in the parser&apos;s chart, so that in fact less paths could be processed overall, regardless of their quality. Either of these conditions could prove fatal. This paper focuses on the aspects of the VERBM</context>
</contexts>
<marker>Hipp, 1993</marker>
<rawString>Dwayne Richard Hipp. 1993. Design and Development of Spoken Natural-Language Dialog Parsing Systems. Ph.D. thesis, Department of Computer Science, Duke University, Durham, NC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Oerder</author>
<author>Hermann Ney</author>
</authors>
<title>Word graphs: An efficient interface between continuous-speech recognition and language understanding.</title>
<date>1993</date>
<booktitle>In Proc. Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<publisher>IEEE Signal Processing Society.</publisher>
<location>Minneapolis, MN.</location>
<contexts>
<context position="1205" citStr="Oerder and Ney, 1993" startWordPosition="184" endWordPosition="187">ths algorithm, and assembled in a robust processing phase. The method has been applied in a speech translation project with large HPSG grammars. 1 Introduction This paper describes a new method of dealing robustly with deficient speech or text input, which may be due to recognition errors, spontaneous speech phenomena, or ungrammatical constructions. Two key features of this approach are: • the priority given to a deep and restrictive grammatical analysis, • and the use of chart-like data structures at each level of processing. The initial input is taken from a Word Hypothesis Graph, or WHG, (Oerder and Ney, 1993) from which the best ranked paths are successively selected until a result is found or a time limit proportional to the length of the utterancel is reached. Each path is parsed with an incremental chart parser that uses a HeadDriven Phrase Structure grammar (HPSG). The parser is adapted to input from WHGs and optimized to meet the needs of real-time speech processing. Since the goal of the parsing component is to process as many WHG paths as possible, in order to find a grammatical utterance iThis is currently up to four times real time. and analyze it with highest accuracy, neither relaxation</context>
</contexts>
<marker>Oerder, Ney, 1993</marker>
<rawString>Martin Oerder and Hermann Ney. 1993. Word graphs: An efficient interface between continuous-speech recognition and language understanding. In Proc. Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Minneapolis, MN. IEEE Signal Processing Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>HeadDriven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="8602" citStr="Pollard and Sag, 1994" startWordPosition="1427" endWordPosition="1431">ssive edges of the chart form a directed acyclic graph which is directly used as input to compute best partial analyses. We note here that the parser processes the nbest paths of a WHG fully incrementally. I.e., when the analysis of a new input path begins, only those input items are added to the chart that have not been part of a previously treated path. Everything else that has been computed up to that point remains in the chart and can be used to process the new input without being recomputed. 2.2 The HPSG Grammars The grammars for English, German, and Japanese follow the paradigm of HPSG (Pollard and Sag, 1994) which is the most advanced unification-based grammatical theory based on typed feature structures. The fundamental concept is that of a sign, a structure incorporating information from all levels of linguistic analysis, such as phonology, morphology, syntax, and semantics. This structure makes all information simultaneously available and provides declarative interfaces between these levels. The grammars use Minimal Recursion Semantics (Copestake et al., 1996) as the semantic representation formalism, allowing us to deal with ambiguity by underspecification. To give an impression of the size o</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. HeadDriven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Ruland</author>
<author>C J Rupp</author>
<author>JOrg Spilker</author>
<author>Hans Weber</author>
<author>Karsten L Worm</author>
</authors>
<title>Making the most of multiplicity: A multi-parser multi-strategy architecture for the robust processing of spoken language.</title>
<date>1998</date>
<booktitle>In Proc. of the 1998 International Conference on Spoken Language Processing (ICSLP 98),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="24013" citStr="Ruland et al., 1998" startWordPosition="4053" endWordPosition="4056">ges a and b) and another edge which has been built from them (edge c), the latter should get a better score than the sequence of the original two edges. If there is another edge from the parser which again spans the complete interval (edge d), it should get a better score than the edge built from the two components. Figure 4: Requirements for the scoring function. The selection is done in two different ways. If there is more than one spanning result, the scores of the spanning results are weighted according to a statistical model describing sequence probabilities based on semantic predicates (Ruland et al., 1998) and the best is selected. Otherwise, the best sequence, i.e., the one with the highest score, is chosen in square time, using a standard graph search algorithm. 5 Empirical Results For an intermediate evaluation of the robust semantic processing phase, we ran our system consisting of HPSG parser and robust semantic processing on a dialogue from the VERBMOBIL corpus of spontaneous appointment negotiation dialogues, producing WHGs from the original recorded audio data. The dialogue consists of 90 turns. These 90 turns were split into 130 segments according to pauses by the speech recognizer. Th</context>
</contexts>
<marker>Ruland, Rupp, Spilker, Weber, Worm, 1998</marker>
<rawString>Tobias Ruland, C. J. Rupp, JOrg Spilker, Hans Weber, and Karsten L. Worm. 1998. Making the most of multiplicity: A multi-parser multi-strategy architecture for the robust processing of spoken language. In Proc. of the 1998 International Conference on Spoken Language Processing (ICSLP 98), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
</authors>
<title>VERBMOBIL — translation of face-to-face dialogs.</title>
<date>1993</date>
<booktitle>In Proc. MT Summit IV,</booktitle>
<pages>127--135</pages>
<location>Kobe, Japan,</location>
<contexts>
<context position="3146" citStr="Wahlster, 1993" startWordPosition="512" endWordPosition="513">WHG has been (unsuccessfully) analyzed, processing in both the restrictive parser and the robustness component proceeds in parallel, with the aid of a parallel virtual machine, until one of the following conditions is fulfilled: 1. a spanning grammatical analysis is found, 2. all the WHG paths have been explored, or 3. the time limit is reached. In the case of either of the latter two conditions, robust semantic processing is allowed a limited time to complete processing and then the best result or sequence of results is selected from its chart. Our approach has been implemented in VERBMOBIL (Wahlster, 1993), a large scale research project in the area of spoken language 2This means that the maximal sequential delay between parsing and robust semantics processing is the parse time for one path. Similarly, the limit on parsing time, essentially, applies to both components 405 translation. Its goal is to develop a system that translates negotiation dialogues between German, English, and Japanese speakers in faceto-face or video conferencing situations. This application highlights the basic problem associated with machine processing of spontaneous speech, namely that the input to the natural language</context>
</contexts>
<marker>Wahlster, 1993</marker>
<rawString>Wolfgang Wahlster. 1993. VERBMOBIL — translation of face-to-face dialogs. In Proc. MT Summit IV, pages 127-135, Kobe, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karsten L Worm</author>
<author>C J Rupp</author>
</authors>
<title>Towards robust understanding of speech by combination of partial analyses.</title>
<date>1998</date>
<booktitle>In Proc. of the 13 th ECAI,</booktitle>
<pages>190--194</pages>
<location>Brighton, UK.</location>
<contexts>
<context position="17497" citStr="Worm and Rupp, 1998" startWordPosition="2929" endWordPosition="2932">s paths containing no lexical edges (where this is possible) and 3This is a slightly simplified form of the evaluation that is actually used for the German grammar. there might be several such paths having the same cost. Longest (sub)paths, however, can be obtained by employing an exponential functions during the evaluation of an edge e E E: Estimate(e) = —(max(E) _ min(s))length(e). 4 Robust Semantic Processing The second phase of processing, after producing a set of partial analyses, consists of assembling and combining the fragments, where possible. We call this robust semantic processing (Worm and Rupp, 1998), since the structures being dealt with are semantic representations (VITs) and the rules applied refer primarily to the semantic content of fragments, though they also consider syntactic and prosodic information, e.g., about irregular boundaries. This phase falls into three tasks: 1. storing the partial analyses from the parser, 2. combining them on the basis of a set of rules, and 3. selecting a result. For storing of partial results, both delivered from the parser or constructed later, we make use of a chart-like data structure we call VIT hypothesis graph (VHG), since it bears a resemblanc</context>
</contexts>
<marker>Worm, Rupp, 1998</marker>
<rawString>Karsten L. Worm and C. J. Rupp. 1998. Towards robust understanding of speech by combination of partial analyses. In Proc. of the 13 th ECAI, pages 190-194, Brighton, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>