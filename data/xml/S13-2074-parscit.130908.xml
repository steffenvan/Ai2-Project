<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002009">
<title confidence="0.9974395">
ASVUniOfLeipzig: Sentiment Analysis in Twitter using Data-driven
Machine Learning Techniques
</title>
<author confidence="0.985898">
Robert Remus
</author>
<affiliation confidence="0.991518666666667">
Natural Language Processing Group,
Department of Computer Science,
University of Leipzig, Germany
</affiliation>
<email confidence="0.98867">
rremus@informatik.uni-leipzig.de
</email>
<sectionHeader confidence="0.995464" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922142857143">
This paper describes University of Leipzig’s
approach to SemEval-2013 task 2B on Sen-
timent Analysis in Twitter: message polar-
ity classification. Our system is designed to
function as a baseline, to see what we can
accomplish with well-understood and purely
data-driven lexical features, simple general-
izations as well as standard machine learning
techniques: We use one-against-one Support
Vector Machines with asymmetric cost fac-
tors and linear “kernels” as classifiers, word
uni- and bigrams as features and additionally
model negation of word uni- and bigrams in
word n-gram feature space. We consider gen-
eralizations of URLs, user names, hash tags,
repeated characters and expressions of laugh-
ter. Our method ranks 23 out of all 48 partic-
ipating systems, achieving an averaged (pos-
itive, negative) F-Score of 0.5456 and an av-
eraged (positive, negative, neutral) F-Score of
0.595, which is above median and average.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999889644444445">
In SemEval-2013’s task 2B on Sentiment Analysis
in Twitter, given a Twitter message, i.e. a tweet, the
goal is to classify whether this tweet is of positive,
negative, or neutral polarity (Wilson et al., 2013),
i.e. the task is a ternary polarity classification.
Due to Twitter’s growing popularity, the availabil-
ity of large amounts of data that go along with that
and the fact, that many people freely express their
opinion on virtually everything using Twitter, re-
search on sentiment analysis in Twitter has received
a lot of attention lately (Go et al., 2009; Pak and
Paroubek, 2010). Language is usually used casu-
ally in Twitter and exhibits interesting properties.
Therefore, some studies specifically address certain
issues, e.g. a tweet’s length limitation of 140 char-
acters, some studies leverage certain language char-
acteristics, e.g. the presence of emoticons etc.
Davidov et al. (2010) identify various “sentiment
types” defined by Twitter hash tags (e.g. #bored)
and smileys (e.g. :S) using words, word n-grams,
punctuation marks and patterns as features. Bar-
bosa and Feng (2010) map words to more general
representations, i.e. part of speech (POS) tags and
the words’ prior subjectivity and polarity. Addi-
tionally, they count the number of re-tweets, hash
tags, replies, links etc. They then combine the out-
puts of 3 online sources of labeled but noisy and bi-
ased Twitter data into a more robust classification
model. Saif et al. (2012) also address data sparsity
via word clustering methods, i.e. semantic smooth-
ing and sentiment-topics extraction. Agarwal et al.
(2011) contrast a word unigram model, a tree ker-
nel model and a model of various features, e.g. POS
tag counts, summed up prior polarity scores, pres-
ence or absence of capitalized text, all applied to bi-
nary and ternary polarity classification. Kouloumpis
et al. (2011) show that Twitter-specific feature engi-
neering, e.g. representing the presence or absence
of abbreviations and character repetitions improves
model quality. Jiang et al. (2011) focus on target-
dependent polarity classification regarding a given
user query.
While various models and features have been pro-
posed, word n-gram models proved to be competi-
tive in many studies (Barbosa and Feng, 2010; Agar-
</bodyText>
<page confidence="0.951694">
450
</page>
<bodyText confidence="0.990870944444444">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 450–454, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
wal et al., 2011; Saif et al., 2012) yet are straight-
forward to implement. Moreover, word n-gram
models do not rely on hand-crafted and generally
{genre, domain}-non-specific resources, e.g. prior
polarity dictionaries like SentiWordNet (Esuli and
Sebastiani, 2006) or Subjectivity Lexicon (Wiebe et
al., 2005). In contrast, purely data-driven word n-
gram models are domain-specific per se: they “let
the data speak for themselves”. Therefore we be-
lieve that carefully designing such a baseline using
well-understood and purely data-driven lexical fea-
tures, simple generalizations as well as standard ma-
chine learning techniques is a worthwhile endeavor.
In the next Section we describe our system. In
Section 3 we discuss its results in SemEval-2013
task 2B and finally conclude in Section 4.
</bodyText>
<sectionHeader confidence="0.985202" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.932847">
We approach the ternary polarity classification via
one-against-one (Hsu and Lin, 2002) Support Vector
Machines (SVMs) (Vapnik, 1995; Cortes and Vap-
nik, 1995) using a linear “kernel” as implemented
by LibSVM1. To deal with the imbalanced class dis-
tribution of positive (+), negative (−) and neutral-
or-objective (0) instances, we use asymmetric cost
factors C+, C−, C0 that allow for penalizing false
positives and false negatives differently inside the
one-against-one SVMs. While the majority class’
C0 is set to 1.0, the minority classes’ C{+,−}s are
set as shown in (1)
#(0-class instances)
C{+,−} � (1)
#({+, −}-class instances)
similar to Morik et al. (1999)’s suggestion.
</bodyText>
<subsectionHeader confidence="0.959819">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999966285714286">
To develop our system, we use all training data avail-
able to us for training and all development data avail-
able to us for testing, after removing 75 duplicates
from the training data and 2 duplicates from the
development data. Please note that 936 tweets of
the originally provided training data and 3 tweets of
the originally provided development data were not
</bodyText>
<footnote confidence="0.9507115">
1http://www.csie.ntu.edu.tw/˜cjlin/
libsvm/
</footnote>
<bodyText confidence="0.971734333333333">
available at our download time2. Table 1 summa-
rizes the used data’s class distribution after duplicate
removal.
</bodyText>
<table confidence="0.9937645">
Data + − 0 E
Training 3,263 1,278 4,132 8,673
Development 384 197 472 1,053
E 3,647 1,475 4,604 9,726
</table>
<tableCaption confidence="0.963908333333333">
Table 1: Class distribution of positive (+), negative (−)
and neutral-or-objective (0) instances in training and de-
velopment data after duplicate removal.
</tableCaption>
<bodyText confidence="0.999762">
For sentence segmentation and tokenization of the
data we use OpenNLP3. An example tweet of the
provided training data is shown in (1):
</bodyText>
<listItem confidence="0.851509">
(1) #nacamam @naca you have to try Sky-
walk Deli on the 2nd floor of the Com-
</listItem>
<bodyText confidence="0.9755035">
erica building on Monroe! #bestlunche
http://instagr.am/p/Rfv-RfTI-3/.
</bodyText>
<subsectionHeader confidence="0.999769">
2.2 Model Selection
</subsectionHeader>
<bodyText confidence="0.999899">
To select an appropriate model, we experiment with
different feature sets (cf. Section 2.2.1) and different
combinations of generalizations (cf. Section 2.2.2).
</bodyText>
<subsectionHeader confidence="0.462096">
2.2.1 Features
</subsectionHeader>
<bodyText confidence="0.803452714285714">
We consider the following feature sets:
a. word unigrams
b. word unigrams plus negation modeling for
word unigrams
c. word uni- and bigrams
d. word uni- and bigrams plus negation modeling
for word unigrams
e. word uni- and bigrams plus negation modeling
for word uni- and bigrams
Word uni- and bigrams are induced data-driven, i.e.
directly extracted from the textual data. We perform
no feature selection; neither stop words nor punc-
tuation marks are removed. We simply encode the
presence or absence of word n-grams.
</bodyText>
<footnote confidence="0.9738435">
2Training data was downloaded on February 21, 2013, 9:18
a.m. and development data was downloaded on February 28,
2013, 10:41 a.m. using the original download script.
3http://opennlp.apache.org
</footnote>
<page confidence="0.99682">
451
</page>
<bodyText confidence="0.9999725">
Whether a word uni- or bigram is negated, i.e.
appears inside of a negation scope (Wiegand et al.,
2010), is detected by LingScope4 (Agarwal and Yu,
2010), a state-of-the-art negation scope detection
based on Conditional Random Fields (Lafferty et
al., 2001). We model the negation of word n-grams
in an augmented word n-gram feature space as de-
tailedly described in Remus (2013): In this feature
space, each word n-gram is either represented as
present ([1, 0]), absent ([0, 0]), present inside a nega-
tion scope ([0,1]) and present both inside and out-
side a negation scope ([1,1]).
We trained a model for each feature set and chose
the one that yields the highest accuracy: word uni-
and bigrams plus negation modeling for word uni-
and bigrams.
</bodyText>
<sectionHeader confidence="0.43184" genericHeader="method">
2.2.2 Generalizations
</sectionHeader>
<bodyText confidence="0.93673425">
To account for Twitter’s typical language char-
acteristics, we consider all possible combinations
of generalizations of the following character se-
quences, inspired by (Montejo-R´aez et al., 2012):
a. User names, that mark so-called mentions in a
Tweet, expressed by @username.
b. Hash tags, that mark keywords or topics in a
Tweet, expressed by #keyword.
</bodyText>
<listItem confidence="0.9533356875">
c. URLs, that mark links to other web pages.
d. Twitpic URLs, that mark links to pictures
hosted by twitpic.com.
e. Repeated Characters, e.g. woooow. We col-
lapse characters re-occuring more than twice,
e.g. woooow is replaced by woow.
f. Expressions of laughter, e.g. hahaha. We
generalize derivatives of the “base forms”
haha, hehe, hihi and huhu. A derivative
must contain the base form and may addition-
ally contain arbitrary uppercased and lower-
cased letters at its beginning and its end. We
collapse these derivatives. E.g., hahahah and
HAHAhaha and hahaaa are all replaced by
their base form haha, eheheh and heheHE
are all replaced by hehe etc.
</listItem>
<footnote confidence="0.959555">
4http://sourceforge.net/projects/
lingscope/
</footnote>
<bodyText confidence="0.999742782608696">
User names, hash tags, URLs and Twitpic URLs are
generalized by either simply removing them (mode
I) or by replacing them with a single unique token
(mode II), i.e. by forming an equivalence class. Re-
peated characters and expressions of laughter are
generalized by collapsing them as described above.
There are 1 + Ek=1 (k) = 64 possible combina-
tions of generalizations including no generalization
at all. We trained a word uni- and bigram plus nega-
tion modeling for word uni- and bigrams model (cf.
Section 2.2.1) for each combination and both mode
I and mode II and chose the one that yields the high-
est accuracy: Generalization of URLs (mode I), re-
peated characters and expressions of laughter.
Although it may appear counterintuitive not to
generalize hash tags and user names, the training
data contains several re-occuring hash tags, that ac-
tually convey sentiment, e.g. #love, #cantwait,
#excited. Similarly, the training data con-
tains several re-occuring mentions of “celebrities”,
that may hint at sentiment which is usually as-
sociated with them, e.g. @justinbieber or
@MittRomney.
</bodyText>
<sectionHeader confidence="0.999576" genericHeader="evaluation">
3 Results &amp; Discussion
</sectionHeader>
<bodyText confidence="0.999678272727273">
To train our final system, we use all available train-
ing and development data (cf. Table 1). The SVM’s
“base” cost factor C is optimized via 10-fold cross
validation, where in each fold 9/10th of the available
data are used for training, the remaining 1/10th is used
for testing. C values are chosen from 12 · 10−3, 2 ·
10−2, 2 · 10−1, 2 · 100, 2 · 101, 2 · 102, 2 · 1031. In-
ternally, the asymmetric cost factors C+, C−, C0 (cf.
Section 2) are then set to C{+,−,0} C · C{+,−,0}.
The final system is then applied to both Twit-
ter and SMS test data (cf. Table 2). Please note
</bodyText>
<table confidence="0.997748666666667">
Test Data + − 0 E
Twitter 1,572 601 1,640 3,813
SMS 492 394 1,208 2,094
</table>
<tableCaption confidence="0.978485">
Table 2: Class distribution of positive (+), negative (−)
and neutral-or-objective (0) instances in Twitter and SMS
testing data.
</tableCaption>
<bodyText confidence="0.793058666666667">
that we only participate in the constrained setting of
SemEval-2013 task 2B (Wilson et al., 2013) as we
did not use any additional training data.
</bodyText>
<page confidence="0.997526">
452
</page>
<bodyText confidence="0.9985982">
Detailed evaluation results on Twitter test data
are shown in Table 3, results on SMS test data are
shown in Table 4. The ranks we achieved in the con-
strained only-ranking and the full constrained and
unconstrained-ranking are shown in Table 5.
</bodyText>
<table confidence="0.999801166666667">
Class P R F
+ 0.7307 0.5833 0.6487
− 0.5795 0.3577 0.4424
0 0.6072 0.8098 0.6940
+, − 0.6551 0.4705 0.5456
+,−,0 0.6391 0.5836 0.5950
</table>
<tableCaption confidence="0.886883">
Table 3: Precision P, Recall R and F-Score F of Univer-
sity of Leipzig’s approach to SemEval-2013 task 2B on
Twitter test data distinguished by classes (+, −, 0) and
averages of +, − and +, −, 0.
</tableCaption>
<table confidence="0.999942666666667">
Class P R F
+ 0.5161 0.5854 0.5486
− 0.5174 0.3020 0.3814
0 0.7289 0.7881 0.7574
+, − 0.5168 0.4437 0.4650
+,−,0 0.5875 0.5585 0.5625
</table>
<tableCaption confidence="0.9024055">
Table 4: Precision P, Recall R and F-Score F of Uni-
versity of Leipzig’s approach to SemEval-2013 task 2B
on SMS test data distinguished by classes (+, −, 0) and
averages of +, − and +, −, 0.
</tableCaption>
<table confidence="0.907663">
Test data Constr. Un/constr.
Twitter 18 of 35 23 of 48
SMS 20 of 28 31 of 42
</table>
<tableCaption confidence="0.951254">
Table 5: Ranks of University of Leipzig’s approach to
SemEval-2013 task 2B on Twitter and SMS test data in
the constrained only (Constr.) and the constrained and
unconstrained setting (Un/constr.).
</tableCaption>
<bodyText confidence="0.999266315789474">
On Twitter test data our system achieved an av-
eraged (+, −) F-Score of 0.5456, which is above
the average (0.5382) and above the median (0.5444).
Our system ranks 23 out of 48 participating systems
in the full constrained and unconstrained-ranking.
Averaging over over +, −, 0 it yields an F-Score of
0.595.
On SMS test data our system performs quite
poorly compared to other participating systems as (i)
we did not adapt our model to the SMS data at all,
e.g. we did not consider more appropriate or other
generalizations, and (ii) its class distribution is quite
different from our training data (cf. Table 1 vs. 2).
Our system achieved an averaged (+, −) F-Score of
0.465, which is below the average (0.5008) and be-
low the median (0.5060). Our system ranks 31 out of
42 participating systems in the full constrained and
unconstrained-ranking. Averaging over over +, −, 0
it yields an F-Score of 0.5625.
</bodyText>
<sectionHeader confidence="0.995519" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999980375">
We described University of Leipzig’s contribution
to SemEval-2013 task 2B on Sentiment Analysis in
Twitter. We approached the message polarity classi-
fication via well-understood and purely data-driven
lexical features, negation modeling, simple general-
izations as well as standard machine learning tech-
niques. Despite being designed as a baseline, our
system ranks midfield on both Twitter and SMS test
data.
As even the state-of-the-art system achieves
(+, −) averaged F-Scores of 0.6902 and 0.6846
on Twitter and SMS test data, respectively, polar-
ity classification of tweets and short messages still
proves to be a difficult task that is far from being
solved. Future enhancements of our system include
the use of more data-driven features, e.g. features
that model the distribution of abbreviations, punctu-
ation marks or capitalized text as well as fine-tuning
our generalization mechanism, e.g. by (i) general-
izing only low-frequency hash tags and usernames,
but not generalizing high-frequency ones, (ii) gener-
alizing acronyms that express laughter, such as lol
(“laughing out loud”) or rofl (“rolling on the floor
laughing”).
</bodyText>
<sectionHeader confidence="0.99818" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9817008">
S. Agarwal and H. Yu. 2010. Biomedical negation
scope detection with conditional random fields. Jour-
nal of the American Medical Informatics Association,
17(6):696–701.
A. Agarwal, B. Xie, I. Vovsha, O. Rambow, and R. Pas-
sonneau. 2011. Sentiment analysis of twitter data. In
Proceedings of the Workshop on Languages in Social
Media (LSM, pages 30–38.
L. Barbosa and J. Feng. 2010. Robust sentiment detec-
tion on twitter from biased and noisy data. In Proceed-
</reference>
<page confidence="0.996752">
453
</page>
<reference confidence="0.997052914285714">
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING), pages 36–44.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine Learning, 20(3):273–297.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Enhanced
sentiment learning using twitter hashtags and smileys.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING), pages 241–
249.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A pub-
licly available lexical resource for opinion mining. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), pages
417–422.
A. Go, R. Bhayani, and L. Huang. 2009. Twitter senti-
ment classification using distant supervision. CS224N
project report, Stanford University.
C. Hsu and C. Lin. 2002. A comparison of methods
for multiclass support vector machines. IEEE Trans-
actions on Neural Networks, 13(2):415–425.
L. Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao. 2011.
Target-dependent twitter sentiment classification. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
151–160.
E. Kouloumpis, T. Wilson, and J. Moore. 2011. Twitter
sentiment analysis: The good the bad and the OMG.
In Proceedings of the 5th International Conference on
Weblogs and Social Media (ICWSM), pages 538–541.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learn-
ing (ICML), pages 282–289.
A. Montejo-R´aez, E. Martınez-C´amara, M.T. Martın-
Valdivia, and L.A. Urena-L´opez. 2012. Random walk
weighting over sentiwordnet for sentiment polarity de-
tection on twitter. In Proceedings of the 3rd Workshop
on Computational Approaches to Subjectivity and Sen-
timent Analysis (WASSA), pages 3–10.
K. Morik, P. Brockhausen, and T. Joachims. 1999. Com-
bining statistical learning with a knowledge-based ap-
proach – a case study in intensive care monitoring. In
Proceedings of the 16th International Conference on
Machine Learning (ICML), pages 268–277.
A. Pak and P. Paroubek. 2010. Twitter as a corpus for
sentiment analysis and opinion mining. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC).
R. Remus. 2013. Negation modeling in machine
learning-based sentiment analysis. In forthcoming.
H. Saif, Y. He, and H. Alani. 2012. Alleviating data
sparsity for twitter sentiment analysis. In Proceedings
of the 2nd Workshop on Making Sense of Microposts
(#MSM).
V. Vapnik. 1995. The Nature of Statistical Learning.
Springer New York, NY.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 1(2):165–210.
M. Wiegand, A. Balahur, B. Roth, D. Klakow, and
A. Montoyo. 2010. A survey on the role of negation in
sentiment analysis. In Proceedings of the 2010 Work-
shop on Negation and Speculation in Natural Lan-
guage Processing (NeSp-NLP), pages 60–68.
T. Wilson, Z. Kozareva, P. Nakov, A. Ritter, S. Rosenthal,
and V. Stoyanov. 2013. SemEval-2013 task 2: Sen-
timent analysis in twitter. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval).
</reference>
<page confidence="0.999169">
454
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.791868">
<title confidence="0.9980245">ASVUniOfLeipzig: Sentiment Analysis in Twitter using Data-driven Machine Learning Techniques</title>
<author confidence="0.935007">Robert</author>
<affiliation confidence="0.949473666666667">Natural Language Processing Department of Computer University of Leipzig,</affiliation>
<email confidence="0.99757">rremus@informatik.uni-leipzig.de</email>
<abstract confidence="0.996693863636364">This paper describes University of Leipzig’s approach to SemEval-2013 task 2B on Sentiment Analysis in Twitter: message polarity classification. Our system is designed to function as a baseline, to see what we can accomplish with well-understood and purely data-driven lexical features, simple generalizations as well as standard machine learning techniques: We use one-against-one Support Vector Machines with asymmetric cost factors and linear “kernels” as classifiers, word uniand bigrams as features and additionally model negation of word uniand bigrams in feature space. We consider generalizations of URLs, user names, hash tags, repeated characters and expressions of laughter. Our method ranks 23 out of all 48 participating systems, achieving an averaged (positive, negative) F-Score of 0.5456 and an averaged (positive, negative, neutral) F-Score of 0.595, which is above median and average.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Agarwal</author>
<author>H Yu</author>
</authors>
<title>Biomedical negation scope detection with conditional random fields.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>17</volume>
<issue>6</issue>
<contexts>
<context position="7308" citStr="Agarwal and Yu, 2010" startWordPosition="1123" endWordPosition="1126">ing for word uni- and bigrams Word uni- and bigrams are induced data-driven, i.e. directly extracted from the textual data. We perform no feature selection; neither stop words nor punctuation marks are removed. We simply encode the presence or absence of word n-grams. 2Training data was downloaded on February 21, 2013, 9:18 a.m. and development data was downloaded on February 28, 2013, 10:41 a.m. using the original download script. 3http://opennlp.apache.org 451 Whether a word uni- or bigram is negated, i.e. appears inside of a negation scope (Wiegand et al., 2010), is detected by LingScope4 (Agarwal and Yu, 2010), a state-of-the-art negation scope detection based on Conditional Random Fields (Lafferty et al., 2001). We model the negation of word n-grams in an augmented word n-gram feature space as detailedly described in Remus (2013): In this feature space, each word n-gram is either represented as present ([1, 0]), absent ([0, 0]), present inside a negation scope ([0,1]) and present both inside and outside a negation scope ([1,1]). We trained a model for each feature set and chose the one that yields the highest accuracy: word uniand bigrams plus negation modeling for word uniand bigrams. 2.2.2 Gener</context>
</contexts>
<marker>Agarwal, Yu, 2010</marker>
<rawString>S. Agarwal and H. Yu. 2010. Biomedical negation scope detection with conditional random fields. Journal of the American Medical Informatics Association, 17(6):696–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Agarwal</author>
<author>B Xie</author>
<author>I Vovsha</author>
<author>O Rambow</author>
<author>R Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media (LSM,</booktitle>
<pages>30--38</pages>
<contexts>
<context position="2765" citStr="Agarwal et al. (2011)" startWordPosition="419" endWordPosition="422">.g. #bored) and smileys (e.g. :S) using words, word n-grams, punctuation marks and patterns as features. Barbosa and Feng (2010) map words to more general representations, i.e. part of speech (POS) tags and the words’ prior subjectivity and polarity. Additionally, they count the number of re-tweets, hash tags, replies, links etc. They then combine the outputs of 3 online sources of labeled but noisy and biased Twitter data into a more robust classification model. Saif et al. (2012) also address data sparsity via word clustering methods, i.e. semantic smoothing and sentiment-topics extraction. Agarwal et al. (2011) contrast a word unigram model, a tree kernel model and a model of various features, e.g. POS tag counts, summed up prior polarity scores, presence or absence of capitalized text, all applied to binary and ternary polarity classification. Kouloumpis et al. (2011) show that Twitter-specific feature engineering, e.g. representing the presence or absence of abbreviations and character repetitions improves model quality. Jiang et al. (2011) focus on targetdependent polarity classification regarding a given user query. While various models and features have been proposed, word n-gram models proved </context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>A. Agarwal, B. Xie, I. Vovsha, O. Rambow, and R. Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media (LSM, pages 30–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Barbosa</author>
<author>J Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>36--44</pages>
<contexts>
<context position="2272" citStr="Barbosa and Feng (2010)" startWordPosition="338" endWordPosition="342">arch on sentiment analysis in Twitter has received a lot of attention lately (Go et al., 2009; Pak and Paroubek, 2010). Language is usually used casually in Twitter and exhibits interesting properties. Therefore, some studies specifically address certain issues, e.g. a tweet’s length limitation of 140 characters, some studies leverage certain language characteristics, e.g. the presence of emoticons etc. Davidov et al. (2010) identify various “sentiment types” defined by Twitter hash tags (e.g. #bored) and smileys (e.g. :S) using words, word n-grams, punctuation marks and patterns as features. Barbosa and Feng (2010) map words to more general representations, i.e. part of speech (POS) tags and the words’ prior subjectivity and polarity. Additionally, they count the number of re-tweets, hash tags, replies, links etc. They then combine the outputs of 3 online sources of labeled but noisy and biased Twitter data into a more robust classification model. Saif et al. (2012) also address data sparsity via word clustering methods, i.e. semantic smoothing and sentiment-topics extraction. Agarwal et al. (2011) contrast a word unigram model, a tree kernel model and a model of various features, e.g. POS tag counts, s</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>L. Barbosa and J. Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="4655" citStr="Cortes and Vapnik, 1995" startWordPosition="702" endWordPosition="706"> ngram models are domain-specific per se: they “let the data speak for themselves”. Therefore we believe that carefully designing such a baseline using well-understood and purely data-driven lexical features, simple generalizations as well as standard machine learning techniques is a worthwhile endeavor. In the next Section we describe our system. In Section 3 we discuss its results in SemEval-2013 task 2B and finally conclude in Section 4. 2 System Description We approach the ternary polarity classification via one-against-one (Hsu and Lin, 2002) Support Vector Machines (SVMs) (Vapnik, 1995; Cortes and Vapnik, 1995) using a linear “kernel” as implemented by LibSVM1. To deal with the imbalanced class distribution of positive (+), negative (−) and neutralor-objective (0) instances, we use asymmetric cost factors C+, C−, C0 that allow for penalizing false positives and false negatives differently inside the one-against-one SVMs. While the majority class’ C0 is set to 1.0, the minority classes’ C{+,−}s are set as shown in (1) #(0-class instances) C{+,−} � (1) #({+, −}-class instances) similar to Morik et al. (1999)’s suggestion. 2.1 Data To develop our system, we use all training data available to us for tra</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support-vector networks. Machine Learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>O Tsur</author>
<author>A Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>241--249</pages>
<contexts>
<context position="2077" citStr="Davidov et al. (2010)" startWordPosition="309" endWordPosition="312">r’s growing popularity, the availability of large amounts of data that go along with that and the fact, that many people freely express their opinion on virtually everything using Twitter, research on sentiment analysis in Twitter has received a lot of attention lately (Go et al., 2009; Pak and Paroubek, 2010). Language is usually used casually in Twitter and exhibits interesting properties. Therefore, some studies specifically address certain issues, e.g. a tweet’s length limitation of 140 characters, some studies leverage certain language characteristics, e.g. the presence of emoticons etc. Davidov et al. (2010) identify various “sentiment types” defined by Twitter hash tags (e.g. #bored) and smileys (e.g. :S) using words, word n-grams, punctuation marks and patterns as features. Barbosa and Feng (2010) map words to more general representations, i.e. part of speech (POS) tags and the words’ prior subjectivity and polarity. Additionally, they count the number of re-tweets, hash tags, replies, links etc. They then combine the outputs of 3 online sources of labeled but noisy and biased Twitter data into a more robust classification model. Saif et al. (2012) also address data sparsity via word clustering</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>D. Davidov, O. Tsur, and A. Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 241– 249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>417--422</pages>
<contexts>
<context position="3948" citStr="Esuli and Sebastiani, 2006" startWordPosition="594" endWordPosition="597">ve been proposed, word n-gram models proved to be competitive in many studies (Barbosa and Feng, 2010; Agar450 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 450–454, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics wal et al., 2011; Saif et al., 2012) yet are straightforward to implement. Moreover, word n-gram models do not rely on hand-crafted and generally {genre, domain}-non-specific resources, e.g. prior polarity dictionaries like SentiWordNet (Esuli and Sebastiani, 2006) or Subjectivity Lexicon (Wiebe et al., 2005). In contrast, purely data-driven word ngram models are domain-specific per se: they “let the data speak for themselves”. Therefore we believe that carefully designing such a baseline using well-understood and purely data-driven lexical features, simple generalizations as well as standard machine learning techniques is a worthwhile endeavor. In the next Section we describe our system. In Section 3 we discuss its results in SemEval-2013 task 2B and finally conclude in Section 4. 2 System Description We approach the ternary polarity classification via</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>A. Esuli and F. Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Go</author>
<author>R Bhayani</author>
<author>L Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N project report,</title>
<date>2009</date>
<institution>Stanford University.</institution>
<contexts>
<context position="1742" citStr="Go et al., 2009" startWordPosition="260" endWordPosition="263">5, which is above median and average. 1 Introduction In SemEval-2013’s task 2B on Sentiment Analysis in Twitter, given a Twitter message, i.e. a tweet, the goal is to classify whether this tweet is of positive, negative, or neutral polarity (Wilson et al., 2013), i.e. the task is a ternary polarity classification. Due to Twitter’s growing popularity, the availability of large amounts of data that go along with that and the fact, that many people freely express their opinion on virtually everything using Twitter, research on sentiment analysis in Twitter has received a lot of attention lately (Go et al., 2009; Pak and Paroubek, 2010). Language is usually used casually in Twitter and exhibits interesting properties. Therefore, some studies specifically address certain issues, e.g. a tweet’s length limitation of 140 characters, some studies leverage certain language characteristics, e.g. the presence of emoticons etc. Davidov et al. (2010) identify various “sentiment types” defined by Twitter hash tags (e.g. #bored) and smileys (e.g. :S) using words, word n-grams, punctuation marks and patterns as features. Barbosa and Feng (2010) map words to more general representations, i.e. part of speech (POS) </context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>A. Go, R. Bhayani, and L. Huang. 2009. Twitter sentiment classification using distant supervision. CS224N project report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hsu</author>
<author>C Lin</author>
</authors>
<title>A comparison of methods for multiclass support vector machines.</title>
<date>2002</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="4584" citStr="Hsu and Lin, 2002" startWordPosition="692" endWordPosition="695">exicon (Wiebe et al., 2005). In contrast, purely data-driven word ngram models are domain-specific per se: they “let the data speak for themselves”. Therefore we believe that carefully designing such a baseline using well-understood and purely data-driven lexical features, simple generalizations as well as standard machine learning techniques is a worthwhile endeavor. In the next Section we describe our system. In Section 3 we discuss its results in SemEval-2013 task 2B and finally conclude in Section 4. 2 System Description We approach the ternary polarity classification via one-against-one (Hsu and Lin, 2002) Support Vector Machines (SVMs) (Vapnik, 1995; Cortes and Vapnik, 1995) using a linear “kernel” as implemented by LibSVM1. To deal with the imbalanced class distribution of positive (+), negative (−) and neutralor-objective (0) instances, we use asymmetric cost factors C+, C−, C0 that allow for penalizing false positives and false negatives differently inside the one-against-one SVMs. While the majority class’ C0 is set to 1.0, the minority classes’ C{+,−}s are set as shown in (1) #(0-class instances) C{+,−} � (1) #({+, −}-class instances) similar to Morik et al. (1999)’s suggestion. 2.1 Data </context>
</contexts>
<marker>Hsu, Lin, 2002</marker>
<rawString>C. Hsu and C. Lin. 2002. A comparison of methods for multiclass support vector machines. IEEE Transactions on Neural Networks, 13(2):415–425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Jiang</author>
<author>M Yu</author>
<author>M Zhou</author>
<author>X Liu</author>
<author>T Zhao</author>
</authors>
<title>Target-dependent twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>151--160</pages>
<contexts>
<context position="3205" citStr="Jiang et al. (2011)" startWordPosition="488" endWordPosition="491">ust classification model. Saif et al. (2012) also address data sparsity via word clustering methods, i.e. semantic smoothing and sentiment-topics extraction. Agarwal et al. (2011) contrast a word unigram model, a tree kernel model and a model of various features, e.g. POS tag counts, summed up prior polarity scores, presence or absence of capitalized text, all applied to binary and ternary polarity classification. Kouloumpis et al. (2011) show that Twitter-specific feature engineering, e.g. representing the presence or absence of abbreviations and character repetitions improves model quality. Jiang et al. (2011) focus on targetdependent polarity classification regarding a given user query. While various models and features have been proposed, word n-gram models proved to be competitive in many studies (Barbosa and Feng, 2010; Agar450 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 450–454, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics wal et al., 2011; Saif et al., 2012) yet are straightforward to implement. Moreover, word n-gram models do not rely on han</context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>L. Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao. 2011. Target-dependent twitter sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kouloumpis</author>
<author>T Wilson</author>
<author>J Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the OMG.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Conference on Weblogs and Social Media (ICWSM),</booktitle>
<pages>538--541</pages>
<contexts>
<context position="3028" citStr="Kouloumpis et al. (2011)" startWordPosition="464" endWordPosition="467">lly, they count the number of re-tweets, hash tags, replies, links etc. They then combine the outputs of 3 online sources of labeled but noisy and biased Twitter data into a more robust classification model. Saif et al. (2012) also address data sparsity via word clustering methods, i.e. semantic smoothing and sentiment-topics extraction. Agarwal et al. (2011) contrast a word unigram model, a tree kernel model and a model of various features, e.g. POS tag counts, summed up prior polarity scores, presence or absence of capitalized text, all applied to binary and ternary polarity classification. Kouloumpis et al. (2011) show that Twitter-specific feature engineering, e.g. representing the presence or absence of abbreviations and character repetitions improves model quality. Jiang et al. (2011) focus on targetdependent polarity classification regarding a given user query. While various models and features have been proposed, word n-gram models proved to be competitive in many studies (Barbosa and Feng, 2010; Agar450 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 450–454, Atlanta, Georgia, June 14-15, </context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>E. Kouloumpis, T. Wilson, and J. Moore. 2011. Twitter sentiment analysis: The good the bad and the OMG. In Proceedings of the 5th International Conference on Weblogs and Social Media (ICWSM), pages 538–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning (ICML),</booktitle>
<pages>282--289</pages>
<contexts>
<context position="7412" citStr="Lafferty et al., 2001" startWordPosition="1137" endWordPosition="1140">om the textual data. We perform no feature selection; neither stop words nor punctuation marks are removed. We simply encode the presence or absence of word n-grams. 2Training data was downloaded on February 21, 2013, 9:18 a.m. and development data was downloaded on February 28, 2013, 10:41 a.m. using the original download script. 3http://opennlp.apache.org 451 Whether a word uni- or bigram is negated, i.e. appears inside of a negation scope (Wiegand et al., 2010), is detected by LingScope4 (Agarwal and Yu, 2010), a state-of-the-art negation scope detection based on Conditional Random Fields (Lafferty et al., 2001). We model the negation of word n-grams in an augmented word n-gram feature space as detailedly described in Remus (2013): In this feature space, each word n-gram is either represented as present ([1, 0]), absent ([0, 0]), present inside a negation scope ([0,1]) and present both inside and outside a negation scope ([1,1]). We trained a model for each feature set and chose the one that yields the highest accuracy: word uniand bigrams plus negation modeling for word uniand bigrams. 2.2.2 Generalizations To account for Twitter’s typical language characteristics, we consider all possible combinati</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning (ICML), pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Montejo-R´aez</author>
<author>E Martınez-C´amara</author>
<author>M T MartınValdivia</author>
<author>L A Urena-L´opez</author>
</authors>
<title>Random walk weighting over sentiwordnet for sentiment polarity detection on twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA),</booktitle>
<pages>3--10</pages>
<marker>Montejo-R´aez, Martınez-C´amara, MartınValdivia, Urena-L´opez, 2012</marker>
<rawString>A. Montejo-R´aez, E. Martınez-C´amara, M.T. MartınValdivia, and L.A. Urena-L´opez. 2012. Random walk weighting over sentiwordnet for sentiment polarity detection on twitter. In Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA), pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Morik</author>
<author>P Brockhausen</author>
<author>T Joachims</author>
</authors>
<title>Combining statistical learning with a knowledge-based approach – a case study in intensive care monitoring.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Conference on Machine Learning (ICML),</booktitle>
<pages>268--277</pages>
<contexts>
<context position="5160" citStr="Morik et al. (1999)" startWordPosition="784" endWordPosition="787">ion via one-against-one (Hsu and Lin, 2002) Support Vector Machines (SVMs) (Vapnik, 1995; Cortes and Vapnik, 1995) using a linear “kernel” as implemented by LibSVM1. To deal with the imbalanced class distribution of positive (+), negative (−) and neutralor-objective (0) instances, we use asymmetric cost factors C+, C−, C0 that allow for penalizing false positives and false negatives differently inside the one-against-one SVMs. While the majority class’ C0 is set to 1.0, the minority classes’ C{+,−}s are set as shown in (1) #(0-class instances) C{+,−} � (1) #({+, −}-class instances) similar to Morik et al. (1999)’s suggestion. 2.1 Data To develop our system, we use all training data available to us for training and all development data available to us for testing, after removing 75 duplicates from the training data and 2 duplicates from the development data. Please note that 936 tweets of the originally provided training data and 3 tweets of the originally provided development data were not 1http://www.csie.ntu.edu.tw/˜cjlin/ libsvm/ available at our download time2. Table 1 summarizes the used data’s class distribution after duplicate removal. Data + − 0 E Training 3,263 1,278 4,132 8,673 Development </context>
</contexts>
<marker>Morik, Brockhausen, Joachims, 1999</marker>
<rawString>K. Morik, P. Brockhausen, and T. Joachims. 1999. Combining statistical learning with a knowledge-based approach – a case study in intensive care monitoring. In Proceedings of the 16th International Conference on Machine Learning (ICML), pages 268–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pak</author>
<author>P Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="1767" citStr="Pak and Paroubek, 2010" startWordPosition="264" endWordPosition="267"> median and average. 1 Introduction In SemEval-2013’s task 2B on Sentiment Analysis in Twitter, given a Twitter message, i.e. a tweet, the goal is to classify whether this tweet is of positive, negative, or neutral polarity (Wilson et al., 2013), i.e. the task is a ternary polarity classification. Due to Twitter’s growing popularity, the availability of large amounts of data that go along with that and the fact, that many people freely express their opinion on virtually everything using Twitter, research on sentiment analysis in Twitter has received a lot of attention lately (Go et al., 2009; Pak and Paroubek, 2010). Language is usually used casually in Twitter and exhibits interesting properties. Therefore, some studies specifically address certain issues, e.g. a tweet’s length limitation of 140 characters, some studies leverage certain language characteristics, e.g. the presence of emoticons etc. Davidov et al. (2010) identify various “sentiment types” defined by Twitter hash tags (e.g. #bored) and smileys (e.g. :S) using words, word n-grams, punctuation marks and patterns as features. Barbosa and Feng (2010) map words to more general representations, i.e. part of speech (POS) tags and the words’ prior</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>A. Pak and P. Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Remus</author>
</authors>
<title>Negation modeling in machine learning-based sentiment analysis.</title>
<date>2013</date>
<booktitle>In forthcoming.</booktitle>
<contexts>
<context position="7533" citStr="Remus (2013)" startWordPosition="1160" endWordPosition="1161">esence or absence of word n-grams. 2Training data was downloaded on February 21, 2013, 9:18 a.m. and development data was downloaded on February 28, 2013, 10:41 a.m. using the original download script. 3http://opennlp.apache.org 451 Whether a word uni- or bigram is negated, i.e. appears inside of a negation scope (Wiegand et al., 2010), is detected by LingScope4 (Agarwal and Yu, 2010), a state-of-the-art negation scope detection based on Conditional Random Fields (Lafferty et al., 2001). We model the negation of word n-grams in an augmented word n-gram feature space as detailedly described in Remus (2013): In this feature space, each word n-gram is either represented as present ([1, 0]), absent ([0, 0]), present inside a negation scope ([0,1]) and present both inside and outside a negation scope ([1,1]). We trained a model for each feature set and chose the one that yields the highest accuracy: word uniand bigrams plus negation modeling for word uniand bigrams. 2.2.2 Generalizations To account for Twitter’s typical language characteristics, we consider all possible combinations of generalizations of the following character sequences, inspired by (Montejo-R´aez et al., 2012): a. User names, tha</context>
</contexts>
<marker>Remus, 2013</marker>
<rawString>R. Remus. 2013. Negation modeling in machine learning-based sentiment analysis. In forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saif</author>
<author>Y He</author>
<author>H Alani</author>
</authors>
<title>Alleviating data sparsity for twitter sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2nd Workshop on Making Sense of Microposts (#MSM).</booktitle>
<contexts>
<context position="2630" citStr="Saif et al. (2012)" startWordPosition="400" endWordPosition="403">istics, e.g. the presence of emoticons etc. Davidov et al. (2010) identify various “sentiment types” defined by Twitter hash tags (e.g. #bored) and smileys (e.g. :S) using words, word n-grams, punctuation marks and patterns as features. Barbosa and Feng (2010) map words to more general representations, i.e. part of speech (POS) tags and the words’ prior subjectivity and polarity. Additionally, they count the number of re-tweets, hash tags, replies, links etc. They then combine the outputs of 3 online sources of labeled but noisy and biased Twitter data into a more robust classification model. Saif et al. (2012) also address data sparsity via word clustering methods, i.e. semantic smoothing and sentiment-topics extraction. Agarwal et al. (2011) contrast a word unigram model, a tree kernel model and a model of various features, e.g. POS tag counts, summed up prior polarity scores, presence or absence of capitalized text, all applied to binary and ternary polarity classification. Kouloumpis et al. (2011) show that Twitter-specific feature engineering, e.g. representing the presence or absence of abbreviations and character repetitions improves model quality. Jiang et al. (2011) focus on targetdependent</context>
</contexts>
<marker>Saif, He, Alani, 2012</marker>
<rawString>H. Saif, Y. He, and H. Alani. 2012. Alleviating data sparsity for twitter sentiment analysis. In Proceedings of the 2nd Workshop on Making Sense of Microposts (#MSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning.</title>
<date>1995</date>
<publisher>Springer</publisher>
<location>New York, NY.</location>
<contexts>
<context position="4629" citStr="Vapnik, 1995" startWordPosition="700" endWordPosition="701">ta-driven word ngram models are domain-specific per se: they “let the data speak for themselves”. Therefore we believe that carefully designing such a baseline using well-understood and purely data-driven lexical features, simple generalizations as well as standard machine learning techniques is a worthwhile endeavor. In the next Section we describe our system. In Section 3 we discuss its results in SemEval-2013 task 2B and finally conclude in Section 4. 2 System Description We approach the ternary polarity classification via one-against-one (Hsu and Lin, 2002) Support Vector Machines (SVMs) (Vapnik, 1995; Cortes and Vapnik, 1995) using a linear “kernel” as implemented by LibSVM1. To deal with the imbalanced class distribution of positive (+), negative (−) and neutralor-objective (0) instances, we use asymmetric cost factors C+, C−, C0 that allow for penalizing false positives and false negatives differently inside the one-against-one SVMs. While the majority class’ C0 is set to 1.0, the minority classes’ C{+,−}s are set as shown in (1) #(0-class instances) C{+,−} � (1) #({+, −}-class instances) similar to Morik et al. (1999)’s suggestion. 2.1 Data To develop our system, we use all training da</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning. Springer New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="3993" citStr="Wiebe et al., 2005" startWordPosition="601" endWordPosition="604">petitive in many studies (Barbosa and Feng, 2010; Agar450 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 450–454, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics wal et al., 2011; Saif et al., 2012) yet are straightforward to implement. Moreover, word n-gram models do not rely on hand-crafted and generally {genre, domain}-non-specific resources, e.g. prior polarity dictionaries like SentiWordNet (Esuli and Sebastiani, 2006) or Subjectivity Lexicon (Wiebe et al., 2005). In contrast, purely data-driven word ngram models are domain-specific per se: they “let the data speak for themselves”. Therefore we believe that carefully designing such a baseline using well-understood and purely data-driven lexical features, simple generalizations as well as standard machine learning techniques is a worthwhile endeavor. In the next Section we describe our system. In Section 3 we discuss its results in SemEval-2013 task 2B and finally conclude in Section 4. 2 System Description We approach the ternary polarity classification via one-against-one (Hsu and Lin, 2002) Support </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 1(2):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wiegand</author>
<author>A Balahur</author>
<author>B Roth</author>
<author>D Klakow</author>
<author>A Montoyo</author>
</authors>
<title>A survey on the role of negation in sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Negation and Speculation in Natural Language Processing (NeSp-NLP),</booktitle>
<pages>60--68</pages>
<contexts>
<context position="7258" citStr="Wiegand et al., 2010" startWordPosition="1115" endWordPosition="1118">grams e. word uni- and bigrams plus negation modeling for word uni- and bigrams Word uni- and bigrams are induced data-driven, i.e. directly extracted from the textual data. We perform no feature selection; neither stop words nor punctuation marks are removed. We simply encode the presence or absence of word n-grams. 2Training data was downloaded on February 21, 2013, 9:18 a.m. and development data was downloaded on February 28, 2013, 10:41 a.m. using the original download script. 3http://opennlp.apache.org 451 Whether a word uni- or bigram is negated, i.e. appears inside of a negation scope (Wiegand et al., 2010), is detected by LingScope4 (Agarwal and Yu, 2010), a state-of-the-art negation scope detection based on Conditional Random Fields (Lafferty et al., 2001). We model the negation of word n-grams in an augmented word n-gram feature space as detailedly described in Remus (2013): In this feature space, each word n-gram is either represented as present ([1, 0]), absent ([0, 0]), present inside a negation scope ([0,1]) and present both inside and outside a negation scope ([1,1]). We trained a model for each feature set and chose the one that yields the highest accuracy: word uniand bigrams plus nega</context>
</contexts>
<marker>Wiegand, Balahur, Roth, Klakow, Montoyo, 2010</marker>
<rawString>M. Wiegand, A. Balahur, B. Roth, D. Klakow, and A. Montoyo. 2010. A survey on the role of negation in sentiment analysis. In Proceedings of the 2010 Workshop on Negation and Speculation in Natural Language Processing (NeSp-NLP), pages 60–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>Z Kozareva</author>
<author>P Nakov</author>
<author>A Ritter</author>
<author>S Rosenthal</author>
<author>V Stoyanov</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval).</booktitle>
<contexts>
<context position="1389" citStr="Wilson et al., 2013" startWordPosition="201" endWordPosition="204">model negation of word uni- and bigrams in word n-gram feature space. We consider generalizations of URLs, user names, hash tags, repeated characters and expressions of laughter. Our method ranks 23 out of all 48 participating systems, achieving an averaged (positive, negative) F-Score of 0.5456 and an averaged (positive, negative, neutral) F-Score of 0.595, which is above median and average. 1 Introduction In SemEval-2013’s task 2B on Sentiment Analysis in Twitter, given a Twitter message, i.e. a tweet, the goal is to classify whether this tweet is of positive, negative, or neutral polarity (Wilson et al., 2013), i.e. the task is a ternary polarity classification. Due to Twitter’s growing popularity, the availability of large amounts of data that go along with that and the fact, that many people freely express their opinion on virtually everything using Twitter, research on sentiment analysis in Twitter has received a lot of attention lately (Go et al., 2009; Pak and Paroubek, 2010). Language is usually used casually in Twitter and exhibits interesting properties. Therefore, some studies specifically address certain issues, e.g. a tweet’s length limitation of 140 characters, some studies leverage cer</context>
<context position="10955" citStr="Wilson et al., 2013" startWordPosition="1733" endWordPosition="1736">1/10th is used for testing. C values are chosen from 12 · 10−3, 2 · 10−2, 2 · 10−1, 2 · 100, 2 · 101, 2 · 102, 2 · 1031. Internally, the asymmetric cost factors C+, C−, C0 (cf. Section 2) are then set to C{+,−,0} C · C{+,−,0}. The final system is then applied to both Twitter and SMS test data (cf. Table 2). Please note Test Data + − 0 E Twitter 1,572 601 1,640 3,813 SMS 492 394 1,208 2,094 Table 2: Class distribution of positive (+), negative (−) and neutral-or-objective (0) instances in Twitter and SMS testing data. that we only participate in the constrained setting of SemEval-2013 task 2B (Wilson et al., 2013) as we did not use any additional training data. 452 Detailed evaluation results on Twitter test data are shown in Table 3, results on SMS test data are shown in Table 4. The ranks we achieved in the constrained only-ranking and the full constrained and unconstrained-ranking are shown in Table 5. Class P R F + 0.7307 0.5833 0.6487 − 0.5795 0.3577 0.4424 0 0.6072 0.8098 0.6940 +, − 0.6551 0.4705 0.5456 +,−,0 0.6391 0.5836 0.5950 Table 3: Precision P, Recall R and F-Score F of University of Leipzig’s approach to SemEval-2013 task 2B on Twitter test data distinguished by classes (+, −, 0) and ave</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyanov, 2013</marker>
<rawString>T. Wilson, Z. Kozareva, P. Nakov, A. Ritter, S. Rosenthal, and V. Stoyanov. 2013. SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>