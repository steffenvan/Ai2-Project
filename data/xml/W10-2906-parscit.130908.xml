<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.822547">
Learning Better Monolingual Models with Unannotated Bilingual Text
</title>
<author confidence="0.993267">
David Burkett† Slav Petrov$ John Blitzer† Dan Klein†
</author>
<affiliation confidence="0.999486">
†University of California, Berkeley $Google Research
</affiliation>
<email confidence="0.999091">
{dburkett,blitzer,klein}@cs.berkeley.edu slav@google.com
</email>
<sectionHeader confidence="0.994805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937952380953">
This work shows how to improve state-of-the-art
monolingual natural language processing models
using unannotated bilingual text. We build a mul-
tiview learning objective that enforces agreement
between monolingual and bilingual models. In
our method the first, monolingual view consists of
supervised predictors learned separately for each
language. The second, bilingual view consists of
log-linear predictors learned over both languages
on bilingual text. Our training procedure estimates
the parameters of the bilingual model using the
output of the monolingual model, and we show how
to combine the two models to account for depen-
dence between views. For the task of named entity
recognition, using bilingual predictors increases F,
by 16.1% absolute over a supervised monolingual
model, and retraining on bilingual predictions
increases monolingual model F, by 14.6%. For
syntactic parsing, our bilingual predictor increases
F, by 2.1% absolute, and retraining a monolingual
model on its output gives an improvement of 2.0%.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999889275862069">
Natural language analysis in one language can be
improved by exploiting translations in another lan-
guage. This observation has formed the basis for
important work on syntax projection across lan-
guages (Yarowsky et al., 2001; Hwa et al., 2005;
Ganchev et al., 2009) and unsupervised syntax
induction in multiple languages (Snyder et al.,
2009), as well as other tasks, such as cross-lingual
named entity recognition (Huang and Vogel, 2002;
Moore, 2003) and information retrieval (Si and
Callan, 2005). In all of these cases, multilingual
models yield increased accuracy because differ-
ent languages present different ambiguities and
therefore offer complementary constraints on the
shared underlying labels.
In the present work, we consider a setting where
we already possess supervised monolingual mod-
els, and wish to improve these models using unan-
notated bilingual parallel text (bitext). We cast this
problem in the multiple-view (multiview) learning
framework (Blum and Mitchell, 1998; Collins and
Singer, 1999; Balcan and Blum, 2005; Ganchev et
al., 2008). Our two views are a monolingual view,
which uses the supervised monolingual models but
not bilingual information, and a bilingual view,
which exploits features that measure agreement
across languages. The parameters of the bilin-
gual view are trained to reproduce the output of
the monolingual view. We show that by introduc-
ing weakened monolingual models into the bilin-
gual view, we can optimize the parameters of the
bilingual model to improve monolingual models.
At prediction time, we automatically account for
the between-view dependence introduced by the
weakened monolingual models with a simple but
effective view-combination heuristic.
We demonstrate the performance of this method
on two problems. The first is named en-
tity recognition (NER). For this problem, our
method automatically learns (a variation on) ear-
lier hand-designed rule-based bilingual NER pre-
dictors (Huang and Vogel, 2002; Moore, 2003),
resulting in absolute performance gains of up to
16.1% F1. The second task we consider is statis-
tical parsing. For this task, we follow the setup
of Burkett and Klein (2008), who improved Chi-
nese and English monolingual parsers using par-
allel, hand-parsed text. We achieve nearly iden-
tical improvements using a purely unlabeled bi-
text. These results carry over to machine transla-
tion, where we can achieve slightly better BLEU
improvements than the supervised model of Bur-
kett and Klein (2008) since we are able to train
our model directly on the parallel data where we
perform rule extraction.
Finally, for both of our tasks, we use our bilin-
gual model to generate additional automatically
labeled monolingual training data. We compare
</bodyText>
<page confidence="0.994215">
46
</page>
<note confidence="0.9552175">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 46–54,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999848666666667">
this approach to monolingual self-training and
show an improvement of up to 14.4% F1 for entity
recognition. Even for parsing, where the bilingual
portion of the treebank is much smaller than the
monolingual, our technique still can improve over
purely monolingual self-training by 0.7% F1.
</bodyText>
<sectionHeader confidence="0.807759" genericHeader="introduction">
2 Prior Work on Learning from
Bilingual Text
</sectionHeader>
<bodyText confidence="0.999956285714286">
Prior work in learning monolingual models from
bitexts falls roughly into three categories: Unsu-
pervised induction, cross-lingual projection, and
bilingual constraints for supervised monolingual
models. Two recent, successful unsupervised
induction methods are those of Blunsom et al.
(2009) and Snyder et al. (2009). Both of them es-
timate hierarchical Bayesian models and employ
bilingual data to constrain the types of models that
can be derived. Projection methods, on the other
hand, were among the first applications of parallel
text (after machine translation) (Yarowsky et al.,
2001; Yarowsky and Ngai, 2001; Hwa et al., 2005;
Ganchev et al., 2009). They assume the existence
of a good, monolingual model for one language
but little or no information about the second lan-
guage. Given a parallel sentence pair, they use the
annotations for one language to heavily constrain
the set of possible annotations for the other.
Our work falls into the final category: We wish
to use bilingual data to improve monolingual mod-
els which are already trained on large amounts of
data and effective on their own (Huang and Vo-
gel, 2002; Smith and Smith, 2004; Snyder and
Barzilay, 2008; Burkett and Klein, 2008). Proce-
durally, our work is most closely related to that
of Burkett and Klein (2008). They used an an-
notated bitext to learn parse reranking models for
English and Chinese, exploiting features that ex-
amine pieces of parse trees in both languages. Our
method can be thought of as the semi-supervised
counterpart to their supervised model. Indeed, we
achieve nearly the same results, but without anno-
tated bitexts. Smith and Smith (2004) consider
a similar setting for parsing both English and Ko-
rean, but instead of learning a joint model, they
consider a fixed combination of two parsers and
a word aligner. Our model learns parameters for
combining two monolingual models and poten-
tially thousands of bilingual features. The result
is that our model significantly improves state-of-
the-art results, for both parsing and NER.
</bodyText>
<sectionHeader confidence="0.976757" genericHeader="method">
3 A Multiview Bilingual Model
</sectionHeader>
<bodyText confidence="0.999926548387097">
Given two input sentences x = (x1, x2) that
are word-aligned translations of each other, we
consider the problem of predicting (structured)
labels y = (y1, y2) by estimating conditional
models on pairs of labels from both languages,
p(y1, y2|x1, x2). Our model consists of two views,
which we will refer to as monolingual and bilin-
gual. The monolingual view estimates the joint
probability as the product of independent marginal
distributions over each language, pm(y|x) =
p1(y1|x1)p2(y2|x2). In our applications, these
marginal distributions will be computed by state-
of-the-art statistical taggers and parsers trained on
large monolingual corpora.
This work focuses on learning parameters for
the bilingual view of the data. We parameterize
the bilingual view using at most one-to-one match-
ings between nodes of structured labels in each
language (Burkett and Klein, 2008). In this work,
we use the term node to indicate a particular com-
ponent of a label, such as a single (multi-word)
named entity or a node in a parse tree. In Fig-
ure 2(a), for example, the nodes labeled NP1 in
both the Chinese and English trees are matched.
Since we don’t know a priori how the components
relate to one another, we treat these matchings as
hidden. For each matching a and pair of labels
y, we define a feature vector φ(y1, a, y2) which
factors on edges in the matching. Our model is
a conditional exponential family distribution over
matchings and labels:
</bodyText>
<equation confidence="0.9011915">
1 ]
p�(y, a|x) = exp θ�φ(y1, a, y2) − A(θ; x) ,
</equation>
<bodyText confidence="0.999588333333333">
where θ is a parameter vector, and A(θ; x) is the
log partition function for a sentence pair x. We
must approximate A(θ; x) because summing over
all at most one-to-one matchings a is #P-hard. We
approximate this sum using the maximum-scoring
matching (Burkett and Klein, 2008):
</bodyText>
<equation confidence="0.47989">
(exp1θTφ(y1 , a, y2)] I .
</equation>
<bodyText confidence="0.99826275">
In order to compute the distribution on labels y, we
must marginalize over hidden alignments between
nodes, which we also approximate by using the
maximum-scoring matching:
</bodyText>
<equation confidence="0.866273777777778">
1 ]
exp θ�φ(y1, a, y2)− �A(θ; x) .
E
�A(θ; x) = log
y
max
a
qe(y|x) def = max
a
</equation>
<page confidence="0.93731">
47
</page>
<figure confidence="0.997461714285714">
Feat. types Examples
Algn Densty INSIDEBOTH=3 INENONLY=0
Indicators LBLMATCH=true BIAS=true
ORG1
the reports of the European Court of Auditors
die Berichte des EuropŠischen Rechnungshofes
ORG1
</figure>
<figureCaption confidence="0.9892865">
Figure 1: An example where English NER can be
used to disambiguate German NER.
</figureCaption>
<bodyText confidence="0.9999605">
We further simplify inference in our model by
working in a reranking setting (Collins, 2000;
Charniak and Johnson, 2005), where we only con-
sider the top k outputs from monolingual models
in both languages, for a total of k2 labels y. In
practice, k2 &lt; 10, 000 for our largest problem.
</bodyText>
<subsectionHeader confidence="0.998983">
3.1 Including Weakened Models
</subsectionHeader>
<bodyText confidence="0.998478951219512">
Now that we have defined our bilingual model, we
could train it to agree with the output of the mono-
lingual model (Collins and Singer, 1999; Ganchev
et al., 2008). As we will see in Section 4, however,
the feature functions 0(y1, a, y2) make no refer-
ence to the input sentences x, other than through a
fixed word alignment. With such limited monolin-
gual information, it is impossible for the bilingual
model to adequately capture all of the information
necessary for NER or parsing. As a simple ex-
ample, a bilingual NER model will be perfectly
happy to label two aligned person names as ORG
instead of PER: both labelings agree equally well.
We briefly illustrate how poorly such a basic bilin-
gual model performs in Section 10.
One way to solve this problem is to include the
output of the full monolingual models as features
in the bilingual view. However, we are training the
bilingual view to match the output of these same
models, which can be trivially achieved by putting
weight on only the monolingual model scores and
never recruiting any bilingual features. There-
fore, we use an intermediate approach: we intro-
duce the output of deliberately weakened mono-
lingual models as features in the bilingual view.
A weakened model is from the same class as the
full monolingual models, but is intentionally crip-
pled in some way (by removing feature templates,
for example). Crucially, the weakened models will
make predictions that are roughly similar to the
full models, but systematically worse. Therefore,
model scores from the weakened models provide
enough power for the bilingual view to make accu-
Table 1: Sample features used for named entity
recognition for the ORG entity in Figure 1.
rate predictions, but ensure that bilingual features
will be required to optimize the training objective.
Let `W1 = log pW 1 (y1|x1), `W2 = log pW2 (y2|x2)
be the log-probability scores from the weakened
models. Our final approximation to the marginal
distribution over labels y is:
</bodyText>
<equation confidence="0.980616">
[exp �1f� 1 + X2f2 +
Where
�A(A1, a2, θ; x) =
exp [A1f�1 + A2f2 + θT
φ(y1, a, y2)]
</equation>
<bodyText confidence="0.762084">
is the updated approximate log partition function.
</bodyText>
<sectionHeader confidence="0.824727" genericHeader="method">
4 NER and Parsing Examples
</sectionHeader>
<bodyText confidence="0.999848357142857">
Before formally describing our algorithm for find-
ing the parameters [λ1, λ2, 01, we first give exam-
ples of our problems of named entity recognition
and syntactic parsing, together with node align-
ments and features for each. Figure 1 depicts a
correctly-labeled sentence fragment in both En-
glish and German. In English, the capitalization of
the phrase European Court ofAuditors helps iden-
tify the span as a named entity. However, in Ger-
man, all nouns are capitalized, and capitalization
is therefore a less useful cue. While a monolin-
gual German tagger is likely to miss the entity in
the German text, by exploiting the parallel English
text and word alignment information, we can hope
to improve the German performance, and correctly
tag Europ¨aischen Rechnungshofes.
The monolingual features are standard features
for discriminative, state-of-the-art entity recogniz-
ers, and we can produce weakened monolingual
models by simply limiting the feature set. The
bilingual features, 0(y1, a, y2), are over pairs of
aligned nodes, where nodes of the labels y1 and
y2 are simply the individual named entities. We
use a small bilingual feature set consisting of two
types of features. First, we use the word alignment
density features from Burkett and Klein (2008),
which measure how well the aligned entity pair
matches up with alignments from an independent
</bodyText>
<figure confidence="0.978193888888889">
qa1,a2,e(y|x) def= max
a
(1)
θT φ(y1, a, y2) − A(A1, A2, θ; x)] .
max
a
�
log
�
</figure>
<page confidence="0.994678">
48
</page>
<bodyText confidence="0.460120333333333">
Input: full and weakened monolingual models: Input: full and weakened monolingual models:
p1(y1|x1), p2(y2|x2), pw1 (y1|x1), pw2 (y2|x2) p1(y1|x1), p2(y2|x2), pw1 (y1|x1), pw2 (y2|x2)
unannotated bilingual data: U bilingual parameters: e, X1, X2
Output: bilingual parameters: e, X1, X2 bilingual input: x = (x1, x2)
1. Label U with full monolingual models: Output: bilingual label: y&amp;quot;
Vx E U, yM = argmaxy p1(y1|x1)p2(y2|x2). Bilingual w/ Weak Bilingual w/ Full
F 1a. l1 = log (pi (y1|x1)) 1b. l1 = log (p1 (y1|x1))
2. Return argmaxλ1,λ2,� x�U q�,λ1,λ2(�yM|x), 2a. l2 = log (pw2 (y2|x2)) 2b. l2 = log (p2(y2|x2))
where qg,λ1,λ2 has the form in Equation 1. 3. Return argmaxy maxa �X1l1 + �X2l2+�eT¢(y1, a, y2)
</bodyText>
<figureCaption confidence="0.996363">
Figure 3: Bilingual training with multiple views.
</figureCaption>
<bodyText confidence="0.999881038461538">
word aligner. We also include two indicator fea-
tures: a bias feature that allows the model to learn
a general preference for matched entities, and a
feature that is active whenever the pair of nodes
has the same label. Figure 1 contains sample val-
ues for each of these features.
Another natural setting where bilingual con-
straints can be exploited is syntactic parsing. Fig-
ure 2 shows an example English prepositional
phrase attachment ambiguity that can be resolved
bilingually by exploiting Chinese. The English
monolingual parse mistakenly attaches to to the
verb increased. In Chinese, however, this ambi-
guity does not exist. Instead, the word Xf, which
aligns to to, has strong selectional preference for
attaching to a noun on the left.
In our parsing experiments, we use the Berke-
ley parser (Petrov et al., 2006; Petrov and Klein,
2007), a split-merge latent variable parser, for our
monolingual models. Our full model is the re-
sult of training the parser with five split-merge
phases. Our weakened model uses only two. For
the bilingual model, we use the same bilingual fea-
ture set as Burkett and Klein (2008). Table 2 gives
some examples, but does not exhaustively enumer-
ate those features.
</bodyText>
<sectionHeader confidence="0.94063" genericHeader="method">
5 Training Bilingual Models
</sectionHeader>
<bodyText confidence="0.999763571428572">
Previous work in multiview learning has focused
on the case of agreement regularization (Collins
and Singer, 1999; Ganchev et al., 2008). If we had
bilingual labeled data, together with our unlabeled
data and monolingual labeled data, we could ex-
ploit these techniques. Because we do not possess
bilingual labeled data, we must train the bilingual
model in another way. Here we advocate train-
ing the bilingual model (consisting of the bilin-
gual features and weakened monolingual models)
to imitate the full monolingual models. In terms
of agreement regularization, our procedure may be
thought of as “regularizing” the bilingual model to
be similar to the full monolingual models.
</bodyText>
<figureCaption confidence="0.921828">
Figure 4: Prediction by combining monolingual
and bilingual models.
</figureCaption>
<bodyText confidence="0.999955130434783">
Our training algorithm is summarized in Fig-
ure 3. For each unlabeled point x = (x1, x2), let
yM be the joint label which has the highest score
from the independent monolingual models (line
1). We then find bilingual parameters 9, �1, �2
that maximize qˆ�,ˆλ1,ˆλ2(Px&apos;x) (line 2). This max-
likelihood optimization can be solved by an EM-
like procedure (Burkett and Klein, 2008). This
procedure iteratively updates the parameter esti-
mates by (a) finding the optimum alignments for
each candidate label pair under the current pa-
rameters and then (b) updating the parameters to
maximize a modified version of Equation 1, re-
stricted to the optimal alignments. Because we re-
strict alignments to the set of at most one-to-one
matchings, the (a) step is tractable using the Hun-
garian algorithm. With the alignments fixed, the
(b) step just involves maximizing likelihood under
a log-linear model with no latent variables – this
problem is convex and can be solved efficiently
using gradient-based methods. The procedure has
no guarantees, but is observed in practice to con-
verge to a local optimum.
</bodyText>
<sectionHeader confidence="0.734816" genericHeader="method">
6 Predicting with Monolingual and
</sectionHeader>
<subsectionHeader confidence="0.750055">
Bilingual Models
</subsectionHeader>
<bodyText confidence="0.989271">
Once we have learned the parameters of the bilin-
gual model, the standard method of bilingual pre-
diction would be to just choose the y that is most
likely under qˆ�,ˆλ1,ˆλ2:
</bodyText>
<equation confidence="0.999294">
y� = argmax
y qˆe,ˆλ1,ˆλ2(y&apos;x) . (2)
</equation>
<bodyText confidence="0.999987857142857">
We refer to prediction under this model as “Bilin-
gual w/ Weak,” to evoke the fact that the model is
making use of weakened monolingual models in
its feature set.
Given that we have two views of the data,
though, we should be able to leverage additional
information in order to make better predictions. In
</bodyText>
<page confidence="0.998905">
49
</page>
<figureCaption confidence="0.980837">
Figure 2: An example of PP attachment that is ambiguous in English, but simple in Chinese. In (a) the
correct parses agree (low PP attachment), whereas in (b) the incorrect parses disagree.
</figureCaption>
<table confidence="0.9995842">
Feature Types Feature Templates Examples
Correct Incorrect
Alignment Density INSIDEBOTH, INSIDEENONLY INSIDEENONLY=0 INSIDEENONLY=1
Span Difference ABSDIFFERENCE ABSDIFFERENCE=3 ABSDIFFERENCE=4
Syntactic Indicators LABEL(E,C), NUMCHILDREN(E,C) LABEL(NP,NP)=true LABEL(VP,NP)=true
</table>
<tableCaption confidence="0.965857">
Table 2: Sample bilingual features used for parsing. The examples are features that would be extracted
by aligning the parents of the PP nodes in Figure 2(a) (Correct) and Figure 2(b) (Incorrect).
</tableCaption>
<figure confidence="0.82210284375">
S
S
S
(a)
S
(b)
VP
VP1
NP1
NP
NP
VB
These measures increased the attractiveness of Tianjin to Taiwanese merchants
NP
VB
i�s Atp-f R VA-n M IR41ji
NP PP PP
NP PP PP
VB NNP
DE NN
VB NNP
DE NN
NP
PP
NP
PP
NP1
NP1
VP
VP
i�s Atp-f R VA-n M IR41ji
These measures increased the attractiveness of Tianjin to Taiwanese merchants
</figure>
<bodyText confidence="0.998681666666667">
particular, the monolingual view uses monolingual
models that are known to be superior to the mono-
lingual information available in the bilingual view.
Thus, we would like to find some way to incorpo-
rate the full monolingual models into our predic-
tion method. One obvious choice is to choose the
labeling that maximizes the “agreement distribu-
tion” (Collins and Singer, 1999; Ganchev et al.,
2008). In our setting, this amounts to choosing:
</bodyText>
<equation confidence="0.988258">
pM(y|x) qˆ�,ˆ�1ˆ�2(y|x) . (3)
</equation>
<bodyText confidence="0.9999591">
This is the correct decision rule if the views are
independent and the labels y are uniformly dis-
tributed a priori,1 but we have deliberately in-
troduced between-view dependence in the form
of the weakened monolingual models. Equa-
tion 3 implicitly double-counts monolingual infor-
mation.
One way to avoid this double-counting is to
simply discard the weakened monolingual models
when making a joint prediction:
</bodyText>
<equation confidence="0.9760904">
yˆ = argmax
y
I
] (4)
exp ˆe�o(y1, a, y2) .
</equation>
<bodyText confidence="0.977504555555555">
1See, e.g. Ando &amp; Zhang(Ando and Zhang, 2007) for a
derivation of the decision rule from Equation 3 under these
assumptions.
This decision rule uniformly combines the two
monolingual models and the bilingual model.
Note, however, that we have already learned non-
uniform weights for the weakened monolingual
models. Our final decision rule uses these weights
as weights for the full monolingual models:
</bodyText>
<equation confidence="0.994142">
[ ˆλ1 log (p1(y1|x1)) +
exp
]ˆλ2 log (p2(y2|x2))+ˆe�o(y1, a, y2) . (5)
</equation>
<bodyText confidence="0.996826181818182">
As we will show in Section 10, this rule for com-
bining the monolingual and bilingual views per-
forms significantly better than the alternatives, and
comes close to the optimal weighting for the bilin-
gual and monolingual models.
We will refer to predictions made with Equa-
tion 5 as “Bilingual w/ Full”, to evoke the use of
the full monolingual models alongside our bilin-
gual features. Prediction using “Bilingual w/
Weak” and “Bilingual w/ Full” is summarized in
Figure 4.
</bodyText>
<sectionHeader confidence="0.983984" genericHeader="method">
7 Retraining Monolingual Models
</sectionHeader>
<bodyText confidence="0.998667">
Although bilingual models have many direct ap-
plications (e.g. in machine translation), we also
wish to be able to apply our models on purely
monolingual data. In this case, we can still take
</bodyText>
<figure confidence="0.933817666666667">
yˆ = argmax
y
max
a
pM(y|x)
yˆ = argmax
y
max
a
</figure>
<page confidence="0.904404">
50
</page>
<bodyText confidence="0.58209675">
Input: annotated monolingual data: L1, L2
unannotated bilingual data: U
monolingual models: p1(y1|x1), p2(y2|x2)
bilingual parameters: e, �λ1, �λ2
</bodyText>
<equation confidence="0.781200166666667">
Output: retrained monolingual models:
pr1(y1|x1), pr2(y2|x2)
b&apos;x = (x1, x2) E U:
Self-Retrained Bilingual-Retrained
1a. yx1 = argmaxy1 p1(y1|x1) 1b. Pick yx, Fig. 4
yx2 = argmaxy2 p2(y2|x2) (Bilingual w/ Full)
</equation>
<listItem confidence="0.67507">
2. Add (x1, D/x1) to L1 and add (x2, �yx2) to L2.
3. Return full monolingual models pr1(y1|x1),
pr2(y2|x2) trained on newly enlarged L1, L2.
</listItem>
<figureCaption confidence="0.998731">
Figure 5: Retraining monolingual models.
</figureCaption>
<bodyText confidence="0.999970964285714">
advantage of parallel corpora by using our bilin-
gual models to generate new training data for the
monolingual models. This can be especially use-
ful when we wish to use our monolingual models
in a domain for which we lack annotated data, but
for which bitexts are plentiful.2
Our retraining procedure is summarized in Fig-
ure 5. Once we have trained our bilingual param-
eters and have a “Bilingual w/ Full” predictor (us-
ing Equation 5), we can use that predictor to an-
notate a large corpus of parallel data (line 1b). We
then retrain the full monolingual models on a con-
catenation of their original training data and the
newly annotated data (line 3). We refer to the new
monolingual models retrained on the output of the
bilingual models as “Bilingual-Retrained,” and we
tested such models for both NER and parsing. For
comparison, we also retrained monolingual mod-
els directly on the output of the original full mono-
lingual models, using the same unannotated bilin-
gual corpora for self-training (line 1a). We refer to
these models as “Self-Retrained”.
We evaluated our retrained monolingual mod-
els on the same test sets as our bilingual mod-
els, but using only monolingual data at test time.
The texts used for retraining overlapped with the
bitexts used for training the bilingual model, but
both sets were disjoint from the test sets.
</bodyText>
<sectionHeader confidence="0.994499" genericHeader="method">
8 NER Experiments
</sectionHeader>
<bodyText confidence="0.9997942">
We demonstrate the utility of multiview learn-
ing for named entity recognition (NER) on En-
glish/German sentence pairs. We built both our
full and weakened monolingual English and Ger-
man models from the CoNLL 2003 shared task
</bodyText>
<footnote confidence="0.98003275">
2Of course, unannotated monolingual data is even more
plentiful, but as we will show, with the same amount of data,
our method is more effective than simple monolingual self-
training.
</footnote>
<bodyText confidence="0.999887418604651">
training data. The bilingual model parameters
were trained on 5,000 parallel sentences extracted
from the Europarl corpus. For the retraining
experiments, we added an additional 5,000 sen-
tences, for 10,000 in all. For testing, we used
the Europarl 2006 development set and the 2007
newswire test set. Neither of these data sets were
annotated with named entities, so we manually an-
notated 200 sentences from each of them.
We used the Stanford NER tagger (Finkel et
al., 2005) with its default configuration as our full
monolingual model for each language. We weak-
ened both the English and German models by re-
moving several non-lexical and word-shape fea-
tures. We made one more crucial change to our
monolingual German model. The German entity
recognizer has extremely low recall (44 %) when
out of domain, so we chose yx from Figure 3 to
be the label in the top five which had the largest
number of named entities.
Table 3 gives results for named entity recogni-
tion. The first two rows are the full and weak-
ened monolingual models alone. The second two
are the multiview trained bilingual models. We
first note that for English, using the full bilin-
gual model yields only slight improvements over
the baseline full monolingual model, and in prac-
tice the predictions were almost identical. For this
problem, the monolingual German model is much
worse than the monolingual English model, and so
the bilingual model doesn’t offer significant im-
provements in English. The bilingual model does
show significant German improvements, however,
including a 16.1% absolute gain in F1 over the
baseline for parliamentary proceedings.
The last two rows of Table 3 give results for
monolingual models which are trained on data that
was automatically labeled using the our models.
English results were again mixed, due to the rel-
atively weak English performance of the bilin-
gual model. For German, though, the “Bilingual-
Retrained” model improves 14.4% F1 over the
“Self-Retrained” baseline.
</bodyText>
<sectionHeader confidence="0.664181" genericHeader="method">
9 Parsing Experiments
</sectionHeader>
<bodyText confidence="0.99992">
Our next set of experiments are on syntactic pars-
ing of English and Chinese. We trained both our
full and weakened monolingual English models
on the Penn Wall Street Journal corpus (Marcus
et al., 1993), as described in Section 4. Our full
and weakened Chinese models were trained on
</bodyText>
<page confidence="0.997649">
51
</page>
<table confidence="0.999654909090909">
Eng Parliament Eng Newswire Ger Parliament Ger Newswire
Prec Rec Fl Prec Rec Fl Prec Rec Fl Prec Rec Fl
Monolingual Models (Baseline)
Weak Monolingual 52.6 65.9 58.5 67.7 83.0 74.6 71.3 36.4 48.2 80.0 51.5 62.7
Full Monolingual 65.7 71.4 68.4 80.1 88.7 84.2 69.8 44.0 54.0 73.0 56.4 63.7
Multiview Trained Bilingual Models
Bilingual w/ Weak 56.2 70.8 62.7 71.4 86.2 78.1 70.1 66.3 68.2 76.5 76.1 76.3
Bilingual w/ Full 65.4 72.4 68.7 80.6 88.7 84.4 70.1 70.1 70.1 74.6 77.3 75.9
Retrained Monolingual Models
Self-Retrained 71.7 74.0 72.9 79.9 87.4 83.5 70.4 44.0 54.2 79.3 58.9 67.6
Bilingual-Retrained 68.6 70.8 69.7 80.7 89.3 84.8 74.5 63.6 68.6 77.9 69.3 73.4
</table>
<tableCaption confidence="0.985456">
Table 3: NER Results. Rows are grouped by data condition. We bold all entries that are best in their
group and beat the strongest monolingual baseline.
</tableCaption>
<table confidence="0.999599666666667">
Chinese English
Monolingual Models (Baseline)
Weak Monolingual 78.3 67.6
Full Monolingual 84.2 75.4
Multiview Trained Bilingual Models
Bilingual w/ Weak 80.4 70.8
Bilingual w/ Full 85.9 77.5
Supervised Trained Bilingual Models
Burkett and Klein (2008) 86.1 78.2
Retrained Monolingual Models
Self-Retrained 83.6 76.7
Bilingual-Retrained 83.9 77.4
</table>
<tableCaption confidence="0.988561">
Table 4: Parsing results. Rows are grouped by data
</tableCaption>
<bodyText confidence="0.9823654">
condition. We bold entries that are best in their
group and beat the the Full Monolingual baseline.
the Penn Chinese treebank (Xue et al., 2002) (ar-
ticles 400-1151), excluding the bilingual portion.
The bilingual data consists of the parallel part of
the Chinese treebank (articles 1-270), which also
includes manually parsed English translations of
each Chinese sentence (Bies et al., 2007). Only
the Chinese sentences and their English transla-
tions were used to train the bilingual models – the
gold trees were ignored. For retraining, we used
the same data, but weighted it to match the sizes
of the original monolingual treebanks. We tested
on the standard Chinese treebank development set,
which also includes English translations.
Table 4 gives results for syntactic parsing. For
comparison, we also show results for the super-
vised bilingual model of Burkett and Klein (2008).
This model uses the same features at prediction
time as the multiview trained “Bilingual w/ Full”
model, but it is trained on hand-annotated parses.
We first examine the first four rows of Table 4. The
“Bilingual w/ Full” model significantly improves
performance in both English and Chinese relative
to the monolingual baseline. Indeed, it performs
</bodyText>
<table confidence="0.999037142857143">
Phrase-Based System
Moses (No Parser) 18.8
Syntactic Systems
Monolingual Parser 18.7
Supervised Bilingual (Treebank Bi-trees) 21.1
Multiview Bilingual (Treebank Bitext) 20.9
Multiview Bilingual (Domain Bitext) 21.2
</table>
<tableCaption confidence="0.999028">
Table 5: Machine translation results.
</tableCaption>
<bodyText confidence="0.999401">
only slightly worse than the supervised model.
The last two rows of Table 4 are the results of
monolingual parsers trained on automatically la-
beled data. In general, gains in English, which
is out of domain relative to the Penn Treebank,
are larger than those in Chinese, which is in do-
main. We also emphasize that, unlike our NER
data, this bitext was fairly small relative to the an-
notated monolingual data. Therefore, while we
still learn good bilingual model parameters which
give a sizable agreement-based boost when doing
bilingual prediction, we don’t expect retraining to
result in a coverage-based boost in monolingual
performance.
</bodyText>
<subsectionHeader confidence="0.99298">
9.1 Machine Translation Experiments
</subsectionHeader>
<bodyText confidence="0.999988785714286">
Although we don’t have hand-labeled data for our
largest Chinese-English parallel corpora, we can
still evaluate our parsing results via our perfor-
mance on a downstream machine translation (MT)
task. Our experimental setup is as follows: first,
we used the first 100,000 sentences of the English-
Chinese bitext from Wang et al. (2007) to train
Moses (Koehn et al., 2007), a phrase-based MT
system that we use as a baseline. We then used the
same sentences to extract tree-to-string transducer
rules from target-side (English) trees (Galley et al.,
2004). We compare the single-reference BLEU
scores of syntactic MT systems that result from
using different parsers to generate these trees.
</bodyText>
<page confidence="0.984983">
52
</page>
<figure confidence="0.99956656">
0.8
0.6
0.4
0.2
0.0
1.4
1.2
1.0
German Weight
(b)
Combined F1
+
Ð
*
81.8-82.1
81.5-81.8
81.2-81.5
80.9-81.2
80.6-80.9
82.1
82.0
81.4
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
0.8
0.6
0.4
0.2
0.0
1.4
1.2
1.0
Chinese Weight
*
70.3
German F1
68-71
65-68
62-65
59-62
56-59
+
70.1
* +
59.1
0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8
(a)
*
+
Ð
English Weight English Weight
</figure>
<figureCaption confidence="0.950866">
Figure 6: (a) NER and (b) parsing results for different values of A1 and A2 (see Equation 6). ‘*’ shows
optimal weights, ‘+’ shows our learned weights, and ‘-’ shows uniform combination weights.
</figureCaption>
<bodyText confidence="0.9999156">
For our syntactic baseline, we used the mono-
lingual English parser. For our remaining experi-
ments, we parsed both English and Chinese simul-
taneously. The supervised model and the first mul-
tiview trained model are the same Chinese tree-
bank trained models for which we reported pars-
ing results. We also used our multiview method to
train an additional bilingual model on part of the
bitext we used to extract translation rules.
The results are shown in Table 5. Once again,
our multiview trained model yields comparable re-
sults to the supervised model. Furthermore, while
the differences are small, our best performance
comes from the model trained on in-domain data,
for which no gold trees exist.
</bodyText>
<sectionHeader confidence="0.629971" genericHeader="method">
10 Analyzing Combined Prediction
</sectionHeader>
<bodyText confidence="0.994587459459459">
In this section, we explore combinations of the full
monolingual models, p1(y1|x1) and p2(y21x2),
and the bilingual model, max
a
parsing, the results in this section are for combined
F1. This simply computes F1 over all of the sen-
tences in both the English and Chinese test sets.
For NER, we just use German F1, since English is
relatively constant across runs.
We begin by examining how poorly our model
performs if we do not consider monolingual in-
formation in the bilingual view. For parsing, the
combined Chinese and English F1 for this model
is 78.7%. When we combine this model uniformly
with the full monolingual model, as in Equation 4,
combined F1 improves to 81.2%, but is still well
below our best combined score of 82.1%. NER
results for a model trained without monolingual
information show an even larger decline.
Now let us consider decision rules of the form:
Note that when A1 = A2 = 1, this is exactly
the uniform decision rule (Equation 4). When
A1 = �1 and A2 = �A2, this is the “Bilingual w/
Full” decision rule (Equation 5). Figure 6 is a
contour plot of F1 with respect to the parameters
A1 and A2. Our decision rule “Bilingual w/ Full”
(Equation 5, marked with a ‘+’) is near the opti-
mum (‘*’), while the uniform decision rule (‘-’)
performs quite poorly. This is true for both NER
(Figure 6a) and parsing (Figure 6b).
There is one more decision rule which we have
yet to consider: the “conditional independence”
decision rule from Equation 3. While this rule can-
not be shown on the plots in Figure 6 (because
it uses both the full and weakened monolingual
models), we note that it also performs poorly in
both cases (80.7% F1 for parsing, for example).
</bodyText>
<sectionHeader confidence="0.987602" genericHeader="conclusions">
11 Conclusions
</sectionHeader>
<bodyText confidence="0.999996076923077">
We show for the first time that state-of-the-art,
discriminative monolingual models can be signifi-
cantly improved using unannotated bilingual text.
We do this by first building bilingual models that
are trained to agree with pairs of independently-
trained monolingual models. Then we combine
the bilingual and monolingual models to account
for dependence across views. By automatically
annotating unlabeled bitexts with these bilingual
models, we can train new monolingual models that
do not rely on bilingual data at test time, but still
perform substantially better than models trained
using only monolingual resources.
</bodyText>
<sectionHeader confidence="0.997641" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997258">
This project is funded in part by NSF grants
0915265 and 0643742, an NSF graduate research
fellowship, the DNI under grant HM1582-09-1-
0021, and BBN under DARPA contract HR0011-
06-C-0022.
</bodyText>
<equation confidence="0.7748615">
T
8 0(y1, a, y2). For
y� = argmax max exp[λ1 log (p1(y1|x1)) +
y a
λ2 lo / T
g(P2(y2|x2))+�9 O(y1, a, y2)] .
</equation>
<page confidence="0.998266">
53
</page>
<sectionHeader confidence="0.993666" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987304494382">
Rie Kubota Ando and Tong Zhang. 2007. Two-view
feature generation model for semi-supervised learn-
ing. In ICML.
Maria-Florina Balcan and Avrim Blum. 2005. A pac-
style model for learning from labeled and unlabeled
data. In COLT.
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English chinese translation treebank
v 1.0. Web download. LDC2007T02.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009.
Bayesian synchronous grammar induction. In NIPS.
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
EMNLP.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In HLT-NAACL.
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-view learning over structured
and non-identical outputs. In UAI.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In ACL.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In ICMI.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Special Issue of the Journal of Natural Language
Engineering on Parallel Texts, 11(3):311–325.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313–330.
Robert Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In EACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In COLING-ACL.
Luo Si and Jamie Callan. 2005. Clef 2005: Multi-
lingual retrieval by combining multiple multilingual
ranked lists. In CLEF.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: using english to
parse korean. In EMNLP.
Benjamin Snyder and Regina Barzilay. 2008. Cross-
lingual propagation for morphological analysis. In
AAAI.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In ACL.
Wen Wang, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with
structured and web-based language models. In IEEE
ASRU Workshop.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In COLING.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In NAACL.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Human Language Technologies.
</reference>
<page confidence="0.999025">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.439270">
<title confidence="0.946158">Learning Better Monolingual Models with Unannotated Bilingual Text</title>
<author confidence="0.461974">of California</author>
<author confidence="0.461974">Berkeley Research</author>
<email confidence="0.998337">slav@google.com</email>
<abstract confidence="0.997844909090909">This work shows how to improve state-of-the-art monolingual natural language processing models using unannotated bilingual text. We build a multiview learning objective that enforces agreement between monolingual and bilingual models. In our method the first, monolingual view consists of supervised predictors learned separately for each language. The second, bilingual view consists of log-linear predictors learned over both languages on bilingual text. Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model, and we show how to combine the two models to account for dependence between views. For the task of named entity using bilingual predictors increases by 16.1% absolute over a supervised monolingual model, and retraining on bilingual predictions 14.6%. For syntactic parsing, our bilingual predictor increases 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>Two-view feature generation model for semi-supervised learning.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="19136" citStr="Ando and Zhang, 2007" startWordPosition="3091" endWordPosition="3094"> (Collins and Singer, 1999; Ganchev et al., 2008). In our setting, this amounts to choosing: pM(y|x) qˆ�,ˆ�1ˆ�2(y|x) . (3) This is the correct decision rule if the views are independent and the labels y are uniformly distributed a priori,1 but we have deliberately introduced between-view dependence in the form of the weakened monolingual models. Equation 3 implicitly double-counts monolingual information. One way to avoid this double-counting is to simply discard the weakened monolingual models when making a joint prediction: yˆ = argmax y I ] (4) exp ˆe�o(y1, a, y2) . 1See, e.g. Ando &amp; Zhang(Ando and Zhang, 2007) for a derivation of the decision rule from Equation 3 under these assumptions. This decision rule uniformly combines the two monolingual models and the bilingual model. Note, however, that we have already learned nonuniform weights for the weakened monolingual models. Our final decision rule uses these weights as weights for the full monolingual models: [ ˆλ1 log (p1(y1|x1)) + exp ]ˆλ2 log (p2(y2|x2))+ˆe�o(y1, a, y2) . (5) As we will show in Section 10, this rule for combining the monolingual and bilingual views performs significantly better than the alternatives, and comes close to the optim</context>
</contexts>
<marker>Ando, Zhang, 2007</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2007. Two-view feature generation model for semi-supervised learning. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria-Florina Balcan</author>
<author>Avrim Blum</author>
</authors>
<title>A pacstyle model for learning from labeled and unlabeled data.</title>
<date>2005</date>
<booktitle>In COLT.</booktitle>
<contexts>
<context position="2319" citStr="Balcan and Blum, 2005" startWordPosition="329" endWordPosition="332">(Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingual parallel text (bitext). We cast this problem in the multiple-view (multiview) learning framework (Blum and Mitchell, 1998; Collins and Singer, 1999; Balcan and Blum, 2005; Ganchev et al., 2008). Our two views are a monolingual view, which uses the supervised monolingual models but not bilingual information, and a bilingual view, which exploits features that measure agreement across languages. The parameters of the bilingual view are trained to reproduce the output of the monolingual view. We show that by introducing weakened monolingual models into the bilingual view, we can optimize the parameters of the bilingual model to improve monolingual models. At prediction time, we automatically account for the between-view dependence introduced by the weakened monoli</context>
</contexts>
<marker>Balcan, Blum, 2005</marker>
<rawString>Maria-Florina Balcan and Avrim Blum. 2005. A pacstyle model for learning from labeled and unlabeled data. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Martha Palmer</author>
<author>Justin Mott</author>
<author>Colin Warner</author>
</authors>
<title>English chinese translation treebank v 1.0. Web download.</title>
<date>2007</date>
<pages>2007--02</pages>
<contexts>
<context position="26497" citStr="Bies et al., 2007" startWordPosition="4301" endWordPosition="4304">al w/ Full 85.9 77.5 Supervised Trained Bilingual Models Burkett and Klein (2008) 86.1 78.2 Retrained Monolingual Models Self-Retrained 83.6 76.7 Bilingual-Retrained 83.9 77.4 Table 4: Parsing results. Rows are grouped by data condition. We bold entries that are best in their group and beat the the Full Monolingual baseline. the Penn Chinese treebank (Xue et al., 2002) (articles 400-1151), excluding the bilingual portion. The bilingual data consists of the parallel part of the Chinese treebank (articles 1-270), which also includes manually parsed English translations of each Chinese sentence (Bies et al., 2007). Only the Chinese sentences and their English translations were used to train the bilingual models – the gold trees were ignored. For retraining, we used the same data, but weighted it to match the sizes of the original monolingual treebanks. We tested on the standard Chinese treebank development set, which also includes English translations. Table 4 gives results for syntactic parsing. For comparison, we also show results for the supervised bilingual model of Burkett and Klein (2008). This model uses the same features at prediction time as the multiview trained “Bilingual w/ Full” model, but</context>
</contexts>
<marker>Bies, Palmer, Mott, Warner, 2007</marker>
<rawString>Ann Bies, Martha Palmer, Justin Mott, and Colin Warner. 2007. English chinese translation treebank v 1.0. Web download. LDC2007T02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT.</booktitle>
<contexts>
<context position="2270" citStr="Blum and Mitchell, 1998" startWordPosition="321" endWordPosition="324">ks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingual parallel text (bitext). We cast this problem in the multiple-view (multiview) learning framework (Blum and Mitchell, 1998; Collins and Singer, 1999; Balcan and Blum, 2005; Ganchev et al., 2008). Our two views are a monolingual view, which uses the supervised monolingual models but not bilingual information, and a bilingual view, which exploits features that measure agreement across languages. The parameters of the bilingual view are trained to reproduce the output of the monolingual view. We show that by introducing weakened monolingual models into the bilingual view, we can optimize the parameters of the bilingual model to improve monolingual models. At prediction time, we automatically account for the between-</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="4807" citStr="Blunsom et al. (2009)" startWordPosition="707" endWordPosition="710">pproach to monolingual self-training and show an improvement of up to 14.4% F1 for entity recognition. Even for parsing, where the bilingual portion of the treebank is much smaller than the monolingual, our technique still can improve over purely monolingual self-training by 0.7% F1. 2 Prior Work on Learning from Bilingual Text Prior work in learning monolingual models from bitexts falls roughly into three categories: Unsupervised induction, cross-lingual projection, and bilingual constraints for supervised monolingual models. Two recent, successful unsupervised induction methods are those of Blunsom et al. (2009) and Snyder et al. (2009). Both of them estimate hierarchical Bayesian models and employ bilingual data to constrain the types of models that can be derived. Projection methods, on the other hand, were among the first applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of </context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009. Bayesian synchronous grammar induction. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3432" citStr="Burkett and Klein (2008)" startWordPosition="502" endWordPosition="505"> At prediction time, we automatically account for the between-view dependence introduced by the weakened monolingual models with a simple but effective view-combination heuristic. We demonstrate the performance of this method on two problems. The first is named entity recognition (NER). For this problem, our method automatically learns (a variation on) earlier hand-designed rule-based bilingual NER predictors (Huang and Vogel, 2002; Moore, 2003), resulting in absolute performance gains of up to 16.1% F1. The second task we consider is statistical parsing. For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. We achieve nearly identical improvements using a purely unlabeled bitext. These results carry over to machine translation, where we can achieve slightly better BLEU improvements than the supervised model of Burkett and Klein (2008) since we are able to train our model directly on the parallel data where we perform rule extraction. Finally, for both of our tasks, we use our bilingual model to generate additional automatically labeled monolingual training data. We compare 46 Proceedings of the Fourteenth Con</context>
<context position="5719" citStr="Burkett and Klein, 2008" startWordPosition="861" endWordPosition="864"> al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedurally, our work is most closely related to that of Burkett and Klein (2008). They used an annotated bitext to learn parse reranking models for English and Chinese, exploiting features that examine pieces of parse trees in both languages. Our method can be thought of as the semi-supervised counterpart to their supervised model. Indeed, we achieve nearly the same results, but without annotated bitexts. Smith and Smith (2004) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word a</context>
<context position="7447" citStr="Burkett and Klein, 2008" startWordPosition="1134" endWordPosition="1137">. Our model consists of two views, which we will refer to as monolingual and bilingual. The monolingual view estimates the joint probability as the product of independent marginal distributions over each language, pm(y|x) = p1(y1|x1)p2(y2|x2). In our applications, these marginal distributions will be computed by stateof-the-art statistical taggers and parsers trained on large monolingual corpora. This work focuses on learning parameters for the bilingual view of the data. We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). In this work, we use the term node to indicate a particular component of a label, such as a single (multi-word) named entity or a node in a parse tree. In Figure 2(a), for example, the nodes labeled NP1 in both the Chinese and English trees are matched. Since we don’t know a priori how the components relate to one another, we treat these matchings as hidden. For each matching a and pair of labels y, we define a feature vector φ(y1, a, y2) which factors on edges in the matching. Our model is a conditional exponential family distribution over matchings and labels: 1 ] p�(y, a|x) = exp θ�φ(y1, </context>
<context position="12609" citStr="Burkett and Klein (2008)" startWordPosition="2016" endWordPosition="2019">sh text and word alignment information, we can hope to improve the German performance, and correctly tag Europ¨aischen Rechnungshofes. The monolingual features are standard features for discriminative, state-of-the-art entity recognizers, and we can produce weakened monolingual models by simply limiting the feature set. The bilingual features, 0(y1, a, y2), are over pairs of aligned nodes, where nodes of the labels y1 and y2 are simply the individual named entities. We use a small bilingual feature set consisting of two types of features. First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent qa1,a2,e(y|x) def= max a (1) θT φ(y1, a, y2) − A(A1, A2, θ; x)] . max a � log � 48 Input: full and weakened monolingual models: Input: full and weakened monolingual models: p1(y1|x1), p2(y2|x2), pw1 (y1|x1), pw2 (y2|x2) p1(y1|x1), p2(y2|x2), pw1 (y1|x1), pw2 (y2|x2) unannotated bilingual data: U bilingual parameters: e, X1, X2 Output: bilingual parameters: e, X1, X2 bilingual input: x = (x1, x2) 1. Label U with full monolingual models: Output: bilingual label: y&amp;quot; Vx E U, yM = argmaxy p1(y1|x1)p2(y2|</context>
<context position="14664" citStr="Burkett and Klein (2008)" startWordPosition="2361" endWordPosition="2364">Chinese. The English monolingual parse mistakenly attaches to to the verb increased. In Chinese, however, this ambiguity does not exist. Instead, the word Xf, which aligns to to, has strong selectional preference for attaching to a noun on the left. In our parsing experiments, we use the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), a split-merge latent variable parser, for our monolingual models. Our full model is the result of training the parser with five split-merge phases. Our weakened model uses only two. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). Table 2 gives some examples, but does not exhaustively enumerate those features. 5 Training Bilingual Models Previous work in multiview learning has focused on the case of agreement regularization (Collins and Singer, 1999; Ganchev et al., 2008). If we had bilingual labeled data, together with our unlabeled data and monolingual labeled data, we could exploit these techniques. Because we do not possess bilingual labeled data, we must train the bilingual model in another way. Here we advocate training the bilingual model (consisting of the bilingual features and weakened monolingual models) to</context>
<context position="15897" citStr="Burkett and Klein, 2008" startWordPosition="2558" endWordPosition="2561">the full monolingual models. In terms of agreement regularization, our procedure may be thought of as “regularizing” the bilingual model to be similar to the full monolingual models. Figure 4: Prediction by combining monolingual and bilingual models. Our training algorithm is summarized in Figure 3. For each unlabeled point x = (x1, x2), let yM be the joint label which has the highest score from the independent monolingual models (line 1). We then find bilingual parameters 9, �1, �2 that maximize qˆ�,ˆλ1,ˆλ2(Px&apos;x) (line 2). This maxlikelihood optimization can be solved by an EMlike procedure (Burkett and Klein, 2008). This procedure iteratively updates the parameter estimates by (a) finding the optimum alignments for each candidate label pair under the current parameters and then (b) updating the parameters to maximize a modified version of Equation 1, restricted to the optimal alignments. Because we restrict alignments to the set of at most one-to-one matchings, the (a) step is tractable using the Hungarian algorithm. With the alignments fixed, the (b) step just involves maximizing likelihood under a log-linear model with no latent variables – this problem is convex and can be solved efficiently using gr</context>
<context position="25960" citStr="Burkett and Klein (2008)" startWordPosition="4220" endWordPosition="4223">7 84.4 70.1 70.1 70.1 74.6 77.3 75.9 Retrained Monolingual Models Self-Retrained 71.7 74.0 72.9 79.9 87.4 83.5 70.4 44.0 54.2 79.3 58.9 67.6 Bilingual-Retrained 68.6 70.8 69.7 80.7 89.3 84.8 74.5 63.6 68.6 77.9 69.3 73.4 Table 3: NER Results. Rows are grouped by data condition. We bold all entries that are best in their group and beat the strongest monolingual baseline. Chinese English Monolingual Models (Baseline) Weak Monolingual 78.3 67.6 Full Monolingual 84.2 75.4 Multiview Trained Bilingual Models Bilingual w/ Weak 80.4 70.8 Bilingual w/ Full 85.9 77.5 Supervised Trained Bilingual Models Burkett and Klein (2008) 86.1 78.2 Retrained Monolingual Models Self-Retrained 83.6 76.7 Bilingual-Retrained 83.9 77.4 Table 4: Parsing results. Rows are grouped by data condition. We bold entries that are best in their group and beat the the Full Monolingual baseline. the Penn Chinese treebank (Xue et al., 2002) (articles 400-1151), excluding the bilingual portion. The bilingual data consists of the parallel part of the Chinese treebank (articles 1-270), which also includes manually parsed English translations of each Chinese sentence (Bies et al., 2007). Only the Chinese sentences and their English translations wer</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9018" citStr="Charniak and Johnson, 2005" startWordPosition="1412" endWordPosition="1415">he distribution on labels y, we must marginalize over hidden alignments between nodes, which we also approximate by using the maximum-scoring matching: 1 ] exp θ�φ(y1, a, y2)− �A(θ; x) . E �A(θ; x) = log y max a qe(y|x) def = max a 47 Feat. types Examples Algn Densty INSIDEBOTH=3 INENONLY=0 Indicators LBLMATCH=true BIAS=true ORG1 the reports of the European Court of Auditors die Berichte des EuropŠischen Rechnungshofes ORG1 Figure 1: An example where English NER can be used to disambiguate German NER. We further simplify inference in our model by working in a reranking setting (Collins, 2000; Charniak and Johnson, 2005), where we only consider the top k outputs from monolingual models in both languages, for a total of k2 labels y. In practice, k2 &lt; 10, 000 for our largest problem. 3.1 Including Weakened Models Now that we have defined our bilingual model, we could train it to agree with the output of the monolingual model (Collins and Singer, 1999; Ganchev et al., 2008). As we will see in Section 4, however, the feature functions 0(y1, a, y2) make no reference to the input sentences x, other than through a fixed word alignment. With such limited monolingual information, it is impossible for the bilingual mod</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2296" citStr="Collins and Singer, 1999" startWordPosition="325" endWordPosition="328"> named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingual parallel text (bitext). We cast this problem in the multiple-view (multiview) learning framework (Blum and Mitchell, 1998; Collins and Singer, 1999; Balcan and Blum, 2005; Ganchev et al., 2008). Our two views are a monolingual view, which uses the supervised monolingual models but not bilingual information, and a bilingual view, which exploits features that measure agreement across languages. The parameters of the bilingual view are trained to reproduce the output of the monolingual view. We show that by introducing weakened monolingual models into the bilingual view, we can optimize the parameters of the bilingual model to improve monolingual models. At prediction time, we automatically account for the between-view dependence introduced</context>
<context position="9352" citStr="Collins and Singer, 1999" startWordPosition="1474" endWordPosition="1477">eports of the European Court of Auditors die Berichte des EuropŠischen Rechnungshofes ORG1 Figure 1: An example where English NER can be used to disambiguate German NER. We further simplify inference in our model by working in a reranking setting (Collins, 2000; Charniak and Johnson, 2005), where we only consider the top k outputs from monolingual models in both languages, for a total of k2 labels y. In practice, k2 &lt; 10, 000 for our largest problem. 3.1 Including Weakened Models Now that we have defined our bilingual model, we could train it to agree with the output of the monolingual model (Collins and Singer, 1999; Ganchev et al., 2008). As we will see in Section 4, however, the feature functions 0(y1, a, y2) make no reference to the input sentences x, other than through a fixed word alignment. With such limited monolingual information, it is impossible for the bilingual model to adequately capture all of the information necessary for NER or parsing. As a simple example, a bilingual NER model will be perfectly happy to label two aligned person names as ORG instead of PER: both labelings agree equally well. We briefly illustrate how poorly such a basic bilingual model performs in Section 10. One way to </context>
<context position="14888" citStr="Collins and Singer, 1999" startWordPosition="2395" endWordPosition="2398">ng to a noun on the left. In our parsing experiments, we use the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), a split-merge latent variable parser, for our monolingual models. Our full model is the result of training the parser with five split-merge phases. Our weakened model uses only two. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). Table 2 gives some examples, but does not exhaustively enumerate those features. 5 Training Bilingual Models Previous work in multiview learning has focused on the case of agreement regularization (Collins and Singer, 1999; Ganchev et al., 2008). If we had bilingual labeled data, together with our unlabeled data and monolingual labeled data, we could exploit these techniques. Because we do not possess bilingual labeled data, we must train the bilingual model in another way. Here we advocate training the bilingual model (consisting of the bilingual features and weakened monolingual models) to imitate the full monolingual models. In terms of agreement regularization, our procedure may be thought of as “regularizing” the bilingual model to be similar to the full monolingual models. Figure 4: Prediction by combinin</context>
<context position="18541" citStr="Collins and Singer, 1999" startWordPosition="2992" endWordPosition="2995">activeness of Tianjin to Taiwanese merchants NP VB i�s Atp-f R VA-n M IR41ji NP PP PP NP PP PP VB NNP DE NN VB NNP DE NN NP PP NP PP NP1 NP1 VP VP i�s Atp-f R VA-n M IR41ji These measures increased the attractiveness of Tianjin to Taiwanese merchants particular, the monolingual view uses monolingual models that are known to be superior to the monolingual information available in the bilingual view. Thus, we would like to find some way to incorporate the full monolingual models into our prediction method. One obvious choice is to choose the labeling that maximizes the “agreement distribution” (Collins and Singer, 1999; Ganchev et al., 2008). In our setting, this amounts to choosing: pM(y|x) qˆ�,ˆ�1ˆ�2(y|x) . (3) This is the correct decision rule if the views are independent and the labels y are uniformly distributed a priori,1 but we have deliberately introduced between-view dependence in the form of the weakened monolingual models. Equation 3 implicitly double-counts monolingual information. One way to avoid this double-counting is to simply discard the weakened monolingual models when making a joint prediction: yˆ = argmax y I ] (4) exp ˆe�o(y1, a, y2) . 1See, e.g. Ando &amp; Zhang(Ando and Zhang, 2007) for </context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="8989" citStr="Collins, 2000" startWordPosition="1410" endWordPosition="1411">er to compute the distribution on labels y, we must marginalize over hidden alignments between nodes, which we also approximate by using the maximum-scoring matching: 1 ] exp θ�φ(y1, a, y2)− �A(θ; x) . E �A(θ; x) = log y max a qe(y|x) def = max a 47 Feat. types Examples Algn Densty INSIDEBOTH=3 INENONLY=0 Indicators LBLMATCH=true BIAS=true ORG1 the reports of the European Court of Auditors die Berichte des EuropŠischen Rechnungshofes ORG1 Figure 1: An example where English NER can be used to disambiguate German NER. We further simplify inference in our model by working in a reranking setting (Collins, 2000; Charniak and Johnson, 2005), where we only consider the top k outputs from monolingual models in both languages, for a total of k2 labels y. In practice, k2 &lt; 10, 000 for our largest problem. 3.1 Including Weakened Models Now that we have defined our bilingual model, we could train it to agree with the output of the monolingual model (Collins and Singer, 1999; Ganchev et al., 2008). As we will see in Section 4, however, the feature functions 0(y1, a, y2) make no reference to the input sentences x, other than through a fixed word alignment. With such limited monolingual information, it is imp</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23083" citStr="Finkel et al., 2005" startWordPosition="3746" endWordPosition="3749">ta is even more plentiful, but as we will show, with the same amount of data, our method is more effective than simple monolingual selftraining. training data. The bilingual model parameters were trained on 5,000 parallel sentences extracted from the Europarl corpus. For the retraining experiments, we added an additional 5,000 sentences, for 10,000 in all. For testing, we used the Europarl 2006 development set and the 2007 newswire test set. Neither of these data sets were annotated with named entities, so we manually annotated 200 sentences from each of them. We used the Stanford NER tagger (Finkel et al., 2005) with its default configuration as our full monolingual model for each language. We weakened both the English and German models by removing several non-lexical and word-shape features. We made one more crucial change to our monolingual German model. The German entity recognizer has extremely low recall (44 %) when out of domain, so we chose yx from Figure 3 to be the label in the top five which had the largest number of named entities. Table 3 gives results for named entity recognition. The first two rows are the full and weakened monolingual models alone. The second two are the multiview trai</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="28820" citStr="Galley et al., 2004" startWordPosition="4663" endWordPosition="4666">ost in monolingual performance. 9.1 Machine Translation Experiments Although we don’t have hand-labeled data for our largest Chinese-English parallel corpora, we can still evaluate our parsing results via our performance on a downstream machine translation (MT) task. Our experimental setup is as follows: first, we used the first 100,000 sentences of the EnglishChinese bitext from Wang et al. (2007) to train Moses (Koehn et al., 2007), a phrase-based MT system that we use as a baseline. We then used the same sentences to extract tree-to-string transducer rules from target-side (English) trees (Galley et al., 2004). We compare the single-reference BLEU scores of syntactic MT systems that result from using different parsers to generate these trees. 52 0.8 0.6 0.4 0.2 0.0 1.4 1.2 1.0 German Weight (b) Combined F1 + Ð * 81.8-82.1 81.5-81.8 81.2-81.5 80.9-81.2 80.6-80.9 82.1 82.0 81.4 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.8 0.6 0.4 0.2 0.0 1.4 1.2 1.0 Chinese Weight * 70.3 German F1 68-71 65-68 62-65 59-62 56-59 + 70.1 * + 59.1 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 (a) * + Ð English Weight English Weight Figure 6: (a) NER and (b) parsing results for different values of A1 and A2 (see Equation 6). ‘*’ shows optimal we</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Joao Graca</author>
<author>John Blitzer</author>
<author>Ben Taskar</author>
</authors>
<title>Multi-view learning over structured and non-identical outputs.</title>
<date>2008</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="2342" citStr="Ganchev et al., 2008" startWordPosition="333" endWordPosition="336"> Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingual parallel text (bitext). We cast this problem in the multiple-view (multiview) learning framework (Blum and Mitchell, 1998; Collins and Singer, 1999; Balcan and Blum, 2005; Ganchev et al., 2008). Our two views are a monolingual view, which uses the supervised monolingual models but not bilingual information, and a bilingual view, which exploits features that measure agreement across languages. The parameters of the bilingual view are trained to reproduce the output of the monolingual view. We show that by introducing weakened monolingual models into the bilingual view, we can optimize the parameters of the bilingual model to improve monolingual models. At prediction time, we automatically account for the between-view dependence introduced by the weakened monolingual models with a sim</context>
<context position="9375" citStr="Ganchev et al., 2008" startWordPosition="1478" endWordPosition="1481">rt of Auditors die Berichte des EuropŠischen Rechnungshofes ORG1 Figure 1: An example where English NER can be used to disambiguate German NER. We further simplify inference in our model by working in a reranking setting (Collins, 2000; Charniak and Johnson, 2005), where we only consider the top k outputs from monolingual models in both languages, for a total of k2 labels y. In practice, k2 &lt; 10, 000 for our largest problem. 3.1 Including Weakened Models Now that we have defined our bilingual model, we could train it to agree with the output of the monolingual model (Collins and Singer, 1999; Ganchev et al., 2008). As we will see in Section 4, however, the feature functions 0(y1, a, y2) make no reference to the input sentences x, other than through a fixed word alignment. With such limited monolingual information, it is impossible for the bilingual model to adequately capture all of the information necessary for NER or parsing. As a simple example, a bilingual NER model will be perfectly happy to label two aligned person names as ORG instead of PER: both labelings agree equally well. We briefly illustrate how poorly such a basic bilingual model performs in Section 10. One way to solve this problem is t</context>
<context position="14911" citStr="Ganchev et al., 2008" startWordPosition="2399" endWordPosition="2402">In our parsing experiments, we use the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), a split-merge latent variable parser, for our monolingual models. Our full model is the result of training the parser with five split-merge phases. Our weakened model uses only two. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). Table 2 gives some examples, but does not exhaustively enumerate those features. 5 Training Bilingual Models Previous work in multiview learning has focused on the case of agreement regularization (Collins and Singer, 1999; Ganchev et al., 2008). If we had bilingual labeled data, together with our unlabeled data and monolingual labeled data, we could exploit these techniques. Because we do not possess bilingual labeled data, we must train the bilingual model in another way. Here we advocate training the bilingual model (consisting of the bilingual features and weakened monolingual models) to imitate the full monolingual models. In terms of agreement regularization, our procedure may be thought of as “regularizing” the bilingual model to be similar to the full monolingual models. Figure 4: Prediction by combining monolingual and bilin</context>
<context position="18564" citStr="Ganchev et al., 2008" startWordPosition="2996" endWordPosition="2999">aiwanese merchants NP VB i�s Atp-f R VA-n M IR41ji NP PP PP NP PP PP VB NNP DE NN VB NNP DE NN NP PP NP PP NP1 NP1 VP VP i�s Atp-f R VA-n M IR41ji These measures increased the attractiveness of Tianjin to Taiwanese merchants particular, the monolingual view uses monolingual models that are known to be superior to the monolingual information available in the bilingual view. Thus, we would like to find some way to incorporate the full monolingual models into our prediction method. One obvious choice is to choose the labeling that maximizes the “agreement distribution” (Collins and Singer, 1999; Ganchev et al., 2008). In our setting, this amounts to choosing: pM(y|x) qˆ�,ˆ�1ˆ�2(y|x) . (3) This is the correct decision rule if the views are independent and the labels y are uniformly distributed a priori,1 but we have deliberately introduced between-view dependence in the form of the weakened monolingual models. Equation 3 implicitly double-counts monolingual information. One way to avoid this double-counting is to simply discard the weakened monolingual models when making a joint prediction: yˆ = argmax y I ] (4) exp ˆe�o(y1, a, y2) . 1See, e.g. Ando &amp; Zhang(Ando and Zhang, 2007) for a derivation of the dec</context>
</contexts>
<marker>Ganchev, Graca, Blitzer, Taskar, 2008</marker>
<rawString>Kuzman Ganchev, Joao Graca, John Blitzer, and Ben Taskar. 2008. Multi-view learning over structured and non-identical outputs. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1547" citStr="Ganchev et al., 2009" startWordPosition="216" endWordPosition="219">gnition, using bilingual predictors increases F, by 16.1% absolute over a supervised monolingual model, and retraining on bilingual predictions increases monolingual model F, by 14.6%. For syntactic parsing, our bilingual predictor increases F, by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%. 1 Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingua</context>
<context position="5171" citStr="Ganchev et al., 2009" startWordPosition="767" endWordPosition="770"> models from bitexts falls roughly into three categories: Unsupervised induction, cross-lingual projection, and bilingual constraints for supervised monolingual models. Two recent, successful unsupervised induction methods are those of Blunsom et al. (2009) and Snyder et al. (2009). Both of them estimate hierarchical Bayesian models and employ bilingual data to constrain the types of models that can be derived. Projection methods, on the other hand, were among the first applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedurally, our work is most closely related to </context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Stephan Vogel</author>
</authors>
<title>Improved named entity translation and bilingual named entity extraction.</title>
<date>2002</date>
<booktitle>In ICMI.</booktitle>
<contexts>
<context position="1720" citStr="Huang and Vogel, 2002" startWordPosition="242" endWordPosition="245">, by 14.6%. For syntactic parsing, our bilingual predictor increases F, by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%. 1 Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingual parallel text (bitext). We cast this problem in the multiple-view (multiview) learning framework (Blum and Mitchell, 1998; Collins and Singer, 1999; Balcan and Blum, 2005;</context>
<context position="3243" citStr="Huang and Vogel, 2002" startWordPosition="469" endWordPosition="472">he monolingual view. We show that by introducing weakened monolingual models into the bilingual view, we can optimize the parameters of the bilingual model to improve monolingual models. At prediction time, we automatically account for the between-view dependence introduced by the weakened monolingual models with a simple but effective view-combination heuristic. We demonstrate the performance of this method on two problems. The first is named entity recognition (NER). For this problem, our method automatically learns (a variation on) earlier hand-designed rule-based bilingual NER predictors (Huang and Vogel, 2002; Moore, 2003), resulting in absolute performance gains of up to 16.1% F1. The second task we consider is statistical parsing. For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. We achieve nearly identical improvements using a purely unlabeled bitext. These results carry over to machine translation, where we can achieve slightly better BLEU improvements than the supervised model of Burkett and Klein (2008) since we are able to train our model directly on the parallel data where we perform rule e</context>
<context position="5643" citStr="Huang and Vogel, 2002" startWordPosition="848" endWordPosition="852">st applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedurally, our work is most closely related to that of Burkett and Klein (2008). They used an annotated bitext to learn parse reranking models for English and Chinese, exploiting features that examine pieces of parse trees in both languages. Our method can be thought of as the semi-supervised counterpart to their supervised model. Indeed, we achieve nearly the same results, but without annotated bitexts. Smith and Smith (2004) consider a similar setting for parsing both English and Korean, but instead of learning </context>
</contexts>
<marker>Huang, Vogel, 2002</marker>
<rawString>Fei Huang and Stephan Vogel. 2002. Improved named entity translation and bilingual named entity extraction. In ICMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Special Issue of the Journal of Natural Language Engineering on Parallel Texts,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="1524" citStr="Hwa et al., 2005" startWordPosition="212" endWordPosition="215"> named entity recognition, using bilingual predictors increases F, by 16.1% absolute over a supervised monolingual model, and retraining on bilingual predictions increases monolingual model F, by 14.6%. For syntactic parsing, our bilingual predictor increases F, by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%. 1 Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models usi</context>
<context position="5148" citStr="Hwa et al., 2005" startWordPosition="763" endWordPosition="766">arning monolingual models from bitexts falls roughly into three categories: Unsupervised induction, cross-lingual projection, and bilingual constraints for supervised monolingual models. Two recent, successful unsupervised induction methods are those of Blunsom et al. (2009) and Snyder et al. (2009). Both of them estimate hierarchical Bayesian models and employ bilingual data to constrain the types of models that can be derived. Projection methods, on the other hand, were among the first applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedurally, our work is m</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Special Issue of the Journal of Natural Language Engineering on Parallel Texts, 11(3):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="28637" citStr="Koehn et al., 2007" startWordPosition="4634" endWordPosition="4637">we still learn good bilingual model parameters which give a sizable agreement-based boost when doing bilingual prediction, we don’t expect retraining to result in a coverage-based boost in monolingual performance. 9.1 Machine Translation Experiments Although we don’t have hand-labeled data for our largest Chinese-English parallel corpora, we can still evaluate our parsing results via our performance on a downstream machine translation (MT) task. Our experimental setup is as follows: first, we used the first 100,000 sentences of the EnglishChinese bitext from Wang et al. (2007) to train Moses (Koehn et al., 2007), a phrase-based MT system that we use as a baseline. We then used the same sentences to extract tree-to-string transducer rules from target-side (English) trees (Galley et al., 2004). We compare the single-reference BLEU scores of syntactic MT systems that result from using different parsers to generate these trees. 52 0.8 0.6 0.4 0.2 0.0 1.4 1.2 1.0 German Weight (b) Combined F1 + Ð * 81.8-82.1 81.5-81.8 81.2-81.5 80.9-81.2 80.6-80.9 82.1 82.0 81.4 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.8 0.6 0.4 0.2 0.0 1.4 1.2 1.0 Chinese Weight * 70.3 German F1 68-71 65-68 62-65 59-62 56-59 + 70.1 * + 59.1 0.0</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="24809" citStr="Marcus et al., 1993" startWordPosition="4030" endWordPosition="4033">e baseline for parliamentary proceedings. The last two rows of Table 3 give results for monolingual models which are trained on data that was automatically labeled using the our models. English results were again mixed, due to the relatively weak English performance of the bilingual model. For German, though, the “BilingualRetrained” model improves 14.4% F1 over the “Self-Retrained” baseline. 9 Parsing Experiments Our next set of experiments are on syntactic parsing of English and Chinese. We trained both our full and weakened monolingual English models on the Penn Wall Street Journal corpus (Marcus et al., 1993), as described in Section 4. Our full and weakened Chinese models were trained on 51 Eng Parliament Eng Newswire Ger Parliament Ger Newswire Prec Rec Fl Prec Rec Fl Prec Rec Fl Prec Rec Fl Monolingual Models (Baseline) Weak Monolingual 52.6 65.9 58.5 67.7 83.0 74.6 71.3 36.4 48.2 80.0 51.5 62.7 Full Monolingual 65.7 71.4 68.4 80.1 88.7 84.2 69.8 44.0 54.0 73.0 56.4 63.7 Multiview Trained Bilingual Models Bilingual w/ Weak 56.2 70.8 62.7 71.4 86.2 78.1 70.1 66.3 68.2 76.5 76.1 76.3 Bilingual w/ Full 65.4 72.4 68.7 80.6 88.7 84.4 70.1 70.1 70.1 74.6 77.3 75.9 Retrained Monolingual Models Self-Re</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
</authors>
<title>Learning translations of namedentity phrases from parallel corpora.</title>
<date>2003</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="1734" citStr="Moore, 2003" startWordPosition="246" endWordPosition="247">ic parsing, our bilingual predictor increases F, by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%. 1 Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingual parallel text (bitext). We cast this problem in the multiple-view (multiview) learning framework (Blum and Mitchell, 1998; Collins and Singer, 1999; Balcan and Blum, 2005; Ganchev et al</context>
<context position="3257" citStr="Moore, 2003" startWordPosition="473" endWordPosition="474"> show that by introducing weakened monolingual models into the bilingual view, we can optimize the parameters of the bilingual model to improve monolingual models. At prediction time, we automatically account for the between-view dependence introduced by the weakened monolingual models with a simple but effective view-combination heuristic. We demonstrate the performance of this method on two problems. The first is named entity recognition (NER). For this problem, our method automatically learns (a variation on) earlier hand-designed rule-based bilingual NER predictors (Huang and Vogel, 2002; Moore, 2003), resulting in absolute performance gains of up to 16.1% F1. The second task we consider is statistical parsing. For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. We achieve nearly identical improvements using a purely unlabeled bitext. These results carry over to machine translation, where we can achieve slightly better BLEU improvements than the supervised model of Burkett and Klein (2008) since we are able to train our model directly on the parallel data where we perform rule extraction. Fin</context>
</contexts>
<marker>Moore, 2003</marker>
<rawString>Robert Moore. 2003. Learning translations of namedentity phrases from parallel corpora. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="14390" citStr="Petrov and Klein, 2007" startWordPosition="2314" endWordPosition="2317">igure 1 contains sample values for each of these features. Another natural setting where bilingual constraints can be exploited is syntactic parsing. Figure 2 shows an example English prepositional phrase attachment ambiguity that can be resolved bilingually by exploiting Chinese. The English monolingual parse mistakenly attaches to to the verb increased. In Chinese, however, this ambiguity does not exist. Instead, the word Xf, which aligns to to, has strong selectional preference for attaching to a noun on the left. In our parsing experiments, we use the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), a split-merge latent variable parser, for our monolingual models. Our full model is the result of training the parser with five split-merge phases. Our weakened model uses only two. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). Table 2 gives some examples, but does not exhaustively enumerate those features. 5 Training Bilingual Models Previous work in multiview learning has focused on the case of agreement regularization (Collins and Singer, 1999; Ganchev et al., 2008). If we had bilingual labeled data, together with our unlabeled data and monoli</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="14365" citStr="Petrov et al., 2006" startWordPosition="2310" endWordPosition="2313">has the same label. Figure 1 contains sample values for each of these features. Another natural setting where bilingual constraints can be exploited is syntactic parsing. Figure 2 shows an example English prepositional phrase attachment ambiguity that can be resolved bilingually by exploiting Chinese. The English monolingual parse mistakenly attaches to to the verb increased. In Chinese, however, this ambiguity does not exist. Instead, the word Xf, which aligns to to, has strong selectional preference for attaching to a noun on the left. In our parsing experiments, we use the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), a split-merge latent variable parser, for our monolingual models. Our full model is the result of training the parser with five split-merge phases. Our weakened model uses only two. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). Table 2 gives some examples, but does not exhaustively enumerate those features. 5 Training Bilingual Models Previous work in multiview learning has focused on the case of agreement regularization (Collins and Singer, 1999; Ganchev et al., 2008). If we had bilingual labeled data, together with our </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luo Si</author>
<author>Jamie Callan</author>
</authors>
<title>Clef 2005: Multilingual retrieval by combining multiple multilingual ranked lists.</title>
<date>2005</date>
<booktitle>In CLEF.</booktitle>
<contexts>
<context position="1782" citStr="Si and Callan, 2005" startWordPosition="251" endWordPosition="254">eases F, by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%. 1 Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingual parallel text (bitext). We cast this problem in the multiple-view (multiview) learning framework (Blum and Mitchell, 1998; Collins and Singer, 1999; Balcan and Blum, 2005; Ganchev et al., 2008). Our two views are a monolingual view, </context>
</contexts>
<marker>Si, Callan, 2005</marker>
<rawString>Luo Si and Jamie Callan. 2005. Clef 2005: Multilingual retrieval by combining multiple multilingual ranked lists. In CLEF.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: using english to parse korean.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5666" citStr="Smith and Smith, 2004" startWordPosition="853" endWordPosition="856">llel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedurally, our work is most closely related to that of Burkett and Klein (2008). They used an annotated bitext to learn parse reranking models for English and Chinese, exploiting features that examine pieces of parse trees in both languages. Our method can be thought of as the semi-supervised counterpart to their supervised model. Indeed, we achieve nearly the same results, but without annotated bitexts. Smith and Smith (2004) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they con</context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: using english to parse korean. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Crosslingual propagation for morphological analysis.</title>
<date>2008</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="5693" citStr="Snyder and Barzilay, 2008" startWordPosition="857" endWordPosition="860">e translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedurally, our work is most closely related to that of Burkett and Klein (2008). They used an annotated bitext to learn parse reranking models for English and Chinese, exploiting features that examine pieces of parse trees in both languages. Our method can be thought of as the semi-supervised counterpart to their supervised model. Indeed, we achieve nearly the same results, but without annotated bitexts. Smith and Smith (2004) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination o</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Crosslingual propagation for morphological analysis. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1625" citStr="Snyder et al., 2009" startWordPosition="227" endWordPosition="230">vised monolingual model, and retraining on bilingual predictions increases monolingual model F, by 14.6%. For syntactic parsing, our bilingual predictor increases F, by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%. 1 Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingual parallel text (bitext). We cast this problem in the multiple-view (multiview</context>
<context position="4832" citStr="Snyder et al. (2009)" startWordPosition="712" endWordPosition="715">f-training and show an improvement of up to 14.4% F1 for entity recognition. Even for parsing, where the bilingual portion of the treebank is much smaller than the monolingual, our technique still can improve over purely monolingual self-training by 0.7% F1. 2 Prior Work on Learning from Bilingual Text Prior work in learning monolingual models from bitexts falls roughly into three categories: Unsupervised induction, cross-lingual projection, and bilingual constraints for supervised monolingual models. Two recent, successful unsupervised induction methods are those of Blunsom et al. (2009) and Snyder et al. (2009). Both of them estimate hierarchical Bayesian models and employ bilingual data to constrain the types of models that can be derived. Projection methods, on the other hand, were among the first applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for </context>
</contexts>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised multilingual grammar induction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
</authors>
<title>Reranking machine translation hypotheses with structured and web-based language models.</title>
<date>2007</date>
<booktitle>In IEEE ASRU Workshop.</booktitle>
<contexts>
<context position="28601" citStr="Wang et al. (2007)" startWordPosition="4627" endWordPosition="4630">monolingual data. Therefore, while we still learn good bilingual model parameters which give a sizable agreement-based boost when doing bilingual prediction, we don’t expect retraining to result in a coverage-based boost in monolingual performance. 9.1 Machine Translation Experiments Although we don’t have hand-labeled data for our largest Chinese-English parallel corpora, we can still evaluate our parsing results via our performance on a downstream machine translation (MT) task. Our experimental setup is as follows: first, we used the first 100,000 sentences of the EnglishChinese bitext from Wang et al. (2007) to train Moses (Koehn et al., 2007), a phrase-based MT system that we use as a baseline. We then used the same sentences to extract tree-to-string transducer rules from target-side (English) trees (Galley et al., 2004). We compare the single-reference BLEU scores of syntactic MT systems that result from using different parsers to generate these trees. 52 0.8 0.6 0.4 0.2 0.0 1.4 1.2 1.0 German Weight (b) Combined F1 + Ð * 81.8-82.1 81.5-81.8 81.2-81.5 80.9-81.2 80.6-80.9 82.1 82.0 81.4 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.8 0.6 0.4 0.2 0.0 1.4 1.2 1.0 Chinese Weight * 70.3 German F1 68-71 65-68 6</context>
</contexts>
<marker>Wang, Stolcke, Zheng, 2007</marker>
<rawString>Wen Wang, Andreas Stolcke, and Jing Zheng. 2007. Reranking machine translation hypotheses with structured and web-based language models. In IEEE ASRU Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>Building a large-scale annotated chinese corpus.</title>
<date>2002</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="26250" citStr="Xue et al., 2002" startWordPosition="4265" endWordPosition="4268">s that are best in their group and beat the strongest monolingual baseline. Chinese English Monolingual Models (Baseline) Weak Monolingual 78.3 67.6 Full Monolingual 84.2 75.4 Multiview Trained Bilingual Models Bilingual w/ Weak 80.4 70.8 Bilingual w/ Full 85.9 77.5 Supervised Trained Bilingual Models Burkett and Klein (2008) 86.1 78.2 Retrained Monolingual Models Self-Retrained 83.6 76.7 Bilingual-Retrained 83.9 77.4 Table 4: Parsing results. Rows are grouped by data condition. We bold entries that are best in their group and beat the the Full Monolingual baseline. the Penn Chinese treebank (Xue et al., 2002) (articles 400-1151), excluding the bilingual portion. The bilingual data consists of the parallel part of the Chinese treebank (articles 1-270), which also includes manually parsed English translations of each Chinese sentence (Bies et al., 2007). Only the Chinese sentences and their English translations were used to train the bilingual models – the gold trees were ignored. For retraining, we used the same data, but weighted it to match the sizes of the original monolingual treebanks. We tested on the standard Chinese treebank development set, which also includes English translations. Table 4</context>
</contexts>
<marker>Xue, Chiou, Palmer, 2002</marker>
<rawString>Nianwen Xue, Fu-Dong Chiou, and Martha Palmer. 2002. Building a large-scale annotated chinese corpus. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In NAACL.</title>
<date>2001</date>
<contexts>
<context position="5130" citStr="Yarowsky and Ngai, 2001" startWordPosition="759" endWordPosition="762">ual Text Prior work in learning monolingual models from bitexts falls roughly into three categories: Unsupervised induction, cross-lingual projection, and bilingual constraints for supervised monolingual models. Two recent, successful unsupervised induction methods are those of Blunsom et al. (2009) and Snyder et al. (2009). Both of them estimate hierarchical Bayesian models and employ bilingual data to constrain the types of models that can be derived. Projection methods, on the other hand, were among the first applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedura</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora. In Human Language Technologies.</title>
<date>2001</date>
<contexts>
<context position="1506" citStr="Yarowsky et al., 2001" startWordPosition="208" endWordPosition="211"> views. For the task of named entity recognition, using bilingual predictors increases F, by 16.1% absolute over a supervised monolingual model, and retraining on bilingual predictions increases monolingual model F, by 14.6%. For syntactic parsing, our bilingual predictor increases F, by 2.1% absolute, and retraining a monolingual model on its output gives an improvement of 2.0%. 1 Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improv</context>
<context position="5105" citStr="Yarowsky et al., 2001" startWordPosition="755" endWordPosition="758">on Learning from Bilingual Text Prior work in learning monolingual models from bitexts falls roughly into three categories: Unsupervised induction, cross-lingual projection, and bilingual constraints for supervised monolingual models. Two recent, successful unsupervised induction methods are those of Blunsom et al. (2009) and Snyder et al. (2009). Both of them estimate hierarchical Bayesian models and employ bilingual data to constrain the types of models that can be derived. Projection methods, on the other hand, were among the first applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett an</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Human Language Technologies.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>