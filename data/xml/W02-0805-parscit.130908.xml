<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.491694166666667">
Polysemy and Sense Proximity in the Senseval-2 Test Suite.
Irina Chugur Julio Gonzalo Felisa Verdejo
irina@lsi.uned.es julio@lsi.uned.es felisa@lsi.uned.es
Departamento de Lenguajes y Sistemas Inform´aticos
Universidad Nacional de Educaci´on a Distancia (UNED)
Madrid, Spain
</title>
<sectionHeader confidence="0.974588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986157894737">
We report on an empirical study of sense
relations in the Senseval-2 test suite. We
apply and extend the method described
in (Resnik and Yarowsky, 1999), estimat-
ing proximity of sense pairs from the evi-
dence collected from native-speaker trans-
lations of 508 contexts across 4 Indoeu-
ropean languages representing 3 language
families. A control set composed of 65
contexts has also been annotated in 12
languages (including 2 non-Indoeuropean
languages) in order to estimate the corre-
lation between parallel polysemy and lan-
guage family distance. A new parame-
ter, sense stability, is introduced to assess
the homogeneity of each individual sense
definition. Finally, we combine the sense
proximity estimation with a classification
of semantic relations between senses.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999686590909091">
Our goal is to characterize sense inventories, both
qualitatively and quantitatively, so that the following
questions can be answered:
Given a pair of senses of the same word, are
they related? If so, in what way and how
closely?
How well are individual senses defined? For
each sense, how homogeneous are its examples
of use? How coarse is its definition? Should it
be split into subsenses?
How do these issues affect the evaluation of
automatic Word Sense Disambiguation (WSD)
systems using the sense inventory? What
penalty should be assigned to a WSD system
that confuses two senses, i.e. how much should
it be penalized according to how close these
senses are? Can the sense inventory be im-
proved for evaluation purposes, for instance,
splitting senses into finer-grained distinctions
or collapsing close senses into coarser clusters?
In particular, we are interested in characterizing
WordNet 1.7 as sense inventory for the Senseval-2
WSD comparative evaluation. Unlike conventional
dictionaries, WordNet does not group senses of the
same word in a hierarchical structure; every sense
belongs to a synset, and can only be related to other
senses via conceptual relations (rather than sense re-
lations). Conceptual relations can be used to de-
fine measures of semantic distance (Sussna, 1993;
Agirre and Rigau, 1996; Resnik, 1995), but topic
relatedness is not well captured by wordnet rela-
tions, and this is a fundamental parameter to es-
timate sense similarity in many NLP applications
(Gonzalo et al., 2000).
The issue of estimating semantic distance be-
tween senses of a polysemous word has been pre-
viously addressed in (Resnik and Yarowsky, 1997;
Resnik and Yarowsky, 1999). They propose a mea-
sure of semantic distance based on the likelihood of
the sense distinction being lexicalized in some target
language. The measure was tested using statistics
collected from native-speaker translations of 222
polysemous contexts across 12 languages. The re-
sults obtained showed that monolingual sense dis-
</bodyText>
<note confidence="0.778267">
Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
Disambiguation: Recent Successes and Future Directions, Philadelphia,
July 2002, pp. 32-39. Association for Computational Linguistics.
</note>
<bodyText confidence="0.993926571428572">
tinctions at most levels of granularity can be effec-
tively captured by translations into some set of sec-
ond languages, especially as language family dis-
tance increases. The distance matrices obtained
reproduced faithfully the hierarchical arrangement
of senses provided by the Hector database used in
Senseval-1 (Kilgarriff and Palmer, 2000).
In order to characterize the Senseval-2 Wordnet
1.7 subset, we have adopted such methodology, ex-
tending it to capture also individual sense homo-
geneity, and comparing both quantitative measures
with a coarse, qualitative classification of semantic
relations between senses of a word.
In Section 2 we introduce the quantitative mea-
sures of sense relatedness (as defined by Resnik &amp;
Yarowsky) and sense stability (as an extension to it).
In Section 3, we describe the qualitative classifica-
tion that will be confronted to such measures. In
Sections 4 and 5, we describe the experiment design
and discuss the results obtained. Finally, we draw
some conclusions.
2 Estimating sense relatedness and sense
stability
In order to characterize a sense repository and eval-
uate the quality and nature of its sense distinc-
tions, two aspects of sense granularity should be ad-
dressed:
Are there sense distinctions that are too close
to be useful in WSD applications, or even to be
clearly distinguished by humans? In general,
what is the semantic distance between senses
of a given word?
Are there sense definitions that are too coarse-
grained, vague or confusing? If so, should they
be split into finer-grain senses?
Our goal is to give a quantitative characterization
of both aspects for the Senseval-2 test suite. Such
measures would enable a finer scoring of WSD sys-
tems, and would provide new criteria to compare this
test suite with data in forthcoming Senseval evalua-
tions.
The first question can be answered with a quanti-
tative estimate of sense proximity. We will use the
cross-linguistic measure of sense distance proposed
in (Resnik and Yarowsky, 1999), where sense relat-
edness between two meanings of a given word,
and , is estimated as the average probability of re-
ceiving the same translation across a set of instances
and a set of languages:
</bodyText>
<equation confidence="0.3369196">
same lexicalization
x w examples
y w examples
same lexicalization
languages
</equation>
<bodyText confidence="0.96178708">
where are the translations of in-
stances into language .
The second question can be addressed by esti-
mating sense stability. Extending the assumption in
(Resnik and Yarowsky, 1999), we propose a sense
stability score based also on cross-lingual evidence:
stability will be estimated with the likelihood for a
pair of occurrences of a word sense of receiving
the same translation for a language , averaged for
as many language (and language families) as possi-
ble:
languages
w examples
This value depends on various factors. Too
coarse-grained sense definition should lead to a
lower stability, since different contexts may high-
light subsenses differently lexicalized across the se-
lected languages. The stability index also reflects
the adequacy of the selected instances and how well
they have been understood by annotators. A sense
with three instances translated into a target language
always by the same word form, will receive the max-
imum stability score ( ). On the contrary, if all trans-
lations are different, the stability index will be min-
imal ( ).
</bodyText>
<sectionHeader confidence="0.925961" genericHeader="introduction">
3 Typology of polysemic relations
</sectionHeader>
<bodyText confidence="0.995416571428571">
According to (Resnik and Yarowsky, 1999), the
cross-lingual estimate of sense proximity introduced
above is highly consistent with the sense groupings
of the Hector database, as used in the Senseval-1
evaluation. However, in our opinion, the hierarchi-
cal structure of senses in Hector (and dictionaries in
general) does not necessarily reflect sense proxim-
ity. Metaphorical sense extensions of a word mean-
ing are a good example: while they are closely re-
lated (in such hierarchical arrangement of senses) to
the source meaning, the metaphorical sense usually
belongs to a different semantic field. If the cross-
lingual measure of sense proximity is also high
for such metaphors, that would mean that they are
highly generalized across languages, but not that the
meanings are related.
In addition, WordNet 1.7, which replaces Hec-
tor as sense inventory in Senseval-2, does not pro-
vide such an explicit arrangement of word senses.
Thus, we decided to classify sense pairs according
to a simple typology of sense extensions (includ-
ing homonymy as absence of relation) and to ver-
ify that the proximity measure is in agreement with
such classification.
We have considered three types of semantic rela-
tion, previously introduced in (Gonzalo et al., 2000):
metonymy (semantic contiguity), for example,
yew-tree and yew-wood or post-letters and post-
system.
metaphor (similarity), for example, child-kid
and child-immature.
specialization/generalization (based on ex-
tending or reducing the scope of the original
sense), for example, fine-greeting and fine-ok.
high were asked to translate marked words in con-
text into their mother tongue.
As working material, we used part of the
Senseval-2 data. Whenever possible, we selected
three contexts (with the highest inter annotator
agreement) for each of the 182 noun and adjective
senses in the Senseval-2 English test suite. How-
ever, for 16 senses there was only one instance in the
corpus and 6 senses had only two occurrences. The
final data set was composed of 508 short contexts for
182 senses of 44 words of the Senseval-2 test suite.
These instances, randomly ordered, were presented
to annotators without sense tag, so that each tagger
had to deduce the sense of the marked word from the
context and type its first preferred translation into his
language in the answer line. This is an example of
the input for annotators:
fine 40129
Mountains on the other side of the valley rose from the
mist like islands, and here and there flecks of cloud, as
pale and tag fine /tag as sea-spray, trailed across
their sombre, wooded slopes.
</bodyText>
<sectionHeader confidence="0.434631" genericHeader="method">
ANSWER: * *
</sectionHeader>
<bodyText confidence="0.995119642857143">
The collected data was used to compute proximity
for every sense pair in the sample. Stability
was computed using the same data, for all senses in
the sample except for 16 cases which had one in-
stance in the corpus.
In order to evaluate how using a more extensive
and more varied set of languages can affect the re-
sults of the experiment, we selected a smaller control
subset of 65 instances of 23 senses corresponding to
3 nouns and 2 adjectives. Annotations for this subset
were collected from 29 speakers of 12 languages 2
covering 5 language families.
homonymy (no relation). For example, bar- 5 Results and discussion
law and bar-unit of pressure.
</bodyText>
<sectionHeader confidence="0.982689" genericHeader="method">
4 Experiment design
</sectionHeader>
<bodyText confidence="0.999474833333333">
Following (Resnik and Yarowsky, 1999), we carried
out an experiment based on free annotations with the
first preferred translation of Senseval-2 nouns and
adjectives in hand-disambiguated contexts.
11 native or bilingual speakers of 4 languages1
with a level of English proficiency from medium to
</bodyText>
<footnote confidence="0.5557065">
1This main set of languages includes Bulgarian, Russian,
Spanish and Urdu.
</footnote>
<bodyText confidence="0.99934225">
Distribution of proximity and stability indexes in the
main set (the whole set of senses tagged in 4 lan-
guages) is shown in Figure 1 and Figure 2.
As can be seen, few sense pairs in Senseval-2
data have been assigned a high proximity index.
This means that most of the senses considered in
Senseval-2 are adequate distinctions to be used in
a WSD evaluation. The average proximity (0.28) is
</bodyText>
<footnote confidence="0.424461">
2This set of languages included Bulgarian, Danish, Dutch,
Finnish, German, Hungarian, Italian, Portuguese, Rumanian,
Russian, Spanish and Urdu.
</footnote>
<figure confidence="0.982192">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
sense proximity
</figure>
<figureCaption confidence="0.997053">
Figure 1: Global distribution of sense proximity
</figureCaption>
<figure confidence="0.9887155">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
sense stability
</figure>
<figureCaption confidence="0.99999">
Figure 2: Distribution of sense stability
</figureCaption>
<bodyText confidence="0.994917333333333">
close to Resnik and Yarowsky’s result for a sample
of Senseval-1 data.
Global stability is very high, which is also a pos-
itive indication (both for sense granularity and for
the quality of the annotated instances). The average
is , and Figure 2 shows that the distribution is
highly skewed towards the high end of the graph.
The discussion of the few cases with low stability is
included in section 5.4 below.
</bodyText>
<subsectionHeader confidence="0.993031">
5.1 Language family distance
</subsectionHeader>
<bodyText confidence="0.999904">
We compared the results for the main set (all annota-
tions in four languages), with the results for the con-
trol set (a subset of the annotations in 12 languages).
Figure 4 shows the average semantic proximity
obtained for the whole Senseval-2 test suite anno-
tated in 4 languages, and for the subset of 23 senses
(65 instances) annotated in 12 languages. The av-
erage difference is large (0.29 vs 0.48); however,
a direct comparison only for the senses annotated
in both samples gives a very similar figure (0.49 vs
0.48). Therefore, it seems that the effect of adding
more languages is not critical, at least for this sense
inventory.
</bodyText>
<subsectionHeader confidence="0.991323">
5.2 Proximity matrices
</subsectionHeader>
<bodyText confidence="0.6088124">
Stability and proximity indexes have been integrated
into proximity matrices as shown in the example in
Figure 3. Stability is shown in the diagonal of the
matrices, and proximity in the cells above the diag-
onal.
</bodyText>
<table confidence="0.721774">
mouth cave lips oral
cave 0.96 0.13 0.13
lips 1.00 1.00
oral 1.00
</table>
<figureCaption confidence="0.996496">
Figure 3: Semantic proximity matrix for mouth
</figureCaption>
<bodyText confidence="0.999379727272727">
On the basis of the translation criterion, all the
senses of mouth in the example have high stability.
Proximity indexes point at two close senses: mouth-
lips and mouth-oral cavity, whereas mouth as open-
ing that resembles a mouth (as of a cave) appears to
be rather distant from the other two, confirming our
intuitions.
These matrices (especially the non-diagonal prox-
imity values) can be used to re-score Senseval-2 sys-
tems applying the measures proposed in (Resnik and
Yarowsky, 1997; Resnik and Yarowsky, 1999).
</bodyText>
<subsectionHeader confidence="0.9712415">
5.3 Similarity and semantic relations between
senses
</subsectionHeader>
<bodyText confidence="0.9999805">
As mentioned in Section 3, all the sense pairs have
been manually classified according to the adopted
typology. Figures 5, 6, 7, 8 show the distribution
of semantic proximity according to this classifica-
tion of sense relations holding between senses of the
same word.
</bodyText>
<sectionHeader confidence="0.468156" genericHeader="method">
5.3.1 Homonyms
</sectionHeader>
<bodyText confidence="0.999388333333333">
As expected, most homonym pairs displayed low
proximity. Only few very specific cases were an ex-
ception, such as the pair formed by bar in the sense
</bodyText>
<figure confidence="0.996637791666667">
140
120
100
80
❯
# pairs
a
❯
60
40
20
0
❱
# senses
n
❱
70
60
50
40
30
20
10
0
</figure>
<table confidence="0.991698933333333">
Main set Control set
language prox. family # taggers language prox. family # taggers
Bulgarian 0.30 Slavonic 1 Bulgarian 0.54 Slavonic 1
Russian 0.25 Slavonic 1 Russian 0.39 Slavonic 1
Spanish 0.31 Romance 8 Spanish 0.53 Romance 8
Urdu 0.28 Indo-Iranian 1 Urdu 0.47 Indo-Iranian 1
Hungarian 0.56 Fino-Hungarian 1
Italian 0.76 Romance 6
Portuguese 0.44 Romance 2
Rumanian 0.59 Romance 1
Danish 0.48 Germanic 3
Dutch 0.40 Germanic 1
Finnish 0.26 Fino-Hungarian 2
German 0.38 Germanic 1
Average 0.29 Average 0.48
</table>
<figureCaption confidence="0.943896">
Figure 4: Average sense proximity
</figureCaption>
<figure confidence="0.948389">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Sense proximity
</figure>
<figureCaption confidence="0.996018">
Figure 5: Distribution of homonyms
</figureCaption>
<figure confidence="0.980666">
0 0.2 0.4 0.6 0.8 1
Sense proximity
</figure>
<figureCaption confidence="0.998199">
Figure 7: Distribution of metaphors
</figureCaption>
<figure confidence="0.99833872">
14
12
10
8
❯
❯
# pairs
a
6
4
2
0
120
100
80
❯
# pairs
a
❯
60
40
20
0
0 0.2 0.4 0.6 0.8 1
Sense proximity
</figure>
<figureCaption confidence="0.998403">
Figure 6: Distribution of metonymy
</figureCaption>
<figure confidence="0.993711">
0 0.2 0.4 0.6 0.8 1
Sense proximity
</figure>
<figureCaption confidence="0.998963">
Figure 8: Distribution of specializations
</figureCaption>
<figure confidence="0.983158782608695">
7
6
5
4
❯
# pairs
a
❯
3
2
1
0
12
10
8
❯
# pairs
a
❯
6
4
2
0
</figure>
<bodyText confidence="0.9998056">
of establishment and bar in the sense of unit ofpres-
sure. These two senses have been loaned by several
languages from English. Therefore, in spite of being
unrelated, these senses yielded a proximity of 0.69.
Except for exceptional cases of this kind, the method
based on multilingual translations proved to be valid
for capturing homonyms. However, it is clear that an
explicit account of homonymy in the sense inventory
(not available in wordnet) would prevent such erro-
neous assignments.
</bodyText>
<subsectionHeader confidence="0.893194">
5.3.2 Metaphors
</subsectionHeader>
<bodyText confidence="0.999951230769231">
One of the features of metaphors is their capa-
bility of linking remote and unrelated semantic do-
mains. Then, sense pairs classified as metaphor-
ically related should have low proximity indexes.
Unexpectedly, we found that this was not always
true. Proximity of 27% of metaphoric sense pairs
was equal or greater than 0.50. Since all of them
were examples of very extended (if not univer-
sal) metaphoric patterns like blind-sight and blind-
irrational or cool-cold and cool-colour, it seems that
calculating sense distance using only the multilin-
gual translation method does not always yield good
estimations.
</bodyText>
<subsectionHeader confidence="0.919277">
5.3.3 Specialization/Generalization
</subsectionHeader>
<bodyText confidence="0.978620451612903">
The sense pairs tagged as instances of special-
ization/generalization, in general, behaved as we
expected, although there also appeared few cases
that contradicted our predictions (medium or high
proximity indexes). The exceptions involved sev-
eral senses offine (fine-superior to the average, fine-
being satisfactory or in satisfactory condition, fine-
all right, being in good health and fine-of weather).
Besides technical issues (discussed in section 5.4),
we believe there are problems of overlapping sense
definitions in WordNet 1.7. Compare for instance:
fine 1, good – (superior to the average; ”in fine spir-
its”; ”a fine student”; ”made good grades”; ”morale was
good”; ”had good weather for the parade”)
with
fine 9 – ((of weather) pleasant; not raining, perhaps with
the sun shining; ”a fine summer evening”)
and
fine 2, all right(predicate), all-right(prenominal), ok,
o.k., okay, hunky-dory – ((informal) being satisfactory
or in satisfactory condition; ”an all-right movie”; ”the
passengers were shaken up but are all right”; ”is every-
thing all right?”; ”everything’s fine”; ”things are okay”;
”dinner and the movies had been fine”; ”another minute
I’d have been fine”)
with
fine 5, all right, fine – (being in good health; ”he’s feel-
ing all right again”; ”I’m fine, how are you?”)fine-all
right, being in good health
Indeed, fine 2, for example, is one of the senses
with lowest stability (0.33) in the sample.
</bodyText>
<subsectionHeader confidence="0.582568">
5.3.4 Metonymy
</subsectionHeader>
<bodyText confidence="0.999983416666667">
As for metonymy, the distribution of proximity in-
dexes indicates that this kind of relation seems to
include different subpatterns (Gonzalo et al., 2000).
A further study of metonymically related senses is
needed in order to correctly interpret sense proxim-
ity in these cases.
Overall, the evidence provided by the classifica-
tion of the results suggests that calculating sense
similarity using multilingual translations is a good
first approximation that should be combined with
additional criteria based on a qualitative consider-
ation of sense relations.
</bodyText>
<subsectionHeader confidence="0.998266">
5.4 Consistency of the data
</subsectionHeader>
<bodyText confidence="0.999951764705882">
The analysis of the results revealed aspects of the ex-
periment design worth mentioning. Free use of syn-
onyms seems to be one of the factors affecting both
sense proximity and stability. As the condition of
being coherent and using the same translation for all
instances of the same sense was not imposed in the
experiment instructions, it is no surprise that some
taggers opted for variability.
Obviously, the inter-annotator agreement for our
data is rather low (54%)3 due to the extensive (and
free) use of synonyms. Even if intra-annotator vari-
ations would have been prevented, there seems to
be no feasible way of guaranteeing that different
speakers of the same language choose the same term
among several synonyms.
A closer look at sense pairs with unexpectedly
low proximity indexes showed that, besides the syn-
</bodyText>
<footnote confidence="0.847309666666667">
3The inter-annotator agreement index has been calculated
using Spanish translations provided by 8 annotators. We found
that two taggers gave the same answer in 54% of cases.
</footnote>
<bodyText confidence="0.667414333333333">
onyms issue, there are other factors that should have
been considered in the experiment design:
Different syntactic realizations:
</bodyText>
<sectionHeader confidence="0.274336" genericHeader="method">
1. N N Adj N N Adj N Prep N
</sectionHeader>
<bodyText confidence="0.999131857142857">
An English noun modifying another noun be-
comes an adjective or a preposition phrase in lan-
guages such as Russian, Bulgarian or Spanish. In the
example below, the marked word is translated with
a noun and with an adjective. Both translations have
the same root, but they were computed as different
translations by the exact-match criterion:
</bodyText>
<listItem confidence="0.614172">
1. As the waves crashed round the hilltops the wiz-
ards’ palaces broke tag free /tag and floated on
the surface of the waves.
</listItem>
<reference confidence="0.73069875">
ANSWER (Spanish): *librarse*
2. In these circumstances, executives feel
tag free /tag to commit corporate crimes.
ANSWER (Spanish): *no cortarse*
3. Suddenly they come on cos they don’t give as much
notice, they are a pain in the tag bum /tag
ANSWER (Spanish): *pesado*
Quality of Senseval-2 annotations
</reference>
<bodyText confidence="0.980541375">
Finally, some erroneous manual annotations of
the Senseval-2 corpus were highlighted by unex-
pected proximity or stability values. For instance,
this sense of fine:
1. Charles put on a low dynamic vice which rose
in crescendo to an order. “Listen, Platoon. Every
man who can beat me to the windmill is excused all
tag fatigues /tag
</bodyText>
<sectionHeader confidence="0.826571" genericHeader="method">
ANSWER (Russian): *rabota*
</sectionHeader>
<bodyText confidence="0.9638785">
2. Though in tag fatigue /tag uniform, they were
almost as smart and well turned out as Charles had
seen such soldiers in peace time behind the railings of
Wellington barracks.
</bodyText>
<sectionHeader confidence="0.683945" genericHeader="method">
ANSWER (Russian): *rabochy*
</sectionHeader>
<reference confidence="0.9243783">
2. Adj Adv
English adjectives in predicative position become
adverbs in other languages. For example, these two
instances offine:
1. Young Duffy was in tag fine /tag form when
he defeated B. Valdimarsson of Iceland 4–0
2. Mr. Frank told Mr. Pilson that Olympics officials
wanted $290 million or more for TV rights to the 1994
Winter Games in Norway. The CBS official said that
price sounded tag fine /tag
</reference>
<bodyText confidence="0.999676833333333">
have a very close meaning. However, in lan-
guages such as Spanish or Russian the first one is
translated by an adjective and the second one by an
adverb. This caused that adverb translations were
computed negatively, as they did not match exactly
with the form of the corresponding adjective.
</bodyText>
<sectionHeader confidence="0.574453" genericHeader="method">
Collocations
</sectionHeader>
<bodyText confidence="0.976444290322581">
Collocations constituted another problem, that
should have been foreseen when selecting represen-
tative contexts for senses. Some of the instances
turned out to be part of complex expressions and
could not be naturally translated separated from the
rest of the collocation. Some examples are:
fine - ((metallurgy); free or impurities; having a high or
specified degree of purity; ”gold 21 carats fine”).
was used to tag (incorrectly) the following in-
stances:
fine 40089
An NTA regional commendation was awarded to Hick-
son &amp; Welch, tag fine /tag chemical specialist,
for its training initiative to develop multi-skilled em-
ployees and ”world-class” standards in safety and ef-
ficiency.
fine 40144
There are many custom and tag fine /tag chemical
manufacturers but few, if any, have EFC’s long experi-
ence in multi-step, complex, organic synthesis – knowl-
edge gained from years of experience in the manufac-
ture of photochemicals, dyes and pharmaceuticals.
fine 40162
The manufacturer’s safety data sheet warns of a poten-
tially hazardous reaction between sodium borohydride
and tag fine /tag dispersed heavy metals but this
reaction with charcoal and solid sodium borohydride
are stored and handled in such a way that the risk of
contact between them is avoided.
Probably the extensive use of Senseval-2 data will
permit pruning such kind of errors in a near future.
</bodyText>
<sectionHeader confidence="0.999722" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.992121621621622">
We have provided a qualitative and quantitative
characterization of the subset of WordNet 1.7 used
as sense inventory in Senseval-2. Individual senses
are given a stability measure that indicates their
degree of homogeneity and whether they should
be revised for further division into sub-senses.
Sense pairs are classified as homonyms, generaliza-
tion/specialization, metonymic or metaphorical ex-
tensions. In addition, a proximity measure gives a
numerical account of their similarity.
The experiment conducted for the Senseval-2
test suite supports the validity of the proposals in
(Resnik and Yarowsky, 1999). Multilingual transla-
tions collected for a set of monolingual senses pro-
vide a helpful measure of semantic proximity. In
addition, the same data can also be used to measure
sense stability successfully. The matrices obtained
in the experiment are of a practical interest: they
can be used to re-score Senseval-2 systems taking
semantic distance into account.
Our global results indicate that WordNet 1.7 is a
reliable sense inventory for developing and testing
WSD systems. Cases of very close sense pairs or
too coarse sense definitions are marginal according
to our data.
We have also provided some evidence that cross-
lingual estimation of sense proximity should how-
ever be combined with some additional criteria re-
lated to the nature of sense relations. In particular,
an explicit account for homonyms and metaphors in
WordNet would help to correct too high estimations
of the translation criterion.
The full set of data obtained in the experiments,
including proximity matrices for all nouns and ad-
jectives in the Senseval-2 test suite, can be down-
loaded from:
http://sensei.lsi.uned.es/senseval2
</bodyText>
<sectionHeader confidence="0.998322" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999918444444444">
This work has been funded by a Spanish government
grant, project Hermes (CICyT TIC2000-0335-C03-
01), by a UNED PhD. grant. We would also like
to thank Adam Kilgarriff, whom we owe the idea
of this study, and the ITRI (University of Brighton)
for their help and support. Our acknowledgments go
as well to the volunteers who annotated the corpus
and without whom this study would not have been
possible.
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998996384615385">
Eneko Agirre and German Rigau. 1996. Word sense dis-
ambiguation using conceptual density. In Proceedings
of COLING’96.
Julio Gonzalo, Irina Chugur, and Felisa Verdejo. 2000.
Sense clustering for information retrieval: evidence
from Semcor and the EWN InterLingual Index. In
Proceedings of the ACL’00 Workshop on Word Senses
and Multilinguality.
A. Kilgarriff and M. Palmer. 2000. Special issue on Sen-
seval. Computers and the Humanities, 34(1-2).
P. Resnik and D. Yarowsky. 1997. A perspective on word
sense disambiguation methods and their evaluation. In
Proc. ACL SIGLEX Workshop on tagging text with lex-
ical semantics: why, what and how?
P. Resnik and D. Yarowsky. 1999. Distinguishing sys-
tems and distinguishing senses: new evaluation meth-
ods for word sense disambiguation. Natural Language
Engineering, 5.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
IJCAI.
M. Sussna. 1993. Word sense disambiguation for free-
text indexing using a massive semantic network. In
Proceedings of the Second International Conference
on Information and Knowledge Base Management,
CIKM’93.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.381217">
<title confidence="0.933518">Polysemy and Sense Proximity in the Senseval-2 Test Suite.</title>
<author confidence="0.848262">Irina Chugur Julio Gonzalo Felisa Verdejo</author>
<affiliation confidence="0.739924">irina@lsi.uned.es julio@lsi.uned.es Departamento de Lenguajes y Sistemas Universidad Nacional de Educaci´on a Distancia</affiliation>
<address confidence="0.911201">Madrid, Spain</address>
<abstract confidence="0.999733">We report on an empirical study of sense relations in the Senseval-2 test suite. We apply and extend the method described in (Resnik and Yarowsky, 1999), estimating proximity of sense pairs from the evidence collected from native-speaker translations of 508 contexts across 4 Indoeuropean languages representing 3 language families. A control set composed of 65 contexts has also been annotated in 12 languages (including 2 non-Indoeuropean languages) in order to estimate the correlation between parallel polysemy and language family distance. A new parameter, sense stability, is introduced to assess the homogeneity of each individual sense definition. Finally, we combine the sense proximity estimation with a classification of semantic relations between senses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>ANSWER</author>
</authors>
<title>librarse* 2. In these circumstances, executives feel tag free /tag to commit corporate crimes. ANSWER (Spanish): *no cortarse* 3. Suddenly they come on cos they don’t give as much notice, they are a pain in the tag bum /tag ANSWER (Spanish): *pesado* Quality of Senseval-2 annotations 2. Adj Adv English adjectives in predicative position become adverbs in other languages. For example, these two instances offine: 1. Young Duffy was in tag fine /tag form when he defeated B.</title>
<journal>Valdimarsson of Iceland</journal>
<volume>4</volume>
<marker>ANSWER, </marker>
<rawString>ANSWER (Spanish): *librarse* 2. In these circumstances, executives feel tag free /tag to commit corporate crimes. ANSWER (Spanish): *no cortarse* 3. Suddenly they come on cos they don’t give as much notice, they are a pain in the tag bum /tag ANSWER (Spanish): *pesado* Quality of Senseval-2 annotations 2. Adj Adv English adjectives in predicative position become adverbs in other languages. For example, these two instances offine: 1. Young Duffy was in tag fine /tag form when he defeated B. Valdimarsson of Iceland 4–0 2. Mr. Frank told Mr. Pilson that Olympics officials wanted $290 million or more for TV rights to the 1994 Winter Games in Norway. The CBS official said that price sounded tag fine /tag</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>German Rigau</author>
</authors>
<title>Word sense disambiguation using conceptual density.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING’96.</booktitle>
<contexts>
<context position="2384" citStr="Agirre and Rigau, 1996" startWordPosition="359" endWordPosition="362">roved for evaluation purposes, for instance, splitting senses into finer-grained distinctions or collapsing close senses into coarser clusters? In particular, we are interested in characterizing WordNet 1.7 as sense inventory for the Senseval-2 WSD comparative evaluation. Unlike conventional dictionaries, WordNet does not group senses of the same word in a hierarchical structure; every sense belongs to a synset, and can only be related to other senses via conceptual relations (rather than sense relations). Conceptual relations can be used to define measures of semantic distance (Sussna, 1993; Agirre and Rigau, 1996; Resnik, 1995), but topic relatedness is not well captured by wordnet relations, and this is a fundamental parameter to estimate sense similarity in many NLP applications (Gonzalo et al., 2000). The issue of estimating semantic distance between senses of a polysemous word has been previously addressed in (Resnik and Yarowsky, 1997; Resnik and Yarowsky, 1999). They propose a measure of semantic distance based on the likelihood of the sense distinction being lexicalized in some target language. The measure was tested using statistics collected from native-speaker translations of 222 polysemous </context>
</contexts>
<marker>Agirre, Rigau, 1996</marker>
<rawString>Eneko Agirre and German Rigau. 1996. Word sense disambiguation using conceptual density. In Proceedings of COLING’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Gonzalo</author>
<author>Irina Chugur</author>
<author>Felisa Verdejo</author>
</authors>
<title>Sense clustering for information retrieval: evidence from Semcor and the EWN InterLingual Index.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL’00 Workshop on Word Senses and Multilinguality.</booktitle>
<contexts>
<context position="2578" citStr="Gonzalo et al., 2000" startWordPosition="391" endWordPosition="394">WordNet 1.7 as sense inventory for the Senseval-2 WSD comparative evaluation. Unlike conventional dictionaries, WordNet does not group senses of the same word in a hierarchical structure; every sense belongs to a synset, and can only be related to other senses via conceptual relations (rather than sense relations). Conceptual relations can be used to define measures of semantic distance (Sussna, 1993; Agirre and Rigau, 1996; Resnik, 1995), but topic relatedness is not well captured by wordnet relations, and this is a fundamental parameter to estimate sense similarity in many NLP applications (Gonzalo et al., 2000). The issue of estimating semantic distance between senses of a polysemous word has been previously addressed in (Resnik and Yarowsky, 1997; Resnik and Yarowsky, 1999). They propose a measure of semantic distance based on the likelihood of the sense distinction being lexicalized in some target language. The measure was tested using statistics collected from native-speaker translations of 222 polysemous contexts across 12 languages. The results obtained showed that monolingual sense disProceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directio</context>
<context position="7858" citStr="Gonzalo et al., 2000" startWordPosition="1229" endWordPosition="1232">of sense proximity is also high for such metaphors, that would mean that they are highly generalized across languages, but not that the meanings are related. In addition, WordNet 1.7, which replaces Hector as sense inventory in Senseval-2, does not provide such an explicit arrangement of word senses. Thus, we decided to classify sense pairs according to a simple typology of sense extensions (including homonymy as absence of relation) and to verify that the proximity measure is in agreement with such classification. We have considered three types of semantic relation, previously introduced in (Gonzalo et al., 2000): metonymy (semantic contiguity), for example, yew-tree and yew-wood or post-letters and postsystem. metaphor (similarity), for example, child-kid and child-immature. specialization/generalization (based on extending or reducing the scope of the original sense), for example, fine-greeting and fine-ok. high were asked to translate marked words in context into their mother tongue. As working material, we used part of the Senseval-2 data. Whenever possible, we selected three contexts (with the highest inter annotator agreement) for each of the 182 noun and adjective senses in the Senseval-2 Engli</context>
<context position="17199" citStr="Gonzalo et al., 2000" startWordPosition="2803" endWordPosition="2806">l-right movie”; ”the passengers were shaken up but are all right”; ”is everything all right?”; ”everything’s fine”; ”things are okay”; ”dinner and the movies had been fine”; ”another minute I’d have been fine”) with fine 5, all right, fine – (being in good health; ”he’s feeling all right again”; ”I’m fine, how are you?”)fine-all right, being in good health Indeed, fine 2, for example, is one of the senses with lowest stability (0.33) in the sample. 5.3.4 Metonymy As for metonymy, the distribution of proximity indexes indicates that this kind of relation seems to include different subpatterns (Gonzalo et al., 2000). A further study of metonymically related senses is needed in order to correctly interpret sense proximity in these cases. Overall, the evidence provided by the classification of the results suggests that calculating sense similarity using multilingual translations is a good first approximation that should be combined with additional criteria based on a qualitative consideration of sense relations. 5.4 Consistency of the data The analysis of the results revealed aspects of the experiment design worth mentioning. Free use of synonyms seems to be one of the factors affecting both sense proximit</context>
</contexts>
<marker>Gonzalo, Chugur, Verdejo, 2000</marker>
<rawString>Julio Gonzalo, Irina Chugur, and Felisa Verdejo. 2000. Sense clustering for information retrieval: evidence from Semcor and the EWN InterLingual Index. In Proceedings of the ACL’00 Workshop on Word Senses and Multilinguality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>M Palmer</author>
</authors>
<date>2000</date>
<booktitle>Special issue on Senseval. Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="3602" citStr="Kilgarriff and Palmer, 2000" startWordPosition="539" endWordPosition="542">olysemous contexts across 12 languages. The results obtained showed that monolingual sense disProceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, Philadelphia, July 2002, pp. 32-39. Association for Computational Linguistics. tinctions at most levels of granularity can be effectively captured by translations into some set of second languages, especially as language family distance increases. The distance matrices obtained reproduced faithfully the hierarchical arrangement of senses provided by the Hector database used in Senseval-1 (Kilgarriff and Palmer, 2000). In order to characterize the Senseval-2 Wordnet 1.7 subset, we have adopted such methodology, extending it to capture also individual sense homogeneity, and comparing both quantitative measures with a coarse, qualitative classification of semantic relations between senses of a word. In Section 2 we introduce the quantitative measures of sense relatedness (as defined by Resnik &amp; Yarowsky) and sense stability (as an extension to it). In Section 3, we describe the qualitative classification that will be confronted to such measures. In Sections 4 and 5, we describe the experiment design and disc</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>A. Kilgarriff and M. Palmer. 2000. Special issue on Senseval. Computers and the Humanities, 34(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>D Yarowsky</author>
</authors>
<title>A perspective on word sense disambiguation methods and their evaluation.</title>
<date>1997</date>
<booktitle>In Proc. ACL SIGLEX Workshop on</booktitle>
<contexts>
<context position="2717" citStr="Resnik and Yarowsky, 1997" startWordPosition="414" endWordPosition="417">senses of the same word in a hierarchical structure; every sense belongs to a synset, and can only be related to other senses via conceptual relations (rather than sense relations). Conceptual relations can be used to define measures of semantic distance (Sussna, 1993; Agirre and Rigau, 1996; Resnik, 1995), but topic relatedness is not well captured by wordnet relations, and this is a fundamental parameter to estimate sense similarity in many NLP applications (Gonzalo et al., 2000). The issue of estimating semantic distance between senses of a polysemous word has been previously addressed in (Resnik and Yarowsky, 1997; Resnik and Yarowsky, 1999). They propose a measure of semantic distance based on the likelihood of the sense distinction being lexicalized in some target language. The measure was tested using statistics collected from native-speaker translations of 222 polysemous contexts across 12 languages. The results obtained showed that monolingual sense disProceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, Philadelphia, July 2002, pp. 32-39. Association for Computational Linguistics. tinctions at most levels of granularity can be effective</context>
<context position="12884" citStr="Resnik and Yarowsky, 1997" startWordPosition="2072" endWordPosition="2075">e cells above the diagonal. mouth cave lips oral cave 0.96 0.13 0.13 lips 1.00 1.00 oral 1.00 Figure 3: Semantic proximity matrix for mouth On the basis of the translation criterion, all the senses of mouth in the example have high stability. Proximity indexes point at two close senses: mouthlips and mouth-oral cavity, whereas mouth as opening that resembles a mouth (as of a cave) appears to be rather distant from the other two, confirming our intuitions. These matrices (especially the non-diagonal proximity values) can be used to re-score Senseval-2 systems applying the measures proposed in (Resnik and Yarowsky, 1997; Resnik and Yarowsky, 1999). 5.3 Similarity and semantic relations between senses As mentioned in Section 3, all the sense pairs have been manually classified according to the adopted typology. Figures 5, 6, 7, 8 show the distribution of semantic proximity according to this classification of sense relations holding between senses of the same word. 5.3.1 Homonyms As expected, most homonym pairs displayed low proximity. Only few very specific cases were an exception, such as the pair formed by bar in the sense 140 120 100 80 ❯ # pairs a ❯ 60 40 20 0 ❱ # senses n ❱ 70 60 50 40 30 20 10 0 Main se</context>
</contexts>
<marker>Resnik, Yarowsky, 1997</marker>
<rawString>P. Resnik and D. Yarowsky. 1997. A perspective on word sense disambiguation methods and their evaluation. In Proc. ACL SIGLEX Workshop on tagging text with lexical semantics: why, what and how?</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>D Yarowsky</author>
</authors>
<title>Distinguishing systems and distinguishing senses: new evaluation methods for word sense disambiguation.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<contexts>
<context position="2745" citStr="Resnik and Yarowsky, 1999" startWordPosition="418" endWordPosition="421">a hierarchical structure; every sense belongs to a synset, and can only be related to other senses via conceptual relations (rather than sense relations). Conceptual relations can be used to define measures of semantic distance (Sussna, 1993; Agirre and Rigau, 1996; Resnik, 1995), but topic relatedness is not well captured by wordnet relations, and this is a fundamental parameter to estimate sense similarity in many NLP applications (Gonzalo et al., 2000). The issue of estimating semantic distance between senses of a polysemous word has been previously addressed in (Resnik and Yarowsky, 1997; Resnik and Yarowsky, 1999). They propose a measure of semantic distance based on the likelihood of the sense distinction being lexicalized in some target language. The measure was tested using statistics collected from native-speaker translations of 222 polysemous contexts across 12 languages. The results obtained showed that monolingual sense disProceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, Philadelphia, July 2002, pp. 32-39. Association for Computational Linguistics. tinctions at most levels of granularity can be effectively captured by translations </context>
<context position="5252" citStr="Resnik and Yarowsky, 1999" startWordPosition="807" endWordPosition="810">e semantic distance between senses of a given word? Are there sense definitions that are too coarsegrained, vague or confusing? If so, should they be split into finer-grain senses? Our goal is to give a quantitative characterization of both aspects for the Senseval-2 test suite. Such measures would enable a finer scoring of WSD systems, and would provide new criteria to compare this test suite with data in forthcoming Senseval evaluations. The first question can be answered with a quantitative estimate of sense proximity. We will use the cross-linguistic measure of sense distance proposed in (Resnik and Yarowsky, 1999), where sense relatedness between two meanings of a given word, and , is estimated as the average probability of receiving the same translation across a set of instances and a set of languages: same lexicalization x w examples y w examples same lexicalization languages where are the translations of instances into language . The second question can be addressed by estimating sense stability. Extending the assumption in (Resnik and Yarowsky, 1999), we propose a sense stability score based also on cross-lingual evidence: stability will be estimated with the likelihood for a pair of occurrences of</context>
<context position="6651" citStr="Resnik and Yarowsky, 1999" startWordPosition="1036" endWordPosition="1039">n various factors. Too coarse-grained sense definition should lead to a lower stability, since different contexts may highlight subsenses differently lexicalized across the selected languages. The stability index also reflects the adequacy of the selected instances and how well they have been understood by annotators. A sense with three instances translated into a target language always by the same word form, will receive the maximum stability score ( ). On the contrary, if all translations are different, the stability index will be minimal ( ). 3 Typology of polysemic relations According to (Resnik and Yarowsky, 1999), the cross-lingual estimate of sense proximity introduced above is highly consistent with the sense groupings of the Hector database, as used in the Senseval-1 evaluation. However, in our opinion, the hierarchical structure of senses in Hector (and dictionaries in general) does not necessarily reflect sense proximity. Metaphorical sense extensions of a word meaning are a good example: while they are closely related (in such hierarchical arrangement of senses) to the source meaning, the metaphorical sense usually belongs to a different semantic field. If the crosslingual measure of sense proxi</context>
<context position="9906" citStr="Resnik and Yarowsky, 1999" startWordPosition="1570" endWordPosition="1573"> Stability was computed using the same data, for all senses in the sample except for 16 cases which had one instance in the corpus. In order to evaluate how using a more extensive and more varied set of languages can affect the results of the experiment, we selected a smaller control subset of 65 instances of 23 senses corresponding to 3 nouns and 2 adjectives. Annotations for this subset were collected from 29 speakers of 12 languages 2 covering 5 language families. homonymy (no relation). For example, bar- 5 Results and discussion law and bar-unit of pressure. 4 Experiment design Following (Resnik and Yarowsky, 1999), we carried out an experiment based on free annotations with the first preferred translation of Senseval-2 nouns and adjectives in hand-disambiguated contexts. 11 native or bilingual speakers of 4 languages1 with a level of English proficiency from medium to 1This main set of languages includes Bulgarian, Russian, Spanish and Urdu. Distribution of proximity and stability indexes in the main set (the whole set of senses tagged in 4 languages) is shown in Figure 1 and Figure 2. As can be seen, few sense pairs in Senseval-2 data have been assigned a high proximity index. This means that most of </context>
<context position="12912" citStr="Resnik and Yarowsky, 1999" startWordPosition="2076" endWordPosition="2079"> mouth cave lips oral cave 0.96 0.13 0.13 lips 1.00 1.00 oral 1.00 Figure 3: Semantic proximity matrix for mouth On the basis of the translation criterion, all the senses of mouth in the example have high stability. Proximity indexes point at two close senses: mouthlips and mouth-oral cavity, whereas mouth as opening that resembles a mouth (as of a cave) appears to be rather distant from the other two, confirming our intuitions. These matrices (especially the non-diagonal proximity values) can be used to re-score Senseval-2 systems applying the measures proposed in (Resnik and Yarowsky, 1997; Resnik and Yarowsky, 1999). 5.3 Similarity and semantic relations between senses As mentioned in Section 3, all the sense pairs have been manually classified according to the adopted typology. Figures 5, 6, 7, 8 show the distribution of semantic proximity according to this classification of sense relations holding between senses of the same word. 5.3.1 Homonyms As expected, most homonym pairs displayed low proximity. Only few very specific cases were an exception, such as the pair formed by bar in the sense 140 120 100 80 ❯ # pairs a ❯ 60 40 20 0 ❱ # senses n ❱ 70 60 50 40 30 20 10 0 Main set Control set language prox.</context>
</contexts>
<marker>Resnik, Yarowsky, 1999</marker>
<rawString>P. Resnik and D. Yarowsky. 1999. Distinguishing systems and distinguishing senses: new evaluation methods for word sense disambiguation. Natural Language Engineering, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="2399" citStr="Resnik, 1995" startWordPosition="363" endWordPosition="364">poses, for instance, splitting senses into finer-grained distinctions or collapsing close senses into coarser clusters? In particular, we are interested in characterizing WordNet 1.7 as sense inventory for the Senseval-2 WSD comparative evaluation. Unlike conventional dictionaries, WordNet does not group senses of the same word in a hierarchical structure; every sense belongs to a synset, and can only be related to other senses via conceptual relations (rather than sense relations). Conceptual relations can be used to define measures of semantic distance (Sussna, 1993; Agirre and Rigau, 1996; Resnik, 1995), but topic relatedness is not well captured by wordnet relations, and this is a fundamental parameter to estimate sense similarity in many NLP applications (Gonzalo et al., 2000). The issue of estimating semantic distance between senses of a polysemous word has been previously addressed in (Resnik and Yarowsky, 1997; Resnik and Yarowsky, 1999). They propose a measure of semantic distance based on the likelihood of the sense distinction being lexicalized in some target language. The measure was tested using statistics collected from native-speaker translations of 222 polysemous contexts across</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sussna</author>
</authors>
<title>Word sense disambiguation for freetext indexing using a massive semantic network.</title>
<date>1993</date>
<booktitle>In Proceedings of the Second International Conference on Information and Knowledge Base Management, CIKM’93.</booktitle>
<contexts>
<context position="2360" citStr="Sussna, 1993" startWordPosition="357" endWordPosition="358">ventory be improved for evaluation purposes, for instance, splitting senses into finer-grained distinctions or collapsing close senses into coarser clusters? In particular, we are interested in characterizing WordNet 1.7 as sense inventory for the Senseval-2 WSD comparative evaluation. Unlike conventional dictionaries, WordNet does not group senses of the same word in a hierarchical structure; every sense belongs to a synset, and can only be related to other senses via conceptual relations (rather than sense relations). Conceptual relations can be used to define measures of semantic distance (Sussna, 1993; Agirre and Rigau, 1996; Resnik, 1995), but topic relatedness is not well captured by wordnet relations, and this is a fundamental parameter to estimate sense similarity in many NLP applications (Gonzalo et al., 2000). The issue of estimating semantic distance between senses of a polysemous word has been previously addressed in (Resnik and Yarowsky, 1997; Resnik and Yarowsky, 1999). They propose a measure of semantic distance based on the likelihood of the sense distinction being lexicalized in some target language. The measure was tested using statistics collected from native-speaker transla</context>
</contexts>
<marker>Sussna, 1993</marker>
<rawString>M. Sussna. 1993. Word sense disambiguation for freetext indexing using a massive semantic network. In Proceedings of the Second International Conference on Information and Knowledge Base Management, CIKM’93.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>