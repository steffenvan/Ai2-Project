<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001507">
<title confidence="0.996372">
Using NLG to Help Language-Impaired Users Tell Stories and
Participate in Social Dialogues
</title>
<author confidence="0.946245">
Ehud Reiter, Ross Turner
</author>
<affiliation confidence="0.981596">
University of Aberdeen
</affiliation>
<address confidence="0.588252">
Aberdeen, UK
</address>
<email confidence="0.9662105">
e.reiter@abdn.ac.uk
csc272@abdn.ac.uk
</email>
<author confidence="0.98794">
Norman Alm, Rolf Black,
Martin Dempster, Annalu Waller
</author>
<affiliation confidence="0.998119">
University of Dundee
</affiliation>
<address confidence="0.74827">
Dundee, UK
</address>
<email confidence="0.9013875">
{nalm,rolfblack,martindempster,
awaller}@computing.dundee.ac.uk
</email>
<sectionHeader confidence="0.995348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997442583333333">
Augmentative and Alternative Communication
(AAC) systems are communication aids for
people who cannot speak because of motor or
cognitive impairments. We are developing
AAC systems where users select information
they wish to communicate, and this is ex-
pressed using an NLG system. We believe
this model will work well in contexts where
AAC users wish to go beyond simply making
requests or answering questions, and have
more complex communicative goals such as
story-telling and social interaction.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99981485">
Many people have difficulty in communicating
linguistically because of cognitive or motor im-
pairments. Such people typically use communi-
cation aids to help them interact with other peo-
ple. Such communication aids range from sim-
ple tools that do not involve computers, such as
picture cards, to complex software systems that
attempt to “speak” for the impaired user.
From a technological perspective, even the
most complex communication aids have typi-
cally been based on fixed (canned) texts or sim-
ple fill-in-the-blank templates; essentially the
user selects a text or template from a set of pos-
sible utterances, and the system utters it. We
believe that while this may be adequate if the
user is simply making a request (e.g., please give
me a drink) or answering a question (e.g., I live
at home), it is not adequate if the user has a more
complex communicative goal, such as engaging
in social interaction, or telling a story.
We are exploring the idea of supporting such
interactions by building a system which uses ex-
ternal data and/or knowledge sources, plus do-
main and conversational models, to dynamically
suggest possible messages (event, facts, or opin-
ions, represented as ontology instances) which
are appropriate to the conversation. The user se-
lects the specific message which he wishes the
system to speak, and possibly adds simple anno-
tations (e.g., I like this) or otherwise edits the
message. The system then creates an appropriate
linguistic utterance from the selected message,
taking into consideration contextual factors.
In this paper we describe two projects on
which we are working within this framework.
The goal of the first project is to help non-
speaking children tell stories about their day at
school to their parents; the goal of the second
project is to help non-speaking adults engage in
social conversation.
</bodyText>
<sectionHeader confidence="0.981413" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.424693">
2.1 Augmentative and alternative commu-
nication
</subsectionHeader>
<bodyText confidence="0.999158777777778">
Augmentative and alternative communication
(AAC) is a term that describes a variety of meth-
ods of communication for non-speaking people
which can supplement or replace speech. The
term covers techniques which require no equip-
ment, such as sign language and cards with im-
ages; and also more technologically complex
systems which use speech synthesis and a variety
of strategies to create utterances.
The most flexible AAC systems allow users to
specify arbitrary words, but communication rates
are extremely low, averaging 2-10 words per
minute. This is because many AAC users interact
slowly with computers because of their impair-
ments. For example, some of the children we
work with cannot use their hands, so they use
scanning interfaces with head switches. In other
words, the computer displays a number of op-
</bodyText>
<note confidence="0.9158225">
Proceedings of the 12th European Workshop on Natural Language Generation, pages 1–8,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.981433">
1
</page>
<bodyText confidence="0.99920465">
tions to them, and then scans through these,
briefly highlighting each option. When the de-
sired option is highlighted, the child selects it by
pressing a switch with her head. This is ade-
quate for communicating basic needs (such as
hunger or thirst); the computer can display a
menu of possible needs, and the child can select
one of the items. But creating arbitrary messages
with such an interface is extremely slow, even if
word prediction is used; and in general such in-
terfaces do not well support complex social in-
teractions such as story telling (Waller, 2006).
A number of research projects in AAC have
developed prototype systems which attempt to
facilitate this type of human-human interaction.
At their most basic, these systems provide users
with a library of fixed “conversational moves”
which can be selected and uttered. These moves
are based on models of the usual shape and con-
tent of conversational encounters (Todman &amp;
Alm, 2003), and for example include standard
conversational openings and closings, such as
Hello and How are you. They also include back-
channel communication such as Uh-huh, Great!,
and Sorry, can you repeat that.
It would be very useful to go beyond standard
openings, closings, and backchannel messages,
and allow the user to select utterances which
were relevant to the particular communicative
context and goals. Dye et al (1998) developed a
system based on scripts of common interactions
(Schank &amp; Abelson, 1977). For example, a user
could activate the MakeAnAppointment script,
and then could select utterances relevant to this
script, such as I would like to make an appoint-
ment to see the doctor. As the interaction pro-
gressed, the system would update the selections
offered to the user based on the current stage of
the script; for example during time negotiation a
possible utterance would be I would like to see
him next week. This system proved effective in
trials, but needed a large number of scripts to be
generally effective. Users could author their own
texts, which were added to the scripts, but this
was time-consuming and had to be done in ad-
vance of the conversation.
Another goal of AAC is to help users narrate
stories. Narrative and storytelling play a very
important part in the communicative repertoire of
all speakers (Schank, 1990). In particular, the
ability to draw on episodes from one’s life his-
tory in current conversation is vital to maintain-
ing a full impression of one’s personality in deal-
ing with others (Polkinghorne, 1991). Story tell-
ing tools for AAC users have been developed,
which include ways to introduce a story, tell it at
the pace required (with diversions) and give
feedback to comments from listeners (Waller,
2006); but again these tools are based on a li-
brary of fixed texts and templates.
</bodyText>
<subsectionHeader confidence="0.987596">
2.2 NLG and AAC
</subsectionHeader>
<bodyText confidence="0.993673638297873">
Natural language generation (NLG) systems
generate texts in English and other human lan-
guages from non-linguistic input (Reiter and
Dale, 2000). In their review of NLP and AAC,
Newell, Langer, and Hickey (1998) suggest that
NLG could be used to generate complete utter-
ances from the limited input that AAC users are
able to provide. For example, the Compansion
project (McCoy, Pennington, Badman 1998)
used NLP and NLG techniques to expand tele-
graphic user input, such as Mary go store?, into
complete utterances, such as Did Mary go to the
store? Netzer and Elhadad (2006) allowed users
to author utterances in the symbolic language
BLISS, and used NLG to translate this to English
and Hebrew texts.
In recent years there has been growing interest
in data-to-text NLG systems (Reiter, 2007);
these systems generate texts based on sensor and
other numerical data, supplemented with ontolo-
gies that specify domain knowledge. In princi-
ple, it seems that data-to-text techniques should
allow NLG systems to provide more assistance
than the syntactic help provided by Compansion.
For example, if the user wanted to talk about a
recent football (soccer) match, a data-to-text sys-
tem could get actual data about the match from
the web, and generate potential utterances from
this data, such as Arsenal beat Chelsea 2-1 and
Van Persie scored two goals; the user could then
select one of these to utter.
In addition to helping users interact with other
people, NLG techniques can also be used to edu-
cate and encourage children with disabilities.
The STANDUP system (Manurung, Ritchie et
al., 2008), for example, used NLG and computa-
tional humour techniques to allow children who
use AAC devices to generate novel punning
jokes. This provided the children with successful
experiences of controlling language, gave them
an opportunity to play with language and explore
new vocabulary (Waller et al., in press). In a
small study with nine children with cerebral
palsy, the children used their regular AAC tools
more and also performed better on a test measur-
ing linguistic abilities after they used STANDUP
for ten weeks.
</bodyText>
<page confidence="0.987979">
2
</page>
<sectionHeader confidence="0.984341" genericHeader="method">
3 Our Architecture
</sectionHeader>
<bodyText confidence="0.999948837837838">
Our goal is help AAC users engage in com-
plex social interaction by using NLG and data-
to-text technology to create potential utterances
and conversational contributions for the users.
The general architecture is shown in Figure 1,
and Sections 4 and 5 describe two systems based
on this architecture.
The system has the following components:
Data analysis: read in data, from sensors,
web information sources, databases, and so forth.
This module analyses this data and identifies
messages (in the sense of Reiter and Dale
(2000)) that the user is likely to want to commu-
nicate; this analysis is partially based on domain,
conversation, and user models, which may be
represented as ontologies.
Editing: allow the user to edit the messages.
Editing ranges from adding simple annotations to
specify opinions (e.g., add BAD to Arsenal beat
Chelsea 2-1 if the user is a Chelsea fan), to using
an on-screen keyboard to type free-text com-
ments. Users can also delete messages, specify
which messages they are most likely to want to
utter, and create new messages. Editing is done
before the actual conversation, so the user does
not have to do this under time pressure. The
amount of editing which can be done partially
depends on the extent of the user’s disabilities.
Narration: allows the user to select mes-
sages, and perhaps conversational moves (e.g.,
Hello), in an actual conversational context. Edit-
ing is possible, but is limited by the need to keep
the conversation flowing.
NLG and Speech Synthesis: Generates actual
utterances from the selected messages, taking
into account linguistic context, especially a dia-
logue model.
</bodyText>
<sectionHeader confidence="0.994898" genericHeader="method">
4 Narrative for Children: How was
</sectionHeader>
<subsectionHeader confidence="0.95658">
School Today
</subsectionHeader>
<bodyText confidence="0.999967925925926">
The goal of the How was School Today project is
to enable non-speaking children with major mo-
tor disabilities but reasonable cognitive skills to
tell a story about what they did at school during
the day. The particular children we are working
with have cerebral palsy, and use wheelchairs. A
few of them can use touch screens, but most of
them use a head switch and scanning interface,
as described above. By ‘story’, we mean some-
thing similar to Labov’s (1972) conversational
narrative, i.e., a series of linked real-world events
which are unusual or otherwise interesting, pos-
sibly annotated with information about the
child’s feelings, which can be narrated orally.
We are not expecting stories in the literary sense,
with character development and complex plots.
The motivation of the project is to provide the
children with successful narrative experience.
Typically developing children develop narrative
skills from an early age with adults scaffolding
conversations to elicit narrative, e.g. “What did
you do at school today?” (Bruner, 1975). As the
child’s vocabulary and language competence
develops, scaffolding is reduced. This progres-
sion is seldom seen in children with complex
communication needs – they respond to closed
questions but seldom take control of conversa-
</bodyText>
<figure confidence="0.604596">
User
</figure>
<figureCaption confidence="0.991218">
Figure 1: General architecture
</figureCaption>
<figure confidence="0.998914827586207">
Sensor
data
Data analysis:
select possible
messages to
communicate
Narration: User
selects what to say
Editing: User adds
annotations
NLG:
Generate
utterance
Speech
synthesis
Web info
sources
Other
external data
Prepare content
Narrate content
Dialogue
model
Conversation
partner
Conversation
model
Domain model
User model
</figure>
<page confidence="0.983704">
3
</page>
<bodyText confidence="0.999868153846154">
tion (von Tetzchner and Grove, 2003). Many
children who use AAC have very limited narra-
tive skills (Soto et al, 2006). Research has shown
that providing children who use AAC with suc-
cessful narrative experiences by providing full
narrative text can help the development of writ-
ten and spoken narrative skills (Waller, 2008).
The system follows the architecture described
above. Input data comes from RFID sensors that
track where the child went during the day; an
RFID reader is mounted on the child’s wheel-
chair, and RFID tags are placed around the
school, especially in doorways so we can moni-
tor children entering and leaving rooms. Teach-
ers have also been given RFID swipe cards
which they can swipe against a reader, to record
that they are interacting with the child; this is
more robust than attempting to infer interaction
automatically by tracking teachers’ position.
Teachers can also record interactions with ob-
jects (toys, musical instruments, etc), by using
special swipe cards associated with these objects.
Last but not least, teachers can record spoken
messages about what happened during the day.
An example of how the child’s wheelchair is set
up is shown in Figure 2.
</bodyText>
<figureCaption confidence="0.997276">
Figure 2: System configuration
</figureCaption>
<bodyText confidence="0.975258704545455">
The data analysis module combines sensor-
derived location and interaction data with a time-
table which records what the child was expected
to do during the day, and a domain knowledge
base which includes information about typical
activities (e.g., if the child’s location is Swim-
mingPool, the child’s activity is probably
Swimming). From this it creates a series of
events (each of which contain a number of mes-
sages) which describe the child’s lessons and
activities, including divergences from what is
expected in the timetable. Several messages may
be associated with an event. The data analysis
module also infers which events and messages it
believes are most interesting to the child; this is
partially based on heuristics about what children
are interested in (e.g., swimming is more inter-
esting than lunch), and partially based on the
general principle that unexpected things (diver-
gences from the timetable) are more interesting
than expected things. No more than five events
are flagged as interesting, and only these events
are shown in the editing interface.
The editing interface allows children to re-
move events they do not want to talk about (per-
haps for privacy reasons) from the list of interest-
ing events. It also allows children to add mes-
sages that express simple opinions about events;
i.e., I liked it or I didn’t like it. The interface is
designed to be used with a scanning interface,
and is based on symbols that represent events,
annotations, etc.
The narration interface, shown in Figure 3, is
similar to the editing interface. It allows children
to choose a specific event to communicate,
which must be one of the ones they selected dur-
ing the editing phase. Children are encouraged
to tell events in temporal order (this is one of the
narration skills we are trying to teach), but this is
not mandated, and they can deviate from tempo-
ral order if they wish.
Events
Messages
for event
</bodyText>
<subsectionHeader confidence="0.417928">
Opinion Annotations
</subsectionHeader>
<figureCaption confidence="0.998394">
Figure 3: Narration Interface
</figureCaption>
<bodyText confidence="0.9538575">
The NLG system generates actual texts from
the events selected by the children. Most of this
</bodyText>
<table confidence="0.725172714285714">
Tablet PC with NLG system and
swipe-card RFID sensor
long range
RFID
sensor for
location
tracking
</table>
<page confidence="0.98744">
4
</page>
<bodyText confidence="0.9998495">
is fairly simple, since the system deliberately
uses simple “child-like” language (Section 6).
However, the system does need to make some
decisions based on discourse context, including
choosing appropriate referring expressions (es-
pecially pronouns), and temporal expressions
(especially when children deviate from pure
temporal order).
</bodyText>
<subsectionHeader confidence="0.932508">
4.1 Example
</subsectionHeader>
<bodyText confidence="0.9619595">
For example, assume that the timetable speci-
fies the following information
</bodyText>
<table confidence="0.783167333333333">
Time Activity Location Teacher
...... ...... ......
13.20 -14 ..... CL_SEC2 Mrs Smith
Arts and
Crafts
14 -14.40 Physiotherapy PHYSIO1 Mrs Jones
</table>
<bodyText confidence="0.324304">
Assume that the sensors then recorded the fol-
lowing information
</bodyText>
<figure confidence="0.988956125">
Event 1
Location: CL_SEC2
Time: 13:23:00.0 - 14:07:00.0
Interactions: Mrs. Smith, Rolf, Ross
Event 2
Location: HALL
Time: 14:10:00.0 – 14:39:00.0
Interactions: none
</figure>
<bodyText confidence="0.9996748">
The data analysis module associates Event 1 with
the Arts and Crafts timetable entry, since the lo-
cation is right, the timetabled teacher is present,
and the times approximately match. From this
two messages are produced: one corresponding
to I had Arts and Crafts this afternoon with Mrs.
Smith (the core activity description), and the oth-
er corresponding to Rolf and Ross were there
(additional information about people not time-
tabled to be there). The child can add opinions
using the editing interface; for example, if he
added a positive annotation to the event, this
would become an additional message corre-
sponding to It was great.
For Event 2, the data analysis module notes
that it does not match a timetabled event. The
timetable indicates the child should be at Physio-
therapy after Art and Crafts; however, the sensor
information indicates they were in the hall. The
system generates a single message corresponding
to Then I went to the Hall instead of Physiother-
apy to describe this event. If the child added a
negative annotation to this message, this would
become an additional message expressed as I
didn’t like it.
</bodyText>
<subsectionHeader confidence="0.945866">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999992863636364">
We conducted an initial evaluation of the How
was School Today system in January, 2009.
Two children used the system for four days: Ju-
lie, age 11, who had good cognitive skills but
was non-verbal because of severe motor impair-
ments; and Jessica, age 13, who had less severe
motor impairments but who had some cognitive
and memory impairments (these are not the chil-
drens’ real names). Julie used the system as a
communication and interaction aid, as described
above; Jessica used the system partially as a
memory aid. The evaluation was primarily
qualitative: we observed how Julie and Jessica
used the system, and interviewed their teachers,
speech therapists, care assistants, and Julie’s
mother (Jessica’s parents were not available).
The system worked very well for Julie; she
learned it quickly, and was able to use it to have
real conversations about her day with adults, al-
most for the first time in her life. This validated
our vision that our technology could help AAC
users engage in real interaction, and go beyond
simple question answering and communication
of basic needs. The system also worked rea-
sonably well as a memory aid for Jessica, but she
had a harder time using it, perhaps because of her
cognitive impairments.
Staff and Julie’s mother were very supportive
and pleased with the system. They had sugges-
tions for improving the system, including a wider
range of annotations; more phrases about the
conversation itself, such as Guess what happened
at school today; and allowing children to request
teenager language (e.g., really cool).
From a technical perspective, the system
worked well overall. School staff were happy to
use the swipe cards, which worked well. There
were some problems with the location sensors,
we need better techniques for distinguishing real
readings from noise. A surprising amount of
effort was needed to enter up-to-date knowledge
(e.g., daily lunch menus), this would need to be
addressed if the system was used for a period of
months as opposed to days.
</bodyText>
<sectionHeader confidence="0.995642" genericHeader="method">
5 Social Conversation for Adults
</sectionHeader>
<bodyText confidence="0.999986714285714">
In our second project, we want to build a tool to
help adults with cerebral palsy engage in social
conversation about a football match, movie,
weather, and so forth. Many people with severe
disabilities have great difficulty developing new
interpersonal relationships, and indeed report that
forming new relationships and taking part in new
</bodyText>
<page confidence="0.985002">
5
</page>
<bodyText confidence="0.999957773195877">
activities are major priorities in their lives (Datil-
lo et al., 2007). Supporting these goals through
the development of appropriate technologies is
important as it could lead to improved social out-
comes.
This project builds on the TALK system
(Todman and Alm, 2003), which helped AAC
users engage in active social conversation.
TALK partially overcame the problem of low
communication rate by requiring users to pre-
author their conversational material ahead of
time, so that when it was needed it could simply
be selected and output. TALK also used insights
from Conversation Analysis (Sacks, 1995) to
provide appropriate functionality in the system
for social conversation. For example, it sup-
ported opening and closing statements, stepwise
topic change, and the use of quick-fire utterances
to provide fast, idiomatic responses to commonly
encountered situations. This approach led to
more dynamic AAC-facilitated interactions with
higher communication rates, and had a positive
impact on the perceived communicative compe-
tence of the user (Todman, Alm et al., 2007).
TALK requires the user to spend a substantial
amount of time pre-authoring material; this is
perhaps its greatest weakness. Our idea is to re-
duce the amount of pre-authoring needed, by us-
ing the architecture shown in Fig 1, where much
of the material is automatically created from data
sources, ontologies, etc, and the user’s role is
largely to edit and annotate this material, not to
create it from scratch.
We developed an initial prototype system to
demonstrate this concept in the domain of foot-
ball results (Dempster, 2008). We are now
working on another prototype, whose goal is to
support social conversations about movies, mu-
sic, television shows, etc (which is a much
broader domain than football). We have created
an ontology which can describe events such as
watching a film, listening to a music track, or
reading a book. Each ‘event’ has both temporal
and spatial properties which allow descriptions to
be produced about where and when an event took
place, and other particulars relating to that par-
ticular class of event. For example, if the user
listened to a radio show, we record the name of
the show, the presenter and the station it was
broadcast on. Ultimately we plan to obtain in-
formation about movies, music tracks, etc from
web-based databases such as IMDB (movies)
and last.fm (music).
Of course, databases such as IMDB do not
contain information such as what the user
thought of the movie, or who he saw it with.
Hence we will allow users to add annotations
with such information. Some of these annota-
tions will be entered via a structured tool, such as
a calendar interface that allows users to specify
when they watched or listened to something. We
would like to use NaturalOWL (Galanis and An-
droutsopoulos, 2007) as the NLG component of
the system; it is well suited to describing objects,
and is intended to be integrated with an ontology.
As with the How Was School Today project,
some of the main low-level NLG challenges are
choosing appropriate referring expressions and
temporal references, based on the current dis-
course context. Speech output is done using Ce-
reproc (Aylett and Pidcock, 2007).
An example of our current narration interface
is shown in Figure 4. In the editing interface, the
user has specified that he went to a concert at
8pm on Thursday, and that he rated it 8 out of
10. The narration interface gives the user a
choice of a number of messages based on this
information, together with some standard mes-
sages such as Thanks and Agree.
Note that unlike the How Was School Today
project, in this project we do not attempt to infer
event information from sensors, but we allow
(and expect) the user to enter much more infor-
mation at the editing stage. We could in princi-
ple use sensors to pick up some information,
such as the fact that the user was in the cinema
from 12 to 2PM on Tuesday, but this is not the
research focus of this project.
We plan to evaluate the system using groups
of both disabled and non-disabled users. This
has been shown in the past to be an effective ap-
proach for the evaluation of prototype AAC sys-
tems (Higginbotham, 1995). Initially pairs of
non-disabled participants will be asked to pro-
duce short conversations with one person using
the prototype and the other conversing normally.
Quantitative measures of the communication rate
</bodyText>
<page confidence="0.998561">
6
</page>
<bodyText confidence="0.999911">
will be taken as well as more qualitative observa-
tions relating to the usability of the system. Af-
ter this evaluation we will improve the system
based on our findings, and then conduct a final
evaluation with a small group of AAC users.
</bodyText>
<sectionHeader confidence="0.964617" genericHeader="method">
6 Discussion: Challenges for NLG
</sectionHeader>
<bodyText confidence="0.999941205479453">
From an NLG perspective, generating AAC texts
of the sort we describe here presents different
challenges from many other NLG applications.
First of all, realization and even microplanning
are probably not difficult, because in this context
the AAC system should generate short simple
sentences if possible. This is because the system
is speaking “for” someone with limited or devel-
oping linguistic abilities, and it should try to pro-
duce something similar to what the user would
say himself if he or she had the time to explicitly
write a text using an on-screen keyboard.
To take a concrete example, we had originally
considered using past-perfect tense (a fairly
complex linguistic construct) in the How was
School project, when the narrative jumped to an
earlier point in time. For example I ate lunch at
12. I had gone swimming at 11. But it was clear
from corpora of child-written texts that these
children never used perfect tenses, so instead we
opted for I ate lunch at 12. I went swimming at
11. This is less linguistically polished, but much
more in line with what the children might actu-
ally produce.
Given this desire for linguistic simplicity, re-
alisation is very simple, as is lexical choice (use
simple words) and aggregation (keep sentences
short). The main microplanning challenges re-
late to discourse coherence, in particular refer-
ring expressions and temporal descriptions.
On the other hand, there are major challenges
in document planning. In particular, in the How
Was School project, we want the output to be a
proper narrative, in the sense of Labov (1972).
That is, not just a list of facts and events, but a
structure with a beginning and end, and with ex-
planatory and other links between components
(e.g., I had math in the afternoon because we
went swimming in the morning, if the child nor-
mally has math in the morning). We also wanted
the narrative to be interesting and hold the inter-
est of the person the child is communicating
with. As pointed out by Reiter et al (2008), cur-
rent NLG systems do not do a good job of gener-
ating narratives.
Similarly, in the Social Conversations project
we want the system to generate a social dialogue,
not just a list of facts about movies and songs.
Little previous research has been done on gener-
ating social (as opposed to task-oriented) dia-
logues. One exception is the NECA Socialite
system (van Deemter et al, 2008), but this fo-
cused on techniques for expressing affect, not on
high-level conversational structure.
For both stories and social conversations, it
would be extremely useful to be able to monitor
what the conversational partner is saying. This is
something we hope to investigate in the future.
As most AAC users interact with a small number
of conversational partners, it may be feasible to
use a speech dictation system to detect at least
some of what the conversational partner says.
Last but not least, a major challenge implicit
in our systems and indeed in the general architec-
ture is letting users control the NLG system.
Our systems are intended to be speaking aids,
ideally they should produce the same utterances
as the user would if he was able to talk. This
means that users must be able to control the sys-
tems, so that it does what they want it to do, in
terms of both content and expression. To the
best of our knowledge, little is known about how
users can best control an NLG system.
</bodyText>
<sectionHeader confidence="0.998375" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999955466666667">
Many people are in the unfortunate position of
not being able to speak or type, due to cognitive
and/or motor impairments. Current AAC tools
allow such people to engage in simple needs-
based communication, but they do not provide
good support for richer use of language, such as
story-telling and social conversation. We are
trying to develop more sophisticated AAC tools
which support such interactions, by using exter-
nal data and knowledge sources to produce can-
didate messages, which can be expressed using
NLG and speech synthesis technology. Our
work is still at an early stage, but we believe that
it has the potential to help AAC users engage in
richer interactions with other people.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.960603">
We are very grateful to Julie, Jessica, and their
teachers, therapists, carers, and parents for their
help in building and evaluating the system de-
scribed in Section 4. Many thanks to the anony-
mous referees and our colleagues at Aberdeen
and Dundee for their very helpful comments.
This research is supported by EPSRC grants
EP/F067151/1 and EP/F066880/1, and by a
Northern Research Partnership studentship.
</bodyText>
<page confidence="0.999327">
7
</page>
<sectionHeader confidence="0.990307" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913431372549">
Aylett, M. and C. Pidcock (2007). The CereVoice
Characterful Speech Synthesiser SDK. Proceed-
ings of Proceedings of the 7th International Con-
ference on Intelligent Virtual Agents, pages 413-
414.
Bruner, J. (1975). From communication to language:
A psychological perspective. Cognition 3: 255-
289.
Datillo, J., G. Estrella, L. Estrella, J. Light, D.
McNaughton and M. Seabury (2007). &amp;quot;I have cho-
sen to live life abundantly&amp;quot;: Perceptions of leisure
by adults who use Augmentative and Alternative
Communication. Augmentative &amp; Alternative
Communication 24(1): 16-28.
van Deemter, K., B Krenn, P Piwek, M Klesen, M
Schröder and S Baumann. Fully generated scripted
dialogue for embodied agents. Artificial Intelli-
gence 172: 1219–1244.
Dempster, M. (2008). Using natural language genera-
tion to encourage effective communication in non-
speaking people. Proceedings of Young Research-
ers Consortium, ICCHP&apos;08.
Dye, R., N. Alm, J. Arnott, G. Harper, and A. Morri-
son (1998). A script-based AAC system for trans-
actional interaction. Natural Language Engineer-
ing, 4(1), 57-71.
Galanis, D. and I. Androutsopoulos (2007). Generat-
ing Multilingual Descriptions from Linguistically
Annotated OWL Ontologies: the NaturalOWL Sys-
tem. Proceedings of ENLG 2007.
Higginbotham, D. J. (1995). Use of nondisabled sub-
jects in AAC Research : Confessions of a research
infidel. Augmentative and Alternative Communica-
tion 11(1): 2-5.
Labov, W (1972). Language in the Inner City. Uni-
versity of Pennsylvania Press.
Manurung, R., G. Ritchie, H. Pain, A. Waller, D.
O&apos;Mara and R. Black (2008). The Construction of a
Pun Generator for Language Skills Development.
Applied Artificial Intelligence 22(9): 841 – 869.
McCoy, K., C. Pennington and A. Badman (1998).
Compansion: From research prototype to practical
integration. Natural Language Engineering 4:73-
95.
Netzer, Y and Elhadad, M (2006). Using Semantic
Authoring for Blissymbols Communication
Boards. In Proc of HLT-2006.
Newell, A., S. Langer and M. Hickey (1998). The role
of natural language processing in alternative and
augmentative communication. Natural Language
Engineering 4:1-16.
Polkinghorne, D. (1991). Narrative and self-concept.
Journal of Narrative and Life History, 1(2/3), 135-
153
Reiter, E (2007). An Architecture for Data-to-Text
Systems. In Proceedings of ENLG-2007, pages
147-155.
Reiter, E. and R. Dale (2000). Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
Reiter, E, A. Gatt, F Portet, and M van der Meulen
(2008). The Importance of Narrative and Other
Lessons from an Evaluation of an NLG System
that Summarises Clinical Data (2007). In Proceed-
ings of INLG-2008, pages 97-104.
Sacks, H. (1995). Lectures on Conversation. G. Jef-
ferson. Cambridge, MA, Blackwell.
Schank, R. C. (1990). Tell me a story: A new look at
real and artificial intelligence. New York, Macmil-
lan Publishing Co.
Schank, R., and R. Abelson (1977). Scripts, plans,
goals, and understanding. New Jersey: Lawrence
Erlbaum.
Soto, G., E. Hartmann, and D. Wilkins (2006). Ex-
ploring the Elements of Narrative that Emerge in
the Interactions between an 8-Year-Old Child who
uses an AAC Device and her Teacher. Augmenta-
tive and Alternative Communication 4:231 – 241.
Todman, J. and N. A. Alm (2003). Modelling conver-
sational pragmatics in communication aids. Jour-
nal of Pragmatics 35: 523-538.
Todman, J., N. A. Alm, D. J. Higginbotham and P.
File (2007). Whole Utterance Approaches in AAC.
Augmentative and Alternative Communication
24(3): 235-254.
von Tetzchner, S. and N. Grove (2003). The devel-
opment of alternative language forms. In S. von
Tetzchner and N. Grove (eds), Augmentative and
Alternative Communication: Developmental Issues,
pages 1-27. Wiley.
Waller, A. (2006). Communication Access to Conver-
sational Narrative. Topics in Language Disorders
26(3): 221-239.
Waller, A. (2008). Narrative-based Augmentative and
Alternative Communication: From transactional to
interactional conversation. Proceedings of ISAAC
2008, pages 149-160.
Waller, A., R. Black, D. A. O&apos;Mara, H. Pain, G. Rit-
chie and R. Manurung (In Press). Evaluating the
STANDUP Pun Generating Software with Chil-
dren with Cerebral Palsy. ACM Transactions on
Accessible Computing.
</reference>
<page confidence="0.998491">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.610453">
<title confidence="0.9986145">Using NLG to Help Language-Impaired Users Tell Stories Participate in Social Dialogues</title>
<author confidence="0.989701">Ehud Reiter</author>
<author confidence="0.989701">Ross</author>
<affiliation confidence="0.999884">University of</affiliation>
<address confidence="0.922638">Aberdeen, UK</address>
<email confidence="0.9703805">e.reiter@abdn.ac.ukcsc272@abdn.ac.uk</email>
<author confidence="0.9712295">Norman Alm</author>
<author confidence="0.9712295">Rolf Martin Dempster</author>
<author confidence="0.9712295">Annalu</author>
<affiliation confidence="0.999763">University of</affiliation>
<address confidence="0.813412">Dundee, UK</address>
<email confidence="0.945407">nalm@computing.dundee.ac.uk</email>
<email confidence="0.945407">rolfblack@computing.dundee.ac.uk</email>
<email confidence="0.945407">martindempster@computing.dundee.ac.uk</email>
<email confidence="0.945407">awaller@computing.dundee.ac.uk</email>
<abstract confidence="0.997833307692308">Augmentative and Alternative Communication (AAC) systems are communication aids for people who cannot speak because of motor or cognitive impairments. We are developing AAC systems where users select information they wish to communicate, and this is expressed using an NLG system. We believe this model will work well in contexts where AAC users wish to go beyond simply making requests or answering questions, and have more complex communicative goals such as story-telling and social interaction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Aylett</author>
<author>C Pidcock</author>
</authors>
<title>The CereVoice Characterful Speech Synthesiser SDK.</title>
<date>2007</date>
<booktitle>Proceedings of Proceedings of the 7th International Conference on Intelligent Virtual Agents,</booktitle>
<pages>413--414</pages>
<contexts>
<context position="22896" citStr="Aylett and Pidcock, 2007" startWordPosition="3700" endWordPosition="3703">n. Some of these annotations will be entered via a structured tool, such as a calendar interface that allows users to specify when they watched or listened to something. We would like to use NaturalOWL (Galanis and Androutsopoulos, 2007) as the NLG component of the system; it is well suited to describing objects, and is intended to be integrated with an ontology. As with the How Was School Today project, some of the main low-level NLG challenges are choosing appropriate referring expressions and temporal references, based on the current discourse context. Speech output is done using Cereproc (Aylett and Pidcock, 2007). An example of our current narration interface is shown in Figure 4. In the editing interface, the user has specified that he went to a concert at 8pm on Thursday, and that he rated it 8 out of 10. The narration interface gives the user a choice of a number of messages based on this information, together with some standard messages such as Thanks and Agree. Note that unlike the How Was School Today project, in this project we do not attempt to infer event information from sensors, but we allow (and expect) the user to enter much more information at the editing stage. We could in principle use</context>
</contexts>
<marker>Aylett, Pidcock, 2007</marker>
<rawString>Aylett, M. and C. Pidcock (2007). The CereVoice Characterful Speech Synthesiser SDK. Proceedings of Proceedings of the 7th International Conference on Intelligent Virtual Agents, pages 413-414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bruner</author>
</authors>
<title>From communication to language: A psychological perspective.</title>
<date>1975</date>
<journal>Cognition</journal>
<volume>3</volume>
<pages>255--289</pages>
<contexts>
<context position="11402" citStr="Bruner, 1975" startWordPosition="1837" endWordPosition="1838"> similar to Labov’s (1972) conversational narrative, i.e., a series of linked real-world events which are unusual or otherwise interesting, possibly annotated with information about the child’s feelings, which can be narrated orally. We are not expecting stories in the literary sense, with character development and complex plots. The motivation of the project is to provide the children with successful narrative experience. Typically developing children develop narrative skills from an early age with adults scaffolding conversations to elicit narrative, e.g. “What did you do at school today?” (Bruner, 1975). As the child’s vocabulary and language competence develops, scaffolding is reduced. This progression is seldom seen in children with complex communication needs – they respond to closed questions but seldom take control of conversaUser Figure 1: General architecture Sensor data Data analysis: select possible messages to communicate Narration: User selects what to say Editing: User adds annotations NLG: Generate utterance Speech synthesis Web info sources Other external data Prepare content Narrate content Dialogue model Conversation partner Conversation model Domain model User model 3 tion (</context>
</contexts>
<marker>Bruner, 1975</marker>
<rawString>Bruner, J. (1975). From communication to language: A psychological perspective. Cognition 3: 255-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Datillo</author>
<author>G Estrella</author>
<author>L Estrella</author>
<author>J Light</author>
<author>D McNaughton</author>
<author>M Seabury</author>
</authors>
<title>I have chosen to live life abundantly&amp;quot;: Perceptions of leisure by adults who use Augmentative and Alternative Communication.</title>
<date>2007</date>
<journal>Augmentative &amp; Alternative Communication</journal>
<volume>24</volume>
<issue>1</issue>
<pages>16--28</pages>
<contexts>
<context position="19764" citStr="Datillo et al., 2007" startWordPosition="3186" endWordPosition="3190">of effort was needed to enter up-to-date knowledge (e.g., daily lunch menus), this would need to be addressed if the system was used for a period of months as opposed to days. 5 Social Conversation for Adults In our second project, we want to build a tool to help adults with cerebral palsy engage in social conversation about a football match, movie, weather, and so forth. Many people with severe disabilities have great difficulty developing new interpersonal relationships, and indeed report that forming new relationships and taking part in new 5 activities are major priorities in their lives (Datillo et al., 2007). Supporting these goals through the development of appropriate technologies is important as it could lead to improved social outcomes. This project builds on the TALK system (Todman and Alm, 2003), which helped AAC users engage in active social conversation. TALK partially overcame the problem of low communication rate by requiring users to preauthor their conversational material ahead of time, so that when it was needed it could simply be selected and output. TALK also used insights from Conversation Analysis (Sacks, 1995) to provide appropriate functionality in the system for social convers</context>
</contexts>
<marker>Datillo, Estrella, Estrella, Light, McNaughton, Seabury, 2007</marker>
<rawString>Datillo, J., G. Estrella, L. Estrella, J. Light, D. McNaughton and M. Seabury (2007). &amp;quot;I have chosen to live life abundantly&amp;quot;: Perceptions of leisure by adults who use Augmentative and Alternative Communication. Augmentative &amp; Alternative Communication 24(1): 16-28.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K van Deemter</author>
<author>B Krenn</author>
<author>P Piwek</author>
<author>M Klesen</author>
<author>M Schröder</author>
<author>S Baumann</author>
</authors>
<title>Fully generated scripted dialogue for embodied agents.</title>
<journal>Artificial Intelligence</journal>
<volume>172</volume>
<pages>1219--1244</pages>
<marker>van Deemter, Krenn, Piwek, Klesen, Schröder, Baumann, </marker>
<rawString>van Deemter, K., B Krenn, P Piwek, M Klesen, M Schröder and S Baumann. Fully generated scripted dialogue for embodied agents. Artificial Intelligence 172: 1219–1244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dempster</author>
</authors>
<title>Using natural language generation to encourage effective communication in nonspeaking people.</title>
<date>2008</date>
<booktitle>Proceedings of Young Researchers Consortium, ICCHP&apos;08.</booktitle>
<contexts>
<context position="21295" citStr="Dempster, 2008" startWordPosition="3429" endWordPosition="3430">act on the perceived communicative competence of the user (Todman, Alm et al., 2007). TALK requires the user to spend a substantial amount of time pre-authoring material; this is perhaps its greatest weakness. Our idea is to reduce the amount of pre-authoring needed, by using the architecture shown in Fig 1, where much of the material is automatically created from data sources, ontologies, etc, and the user’s role is largely to edit and annotate this material, not to create it from scratch. We developed an initial prototype system to demonstrate this concept in the domain of football results (Dempster, 2008). We are now working on another prototype, whose goal is to support social conversations about movies, music, television shows, etc (which is a much broader domain than football). We have created an ontology which can describe events such as watching a film, listening to a music track, or reading a book. Each ‘event’ has both temporal and spatial properties which allow descriptions to be produced about where and when an event took place, and other particulars relating to that particular class of event. For example, if the user listened to a radio show, we record the name of the show, the prese</context>
</contexts>
<marker>Dempster, 2008</marker>
<rawString>Dempster, M. (2008). Using natural language generation to encourage effective communication in nonspeaking people. Proceedings of Young Researchers Consortium, ICCHP&apos;08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dye</author>
<author>N Alm</author>
<author>J Arnott</author>
<author>G Harper</author>
<author>A Morrison</author>
</authors>
<title>A script-based AAC system for transactional interaction.</title>
<date>1998</date>
<journal>Natural Language Engineering,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>57--71</pages>
<contexts>
<context position="5120" citStr="Dye et al (1998)" startWordPosition="804" endWordPosition="807">ith a library of fixed “conversational moves” which can be selected and uttered. These moves are based on models of the usual shape and content of conversational encounters (Todman &amp; Alm, 2003), and for example include standard conversational openings and closings, such as Hello and How are you. They also include backchannel communication such as Uh-huh, Great!, and Sorry, can you repeat that. It would be very useful to go beyond standard openings, closings, and backchannel messages, and allow the user to select utterances which were relevant to the particular communicative context and goals. Dye et al (1998) developed a system based on scripts of common interactions (Schank &amp; Abelson, 1977). For example, a user could activate the MakeAnAppointment script, and then could select utterances relevant to this script, such as I would like to make an appointment to see the doctor. As the interaction progressed, the system would update the selections offered to the user based on the current stage of the script; for example during time negotiation a possible utterance would be I would like to see him next week. This system proved effective in trials, but needed a large number of scripts to be generally ef</context>
</contexts>
<marker>Dye, Alm, Arnott, Harper, Morrison, 1998</marker>
<rawString>Dye, R., N. Alm, J. Arnott, G. Harper, and A. Morrison (1998). A script-based AAC system for transactional interaction. Natural Language Engineering, 4(1), 57-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Galanis</author>
<author>I Androutsopoulos</author>
</authors>
<date>2007</date>
<booktitle>Generating Multilingual Descriptions from Linguistically Annotated OWL Ontologies: the NaturalOWL System. Proceedings of ENLG</booktitle>
<contexts>
<context position="22508" citStr="Galanis and Androutsopoulos, 2007" startWordPosition="3635" endWordPosition="3639">f the show, the presenter and the station it was broadcast on. Ultimately we plan to obtain information about movies, music tracks, etc from web-based databases such as IMDB (movies) and last.fm (music). Of course, databases such as IMDB do not contain information such as what the user thought of the movie, or who he saw it with. Hence we will allow users to add annotations with such information. Some of these annotations will be entered via a structured tool, such as a calendar interface that allows users to specify when they watched or listened to something. We would like to use NaturalOWL (Galanis and Androutsopoulos, 2007) as the NLG component of the system; it is well suited to describing objects, and is intended to be integrated with an ontology. As with the How Was School Today project, some of the main low-level NLG challenges are choosing appropriate referring expressions and temporal references, based on the current discourse context. Speech output is done using Cereproc (Aylett and Pidcock, 2007). An example of our current narration interface is shown in Figure 4. In the editing interface, the user has specified that he went to a concert at 8pm on Thursday, and that he rated it 8 out of 10. The narration</context>
</contexts>
<marker>Galanis, Androutsopoulos, 2007</marker>
<rawString>Galanis, D. and I. Androutsopoulos (2007). Generating Multilingual Descriptions from Linguistically Annotated OWL Ontologies: the NaturalOWL System. Proceedings of ENLG 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Higginbotham</author>
</authors>
<title>Use of nondisabled subjects in AAC Research : Confessions of a research infidel.</title>
<date>1995</date>
<journal>Augmentative and Alternative Communication</journal>
<volume>11</volume>
<issue>1</issue>
<pages>2--5</pages>
<contexts>
<context position="23871" citStr="Higginbotham, 1995" startWordPosition="3884" endWordPosition="3885">ree. Note that unlike the How Was School Today project, in this project we do not attempt to infer event information from sensors, but we allow (and expect) the user to enter much more information at the editing stage. We could in principle use sensors to pick up some information, such as the fact that the user was in the cinema from 12 to 2PM on Tuesday, but this is not the research focus of this project. We plan to evaluate the system using groups of both disabled and non-disabled users. This has been shown in the past to be an effective approach for the evaluation of prototype AAC systems (Higginbotham, 1995). Initially pairs of non-disabled participants will be asked to produce short conversations with one person using the prototype and the other conversing normally. Quantitative measures of the communication rate 6 will be taken as well as more qualitative observations relating to the usability of the system. After this evaluation we will improve the system based on our findings, and then conduct a final evaluation with a small group of AAC users. 6 Discussion: Challenges for NLG From an NLG perspective, generating AAC texts of the sort we describe here presents different challenges from many ot</context>
</contexts>
<marker>Higginbotham, 1995</marker>
<rawString>Higginbotham, D. J. (1995). Use of nondisabled subjects in AAC Research : Confessions of a research infidel. Augmentative and Alternative Communication 11(1): 2-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Labov</author>
</authors>
<title>Language in the Inner City.</title>
<date>1972</date>
<publisher>University of Pennsylvania Press.</publisher>
<contexts>
<context position="25934" citStr="Labov (1972)" startWordPosition="4226" endWordPosition="4227"> lunch at 12. I went swimming at 11. This is less linguistically polished, but much more in line with what the children might actually produce. Given this desire for linguistic simplicity, realisation is very simple, as is lexical choice (use simple words) and aggregation (keep sentences short). The main microplanning challenges relate to discourse coherence, in particular referring expressions and temporal descriptions. On the other hand, there are major challenges in document planning. In particular, in the How Was School project, we want the output to be a proper narrative, in the sense of Labov (1972). That is, not just a list of facts and events, but a structure with a beginning and end, and with explanatory and other links between components (e.g., I had math in the afternoon because we went swimming in the morning, if the child normally has math in the morning). We also wanted the narrative to be interesting and hold the interest of the person the child is communicating with. As pointed out by Reiter et al (2008), current NLG systems do not do a good job of generating narratives. Similarly, in the Social Conversations project we want the system to generate a social dialogue, not just a </context>
</contexts>
<marker>Labov, 1972</marker>
<rawString>Labov, W (1972). Language in the Inner City. University of Pennsylvania Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Manurung</author>
<author>G Ritchie</author>
<author>H Pain</author>
<author>A Waller</author>
<author>D O&apos;Mara</author>
<author>R Black</author>
</authors>
<title>The Construction of a Pun Generator for Language Skills Development.</title>
<date>2008</date>
<journal>Applied Artificial Intelligence</journal>
<volume>22</volume>
<issue>9</issue>
<pages>841--869</pages>
<marker>Manurung, Ritchie, Pain, Waller, O&apos;Mara, Black, 2008</marker>
<rawString>Manurung, R., G. Ritchie, H. Pain, A. Waller, D. O&apos;Mara and R. Black (2008). The Construction of a Pun Generator for Language Skills Development. Applied Artificial Intelligence 22(9): 841 – 869.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McCoy</author>
<author>C Pennington</author>
<author>A Badman</author>
</authors>
<title>Compansion: From research prototype to practical integration.</title>
<date>1998</date>
<journal>Natural Language Engineering</journal>
<pages>4--73</pages>
<marker>McCoy, Pennington, Badman, 1998</marker>
<rawString>McCoy, K., C. Pennington and A. Badman (1998). Compansion: From research prototype to practical integration. Natural Language Engineering 4:73-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Netzer</author>
<author>M Elhadad</author>
</authors>
<title>Using Semantic Authoring for Blissymbols Communication Boards.</title>
<date>2006</date>
<booktitle>In Proc of HLT-2006.</booktitle>
<contexts>
<context position="7120" citStr="Netzer and Elhadad (2006)" startWordPosition="1143" endWordPosition="1146">brary of fixed texts and templates. 2.2 NLG and AAC Natural language generation (NLG) systems generate texts in English and other human languages from non-linguistic input (Reiter and Dale, 2000). In their review of NLP and AAC, Newell, Langer, and Hickey (1998) suggest that NLG could be used to generate complete utterances from the limited input that AAC users are able to provide. For example, the Compansion project (McCoy, Pennington, Badman 1998) used NLP and NLG techniques to expand telegraphic user input, such as Mary go store?, into complete utterances, such as Did Mary go to the store? Netzer and Elhadad (2006) allowed users to author utterances in the symbolic language BLISS, and used NLG to translate this to English and Hebrew texts. In recent years there has been growing interest in data-to-text NLG systems (Reiter, 2007); these systems generate texts based on sensor and other numerical data, supplemented with ontologies that specify domain knowledge. In principle, it seems that data-to-text techniques should allow NLG systems to provide more assistance than the syntactic help provided by Compansion. For example, if the user wanted to talk about a recent football (soccer) match, a data-to-text sy</context>
</contexts>
<marker>Netzer, Elhadad, 2006</marker>
<rawString>Netzer, Y and Elhadad, M (2006). Using Semantic Authoring for Blissymbols Communication Boards. In Proc of HLT-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Newell</author>
<author>S Langer</author>
<author>M Hickey</author>
</authors>
<title>The role of natural language processing in alternative and augmentative communication.</title>
<date>1998</date>
<journal>Natural Language Engineering</journal>
<pages>4--1</pages>
<marker>Newell, Langer, Hickey, 1998</marker>
<rawString>Newell, A., S. Langer and M. Hickey (1998). The role of natural language processing in alternative and augmentative communication. Natural Language Engineering 4:1-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Polkinghorne</author>
</authors>
<title>Narrative and self-concept.</title>
<date>1991</date>
<journal>Journal of Narrative and Life History,</journal>
<volume>1</volume>
<issue>2</issue>
<pages>135--153</pages>
<contexts>
<context position="6250" citStr="Polkinghorne, 1991" startWordPosition="997" endWordPosition="998">ystem proved effective in trials, but needed a large number of scripts to be generally effective. Users could author their own texts, which were added to the scripts, but this was time-consuming and had to be done in advance of the conversation. Another goal of AAC is to help users narrate stories. Narrative and storytelling play a very important part in the communicative repertoire of all speakers (Schank, 1990). In particular, the ability to draw on episodes from one’s life history in current conversation is vital to maintaining a full impression of one’s personality in dealing with others (Polkinghorne, 1991). Story telling tools for AAC users have been developed, which include ways to introduce a story, tell it at the pace required (with diversions) and give feedback to comments from listeners (Waller, 2006); but again these tools are based on a library of fixed texts and templates. 2.2 NLG and AAC Natural language generation (NLG) systems generate texts in English and other human languages from non-linguistic input (Reiter and Dale, 2000). In their review of NLP and AAC, Newell, Langer, and Hickey (1998) suggest that NLG could be used to generate complete utterances from the limited input that A</context>
</contexts>
<marker>Polkinghorne, 1991</marker>
<rawString>Polkinghorne, D. (1991). Narrative and self-concept. Journal of Narrative and Life History, 1(2/3), 135-153</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
</authors>
<title>An Architecture for Data-to-Text Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of ENLG-2007,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="7338" citStr="Reiter, 2007" startWordPosition="1180" endWordPosition="1181">ewell, Langer, and Hickey (1998) suggest that NLG could be used to generate complete utterances from the limited input that AAC users are able to provide. For example, the Compansion project (McCoy, Pennington, Badman 1998) used NLP and NLG techniques to expand telegraphic user input, such as Mary go store?, into complete utterances, such as Did Mary go to the store? Netzer and Elhadad (2006) allowed users to author utterances in the symbolic language BLISS, and used NLG to translate this to English and Hebrew texts. In recent years there has been growing interest in data-to-text NLG systems (Reiter, 2007); these systems generate texts based on sensor and other numerical data, supplemented with ontologies that specify domain knowledge. In principle, it seems that data-to-text techniques should allow NLG systems to provide more assistance than the syntactic help provided by Compansion. For example, if the user wanted to talk about a recent football (soccer) match, a data-to-text system could get actual data about the match from the web, and generate potential utterances from this data, such as Arsenal beat Chelsea 2-1 and Van Persie scored two goals; the user could then select one of these to ut</context>
</contexts>
<marker>Reiter, 2007</marker>
<rawString>Reiter, E (2007). An Architecture for Data-to-Text Systems. In Proceedings of ENLG-2007, pages 147-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6690" citStr="Reiter and Dale, 2000" startWordPosition="1069" endWordPosition="1072">ability to draw on episodes from one’s life history in current conversation is vital to maintaining a full impression of one’s personality in dealing with others (Polkinghorne, 1991). Story telling tools for AAC users have been developed, which include ways to introduce a story, tell it at the pace required (with diversions) and give feedback to comments from listeners (Waller, 2006); but again these tools are based on a library of fixed texts and templates. 2.2 NLG and AAC Natural language generation (NLG) systems generate texts in English and other human languages from non-linguistic input (Reiter and Dale, 2000). In their review of NLP and AAC, Newell, Langer, and Hickey (1998) suggest that NLG could be used to generate complete utterances from the limited input that AAC users are able to provide. For example, the Compansion project (McCoy, Pennington, Badman 1998) used NLP and NLG techniques to expand telegraphic user input, such as Mary go store?, into complete utterances, such as Did Mary go to the store? Netzer and Elhadad (2006) allowed users to author utterances in the symbolic language BLISS, and used NLG to translate this to English and Hebrew texts. In recent years there has been growing int</context>
<context position="9207" citStr="Reiter and Dale (2000)" startWordPosition="1481" endWordPosition="1484">st measuring linguistic abilities after they used STANDUP for ten weeks. 2 3 Our Architecture Our goal is help AAC users engage in complex social interaction by using NLG and datato-text technology to create potential utterances and conversational contributions for the users. The general architecture is shown in Figure 1, and Sections 4 and 5 describe two systems based on this architecture. The system has the following components: Data analysis: read in data, from sensors, web information sources, databases, and so forth. This module analyses this data and identifies messages (in the sense of Reiter and Dale (2000)) that the user is likely to want to communicate; this analysis is partially based on domain, conversation, and user models, which may be represented as ontologies. Editing: allow the user to edit the messages. Editing ranges from adding simple annotations to specify opinions (e.g., add BAD to Arsenal beat Chelsea 2-1 if the user is a Chelsea fan), to using an on-screen keyboard to type free-text comments. Users can also delete messages, specify which messages they are most likely to want to utter, and create new messages. Editing is done before the actual conversation, so the user does not ha</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Reiter, E. and R. Dale (2000). Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>A Gatt</author>
<author>F Portet</author>
<author>M van der Meulen</author>
</authors>
<date>2008</date>
<booktitle>The Importance of Narrative and Other Lessons from an Evaluation of an NLG System that Summarises Clinical Data</booktitle>
<pages>97--104</pages>
<marker>Reiter, Gatt, Portet, van der Meulen, 2008</marker>
<rawString>Reiter, E, A. Gatt, F Portet, and M van der Meulen (2008). The Importance of Narrative and Other Lessons from an Evaluation of an NLG System that Summarises Clinical Data (2007). In Proceedings of INLG-2008, pages 97-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sacks</author>
</authors>
<date>1995</date>
<booktitle>Lectures on Conversation. G. Jefferson.</booktitle>
<location>Cambridge, MA, Blackwell.</location>
<contexts>
<context position="20294" citStr="Sacks, 1995" startWordPosition="3272" endWordPosition="3273">art in new 5 activities are major priorities in their lives (Datillo et al., 2007). Supporting these goals through the development of appropriate technologies is important as it could lead to improved social outcomes. This project builds on the TALK system (Todman and Alm, 2003), which helped AAC users engage in active social conversation. TALK partially overcame the problem of low communication rate by requiring users to preauthor their conversational material ahead of time, so that when it was needed it could simply be selected and output. TALK also used insights from Conversation Analysis (Sacks, 1995) to provide appropriate functionality in the system for social conversation. For example, it supported opening and closing statements, stepwise topic change, and the use of quick-fire utterances to provide fast, idiomatic responses to commonly encountered situations. This approach led to more dynamic AAC-facilitated interactions with higher communication rates, and had a positive impact on the perceived communicative competence of the user (Todman, Alm et al., 2007). TALK requires the user to spend a substantial amount of time pre-authoring material; this is perhaps its greatest weakness. Our </context>
</contexts>
<marker>Sacks, 1995</marker>
<rawString>Sacks, H. (1995). Lectures on Conversation. G. Jefferson. Cambridge, MA, Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>Tell me a story: A new look at real and artificial intelligence.</title>
<date>1990</date>
<publisher>Macmillan Publishing Co.</publisher>
<location>New York,</location>
<contexts>
<context position="6047" citStr="Schank, 1990" startWordPosition="963" endWordPosition="964">would update the selections offered to the user based on the current stage of the script; for example during time negotiation a possible utterance would be I would like to see him next week. This system proved effective in trials, but needed a large number of scripts to be generally effective. Users could author their own texts, which were added to the scripts, but this was time-consuming and had to be done in advance of the conversation. Another goal of AAC is to help users narrate stories. Narrative and storytelling play a very important part in the communicative repertoire of all speakers (Schank, 1990). In particular, the ability to draw on episodes from one’s life history in current conversation is vital to maintaining a full impression of one’s personality in dealing with others (Polkinghorne, 1991). Story telling tools for AAC users have been developed, which include ways to introduce a story, tell it at the pace required (with diversions) and give feedback to comments from listeners (Waller, 2006); but again these tools are based on a library of fixed texts and templates. 2.2 NLG and AAC Natural language generation (NLG) systems generate texts in English and other human languages from n</context>
</contexts>
<marker>Schank, 1990</marker>
<rawString>Schank, R. C. (1990). Tell me a story: A new look at real and artificial intelligence. New York, Macmillan Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schank</author>
<author>R Abelson</author>
</authors>
<title>Scripts, plans, goals, and understanding.</title>
<date>1977</date>
<location>New Jersey: Lawrence Erlbaum.</location>
<contexts>
<context position="5204" citStr="Schank &amp; Abelson, 1977" startWordPosition="817" endWordPosition="820">ed. These moves are based on models of the usual shape and content of conversational encounters (Todman &amp; Alm, 2003), and for example include standard conversational openings and closings, such as Hello and How are you. They also include backchannel communication such as Uh-huh, Great!, and Sorry, can you repeat that. It would be very useful to go beyond standard openings, closings, and backchannel messages, and allow the user to select utterances which were relevant to the particular communicative context and goals. Dye et al (1998) developed a system based on scripts of common interactions (Schank &amp; Abelson, 1977). For example, a user could activate the MakeAnAppointment script, and then could select utterances relevant to this script, such as I would like to make an appointment to see the doctor. As the interaction progressed, the system would update the selections offered to the user based on the current stage of the script; for example during time negotiation a possible utterance would be I would like to see him next week. This system proved effective in trials, but needed a large number of scripts to be generally effective. Users could author their own texts, which were added to the scripts, but th</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Schank, R., and R. Abelson (1977). Scripts, plans, goals, and understanding. New Jersey: Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Soto</author>
<author>E Hartmann</author>
<author>D Wilkins</author>
</authors>
<title>Exploring the Elements of Narrative that Emerge in the Interactions between an 8-Year-Old Child who uses an AAC Device and her Teacher. Augmentative and Alternative Communication 4:231</title>
<date>2006</date>
<pages>241</pages>
<contexts>
<context position="12113" citStr="Soto et al, 2006" startWordPosition="1941" endWordPosition="1944">gression is seldom seen in children with complex communication needs – they respond to closed questions but seldom take control of conversaUser Figure 1: General architecture Sensor data Data analysis: select possible messages to communicate Narration: User selects what to say Editing: User adds annotations NLG: Generate utterance Speech synthesis Web info sources Other external data Prepare content Narrate content Dialogue model Conversation partner Conversation model Domain model User model 3 tion (von Tetzchner and Grove, 2003). Many children who use AAC have very limited narrative skills (Soto et al, 2006). Research has shown that providing children who use AAC with successful narrative experiences by providing full narrative text can help the development of written and spoken narrative skills (Waller, 2008). The system follows the architecture described above. Input data comes from RFID sensors that track where the child went during the day; an RFID reader is mounted on the child’s wheelchair, and RFID tags are placed around the school, especially in doorways so we can monitor children entering and leaving rooms. Teachers have also been given RFID swipe cards which they can swipe against a rea</context>
</contexts>
<marker>Soto, Hartmann, Wilkins, 2006</marker>
<rawString>Soto, G., E. Hartmann, and D. Wilkins (2006). Exploring the Elements of Narrative that Emerge in the Interactions between an 8-Year-Old Child who uses an AAC Device and her Teacher. Augmentative and Alternative Communication 4:231 – 241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Todman</author>
<author>N A Alm</author>
</authors>
<title>Modelling conversational pragmatics in communication aids.</title>
<date>2003</date>
<journal>Journal of Pragmatics</journal>
<volume>35</volume>
<pages>523--538</pages>
<contexts>
<context position="19961" citStr="Todman and Alm, 2003" startWordPosition="3218" endWordPosition="3221"> for Adults In our second project, we want to build a tool to help adults with cerebral palsy engage in social conversation about a football match, movie, weather, and so forth. Many people with severe disabilities have great difficulty developing new interpersonal relationships, and indeed report that forming new relationships and taking part in new 5 activities are major priorities in their lives (Datillo et al., 2007). Supporting these goals through the development of appropriate technologies is important as it could lead to improved social outcomes. This project builds on the TALK system (Todman and Alm, 2003), which helped AAC users engage in active social conversation. TALK partially overcame the problem of low communication rate by requiring users to preauthor their conversational material ahead of time, so that when it was needed it could simply be selected and output. TALK also used insights from Conversation Analysis (Sacks, 1995) to provide appropriate functionality in the system for social conversation. For example, it supported opening and closing statements, stepwise topic change, and the use of quick-fire utterances to provide fast, idiomatic responses to commonly encountered situations.</context>
<context position="4697" citStr="Todman &amp; Alm, 2003" startWordPosition="737" endWordPosition="740">e of the items. But creating arbitrary messages with such an interface is extremely slow, even if word prediction is used; and in general such interfaces do not well support complex social interactions such as story telling (Waller, 2006). A number of research projects in AAC have developed prototype systems which attempt to facilitate this type of human-human interaction. At their most basic, these systems provide users with a library of fixed “conversational moves” which can be selected and uttered. These moves are based on models of the usual shape and content of conversational encounters (Todman &amp; Alm, 2003), and for example include standard conversational openings and closings, such as Hello and How are you. They also include backchannel communication such as Uh-huh, Great!, and Sorry, can you repeat that. It would be very useful to go beyond standard openings, closings, and backchannel messages, and allow the user to select utterances which were relevant to the particular communicative context and goals. Dye et al (1998) developed a system based on scripts of common interactions (Schank &amp; Abelson, 1977). For example, a user could activate the MakeAnAppointment script, and then could select utte</context>
</contexts>
<marker>Todman, Alm, 2003</marker>
<rawString>Todman, J. and N. A. Alm (2003). Modelling conversational pragmatics in communication aids. Journal of Pragmatics 35: 523-538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Todman</author>
<author>N A Alm</author>
<author>D J Higginbotham</author>
<author>P File</author>
</authors>
<title>Whole Utterance Approaches in AAC.</title>
<date>2007</date>
<journal>Augmentative and Alternative Communication</journal>
<volume>24</volume>
<issue>3</issue>
<pages>235--254</pages>
<marker>Todman, Alm, Higginbotham, File, 2007</marker>
<rawString>Todman, J., N. A. Alm, D. J. Higginbotham and P. File (2007). Whole Utterance Approaches in AAC. Augmentative and Alternative Communication 24(3): 235-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S von Tetzchner</author>
<author>N Grove</author>
</authors>
<title>The development of alternative language forms.</title>
<date>2003</date>
<booktitle>In S. von Tetzchner</booktitle>
<pages>1--27</pages>
<publisher>Wiley.</publisher>
<marker>von Tetzchner, Grove, 2003</marker>
<rawString>von Tetzchner, S. and N. Grove (2003). The development of alternative language forms. In S. von Tetzchner and N. Grove (eds), Augmentative and Alternative Communication: Developmental Issues, pages 1-27. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Waller</author>
</authors>
<title>Communication Access to Conversational Narrative.</title>
<date>2006</date>
<journal>Topics in Language Disorders</journal>
<volume>26</volume>
<issue>3</issue>
<pages>221--239</pages>
<contexts>
<context position="4316" citStr="Waller, 2006" startWordPosition="678" endWordPosition="679">9 Association for Computational Linguistics 1 tions to them, and then scans through these, briefly highlighting each option. When the desired option is highlighted, the child selects it by pressing a switch with her head. This is adequate for communicating basic needs (such as hunger or thirst); the computer can display a menu of possible needs, and the child can select one of the items. But creating arbitrary messages with such an interface is extremely slow, even if word prediction is used; and in general such interfaces do not well support complex social interactions such as story telling (Waller, 2006). A number of research projects in AAC have developed prototype systems which attempt to facilitate this type of human-human interaction. At their most basic, these systems provide users with a library of fixed “conversational moves” which can be selected and uttered. These moves are based on models of the usual shape and content of conversational encounters (Todman &amp; Alm, 2003), and for example include standard conversational openings and closings, such as Hello and How are you. They also include backchannel communication such as Uh-huh, Great!, and Sorry, can you repeat that. It would be ver</context>
<context position="6454" citStr="Waller, 2006" startWordPosition="1031" endWordPosition="1032">one in advance of the conversation. Another goal of AAC is to help users narrate stories. Narrative and storytelling play a very important part in the communicative repertoire of all speakers (Schank, 1990). In particular, the ability to draw on episodes from one’s life history in current conversation is vital to maintaining a full impression of one’s personality in dealing with others (Polkinghorne, 1991). Story telling tools for AAC users have been developed, which include ways to introduce a story, tell it at the pace required (with diversions) and give feedback to comments from listeners (Waller, 2006); but again these tools are based on a library of fixed texts and templates. 2.2 NLG and AAC Natural language generation (NLG) systems generate texts in English and other human languages from non-linguistic input (Reiter and Dale, 2000). In their review of NLP and AAC, Newell, Langer, and Hickey (1998) suggest that NLG could be used to generate complete utterances from the limited input that AAC users are able to provide. For example, the Compansion project (McCoy, Pennington, Badman 1998) used NLP and NLG techniques to expand telegraphic user input, such as Mary go store?, into complete utter</context>
</contexts>
<marker>Waller, 2006</marker>
<rawString>Waller, A. (2006). Communication Access to Conversational Narrative. Topics in Language Disorders 26(3): 221-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Waller</author>
</authors>
<title>Narrative-based Augmentative and Alternative Communication: From transactional to interactional conversation.</title>
<date>2008</date>
<booktitle>Proceedings of ISAAC</booktitle>
<pages>149--160</pages>
<contexts>
<context position="12319" citStr="Waller, 2008" startWordPosition="1975" endWordPosition="1976">ossible messages to communicate Narration: User selects what to say Editing: User adds annotations NLG: Generate utterance Speech synthesis Web info sources Other external data Prepare content Narrate content Dialogue model Conversation partner Conversation model Domain model User model 3 tion (von Tetzchner and Grove, 2003). Many children who use AAC have very limited narrative skills (Soto et al, 2006). Research has shown that providing children who use AAC with successful narrative experiences by providing full narrative text can help the development of written and spoken narrative skills (Waller, 2008). The system follows the architecture described above. Input data comes from RFID sensors that track where the child went during the day; an RFID reader is mounted on the child’s wheelchair, and RFID tags are placed around the school, especially in doorways so we can monitor children entering and leaving rooms. Teachers have also been given RFID swipe cards which they can swipe against a reader, to record that they are interacting with the child; this is more robust than attempting to infer interaction automatically by tracking teachers’ position. Teachers can also record interactions with obj</context>
</contexts>
<marker>Waller, 2008</marker>
<rawString>Waller, A. (2008). Narrative-based Augmentative and Alternative Communication: From transactional to interactional conversation. Proceedings of ISAAC 2008, pages 149-160.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Waller</author>
<author>R Black</author>
<author>D A O&apos;Mara</author>
<author>H Pain</author>
<author>G Ritchie</author>
<author>R Manurung</author>
</authors>
<booktitle>Evaluating the STANDUP Pun Generating Software with Children with Cerebral Palsy. ACM Transactions on Accessible Computing.</booktitle>
<marker>Waller, Black, O&apos;Mara, Pain, Ritchie, Manurung, </marker>
<rawString>Waller, A., R. Black, D. A. O&apos;Mara, H. Pain, G. Ritchie and R. Manurung (In Press). Evaluating the STANDUP Pun Generating Software with Children with Cerebral Palsy. ACM Transactions on Accessible Computing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>