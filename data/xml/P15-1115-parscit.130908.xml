<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000799">
<title confidence="0.978814">
Semantic Role Labeling Improves Incremental Parsing
</title>
<author confidence="0.984386">
Ioannis Konstas and Frank Keller
</author>
<affiliation confidence="0.998524">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<email confidence="0.991155">
{ikonstas,keller}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993681" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890388888889">
Incremental parsing is the task of assign-
ing a syntactic structure to an input sen-
tence as it unfolds word by word. Incre-
mental parsing is more difficult than full-
sentence parsing, as incomplete input in-
creases ambiguity. Intuitively, an incre-
mental parser that has access to seman-
tic information should be able to reduce
ambiguity by ruling out semantically im-
plausible analyses, even for incomplete in-
put. In this paper, we test this hypothesis
by combining an incremental TAG parser
with an incremental semantic role labeler
in a discriminative framework. We show
a substantial improvement in parsing per-
formance compared to the baseline parser,
both in full-sentence F-score and in incre-
mental F-score.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.978124266666667">
When humans listen to speech, the input becomes
available gradually as the speech signal unfolds.
Reading happens in a similarly gradual manner
when the eyes scan a text. There is good evidence
that the human language processor is adapted to
this and works incrementally, i.e., computes an in-
terpretation for an incoming sentence on a word-
by-word basis (Tanenhaus et al., 1995; Altmann
and Kamide, 1999). Also language processing
systems often deal with speech as it is spoken, or
text as it is typed. A dialogue system should start
interpreting a sentence while it is spoken, and an
information retrieval system should start retrieving
results while the user is typing.
Incremental processing is therefore essential
both for realistic models of human language pro-
cessing and for NLP applications that react to
user input in real time. In response to this, a
number of incremental parsers have been devel-
oped, which use context-free grammar (Roark,
2001; Schuler et al., 2010), dependency grammar
(Chelba and Jelinek, 2000; Nivre, 2007; Huang
and Sagae, 2010), or tree-substitution grammars
(Sangati and Keller, 2013). Typical applications
of incremental parsers include speech recognition
(Chelba and Jelinek, 2000; Roark, 2001; Xu et al.,
2002), machine translation (Schwartz et al., 2011;
Tan et al., 2011), reading time modeling (Demberg
and Keller, 2008), or dialogue systems (Stoness
et al., 2004).
Incremental parsing, however, is considerably
harder than full-sentence parsing: when process-
ing the n-th word in a sentence, an, the parser only
has access to the left context (words a1 ...an−1);
the right context (words an+1 ...aN) is not known
yet. This can lead to local ambiguity, i.e., pro-
duce additional syntactic analyses that are valid
for the sentence prefix, but become invalid as the
right context is processed. As an example consider
the sentence prefix in (1):
(1) The athlete realized her goals ...
a. at the competition
b. were out of reach
The prefix could continue as in (1-a), i.e., as a
main clause structure. Or the next words could
be as in (1-b), in which case her goals is part of a
subordinate clause.
Intuitively, an incremental parser that has access
to semantic information would be able to decide
which of these two analyses is likely to be correct,
even without knowing the rest of the sentence. If
the NP her goals is a likely ARG1 of realized the
parser should prefer the main clause structure. On
the other hand, if the NP is a likely ARG0 of an (as
yet unseen) embedded verb, then the parser should
go for the subordinate clause structure. This is il-
lustrated in Figure 2. Note that the preference can
easily be reversed: if the prefix was the athlete re-
alized her shoes, then her shoes is very likely to
be an ARG0 rather than an ARG1.
</bodyText>
<page confidence="0.940968">
1191
</page>
<note confidence="0.976497666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1191–1201,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999485666666667">
The basis of this paper is the hypothesis that
semantic information can aid incremental parsing.
To test this hypothesis, we combine an incremen-
tal TAG parser with an incremental semantic role
labeling (iSRL) system. The iSRL system takes
prefix trees and computes their most likely seman-
tic role assignments. We show that these role as-
signments can be used to re-rank the output of
the incremental parser, leading to substantial im-
provements in parsing performance compared to
the baseline parser, both in full-sentence F-score
and in incremental F-score.
</bodyText>
<sectionHeader confidence="0.99895" genericHeader="method">
2 Incremental Semantic Role Labeling
</sectionHeader>
<bodyText confidence="0.999935428571429">
The current work builds on an existing incremen-
tal parser, the Psycholinguistically Motivated Tree
Adjoining Grammar (PLTAG) parser of Demberg
et al. (2013). The distinguishing feature of this
parser is that it builds fully connected structures
(no words are left unattached during incremental
parsing); this requires it to make predictions about
the right context, which are verified as more of
the input becomes available. Konstas et al. (2014)
show that semantic information can be attached to
PLTAG structures, making it possible to assign se-
mantic roles incrementally. In the present paper,
we use these semantic roles to re-rank the output
of the PLTAG parser.
</bodyText>
<subsectionHeader confidence="0.987124">
2.1 Psycholinguistically Motivated TAG
</subsectionHeader>
<bodyText confidence="0.999974272727273">
PLTAG extends standard TAG (Joshi and Sch-
abes, 1992) in order to enable incremental parsing.
Standard TAG assumes a lexicon of elementary
trees, each of which contains at least one lexical
item as an anchor and at most one leaf node as
a foot node, marked with A∗. All other leaves
are marked with Al and are called substitution
nodes. To derive a TAG parse for a sentence, we
start with the elementary tree of the head of the
sentence and integrate the elementary trees of the
other lexical items of the sentence using two oper-
ations: adjunction at an internal node and substi-
tution at a substitution node (the node at which the
operation applies is the integration point). Stan-
dard TAG derivations are not guaranteed to be in-
cremental, as adjunction can happen anywhere in
a sentence, possibly violating left-to-right process-
ing order. PLTAG addresses this limitation by in-
troducing prediction trees, elementary trees with-
out a lexical anchor. These are used to predict
syntactic structure anchored by words that appears
later in an incremental derivation. This ensures
</bodyText>
<figure confidence="0.998833">
(a) valid (b) invalid
</figure>
<figureCaption confidence="0.792883333333333">
Figure 1: The current fringe (dashed line) indi-
cates where valid substitutions can occur. Other
substitutions result in an invalid prefix tree.
</figureCaption>
<bodyText confidence="0.990512076923077">
that fully connected prefix trees can be built for
every prefix of the input.
In order to efficiently parse PLTAG, Demberg
et al. (2013) introduce the concept of fringes.
Fringes capture the fact that in an incremental
derivation, a prefix tree can only be combined with
an elementary tree at a limited set of nodes. For
instance, the prefix tree in Figure 1 has two substi-
tution nodes, for B and C. However, only substi-
tution into B leads to a valid new prefix tree; if we
substitute into C, we obtain the tree in Figure 1b,
which is not a valid prefix tree (i.e., it represents a
non-incremental derivation).
</bodyText>
<subsectionHeader confidence="0.99783">
2.2 Incremental Role Propagation
</subsectionHeader>
<bodyText confidence="0.999987821428571">
The output of a semantic role labeler is a set of
semantic dependency triples (l,r, p), where l is a
semantic role label (e.g., ARG0, ARG1, ARGM in
Propbank), and r and p are the words (argument
and predicate) to which the role applies. An incre-
mental semantic role labeler assigns semantic de-
pendency triples to a prefix of the input sentence.
Note that not every word is an argument to a pred-
icate, therefore the set of triples will not necessar-
ily change at every input word. Also, triples can be
incomplete, as either the predicate or the argument
may not have been observed yet.
Konstas et al. (2014) propose an iSRL system
based on a PLTAG parser with a semantically aug-
mented lexicon. They parse an input sentence in-
crementally, applying their incremental role prop-
agation algorithm (IRPA) to the resulting prefix
trees. This creates new semantic triples (or up-
dates existing, incomplete ones) whenever an el-
ementary or prediction tree that carries semantic
role information is attached to the prefix tree. As
soon as a triple is completed a two-stage classifica-
tion process is applied, that first identifies whether
the predicate/argument pair is a good candidate,
and then disambiguates the role label (often multi-
ple roles are possible for a lexical entry). Figure 2
shows the incremental role assignment for the two
readings of the prefix the athlete realized her goals
</bodyText>
<figure confidence="0.999084375">
S
a By Cy a
S
B Cy
b
S
a By C
c
</figure>
<page confidence="0.632574">
21192
</page>
<figure confidence="0.8229214">
S
VP
NP
NNS
goal
</figure>
<page confidence="0.772263">
31193
</page>
<equation confidence="0.975214">
an+1 by combining the prefix tree ˆyn with an+1:
ADV((xn,ˆyn,n)) = (xn,ˆyn,n) ® an+1
= {(xn+1, ˆyn+1,n+ 1)}
</equation>
<bodyText confidence="0.9973118">
Next, we define the set of states representing
prefix trees as 7L, with 7L0 = {(x0, 0,0)}, and
7Ln = U7L�E7Ln_1ADV(7L�). We can now redefine
GEN(xn) = 7Ln, for any prefix of length n.
We enumerate prefix trees (function GEN) with
the incremental parser of Demberg et al. (2013).
The states of the model are stored in a chart; each
cell holds the top-k prefix trees. The transition
to the next state (function ADV) is performed by
combining each prefix tree with a set of candidate
of elementary (and prediction) trees via adjunc-
tion and substitution, subject to restrictions im-
posed by incrementallity (see Figure 2). In or-
der to efficiently compute all combinations, the
PLTAG parser computes only the fringes (see Sec-
tion 2) of the prefix tree, and the candidate ele-
mentary trees and matches these two; this avoids
the computation of the prefix tree entirely.1
Each prefix tree is weighted using a probabil-
ity model estimated over PLTAG operations and
the lexicon. This probability is used as a feature
in (D. In addition, we define a set of features of
increasing sophistication, which include features
specific to PLTAG, standard tree-based features,
and, crucially, features extracted from the seman-
tic role triples produced incrementally by the iSRL
system of Konstas et al. (2014). The features are
computed for each prefix tree yn, so (D can be re-
written as (D(xn,yn), and therefore Equation (1) be-
comes:
</bodyText>
<equation confidence="0.9831215">
ˆyn = argmax (D(xn,yn) · w¯ (2)
ynE7Ln
</equation>
<bodyText confidence="0.996388764705882">
Our goal now becomes to learn mappings between
sentence prefixes xn and prefix trees ˆyn. In contrast
to models that estimate features weights on full
sentence parses (Collins and Roark, 2004; Char-
niak and Johnson, 2005), we do not observe gold-
standard prefix trees during training. However, we
can use gold-standard lexicon entries when pars-
ing the training data with the PLTAG parser, which
gives an approximation of gold-standard prefix
trees y+n . Finally, during testing, given an unseen
sentence x and a trained set of feature weights ¯w,
our model generates prefix trees yn for every sen-
tence prefix of size n.
1As in a chart parser, the prefix tree can be re-constructed
by following backpointers in the chart. This is done only
for evaluation at the end of the sentence or incrementally on
demand.
</bodyText>
<sectionHeader confidence="0.96243" genericHeader="method">
4 Reranking Features
</sectionHeader>
<bodyText confidence="0.999584083333333">
This section describes the features used for rerank-
ing the prefix trees generated by the incremental
parser. We include three different classes of fea-
tures, based on local information from PLTAG el-
ementary trees, based on global and structural in-
formation from prefix trees, and based on seman-
tic information provided by iSRL triples. In con-
trast to work on discriminative full-sentence pars-
ing (e.g., Charniak and Johnson, 2005; Collins and
Koo, 2005), we can only use features extracted
from the prefix trees being constructed incremen-
tally as the sentence is parsed. The right context of
the current word cannot be used, as this would vio-
late incrementality. Every feature combination we
try also includes the following baseline features:
Prefix Tree Probability is the log probability of
the prefix tree as scored by the probability model
of the baseline parser. The score is normalized by
prefix length, to avoid getting larger negative log
probability scores for longer prefixes.
Elementary Tree Probability is the log proba-
bility of the elementary tree corresponding to the
word just added to the prefix tree according to the
probability model of the baseline parser.
</bodyText>
<subsectionHeader confidence="0.984738">
4.1 PLTAG Features
</subsectionHeader>
<bodyText confidence="0.999966954545455">
The baseline generative model of the PLTAG
parser employs features based on parsing actions,
the elementary trees used at each timestamp, and
the previous word and PoS tag. In the discrimi-
native model, we extend the locality of these fea-
tures, as well as addressing sparsity issues arising
from rare elementary trees. In all cases, both lex-
icalized and unlexicalized versions of the elemen-
tary trees are used.
Unigram Trees is a family of binary features
that record the local elementary trees chosen by
the parser for the n-th word, i.e., current word for
n = 1 and previous word for n = 2.
Parent-Unigram Trees is a variation of the pre-
vious feature, where we encode the elementary
tree of the current word along with the category of
the node it attaches to in the prefix tree. This cap-
tures the attachment decisions the parser makes.
Bigram Trees are pairs of elementary trees for
adjacent words (i.e., the elementary tree currently
added to the prefix tree and the previous one).
This extends the history the parser has access to,
</bodyText>
<page confidence="0.978646">
41194
</page>
<bodyText confidence="0.99992025">
and captures pairs of elementary trees that are fre-
quently chosen together, e.g., a verb-headed tree
with a PP foot node, followed by an NP-headed
prepositional tree.
</bodyText>
<subsectionHeader confidence="0.99465">
4.2 Tree Features
</subsectionHeader>
<bodyText confidence="0.999821979166667">
The following features are inspired by Charniak
and Johnson (2005) and attempt to encode proper-
ties of the prefix tree, as well as capture regulari-
ties for specific syntactic construction such as co-
ordination. Even though the PLTAG parser builds
fully connected structures and predicts upcoming
context, some constituents in a given prefix tree
may be incomplete. We therefore compute the fea-
tures in this group only for those constituents that
have been completed in the current prefix tree (i.e.,
constituents that are complete at word ai, but were
incomplete at word ai−1). This ensures each of
the features is only counted once per constituent.
For example, the coordination parallelism feature
(see below) should be computed only after all the
words in the yield of the CC non-terminal have
been observed.
Right Branch encodes the number of nodes on
the longest path from the root of the prefix tree to
the rightmost pre-terminal. We also include the
symmetrical feature which records the number of
the remaining nodes in the prefix tree. This feature
allows the parser to prefer right-branching trees,
commonly found in English syntax.
Coordination Parallelism records whether the
two sibling subtrees of a coordination node are
identical in terms of structure and node categories
up to depth l. We encode identity in a bit mask,
and set l = 4 (e.g., 1100 means the subtrees have
identical children and grandchildren).
Coordination Parallelism Length indicates the
binned difference in size between the yields of
each sibling subtree under a coordination node. It
also stores whether the second subtree is at the end
of the sentence.
Heavy stores the category of each node in a
completed constituent, along with the binned
length of its yield and whether it is at the end of
the sentence. This feature captures the tendency
of larger constituents to occur towards the end of
the sentence.
Neighbors encodes the category of each node in
the completed constituent, the binned yield size,
and the PoS tags of the l preceding words, were
l = 1 or 2.
Word stores the current word along with the cat-
egories of its l immediate ancestor nodes (exclud-
ing pre-terminals); l = 2 or 3.
</bodyText>
<subsectionHeader confidence="0.994703">
4.3 SRL Features
</subsectionHeader>
<bodyText confidence="0.999900170731707">
The features in this group are extracted from the
output of iSRL system of Konstas et al. (2014),
which annotates prefix trees with semantic roles.
The setup proposed in the current paper makes
it possible to feed the semantic information back
to the PLTAG parser by using it to re-rank the k-
best prefix trees generated by the parser. (The re-
ranked prefix trees could then also result in better
iSRL performance, an issue we will return to in
Section 6.3.)
Recall that the SRL information comes in the
form of triples (l,r, p), where l is a semantic role
label and r and p are the words to which the role
applies (see Figure 2 for examples). For each fea-
ture, we also compute an unlexicalized version
by replacing the argument and predicates in the
triples with their PoS tags.
Complete SRL Triples stores the complete
triples (if any) generated by the current word. The
word can be the predicate or the argument in one
or more dependency relations involving previous
words.
Semantic Frame records all the arguments of
a predicate (if present) for frequent semantic la-
bels, i.e., A0, A1 and A2, as well as the presence
of a modifier (e.g., AM-TMP, AM-LOC, etc.).
This feature usually fires when a verb is added to
the prefix tree and generates several complete SRL
triples. The feature captures the semantic frame of
a verb as a whole (while the previous feature just
records it as a collection of triples).
Back-off SRL Triples are generated by remov-
ing either the argument, or the predicate, or the
role label, from a complete triple. This provides
a way of generalizing between triples that share
some information without being completely iden-
tical.
Predicate/Argument/Role encodes the ele-
ments of a complete SRL triple individually
(argument, predicate, or role). This allows for
further generalization and reduces sparsity.
</bodyText>
<page confidence="0.976223">
51195
</page>
<sectionHeader confidence="0.995932" genericHeader="method">
5 Feature Weight Estimation
</sectionHeader>
<bodyText confidence="0.999986875">
We estimate the vector of feature weights w¯ in
Equation (2) using the averaged structured percep-
tron algorithm of Collins (2002); we give the pseu-
docode in Algorithm 1. The perceptron makes
T passes over L training examples. In each it-
eration, for each sentence prefix/prefix tree pair
(xn,yn), it computes the best scoring prefix tree ˆyn
among the candidate prefix trees, given the cur-
rent feature weights ¯w. In line 7, the algorithm
updates w¯ with the difference (if any) between the
feature representations of the best scoring prefix
tree ˆyn and the approximate gold-standard prefix
tree y+n (see Section 3.2). Note that since we use
a constant beam during decoding with the PLTAG
parser in order to enumerate the set of prefix trees
πn, there is no guarantee that the argmax in line 5
will find the highest scoring (in terms of F-score)
prefix tree yn =� ˆyn. Search errors due to the best
analysis falling out of the beam at a given pre-
fix length will create errors both when decoding
unseen sentences at test time, and when learning
the feature weights with the perceptron algorithm.
The final weight vector w¯ is the average of the
weight vectors over T iterations, L examples and
N words. The averaging avoids overfitting and
produces more stable results (Collins, 2002).
Note that features are computed for every prefix
of the input sentence. Recall that the parser avoids
the explicit computation of the prefix trees in πn
through the use of the fringes (see Sections 2.1
and 3.2). This is sufficient for the computation of
PLTAG and SRL features, but we need to explic-
itly calculate every prefix tree yn for the computa-
tion of the tree features (see Section 4.2). This is
an expensive operation if we are parsing the whole
training corpus. To overcome this time bottleneck,
we compute features only for those analyses of
every input sentence prefix that belongs to the k-
best analyses at the end of the sentence. In other
words, πn is the set of only those prefix trees that
are used by the k-best analyses at the end of the
sentence. This results in a much smaller number
of prefix trees that need to be computed for each
word. However, during testing, given the trained
w¯ and an unseen sentence, we compute all features
for each prefix length of the sentence, hence calcu-
late all prefix trees in πn and incrementally re-rank
the chart entries of the parser on the fly.
</bodyText>
<figure confidence="0.389781">
Algorithm 1: Averaged Structured Perceptron
Input: Training Examples: (x,y)L i=1,xi = a1...aN
1 w¯&lt;-- 0
2 for t &lt;-- 1...T do
3 for i &lt;-- 1...L do
4 for n &lt;-- 1...N do
5 ˆyn = argmaxynEπn Φ(xn,yn) &apos; w¯
if y+
6 n =� ˆyn then
w¯ &lt;-- w¯ +Φ(xn,y+
7 n ) − Φ(xn,yn)
8 return 1T ∑T=11 ∑Li=1 ∑n=1 1 wt,i,n
</figure>
<sectionHeader confidence="0.921648" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.986822">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.997932619047619">
We use the PLTAG parser of Demberg et al. (2013)
to enumerate prefix trees yn and to compute the
prefix tree and word probability scores which we
use as features. We also use the iSRL system
of Konstas et al. (2014) to generate incremental
SRL triples. Their system includes a semantically-
enriched lexicon extracted from the Wall Street
Journal (WSJ) part of the Penn Treebank corpus
(Marcus et al., 1993), converted to PLTAG for-
mat. Semantic role annotation is sourced from
Propbank. We trained the probability model of
the parser and the identification and labeling clas-
sifiers of the iSRL system using the intersection of
Sections 2–21 of WSJ and the English portion of
the CoNLL 2009 Shared Task (Hajiˇc et al., 2009).
We learn the weight vector w¯ by training the per-
ceptron algorithm also on Sections 2–21 of WSJ
(see Section 5 for details). We use the PoS tags
predicted by the parser, rather than gold standard
PoS tags. Testing is performed on section 23 of
WSJ, for sentences up to 40 words.
</bodyText>
<subsectionHeader confidence="0.995393">
6.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999850125">
In addition to standard full-sentence labeled
bracket score, we evaluate our model incremen-
tally, by scoring the prefix trees generated for each
sentence prefix (Sangati and Keller, 2013). For
each prefix of the input sentence (two words or
more), we compute the labeled bracket score for
the minimal structure spanning that prefix. The
minimal structure is defined as the subtree rooted
in the lowest common ancestor of the prefix nodes,
while removing any leftover intermediate nodes
on the right edge of the subtree that do not have
a word in the prefix as their yield.
Although not the main focus of this paper, we
also report full-sentence combined SRL accuracy
(counting verb-predicates only). This score is ob-
tained by re-applying the iSRL system to the syn-
</bodyText>
<page confidence="0.98567">
61196
</page>
<table confidence="0.999175285714286">
System Prec Rec F AUC SRL
BASELINE 75.51 76.93 76.21 71.49 69.43
TREE 75.99 77.52 76.75 73.02 68.80
SRL 75.99 77.65 76.81 73.97 69.96
TREE+PLTAG 76.67 78.27 77.47 72.27 70.27
TREE+PLTAG 77.00 78.57 77.77 74.97 70.00
+SRL
</table>
<tableCaption confidence="0.998677">
Table 1: Full-sentence parsing results2, area under
</tableCaption>
<bodyText confidence="0.997974918918919">
the curve (AUC) for the incremental parsing re-
sults of Figure 3, and combined SRL score across
different groups of features.
tactic parses output by our re-ranker. (In contrast,
Konstas et al. (2014) work with gold-standard syn-
tactic parses.)
We evaluate four variants of our model (see Sec-
tion 4 for an explanation of the different groups of
features):
TREE is the model that uses tree features
only; this essentially simulates standard parse re-
ranking approaches such as the one of Charniak
and Johnson (2005).
SRL uses only features based on iSRL triples;
it provides a proof-of-concept, demonstrating that
the semantic information encoded in SRL triples
can help the parser building better syntactic trees.
TREE+PLTAG adds PLTAG Features to the
TREE model, taking advantage of local infor-
mation specific to elementary PLTAG trees;
TREE+PLTAG essentially provides a strong
syntax-only baseline.
TREE+PLTAG+SRL combines SRL features
and syntactic features.
Finally, our baseline is the PLTAG parser of
Demberg et al. (2013), using the original proba-
bility model without any re-ranking. A compari-
son with other incremental parsers would be de-
sirable, but is not trivial to achieve. This is be-
cause the PLTAG parser is trained and evaluated
on a version of the Penn Treebank that was con-
verted to PLTAG format. This renders our results
not directly comparable to parsers that reproduce
the Penn Treebank bracketing. For example, the
PLTAG parser produces deeper tree structures in-
formed by Propbank and the noun phrase annota-
tion of Vadas and Curran (2007).
</bodyText>
<figure confidence="0.968545">
10 20 30 40
Prefix Length
</figure>
<figureCaption confidence="0.935554">
Figure 3: Incremental parsing F-score for increas-
ing sentence prefixes, up to 40 words.
</figureCaption>
<subsectionHeader confidence="0.669812">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999523518518519">
Figure 3 gives the results of evaluating incre-
mental parsing performance. The x-axis shows
prefix length, and the y-axis shows incremental
F-score computed as suggested by Sangati and
Keller (2013). Each point is averaged over all pre-
fixes of a given length in the test set. To quantify
the trends shown in this figure, we also compute
the area under the curve (AUC) for each feature
combination; this is given in Table 1.
We find that TREE performs consistently bet-
ter than the baseline for short prefixes (up to the
first 20 words), and then is very close to the base-
line. This is expected given that tree features add
structure-specific information (e.g., about coordi-
nation) to the baseline model, and is consistent
with results obtained using similar features in the
literature (Charniak and Johnson, 2005). Adding
PLTAG features (TREE+PLTAG) hurts incremen-
tal performance for short prefixes (up to about 20
words), but then performance gradually increases
over the baseline and over TREE alone. It seems
that the PLTAG features, which are specific to the
grammar formalism used, are able to help with
longer and more complex prefixes, but introduce
noise in smaller prefixes.
The SRL feature set, on the other hand, results
in a consistent increase in performance compared
</bodyText>
<footnote confidence="0.9157526">
2Note that the baseline score is lower than the published
F = 77.41 of Demberg et al. (2013). This is expected, since
we use a semantically-enriched lexicon, which increases the
size of the lexicon, resulting in higher ambiguity per word as
well as increased sparsity in the probability model.
</footnote>
<figure confidence="0.991959454545454">
BASELINE
TREE
SRL
TREE+PLTAG
TREE+PLTAG+SRL
F-score 0.9
0.85
0.8
0.75
0.7
0.65
</figure>
<page confidence="0.991288">
71197
</page>
<bodyText confidence="0.999981725490196">
to the baseline, across all prefix lengths. SRL pro-
vides semantic knowledge, while TREE provides
syntactic knowledge, but the performance of both
feature sets is very close to each other, up to a
prefix length of about 30 words, after which SRL
has a clear advantage. SRL features seem to fil-
ter out local ambiguity caused by creating pre-
fix trees incrementally and result in correct parses
closer to the end of sentence, even without the
use of the syntactic information contained in the
TREE+PLTAG feature set. Recall that SRL uses in-
formation provided by the semantic frame, some-
thing that a syntax-only model does not have ac-
cess to. It seems that this makes it possible for SRL
to (partially) compensate for mistakes made by the
parser. The AUC of SRL is higher by 0.95 and 1.7
points compared to TREE and TREE+PLTAG, re-
spectively.
We observe an additional boost in perfor-
mance when using all features together in the
TREE+PLTAG+SRL configuration, which outper-
forms SRL alone by 1.0 points in AUC. Recall that
SRL features do not apply to every word; they fire
only when semantic information is introduced to
the parser via the semantically-enriched lexicon.
Hence by adding tree and PLTAG features, which
normally apply for every new word, we are able to
perform effective re-ranking for all sentence pre-
fixes, which explains the boost in performance.
Note that for all variants of our model we observe
a dip in performance at around 38 words. This is
probably due to noise, caused by the small number
of sentences of this length. The upward trend seen
around word 40 is probably the effect of observ-
ing the end of the sentence, which boosts parsing
accuracy.
Turning to full sentence evaluation (Table 1),
we observe a similar trend. Both TREE and
SRL beat the baseline by about 0.55 points in F-
score. Progressively adding features increases per-
formance, with the greatest gain of 1.56 points
attained by the combination of all features in
TREE+PLTAG+SRL.
We also report combined SRL F-score com-
puted on the re-ranked syntactic trees (rightmost
column of Table 1). We find that compared to
the baseline, only a small improvement of 0.55
points is achieved by TREE+PLTAG+SRL, while
TREE+PLTAG improves by 0.84 points. The
syntax-only variant therefore outperforms the full
model, but only by a small margin.
</bodyText>
<sectionHeader confidence="0.999351" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999949680851064">
The most similar approach in the literature is
Collins and Roark’s (2004) re-ranking model for
incremental parsing. They learn the syntactic fea-
tures of Roark (2001) using the perceptron model
of Collins (2002). Similar to us, they use the in-
cremental parser to search over candidate parses.
However, they limited themselves to local deriva-
tion features (akin to our PLTAG features), and do
not explore global syntactic feature (tree features)
or SRL features. Even though they re-rank the
output of an incremental parser, they only evalu-
ate full sentence parsing performance. Other re-
ranking approaches to syntactic parsing make use
of an extensive set of global features, but apply it
on the k-best list of full sentence parses (Charniak
and Johnson, 2005; Collins and Koo, 2005) or the
k-best list of derivations of a packed forest (Huang,
2008), i.e., these approaches are not incremental.
Based on the CoNLL Shared Tasks (e.g., Hajiˇc
et al., 2009), a number of systems exist that per-
form syntactic parsing and semantic role label-
ing jointly. Toutanova et al. (2008), Sutton and
McCallum (2005) and Li et al. (2010) combine
the scores of two separate models, i.e., a syntac-
tic parser and a semantic role labeler, and re-rank
the combination using features from each domain.
Titov et al. (2009) and Gesmundo et al. (2009),
instead of combining models, create a common
search space for syntactic parsing and SRL, using
a shift reduce-style technique (Nivre, 2007) and
learn a latent variable model (Incremental Sigmoid
Belief Networks) that optimizes over both tasks at
the same time. Volokh and Neumann (2008) use a
variant of Nivre’s (2007) incremental shift-reduce
parser and rely only on the current word and pre-
vious content to output partial dependency trees;
then they output role labels given the full parser
output. In contrast to all the joint approaches, we
perform both parsing and semantic role labeling
strictly incrementally, without having access to the
whole sentence, outputting prefix trees and iSRL
triples for every sentence prefix. Our approach
creates a feedback loop, i.e., we generate a prefix
tree using the baseline model, give it as input to
iSRL, then re-rank it using a set of syntactic and
SRL features. The resulting new prefix tree can
then be fed back into iSRL, etc.
</bodyText>
<page confidence="0.988257">
81198
</page>
<sectionHeader confidence="0.998822" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999991043478261">
We started from the observation that human pars-
ing uses semantic knowledge to rule out parses
that lead to implausible interpretations. Based on
this, we hypothesized that also in NLP, an incre-
mental syntactic parser should benefit from se-
mantic information. To test this hypothesis, we
combined an incremental TAG parser with an in-
cremental semantic role labeler. We used the out-
put of the iSRL system to derive features that can
be used to re-rank the prefix trees generated by the
incremental parser. We found that SRL features,
both in isolation and together with standard syn-
tactic features, improve parsing performance, both
when measured using full-sentence F-score, and in
terms of incremental F-score.
In future work, we plan to combine our incre-
mental parsing/role labeling approach with a com-
positional model of semantics, which would have
to be modified to take semantic role triples as in-
put (rather than words or word pairs). The re-
sulting plausibility estimates could then be used
as another source of semantic information for the
parser, or employed in down-stream tasks.
</bodyText>
<sectionHeader confidence="0.997477" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994828">
EPSRC support through grant EP/I032916/1 “An
integrated model of syntactic and semantic pre-
diction in human language processing” to Frank
Keller and Mirella Lapata is gratefully acknowl-
edged.
</bodyText>
<sectionHeader confidence="0.968229" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.6738245">
Altmann, Gerry T. M. and Yuki Kamide. 1999.
Incremental interpretation at verbs: Restricting
the domain of subsequent reference. Cognition
73:247–264.
</bodyText>
<reference confidence="0.731807266666667">
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and maxent dis-
criminative reranking. In Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics. Ann Arbor, Michi-
gan, pages 173–180.
Chelba, Ciprian and Frederick Jelinek. 2000.
Structured language modeling. Computer
Speech and Language 14:283–332.
Collins, Michael. 2002. Discriminative training
methods for hidden markov models: Theory
and experiments with perceptron algorithms. In
Proceedings of the 2002 Conference on Empir-
ical Methods in Natural Language Processing.
pages 1–8.
Collins, Michael and Terry Koo. 2005. Discrim-
inative reranking for natural language parsing.
Computational Linguistics 31(1):25–70.
Collins, Michael and Brian Roark. 2004. Incre-
mental parsing with the perceptron algorithm.
In Proceedings of the 42nd Meeting of the As-
sociation for Computational Linguistics, Main
Volume. Barcelona, Spain, pages 111–118.
Demberg, Vera and Frank Keller. 2008. Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition
101(2):193–210.
Demberg, Vera, Frank Keller, and Alexander
Koller. 2013. Incremental, predictive pars-
ing with psycholinguistically motivated tree-
adjoining grammar. Computational Linguistics
39(4):1025–1066.
Gesmundo, Andrea, James Henderson, Paola
Merlo, and Ivan Titov. 2009. A latent vari-
able model of synchronous syntactic-semantic
parsing for multiple languages. In Proceed-
ings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared
Task. pages 37–42.
Hajiˇc, Jan, Massimiliano Ciaramita, Richard Jo-
hansson, Daisuke Kawahara, Maria Ant`onia
Mart´ı, Lluis M`arquez, Adam Meyers, Joakim
Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel
Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task:
</reference>
<page confidence="0.972964">
91199
</page>
<reference confidence="0.9970839">
Syntactic and semantic dependencies in multi-
ple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language
Learning. Boulder, Colorado, USA.
Huang, Liang. 2008. Forest reranking: Dis-
criminative parsing with non-local features. In
Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics.
Columbus, Ohio, pages 586–594.
Huang, Liang and Kenji Sagae. 2010. Dynamic
programming for linear-time incremental pars-
ing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguis-
tics. Uppsala, pages 1077–1086.
Joshi, Aravind K. and Yves Schabes. 1992. Tree
adjoining grammars and lexicalized grammars.
In Maurice Nivat and Andreas Podelski, editors,
Tree Automata and Languages, North-Holland,
Amsterdam, pages 409–432.
Konstas, Ioannis, Frank Keller, Vera Demberg,
and Mirella Lapata. 2014. Incremental seman-
tic role labeling with tree adjoining grammar. In
Proceedings of the 2014 Conference on Empir-
ical Methods in Natural Language Processing.
Doha, Qatar, pages 301–312.
Li, Junhui, Guodong Zhou, and Tou Hwee Ng.
2010. Joint syntactic and semantic parsing of
chinese. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics. Association for Computational Lin-
guistics, pages 1108–1117.
Marcus, Mitchell P., Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building a large
annotated corpus of english: The Penn tree-
bank. Computational Linguistics 19(2):313–
330.
Nivre, Joakim. 2007. Incremental non-projective
dependency parsing. In Human Language Tech-
nologies: The Conference of the North Amer-
ican Chapter of the Association for Compu-
tational Linguistics; Proceedings of the Main
Conference. Rochester, New York, pages 396–
403.
Roark, Brian. 2001. Probabilistic top-down pars-
ing and language modeling. Computational
Linguististics 27:249–276.
Sangati, Federico and Frank Keller. 2013. In-
cremental tree substitution grammar for pars-
ing and word prediction. Transactions of
the Association for Computational Linguistics
1(May):111–124.
Schuler, William, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2010. Broad-
coverage parsing using human-like memory
constraints. Computational Linguististics
36(1):1–30.
Schwartz, Lane, Chris Callison-Burch, William
Schuler, and Stephen Wu. 2011. Incremen-
tal syntactic language models for phrase-based
translation. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, Volume 1. Portland, OR, pages 620–631.
Stoness, Scott C., Joel Tetreault, and James Allen.
2004. Incremental parsing with reference inter-
action. In Frank Keller, Stephen Clark, Matthew
Crocker, and Mark Steedman, editors, Proceed-
ings of the ACL Workshop Incremental Parsing:
Bringing Engineering and Cognition Together.
Barcelona, pages 18–25.
Sutton, Charles and Andrew McCallum. 2005.
Joint parsing and semantic role labeling. In Pro-
ceedings of the Ninth Conference on Computa-
tional Natural Language Learning. Ann Arbor,
Michigan, pages 225–228.
Tan, Ming, Wenli Zhou, Lei Zheng, and Shaojun
Wang. 2011. A large scale distributed syntactic,
semantic and lexical language model for ma-
chine translation. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Tech-
nologies, Volume 1. Portland, OR, pages 201–
210.
Tanenhaus, Michael K., Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehen-
sion. Science 268:1632–1634.
Titov, Ivan, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planari-
sation for synchronous parsing of semantic and
syntactic dependencies. In Proceedings of the
21st International Jont Conference on Artifi-
cal Intelligence. Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA, pages 1562–
1567.
Toutanova, Kristina, Aria Haghighi, and Christo-
pher D. Manning. 2008. A global joint model
for semantic role labeling. Computational Lin-
guistics 34(2):161–191.
</reference>
<page confidence="0.798223">
11200
</page>
<reference confidence="0.985862444444444">
Vadas, David and James Curran. 2007. Adding
noun phrase structure to the penn treebank.
In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguis-
tics. Association for Computational Linguistics,
Prague, Czech Republic, pages 240–247.
Volokh, Alexander and G¨unter Neumann. 2008.
Proceedings of the Twelfth Conference on Com-
putational Natural Language Learning, Coling
2008 Organizing Committee, chapter A Puris-
tic Approach for Joint Dependency Parsing and
Semantic Role Labeling, pages 213–217.
Xu, Peng, Ciprian Chelba, and Frederick Jelinek.
2002. A study on richer syntactic dependen-
cies for structured language modeling. In Pro-
ceedings of the 40th Annual Meeting on Associ-
ation for Computational Linguistics. Philadel-
phia, pages 191–198.
</reference>
<page confidence="0.994635">
111201
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.592915">
<title confidence="0.999625">Semantic Role Labeling Improves Incremental Parsing</title>
<author confidence="0.926382">Ioannis Konstas</author>
<author confidence="0.926382">Frank</author>
<affiliation confidence="0.9702815">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<abstract confidence="0.981751789473684">Incremental parsing is the task of assigning a syntactic structure to an input sentence as it unfolds word by word. Incremental parsing is more difficult than fullsentence parsing, as incomplete input increases ambiguity. Intuitively, an incremental parser that has access to semantic information should be able to reduce ambiguity by ruling out semantically implausible analyses, even for incomplete input. In this paper, we test this hypothesis by combining an incremental TAG parser with an incremental semantic role labeler in a discriminative framework. We show a substantial improvement in parsing performance compared to the baseline parser, both in full-sentence F-score and in incremental F-score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="10392" citStr="Charniak and Johnson, 2005" startWordPosition="1713" endWordPosition="1717">eatures of increasing sophistication, which include features specific to PLTAG, standard tree-based features, and, crucially, features extracted from the semantic role triples produced incrementally by the iSRL system of Konstas et al. (2014). The features are computed for each prefix tree yn, so (D can be rewritten as (D(xn,yn), and therefore Equation (1) becomes: ˆyn = argmax (D(xn,yn) · w¯ (2) ynE7Ln Our goal now becomes to learn mappings between sentence prefixes xn and prefix trees ˆyn. In contrast to models that estimate features weights on full sentence parses (Collins and Roark, 2004; Charniak and Johnson, 2005), we do not observe goldstandard prefix trees during training. However, we can use gold-standard lexicon entries when parsing the training data with the PLTAG parser, which gives an approximation of gold-standard prefix trees y+n . Finally, during testing, given an unseen sentence x and a trained set of feature weights ¯w, our model generates prefix trees yn for every sentence prefix of size n. 1As in a chart parser, the prefix tree can be re-constructed by following backpointers in the chart. This is done only for evaluation at the end of the sentence or incrementally on demand. 4 Reranking F</context>
<context position="13486" citStr="Charniak and Johnson (2005)" startWordPosition="2230" endWordPosition="2233">ode the elementary tree of the current word along with the category of the node it attaches to in the prefix tree. This captures the attachment decisions the parser makes. Bigram Trees are pairs of elementary trees for adjacent words (i.e., the elementary tree currently added to the prefix tree and the previous one). This extends the history the parser has access to, 41194 and captures pairs of elementary trees that are frequently chosen together, e.g., a verb-headed tree with a PP foot node, followed by an NP-headed prepositional tree. 4.2 Tree Features The following features are inspired by Charniak and Johnson (2005) and attempt to encode properties of the prefix tree, as well as capture regularities for specific syntactic construction such as coordination. Even though the PLTAG parser builds fully connected structures and predicts upcoming context, some constituents in a given prefix tree may be incomplete. We therefore compute the features in this group only for those constituents that have been completed in the current prefix tree (i.e., constituents that are complete at word ai, but were incomplete at word ai−1). This ensures each of the features is only counted once per constituent. For example, the </context>
<context position="22759" citStr="Charniak and Johnson (2005)" startWordPosition="3823" endWordPosition="3826">77.47 72.27 70.27 TREE+PLTAG 77.00 78.57 77.77 74.97 70.00 +SRL Table 1: Full-sentence parsing results2, area under the curve (AUC) for the incremental parsing results of Figure 3, and combined SRL score across different groups of features. tactic parses output by our re-ranker. (In contrast, Konstas et al. (2014) work with gold-standard syntactic parses.) We evaluate four variants of our model (see Section 4 for an explanation of the different groups of features): TREE is the model that uses tree features only; this essentially simulates standard parse reranking approaches such as the one of Charniak and Johnson (2005). SRL uses only features based on iSRL triples; it provides a proof-of-concept, demonstrating that the semantic information encoded in SRL triples can help the parser building better syntactic trees. TREE+PLTAG adds PLTAG Features to the TREE model, taking advantage of local information specific to elementary PLTAG trees; TREE+PLTAG essentially provides a strong syntax-only baseline. TREE+PLTAG+SRL combines SRL features and syntactic features. Finally, our baseline is the PLTAG parser of Demberg et al. (2013), using the original probability model without any re-ranking. A comparison with other</context>
<context position="24743" citStr="Charniak and Johnson, 2005" startWordPosition="4145" endWordPosition="4148">Keller (2013). Each point is averaged over all prefixes of a given length in the test set. To quantify the trends shown in this figure, we also compute the area under the curve (AUC) for each feature combination; this is given in Table 1. We find that TREE performs consistently better than the baseline for short prefixes (up to the first 20 words), and then is very close to the baseline. This is expected given that tree features add structure-specific information (e.g., about coordination) to the baseline model, and is consistent with results obtained using similar features in the literature (Charniak and Johnson, 2005). Adding PLTAG features (TREE+PLTAG) hurts incremental performance for short prefixes (up to about 20 words), but then performance gradually increases over the baseline and over TREE alone. It seems that the PLTAG features, which are specific to the grammar formalism used, are able to help with longer and more complex prefixes, but introduce noise in smaller prefixes. The SRL feature set, on the other hand, results in a consistent increase in performance compared 2Note that the baseline score is lower than the published F = 77.41 of Demberg et al. (2013). This is expected, since we use a seman</context>
<context position="28676" citStr="Charniak and Johnson, 2005" startWordPosition="4797" endWordPosition="4800">syntactic features of Roark (2001) using the perceptron model of Collins (2002). Similar to us, they use the incremental parser to search over candidate parses. However, they limited themselves to local derivation features (akin to our PLTAG features), and do not explore global syntactic feature (tree features) or SRL features. Even though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, c</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, Eugene and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics. Ann Arbor, Michigan, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language</journal>
<pages>14--283</pages>
<contexts>
<context position="1966" citStr="Chelba and Jelinek, 2000" startWordPosition="302" endWordPosition="305">1999). Also language processing systems often deal with speech as it is spoken, or text as it is typed. A dialogue system should start interpreting a sentence while it is spoken, and an information retrieval system should start retrieving results while the user is typing. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only has access to the left context (words a1 ...an−1); the right context (w</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Chelba, Ciprian and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language 14:283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>1--8</pages>
<contexts>
<context position="17609" citStr="Collins (2002)" startWordPosition="2927" endWordPosition="2928"> as a collection of triples). Back-off SRL Triples are generated by removing either the argument, or the predicate, or the role label, from a complete triple. This provides a way of generalizing between triples that share some information without being completely identical. Predicate/Argument/Role encodes the elements of a complete SRL triple individually (argument, predicate, or role). This allows for further generalization and reduces sparsity. 51195 5 Feature Weight Estimation We estimate the vector of feature weights w¯ in Equation (2) using the averaged structured perceptron algorithm of Collins (2002); we give the pseudocode in Algorithm 1. The perceptron makes T passes over L training examples. In each iteration, for each sentence prefix/prefix tree pair (xn,yn), it computes the best scoring prefix tree ˆyn among the candidate prefix trees, given the current feature weights ¯w. In line 7, the algorithm updates w¯ with the difference (if any) between the feature representations of the best scoring prefix tree ˆyn and the approximate gold-standard prefix tree y+n (see Section 3.2). Note that since we use a constant beam during decoding with the PLTAG parser in order to enumerate the set of </context>
<context position="28129" citStr="Collins (2002)" startWordPosition="4710" endWordPosition="4711">on of all features in TREE+PLTAG+SRL. We also report combined SRL F-score computed on the re-ranked syntactic trees (rightmost column of Table 1). We find that compared to the baseline, only a small improvement of 0.55 points is achieved by TREE+PLTAG+SRL, while TREE+PLTAG improves by 0.84 points. The syntax-only variant therefore outperforms the full model, but only by a small margin. 7 Related Work The most similar approach in the literature is Collins and Roark’s (2004) re-ranking model for incremental parsing. They learn the syntactic features of Roark (2001) using the perceptron model of Collins (2002). Similar to us, they use the incremental parser to search over candidate parses. However, they limited themselves to local derivation features (akin to our PLTAG features), and do not explore global syntactic feature (tree features) or SRL features. Even though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of deriva</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, Michael. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing. pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="11452" citStr="Collins and Koo, 2005" startWordPosition="1890" endWordPosition="1893">an be re-constructed by following backpointers in the chart. This is done only for evaluation at the end of the sentence or incrementally on demand. 4 Reranking Features This section describes the features used for reranking the prefix trees generated by the incremental parser. We include three different classes of features, based on local information from PLTAG elementary trees, based on global and structural information from prefix trees, and based on semantic information provided by iSRL triples. In contrast to work on discriminative full-sentence parsing (e.g., Charniak and Johnson, 2005; Collins and Koo, 2005), we can only use features extracted from the prefix trees being constructed incrementally as the sentence is parsed. The right context of the current word cannot be used, as this would violate incrementality. Every feature combination we try also includes the following baseline features: Prefix Tree Probability is the log probability of the prefix tree as scored by the probability model of the baseline parser. The score is normalized by prefix length, to avoid getting larger negative log probability scores for longer prefixes. Elementary Tree Probability is the log probability of the elementa</context>
<context position="28700" citStr="Collins and Koo, 2005" startWordPosition="4801" endWordPosition="4804">(2001) using the perceptron model of Collins (2002). Similar to us, they use the incremental parser to search over candidate parses. However, they limited themselves to local derivation features (akin to our PLTAG features), and do not explore global syntactic feature (tree features) or SRL features. Even though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search sp</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Collins, Michael and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume.</booktitle>
<pages>111--118</pages>
<location>Barcelona,</location>
<contexts>
<context position="10363" citStr="Collins and Roark, 2004" startWordPosition="1709" endWordPosition="1712">ion, we define a set of features of increasing sophistication, which include features specific to PLTAG, standard tree-based features, and, crucially, features extracted from the semantic role triples produced incrementally by the iSRL system of Konstas et al. (2014). The features are computed for each prefix tree yn, so (D can be rewritten as (D(xn,yn), and therefore Equation (1) becomes: ˆyn = argmax (D(xn,yn) · w¯ (2) ynE7Ln Our goal now becomes to learn mappings between sentence prefixes xn and prefix trees ˆyn. In contrast to models that estimate features weights on full sentence parses (Collins and Roark, 2004; Charniak and Johnson, 2005), we do not observe goldstandard prefix trees during training. However, we can use gold-standard lexicon entries when parsing the training data with the PLTAG parser, which gives an approximation of gold-standard prefix trees y+n . Finally, during testing, given an unseen sentence x and a trained set of feature weights ¯w, our model generates prefix trees yn for every sentence prefix of size n. 1As in a chart parser, the prefix tree can be re-constructed by following backpointers in the chart. This is done only for evaluation at the end of the sentence or increment</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Collins, Michael and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume. Barcelona, Spain, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>Data from eye-tracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition</journal>
<volume>101</volume>
<issue>2</issue>
<contexts>
<context position="2303" citStr="Demberg and Keller, 2008" startWordPosition="350" endWordPosition="353">c models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only has access to the left context (words a1 ...an−1); the right context (words an+1 ...aN) is not known yet. This can lead to local ambiguity, i.e., produce additional syntactic analyses that are valid for the sentence prefix, but become invalid as the right context is processed. As an example consider the sentence prefix in (1): (1) The athlete realized her goals ... a. at the competition b. were out of rea</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Demberg, Vera and Frank Keller. 2008. Data from eye-tracking corpora as evidence for theories of syntactic processing complexity. Cognition 101(2):193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
<author>Alexander Koller</author>
</authors>
<title>Incremental, predictive parsing with psycholinguistically motivated treeadjoining grammar.</title>
<date>2013</date>
<journal>Computational Linguistics</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="4738" citStr="Demberg et al. (2013)" startWordPosition="754" endWordPosition="757">ine an incremental TAG parser with an incremental semantic role labeling (iSRL) system. The iSRL system takes prefix trees and computes their most likely semantic role assignments. We show that these role assignments can be used to re-rank the output of the incremental parser, leading to substantial improvements in parsing performance compared to the baseline parser, both in full-sentence F-score and in incremental F-score. 2 Incremental Semantic Role Labeling The current work builds on an existing incremental parser, the Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG) parser of Demberg et al. (2013). The distinguishing feature of this parser is that it builds fully connected structures (no words are left unattached during incremental parsing); this requires it to make predictions about the right context, which are verified as more of the input becomes available. Konstas et al. (2014) show that semantic information can be attached to PLTAG structures, making it possible to assign semantic roles incrementally. In the present paper, we use these semantic roles to re-rank the output of the PLTAG parser. 2.1 Psycholinguistically Motivated TAG PLTAG extends standard TAG (Joshi and Schabes, 199</context>
<context position="6662" citStr="Demberg et al. (2013)" startWordPosition="1069" endWordPosition="1072">an happen anywhere in a sentence, possibly violating left-to-right processing order. PLTAG addresses this limitation by introducing prediction trees, elementary trees without a lexical anchor. These are used to predict syntactic structure anchored by words that appears later in an incremental derivation. This ensures (a) valid (b) invalid Figure 1: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefix tree. that fully connected prefix trees can be built for every prefix of the input. In order to efficiently parse PLTAG, Demberg et al. (2013) introduce the concept of fringes. Fringes capture the fact that in an incremental derivation, a prefix tree can only be combined with an elementary tree at a limited set of nodes. For instance, the prefix tree in Figure 1 has two substitution nodes, for B and C. However, only substitution into B leads to a valid new prefix tree; if we substitute into C, we obtain the tree in Figure 1b, which is not a valid prefix tree (i.e., it represents a non-incremental derivation). 2.2 Incremental Role Propagation The output of a semantic role labeler is a set of semantic dependency triples (l,r, p), wher</context>
<context position="8997" citStr="Demberg et al. (2013)" startWordPosition="1481" endWordPosition="1484">abel (often multiple roles are possible for a lexical entry). Figure 2 shows the incremental role assignment for the two readings of the prefix the athlete realized her goals S a By Cy a S B Cy b S a By C c 21192 S VP NP NNS goal 31193 an+1 by combining the prefix tree ˆyn with an+1: ADV((xn,ˆyn,n)) = (xn,ˆyn,n) ® an+1 = {(xn+1, ˆyn+1,n+ 1)} Next, we define the set of states representing prefix trees as 7L, with 7L0 = {(x0, 0,0)}, and 7Ln = U7L�E7Ln_1ADV(7L�). We can now redefine GEN(xn) = 7Ln, for any prefix of length n. We enumerate prefix trees (function GEN) with the incremental parser of Demberg et al. (2013). The states of the model are stored in a chart; each cell holds the top-k prefix trees. The transition to the next state (function ADV) is performed by combining each prefix tree with a set of candidate of elementary (and prediction) trees via adjunction and substitution, subject to restrictions imposed by incrementallity (see Figure 2). In order to efficiently compute all combinations, the PLTAG parser computes only the fringes (see Section 2) of the prefix tree, and the candidate elementary trees and matches these two; this avoids the computation of the prefix tree entirely.1 Each prefix tr</context>
<context position="20236" citStr="Demberg et al. (2013)" startWordPosition="3400" endWordPosition="3403">ch word. However, during testing, given the trained w¯ and an unseen sentence, we compute all features for each prefix length of the sentence, hence calculate all prefix trees in πn and incrementally re-rank the chart entries of the parser on the fly. Algorithm 1: Averaged Structured Perceptron Input: Training Examples: (x,y)L i=1,xi = a1...aN 1 w¯&lt;-- 0 2 for t &lt;-- 1...T do 3 for i &lt;-- 1...L do 4 for n &lt;-- 1...N do 5 ˆyn = argmaxynEπn Φ(xn,yn) &apos; w¯ if y+ 6 n =� ˆyn then w¯ &lt;-- w¯ +Φ(xn,y+ 7 n ) − Φ(xn,yn) 8 return 1T ∑T=11 ∑Li=1 ∑n=1 1 wt,i,n 6 Experiments 6.1 Setup We use the PLTAG parser of Demberg et al. (2013) to enumerate prefix trees yn and to compute the prefix tree and word probability scores which we use as features. We also use the iSRL system of Konstas et al. (2014) to generate incremental SRL triples. Their system includes a semanticallyenriched lexicon extracted from the Wall Street Journal (WSJ) part of the Penn Treebank corpus (Marcus et al., 1993), converted to PLTAG format. Semantic role annotation is sourced from Propbank. We trained the probability model of the parser and the identification and labeling classifiers of the iSRL system using the intersection of Sections 2–21 of WSJ an</context>
<context position="23273" citStr="Demberg et al. (2013)" startWordPosition="3897" endWordPosition="3900">is essentially simulates standard parse reranking approaches such as the one of Charniak and Johnson (2005). SRL uses only features based on iSRL triples; it provides a proof-of-concept, demonstrating that the semantic information encoded in SRL triples can help the parser building better syntactic trees. TREE+PLTAG adds PLTAG Features to the TREE model, taking advantage of local information specific to elementary PLTAG trees; TREE+PLTAG essentially provides a strong syntax-only baseline. TREE+PLTAG+SRL combines SRL features and syntactic features. Finally, our baseline is the PLTAG parser of Demberg et al. (2013), using the original probability model without any re-ranking. A comparison with other incremental parsers would be desirable, but is not trivial to achieve. This is because the PLTAG parser is trained and evaluated on a version of the Penn Treebank that was converted to PLTAG format. This renders our results not directly comparable to parsers that reproduce the Penn Treebank bracketing. For example, the PLTAG parser produces deeper tree structures informed by Propbank and the noun phrase annotation of Vadas and Curran (2007). 10 20 30 40 Prefix Length Figure 3: Incremental parsing F-score for</context>
<context position="25303" citStr="Demberg et al. (2013)" startWordPosition="4237" endWordPosition="4240">milar features in the literature (Charniak and Johnson, 2005). Adding PLTAG features (TREE+PLTAG) hurts incremental performance for short prefixes (up to about 20 words), but then performance gradually increases over the baseline and over TREE alone. It seems that the PLTAG features, which are specific to the grammar formalism used, are able to help with longer and more complex prefixes, but introduce noise in smaller prefixes. The SRL feature set, on the other hand, results in a consistent increase in performance compared 2Note that the baseline score is lower than the published F = 77.41 of Demberg et al. (2013). This is expected, since we use a semantically-enriched lexicon, which increases the size of the lexicon, resulting in higher ambiguity per word as well as increased sparsity in the probability model. BASELINE TREE SRL TREE+PLTAG TREE+PLTAG+SRL F-score 0.9 0.85 0.8 0.75 0.7 0.65 71197 to the baseline, across all prefix lengths. SRL provides semantic knowledge, while TREE provides syntactic knowledge, but the performance of both feature sets is very close to each other, up to a prefix length of about 30 words, after which SRL has a clear advantage. SRL features seem to filter out local ambigui</context>
</contexts>
<marker>Demberg, Keller, Koller, 2013</marker>
<rawString>Demberg, Vera, Frank Keller, and Alexander Koller. 2013. Incremental, predictive parsing with psycholinguistically motivated treeadjoining grammar. Computational Linguistics 39(4):1025–1066.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous syntactic-semantic parsing for multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task.</booktitle>
<pages>37--42</pages>
<contexts>
<context position="29244" citStr="Gesmundo et al. (2009)" startWordPosition="4894" endWordPosition="4897">st of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and previous content to output partial dependency trees; then they output role labels given the full parser output. In contrast to all the joint approaches, we perform both parsing and semantic role labeling stric</context>
</contexts>
<marker>Gesmundo, Henderson, Merlo, Titov, 2009</marker>
<rawString>Gesmundo, Andrea, James Henderson, Paola Merlo, and Ivan Titov. 2009. A latent variable model of synchronous syntactic-semantic parsing for multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task. pages 37–42.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Lluis M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning.</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Hajiˇc, Jan, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Lluis M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning. Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>586--594</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="28767" citStr="Huang, 2008" startWordPosition="4815" endWordPosition="4816">the incremental parser to search over candidate parses. However, they limited themselves to local derivation features (akin to our PLTAG features), and do not explore global syntactic feature (tree features) or SRL features. Even though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style techn</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Huang, Liang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. Columbus, Ohio, pages 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>1077--1086</pages>
<location>Uppsala,</location>
<contexts>
<context position="2003" citStr="Huang and Sagae, 2010" startWordPosition="308" endWordPosition="311"> often deal with speech as it is spoken, or text as it is typed. A dialogue system should start interpreting a sentence while it is spoken, and an information retrieval system should start retrieving results while the user is typing. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only has access to the left context (words a1 ...an−1); the right context (words an+1 ...aN) is not known yet. Th</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Huang, Liang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Tree adjoining grammars and lexicalized grammars.</title>
<date>1992</date>
<booktitle>In Maurice Nivat and Andreas Podelski, editors, Tree Automata and Languages, North-Holland,</booktitle>
<pages>409--432</pages>
<location>Amsterdam,</location>
<contexts>
<context position="5340" citStr="Joshi and Schabes, 1992" startWordPosition="847" endWordPosition="851"> Demberg et al. (2013). The distinguishing feature of this parser is that it builds fully connected structures (no words are left unattached during incremental parsing); this requires it to make predictions about the right context, which are verified as more of the input becomes available. Konstas et al. (2014) show that semantic information can be attached to PLTAG structures, making it possible to assign semantic roles incrementally. In the present paper, we use these semantic roles to re-rank the output of the PLTAG parser. 2.1 Psycholinguistically Motivated TAG PLTAG extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with Al and are called substitution nodes. To derive a TAG parse for a sentence, we start with the elementary tree of the head of the sentence and integrate the elementary trees of the other lexical items of the sentence using two operations: adjunction at an internal node and substitution at a substitution node (the node at which the operation applies is t</context>
</contexts>
<marker>Joshi, Schabes, 1992</marker>
<rawString>Joshi, Aravind K. and Yves Schabes. 1992. Tree adjoining grammars and lexicalized grammars. In Maurice Nivat and Andreas Podelski, editors, Tree Automata and Languages, North-Holland, Amsterdam, pages 409–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Konstas</author>
<author>Frank Keller</author>
<author>Vera Demberg</author>
<author>Mirella Lapata</author>
</authors>
<title>Incremental semantic role labeling with tree adjoining grammar.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>301--312</pages>
<location>Doha, Qatar,</location>
<contexts>
<context position="5028" citStr="Konstas et al. (2014)" startWordPosition="799" endWordPosition="802">ubstantial improvements in parsing performance compared to the baseline parser, both in full-sentence F-score and in incremental F-score. 2 Incremental Semantic Role Labeling The current work builds on an existing incremental parser, the Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG) parser of Demberg et al. (2013). The distinguishing feature of this parser is that it builds fully connected structures (no words are left unattached during incremental parsing); this requires it to make predictions about the right context, which are verified as more of the input becomes available. Konstas et al. (2014) show that semantic information can be attached to PLTAG structures, making it possible to assign semantic roles incrementally. In the present paper, we use these semantic roles to re-rank the output of the PLTAG parser. 2.1 Psycholinguistically Motivated TAG PLTAG extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with Al and are called substitution nodes. To d</context>
<context position="7775" citStr="Konstas et al. (2014)" startWordPosition="1269" endWordPosition="1272">ole Propagation The output of a semantic role labeler is a set of semantic dependency triples (l,r, p), where l is a semantic role label (e.g., ARG0, ARG1, ARGM in Propbank), and r and p are the words (argument and predicate) to which the role applies. An incremental semantic role labeler assigns semantic dependency triples to a prefix of the input sentence. Note that not every word is an argument to a predicate, therefore the set of triples will not necessarily change at every input word. Also, triples can be incomplete, as either the predicate or the argument may not have been observed yet. Konstas et al. (2014) propose an iSRL system based on a PLTAG parser with a semantically augmented lexicon. They parse an input sentence incrementally, applying their incremental role propagation algorithm (IRPA) to the resulting prefix trees. This creates new semantic triples (or updates existing, incomplete ones) whenever an elementary or prediction tree that carries semantic role information is attached to the prefix tree. As soon as a triple is completed a two-stage classification process is applied, that first identifies whether the predicate/argument pair is a good candidate, and then disambiguates the role </context>
<context position="10007" citStr="Konstas et al. (2014)" startWordPosition="1647" endWordPosition="1650">, the PLTAG parser computes only the fringes (see Section 2) of the prefix tree, and the candidate elementary trees and matches these two; this avoids the computation of the prefix tree entirely.1 Each prefix tree is weighted using a probability model estimated over PLTAG operations and the lexicon. This probability is used as a feature in (D. In addition, we define a set of features of increasing sophistication, which include features specific to PLTAG, standard tree-based features, and, crucially, features extracted from the semantic role triples produced incrementally by the iSRL system of Konstas et al. (2014). The features are computed for each prefix tree yn, so (D can be rewritten as (D(xn,yn), and therefore Equation (1) becomes: ˆyn = argmax (D(xn,yn) · w¯ (2) ynE7Ln Our goal now becomes to learn mappings between sentence prefixes xn and prefix trees ˆyn. In contrast to models that estimate features weights on full sentence parses (Collins and Roark, 2004; Charniak and Johnson, 2005), we do not observe goldstandard prefix trees during training. However, we can use gold-standard lexicon entries when parsing the training data with the PLTAG parser, which gives an approximation of gold-standard pr</context>
<context position="15713" citStr="Konstas et al. (2014)" startWordPosition="2606" endWordPosition="2609">h node in a completed constituent, along with the binned length of its yield and whether it is at the end of the sentence. This feature captures the tendency of larger constituents to occur towards the end of the sentence. Neighbors encodes the category of each node in the completed constituent, the binned yield size, and the PoS tags of the l preceding words, were l = 1 or 2. Word stores the current word along with the categories of its l immediate ancestor nodes (excluding pre-terminals); l = 2 or 3. 4.3 SRL Features The features in this group are extracted from the output of iSRL system of Konstas et al. (2014), which annotates prefix trees with semantic roles. The setup proposed in the current paper makes it possible to feed the semantic information back to the PLTAG parser by using it to re-rank the kbest prefix trees generated by the parser. (The reranked prefix trees could then also result in better iSRL performance, an issue we will return to in Section 6.3.) Recall that the SRL information comes in the form of triples (l,r, p), where l is a semantic role label and r and p are the words to which the role applies (see Figure 2 for examples). For each feature, we also compute an unlexicalized ver</context>
<context position="20403" citStr="Konstas et al. (2014)" startWordPosition="3431" endWordPosition="3434">ix trees in πn and incrementally re-rank the chart entries of the parser on the fly. Algorithm 1: Averaged Structured Perceptron Input: Training Examples: (x,y)L i=1,xi = a1...aN 1 w¯&lt;-- 0 2 for t &lt;-- 1...T do 3 for i &lt;-- 1...L do 4 for n &lt;-- 1...N do 5 ˆyn = argmaxynEπn Φ(xn,yn) &apos; w¯ if y+ 6 n =� ˆyn then w¯ &lt;-- w¯ +Φ(xn,y+ 7 n ) − Φ(xn,yn) 8 return 1T ∑T=11 ∑Li=1 ∑n=1 1 wt,i,n 6 Experiments 6.1 Setup We use the PLTAG parser of Demberg et al. (2013) to enumerate prefix trees yn and to compute the prefix tree and word probability scores which we use as features. We also use the iSRL system of Konstas et al. (2014) to generate incremental SRL triples. Their system includes a semanticallyenriched lexicon extracted from the Wall Street Journal (WSJ) part of the Penn Treebank corpus (Marcus et al., 1993), converted to PLTAG format. Semantic role annotation is sourced from Propbank. We trained the probability model of the parser and the identification and labeling classifiers of the iSRL system using the intersection of Sections 2–21 of WSJ and the English portion of the CoNLL 2009 Shared Task (Hajiˇc et al., 2009). We learn the weight vector w¯ by training the perceptron algorithm also on Sections 2–21 of </context>
<context position="22447" citStr="Konstas et al. (2014)" startWordPosition="3771" endWordPosition="3774">so report full-sentence combined SRL accuracy (counting verb-predicates only). This score is obtained by re-applying the iSRL system to the syn61196 System Prec Rec F AUC SRL BASELINE 75.51 76.93 76.21 71.49 69.43 TREE 75.99 77.52 76.75 73.02 68.80 SRL 75.99 77.65 76.81 73.97 69.96 TREE+PLTAG 76.67 78.27 77.47 72.27 70.27 TREE+PLTAG 77.00 78.57 77.77 74.97 70.00 +SRL Table 1: Full-sentence parsing results2, area under the curve (AUC) for the incremental parsing results of Figure 3, and combined SRL score across different groups of features. tactic parses output by our re-ranker. (In contrast, Konstas et al. (2014) work with gold-standard syntactic parses.) We evaluate four variants of our model (see Section 4 for an explanation of the different groups of features): TREE is the model that uses tree features only; this essentially simulates standard parse reranking approaches such as the one of Charniak and Johnson (2005). SRL uses only features based on iSRL triples; it provides a proof-of-concept, demonstrating that the semantic information encoded in SRL triples can help the parser building better syntactic trees. TREE+PLTAG adds PLTAG Features to the TREE model, taking advantage of local information </context>
</contexts>
<marker>Konstas, Keller, Demberg, Lapata, 2014</marker>
<rawString>Konstas, Ioannis, Frank Keller, Vera Demberg, and Mirella Lapata. 2014. Incremental semantic role labeling with tree adjoining grammar. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages 301–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Guodong Zhou</author>
<author>Tou Hwee Ng</author>
</authors>
<title>Joint syntactic and semantic parsing of chinese.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>1108--1117</pages>
<contexts>
<context position="29039" citStr="Li et al. (2010)" startWordPosition="4859" endWordPosition="4862">ncremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and prev</context>
</contexts>
<marker>Li, Zhou, Ng, 2010</marker>
<rawString>Li, Junhui, Guodong Zhou, and Tou Hwee Ng. 2010. Joint syntactic and semantic parsing of chinese. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 1108–1117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>330</pages>
<contexts>
<context position="20593" citStr="Marcus et al., 1993" startWordPosition="3460" endWordPosition="3463">for t &lt;-- 1...T do 3 for i &lt;-- 1...L do 4 for n &lt;-- 1...N do 5 ˆyn = argmaxynEπn Φ(xn,yn) &apos; w¯ if y+ 6 n =� ˆyn then w¯ &lt;-- w¯ +Φ(xn,y+ 7 n ) − Φ(xn,yn) 8 return 1T ∑T=11 ∑Li=1 ∑n=1 1 wt,i,n 6 Experiments 6.1 Setup We use the PLTAG parser of Demberg et al. (2013) to enumerate prefix trees yn and to compute the prefix tree and word probability scores which we use as features. We also use the iSRL system of Konstas et al. (2014) to generate incremental SRL triples. Their system includes a semanticallyenriched lexicon extracted from the Wall Street Journal (WSJ) part of the Penn Treebank corpus (Marcus et al., 1993), converted to PLTAG format. Semantic role annotation is sourced from Propbank. We trained the probability model of the parser and the identification and labeling classifiers of the iSRL system using the intersection of Sections 2–21 of WSJ and the English portion of the CoNLL 2009 Shared Task (Hajiˇc et al., 2009). We learn the weight vector w¯ by training the perceptron algorithm also on Sections 2–21 of WSJ (see Section 5 for details). We use the PoS tags predicted by the parser, rather than gold standard PoS tags. Testing is performed on section 23 of WSJ, for sentences up to 40 words. 6.2</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Marcus, Mitchell P., Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The Penn treebank. Computational Linguistics 19(2):313– 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incremental non-projective dependency parsing.</title>
<date>2007</date>
<booktitle>In Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference.</booktitle>
<pages>396--403</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="1979" citStr="Nivre, 2007" startWordPosition="306" endWordPosition="307">ssing systems often deal with speech as it is spoken, or text as it is typed. A dialogue system should start interpreting a sentence while it is spoken, and an information retrieval system should start retrieving results while the user is typing. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only has access to the left context (words a1 ...an−1); the right context (words an+1 ...</context>
<context position="29385" citStr="Nivre, 2007" startWordPosition="4917" endWordPosition="4918">, these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and previous content to output partial dependency trees; then they output role labels given the full parser output. In contrast to all the joint approaches, we perform both parsing and semantic role labeling strictly incrementally, without having access to the whole sentence, outputting prefix trees and iSRL triples for every sentence prefix. Our appro</context>
</contexts>
<marker>Nivre, 2007</marker>
<rawString>Nivre, Joakim. 2007. Incremental non-projective dependency parsing. In Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference. Rochester, New York, pages 396– 403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguististics</journal>
<pages>27--249</pages>
<contexts>
<context position="1897" citStr="Roark, 2001" startWordPosition="294" endWordPosition="295">word basis (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Also language processing systems often deal with speech as it is spoken, or text as it is typed. A dialogue system should start interpreting a sentence while it is spoken, and an information retrieval system should start retrieving results while the user is typing. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only ha</context>
<context position="28084" citStr="Roark (2001)" startWordPosition="4703" endWordPosition="4704">in of 1.56 points attained by the combination of all features in TREE+PLTAG+SRL. We also report combined SRL F-score computed on the re-ranked syntactic trees (rightmost column of Table 1). We find that compared to the baseline, only a small improvement of 0.55 points is achieved by TREE+PLTAG+SRL, while TREE+PLTAG improves by 0.84 points. The syntax-only variant therefore outperforms the full model, but only by a small margin. 7 Related Work The most similar approach in the literature is Collins and Roark’s (2004) re-ranking model for incremental parsing. They learn the syntactic features of Roark (2001) using the perceptron model of Collins (2002). Similar to us, they use the incremental parser to search over candidate parses. However, they limited themselves to local derivation features (akin to our PLTAG features), and do not explore global syntactic feature (tree features) or SRL features. Even though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collin</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Roark, Brian. 2001. Probabilistic top-down parsing and language modeling. Computational Linguististics 27:249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Federico Sangati</author>
<author>Frank Keller</author>
</authors>
<title>Incremental tree substitution grammar for parsing and word prediction. Transactions of the Association for Computational Linguistics 1(May):111–124.</title>
<date>2013</date>
<contexts>
<context position="2061" citStr="Sangati and Keller, 2013" startWordPosition="315" endWordPosition="318">is typed. A dialogue system should start interpreting a sentence while it is spoken, and an information retrieval system should start retrieving results while the user is typing. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only has access to the left context (words a1 ...an−1); the right context (words an+1 ...aN) is not known yet. This can lead to local ambiguity, i.e., produce additional s</context>
<context position="21392" citStr="Sangati and Keller, 2013" startWordPosition="3595" endWordPosition="3598"> of the iSRL system using the intersection of Sections 2–21 of WSJ and the English portion of the CoNLL 2009 Shared Task (Hajiˇc et al., 2009). We learn the weight vector w¯ by training the perceptron algorithm also on Sections 2–21 of WSJ (see Section 5 for details). We use the PoS tags predicted by the parser, rather than gold standard PoS tags. Testing is performed on section 23 of WSJ, for sentences up to 40 words. 6.2 Evaluation In addition to standard full-sentence labeled bracket score, we evaluate our model incrementally, by scoring the prefix trees generated for each sentence prefix (Sangati and Keller, 2013). For each prefix of the input sentence (two words or more), we compute the labeled bracket score for the minimal structure spanning that prefix. The minimal structure is defined as the subtree rooted in the lowest common ancestor of the prefix nodes, while removing any leftover intermediate nodes on the right edge of the subtree that do not have a word in the prefix as their yield. Although not the main focus of this paper, we also report full-sentence combined SRL accuracy (counting verb-predicates only). This score is obtained by re-applying the iSRL system to the syn61196 System Prec Rec F</context>
<context position="24129" citStr="Sangati and Keller (2013)" startWordPosition="4039" endWordPosition="4042"> the Penn Treebank that was converted to PLTAG format. This renders our results not directly comparable to parsers that reproduce the Penn Treebank bracketing. For example, the PLTAG parser produces deeper tree structures informed by Propbank and the noun phrase annotation of Vadas and Curran (2007). 10 20 30 40 Prefix Length Figure 3: Incremental parsing F-score for increasing sentence prefixes, up to 40 words. 6.3 Results Figure 3 gives the results of evaluating incremental parsing performance. The x-axis shows prefix length, and the y-axis shows incremental F-score computed as suggested by Sangati and Keller (2013). Each point is averaged over all prefixes of a given length in the test set. To quantify the trends shown in this figure, we also compute the area under the curve (AUC) for each feature combination; this is given in Table 1. We find that TREE performs consistently better than the baseline for short prefixes (up to the first 20 words), and then is very close to the baseline. This is expected given that tree features add structure-specific information (e.g., about coordination) to the baseline model, and is consistent with results obtained using similar features in the literature (Charniak and </context>
</contexts>
<marker>Sangati, Keller, 2013</marker>
<rawString>Sangati, Federico and Frank Keller. 2013. Incremental tree substitution grammar for parsing and word prediction. Transactions of the Association for Computational Linguistics 1(May):111–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Samir AbdelRahman</author>
<author>Tim Miller</author>
<author>Lane Schwartz</author>
</authors>
<title>Broadcoverage parsing using human-like memory constraints.</title>
<date>2010</date>
<journal>Computational Linguististics</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="1920" citStr="Schuler et al., 2010" startWordPosition="296" endWordPosition="299">anenhaus et al., 1995; Altmann and Kamide, 1999). Also language processing systems often deal with speech as it is spoken, or text as it is typed. A dialogue system should start interpreting a sentence while it is spoken, and an information retrieval system should start retrieving results while the user is typing. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only has access to the left co</context>
</contexts>
<marker>Schuler, AbdelRahman, Miller, Schwartz, 2010</marker>
<rawString>Schuler, William, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2010. Broadcoverage parsing using human-like memory constraints. Computational Linguististics 36(1):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lane Schwartz</author>
<author>Chris Callison-Burch</author>
<author>William Schuler</author>
<author>Stephen Wu</author>
</authors>
<title>Incremental syntactic language models for phrase-based translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Volume 1.</booktitle>
<pages>620--631</pages>
<location>Portland, OR,</location>
<contexts>
<context position="2234" citStr="Schwartz et al., 2011" startWordPosition="339" endWordPosition="342">. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only has access to the left context (words a1 ...an−1); the right context (words an+1 ...aN) is not known yet. This can lead to local ambiguity, i.e., produce additional syntactic analyses that are valid for the sentence prefix, but become invalid as the right context is processed. As an example consider the sentence prefix in (1): (1) The at</context>
</contexts>
<marker>Schwartz, Callison-Burch, Schuler, Wu, 2011</marker>
<rawString>Schwartz, Lane, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language models for phrase-based translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Volume 1. Portland, OR, pages 620–631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Stoness</author>
<author>Joel Tetreault</author>
<author>James Allen</author>
</authors>
<title>Incremental parsing with reference interaction.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together.</booktitle>
<pages>18--25</pages>
<editor>In Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman, editors,</editor>
<location>Barcelona,</location>
<contexts>
<context position="2347" citStr="Stoness et al., 2004" startWordPosition="357" endWordPosition="360">P applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only has access to the left context (words a1 ...an−1); the right context (words an+1 ...aN) is not known yet. This can lead to local ambiguity, i.e., produce additional syntactic analyses that are valid for the sentence prefix, but become invalid as the right context is processed. As an example consider the sentence prefix in (1): (1) The athlete realized her goals ... a. at the competition b. were out of reach The prefix could continue as in (1-a), i.</context>
</contexts>
<marker>Stoness, Tetreault, Allen, 2004</marker>
<rawString>Stoness, Scott C., Joel Tetreault, and James Allen. 2004. Incremental parsing with reference interaction. In Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together. Barcelona, pages 18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Joint parsing and semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning.</booktitle>
<pages>225--228</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="29018" citStr="Sutton and McCallum (2005)" startWordPosition="4854" endWordPosition="4857">they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the </context>
</contexts>
<marker>Sutton, McCallum, 2005</marker>
<rawString>Sutton, Charles and Andrew McCallum. 2005. Joint parsing and semantic role labeling. In Proceedings of the Ninth Conference on Computational Natural Language Learning. Ann Arbor, Michigan, pages 225–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Tan</author>
<author>Wenli Zhou</author>
<author>Lei Zheng</author>
<author>Shaojun Wang</author>
</authors>
<title>A large scale distributed syntactic, semantic and lexical language model for machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Volume 1.</booktitle>
<pages>201--210</pages>
<location>Portland, OR,</location>
<contexts>
<context position="2253" citStr="Tan et al., 2011" startWordPosition="343" endWordPosition="346">g is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only has access to the left context (words a1 ...an−1); the right context (words an+1 ...aN) is not known yet. This can lead to local ambiguity, i.e., produce additional syntactic analyses that are valid for the sentence prefix, but become invalid as the right context is processed. As an example consider the sentence prefix in (1): (1) The athlete realized her </context>
</contexts>
<marker>Tan, Zhou, Zheng, Wang, 2011</marker>
<rawString>Tan, Ming, Wenli Zhou, Lei Zheng, and Shaojun Wang. 2011. A large scale distributed syntactic, semantic and lexical language model for machine translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Volume 1. Portland, OR, pages 201– 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J SpiveyKnowlton</author>
<author>Kathleen M Eberhard</author>
<author>Julie C Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science</journal>
<pages>268--1632</pages>
<contexts>
<context position="1320" citStr="Tanenhaus et al., 1995" startWordPosition="199" endWordPosition="202">AG parser with an incremental semantic role labeler in a discriminative framework. We show a substantial improvement in parsing performance compared to the baseline parser, both in full-sentence F-score and in incremental F-score. 1 Introduction When humans listen to speech, the input becomes available gradually as the speech signal unfolds. Reading happens in a similarly gradual manner when the eyes scan a text. There is good evidence that the human language processor is adapted to this and works incrementally, i.e., computes an interpretation for an incoming sentence on a wordby-word basis (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Also language processing systems often deal with speech as it is spoken, or text as it is typed. A dialogue system should start interpreting a sentence while it is spoken, and an information retrieval system should start retrieving results while the user is typing. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010)</context>
</contexts>
<marker>Tanenhaus, SpiveyKnowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Tanenhaus, Michael K., Michael J. SpiveyKnowlton, Kathleen M. Eberhard, and Julie C. Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science 268:1632–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Online graph planarisation for synchronous parsing of semantic and syntactic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Jont Conference on Artifical Intelligence.</booktitle>
<pages>1562--1567</pages>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="29217" citStr="Titov et al. (2009)" startWordPosition="4889" endWordPosition="4892">pply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and previous content to output partial dependency trees; then they output role labels given the full parser output. In contrast to all the joint approaches, we perform both parsing and s</context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Titov, Ivan, James Henderson, Paola Merlo, and Gabriele Musillo. 2009. Online graph planarisation for synchronous parsing of semantic and syntactic dependencies. In Proceedings of the 21st International Jont Conference on Artifical Intelligence. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, pages 1562– 1567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="28990" citStr="Toutanova et al. (2008)" startWordPosition="4850" endWordPosition="4853">RL features. Even though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce </context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Toutanova, Kristina, Aria Haghighi, and Christopher D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics 34(2):161–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James Curran</author>
</authors>
<title>Adding noun phrase structure to the penn treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>240--247</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="23804" citStr="Vadas and Curran (2007)" startWordPosition="3987" endWordPosition="3990">s and syntactic features. Finally, our baseline is the PLTAG parser of Demberg et al. (2013), using the original probability model without any re-ranking. A comparison with other incremental parsers would be desirable, but is not trivial to achieve. This is because the PLTAG parser is trained and evaluated on a version of the Penn Treebank that was converted to PLTAG format. This renders our results not directly comparable to parsers that reproduce the Penn Treebank bracketing. For example, the PLTAG parser produces deeper tree structures informed by Propbank and the noun phrase annotation of Vadas and Curran (2007). 10 20 30 40 Prefix Length Figure 3: Incremental parsing F-score for increasing sentence prefixes, up to 40 words. 6.3 Results Figure 3 gives the results of evaluating incremental parsing performance. The x-axis shows prefix length, and the y-axis shows incremental F-score computed as suggested by Sangati and Keller (2013). Each point is averaged over all prefixes of a given length in the test set. To quantify the trends shown in this figure, we also compute the area under the curve (AUC) for each feature combination; this is given in Table 1. We find that TREE performs consistently better th</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>Vadas, David and James Curran. 2007. Adding noun phrase structure to the penn treebank. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics, Prague, Czech Republic, pages 240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Volokh</author>
<author>G¨unter Neumann</author>
</authors>
<title>Organizing Committee, chapter A Puristic Approach for Joint Dependency Parsing and Semantic Role Labeling,</title>
<date>2008</date>
<booktitle>Proceedings of the Twelfth Conference on Computational Natural Language Learning, Coling</booktitle>
<pages>213--217</pages>
<contexts>
<context position="29532" citStr="Volokh and Neumann (2008)" startWordPosition="4938" endWordPosition="4941">rform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and previous content to output partial dependency trees; then they output role labels given the full parser output. In contrast to all the joint approaches, we perform both parsing and semantic role labeling strictly incrementally, without having access to the whole sentence, outputting prefix trees and iSRL triples for every sentence prefix. Our approach creates a feedback loop, i.e., we generate a prefix tree using the baseline model, give it as input to iSRL, then re-rank it using a set of syn</context>
</contexts>
<marker>Volokh, Neumann, 2008</marker>
<rawString>Volokh, Alexander and G¨unter Neumann. 2008. Proceedings of the Twelfth Conference on Computational Natural Language Learning, Coling 2008 Organizing Committee, chapter A Puristic Approach for Joint Dependency Parsing and Semantic Role Labeling, pages 213–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>A study on richer syntactic dependencies for structured language modeling.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</booktitle>
<pages>191--198</pages>
<location>Philadelphia,</location>
<contexts>
<context position="2190" citStr="Xu et al., 2002" startWordPosition="333" endWordPosition="336">ieving results while the user is typing. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an, the parser only has access to the left context (words a1 ...an−1); the right context (words an+1 ...aN) is not known yet. This can lead to local ambiguity, i.e., produce additional syntactic analyses that are valid for the sentence prefix, but become invalid as the right context is processed. As an example con</context>
</contexts>
<marker>Xu, Chelba, Jelinek, 2002</marker>
<rawString>Xu, Peng, Ciprian Chelba, and Frederick Jelinek. 2002. A study on richer syntactic dependencies for structured language modeling. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Philadelphia, pages 191–198.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>