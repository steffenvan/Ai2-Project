<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.063858">
<title confidence="0.961641">
UOY: A Hypergraph Model For Word Sense Induction &amp; Disambiguation
</title>
<author confidence="0.964675">
Ioannis P. Klapaftis
</author>
<affiliation confidence="0.998997">
University of York
Department of Computer Science
</affiliation>
<email confidence="0.99246">
giannis@cs.york.ac.uk
</email>
<author confidence="0.990027">
Suresh Manandhar
</author>
<affiliation confidence="0.999742">
University of York
Department of Computer Science
</affiliation>
<email confidence="0.996796">
suresh@cs.york.ac.uk
</email>
<sectionHeader confidence="0.995625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999791071428571">
This paper is an outcome of ongoing re-
search and presents an unsupervised method
for automatic word sense induction (WSI)
and disambiguation (WSD). The induction
algorithm is based on modeling the co-
occurrences of two or more words using
hypergraphs. WSI takes place by detect-
ing high-density components in the co-
occurrence hypergraphs. WSD assigns to
each induced cluster a score equal to the sum
of weights of its hyperedges found in the lo-
cal context of the target word. Our system
participates in SemEval-2007 word sense in-
duction and discrimination task.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996362">
The majority of both supervised and unsupervised
approaches to WSD is based on the “fixed-list” of
senses paradigm where the senses of a target word
is a closed list of definitions coming from a stan-
dard dictionary (Agirre et al., 2006). Lexicographers
have long warned about the problems of such an ap-
proach, since dictionaries are not suited to this task;
they often contain general definitions, they suffer
from the lack of explicit semantic and topical rela-
tions or interconnections, and they often do not re-
flect the exact content of the context, in which the
target word appears (Veronis, 2004).
To overcome this limitation, unsupervised WSD
has moved towards inducing the senses of a target
word directly from a corpus, and then disambiguat-
ing each instance of it. Most of the work in WSI
is based on the vector space model, where the con-
text of each instance of a target word is represented
as a vector of features (e.g second-order word co-
occurrences) (Schutze, 1998; Purandare and Peder-
sen, 2004). These vectors are clustered and the re-
sulting clusters represent the induced senses. How-
ever, as shown experimentally in (Veronis, 2004),
vector-based techniques are unable to detect low-
frequency senses of a target word.
Recently, graph-based methods were employed in
WSI to isolate highly infrequent senses of a target
word. HyperLex (Veronis, 2004) and the adaptation
of PageRank (Brin and Page, 1998) in (Agirre et al.,
2006) have been shown to outperform the most fre-
quent sense (MFS) baseline in terms of supervised
recall, but they still fall short of supervised WSD
systems.
Graph-based approaches operate on a 2-
dimensional space, assuming a one-to-one relation-
ship between co-occurring words. However, this
assumption is insufficient, taking into account the
fact that two or more words are usually combined
to form a relationship of concepts in the context.
Additionally, graph-based approaches fail to model
and exploit the existence of collocations or terms
consisting of more than two words.
This paper proposes a method for WSI, which
is based on a hypergraph model operating on
a n-dimensional space. In such a model, co-
occurrences of two or more words are represented
using weighted hyperedges. A hyperedge is a more
expressive representation than a simple edge, be-
cause it is able to capture the information shared
by two or more words. Our system participates in
</bodyText>
<page confidence="0.982795">
414
</page>
<bodyText confidence="0.92112575">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 414–417,
Prague, June 2007. c�2007 Association for Computational Linguistics
SemEval-2007 word sense induction and discrimi-
nation task (SWSID) (Agirre and Soroa, 2007).
</bodyText>
<sectionHeader confidence="0.869226" genericHeader="method">
2 Sense Induction &amp; Disambiguation
</sectionHeader>
<bodyText confidence="0.999801">
This section presents the induction and disambigua-
tion algorithms.
</bodyText>
<subsectionHeader confidence="0.9705255">
2.1 Sense Induction
2.1.1 The Hypergraph Model
</subsectionHeader>
<bodyText confidence="0.99983">
A hypergraph H = (V, F) is a generalization of
a graph, which consists of a set of vertices V and a
set of hyperedges F; each hyperedge is a subset of
vertices. While an edge relates 2 vertices, a hyper-
edge relates n vertices (where n &gt; 1). In our prob-
lem, we represent each word by a vertex and any
set of co-occurring related words by a hyperedge.
In our approach, we restrict hyperedges to 2, 3 or
4 words. Figure 1 shows an example of an abstract
hypergraph model 1.
</bodyText>
<figureCaption confidence="0.998652">
Figure 1: An example of a Hypergraph
</figureCaption>
<bodyText confidence="0.999970416666667">
The degree of a vertex is the number of hyper-
edges it belongs to, and the degree of a hyperedge is
the number of vertices it contains. A path in the hy-
pergraph model is a sequence of vertices and hyper-
edges such as v1, f1, ..., vi−1, fi−1, vi, where vk are
vertices, fk are hyperedges, each hyperedge fk con-
tains vertices to its left and right in the path and no
hyperedge or vertex is repeated. The length of a path
is the number of hyperedges it contains, the distance
between two vertices is the shortest path between
them and the distance between two hyperedges is the
minimum distance of all the pairs of their vertices.
</bodyText>
<subsectionHeader confidence="0.980264">
2.1.2 Building The Hypergraph
</subsectionHeader>
<bodyText confidence="0.984900222222223">
Let bp be the base corpus from which we induce
the senses of a target word tw. Our bp consists of
BNC and all the SWSID paragraphs containing the
1Image was taken from Wikipedia (Rocchini, 2006)
target word. The total size of bp is 2000 paragraphs.
Note that if SWSID paragraphs of tw are more than
2000, BNC is not used.
In order to build the hypergraph, tw is removed
from bp and each paragraph pi is POS-tagged. Fol-
lowing the example in (Agirre et al., 2006), only
nouns are kept and lemmatised. We apply two fil-
tering heuristics. The first one is the minimum fre-
quency of nouns (parameter p1), and the second one
is the minimum size of a paragraph (parameter p2).
A key problem at this stage is the determination of
related vertices (nouns), which can be grouped into
hyperedges and the weighting of each such hyper-
edge. We deal with this problem by using associa-
tion rules (Agrawal and Srikant, 1994). Frequent hy-
peredges are detected by calculating support, which
should exceed a user-defined threshold (parameter
p3).
Let f be a candidate hyperedge and a, b, c its ver-
tices. Then freq(a, b, c) is the number of para-
graphs in bp, which contain all the vertices of f, and
n is the total size of bp. Support of f is shown in
Equation 1.
</bodyText>
<equation confidence="0.981187">
support(f) = freq(a, b, c)
n
</equation>
<bodyText confidence="0.9986042">
The weight assigned to each collected hyperedge,
f, is the average of m calculated confidences, where
m is the size of f. Let f be a hyperedge containing
the vertices a, b, c. The confidence for the rule r0 =
{a, b} =&gt; {c} is defined in Equation 2.
</bodyText>
<equation confidence="0.883866666666667">
req(a,b,c
confidence(r0) = (2)
freq(a,b)
</equation>
<bodyText confidence="0.999229666666667">
Since there is a three-way relationship among a, b
and c, we have two more rules r1 = {a, c} =&gt; {b}
and r2 = {b, c} =&gt; {a}. Hence, the weighting of
f is the average of the 3 calculated confidences. We
apply a filtering heuristic (parameter p4) to remove
hyperedges with low weights from the hypergraph.
At the end of this stage, the constructed hypergraph
is reduced, so that our hypergraph model agrees with
the one described in subsection 2.1.1.
</bodyText>
<subsectionHeader confidence="0.944783">
2.1.3 Extracting Senses
</subsectionHeader>
<bodyText confidence="0.998067">
Preliminary experiments on 10 nouns of
SensEval-3 English lexical-sample task (Mihalcea
et al., 2004) (S3LS), suggested that our hypergraphs
</bodyText>
<equation confidence="0.874163">
(1)
</equation>
<page confidence="0.980106">
415
</page>
<bodyText confidence="0.999792888888889">
are small-world networks, since they exhibited
a high clustering coefficient and a small average
path length. Furthermore, the frequency of vertices
with a given degree plotted against the degree
showed that our hypergraphs satisfy a power-law
distribution P(d) = c * d−1, where d is the vertex
degree, P(d) is the frequency of vertices with
degree d. Figure 2 shows the log-log plot for the
noun difference of S3LS.
</bodyText>
<figureCaption confidence="0.99713">
Figure 2: Log-log plot for the noun difference.
</figureCaption>
<bodyText confidence="0.999994052631579">
In order to extract the senses of the target word,
we modify the HyperLex algorithm (Veronis, 2004)
for selecting the root hubs of the hypergraph as fol-
lows. At each step, the algorithm finds the vertex vi
with the highest degree, which is selected as a root
hub, according to two criteria.
The first one is the minimum number of hyper-
edges it belongs to (parameter p5), and the second is
the average weight of the first p5 hyperedges (para-
meter p6) 2. If these criteria are satisfied, then hyper-
edges containing vi are grouped to a single cluster cj
(new sense) with a 0 distance from vi, and removed
from the hypergraph. The process stops, when there
is no vertex eligible to be a root hub.
Each remaining hyperedge, fk, is assigned to the
cluster, cj, closest to it, by calculating the minimum
distance between fk and each hyperedge of cj as de-
fined in subsection 2.1.1. The weight assigned to fk
is inversely proportional to its distance from cj.
</bodyText>
<subsectionHeader confidence="0.998739">
2.2 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.9998615">
Given an instance of the target word, tw, paragraph
pi containing tw is POS-tagged, nouns are kept and
</bodyText>
<subsectionHeader confidence="0.439826">
2Hyperedges are sorted in decreasing order of weight
</subsectionHeader>
<bodyText confidence="0.996027666666667">
lemmatised. Next, each induced cluster cj is as-
signed a score equal to the sum of weights of its
hyperedges found in pi.
</bodyText>
<sectionHeader confidence="0.99938" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999729">
3.1 Preliminary Experiments
</subsectionHeader>
<bodyText confidence="0.992183">
This method is an outcome of ongoing research.
Due to time restrictions we were able to test and
tune (Table 1), but not optimize, our system only on
a very small set of nouns of S3LS targeting at a high
supervised recall. Our supervised recall on the 10
first nouns of S3LS was 66.8%, 9.8% points above
the MFS baseline.
</bodyText>
<table confidence="0.996944428571429">
Parameter Value
p1:Minimum frequency of a noun 8
p2:Minimum size of a paragraph 4
p3:Support threshold 0.002
p4:Average confidence threshold 0.2
pS:Minimum number of hyperedges 6
p6:Minimum average weight of hyperedges 0.25
</table>
<tableCaption confidence="0.99984">
Table 1: Chosen parameters for our system
</tableCaption>
<subsectionHeader confidence="0.9989">
3.2 SemEval-2007 Results
</subsectionHeader>
<bodyText confidence="0.99874925">
Tables 2 and 3 show the average supervised recall,
FScore, entropy and purity of our system on nouns
and verbs of the test data respectively. The submit-
ted answer consisted only of the winning cluster per
instance of a target word, in effect assigning it with
weight 1 (default).
Entropy measures how well the various gold stan-
dard senses are distributed within each cluster, while
purity measures how pure a cluster is, containing ob-
jects from primarily one class. In general, the lower
the entropy and the larger the purity values, the bet-
ter the clustering algorithm performs.
</bodyText>
<table confidence="0.9996114">
Measure Proposed methodology MFS
Entropy 25.5 46.3
Purity 89.8 82.4
FScore 65.8 80.7
Sup. Recall 81.6 80.9
</table>
<tableCaption confidence="0.999687">
Table 2: System performance for nouns.
</tableCaption>
<bodyText confidence="0.999549">
For nouns our system achieves a low entropy and
a high purity outperforming the MFS baseline, but a
lower FScore. This can be explained by the fact that
the average number of clusters we produce for nouns
is 11, while the gold standard average of senses is
around 2.8. For verbs the performance of our system
</bodyText>
<page confidence="0.998069">
416
</page>
<bodyText confidence="0.999788583333333">
is worse than for nouns, although entropy and purity
still outperform the MFS baseline. FScore is very
low, despite that the average number of clusters we
produce for verbs (around 8) is less than the number
of clusters we produce for nouns. This means that
for verbs the senses of gold standard are much more
spread among induced clusters than for nouns, caus-
ing a low unsupervised recall. Overall, FScore re-
sults are in accordance with the idea of microsenses
mentioned in (Agirre et al., 2006). FScore is biased
towards clusters similar to the gold standard senses
and cannot capture that theory.
</bodyText>
<table confidence="0.9993878">
Measure Proposed methodology MFS
Entropy 28.9 44.4
Purity 82.0 77
F-score 45.1 76.8
Sup. Recall 73.3 76.2
</table>
<tableCaption confidence="0.999914">
Table 3: System performance for verbs.
</tableCaption>
<bodyText confidence="0.999788928571429">
Our supervised recall for verbs is 73.3%, and be-
low the MFS baseline (76.2%), which no system
managed to outperform. For nouns our supervised
recall is 81.6%, which is around 0.7% above the
MFS baseline. In order to fully examine the perfor-
mance of our system we applied a second evaluation
of our methodology using the SWSID official soft-
ware.
The solution per target word instance included the
entire set of clusters with their associated weights
(Table 4). Results show that the submitted answer
(instance - winning cluster), was degrading seri-
ously our performance both for verbs and nouns due
to the loss of information in the mapping step.
</bodyText>
<table confidence="0.99933125">
POS Proposed Methodology MFS
Nouns 84.3 80.9
Verbs 75.6 76.2
Total 80.2 78.7
</table>
<tableCaption confidence="0.999716">
Table 4: Supervised recall in second evaluation.
</tableCaption>
<bodyText confidence="0.9999365">
Our supervised recall for nouns has outperformed
the MFS baseline by 3.4% with the best system
achieving 86.8%. Performance for verbs is 75.6%,
0.6% below the best system and MFS.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999983">
We have presented a hypergraph model for word
sense induction and disambiguation. Preliminary
experiments suggested that our reduced hypergraphs
are small-world networks. WSI identifies the highly
connected components (hubs) in the hypergraph,
while WSD assigns to each cluster a score equal to
the sum of weights of its hyperedges found in the
local context of a target word.
Results show that our system achieves high en-
tropy and purity performance outperforming the
MFS baseline. Our methodology achieves a low
FScore producing clusters that are dissimilar to the
gold standard senses. Our supervised recall for
nouns is 3.4% above the MFS baseline. For verbs,
our supervised recall is below the MFS baseline,
which no system managed to outperform.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999774266666667">
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
2: Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007. ACL.
Eneko Agirre, David Martinez, Oier L´opez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art wsd. In Proceedings of the EMNLP
Conference, pages 585–593. ACL.
Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast
algorithms for mining association rules in large data-
bases. In VLDB ’94: Proceedings of the 20th Inter-
national Conference on Very Large DataBases, pages
487–499, USA. Morgan Kaufmann Publishers Inc.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1–7):107–117.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The senseval-3 english lexical sample task.
In R. Mihaleca and P. Edmonds, editors, SensEval-3
Proceedings, pages 25–28, Spain, July. ACL.
Amruta Purandare and Ted Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of CoNLL-2004,
pages 41–48. ACL.
Claudio Rocchini. 2006. Hypergraph sample image.
Wikipedia.
Hinrich Schutze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–123.
Jean Veronis. 2004. Hyperlex:lexical cartography for
information retrieval. Computer Speech &amp; Language,
18(3).
</reference>
<page confidence="0.997456">
417
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.677852">
<title confidence="0.999828">UOY: A Hypergraph Model For Word Sense Induction &amp; Disambiguation</title>
<author confidence="0.998359">Ioannis P Klapaftis</author>
<affiliation confidence="0.9994135">University of York Department of Computer Science</affiliation>
<email confidence="0.97759">giannis@cs.york.ac.uk</email>
<author confidence="0.737237">Suresh Manandhar</author>
<affiliation confidence="0.9988315">University of York Department of Computer Science</affiliation>
<email confidence="0.993467">suresh@cs.york.ac.uk</email>
<abstract confidence="0.9962104">This paper is an outcome of ongoing research and presents an unsupervised method for automatic word sense induction (WSI) and disambiguation (WSD). The induction algorithm is based on modeling the cooccurrences of two or more words using hypergraphs. WSI takes place by detecting high-density components in the cooccurrence hypergraphs. WSD assigns to each induced cluster a score equal to the sum of weights of its hyperedges found in the local context of the target word. Our system participates in SemEval-2007 word sense induction and discrimination task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 2: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007. ACL.</booktitle>
<contexts>
<context position="3484" citStr="Agirre and Soroa, 2007" startWordPosition="549" endWordPosition="552">ses a method for WSI, which is based on a hypergraph model operating on a n-dimensional space. In such a model, cooccurrences of two or more words are represented using weighted hyperedges. A hyperedge is a more expressive representation than a simple edge, because it is able to capture the information shared by two or more words. Our system participates in 414 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 414–417, Prague, June 2007. c�2007 Association for Computational Linguistics SemEval-2007 word sense induction and discrimination task (SWSID) (Agirre and Soroa, 2007). 2 Sense Induction &amp; Disambiguation This section presents the induction and disambiguation algorithms. 2.1 Sense Induction 2.1.1 The Hypergraph Model A hypergraph H = (V, F) is a generalization of a graph, which consists of a set of vertices V and a set of hyperedges F; each hyperedge is a subset of vertices. While an edge relates 2 vertices, a hyperedge relates n vertices (where n &gt; 1). In our problem, we represent each word by a vertex and any set of co-occurring related words by a hyperedge. In our approach, we restrict hyperedges to 2, 3 or 4 words. Figure 1 shows an example of an abstrac</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 2: Evaluating word sense induction and discrimination systems. In Proceedings of SemEval-2007. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
<author>Oier L´opez de Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Two graph-based algorithms for state-of-the-art wsd.</title>
<date>2006</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>585--593</pages>
<publisher>ACL.</publisher>
<marker>Agirre, Martinez, de Lacalle, Soroa, 2006</marker>
<rawString>Eneko Agirre, David Martinez, Oier L´opez de Lacalle, and Aitor Soroa. 2006. Two graph-based algorithms for state-of-the-art wsd. In Proceedings of the EMNLP Conference, pages 585–593. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Ramakrishnan Srikant</author>
</authors>
<title>Fast algorithms for mining association rules in large databases.</title>
<date>1994</date>
<booktitle>In VLDB ’94: Proceedings of the 20th International Conference on Very Large DataBases,</booktitle>
<pages>487--499</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>USA.</location>
<contexts>
<context position="5704" citStr="Agrawal and Srikant, 1994" startWordPosition="957" endWordPosition="960">of tw are more than 2000, BNC is not used. In order to build the hypergraph, tw is removed from bp and each paragraph pi is POS-tagged. Following the example in (Agirre et al., 2006), only nouns are kept and lemmatised. We apply two filtering heuristics. The first one is the minimum frequency of nouns (parameter p1), and the second one is the minimum size of a paragraph (parameter p2). A key problem at this stage is the determination of related vertices (nouns), which can be grouped into hyperedges and the weighting of each such hyperedge. We deal with this problem by using association rules (Agrawal and Srikant, 1994). Frequent hyperedges are detected by calculating support, which should exceed a user-defined threshold (parameter p3). Let f be a candidate hyperedge and a, b, c its vertices. Then freq(a, b, c) is the number of paragraphs in bp, which contain all the vertices of f, and n is the total size of bp. Support of f is shown in Equation 1. support(f) = freq(a, b, c) n The weight assigned to each collected hyperedge, f, is the average of m calculated confidences, where m is the size of f. Let f be a hyperedge containing the vertices a, b, c. The confidence for the rule r0 = {a, b} =&gt; {c} is defined i</context>
</contexts>
<marker>Agrawal, Srikant, 1994</marker>
<rawString>Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast algorithms for mining association rules in large databases. In VLDB ’94: Proceedings of the 20th International Conference on Very Large DataBases, pages 487–499, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<pages>30--1</pages>
<contexts>
<context position="2242" citStr="Brin and Page, 1998" startWordPosition="356" endWordPosition="359"> WSI is based on the vector space model, where the context of each instance of a target word is represented as a vector of features (e.g second-order word cooccurrences) (Schutze, 1998; Purandare and Pedersen, 2004). These vectors are clustered and the resulting clusters represent the induced senses. However, as shown experimentally in (Veronis, 2004), vector-based techniques are unable to detect lowfrequency senses of a target word. Recently, graph-based methods were employed in WSI to isolate highly infrequent senses of a target word. HyperLex (Veronis, 2004) and the adaptation of PageRank (Brin and Page, 1998) in (Agirre et al., 2006) have been shown to outperform the most frequent sense (MFS) baseline in terms of supervised recall, but they still fall short of supervised WSD systems. Graph-based approaches operate on a 2- dimensional space, assuming a one-to-one relationship between co-occurring words. However, this assumption is insufficient, taking into account the fact that two or more words are usually combined to form a relationship of concepts in the context. Additionally, graph-based approaches fail to model and exploit the existence of collocations or terms consisting of more than two word</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems, 30(1–7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The senseval-3 english lexical sample task.</title>
<date>2004</date>
<booktitle>SensEval-3 Proceedings,</booktitle>
<pages>25--28</pages>
<editor>In R. Mihaleca and P. Edmonds, editors,</editor>
<publisher>ACL.</publisher>
<contexts>
<context position="6932" citStr="Mihalcea et al., 2004" startWordPosition="1180" endWordPosition="1183">tion 2. req(a,b,c confidence(r0) = (2) freq(a,b) Since there is a three-way relationship among a, b and c, we have two more rules r1 = {a, c} =&gt; {b} and r2 = {b, c} =&gt; {a}. Hence, the weighting of f is the average of the 3 calculated confidences. We apply a filtering heuristic (parameter p4) to remove hyperedges with low weights from the hypergraph. At the end of this stage, the constructed hypergraph is reduced, so that our hypergraph model agrees with the one described in subsection 2.1.1. 2.1.3 Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al., 2004) (S3LS), suggested that our hypergraphs (1) 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. Furthermore, the frequency of vertices with a given degree plotted against the degree showed that our hypergraphs satisfy a power-law distribution P(d) = c * d−1, where d is the vertex degree, P(d) is the frequency of vertices with degree d. Figure 2 shows the log-log plot for the noun difference of S3LS. Figure 2: Log-log plot for the noun difference. In order to extract the senses of the target word, we modify the HyperLex algorithm (Ve</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. 2004. The senseval-3 english lexical sample task. In R. Mihaleca and P. Edmonds, editors, SensEval-3 Proceedings, pages 25–28, Spain, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Ted Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004,</booktitle>
<pages>41--48</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1837" citStr="Purandare and Pedersen, 2004" startWordPosition="293" endWordPosition="297">general definitions, they suffer from the lack of explicit semantic and topical relations or interconnections, and they often do not reflect the exact content of the context, in which the target word appears (Veronis, 2004). To overcome this limitation, unsupervised WSD has moved towards inducing the senses of a target word directly from a corpus, and then disambiguating each instance of it. Most of the work in WSI is based on the vector space model, where the context of each instance of a target word is represented as a vector of features (e.g second-order word cooccurrences) (Schutze, 1998; Purandare and Pedersen, 2004). These vectors are clustered and the resulting clusters represent the induced senses. However, as shown experimentally in (Veronis, 2004), vector-based techniques are unable to detect lowfrequency senses of a target word. Recently, graph-based methods were employed in WSI to isolate highly infrequent senses of a target word. HyperLex (Veronis, 2004) and the adaptation of PageRank (Brin and Page, 1998) in (Agirre et al., 2006) have been shown to outperform the most frequent sense (MFS) baseline in terms of supervised recall, but they still fall short of supervised WSD systems. Graph-based appr</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>Amruta Purandare and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proceedings of CoNLL-2004, pages 41–48. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Rocchini</author>
</authors>
<date>2006</date>
<note>Hypergraph sample image. Wikipedia.</note>
<contexts>
<context position="4993" citStr="Rocchini, 2006" startWordPosition="830" endWordPosition="831">i−1, vi, where vk are vertices, fk are hyperedges, each hyperedge fk contains vertices to its left and right in the path and no hyperedge or vertex is repeated. The length of a path is the number of hyperedges it contains, the distance between two vertices is the shortest path between them and the distance between two hyperedges is the minimum distance of all the pairs of their vertices. 2.1.2 Building The Hypergraph Let bp be the base corpus from which we induce the senses of a target word tw. Our bp consists of BNC and all the SWSID paragraphs containing the 1Image was taken from Wikipedia (Rocchini, 2006) target word. The total size of bp is 2000 paragraphs. Note that if SWSID paragraphs of tw are more than 2000, BNC is not used. In order to build the hypergraph, tw is removed from bp and each paragraph pi is POS-tagged. Following the example in (Agirre et al., 2006), only nouns are kept and lemmatised. We apply two filtering heuristics. The first one is the minimum frequency of nouns (parameter p1), and the second one is the minimum size of a paragraph (parameter p2). A key problem at this stage is the determination of related vertices (nouns), which can be grouped into hyperedges and the wei</context>
</contexts>
<marker>Rocchini, 2006</marker>
<rawString>Claudio Rocchini. 2006. Hypergraph sample image. Wikipedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="1806" citStr="Schutze, 1998" startWordPosition="291" endWordPosition="292"> often contain general definitions, they suffer from the lack of explicit semantic and topical relations or interconnections, and they often do not reflect the exact content of the context, in which the target word appears (Veronis, 2004). To overcome this limitation, unsupervised WSD has moved towards inducing the senses of a target word directly from a corpus, and then disambiguating each instance of it. Most of the work in WSI is based on the vector space model, where the context of each instance of a target word is represented as a vector of features (e.g second-order word cooccurrences) (Schutze, 1998; Purandare and Pedersen, 2004). These vectors are clustered and the resulting clusters represent the induced senses. However, as shown experimentally in (Veronis, 2004), vector-based techniques are unable to detect lowfrequency senses of a target word. Recently, graph-based methods were employed in WSI to isolate highly infrequent senses of a target word. HyperLex (Veronis, 2004) and the adaptation of PageRank (Brin and Page, 1998) in (Agirre et al., 2006) have been shown to outperform the most frequent sense (MFS) baseline in terms of supervised recall, but they still fall short of supervise</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>Hinrich Schutze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Veronis</author>
</authors>
<title>Hyperlex:lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="1431" citStr="Veronis, 2004" startWordPosition="225" endWordPosition="226"> Introduction The majority of both supervised and unsupervised approaches to WSD is based on the “fixed-list” of senses paradigm where the senses of a target word is a closed list of definitions coming from a standard dictionary (Agirre et al., 2006). Lexicographers have long warned about the problems of such an approach, since dictionaries are not suited to this task; they often contain general definitions, they suffer from the lack of explicit semantic and topical relations or interconnections, and they often do not reflect the exact content of the context, in which the target word appears (Veronis, 2004). To overcome this limitation, unsupervised WSD has moved towards inducing the senses of a target word directly from a corpus, and then disambiguating each instance of it. Most of the work in WSI is based on the vector space model, where the context of each instance of a target word is represented as a vector of features (e.g second-order word cooccurrences) (Schutze, 1998; Purandare and Pedersen, 2004). These vectors are clustered and the resulting clusters represent the induced senses. However, as shown experimentally in (Veronis, 2004), vector-based techniques are unable to detect lowfreque</context>
<context position="7544" citStr="Veronis, 2004" startWordPosition="1283" endWordPosition="1284">4) (S3LS), suggested that our hypergraphs (1) 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. Furthermore, the frequency of vertices with a given degree plotted against the degree showed that our hypergraphs satisfy a power-law distribution P(d) = c * d−1, where d is the vertex degree, P(d) is the frequency of vertices with degree d. Figure 2 shows the log-log plot for the noun difference of S3LS. Figure 2: Log-log plot for the noun difference. In order to extract the senses of the target word, we modify the HyperLex algorithm (Veronis, 2004) for selecting the root hubs of the hypergraph as follows. At each step, the algorithm finds the vertex vi with the highest degree, which is selected as a root hub, according to two criteria. The first one is the minimum number of hyperedges it belongs to (parameter p5), and the second is the average weight of the first p5 hyperedges (parameter p6) 2. If these criteria are satisfied, then hyperedges containing vi are grouped to a single cluster cj (new sense) with a 0 distance from vi, and removed from the hypergraph. The process stops, when there is no vertex eligible to be a root hub. Each r</context>
</contexts>
<marker>Veronis, 2004</marker>
<rawString>Jean Veronis. 2004. Hyperlex:lexical cartography for information retrieval. Computer Speech &amp; Language, 18(3).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>