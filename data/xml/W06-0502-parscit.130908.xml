<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009359">
<title confidence="0.991439">
Multilingual Ontology Acquisition from Multiple MRDs
</title>
<author confidence="0.997193">
Eric Nichols♭, Francis Bond♮, Takaaki Tanaka♮, Sanae Fujita♮, Dan Flickinger ♯
</author>
<affiliation confidence="0.928189">
♭ Nara Inst. of Science and Technology ♮ NTT Communication Science Labs ♯ Stanford University
Grad. School of Information Science Natural Language Research Group CSLI
Nara, Japan Keihanna, Japan Stanford, CA
</affiliation>
<email confidence="0.996848">
eric-n@is.naist.jp {bond,takaaki,sanae}@cslab.kecl.ntt.co.jp danf@csli.stanford.edu
</email>
<sectionHeader confidence="0.993862" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909772727273">
In this paper, we outline the develop-
ment of a system that automatically con-
structs ontologies by extracting knowledge
from dictionary definition sentences us-
ing Robust Minimal Recursion Semantics
(RMRS). Combining deep and shallow
parsing resource through the common for-
malism of RMRS allows us to extract on-
tological relations in greater quantity and
quality than possible with any of the meth-
ods independently. Using this method,
we construct ontologies from two differ-
ent Japanese lexicons and one English lex-
icon. We then link them to existing, hand-
crafted ontologies, aligning them at the
word-sense level. This alignment provides
a representative evaluation of the qual-
ity of the relations being extracted. We
present the results of this ontology con-
struction and discuss how our system was
designed to handle multiple lexicons and
languages.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985160714286">
Automatic methods of ontology acquisition have a
long history in the field of natural language pro-
cessing. The information contained in ontolo-
gies is important for a number of tasks, for ex-
ample word sense disambiguation, question an-
swering and machine translation. In this paper,
we present the results of experiments conducted
in automatic ontological acquisition over two lan-
guages, English and Japanese, and from three dif-
ferent machine-readable dictionaries.
Useful semantic relations can be extracted from
large corpora using relatively simple patterns (e.g.,
(Pantel et al., 2004)). While large corpora often
contain information not found in lexicons, even a
very large corpus may not include all the familiar
words of a language, let alone those words occur-
ring in useful patterns (Amano and Kondo, 1999).
Therefore it makes sense to also extract data from
machine readable dictionaries (MRDs).
There is a great deal of work on the creation
of ontologies from machine readable dictionaries
(a good summary is (Wilkes et al., 1996)), mainly
for English. Recently, there has also been inter-
est in Japanese (Tokunaga et al., 2001; Nichols
et al., 2005). Most approaches use either a special-
ized parser or a set of regular expressions tuned
to a particular dictionary, often with hundreds of
rules. Agirre et al. (2000) extracted taxonomic
relations from a Basque dictionary with high ac-
curacy using Constraint Grammar together with
hand-crafted rules. However, such a system is lim-
ited to one language, and it has yet to be seen
how the rules will scale when deeper semantic re-
lations are extracted. In comparison, as we will
demonstrate, our system produces comparable re-
sults while the framework is immediately applica-
ble to any language with the resources to produce
RMRS. Advances in the state-of-the-art in pars-
ing have made it practical to use deep processing
systems that produce rich syntactic and semantic
analyses to parse lexicons. This high level of se-
mantic information makes it easy to identify the
relations between words that make up an ontol-
ogy. Such an approach was taken by the MindNet
project (Richardson et al., 1998). However, deep
parsing systems often suffer from small lexicons
and large amounts of parse ambiguity, making it
difficult to apply this knowledge broadly.
Our ontology extraction system uses Robust
Minimal Recursion Semantics (RMRS), a formal-
ism that provides a high level of detail while, at
the same time, allowing for the flexibility of un-
derspecification. RMRS encodes syntactic infor-
mation in a general enough manner to make pro-
cessing of and extraction from syntactic phenom-
ena including coordination, relative clause analy-
</bodyText>
<page confidence="0.984318">
10
</page>
<note confidence="0.731758">
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 10–17,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999975625">
sis and the treatment of argument structure from
verbs and verbal nouns. It provides a common for-
mat for naming semantic relations, allowing them
to be generalized over languages. Because of this,
we are able to extend our system to cover new lan-
guages that have RMRS resourses available with
a minimal amount of effort. The underspecifica-
tion mechanism in RMRS makes it possible for us
to produce input that is compatible with our sys-
tem from a variety of different parsers. By select-
ing parsers of various different levels of robustness
and informativeness, we avoid the coverage prob-
lem that is classically associated with approaches
using deep-processing; using heterogeneous pars-
ing resources maximizes the quality and quantity
of ontological relations extracted. Currently, our
system uses input from parsers from three lev-
els: with morphological analyzers the shallowest,
parsers using Head-driven Phrase Structure Gram-
mars (HPSG) the deepest and dependency parsers
providing a middle ground.
Our system was initially developed for one
Japanese dictionary (Lexeed). The use of the ab-
stract formalism, RMRS, made it easy to extend to
a different Japanese lexicon (Iwanami) and even a
lexicon in a different language (GCIDE).
Section 2 provides a description of RMRS and
the tools used by our system. The ontological ac-
quisition system is presented in Section 3. The re-
sults of evaluating our ontologies by comparison
with existing resources are given in Section 4. We
discuss our findings in Section 5.
</bodyText>
<sectionHeader confidence="0.99917" genericHeader="introduction">
2 Resources
</sectionHeader>
<subsectionHeader confidence="0.995309">
2.1 The Lexeed Semantic Database of
Japanese
</subsectionHeader>
<bodyText confidence="0.999926923076923">
The Lexeed Semantic Database of Japanese is a
machine readable dictionary that covers the most
familiar open class words in Japanese as measured
by a series of psycholinguistic experiments (Kasa-
hara et al., 2004). Lexeed consists of all open class
words with a familiarity greater than or equal to
five on a scale of one to seven. This gives 28,000
words divided into 46,000 senses and defined with
75,000 definition sentences. All definition sen-
tences and example sentences have been rewritten
to use only the 28,000 familiar open class words.
The definition and example sentences have been
treebanked with the JACY grammar (§ 2.4.2).
</bodyText>
<subsectionHeader confidence="0.996486">
2.2 The Iwanami Dictionary of Japanese
</subsectionHeader>
<bodyText confidence="0.9996987">
The Iwanami Kokugo Jiten (Iwanami) (Nishio
et al., 1994) is a concise Japanese dictionary.
A machine tractable version was made avail-
able by the Real World Computing Project for
the SENSEVAL-2 Japanese lexical task (Shirai,
2003). Iwanami has 60,321 headwords and 85,870
word senses. Each sense in the dictionary con-
sists of a sense ID and morphological information
(word segmentation, POS tag, base form and read-
ing, all manually post-edited).
</bodyText>
<subsectionHeader confidence="0.9975535">
2.3 The Gnu Contemporary International
Dictionary of English
</subsectionHeader>
<bodyText confidence="0.999548192307692">
The GNU Collaborative International Dictionary
of English (GCIDE) is a freely available dic-
tionary of English based on Webster’s Revised
Unabridged Dictionary (published in 1913), and
supplemented with entries from WordNet and ad-
ditional submissions from users. It currently
contains over 148,000 definitions. The version
used in this research is formatted in XML and is
available for download from www.ibiblio.org/
webster/.
We arranged the headwords by frequency and
segmented their definition sentences into sub-
sentences by tokenizing on semicolons (;). This
produced a total of 397,460 pairs of headwords
and sub-sentences, for an average of slightly less
than four sub-sentences per definition sentence.
For corpus data, we selected the first 100,000 def-
inition sub-sentences of the headwords with the
highest frequency. This subset of definition sen-
tences contains 12,440 headwords with 36,313
senses, covering approximately 25% of the defi-
nition sentences in the GCIDE. The GCIDE has
the most polysemy of the lexicons used in this re-
search. It averages over 3 senses per word defined
in comparison to Lexeed and Iwanami which both
have less than 2.
</bodyText>
<subsectionHeader confidence="0.999576">
2.4 Parsing Resources
</subsectionHeader>
<bodyText confidence="0.993099">
We used Robust Minimal Recursion Semantics
(RMRS) designed as part of the Deep Thought
project (Callmeier et al., 2004) as the formal-
ism for our ontological relation extraction en-
gine. We used deep-processing tools from the
Deep Linguistic Processing with HPSG Initiative
(DELPH-IN: http://www.delph-in.net/) as
well as medium- and shallow-processing tools for
Japanese processing (the morphological analyzer
</bodyText>
<page confidence="0.998576">
11
</page>
<bodyText confidence="0.9937905">
ChaSen and the dependency parser CaboCha)
from the Matsumoto Laboratory.
</bodyText>
<subsectionHeader confidence="0.908702">
2.4.1 Robust Minimal Recursion Semantics
</subsectionHeader>
<bodyText confidence="0.99942976744186">
Robust Minimal Recursion Semantics is a form
of flat semantics which is designed to allow deep
and shallow processing to use a compatible se-
mantic representation, with fine-grained atomic
components of semantic content so shallow meth-
ods can contribute just what they know, yet with
enough expressive power for rich semantic content
including generalized quantifiers (Frank, 2004).
The architecture of the representation is based on
Minimal Recursion Semantics (Copestake et al.,
2005), including a bag of labeled elementary pred-
icates (EPs) and their arguments, a list of scoping
constraints which enable scope underspecification,
and a handle that provides a hook into the repre-
sentation.
The representation can be underspecified in
three ways: relationships can be omitted (such
as quantifiers, messages, conjunctions and so on);
predicate-argument relations can be omitted; and
predicate names can be simplified. Predicate
names are defined in such a way as to be as
compatible (predictable) as possible among differ-
ent analysis engines, using a lemma pos subsense
naming convention, where the subsense is optional
and the part-of-speech (pos) for coarse-grained
sense distinctions is drawn from a small set of gen-
eral types (noun, verb, sahen (verbal noun), ... ).
The predicate unten s (A+Kunten “drive”), for
example, is less specific than unten s 2 and thus
subsumes it. In order to simplify the combination
of different analyses, the EPs are indexed to the
corresponding character positions in the original
input sentence.
Examples of deep and shallow results for the
same sentenceHft*�A+Kt7DJ�,jid¯osha wo
unten suru hito “a person who drives a car (lit:
car-ACC drive do person)” are given in Figures 1
and 2 (omitting the indexing). Real predicates are
prefixed by an under-bar ( ). The deep parse gives
information about the scope, message types and
argument structure, while the shallow parse gives
little more than a list of real and grammatical pred-
icates with a hook.
</bodyText>
<subsectionHeader confidence="0.435792">
2.4.2 Deep Parsers (JACY, ERG and PET)
</subsectionHeader>
<bodyText confidence="0.999213450980392">
For both Japanese and English, we used the PET
System for the high-efficiency processing of typed
feature structures (Callmeier, 2000). For Japanese,
we used JACY (Siegel, 2000), for English we used
the English Resource Grammar (ERG: Flickinger
2000).1
JACY The JACY grammar is an HPSG-based
grammar of Japanese which originates from work
done in the Verbmobil project (Siegel, 2000) on
machine translation of spoken dialogues in the do-
main of travel planning. It has since been ex-
tended to accommodate written Japanese and new
domains (such as electronic commerce customer
email and machine readable dictionaries).
The grammar implementation is based on a sys-
tem of types. There are around 900 lexical types
that define the syntactic, semantic and pragmatic
properties of the Japanese words, and 188 types
that define the properties of phrases and lexical
rules. The grammar includes 50 lexical rules
for inflectional and derivational morphology and
47 phrase structure rules. The lexicon contains
around 36,000 lexemes.
The English Resource Grammar (ERG) The
English Resource Grammar (ERG: (Flickinger,
2000)) is a broad-coverage, linguistically precise
grammar of English, developed within the Head-
driven Phrase Structure Grammar (HPSG) frame-
work, and designed for both parsing and gen-
eration. It was also originally launched within
the Verbmobil (Wahlster, 2000) spoken language
machine translation project for the particular do-
mains of meeting scheduling and travel planning.
The ERG has since been substantially extended in
both grammatical and lexical coverage, reaching
80-90% coverage of sizeable corpora in two ad-
ditional domains: electronic commerce customer
email and tourism brochures.
The grammar includes a hand-built lexicon of
23,000 lemmas instantiating 850 lexical types, a
highly schematic set of 150 grammar rules, and a
set of 40 lexical rules, all organized in a rich multi-
ple inheritance hierarchy of some 3000 typed fea-
ture structures. Like other DELPH-IN grammars,
the ERG can be processed by several parsers and
generators, including the LKB (Copestake, 2002)
and PET (Callmeier, 2000). Each successful ERG
analysis of a sentence or fragment includes a fine-
grained semantic representation in MRS.
For the task of parsing the dictionary defini-
tions in GCIDE (the GNU Collaborative Interna-
</bodyText>
<footnote confidence="0.958546">
1Both grammars, the LKB and PET are available at
&lt;http://www.delph-in.net/&gt;.
</footnote>
<page confidence="0.981827">
12
</page>
<figure confidence="0.994403264705883">
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎤
⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
TEXTnR*�A$$ �6�
TOP h1
jidousha n rel
LBL h4
ARG0 x5
⎤ ⎡
⎦ ⎥ ⎣
⎡
⎢ ⎣
⎡
⎤⎢
⎦ ⎢ ⎢ ⎣
udef rel
LBL h6
ARG0 x5
RSTR h7
BODY h8
RELS
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
hito n rel
LBL h12
ARG0 x10
⎡
⎣
⎤ ⎡
⎥ ⎢
⎦ ⎣
proposition m rel
LBL h10001
ARG0 e11tense=present
MARG h16
⎡
⎢ ⎣
⎡
⎤⎢
⎦ ⎢ ⎢ ⎣
⎤
⎦⎥⎥⎥
⎤ ⎡ ⎤ ⎫
unten s rel ⎪⎪
⎥ ⎢LBL h9 ⎥ ⎪⎪
⎥ ⎢ ⎥ ⎪⎪
⎥ ⎢ARG0 e11tense=present ⎥ ⎪⎪
⎦ ⎣ ⎦ ⎪⎪
ARG1 x10 ⎪⎪⎬
ARG2 x5
⎤ ⎪
unknown rel ⎪⎪⎪⎪
LBL h17 ⎥ ⎪⎪⎪
ARG0 e2tense=present ⎦ ⎪⎪⎪
ARG x10 ⎭⎪
proposition m rel
LBL h1
ARG0 e2tense=present
MARG h3
udef rel
LBL h13
ARG0 x10
RSTR h14
BODY h15
HCONS {h3 qeq h17,h7 qeq h4,h14 qeq h12,h16 qeq h9}
ING {h12 ing h10001}
</figure>
<figureCaption confidence="0.998074">
Figure 1: RMRS for the Sense 2 of doraiba- “driver” (Cabocha/JACY)
</figureCaption>
<figure confidence="0.980754833333333">
⎡
⎢ ⎢ ⎢ ⎣
TEXTnR*�A$$ �6�
TOP h9
⎧
⎨
⎩
RELS
⎤
⎦⎥⎥⎥
⎡jidousha n rel
⎣LBL h1
ARG0 x2
⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎫
o p relunten s rel suru v rel hito n rel ⎬
⎦ ⎣LBL h3 ⎦ ⎣LBL h5 ⎦⎣LBL h7 ⎦ ⎣LBL h9 ⎦
⎭
ARG0 u4 ARG0 e6 ARG0 x8 ARG0 x10
</figure>
<figureCaption confidence="0.998662">
Figure 2: RMRS for the Sense 2 of doraiba- “driver” (ChaSen)
</figureCaption>
<bodyText confidence="0.999862">
tional Dictionary of English; see below), the ERG
was minimally extended to include two additional
fragment rules, for gap-containing VPs and PPs
(idiosyncratic to this domain), and additional lex-
ical entries were manually added for all missing
words in the alphabetically first 10,000 definition
sentences.
These first 10,000 sentences were parsed and
then manually tree-banked to provide the train-
ing material for constructing the stochastic model
used for best-only parsing of the rest of the defini-
tion sentences. Using POS-based unknown-word
guessing for missing lexical entries, MRSes were
obtained for about 75% of the first 100,000 defini-
tion sentences.
</bodyText>
<subsectionHeader confidence="0.588675">
2.4.3 Medium Parser (CaboCha-RMRS)
</subsectionHeader>
<bodyText confidence="0.988175695652174">
For Japanese, we produce RMRS from the de-
pendency parser Cabocha (Kudo and Matsumoto,
2002). The method is similar to that of Spreyer
and Frank (2005), who produce RMRS from de-
tailed German dependencies. CaboCha provides
fairly minimal dependencies: there are three links
(dependent, parallel, apposition) and they link
base phrases (Japanese bunsetsu), marked with
the syntactic and semantic head. The CaboCha-
RMRS parser uses this information, along with
heuristics based on the parts-of-speech, to produce
underspecified RMRSs. CaboCha-RMRS is ca-
pable of making use of HPSG resources, includ-
ing verbal case frames, to further enrich its out-
put. This allows it to produce RMRS that ap-
proaches the granularity of the analyses given by
HPSG parsers. Indeed, CaboCha-RMRS and JACY
give identical parses for the example sentence in
Figure 1. One of our motivations in including a
medium parser in our system is to extract more re-
lations that require special processing; the flexibil-
ity of CaboCha-RMRS and the RMRS formalism
make this possible.
</bodyText>
<subsectionHeader confidence="0.723086">
2.4.4 Shallow Parser (ChaSen-RMRS)
</subsectionHeader>
<bodyText confidence="0.999963">
The part-of-speech tagger, ChaSen (Matsumoto
et al., 2000) was used for shallow processing of
Japanese. Predicate names were produced by
transliterating the pronunciation field and map-
ping the part-of-speech codes to the RMRS super
types. The part-of-speech codes were also used
to judge whether predicates were real or gram-
matical. Since Japanese is a head-final language,
the hook value was set to be the handle of the
right-most real predicate. This is easy to do for
Japanese, but difficult for English.
</bodyText>
<sectionHeader confidence="0.997607" genericHeader="method">
3 Ontology Construction
</sectionHeader>
<bodyText confidence="0.999988818181818">
We adopt the ontological relation extraction algo-
rithm used by Nichols et al. (2005). Its goal is to
identify the semantic head(s) of a dictionary def-
inition sentence – the relation(s) that best sum-
marize it. The algorithm does this by traversing
the RMRS structure of a given definition sentence
starting at the HOOK (the highest-scoping seman-
tic relationship) and following its argument struc-
ture. When the algorithm can proceed no fur-
ther, it returns the a tuple consisting of the def-
inition word and the word identified by the se-
</bodyText>
<page confidence="0.997952">
13
</page>
<bodyText confidence="0.999959">
mantic relation where the algorithm halted. Our
extended algorithm has the following characteris-
tics: sentences with only one content-bearing re-
lation are assumed to identify a synonym; spe-
cial relation processing (§ 3.1) is used to gather
meta-information and identify ontological rela-
tions; processing of coordination allows for ex-
traction of multiple ontological relations; filtering
by part-of-speech screens out unlikely relations
(§ 3.2).
</bodyText>
<subsectionHeader confidence="0.997276">
3.1 Special Relations
</subsectionHeader>
<bodyText confidence="0.999512954545455">
Occasionally, relations which provide ontological
meta-information, such as the specification of do-
main or temporal expressions, or which help iden-
tify the type of ontological relation present are en-
countered. Nichols et al. (2005) identified these
as special relations. We use a small number of
rules to determine where the semantic head is and
what ontological relation should be extracted. A
sample of the special relations are listed in Ta-
ble 1. This technique follows in a long tradition of
special treatment of certain words that have been
shown to be particularly relevant to the task of
ontology construction or which are semantically
content-free. These words or relations have also
be referred to as “empty heads”, “function nouns”,
or “relators” in the literature (Wilkes et al., 1996).
Our approach generalizes the treatment of these
special relations to rules that are portable for any
RMRS (modulo the language specific predicate
names) giving it portability that cannot be found
in approaches that use regular expressions or spe-
cialized parsers.
</bodyText>
<table confidence="0.85175675">
Special Predicate (s) Ontological
Japanese English Relation
isshu, hitotsu form, kind, one hypernym
ryaku(shou) abbreviation abbreviation
bubun, ichibu part, peice meronym
meishou name name
keishou ’polite name for’ name:honorific
zokushou ’slang for’ name:slang
</table>
<tableCaption confidence="0.9815075">
Table 1: Special predicates and their associated
ontological relations
</tableCaption>
<bodyText confidence="0.999895888888889">
Augmenting the system to work on English def-
inition sentence simply entailed writing rules to
handle special relations that occur in English. Our
system currently has 26 rules for Japanese and 50
rules for English. These rules provide process-
ing of relations like those found in Table 1, and
they also handle processing of coordinate struc-
tures, such as noun phrases joined together with
conjunctions such as and, or, and punctuation.
</bodyText>
<subsectionHeader confidence="0.999625">
3.2 Filtering by Part-of-Speech
</subsectionHeader>
<bodyText confidence="0.999967653846154">
One of the problems encountered in expanding the
approach in Nichols et al. (2005) to handle En-
glish dictionaries is that many of the definition
sentences have a semantic head with a part-of-
speech different than that of the definition word.
We found that differing parts-of-speech often indi-
cated an undesirable ontological relation. One rea-
son such relations can be extracted is when a sen-
tence with a non-defining role, for example indi-
cating usage, is encountered. Definition sentence
for non-content-bearing words such as of or the
also pose problems for extraction.
We avoid these problems by filtering by parts-
of-speech twice in the extraction process. First, we
select candidate sentences for extraction by veri-
fying that the definition word has a content word
POS (i.e. adjective, adverb, noun, or verb). Fi-
nally, before we extract any ontological relation,
we make sure that the definition word and the se-
mantic head are in compatible POS classes.
While adopting this strategy does reduce the
number of total ontological relations that we ac-
quire, it increases their reliability. The addition of
a medium parser gives us more RMRS structures
to extract from, which helps compensate for any
loss in number.
</bodyText>
<sectionHeader confidence="0.999603" genericHeader="evaluation">
4 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.999988863636364">
We summarize the relationships acquired in Ta-
ble 2. The columns specify source dictionary
and parsing method while the rows show the rela-
tion type. These counts represent the total num-
ber of relations extracted for each source and
method combination. The majority of relations
extracted are synonyms and hypernyms; however,
some higher-level relations such as meronym and
abbreviation are also acquired. It should also
be noted that both the medium and deep meth-
ods were able to extract a fair number of spe-
cial relations. In many cases, the medium method
even extracted more special relations than the deep
method. This is yet another indication of the
flexibility of dependency parsing. Altogether, we
extracted 105,613 unique relations from Lexeed
(for 46,000 senses), 183,927 unique relations from
Iwanami (for 85,870 senses), and 65,593 unique
relations from GCIDE (for 36,313 senses). As can
be expected, a general pattern in our results is that
the shallow method extracts the most relations in
total followed by the medium method, and finally
</bodyText>
<page confidence="0.99787">
14
</page>
<table confidence="0.999666714285714">
Relation Shallow Lexeed Deep Shallow Iwanami Deep GCIDE
Medium Medium Deep
hypernym 47,549 43,006 41,553 113,120 113,433 66,713 40,583
synonym 12,692 13,126 9,114 31,682 32,261 18,080 21,643
abbreviation 340 429 1,533 739
meronym 235 189 395 202 472
name 100 89 271 140
</table>
<tableCaption confidence="0.998193">
Table 2: Results of Ontology Extraction
</tableCaption>
<bodyText confidence="0.86042">
the deep method.
</bodyText>
<subsectionHeader confidence="0.968021">
4.1 Verification with Hand-crafted
Ontologies
</subsectionHeader>
<bodyText confidence="0.999987157894737">
Because we are interested in comparing lexical se-
mantics across languages, we compared the ex-
tracted ontology with resources in both the same
and different languages.
For Japanese we verified our results by com-
paring the hypernym links to the manually con-
structed Japanese ontology Goi-Taikei (GT). It is
a hierarchy of 2,710 semantic classes, defined for
over 264,312 nouns Ikehara et al. (1997). The se-
mantic classes are mostly defined for nouns (and
verbal nouns), although there is some information
for verbs and adjectives. For English, we com-
pared relations to WordNet 2.0 (Fellbaum, 1998).
Comparison for hypernyms is done as follows:
look up the semantic class or synset C for both the
headword (wi) and genus term(s) (wg). If at least
one of the index word’s classes is subsumed by at
least one of the genus’ classes, then we consider
the relationship confirmed (1).
</bodyText>
<equation confidence="0.782913">
](ch,cg) : Ich C cg;ch E C(wh);cg E C(wg)} (1)
</equation>
<bodyText confidence="0.99984425">
To test cross-linguistically, we looked up the
headwords in a translation lexicon (ALT-J/E (Ike-
hara et al., 1991) and EDICT (Breen, 2004)) and
then did the confirmation on the set of translations
ci C C(T(wi)). Although looking up the transla-
tion adds noise, the additional filter of the relation-
ship triple effectively filters it out again.
The total figures given in Table 3 do not match
the totals given in Table 2. These totals represent
the number of relations where both the definition
word and semantic head were found in at least one
of the ontologies being used in this comparison.
By comparing these numbers to the totals given
in Section 4, we can get an idea of the coverage
of the ontologies being used in comparison. Lex-
eed has a coverage of approx. 55.74% ( 58,867
105,613),
with Iwanami the lowest at 48.20% ( 88,662
183,927), and
GCIDE the highest at 69.85% (45,814
</bodyText>
<page confidence="0.509673">
65,593).
</page>
<bodyText confidence="0.9991785">
It is clear
that there are a lot of relations in each lexicon that
are not covered by the hand-crafted ontologies.
This demonstrates that machine-readable dictio-
naries are still a valuable resource for constructing
ontologies.
</bodyText>
<subsectionHeader confidence="0.802338">
4.1.1 Lexeed
</subsectionHeader>
<bodyText confidence="0.999989161290322">
Our results using JACY achieve a confirmation
rate of 66.84% for nouns only and 60.67% over-
all (Table 3). This is an improvement over both
Tokunaga et al. (2001), who reported 61.4% for
nouns only, and Nichols et al. (2005) who reported
63.31% for nouns and 57.74% overall. We also
achieve an impressive 33,333 confirmed relations
for a rate of 56.62% overall. It is important to
note that our total counts include all unique re-
lations regardless of source, unlike Nichols et al.
(2005) who take only the relation from the deepest
source whenever multiple relations are extracted.
It is interesting to note that shallow processing out
performs medium with 22,540 verified relations
(59.40%) compared to 21,806 (57.76%). This
would seem to suggest that for the simplest task of
retrieving hyperynms and synonyms, more infor-
mation than that is not necessary. However, since
medium and deep parsing obtain relations not cov-
ered by shallow parsing and can extract special re-
lations, a task that cannot be performed without
syntactic information, it is beneficial to use them
as well.
Agirre et al. (2000) reported an error rate of
2.8% in a hand-evaluation of the semantic rela-
tions they automatically extracted from a machine-
readable Basque dictionary. In a similar hand-
evaluation of a stratified sampling of relations ex-
tracted from Lexeed, we achieved an error rate
of 9.2%, demonstrating that our method is also
highly accurate (Nichols et al., 2005).
</bodyText>
<subsectionHeader confidence="0.956374">
4.2 Iwanami
</subsectionHeader>
<bodyText confidence="0.9998084">
Iwanami’s verification results are similar to Lex-
eed’s (Table 3). There are on average around 3%
more verifications and a total of almost 20,000
more verified relations extracted. It is particu-
larly interesting to note that deep processing per-
</bodyText>
<page confidence="0.99333">
15
</page>
<table confidence="0.999803833333333">
Method /Relation hypernym Confirmed Relations in Lexeed Total
synonym
Shallow 58.55 % ( 16585 / 28328 ) 61.93 % ( 5955 / 9615 ) 59.40 % ( 22540 / 37943 )
Medium 55.97 % ( 15431 / 27570 ) 62.61 % ( 6375 / 10182 ) 57.76 % ( 21806 / 37752 )
Deep 54.78 % ( 4954 / 9043 ) 67.76 % ( 5098 / 7524 ) 60.67 % ( 10052 / 16567 )
All 55.22 % ( 23802 / 43102 ) 60.46 % ( 9531 / 15765 ) 56.62 % ( 33333 / 58867 )
Confirmed Relations in Iwanami
Method /Relation hypernym synonym Total
Shallow 61.20 % ( 35208 / 57533 ) 63.57 % ( 11362 / 17872 ) 61.76 % ( 46570 / 75405 )
Medium 60.69 % ( 35621 / 58698 ) 62.86 % ( 11037 / 17557 ) 61.19 % ( 46658 / 76255 )
Deep 63.59 % ( 22936 / 36068 ) 64.44 % ( 8395 / 13027 ) 63.82 % ( 31331 / 49095 )
All 59.36 % ( 40179 / 67689 ) 61.66 % ( 12931 / 20973 ) 59.90 % ( 53110 / 88662 )
Confirmed Relations in GCIDE
POS / Relation hypernym synonym Total
Adjective 2.88 % ( 37 / 1283 ) 16.77 % ( 705 / 4203 ) 13.53 % ( 742 / 5486 )
Noun 57.60 % ( 7518 / 13053 ) 50.71 % ( 3522 / 6945 ) 55.21 % ( 11040 / 19998 )
Verb 24.22 % ( 3006 / 12411 ) 21.40 % ( 1695 / 7919 ) 23.12 % ( 4701 / 20330 )
Total 39.48 % ( 10561 / 26747 ) 31.06 % ( 5922 / 19067 ) 35.98 % ( 16483 / 45814 )
</table>
<tableCaption confidence="0.999927">
Table 3: Confirmed Relations, measured against GT and WordNet
</tableCaption>
<bodyText confidence="0.999944727272727">
forms better here than on Lexeed (63.82% vs
60.67%), even though the grammar was developed
and tested on Lexeed. There are two reasons for
this: The first is that the process of rewriting Lex-
eed to use only familiar words actually makes the
sentences harder to parse. The second is that the
less familiar words in Iwanami have fewer senses,
and easier to parse definition sentences. In any
case, the results support our claims that our onto-
logical relation extraction system is easily adapt-
able to new lexicons.
</bodyText>
<subsectionHeader confidence="0.93562">
4.3 GCIDE
</subsectionHeader>
<bodyText confidence="0.9999558">
At first glance, it would seem that GCIDE has
the most disappointing of the verification results
with overall verification of not even 36% and only
16,483 relations confirmed. However, on closer
inspection one can see that noun hypernyms are a
respectable 57.60% with over 55% for all nouns.
These figures are comparable with the results we
are obtaining with the other lexicons. One should
also bear in mind that the definitions found in
GCIDE can be archaic; after all this dictionary
was first published in 1913. This could be one
cause of parsing errors for ERG. Despite these ob-
stacles, we feel that GCIDE has a lot of poten-
tial for ontological acquisition. A dictionary of
its size and coverage will most likely contain rela-
tions that may not be represented in other sources.
One only has to look at the definition of F 7
4 ) � - “driver”/driver to confirm this; GT has
two senses (“screwdriver” and “vehicle operator”)
Lexeed and Iwanami have 3 senses each (adding
“golf club”), and WordNet has 5 (including “soft-
ware driver”), but GCIDE has 6, not including
“software driver” but including spanker “a kind of
sail”. It should be beneficial to propagate these
different senses across ontologies.
</bodyText>
<sectionHeader confidence="0.99696" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999983961538461">
We were able to successfully combine deep pro-
cessing of various levels of depth in order to
extract ontological information from lexical re-
sources. We showed that, by using a well defined
semantic representation, the extraction can be gen-
eralized so much that it can be used on very differ-
ent dictionaries from different languages. This is
an improvement on the common approach to using
more and more detailed regular expressions (e.g.
Tokunaga et al. (2001)). Although this provides a
quick start, the results are not generally reusable.
In comparison, the shallower RMRS engines are
immediately useful for a variety of other tasks.
However, because the hook is the only syntactic
information returned by the shallow parser, onto-
logical relation extraction is essentially performed
by this hook-identifying heuristic. While this is
sufficient for a large number of sentences, it is not
possible to process special relations with the shal-
low parser since none of the arguments are linked
with the predicates to which they belong. Thus, as
Table 2 shows, our shallow parser is only capable
of retrieving hypernyms and synonyms. It is im-
portant to extract a variety of semantic relations in
order to form a useful ontology. This is one of the
reasons why we use a combination of parsers of
</bodyText>
<page confidence="0.990155">
16
</page>
<bodyText confidence="0.999838461538461">
different analytic levels rather than depending on
a single resource.
The other innovation of our approach is the
cross-lingual evaluation. As a by-product of
the evaluation we enhance the existing resources
(such as GT or WordNet) by linking them, so
that information can be shared between them. In
this way we can use the cross-lingual links to fill
gaps in the monolingual resources. GT and Word-
Net both lack complete cover - over half the rela-
tions were confirmed with only one resource. This
shows that the machine readable dictionary is a
useful source of these relations.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999535">
In this paper, we presented the results of experi-
ments conducted in automatic ontological acqui-
sition over two languages, English and Japanese,
and from three different machine-readable dictio-
naries. Our system is unique in combining parsers
of various levels of analysis to generate its input
semantic structures. The system is language ag-
nostic and we give results for both Japanese and
English MRDs. Finally, we presented evaluation
of the ontologies constructed by comparing them
with existing hand-crafted English and Japanese
ontologies.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999942136842105">
Eneko Agirre, Olatz Ansa, Xabier Arregi, Xabier Artola,
Arantza Diaz de Ilarraza, Mikel Lersundi, David Martinez,
Kepa Sarasola, and Ruben Urizar. 2000. Extraction of
semantic relations from a Basque monolingual dictionary
using Constraint Grammar. In EURALEX2000.
Shigeaki Amano and Tadahisa Kondo. 1999. Nihongo-no
Goi-Tokusei (Lexical properties ofJapanese). Sanseido.
J. W. Breen. 2004. JMDict: a Japanese-multilingual dictio-
nary. In Coling 2004 Workshop on Multilingual Linguistic
Resources, pages 71–78. Geneva.
Ulrich Callmeier. 2000. PET - a platform for experimenta-
tion with efficient HPSG processing techniques. Natural
Language Engineering, 6(1):99–108.
Ulrich Callmeier, Andreas Eisele, Ulrich Sch¨afer, and
Melanie Siegel. 2004. The DeepThought core architecture
framework. In Proceedings of LREC-2004, volume IV.
Lisbon.
Ann Copestake. 2002. Implementing Typed Feature Structure
Grammars. CSLI Publications.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.
Sag. 2005. Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4):281–
332.
Christine Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15–28. (Special Issue on Efficient Processing with
HPSG).
Anette Frank. 2004. Constraint-based RMRS construction
from shallow grammars. In 20th International Con-
ference on Computational Linguistics: COLING-2004,
pages 1269–1272. Geneva.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei —
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
— effects of new methods in ALT-J/E —. In Third Ma-
chine Translation Summit: MT Summit III, pages 101–
106. Washington DC. (http://xxx.lanl.gov/abs/
cmp-lg/9510008).
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lex-
icon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in Japanese).
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In CoNLL 2002:
Proceedings of the 6th Conference on Natural Language
Learning 2002 (COLING 2002 Post-Conference Work-
shops), pages 63–69. Taipei.
Yuji Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda,
and Asahara. 2000. Nihongo Keitaiso Kaiseki System:
Chasen.http://chasen.naist.jp/hiki/ChaSen/.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictio-
naries. In Proceedings of the International Joint Confer-
ence on Artificial Intelligence IJCAI-2005, pages 1111–
1116. Edinburgh.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami
Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo.
(in Japanese).
Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.
2004. Towards terascale knowledge acquisition. In 20th
International Conference on Computational Linguistics:
COLING-2004, pages 771–777. Geneva.
Stephen D. Richardson, William B. Dolan, and Lucy Van-
derwende. 1998. MindNet: acquiring and structuring se-
mantic information from text. In 36th Annual Meeting
of the Association for Computational Linguistics and 17th
International Conference on Computational Linguistics:
COLING/ACL-98, pages 1098–1102. Montreal.
Kiyoaki Shirai. 2003. SENSEVAL-2 Japanese dictionary
task. Journal of Natural Language Processing, 10(3):3–
24. (in Japanese).
Melanie Siegel. 2000. HPSG analysis of Japanese. In
Wahlster (2000), pages 265–280.
Kathrin Spreyer and Anette Frank. 2005. The TIGER RMRS
700 bank: RMRS construction from dependencies. In Pro-
ceedings of the 6th International Workshop on Linguisti-
cally Interpreted Corpora (LINC 2005), pages 1–10. Jeju
Island, Korea.
Takenobu Tokunaga, Yasuhiro Syotu, Hozumi Tanaka, and
Kiyoaki Shirai. 2001. Integration of heterogeneous lan-
guage resources: A monolingual dictionary and a the-
saurus. In Proceedings of the 6th Natural Language Pro-
cessing Pacific Rim Symposium, NLPRS2001, pages 135–
142. Tokyo.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer, Berlin, Germany.
Yorick A. Wilkes, Brian M. Slator, and Louise M. Guthrie.
1996. Electric Words. MIT Press.
</reference>
<page confidence="0.99941">
17
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.286239">
<title confidence="0.99982">Multilingual Ontology Acquisition from Multiple MRDs</title>
<author confidence="0.98294">Francis Takaaki Sanae Dan Flickinger</author>
<affiliation confidence="0.616799">Inst. of Science and Technology Communication Science Labs University Grad. School of Information Science Natural Language Research Group CSLI Nara, Japan Keihanna, Japan Stanford,</affiliation>
<email confidence="0.999963">danf@csli.stanford.edu</email>
<abstract confidence="0.996593521739131">In this paper, we outline the development of a system that automatically constructs ontologies by extracting knowledge from dictionary definition sentences using Robust Minimal Recursion Semantics (RMRS). Combining deep and shallow parsing resource through the common formalism of RMRS allows us to extract ontological relations in greater quantity and quality than possible with any of the methods independently. Using this method, we construct ontologies from two different Japanese lexicons and one English lexicon. We then link them to existing, handcrafted ontologies, aligning them at the word-sense level. This alignment provides a representative evaluation of the quality of the relations being extracted. We present the results of this ontology construction and discuss how our system was designed to handle multiple lexicons and languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Olatz Ansa, Xabier Arregi, Xabier Artola, Arantza Diaz de Ilarraza, Mikel Lersundi,</title>
<date>2000</date>
<booktitle>In EURALEX2000.</booktitle>
<location>David Martinez, Kepa</location>
<marker>Agirre, 2000</marker>
<rawString>Eneko Agirre, Olatz Ansa, Xabier Arregi, Xabier Artola, Arantza Diaz de Ilarraza, Mikel Lersundi, David Martinez, Kepa Sarasola, and Ruben Urizar. 2000. Extraction of semantic relations from a Basque monolingual dictionary using Constraint Grammar. In EURALEX2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shigeaki Amano</author>
<author>Tadahisa Kondo</author>
</authors>
<title>Nihongo-no Goi-Tokusei (Lexical properties ofJapanese).</title>
<date>1999</date>
<publisher>Sanseido.</publisher>
<contexts>
<context position="2110" citStr="Amano and Kondo, 1999" startWordPosition="309" endWordPosition="312">mple word sense disambiguation, question answering and machine translation. In this paper, we present the results of experiments conducted in automatic ontological acquisition over two languages, English and Japanese, and from three different machine-readable dictionaries. Useful semantic relations can be extracted from large corpora using relatively simple patterns (e.g., (Pantel et al., 2004)). While large corpora often contain information not found in lexicons, even a very large corpus may not include all the familiar words of a language, let alone those words occurring in useful patterns (Amano and Kondo, 1999). Therefore it makes sense to also extract data from machine readable dictionaries (MRDs). There is a great deal of work on the creation of ontologies from machine readable dictionaries (a good summary is (Wilkes et al., 1996)), mainly for English. Recently, there has also been interest in Japanese (Tokunaga et al., 2001; Nichols et al., 2005). Most approaches use either a specialized parser or a set of regular expressions tuned to a particular dictionary, often with hundreds of rules. Agirre et al. (2000) extracted taxonomic relations from a Basque dictionary with high accuracy using Constrai</context>
</contexts>
<marker>Amano, Kondo, 1999</marker>
<rawString>Shigeaki Amano and Tadahisa Kondo. 1999. Nihongo-no Goi-Tokusei (Lexical properties ofJapanese). Sanseido.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Breen</author>
</authors>
<title>JMDict: a Japanese-multilingual dictionary.</title>
<date>2004</date>
<booktitle>In Coling 2004 Workshop on Multilingual Linguistic Resources,</booktitle>
<pages>71--78</pages>
<location>Geneva.</location>
<contexts>
<context position="22967" citStr="Breen, 2004" startWordPosition="3697" endWordPosition="3698">(and verbal nouns), although there is some information for verbs and adjectives. For English, we compared relations to WordNet 2.0 (Fellbaum, 1998). Comparison for hypernyms is done as follows: look up the semantic class or synset C for both the headword (wi) and genus term(s) (wg). If at least one of the index word’s classes is subsumed by at least one of the genus’ classes, then we consider the relationship confirmed (1). ](ch,cg) : Ich C cg;ch E C(wh);cg E C(wg)} (1) To test cross-linguistically, we looked up the headwords in a translation lexicon (ALT-J/E (Ikehara et al., 1991) and EDICT (Breen, 2004)) and then did the confirmation on the set of translations ci C C(T(wi)). Although looking up the translation adds noise, the additional filter of the relationship triple effectively filters it out again. The total figures given in Table 3 do not match the totals given in Table 2. These totals represent the number of relations where both the definition word and semantic head were found in at least one of the ontologies being used in this comparison. By comparing these numbers to the totals given in Section 4, we can get an idea of the coverage of the ontologies being used in comparison. Lexeed</context>
</contexts>
<marker>Breen, 2004</marker>
<rawString>J. W. Breen. 2004. JMDict: a Japanese-multilingual dictionary. In Coling 2004 Workshop on Multilingual Linguistic Resources, pages 71–78. Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Callmeier</author>
</authors>
<title>PET - a platform for experimentation with efficient HPSG processing techniques.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="10733" citStr="Callmeier, 2000" startWordPosition="1664" endWordPosition="1665">f deep and shallow results for the same sentenceHft*�A+Kt7DJ�,jid¯osha wo unten suru hito “a person who drives a car (lit: car-ACC drive do person)” are given in Figures 1 and 2 (omitting the indexing). Real predicates are prefixed by an under-bar ( ). The deep parse gives information about the scope, message types and argument structure, while the shallow parse gives little more than a list of real and grammatical predicates with a hook. 2.4.2 Deep Parsers (JACY, ERG and PET) For both Japanese and English, we used the PET System for the high-efficiency processing of typed feature structures (Callmeier, 2000). For Japanese, we used JACY (Siegel, 2000), for English we used the English Resource Grammar (ERG: Flickinger 2000).1 JACY The JACY grammar is an HPSG-based grammar of Japanese which originates from work done in the Verbmobil project (Siegel, 2000) on machine translation of spoken dialogues in the domain of travel planning. It has since been extended to accommodate written Japanese and new domains (such as electronic commerce customer email and machine readable dictionaries). The grammar implementation is based on a system of types. There are around 900 lexical types that define the syntactic</context>
<context position="12711" citStr="Callmeier, 2000" startWordPosition="1965" endWordPosition="1966">ce been substantially extended in both grammatical and lexical coverage, reaching 80-90% coverage of sizeable corpora in two additional domains: electronic commerce customer email and tourism brochures. The grammar includes a hand-built lexicon of 23,000 lemmas instantiating 850 lexical types, a highly schematic set of 150 grammar rules, and a set of 40 lexical rules, all organized in a rich multiple inheritance hierarchy of some 3000 typed feature structures. Like other DELPH-IN grammars, the ERG can be processed by several parsers and generators, including the LKB (Copestake, 2002) and PET (Callmeier, 2000). Each successful ERG analysis of a sentence or fragment includes a finegrained semantic representation in MRS. For the task of parsing the dictionary definitions in GCIDE (the GNU Collaborative Interna1Both grammars, the LKB and PET are available at &lt;http://www.delph-in.net/&gt;. 12 ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ TEXTnR*�A$$ �6� TOP h1 jidousha n rel LBL h4 ARG0 x5 ⎤ ⎡ ⎦ ⎥ ⎣ ⎡ ⎢ ⎣ ⎡ ⎤⎢ ⎦ ⎢ ⎢ ⎣ udef rel LBL h6 ARG0 x5 RSTR h7 BODY h8 RELS ⎧ ⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ hito n rel LBL h12 ARG0 x10 ⎡ ⎣ ⎤ ⎡ ⎥ ⎢ ⎦ ⎣ proposition m rel LBL h10001 </context>
</contexts>
<marker>Callmeier, 2000</marker>
<rawString>Ulrich Callmeier. 2000. PET - a platform for experimentation with efficient HPSG processing techniques. Natural Language Engineering, 6(1):99–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Callmeier</author>
<author>Andreas Eisele</author>
<author>Ulrich Sch¨afer</author>
<author>Melanie Siegel</author>
</authors>
<title>The DeepThought core architecture framework.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC-2004, volume IV. Lisbon.</booktitle>
<marker>Callmeier, Eisele, Sch¨afer, Siegel, 2004</marker>
<rawString>Ulrich Callmeier, Andreas Eisele, Ulrich Sch¨afer, and Melanie Siegel. 2004. The DeepThought core architecture framework. In Proceedings of LREC-2004, volume IV. Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Implementing Typed Feature Structure Grammars.</title>
<date>2002</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="12685" citStr="Copestake, 2002" startWordPosition="1961" endWordPosition="1962"> planning. The ERG has since been substantially extended in both grammatical and lexical coverage, reaching 80-90% coverage of sizeable corpora in two additional domains: electronic commerce customer email and tourism brochures. The grammar includes a hand-built lexicon of 23,000 lemmas instantiating 850 lexical types, a highly schematic set of 150 grammar rules, and a set of 40 lexical rules, all organized in a rich multiple inheritance hierarchy of some 3000 typed feature structures. Like other DELPH-IN grammars, the ERG can be processed by several parsers and generators, including the LKB (Copestake, 2002) and PET (Callmeier, 2000). Each successful ERG analysis of a sentence or fragment includes a finegrained semantic representation in MRS. For the task of parsing the dictionary definitions in GCIDE (the GNU Collaborative Interna1Both grammars, the LKB and PET are available at &lt;http://www.delph-in.net/&gt;. 12 ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ TEXTnR*�A$$ �6� TOP h1 jidousha n rel LBL h4 ARG0 x5 ⎤ ⎡ ⎦ ⎥ ⎣ ⎡ ⎢ ⎣ ⎡ ⎤⎢ ⎦ ⎢ ⎢ ⎣ udef rel LBL h6 ARG0 x5 RSTR h7 BODY h8 RELS ⎧ ⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ hito n rel LBL h12 ARG0 x10 ⎡ ⎣ ⎤ ⎡ ⎥ ⎢ ⎦ ⎣ pro</context>
</contexts>
<marker>Copestake, 2002</marker>
<rawString>Ann Copestake. 2002. Implementing Typed Feature Structure Grammars. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal Recursion Semantics. An introduction.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>332</pages>
<contexts>
<context position="9060" citStr="Copestake et al., 2005" startWordPosition="1396" endWordPosition="1399">rphological analyzer 11 ChaSen and the dependency parser CaboCha) from the Matsumoto Laboratory. 2.4.1 Robust Minimal Recursion Semantics Robust Minimal Recursion Semantics is a form of flat semantics which is designed to allow deep and shallow processing to use a compatible semantic representation, with fine-grained atomic components of semantic content so shallow methods can contribute just what they know, yet with enough expressive power for rich semantic content including generalized quantifiers (Frank, 2004). The architecture of the representation is based on Minimal Recursion Semantics (Copestake et al., 2005), including a bag of labeled elementary predicates (EPs) and their arguments, a list of scoping constraints which enable scope underspecification, and a handle that provides a hook into the representation. The representation can be underspecified in three ways: relationships can be omitted (such as quantifiers, messages, conjunctions and so on); predicate-argument relations can be omitted; and predicate names can be simplified. Predicate names are defined in such a way as to be as compatible (predictable) as possible among different analysis engines, using a lemma pos subsense naming conventio</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag. 2005. Minimal Recursion Semantics. An introduction. Research on Language and Computation, 3(4):281– 332.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christine Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christine Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<booktitle>(Special Issue on Efficient Processing with HPSG).</booktitle>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="10849" citStr="Flickinger 2000" startWordPosition="1682" endWordPosition="1683"> (lit: car-ACC drive do person)” are given in Figures 1 and 2 (omitting the indexing). Real predicates are prefixed by an under-bar ( ). The deep parse gives information about the scope, message types and argument structure, while the shallow parse gives little more than a list of real and grammatical predicates with a hook. 2.4.2 Deep Parsers (JACY, ERG and PET) For both Japanese and English, we used the PET System for the high-efficiency processing of typed feature structures (Callmeier, 2000). For Japanese, we used JACY (Siegel, 2000), for English we used the English Resource Grammar (ERG: Flickinger 2000).1 JACY The JACY grammar is an HPSG-based grammar of Japanese which originates from work done in the Verbmobil project (Siegel, 2000) on machine translation of spoken dialogues in the domain of travel planning. It has since been extended to accommodate written Japanese and new domains (such as electronic commerce customer email and machine readable dictionaries). The grammar implementation is based on a system of types. There are around 900 lexical types that define the syntactic, semantic and pragmatic properties of the Japanese words, and 188 types that define the properties of phrases and l</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. Natural Language Engineering, 6(1):15–28. (Special Issue on Efficient Processing with HPSG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
</authors>
<title>Constraint-based RMRS construction from shallow grammars.</title>
<date>2004</date>
<booktitle>In 20th International Conference on Computational Linguistics: COLING-2004,</booktitle>
<pages>1269--1272</pages>
<location>Geneva.</location>
<contexts>
<context position="8955" citStr="Frank, 2004" startWordPosition="1383" endWordPosition="1384">delph-in.net/) as well as medium- and shallow-processing tools for Japanese processing (the morphological analyzer 11 ChaSen and the dependency parser CaboCha) from the Matsumoto Laboratory. 2.4.1 Robust Minimal Recursion Semantics Robust Minimal Recursion Semantics is a form of flat semantics which is designed to allow deep and shallow processing to use a compatible semantic representation, with fine-grained atomic components of semantic content so shallow methods can contribute just what they know, yet with enough expressive power for rich semantic content including generalized quantifiers (Frank, 2004). The architecture of the representation is based on Minimal Recursion Semantics (Copestake et al., 2005), including a bag of labeled elementary predicates (EPs) and their arguments, a list of scoping constraints which enable scope underspecification, and a handle that provides a hook into the representation. The representation can be underspecified in three ways: relationships can be omitted (such as quantifiers, messages, conjunctions and so on); predicate-argument relations can be omitted; and predicate names can be simplified. Predicate names are defined in such a way as to be as compatibl</context>
</contexts>
<marker>Frank, 2004</marker>
<rawString>Anette Frank. 2004. Constraint-based RMRS construction from shallow grammars. In 20th International Conference on Computational Linguistics: COLING-2004, pages 1269–1272. Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Masahiro Miyazaki</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1997</date>
<booktitle>Goi-Taikei — A Japanese Lexicon. Iwanami Shoten,</booktitle>
<volume>5</volume>
<pages>volumes/CDROM.</pages>
<location>Tokyo.</location>
<contexts>
<context position="22303" citStr="Ikehara et al. (1997)" startWordPosition="3581" endWordPosition="3584">2 13,126 9,114 31,682 32,261 18,080 21,643 abbreviation 340 429 1,533 739 meronym 235 189 395 202 472 name 100 89 271 140 Table 2: Results of Ontology Extraction the deep method. 4.1 Verification with Hand-crafted Ontologies Because we are interested in comparing lexical semantics across languages, we compared the extracted ontology with resources in both the same and different languages. For Japanese we verified our results by comparing the hypernym links to the manually constructed Japanese ontology Goi-Taikei (GT). It is a hierarchy of 2,710 semantic classes, defined for over 264,312 nouns Ikehara et al. (1997). The semantic classes are mostly defined for nouns (and verbal nouns), although there is some information for verbs and adjectives. For English, we compared relations to WordNet 2.0 (Fellbaum, 1998). Comparison for hypernyms is done as follows: look up the semantic class or synset C for both the headword (wi) and genus term(s) (wg). If at least one of the index word’s classes is subsumed by at least one of the genus’ classes, then we consider the relationship confirmed (1). ](ch,cg) : Ich C cg;ch E C(wh);cg E C(wg)} (1) To test cross-linguistically, we looked up the headwords in a translation</context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei — A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 volumes/CDROM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Satoshi Shirai</author>
<author>Akio Yokoo</author>
<author>Hiromi Nakaiwa</author>
</authors>
<title>Toward an MT system without pre-editing — effects of new methods in ALT-J/E —.</title>
<date>1991</date>
<booktitle>In Third Machine Translation Summit: MT Summit III,</booktitle>
<pages>101--106</pages>
<contexts>
<context position="22943" citStr="Ikehara et al., 1991" startWordPosition="3690" endWordPosition="3694">ses are mostly defined for nouns (and verbal nouns), although there is some information for verbs and adjectives. For English, we compared relations to WordNet 2.0 (Fellbaum, 1998). Comparison for hypernyms is done as follows: look up the semantic class or synset C for both the headword (wi) and genus term(s) (wg). If at least one of the index word’s classes is subsumed by at least one of the genus’ classes, then we consider the relationship confirmed (1). ](ch,cg) : Ich C cg;ch E C(wh);cg E C(wg)} (1) To test cross-linguistically, we looked up the headwords in a translation lexicon (ALT-J/E (Ikehara et al., 1991) and EDICT (Breen, 2004)) and then did the confirmation on the set of translations ci C C(T(wi)). Although looking up the translation adds noise, the additional filter of the relationship triple effectively filters it out again. The total figures given in Table 3 do not match the totals given in Table 2. These totals represent the number of relations where both the definition word and semantic head were found in at least one of the ontologies being used in this comparison. By comparing these numbers to the totals given in Section 4, we can get an idea of the coverage of the ontologies being us</context>
</contexts>
<marker>Ikehara, Shirai, Yokoo, Nakaiwa, 1991</marker>
<rawString>Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi Nakaiwa. 1991. Toward an MT system without pre-editing — effects of new methods in ALT-J/E —. In Third Machine Translation Summit: MT Summit III, pages 101– 106. Washington DC. (http://xxx.lanl.gov/abs/ cmp-lg/9510008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaname Kasahara</author>
<author>Hiroshi Sato</author>
<author>Francis Bond</author>
</authors>
<title>Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki Amano.</title>
<date>2004</date>
<booktitle>Construction of a Japanese semantic lexicon: Lexeed. SIG NLC-159, IPSJ,</booktitle>
<location>Tokyo.</location>
<note>(in Japanese).</note>
<contexts>
<context position="5902" citStr="Kasahara et al., 2004" startWordPosition="915" endWordPosition="919">nese lexicon (Iwanami) and even a lexicon in a different language (GCIDE). Section 2 provides a description of RMRS and the tools used by our system. The ontological acquisition system is presented in Section 3. The results of evaluating our ontologies by comparison with existing resources are given in Section 4. We discuss our findings in Section 5. 2 Resources 2.1 The Lexeed Semantic Database of Japanese The Lexeed Semantic Database of Japanese is a machine readable dictionary that covers the most familiar open class words in Japanese as measured by a series of psycholinguistic experiments (Kasahara et al., 2004). Lexeed consists of all open class words with a familiarity greater than or equal to five on a scale of one to seven. This gives 28,000 words divided into 46,000 senses and defined with 75,000 definition sentences. All definition sentences and example sentences have been rewritten to use only the 28,000 familiar open class words. The definition and example sentences have been treebanked with the JACY grammar (§ 2.4.2). 2.2 The Iwanami Dictionary of Japanese The Iwanami Kokugo Jiten (Iwanami) (Nishio et al., 1994) is a concise Japanese dictionary. A machine tractable version was made available</context>
</contexts>
<marker>Kasahara, Sato, Bond, 2004</marker>
<rawString>Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki Amano. 2004. Construction of a Japanese semantic lexicon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In CoNLL 2002: Proceedings of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-Conference Workshops),</booktitle>
<pages>63--69</pages>
<location>Taipei.</location>
<contexts>
<context position="14857" citStr="Kudo and Matsumoto, 2002" startWordPosition="2409" endWordPosition="2412"> this domain), and additional lexical entries were manually added for all missing words in the alphabetically first 10,000 definition sentences. These first 10,000 sentences were parsed and then manually tree-banked to provide the training material for constructing the stochastic model used for best-only parsing of the rest of the definition sentences. Using POS-based unknown-word guessing for missing lexical entries, MRSes were obtained for about 75% of the first 100,000 definition sentences. 2.4.3 Medium Parser (CaboCha-RMRS) For Japanese, we produce RMRS from the dependency parser Cabocha (Kudo and Matsumoto, 2002). The method is similar to that of Spreyer and Frank (2005), who produce RMRS from detailed German dependencies. CaboCha provides fairly minimal dependencies: there are three links (dependent, parallel, apposition) and they link base phrases (Japanese bunsetsu), marked with the syntactic and semantic head. The CaboChaRMRS parser uses this information, along with heuristics based on the parts-of-speech, to produce underspecified RMRSs. CaboCha-RMRS is capable of making use of HPSG resources, including verbal case frames, to further enrich its output. This allows it to produce RMRS that approach</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In CoNLL 2002: Proceedings of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-Conference Workshops), pages 63–69. Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Yamashita Kitauchi</author>
<author>Matsuda Hirano</author>
<author>Asahara</author>
</authors>
<title>Nihongo Keitaiso Kaiseki System:</title>
<date>2000</date>
<publisher>Chasen.http://chasen.naist.jp/hiki/ChaSen/.</publisher>
<contexts>
<context position="15901" citStr="Matsumoto et al., 2000" startWordPosition="2571" endWordPosition="2574">rspecified RMRSs. CaboCha-RMRS is capable of making use of HPSG resources, including verbal case frames, to further enrich its output. This allows it to produce RMRS that approaches the granularity of the analyses given by HPSG parsers. Indeed, CaboCha-RMRS and JACY give identical parses for the example sentence in Figure 1. One of our motivations in including a medium parser in our system is to extract more relations that require special processing; the flexibility of CaboCha-RMRS and the RMRS formalism make this possible. 2.4.4 Shallow Parser (ChaSen-RMRS) The part-of-speech tagger, ChaSen (Matsumoto et al., 2000) was used for shallow processing of Japanese. Predicate names were produced by transliterating the pronunciation field and mapping the part-of-speech codes to the RMRS super types. The part-of-speech codes were also used to judge whether predicates were real or grammatical. Since Japanese is a head-final language, the hook value was set to be the handle of the right-most real predicate. This is easy to do for Japanese, but difficult for English. 3 Ontology Construction We adopt the ontological relation extraction algorithm used by Nichols et al. (2005). Its goal is to identify the semantic hea</context>
</contexts>
<marker>Matsumoto, Kitauchi, Hirano, Asahara, 2000</marker>
<rawString>Yuji Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda, and Asahara. 2000. Nihongo Keitaiso Kaiseki System: Chasen.http://chasen.naist.jp/hiki/ChaSen/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Nichols</author>
<author>Francis Bond</author>
<author>Daniel Flickinger</author>
</authors>
<title>Robust ontology acquisition from machine-readable dictionaries.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence IJCAI-2005,</booktitle>
<pages>1111--1116</pages>
<location>Edinburgh.</location>
<contexts>
<context position="2455" citStr="Nichols et al., 2005" startWordPosition="366" endWordPosition="369">tively simple patterns (e.g., (Pantel et al., 2004)). While large corpora often contain information not found in lexicons, even a very large corpus may not include all the familiar words of a language, let alone those words occurring in useful patterns (Amano and Kondo, 1999). Therefore it makes sense to also extract data from machine readable dictionaries (MRDs). There is a great deal of work on the creation of ontologies from machine readable dictionaries (a good summary is (Wilkes et al., 1996)), mainly for English. Recently, there has also been interest in Japanese (Tokunaga et al., 2001; Nichols et al., 2005). Most approaches use either a specialized parser or a set of regular expressions tuned to a particular dictionary, often with hundreds of rules. Agirre et al. (2000) extracted taxonomic relations from a Basque dictionary with high accuracy using Constraint Grammar together with hand-crafted rules. However, such a system is limited to one language, and it has yet to be seen how the rules will scale when deeper semantic relations are extracted. In comparison, as we will demonstrate, our system produces comparable results while the framework is immediately applicable to any language with the res</context>
<context position="16459" citStr="Nichols et al. (2005)" startWordPosition="2661" endWordPosition="2664">MRS) The part-of-speech tagger, ChaSen (Matsumoto et al., 2000) was used for shallow processing of Japanese. Predicate names were produced by transliterating the pronunciation field and mapping the part-of-speech codes to the RMRS super types. The part-of-speech codes were also used to judge whether predicates were real or grammatical. Since Japanese is a head-final language, the hook value was set to be the handle of the right-most real predicate. This is easy to do for Japanese, but difficult for English. 3 Ontology Construction We adopt the ontological relation extraction algorithm used by Nichols et al. (2005). Its goal is to identify the semantic head(s) of a dictionary definition sentence – the relation(s) that best summarize it. The algorithm does this by traversing the RMRS structure of a given definition sentence starting at the HOOK (the highest-scoping semantic relationship) and following its argument structure. When the algorithm can proceed no further, it returns the a tuple consisting of the definition word and the word identified by the se13 mantic relation where the algorithm halted. Our extended algorithm has the following characteristics: sentences with only one content-bearing relati</context>
<context position="19322" citStr="Nichols et al. (2005)" startWordPosition="3103" endWordPosition="3106"> Table 1: Special predicates and their associated ontological relations Augmenting the system to work on English definition sentence simply entailed writing rules to handle special relations that occur in English. Our system currently has 26 rules for Japanese and 50 rules for English. These rules provide processing of relations like those found in Table 1, and they also handle processing of coordinate structures, such as noun phrases joined together with conjunctions such as and, or, and punctuation. 3.2 Filtering by Part-of-Speech One of the problems encountered in expanding the approach in Nichols et al. (2005) to handle English dictionaries is that many of the definition sentences have a semantic head with a part-ofspeech different than that of the definition word. We found that differing parts-of-speech often indicated an undesirable ontological relation. One reason such relations can be extracted is when a sentence with a non-defining role, for example indicating usage, is encountered. Definition sentence for non-content-bearing words such as of or the also pose problems for extraction. We avoid these problems by filtering by partsof-speech twice in the extraction process. First, we select candid</context>
<context position="24185" citStr="Nichols et al. (2005)" startWordPosition="3905" endWordPosition="3908">eed has a coverage of approx. 55.74% ( 58,867 105,613), with Iwanami the lowest at 48.20% ( 88,662 183,927), and GCIDE the highest at 69.85% (45,814 65,593). It is clear that there are a lot of relations in each lexicon that are not covered by the hand-crafted ontologies. This demonstrates that machine-readable dictionaries are still a valuable resource for constructing ontologies. 4.1.1 Lexeed Our results using JACY achieve a confirmation rate of 66.84% for nouns only and 60.67% overall (Table 3). This is an improvement over both Tokunaga et al. (2001), who reported 61.4% for nouns only, and Nichols et al. (2005) who reported 63.31% for nouns and 57.74% overall. We also achieve an impressive 33,333 confirmed relations for a rate of 56.62% overall. It is important to note that our total counts include all unique relations regardless of source, unlike Nichols et al. (2005) who take only the relation from the deepest source whenever multiple relations are extracted. It is interesting to note that shallow processing out performs medium with 22,540 verified relations (59.40%) compared to 21,806 (57.76%). This would seem to suggest that for the simplest task of retrieving hyperynms and synonyms, more inform</context>
<context position="25412" citStr="Nichols et al., 2005" startWordPosition="4104" endWordPosition="4107">than that is not necessary. However, since medium and deep parsing obtain relations not covered by shallow parsing and can extract special relations, a task that cannot be performed without syntactic information, it is beneficial to use them as well. Agirre et al. (2000) reported an error rate of 2.8% in a hand-evaluation of the semantic relations they automatically extracted from a machinereadable Basque dictionary. In a similar handevaluation of a stratified sampling of relations extracted from Lexeed, we achieved an error rate of 9.2%, demonstrating that our method is also highly accurate (Nichols et al., 2005). 4.2 Iwanami Iwanami’s verification results are similar to Lexeed’s (Table 3). There are on average around 3% more verifications and a total of almost 20,000 more verified relations extracted. It is particularly interesting to note that deep processing per15 Method /Relation hypernym Confirmed Relations in Lexeed Total synonym Shallow 58.55 % ( 16585 / 28328 ) 61.93 % ( 5955 / 9615 ) 59.40 % ( 22540 / 37943 ) Medium 55.97 % ( 15431 / 27570 ) 62.61 % ( 6375 / 10182 ) 57.76 % ( 21806 / 37752 ) Deep 54.78 % ( 4954 / 9043 ) 67.76 % ( 5098 / 7524 ) 60.67 % ( 10052 / 16567 ) All 55.22 % ( 23802 / 4</context>
</contexts>
<marker>Nichols, Bond, Flickinger, 2005</marker>
<rawString>Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Robust ontology acquisition from machine-readable dictionaries. In Proceedings of the International Joint Conference on Artificial Intelligence IJCAI-2005, pages 1111– 1116. Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minoru Nishio</author>
<author>Etsutaro Iwabuchi</author>
<author>Shizuo Mizutani</author>
</authors>
<date>1994</date>
<booktitle>Iwanami Kokugo Jiten Dai Go Han [Iwanami Japanese Dictionary Edition 5]. Iwanami Shoten,</booktitle>
<location>Tokyo.</location>
<note>(in Japanese).</note>
<contexts>
<context position="6421" citStr="Nishio et al., 1994" startWordPosition="1001" endWordPosition="1004">ass words in Japanese as measured by a series of psycholinguistic experiments (Kasahara et al., 2004). Lexeed consists of all open class words with a familiarity greater than or equal to five on a scale of one to seven. This gives 28,000 words divided into 46,000 senses and defined with 75,000 definition sentences. All definition sentences and example sentences have been rewritten to use only the 28,000 familiar open class words. The definition and example sentences have been treebanked with the JACY grammar (§ 2.4.2). 2.2 The Iwanami Dictionary of Japanese The Iwanami Kokugo Jiten (Iwanami) (Nishio et al., 1994) is a concise Japanese dictionary. A machine tractable version was made available by the Real World Computing Project for the SENSEVAL-2 Japanese lexical task (Shirai, 2003). Iwanami has 60,321 headwords and 85,870 word senses. Each sense in the dictionary consists of a sense ID and morphological information (word segmentation, POS tag, base form and reading, all manually post-edited). 2.3 The Gnu Contemporary International Dictionary of English The GNU Collaborative International Dictionary of English (GCIDE) is a freely available dictionary of English based on Webster’s Revised Unabridged Di</context>
</contexts>
<marker>Nishio, Iwabuchi, Mizutani, 1994</marker>
<rawString>Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani. 1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Towards terascale knowledge acquisition.</title>
<date>2004</date>
<booktitle>In 20th International Conference on Computational Linguistics: COLING-2004,</booktitle>
<pages>771--777</pages>
<location>Geneva.</location>
<contexts>
<context position="1885" citStr="Pantel et al., 2004" startWordPosition="271" endWordPosition="274">ns and languages. 1 Introduction Automatic methods of ontology acquisition have a long history in the field of natural language processing. The information contained in ontologies is important for a number of tasks, for example word sense disambiguation, question answering and machine translation. In this paper, we present the results of experiments conducted in automatic ontological acquisition over two languages, English and Japanese, and from three different machine-readable dictionaries. Useful semantic relations can be extracted from large corpora using relatively simple patterns (e.g., (Pantel et al., 2004)). While large corpora often contain information not found in lexicons, even a very large corpus may not include all the familiar words of a language, let alone those words occurring in useful patterns (Amano and Kondo, 1999). Therefore it makes sense to also extract data from machine readable dictionaries (MRDs). There is a great deal of work on the creation of ontologies from machine readable dictionaries (a good summary is (Wilkes et al., 1996)), mainly for English. Recently, there has also been interest in Japanese (Tokunaga et al., 2001; Nichols et al., 2005). Most approaches use either a</context>
</contexts>
<marker>Pantel, Ravichandran, Hovy, 2004</marker>
<rawString>Patrick Pantel, Deepak Ravichandran, and Eduard Hovy. 2004. Towards terascale knowledge acquisition. In 20th International Conference on Computational Linguistics: COLING-2004, pages 771–777. Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen D Richardson</author>
<author>William B Dolan</author>
<author>Lucy Vanderwende</author>
</authors>
<title>MindNet: acquiring and structuring semantic information from text.</title>
<date>1998</date>
<booktitle>In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics: COLING/ACL-98,</booktitle>
<pages>1098--1102</pages>
<location>Montreal.</location>
<contexts>
<context position="3441" citStr="Richardson et al., 1998" startWordPosition="530" endWordPosition="533">yet to be seen how the rules will scale when deeper semantic relations are extracted. In comparison, as we will demonstrate, our system produces comparable results while the framework is immediately applicable to any language with the resources to produce RMRS. Advances in the state-of-the-art in parsing have made it practical to use deep processing systems that produce rich syntactic and semantic analyses to parse lexicons. This high level of semantic information makes it easy to identify the relations between words that make up an ontology. Such an approach was taken by the MindNet project (Richardson et al., 1998). However, deep parsing systems often suffer from small lexicons and large amounts of parse ambiguity, making it difficult to apply this knowledge broadly. Our ontology extraction system uses Robust Minimal Recursion Semantics (RMRS), a formalism that provides a high level of detail while, at the same time, allowing for the flexibility of underspecification. RMRS encodes syntactic information in a general enough manner to make processing of and extraction from syntactic phenomena including coordination, relative clause analy10 Proceedings of the 2nd Workshop on Ontology Learning and Population</context>
</contexts>
<marker>Richardson, Dolan, Vanderwende, 1998</marker>
<rawString>Stephen D. Richardson, William B. Dolan, and Lucy Vanderwende. 1998. MindNet: acquiring and structuring semantic information from text. In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics: COLING/ACL-98, pages 1098–1102. Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
</authors>
<title>SENSEVAL-2 Japanese dictionary task.</title>
<date>2003</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>10</volume>
<issue>3</issue>
<pages>24</pages>
<note>(in Japanese).</note>
<contexts>
<context position="6594" citStr="Shirai, 2003" startWordPosition="1030" endWordPosition="1031">ual to five on a scale of one to seven. This gives 28,000 words divided into 46,000 senses and defined with 75,000 definition sentences. All definition sentences and example sentences have been rewritten to use only the 28,000 familiar open class words. The definition and example sentences have been treebanked with the JACY grammar (§ 2.4.2). 2.2 The Iwanami Dictionary of Japanese The Iwanami Kokugo Jiten (Iwanami) (Nishio et al., 1994) is a concise Japanese dictionary. A machine tractable version was made available by the Real World Computing Project for the SENSEVAL-2 Japanese lexical task (Shirai, 2003). Iwanami has 60,321 headwords and 85,870 word senses. Each sense in the dictionary consists of a sense ID and morphological information (word segmentation, POS tag, base form and reading, all manually post-edited). 2.3 The Gnu Contemporary International Dictionary of English The GNU Collaborative International Dictionary of English (GCIDE) is a freely available dictionary of English based on Webster’s Revised Unabridged Dictionary (published in 1913), and supplemented with entries from WordNet and additional submissions from users. It currently contains over 148,000 definitions. The version u</context>
</contexts>
<marker>Shirai, 2003</marker>
<rawString>Kiyoaki Shirai. 2003. SENSEVAL-2 Japanese dictionary task. Journal of Natural Language Processing, 10(3):3– 24. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Siegel</author>
</authors>
<title>HPSG analysis of Japanese. In Wahlster</title>
<date>2000</date>
<pages>265--280</pages>
<contexts>
<context position="10776" citStr="Siegel, 2000" startWordPosition="1671" endWordPosition="1672">ceHft*�A+Kt7DJ�,jid¯osha wo unten suru hito “a person who drives a car (lit: car-ACC drive do person)” are given in Figures 1 and 2 (omitting the indexing). Real predicates are prefixed by an under-bar ( ). The deep parse gives information about the scope, message types and argument structure, while the shallow parse gives little more than a list of real and grammatical predicates with a hook. 2.4.2 Deep Parsers (JACY, ERG and PET) For both Japanese and English, we used the PET System for the high-efficiency processing of typed feature structures (Callmeier, 2000). For Japanese, we used JACY (Siegel, 2000), for English we used the English Resource Grammar (ERG: Flickinger 2000).1 JACY The JACY grammar is an HPSG-based grammar of Japanese which originates from work done in the Verbmobil project (Siegel, 2000) on machine translation of spoken dialogues in the domain of travel planning. It has since been extended to accommodate written Japanese and new domains (such as electronic commerce customer email and machine readable dictionaries). The grammar implementation is based on a system of types. There are around 900 lexical types that define the syntactic, semantic and pragmatic properties of the </context>
</contexts>
<marker>Siegel, 2000</marker>
<rawString>Melanie Siegel. 2000. HPSG analysis of Japanese. In Wahlster (2000), pages 265–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathrin Spreyer</author>
<author>Anette Frank</author>
</authors>
<title>The TIGER RMRS 700 bank: RMRS construction from dependencies.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Workshop on Linguistically Interpreted Corpora (LINC</booktitle>
<pages>1--10</pages>
<publisher>Jeju Island,</publisher>
<contexts>
<context position="14916" citStr="Spreyer and Frank (2005)" startWordPosition="2420" endWordPosition="2423">added for all missing words in the alphabetically first 10,000 definition sentences. These first 10,000 sentences were parsed and then manually tree-banked to provide the training material for constructing the stochastic model used for best-only parsing of the rest of the definition sentences. Using POS-based unknown-word guessing for missing lexical entries, MRSes were obtained for about 75% of the first 100,000 definition sentences. 2.4.3 Medium Parser (CaboCha-RMRS) For Japanese, we produce RMRS from the dependency parser Cabocha (Kudo and Matsumoto, 2002). The method is similar to that of Spreyer and Frank (2005), who produce RMRS from detailed German dependencies. CaboCha provides fairly minimal dependencies: there are three links (dependent, parallel, apposition) and they link base phrases (Japanese bunsetsu), marked with the syntactic and semantic head. The CaboChaRMRS parser uses this information, along with heuristics based on the parts-of-speech, to produce underspecified RMRSs. CaboCha-RMRS is capable of making use of HPSG resources, including verbal case frames, to further enrich its output. This allows it to produce RMRS that approaches the granularity of the analyses given by HPSG parsers. I</context>
</contexts>
<marker>Spreyer, Frank, 2005</marker>
<rawString>Kathrin Spreyer and Anette Frank. 2005. The TIGER RMRS 700 bank: RMRS construction from dependencies. In Proceedings of the 6th International Workshop on Linguistically Interpreted Corpora (LINC 2005), pages 1–10. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takenobu Tokunaga</author>
<author>Yasuhiro Syotu</author>
<author>Hozumi Tanaka</author>
<author>Kiyoaki Shirai</author>
</authors>
<title>Integration of heterogeneous language resources: A monolingual dictionary and a thesaurus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium, NLPRS2001,</booktitle>
<pages>135--142</pages>
<location>Tokyo.</location>
<contexts>
<context position="2432" citStr="Tokunaga et al., 2001" startWordPosition="362" endWordPosition="365">arge corpora using relatively simple patterns (e.g., (Pantel et al., 2004)). While large corpora often contain information not found in lexicons, even a very large corpus may not include all the familiar words of a language, let alone those words occurring in useful patterns (Amano and Kondo, 1999). Therefore it makes sense to also extract data from machine readable dictionaries (MRDs). There is a great deal of work on the creation of ontologies from machine readable dictionaries (a good summary is (Wilkes et al., 1996)), mainly for English. Recently, there has also been interest in Japanese (Tokunaga et al., 2001; Nichols et al., 2005). Most approaches use either a specialized parser or a set of regular expressions tuned to a particular dictionary, often with hundreds of rules. Agirre et al. (2000) extracted taxonomic relations from a Basque dictionary with high accuracy using Constraint Grammar together with hand-crafted rules. However, such a system is limited to one language, and it has yet to be seen how the rules will scale when deeper semantic relations are extracted. In comparison, as we will demonstrate, our system produces comparable results while the framework is immediately applicable to an</context>
<context position="24123" citStr="Tokunaga et al. (2001)" startWordPosition="3894" endWordPosition="3897">of the coverage of the ontologies being used in comparison. Lexeed has a coverage of approx. 55.74% ( 58,867 105,613), with Iwanami the lowest at 48.20% ( 88,662 183,927), and GCIDE the highest at 69.85% (45,814 65,593). It is clear that there are a lot of relations in each lexicon that are not covered by the hand-crafted ontologies. This demonstrates that machine-readable dictionaries are still a valuable resource for constructing ontologies. 4.1.1 Lexeed Our results using JACY achieve a confirmation rate of 66.84% for nouns only and 60.67% overall (Table 3). This is an improvement over both Tokunaga et al. (2001), who reported 61.4% for nouns only, and Nichols et al. (2005) who reported 63.31% for nouns and 57.74% overall. We also achieve an impressive 33,333 confirmed relations for a rate of 56.62% overall. It is important to note that our total counts include all unique relations regardless of source, unlike Nichols et al. (2005) who take only the relation from the deepest source whenever multiple relations are extracted. It is interesting to note that shallow processing out performs medium with 22,540 verified relations (59.40%) compared to 21,806 (57.76%). This would seem to suggest that for the s</context>
<context position="29136" citStr="Tokunaga et al. (2001)" startWordPosition="4823" endWordPosition="4826">oftware driver” but including spanker “a kind of sail”. It should be beneficial to propagate these different senses across ontologies. 5 Discussion and Future Work We were able to successfully combine deep processing of various levels of depth in order to extract ontological information from lexical resources. We showed that, by using a well defined semantic representation, the extraction can be generalized so much that it can be used on very different dictionaries from different languages. This is an improvement on the common approach to using more and more detailed regular expressions (e.g. Tokunaga et al. (2001)). Although this provides a quick start, the results are not generally reusable. In comparison, the shallower RMRS engines are immediately useful for a variety of other tasks. However, because the hook is the only syntactic information returned by the shallow parser, ontological relation extraction is essentially performed by this hook-identifying heuristic. While this is sufficient for a large number of sentences, it is not possible to process special relations with the shallow parser since none of the arguments are linked with the predicates to which they belong. Thus, as Table 2 shows, our </context>
</contexts>
<marker>Tokunaga, Syotu, Tanaka, Shirai, 2001</marker>
<rawString>Takenobu Tokunaga, Yasuhiro Syotu, Hozumi Tanaka, and Kiyoaki Shirai. 2001. Integration of heterogeneous language resources: A monolingual dictionary and a thesaurus. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium, NLPRS2001, pages 135– 142. Tokyo.</rawString>
</citation>
<citation valid="true">
<title>Verbmobil: Foundations of Speech-to-Speech Translation.</title>
<date>2000</date>
<editor>Wolfgang Wahlster, editor.</editor>
<publisher>Springer,</publisher>
<location>Berlin, Germany. Yorick</location>
<contexts>
<context position="2621" citStr="(2000)" startWordPosition="397" endWordPosition="397">words of a language, let alone those words occurring in useful patterns (Amano and Kondo, 1999). Therefore it makes sense to also extract data from machine readable dictionaries (MRDs). There is a great deal of work on the creation of ontologies from machine readable dictionaries (a good summary is (Wilkes et al., 1996)), mainly for English. Recently, there has also been interest in Japanese (Tokunaga et al., 2001; Nichols et al., 2005). Most approaches use either a specialized parser or a set of regular expressions tuned to a particular dictionary, often with hundreds of rules. Agirre et al. (2000) extracted taxonomic relations from a Basque dictionary with high accuracy using Constraint Grammar together with hand-crafted rules. However, such a system is limited to one language, and it has yet to be seen how the rules will scale when deeper semantic relations are extracted. In comparison, as we will demonstrate, our system produces comparable results while the framework is immediately applicable to any language with the resources to produce RMRS. Advances in the state-of-the-art in parsing have made it practical to use deep processing systems that produce rich syntactic and semantic ana</context>
<context position="25062" citStr="(2000)" startWordPosition="4050" endWordPosition="4050">e only the relation from the deepest source whenever multiple relations are extracted. It is interesting to note that shallow processing out performs medium with 22,540 verified relations (59.40%) compared to 21,806 (57.76%). This would seem to suggest that for the simplest task of retrieving hyperynms and synonyms, more information than that is not necessary. However, since medium and deep parsing obtain relations not covered by shallow parsing and can extract special relations, a task that cannot be performed without syntactic information, it is beneficial to use them as well. Agirre et al. (2000) reported an error rate of 2.8% in a hand-evaluation of the semantic relations they automatically extracted from a machinereadable Basque dictionary. In a similar handevaluation of a stratified sampling of relations extracted from Lexeed, we achieved an error rate of 9.2%, demonstrating that our method is also highly accurate (Nichols et al., 2005). 4.2 Iwanami Iwanami’s verification results are similar to Lexeed’s (Table 3). There are on average around 3% more verifications and a total of almost 20,000 more verified relations extracted. It is particularly interesting to note that deep process</context>
</contexts>
<marker>2000</marker>
<rawString>Wolfgang Wahlster, editor. 2000. Verbmobil: Foundations of Speech-to-Speech Translation. Springer, Berlin, Germany. Yorick A. Wilkes, Brian M. Slator, and Louise M. Guthrie.</rawString>
</citation>
<citation valid="true">
<title>Electric Words.</title>
<date>1996</date>
<publisher>MIT Press.</publisher>
<marker>1996</marker>
<rawString>1996. Electric Words. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>