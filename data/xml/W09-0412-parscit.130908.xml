<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039062">
<title confidence="0.987404">
NUS at WMT09: Domain Adaptation Experiments for English-Spanish
Machine Translation of News Commentary Text
</title>
<author confidence="0.992662">
Preslav Nakov
</author>
<affiliation confidence="0.999826">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.974817">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.998496">
nakov@comp.nus.edu.sg
</email>
<author confidence="0.98595">
Hwee Tou Ng
</author>
<affiliation confidence="0.999807">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.9747765">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.998544">
nght@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993873" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959947368421">
We describe the system developed by the
team of the National University of Singa-
pore for English to Spanish machine trans-
lation of News Commentary text for the
WMT09 Shared Translation Task. Our
approach is based on domain adaptation,
combining a small in-domain News Com-
mentary bi-text and a large out-of-domain
one from the Europarl corpus, from which
we built and combined two separate phrase
tables. We further combined two language
models (in-domain and out-of-domain),
and we experimented with cognates, im-
proved tokenization and recasing, achiev-
ing the highest lowercased NIST score of
6.963 and the second best lowercased Bleu
score of 24.91% for training without us-
ing additional external data for English-to-
Spanish translation at the shared task.
</bodyText>
<sectionHeader confidence="0.999097" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995571769230769">
Modern Statistical Machine Translation (SMT)
systems are typically trained on sentence-aligned
parallel texts (bi-texts) from a particular domain.
When tested on text from that domain, they
demonstrate state-of-the art performance, but on
out-of-domain test data the results can deteriorate
significantly. For example, on the WMT06 Shared
Translation Task, the scores for French-to-English
translation dropped from about 30 to about 20
Bleu points for nearly all systems when tested on
News Commentary instead of the Europarl1 text,
which was used for training (Koehn and Monz,
2006).
</bodyText>
<footnote confidence="0.810674">
1See (Koehn, 2005) for details about the Europarl corpus.
</footnote>
<bodyText confidence="0.9999378">
Subsequently, in 2007 and 2008, the WMT
Shared Translation Task organizers provided a
limited amount of bilingual News Commentary
training data (1-1.3M words) in addition to the
large amount of Europarl data (30-32M words),
and set up separate evaluations on News Commen-
tary and on Europarl data, thus inviting interest in
domain adaptation experiments for the News do-
main (Callison-Burch et al., 2007; Callison-Burch
et al., 2008). This year, the evaluation is on News
Commentary only, which makes domain adapta-
tion the central focus of the Shared Translation
Task.
The team of the National University of Singa-
pore (NUS) participated in the WMT09 Shared
Translation Task with an English-to-Spanish sys-
tem.2 Our approach is based on domain adapta-
tion, combining the small in-domain News Com-
mentary bi-text (1.8M words) and the large out-
of-domain one from the Europarl corpus (40M
words), from which we built and combined two
separate phrase tables. We further used two
language models (in-domain and out-of-domain),
cognates, improved tokenization, and additional
smart recasing as a post-processing step.
</bodyText>
<sectionHeader confidence="0.95659" genericHeader="method">
2 The NUS System
</sectionHeader>
<bodyText confidence="0.999768">
Below we describe separately the standard and the
nonstandard settings of our system.
</bodyText>
<subsectionHeader confidence="0.997235">
2.1 Standard Settings
</subsectionHeader>
<bodyText confidence="0.999513">
In our baseline experiments, we used the follow-
ing general setup: First, we tokenized the par-
</bodyText>
<footnote confidence="0.9989396">
2The task organizers invited submissions translating for-
ward and/or backward between English and five other Euro-
pean languages (French, Spanish, German, Czech and Hun-
garian), but we only participated in English→Spanish, due to
time limitations.
</footnote>
<note confidence="0.4985225">
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 75–79,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999353">
75
</page>
<bodyText confidence="0.999969647058824">
allel bi-text, converted it to lowercase, and fil-
tered out the overly-long training sentences, which
complicate word alignments (we tried maximum
length limits of 40 and 100). We then built sep-
arate English-to-Spanish and Spanish-to-English
directed word alignments using IBM model 4
(Brown et al., 1993), combined them using the in-
tersect+grow heuristic (Och and Ney, 2003), and
extracted phrase-level translation pairs of maxi-
mum length 7 using the alignment template ap-
proach (Och and Ney, 2004). We thus obtained
a phrase table where each phrase translation pair
is associated with the following five standard pa-
rameters: forward and reverse phrase translation
probabilities, forward and reverse lexical transla-
tion probabilities, and phrase penalty.
We then trained a log-linear model using the
standard feature functions: language model proba-
bility, word penalty, distortion costs (we tried dis-
tance based and lexicalized reordering models),
and the parameters from the phrase table. We
set all feature weights by optimizing Bleu (Pap-
ineni et al., 2002) directly using minimum error
rate training (MERT) (Och, 2003) on the tuning
part of the development set (dev-test2009a).
We used these weights in a beam search decoder
(Koehn et al., 2007) to translate the test sentences
(the English part of dev-test2009b, tokenized
and lowercased). We then recased the output us-
ing a monotone model that translates from low-
ercase to uppercase Spanish, we post-cased it us-
ing a simple heuristic, de-tokenized the result, and
compared it to the gold standard (the Spanish part
of dev-test2009b) using Bleu and NIST.
</bodyText>
<subsectionHeader confidence="0.998785">
2.2 Nonstandard Settings
</subsectionHeader>
<bodyText confidence="0.99765155">
The nonstandard features of our system can be
summarized as follows:
Two Language Models. Following Nakov
and Hearst (2007), we used two language mod-
els (LM) – an in-domain one (trained on a con-
catenation of the provided monolingual Spanish
News Commentary data and the Spanish side of the
training News Commentary bi-text) and an out-of-
domain one (trained on the provided monolingual
Spanish Europarl data). For both LMs, we used
5-gram models with Kneser-Ney smoothing.
Merging Two Phrase Tables. Following
Nakov (2008), we trained and merged two phrase-
based SMT systems: a small in-domain one using
the News Commentary bi-text, and a large out-of-
domain one using the Europarl bi-text. As a result,
we obtained two phrase tables, Tnews and Teuro,
and two lexicalized reordering models, Rnews and
Reuro. We merged the phrase table as follows.
First, we kept all phrase pairs from Tnews. Then
we added those phrase pairs from Teuro which
were not present in Tnews. For each phrase pair
added, we retained its associated features: forward
and reverse phrase translation probabilities, for-
ward and reverse lexical translation probabilities,
and phrase penalty. We further added two new fea-
tures, Fnews and Feuro, which show the source of
each phrase. Their values are 1 and 0.5 when the
phrase was extracted from the News Commentary
bi-text, 0.5 and 1 when it was extracted from the
Europarl bi-text, and 1 and 1 when it was extracted
from both. As a result, we ended up with seven pa-
rameters for each entry in the merged phrase table.
Merging Two Lexicalized Reordering Tables.
When building the two phrase tables, we also
built two lexicalized reordering tables (Koehn et
al., 2005) for them, Rnews and Reuro, which we
merged as follows: We first kept all phrases from
Rnews, then we added those from Reuro which
were not present in Rnews. This resulting lexical-
ized reordering table was used together with the
above-described merged phrase table.
Cognates. Previous research has shown that us-
ing cognates can yield better word alignments (Al-
Onaizan et al., 1999; Kondrak et al., 2003), which
in turn often means higher-quality phrase pairs and
better SMT systems. Linguists define cognates
as words derived from a common root (Bickford
and Tuggy, 2002). Following previous researchers
in computational linguistics (Bergsma and Kon-
drak, 2007; Mann and Yarowsky, 2001; Melamed,
1999), however, we adopted a simplified definition
which ignores origin, defining cognates as words
in different languages that are mutual translations
and have a similar orthography. We extracted and
used such potential cognates in order to bias the
training of the IBM word alignment models. Fol-
lowing Melamed (1995), we measured the ortho-
graphic similarity using longest common subse-
quence ratio (LCSR), which is defined as follows:
</bodyText>
<equation confidence="0.990271">
LCSR(s1, s2) = |LCS(s1,s2)|
Max(|s1|,|s2|)
</equation>
<bodyText confidence="0.9947104">
where LCS(s1, s2) is the longest common subse-
quence of s1 and s2, and Is is the length of s.
Following Nakov et al. (2007), we combined the
LCSR similarity measure with competitive linking
(Melamed, 2000) in order to extract potential cog-
</bodyText>
<page confidence="0.926891">
76
</page>
<bodyText confidence="0.999376142857143">
nates from the training bi-text. Competitive link-
ing assumes that, given a source English sentence
and its Spanish translation, a source word is ei-
ther translated with a single target word or is not
translated at all. Given an English-Spanish sen-
tence pair, we calculated LCSR for all cross-lingual
word pairs (excluding stopwords and words of
length 3 or less), which induced a fully-connected
weighted bipartite graph. Then, we performed a
greedy approximation to the maximum weighted
bipartite matching in that graph (competitive link-
ing) as follows: First, we aligned the most sim-
ilar pair of unaligned words and we discarded
these words from further consideration. Then, we
aligned the next most similar pair of unaligned
words, and so forth. The process was repeated un-
til there were no words left or the maximal word
pair similarity fell below a pre-specified threshold
0 (0 &lt; 0 &lt; 1), which typically left some words
unaligned.3 As a result we ended up with a list C
of potential cognate pairs. Following (Al-Onaizan
et al., 1999; Kondrak et al., 2003; Nakov et al.,
2007) we filtered out the duplicates in C, and we
added the remaining cognate pairs as additional
“sentence” pairs to the bi-text in order to bias the
subsequent training of the IBM word alignment
models.
Improved (De-)tokenization. The default to-
kenizer does not split on hyphenated compound
words like nation-building, well-rehearsed, self-
assured, Arab-Israeli, domestically-oriented, etc.
While linguistically correct, this can be problem-
atic for machine translation since it can cause data
sparsity issues. For example, the system might
know how to translate into Spanish both well and
rehearsed, but not well-rehearsed, and thus at
translation time it would be forced to handle it as
an unknown word, i.e., copy it to the output un-
translated. A similar problem is related to double
dashes, as illustrated by the following training sen-
tence: “So the question now is what can China do
to freeze--and, if possible, to reverse--North Ko-
rea’s nuclear program.” We changed the tokenizer
so that it splits on ‘-’ and ‘--’; we altered the de-
tokenizer accordingly.
Improved Recaser. The default recaser sug-
gested by the WMT09 organizers was based on a
monotone translation model. We trained such a
recaser on the Spanish side of the News Commen-
</bodyText>
<footnote confidence="0.739422333333333">
3For News Commentary, we used 0 = 0.4, which was
found by optimizing on the development set; for Europarl,
we set 0 = 0.58 as suggested by Kondrak et al. (2003).
</footnote>
<bodyText confidence="0.9990518125">
tary bi-text that translates from lowercase to up-
percase Spanish. While being good overall, it had
a problem with unknown words, leaving them in
lowercase. In a News Commentary text, however,
most unknown words are named entities – persons,
organization, locations – which are spelled with a
capitalized initial in Spanish. Therefore, we used
an additional recasing script, which runs over the
output of the default recaser and sets the casing of
the unknown words to the original casing they had
in the English input. It also makes sure all sen-
tences start with a capitalized initial.
Rule-based Post-editing. We did a quick study
of the system errors on the development set, and
we designed some heuristic post-editing rules,
e.g.,
</bodyText>
<listItem confidence="0.941481857142857">
• ? or ! without L or ¡ to the left: if so, we
insert X/¡ at the sentence beginning;
• numbers: we change English numbers like
1,185.32 to Spanish-style 1.185,32;
• duplicate punctuation: we remove dupli-
cate sentence end markers, quotes, commas,
parentheses, etc.
</listItem>
<sectionHeader confidence="0.984277" genericHeader="evaluation">
3 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.99994568">
Table 1 shows the performance of a simple
baseline system and the impact of different
cumulative modifications to that system when
tuning on dev-test2009a and testing on
dev-test2009b. The table report the Bleu and
NIST scores measured on the detokenized out-
put under three conditions: (1) without recasing
(‘Lowercased’), 2) using the default recaser (‘Re-
cased (default)’), and (3) using an improved re-
caser and post-editing rules Post-cased &amp; Post-
edited’). In the following discussion, we will dis-
cuss the Bleu results under condition (3).
System 1 uses sentences of length up to 40
tokens from the News Commentary bi-text, the
default (de-)tokenizer, distance reordering, and a
3-gram language model trained on the Spanish
side of the bi-text. Its performance is quite mod-
est: 15.32% of Bleu with the default recaser, and
16.92% when the improved recaser and the post-
editing rules are used.
System 2 increases to 100 the maximum length
of the sentences in the bi-text, which yields 0.55%
absolute improvement in Bleu.
System 3 uses the new (de-)tokenizer, but this
turns out to make almost no difference.
</bodyText>
<page confidence="0.998283">
77
</page>
<table confidence="0.997901384615385">
# Bitezt System Lowercased Recased Post-cased &amp;
Bleu NIST (default) Post-edited
Bleu NIST Bleu NIST
1 news News Commentary baseline 18.38 5.7837 15.32 5.2266 16.92 5.5091
2 news + max sentence length 100 18.91 5.8540 15.93 5.3119 17.47 5.5874
3 news + improved (de-)tokenizer 18.96 5.8706 15.97 5.3254 17.48 5.6020
4 news + lexicalized reordering 19.81 5.9422 16.64 5.3793 18.28 5.6696
5 news + LM: old+monol. News, 5-gram 22.29 6.2791 18.91 5.6901 20.55 5.9924
6 news + LM2: Europarl, 5-gram 22.46 6.2438 19.10 5.6606 20.75 5.9570
7 news + cognates 23.14 6.3504 19.64 5.7478 21.32 6.0478
8 euro Europarl (- system 6) 23.73 6.4673 20.23 5.8707 21.89 6.1577
9 euro + cognates (- system 7) 23.95 6.4709 20.44 5.8742 22.10 6.1607
10 both Combining 7 &amp; 9 24.40 6.5723 20.74 5.9575 22.37 6.2506
</table>
<tableCaption confidence="0.999339">
Table 1: Impact of the combined modifications for English-to-Spanish machine translation on
</tableCaption>
<bodyText confidence="0.990594113636364">
dev-test2009b. We report the Bleu and NIST scores measured on the detokenized output under
three conditions: (1) without recasing (‘Lowercased’), (2) using the default recaser (‘Recased (default)’),
and (3) using an improved recaser and post-editing rules (‘Post-cased &amp; Post-edited’). The News Com-
mentary baseline system uses sentences of length up to 40 tokens from the News Commentary bi-text,
the default tokenizer and de-tokenizer, a distance-based reordering model, and a trigram language model
trained on the Spanish side of the bi-text. The Europarl system is the same as system 6, except that it
uses the Europarl bi-text instead of the News Commentary bi-text.
System 4 adds a lexicalized re-ordering model,
which yields 0.8% absolute improvement.
System 5 improves the language model. It adds
the additional monolingual Spanish News Com-
mentary data provided by the organizers to the
Spanish side of the bi-text, and uses a 5-gram lan-
guage model instead of the 3-gram LM used by
Systems 1-4. This yields a sizable absolute gain in
Bleu: 2.27%.
System 6 adds a second 5-gram LM trained on
the monolingual Europarl data, gaining 0.2%.
System 7 augments the training bi-text with
cognate pairs, gaining another 0.57%.
System 8 is the same as System 6, except that
it is trained on the out-of-domain Europarl bi-
text instead of the in-domain News Commentary
bi-text. Surprisingly, this turns out to work bet-
ter than the in-domain System 6 by 1.14% of
Bleu. This is a quite surprising result since in
both WMT07 and WMT08, for which compara-
ble kinds and size of training data was provided,
training on the out-of-domain Europarl was al-
ways worse than training on the in-domain News
Commentary. We are not sure why it is different
this year, but it could be due to the way the dev-
train and dev-test was created for the 2009 data –
by extracting alternating sentences from the origi-
nal development set.
System 9 augments the Europarl bi-text with
cognate pairs, gaining another 0.21%.
System 10 merges the phrase tables of systems
7 and 9, and is otherwise the same as them. This
adds another 0.27%.
Our official submission to WMT09 is the post-
edited System 10, re-tuned on the full development
set: dev-test2009a+ dev-test2009b (in
order to produce more stable results with MERT).
</bodyText>
<sectionHeader confidence="0.982189" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999991692307692">
As we can see in Table 1, we have achieved not
only a huge ‘vertical’ absolute improvement of
5.5-6% in Bleu from System 1 to System 10, but
also a significant ‘horizontal’ one: our recased and
post-edited result for System 10 is better than that
of the default recaser by 1.63% in Bleu (22.37%
vs. 20.74%). Still, the lowercased Bleu of 24.40%
suggests that there may be a lot of room for fur-
ther improvement in recasing – we are still about
2% below it. While this is probably due primarily
to the system choosing a different sentence-initial
word, it certainly deserves further investigation in
future work.
</bodyText>
<sectionHeader confidence="0.997923" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.619294">
This research was supported by research grant
POD0713875.
</bodyText>
<page confidence="0.997243">
78
</page>
<sectionHeader confidence="0.989153" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999845327272727">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz Joseph
Och, David Purdy, Noah Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, CLSP, Johns Hopkins University,
Baltimore, MD.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’07), pages
656–663, Prague, Czech Republic.
Albert Bickford and David Tuggy. 2002.
Electronic glossary of linguistic terms.
http://www.sil.org/mexico/ling/glosario/E005ai-
Glossary.htm.
Peter Brown, Vincent Della Pietra, Stephen Della
Pietra, and Robert Mercer. 1993. The mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 136–158, Prague, Czech
Republic.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70–106, Columbus,
OH, USA.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of the First
Workshop on Statistical Machine Translation, pages
102–121, New York, NY, USA.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation (IWSLT’05), Pitts-
burgh, PA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL’07). Demonstration session, pages 177–180,
Prague, Czech Republic.
P. Koehn. 2005. Europarl: A parallel corpus for eval-
uation of machine translation. In Proceedings of the
X MT Summit, pages 79–86, Phuket, Thailand.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proceedings of the Annual Meeting ofthe
North American Association for Computational Lin-
guistics (NAACL’03), pages 46–48, Sapporo, Japan.
Gideon Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages.
In Proceedings of the Annual Meeting of the North
American Association for Computational Linguis-
tics (NAACL’01), pages 1–8, Pittsburgh, PA, USA.
Dan Melamed. 1995. Automatic evaluation and uni-
form filter cascades for inducing N-best translation
lexicons. In Proceedings of the Third Workshop on
Very Large Corpora, pages 184–198, Cambridge,
MA, USA.
Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107–130.
Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221–249.
Preslav Nakov and Marti Hearst. 2007. UCB system
description for the WMT 2007 shared task. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 212–215, Prague, Czech
Republic.
Preslav Nakov, Svetlin Nakov, and Elena Paskaleva.
2007. Improved word alignments using the Web as
a corpus. In Proceedigs of Recent Advances in Nat-
ural Language Processing (RANLP’07), pages 400–
405, Borovets, Bulgaria.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 147–150,
Columbus, OH, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL’03), pages 160–167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL’02), pages 311–318,
Philadelphia, PA, USA.
</reference>
<page confidence="0.999049">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.275435">
<title confidence="0.976002">NUS at WMT09: Domain Adaptation Experiments for Machine Translation of News Commentary Text</title>
<author confidence="0.757188">Preslav</author>
<affiliation confidence="0.911093666666667">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.937566">Singapore</address>
<email confidence="0.978686">nakov@comp.nus.edu.sg</email>
<author confidence="0.977942">Hwee Tou</author>
<affiliation confidence="0.911509333333333">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.939476">Singapore</address>
<email confidence="0.990764">nght@comp.nus.edu.sg</email>
<abstract confidence="0.99068355">We describe the system developed by the team of the National University of Singapore for English to Spanish machine translation of News Commentary text for the WMT09 Shared Translation Task. Our approach is based on domain adaptation, a small in-domain Comand a large out-of-domain from the from which we built and combined two separate phrase tables. We further combined two language models (in-domain and out-of-domain), and we experimented with cognates, improved tokenization and recasing, achieving the highest lowercased NIST score of 6.963 and the second best lowercased Bleu score of 24.91% for training without using additional external data for English-to- Spanish translation at the shared task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz Joseph Och</author>
<author>David Purdy</author>
<author>Noah Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation.</title>
<date>1999</date>
<tech>Technical report, CLSP,</tech>
<institution>Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<contexts>
<context position="9316" citStr="Al-Onaizan et al., 1999" startWordPosition="1456" endWordPosition="1459"> graph. Then, we performed a greedy approximation to the maximum weighted bipartite matching in that graph (competitive linking) as follows: First, we aligned the most similar pair of unaligned words and we discarded these words from further consideration. Then, we aligned the next most similar pair of unaligned words, and so forth. The process was repeated until there were no words left or the maximal word pair similarity fell below a pre-specified threshold 0 (0 &lt; 0 &lt; 1), which typically left some words unaligned.3 As a result we ended up with a list C of potential cognate pairs. Following (Al-Onaizan et al., 1999; Kondrak et al., 2003; Nakov et al., 2007) we filtered out the duplicates in C, and we added the remaining cognate pairs as additional “sentence” pairs to the bi-text in order to bias the subsequent training of the IBM word alignment models. Improved (De-)tokenization. The default tokenizer does not split on hyphenated compound words like nation-building, well-rehearsed, selfassured, Arab-Israeli, domestically-oriented, etc. While linguistically correct, this can be problematic for machine translation since it can cause data sparsity issues. For example, the system might know how to translate</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz Joseph Och, David Purdy, Noah Smith, and David Yarowsky. 1999. Statistical machine translation. Technical report, CLSP, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Alignment-based discriminative string similarity.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL’07),</booktitle>
<pages>656--663</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7524" citStr="Bergsma and Kondrak, 2007" startWordPosition="1162" endWordPosition="1166">we merged as follows: We first kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews. This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR(s1, s2) = |LCS(s1,s2)| Max(|s1|,|s2|) where LCS(s1, s2) is the longest common subsequence of s1 and s2, and Is is the lengt</context>
</contexts>
<marker>Bergsma, Kondrak, 2007</marker>
<rawString>Shane Bergsma and Grzegorz Kondrak. 2007. Alignment-based discriminative string similarity. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL’07), pages 656–663, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bickford</author>
<author>David Tuggy</author>
</authors>
<title>Electronic glossary of linguistic terms.</title>
<date>2002</date>
<note>http://www.sil.org/mexico/ling/glosario/E005aiGlossary.htm.</note>
<contexts>
<context position="7436" citStr="Bickford and Tuggy, 2002" startWordPosition="1152" endWordPosition="1155">two lexicalized reordering tables (Koehn et al., 2005) for them, Rnews and Reuro, which we merged as follows: We first kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews. This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR(s1, s2) = |LCS(s1,s2)| Max(|s1|,|s2</context>
</contexts>
<marker>Bickford, Tuggy, 2002</marker>
<rawString>Albert Bickford and David Tuggy. 2002. Electronic glossary of linguistic terms. http://www.sil.org/mexico/ling/glosario/E005aiGlossary.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Vincent Della Pietra</author>
<author>Stephen Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3853" citStr="Brown et al., 1993" startWordPosition="573" endWordPosition="576">uropean languages (French, Spanish, German, Czech and Hungarian), but we only participated in English→Spanish, due to time limitations. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 75–79, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 75 allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distanc</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Vincent Della Pietra, Stephen Della Pietra, and Robert Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>136--158</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2202" citStr="Callison-Burch et al., 2007" startWordPosition="322" endWordPosition="325">out 30 to about 20 Bleu points for nearly all systems when tested on News Commentary instead of the Europarl1 text, which was used for training (Koehn and Monz, 2006). 1See (Koehn, 2005) for details about the Europarl corpus. Subsequently, in 2007 and 2008, the WMT Shared Translation Task organizers provided a limited amount of bilingual News Commentary training data (1-1.3M words) in addition to the large amount of Europarl data (30-32M words), and set up separate evaluations on News Commentary and on Europarl data, thus inviting interest in domain adaptation experiments for the News domain (Callison-Burch et al., 2007; Callison-Burch et al., 2008). This year, the evaluation is on News Commentary only, which makes domain adaptation the central focus of the Shared Translation Task. The team of the National University of Singapore (NUS) participated in the WMT09 Shared Translation Task with an English-to-Spanish system.2 Our approach is based on domain adaptation, combining the small in-domain News Commentary bi-text (1.8M words) and the large outof-domain one from the Europarl corpus (40M words), from which we built and combined two separate phrase tables. We further used two language models (in-domain and o</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 136–158, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="2232" citStr="Callison-Burch et al., 2008" startWordPosition="326" endWordPosition="329">s for nearly all systems when tested on News Commentary instead of the Europarl1 text, which was used for training (Koehn and Monz, 2006). 1See (Koehn, 2005) for details about the Europarl corpus. Subsequently, in 2007 and 2008, the WMT Shared Translation Task organizers provided a limited amount of bilingual News Commentary training data (1-1.3M words) in addition to the large amount of Europarl data (30-32M words), and set up separate evaluations on News Commentary and on Europarl data, thus inviting interest in domain adaptation experiments for the News domain (Callison-Burch et al., 2007; Callison-Burch et al., 2008). This year, the evaluation is on News Commentary only, which makes domain adaptation the central focus of the Shared Translation Task. The team of the National University of Singapore (NUS) participated in the WMT09 Shared Translation Task with an English-to-Spanish system.2 Our approach is based on domain adaptation, combining the small in-domain News Commentary bi-text (1.8M words) and the large outof-domain one from the Europarl corpus (40M words), from which we built and combined two separate phrase tables. We further used two language models (in-domain and out-of-domain), cognates, impro</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 70–106, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between European languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the First Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="1741" citStr="Koehn and Monz, 2006" startWordPosition="250" endWordPosition="253">on at the shared task. 1 Introduction Modern Statistical Machine Translation (SMT) systems are typically trained on sentence-aligned parallel texts (bi-texts) from a particular domain. When tested on text from that domain, they demonstrate state-of-the art performance, but on out-of-domain test data the results can deteriorate significantly. For example, on the WMT06 Shared Translation Task, the scores for French-to-English translation dropped from about 30 to about 20 Bleu points for nearly all systems when tested on News Commentary instead of the Europarl1 text, which was used for training (Koehn and Monz, 2006). 1See (Koehn, 2005) for details about the Europarl corpus. Subsequently, in 2007 and 2008, the WMT Shared Translation Task organizers provided a limited amount of bilingual News Commentary training data (1-1.3M words) in addition to the large amount of Europarl data (30-32M words), and set up separate evaluations on News Commentary and on Europarl data, thus inviting interest in domain adaptation experiments for the News domain (Callison-Burch et al., 2007; Callison-Burch et al., 2008). This year, the evaluation is on News Commentary only, which makes domain adaptation the central focus of th</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between European languages. In Proceedings of the First Workshop on Statistical Machine Translation, pages 102–121, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT’05),</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="6865" citStr="Koehn et al., 2005" startWordPosition="1059" endWordPosition="1062">ation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We further added two new features, Fnews and Feuro, which show the source of each phrase. Their values are 1 and 0.5 when the phrase was extracted from the News Commentary bi-text, 0.5 and 1 when it was extracted from the Europarl bi-text, and 1 and 1 when it was extracted from both. As a result, we ended up with seven parameters for each entry in the merged phrase table. Merging Two Lexicalized Reordering Tables. When building the two phrase tables, we also built two lexicalized reordering tables (Koehn et al., 2005) for them, Rnews and Reuro, which we merged as follows: We first kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews. This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous research</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT’05), Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL’07). Demonstration session,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="4797" citStr="Koehn et al., 2007" startWordPosition="719" endWordPosition="722">ward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from the phrase table. We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate training (MERT) (Och, 2003) on the tuning part of the development set (dev-test2009a). We used these weights in a beam search decoder (Koehn et al., 2007) to translate the test sentences (the English part of dev-test2009b, tokenized and lowercased). We then recased the output using a monotone model that translates from lowercase to uppercase Spanish, we post-cased it using a simple heuristic, de-tokenized the result, and compared it to the gold standard (the Spanish part of dev-test2009b) using Bleu and NIST. 2.2 Nonstandard Settings The nonstandard features of our system can be summarized as follows: Two Language Models. Following Nakov and Hearst (2007), we used two language models (LM) – an in-domain one (trained on a concatenation of the pr</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL’07). Demonstration session, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the X MT Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="1761" citStr="Koehn, 2005" startWordPosition="255" endWordPosition="256">oduction Modern Statistical Machine Translation (SMT) systems are typically trained on sentence-aligned parallel texts (bi-texts) from a particular domain. When tested on text from that domain, they demonstrate state-of-the art performance, but on out-of-domain test data the results can deteriorate significantly. For example, on the WMT06 Shared Translation Task, the scores for French-to-English translation dropped from about 30 to about 20 Bleu points for nearly all systems when tested on News Commentary instead of the Europarl1 text, which was used for training (Koehn and Monz, 2006). 1See (Koehn, 2005) for details about the Europarl corpus. Subsequently, in 2007 and 2008, the WMT Shared Translation Task organizers provided a limited amount of bilingual News Commentary training data (1-1.3M words) in addition to the large amount of Europarl data (30-32M words), and set up separate evaluations on News Commentary and on Europarl data, thus inviting interest in domain adaptation experiments for the News domain (Callison-Burch et al., 2007; Callison-Burch et al., 2008). This year, the evaluation is on News Commentary only, which makes domain adaptation the central focus of the Shared Translation</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for evaluation of machine translation. In Proceedings of the X MT Summit, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
</authors>
<title>Cognates can improve statistical translation models.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Meeting ofthe North American Association for Computational Linguistics (NAACL’03),</booktitle>
<pages>46--48</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="7268" citStr="Kondrak et al., 2003" startWordPosition="1126" endWordPosition="1129">d up with seven parameters for each entry in the merged phrase table. Merging Two Lexicalized Reordering Tables. When building the two phrase tables, we also built two lexicalized reordering tables (Koehn et al., 2005) for them, Rnews and Reuro, which we merged as follows: We first kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews. This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Mela</context>
<context position="9338" citStr="Kondrak et al., 2003" startWordPosition="1460" endWordPosition="1463">d a greedy approximation to the maximum weighted bipartite matching in that graph (competitive linking) as follows: First, we aligned the most similar pair of unaligned words and we discarded these words from further consideration. Then, we aligned the next most similar pair of unaligned words, and so forth. The process was repeated until there were no words left or the maximal word pair similarity fell below a pre-specified threshold 0 (0 &lt; 0 &lt; 1), which typically left some words unaligned.3 As a result we ended up with a list C of potential cognate pairs. Following (Al-Onaizan et al., 1999; Kondrak et al., 2003; Nakov et al., 2007) we filtered out the duplicates in C, and we added the remaining cognate pairs as additional “sentence” pairs to the bi-text in order to bias the subsequent training of the IBM word alignment models. Improved (De-)tokenization. The default tokenizer does not split on hyphenated compound words like nation-building, well-rehearsed, selfassured, Arab-Israeli, domestically-oriented, etc. While linguistically correct, this can be problematic for machine translation since it can cause data sparsity issues. For example, the system might know how to translate into Spanish both wel</context>
<context position="10754" citStr="Kondrak et al. (2003)" startWordPosition="1694" endWordPosition="1697"> double dashes, as illustrated by the following training sentence: “So the question now is what can China do to freeze--and, if possible, to reverse--North Korea’s nuclear program.” We changed the tokenizer so that it splits on ‘-’ and ‘--’; we altered the detokenizer accordingly. Improved Recaser. The default recaser suggested by the WMT09 organizers was based on a monotone translation model. We trained such a recaser on the Spanish side of the News Commen3For News Commentary, we used 0 = 0.4, which was found by optimizing on the development set; for Europarl, we set 0 = 0.58 as suggested by Kondrak et al. (2003). tary bi-text that translates from lowercase to uppercase Spanish. While being good overall, it had a problem with unknown words, leaving them in lowercase. In a News Commentary text, however, most unknown words are named entities – persons, organization, locations – which are spelled with a capitalized initial in Spanish. Therefore, we used an additional recasing script, which runs over the output of the default recaser and sets the casing of the unknown words to the original casing they had in the English input. It also makes sure all sentences start with a capitalized initial. Rule-based P</context>
</contexts>
<marker>Kondrak, Marcu, Knight, 2003</marker>
<rawString>Grzegorz Kondrak, Daniel Marcu, and Kevin Knight. 2003. Cognates can improve statistical translation models. In Proceedings of the Annual Meeting ofthe North American Association for Computational Linguistics (NAACL’03), pages 46–48, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Multipath translation lexicon induction via bridge languages.</title>
<date>2001</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Association for Computational Linguistics (NAACL’01),</booktitle>
<pages>1--8</pages>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="7549" citStr="Mann and Yarowsky, 2001" startWordPosition="1167" endWordPosition="1170">rst kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews. This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR(s1, s2) = |LCS(s1,s2)| Max(|s1|,|s2|) where LCS(s1, s2) is the longest common subsequence of s1 and s2, and Is is the length of s. Following Nakov e</context>
</contexts>
<marker>Mann, Yarowsky, 2001</marker>
<rawString>Gideon Mann and David Yarowsky. 2001. Multipath translation lexicon induction via bridge languages. In Proceedings of the Annual Meeting of the North American Association for Computational Linguistics (NAACL’01), pages 1–8, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Automatic evaluation and uniform filter cascades for inducing N-best translation lexicons.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>184--198</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="7878" citStr="Melamed (1995)" startWordPosition="1219" endWordPosition="1220">003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR(s1, s2) = |LCS(s1,s2)| Max(|s1|,|s2|) where LCS(s1, s2) is the longest common subsequence of s1 and s2, and Is is the length of s. Following Nakov et al. (2007), we combined the LCSR similarity measure with competitive linking (Melamed, 2000) in order to extract potential cog76 nates from the training bi-text. Competitive linking assumes that, given a source English sentence and its Spanish translation, a source word is either translated with a single target word or is not</context>
</contexts>
<marker>Melamed, 1995</marker>
<rawString>Dan Melamed. 1995. Automatic evaluation and uniform filter cascades for inducing N-best translation lexicons. In Proceedings of the Third Workshop on Very Large Corpora, pages 184–198, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Bitext maps and alignment via pattern recognition.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="7565" citStr="Melamed, 1999" startWordPosition="1171" endWordPosition="1172"> Rnews, then we added those from Reuro which were not present in Rnews. This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR(s1, s2) = |LCS(s1,s2)| Max(|s1|,|s2|) where LCS(s1, s2) is the longest common subsequence of s1 and s2, and Is is the length of s. Following Nakov et al. (2007), we</context>
</contexts>
<marker>Melamed, 1999</marker>
<rawString>Dan Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, 25(1):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="8243" citStr="Melamed, 2000" startWordPosition="1278" endWordPosition="1279">in, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR(s1, s2) = |LCS(s1,s2)| Max(|s1|,|s2|) where LCS(s1, s2) is the longest common subsequence of s1 and s2, and Is is the length of s. Following Nakov et al. (2007), we combined the LCSR similarity measure with competitive linking (Melamed, 2000) in order to extract potential cog76 nates from the training bi-text. Competitive linking assumes that, given a source English sentence and its Spanish translation, a source word is either translated with a single target word or is not translated at all. Given an English-Spanish sentence pair, we calculated LCSR for all cross-lingual word pairs (excluding stopwords and words of length 3 or less), which induced a fully-connected weighted bipartite graph. Then, we performed a greedy approximation to the maximum weighted bipartite matching in that graph (competitive linking) as follows: First, we</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>Dan Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>UCB system description for the WMT</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>212--215</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5306" citStr="Nakov and Hearst (2007)" startWordPosition="799" endWordPosition="802">g part of the development set (dev-test2009a). We used these weights in a beam search decoder (Koehn et al., 2007) to translate the test sentences (the English part of dev-test2009b, tokenized and lowercased). We then recased the output using a monotone model that translates from lowercase to uppercase Spanish, we post-cased it using a simple heuristic, de-tokenized the result, and compared it to the gold standard (the Spanish part of dev-test2009b) using Bleu and NIST. 2.2 Nonstandard Settings The nonstandard features of our system can be summarized as follows: Two Language Models. Following Nakov and Hearst (2007), we used two language models (LM) – an in-domain one (trained on a concatenation of the provided monolingual Spanish News Commentary data and the Spanish side of the training News Commentary bi-text) and an out-ofdomain one (trained on the provided monolingual Spanish Europarl data). For both LMs, we used 5-gram models with Kneser-Ney smoothing. Merging Two Phrase Tables. Following Nakov (2008), we trained and merged two phrasebased SMT systems: a small in-domain one using the News Commentary bi-text, and a large out-ofdomain one using the Europarl bi-text. As a result, we obtained two phrase</context>
</contexts>
<marker>Nakov, Hearst, 2007</marker>
<rawString>Preslav Nakov and Marti Hearst. 2007. UCB system description for the WMT 2007 shared task. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 212–215, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Svetlin Nakov</author>
<author>Elena Paskaleva</author>
</authors>
<title>Improved word alignments using the Web as a corpus.</title>
<date>2007</date>
<booktitle>In Proceedigs of Recent Advances in Natural Language Processing (RANLP’07),</booktitle>
<pages>400--405</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="8161" citStr="Nakov et al. (2007)" startWordPosition="1265" endWordPosition="1268">y, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR(s1, s2) = |LCS(s1,s2)| Max(|s1|,|s2|) where LCS(s1, s2) is the longest common subsequence of s1 and s2, and Is is the length of s. Following Nakov et al. (2007), we combined the LCSR similarity measure with competitive linking (Melamed, 2000) in order to extract potential cog76 nates from the training bi-text. Competitive linking assumes that, given a source English sentence and its Spanish translation, a source word is either translated with a single target word or is not translated at all. Given an English-Spanish sentence pair, we calculated LCSR for all cross-lingual word pairs (excluding stopwords and words of length 3 or less), which induced a fully-connected weighted bipartite graph. Then, we performed a greedy approximation to the maximum wei</context>
</contexts>
<marker>Nakov, Nakov, Paskaleva, 2007</marker>
<rawString>Preslav Nakov, Svetlin Nakov, and Elena Paskaleva. 2007. Improved word alignments using the Web as a corpus. In Proceedigs of Recent Advances in Natural Language Processing (RANLP’07), pages 400– 405, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Improving English-Spanish statistical machine translation: Experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>147--150</pages>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="5704" citStr="Nakov (2008)" startWordPosition="865" endWordPosition="866">standard (the Spanish part of dev-test2009b) using Bleu and NIST. 2.2 Nonstandard Settings The nonstandard features of our system can be summarized as follows: Two Language Models. Following Nakov and Hearst (2007), we used two language models (LM) – an in-domain one (trained on a concatenation of the provided monolingual Spanish News Commentary data and the Spanish side of the training News Commentary bi-text) and an out-ofdomain one (trained on the provided monolingual Spanish Europarl data). For both LMs, we used 5-gram models with Kneser-Ney smoothing. Merging Two Phrase Tables. Following Nakov (2008), we trained and merged two phrasebased SMT systems: a small in-domain one using the News Commentary bi-text, and a large out-ofdomain one using the Europarl bi-text. As a result, we obtained two phrase tables, Tnews and Teuro, and two lexicalized reordering models, Rnews and Reuro. We merged the phrase table as follows. First, we kept all phrase pairs from Tnews. Then we added those phrase pairs from Teuro which were not present in Tnews. For each phrase pair added, we retained its associated features: forward and reverse phrase translation probabilities, forward and reverse lexical translati</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008. Improving English-Spanish statistical machine translation: Experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 147–150, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3923" citStr="Och and Ney, 2003" startWordPosition="584" endWordPosition="587">e only participated in English→Spanish, due to time limitations. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 75–79, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 75 allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from th</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="4047" citStr="Och and Ney, 2004" startWordPosition="604" endWordPosition="607">ranslation , pages 75–79, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 75 allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from the phrase table. We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate trai</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL’03),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4670" citStr="Och, 2003" startWordPosition="699" endWordPosition="700">ained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from the phrase table. We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate training (MERT) (Och, 2003) on the tuning part of the development set (dev-test2009a). We used these weights in a beam search decoder (Koehn et al., 2007) to translate the test sentences (the English part of dev-test2009b, tokenized and lowercased). We then recased the output using a monotone model that translates from lowercase to uppercase Spanish, we post-cased it using a simple heuristic, de-tokenized the result, and compared it to the gold standard (the Spanish part of dev-test2009b) using Bleu and NIST. 2.2 Nonstandard Settings The nonstandard features of our system can be summarized as follows: Two Language Model</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL’03), pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL’02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="4608" citStr="Papineni et al., 2002" startWordPosition="687" endWordPosition="691">h 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from the phrase table. We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate training (MERT) (Och, 2003) on the tuning part of the development set (dev-test2009a). We used these weights in a beam search decoder (Koehn et al., 2007) to translate the test sentences (the English part of dev-test2009b, tokenized and lowercased). We then recased the output using a monotone model that translates from lowercase to uppercase Spanish, we post-cased it using a simple heuristic, de-tokenized the result, and compared it to the gold standard (the Spanish part of dev-test2009b) using Bleu and NIST. 2.2 Nonstandard Settings The nonstandard features </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL’02), pages 311–318, Philadelphia, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>