<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016563">
<title confidence="0.997491">
Nbest Dependency Parsing with linguistically rich models
</title>
<author confidence="0.998672">
Xiaodong Shi
</author>
<affiliation confidence="0.885823333333333">
Institute of Artificial Intelligence
Department of Computer Science
Xiamen University, Xiamen 361005
</affiliation>
<email confidence="0.998627">
mandel@xmu.edu.cn
</email>
<author confidence="0.992783">
Yidong Chen
</author>
<affiliation confidence="0.885632">
Institute of Artificial Intelligence
Department of Computer Science
Xiamen University, Xiamen 361005
</affiliation>
<email confidence="0.99841">
ydchen@xmu.edu.cn
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999688">
We try to improve the classifier-based de-
terministic dependency parsing in two
ways: by introducing a better search
method based on a non-deterministic nbest
algorithm and by devising a series of lin-
guistically richer models. It is experimen-
tally shown on a ConLL 2007 shared task
that this results in a system with higher per-
formance while still keeping it simple
enough for an efficient implementation.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997039">
This work tries to improve the deterministic de-
pendency parsing paradigm introduced in (Coving-
ton 2001, Nivre 2003, Nivre and Hall, 2005) where
parsing is performed incrementally in a strict left-
to-right order and a machine learned classifier is
used to predict deterministically the next parser
action. Although this approach is very simple, it
achieved the state-of-art parsing accuracy. How-
ever, there are still some problems that leave fur-
ther room for improvement:
</bodyText>
<listItem confidence="0.972010307692308">
(1) A greedy algorithm without backtracking
cannot ensure to find the optimal solution. In the
course of left-to-right parsing, when further con-
text is seen, the previous decisions may be wrong
but a deterministic parser cannot correct it. The
usual way of preventing early error “commitment”
is to enable a k-best or beam-search strategy
(Huang and Chiang 2005, Sagae and Lavie 2006).
(2) A classifier based approach (e.g. using SVM
or memory based learning) is usually linguistically
naïve, to make it applicable to multiple languages.
However, a few studies (Collins 1999, Charniak et
al 2003, Galley et al 2006) have shown that lin-
</listItem>
<bodyText confidence="0.978672461538462">
guistically sophisticated models can have a better
accuracy at parsing, language modeling, and ma-
chine translation, among others.
In this paper we explore ways to improve on the
above-mentioned deterministic parsing model to
overcome the two problems. The rest of the paper
is organized as follows. Section 2 argues for a
search strategy better at finding the optimal solu-
tion. In section 3 we built a series of linguistically
richer models and show experimental results dem-
onstrating their practical consequences. Finally we
draw our conclusions and point out areas to be ex-
plored further.
</bodyText>
<sectionHeader confidence="0.977179" genericHeader="method">
2 Dependency Parsing Enhancements
</sectionHeader>
<bodyText confidence="0.995695454545455">
In the classifier-based approach as in Nivre (2003)
a parse tree is produced by a series of actions
similar to a left-to-right shift-reduce parser. The
main source of errors in this method is the
irrevocability of the parsing action and a wrong
decision can therefore lead to further inaccuracies
in later stages. So it cannot usually handle garden-
path sentences. Moreover, each action is usually
predicted using only the local features of the words
in a limited window, although dynamic features of
the local context can be exploited (Carreras 2006).
To remedy this situation, we just add a scoring
function and a priority queue which records nbest
partial parses. The scoring function is defined on
the parsing actions and the features of a partial
parse. It can be decomposed into two subfunctions:
score(a,y)=parsing_cost(a,y) + lm(y)
where a is parsing actions and y is partial parses,
and parsing cost (parsing_cost) is used to imple-
ment certain parsing preferences while the lingus-
tic model score (lm) is usually modeled in the lin-
guistic (in our case, dependency model) framework.
</bodyText>
<page confidence="0.888498">
80
</page>
<bodyText confidence="0.973619111111111">
Proceedings of the 10th Conference on Parsing Technologies, pages 80–82,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
In the usual nbest or beam-search implementation
(e.g. Huang and Chiang 2005, Sagae and Lavie
2006), only lm is present.
We give justification of the first term as follows:
Many probability functions need to know the de-
pendency label and relative distance between the
dependent and the head. However, during parsing
sometimes this head-binding can be very late. This
means a right-headed word may need to wait very
long for its right head, and so a big partial-parse
queue is needed, while psychological evidence
suggests that there is some overhead involved in
processing every word and a word tends to attach
locally. By modeling parsing cost we can first use
a coarse probability model to guide the nbest par-
tial results in order not to defer the probability cal-
culation. As parsing progresses, more information
becomes available; we can have a better estimation
of our linguistic probability model to rectify the
inaccuracy.
This use of a coarse scoring mechanism to guide
the early parsing for possible later rectification of
the decision is a novel feature of our parsing
framework and enables better searching of the so-
lution space. To implement it, we just remember
the exact score of the every major decision (wait,
add a dependent or attach a head) in parsing, and
re-score when more context is available. Compared
with (Charniak 2005), our parsing process requires
only one pass.
Thus, we can strike a balance between accuracy,
memory and speed. With a moderately-sized n
(best partial results), we can reduce memory use
and get higher speed to get a same accuracy. An
added advantage is that this idea is also useful in
other bottom-up parsing paradigms (not only in a
dependency framework).
In a word, our main innovation is the use of a
parsing cost to influence the search paths, and the
use of an evolving lm function to enable progres-
sively better modeling. The nbest framework is
general enough to make this a very simple modifi-
cation to the basic algorithm of Nivre (2003).
</bodyText>
<sectionHeader confidence="0.994593" genericHeader="method">
3 Better Linguistic Modeling
</sectionHeader>
<bodyText confidence="0.996071333333333">
In our modeling we combine different linguistic
models by using many probability functions:
lm(y)=ΣlogP(wi,wj,x,y) =ΣW*log P
where w are the trained weight vector and P is a
vector of probability functions. In our system we
considered the following functions:
</bodyText>
<listItem confidence="0.978201545454546">
P1: function measuring the probability of a head
and a dependent. This is the base function in most
dependency parsing framework.
P2: function calculating the subcategorization
frame probability;
P3: function calculating the semantic frame us-
ing a Chinese FrameNet (Liu 2006).
P4: function measuring the semantic affinity be-
tween a head and a dependent using resources such
as Hownet (Dong 2003).
P5: Other Chinese specific probability functions
</listItem>
<bodyText confidence="0.960376952380952">
defined on the features of the head, the dependents,
the partial parse and the input.
Model P2 is a probability function on pseudo
subcategorization frames (as a concatenation of all
the dependents’ labels) as we don’t know the dis-
tinction of arguments and adjuncts in the depend-
ency Treebank. We used a Markovian subcategori-
zation scheme with left and right STOP delimiters
to ease the data sparseness. And as a first approxi-
mation, we also experimented with a model where
each label can only be used a certain times in a
direction. This model is called P2’ in Table 4.
Other functions (P3-P5) are also very useful
with its different linguistic content. Model P5 actu-
ally contains a lot of Chinese-specific functions,
e.g. between a sentence-final particle and a verb.
We designed a series of experiments to show to
effectiveness of each model. We use the Chinese
training data of the ConLL 2007 shared task. We
divided the training data by a 9:1 split. Table 1
shows the statistics.
</bodyText>
<table confidence="0.860757333333333">
Training testing
sentences 51777 5180
Words 302943 34232
</table>
<tableCaption confidence="0.998215">
Table 1. Experimental data
</tableCaption>
<bodyText confidence="0.9854485">
In the baseline model, we train a simple probability
function between a head and a dependent using
deleted interpolation. For nbest=1, we have a
deterministic model.
</bodyText>
<table confidence="0.97283025">
LAS UAS time
Deterministic 41.64 % 44.11 % 8s
nbest = 50 71.30 % 76.34 % 72s
nbest = 500 71.90 % 76.99 % 827s
</table>
<tableCaption confidence="0.704213333333333">
Table 2. baseline systems
It can be seen (Table 3) that combing different
linguistic information can lead to significant in-
</tableCaption>
<page confidence="0.998257">
81
</page>
<bodyText confidence="0.9921765">
crease of the accuracy. However, different models
have different contributions. Our experiments con-
firm with Collins’s result in that subcategorization
carries very important linguistic content.
</bodyText>
<table confidence="0.999529555555556">
LAS UAS time
P1 71.90 % 76.99 % 827s
P1 + P2’ 73.45 % 78.44 % 832s
P1 + P2’ + P2 77.92 % 82.42 % 855s
P1 + P2 + P3 79.13% 83.57% 1003s
P1-4 81.21% 85.78% 1597s
P1-5 83.12% 87.03% 2100s
Verb valency 85.32 % 89.12 % -
DE refinement 85.98% 90.20% -
</table>
<tableCaption confidence="0.99989">
Table 3. systems with different linguistic models
</tableCaption>
<subsectionHeader confidence="0.998474">
3.1 Relabeling of the parse treebank
</subsectionHeader>
<bodyText confidence="0.999709">
Sometimes the information needed in the modeling
is not in the data explicitly. Implicit information
can be made explicit and accessible to the parser.
In the Chinese Treebank the relation label is
often determined by the head word’s semantic type.
We tried the relabeling of coarse POS info of the
verb in a effort to detect its valency; and refine-
ment of the auxiliary word 的 DE (as error analy-
sis shows it is the where the most errors occur).
Results are in Table 3.
We also tried refinement of the relation label by
using the two connected words. However, this does
not improve the result. Automatic linguistic model-
ing using latent label (Matsuzaki 2005) can also be
attempted but is not yet done.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999179117647059">
In this paper we showed that simple classifier-
based deterministic dependency parsing can be
improved using a more flexible search strategy
over an nbest parsing framework and a variety of
linguistically richer models. By incorporating dif-
ferent linguistic knowledge, the parsing model can
be made more accurate and thus achieves better
results.
Further work to be done includes ways to com-
bine machine learning based on the automatic fea-
ture selection with manual linguistic modeling: an
interactive approach for better synergistic model-
ing (where the machine proposes and the human
guides). Various a priori models can be tried by the
machine and patterns inherent in the data can be
revealed to the human who can then explore more
complex models.
</bodyText>
<sectionHeader confidence="0.98964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999718065217392">
Xavier Carreras, Mihai Surdeanu, and Lluís Màrquez.
2006. Projective Dependency Parsing with Percep-
tron. In Proceedings of CoNLL-X. 181-185.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Eugene Charniak; K. Knight, and K.Yamada. 2003.
Syntax-based language models for statistical ma-
chine translation. In MT Summit IX. Intl. Assoc. for
Machine Translation.
Eugene Charniak and Mark Johnson. 2005. Coarse-
tofine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL.
Michael Collins. 1999. Head-Driven Statistical Models-
for Natural Language Parsing. PhD Dissertation,
University of Pennsylvania.
Michael Collins. 2004. Parameter Estimation for Statis-
tical Parsing Models: Theory and Practice of Distri-
bution-Free Methods. In Harry Bunt el al, New De-
velopments in Parsing Technology, Kluwer.
Michael A. Covington. 2001. A fundamental algorithm
for dependency parsing. Proceedings of the 39th An-
nual ACM Southeast Conference, pp. 95-102.
Zhendong Dong and Qiang Dong. 2003. HowNet - a
hybrid language and knowledge resource. In Pro-
ceeding of Natural Language Processing and Knowl-
edge Engineering.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable Inference
and Training of Context-Rich Syntactic Models. In
Proc. ACL-COLING.
Kaiying Liu. 2006. Building a Chinese FrameNet. In
Proceeding of 25th anniversary of Chinese Informa-
tion Processing Society of China.
Takuya Matsuzaki, Yusuke Miyao, Jun&apos;ichi Tsujii. 2005.
Probabilistic CFG with latent annotations. In Pro-
ceedings of ACL-2005.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of IWPT.
149-160.
Joakim Nivre and Johan Hall. 2005. MaltParser: A Lan-
guage-Independent System for Data-Driven Depend-
ency Parsing. In Proceedings of the Fourth Work-
shop on Treebanks and Linguistic. Theories, Barce-
lona, 9-10 December 2005. 137-148.
Sagae, K. and Lavie, A. 2006 A best-first probabilistic
shift-reduce parser. In Proceedings of ACL.
</reference>
<page confidence="0.99913">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.090951">
<title confidence="0.958512">Nbest Dependency Parsing with linguistically rich models</title>
<author confidence="0.753036">Xiaodong</author>
<affiliation confidence="0.841806666666667">Institute of Artificial Department of Computer Xiamen University, Xiamen 361005</affiliation>
<email confidence="0.869026">mandel@xmu.edu.cn</email>
<author confidence="0.409195">Yidong</author>
<affiliation confidence="0.9892575">Institute of Artificial Department of Computer</affiliation>
<address confidence="0.692516">Xiamen University, Xiamen 361005</address>
<email confidence="0.955347">ydchen@xmu.edu.cn</email>
<abstract confidence="0.998333454545454">We try to improve the classifier-based deterministic dependency parsing in two ways: by introducing a better search method based on a non-deterministic nbest algorithm and by devising a series of linguistically richer models. It is experimentally shown on a ConLL 2007 shared task that this results in a system with higher performance while still keeping it simple enough for an efficient implementation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Mihai Surdeanu</author>
<author>Lluís Màrquez</author>
</authors>
<title>Projective Dependency Parsing with Perceptron.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<pages>181--185</pages>
<marker>Carreras, Surdeanu, Màrquez, 2006</marker>
<rawString>Xavier Carreras, Mihai Surdeanu, and Lluís Màrquez. 2006. Projective Dependency Parsing with Perceptron. In Proceedings of CoNLL-X. 181-185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="1580" citStr="Huang and Chiang 2005" startWordPosition="233" endWordPosition="236">and a machine learned classifier is used to predict deterministically the next parser action. Although this approach is very simple, it achieved the state-of-art parsing accuracy. However, there are still some problems that leave further room for improvement: (1) A greedy algorithm without backtracking cannot ensure to find the optimal solution. In the course of left-to-right parsing, when further context is seen, the previous decisions may be wrong but a deterministic parser cannot correct it. The usual way of preventing early error “commitment” is to enable a k-best or beam-search strategy (Huang and Chiang 2005, Sagae and Lavie 2006). (2) A classifier based approach (e.g. using SVM or memory based learning) is usually linguistically naïve, to make it applicable to multiple languages. However, a few studies (Collins 1999, Charniak et al 2003, Galley et al 2006) have shown that linguistically sophisticated models can have a better accuracy at parsing, language modeling, and machine translation, among others. In this paper we explore ways to improve on the above-mentioned deterministic parsing model to overcome the two problems. The rest of the paper is organized as follows. Section 2 argues for a sear</context>
<context position="3804" citStr="Huang and Chiang 2005" startWordPosition="586" endWordPosition="589"> parsing actions and the features of a partial parse. It can be decomposed into two subfunctions: score(a,y)=parsing_cost(a,y) + lm(y) where a is parsing actions and y is partial parses, and parsing cost (parsing_cost) is used to implement certain parsing preferences while the lingustic model score (lm) is usually modeled in the linguistic (in our case, dependency model) framework. 80 Proceedings of the 10th Conference on Parsing Technologies, pages 80–82, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics In the usual nbest or beam-search implementation (e.g. Huang and Chiang 2005, Sagae and Lavie 2006), only lm is present. We give justification of the first term as follows: Many probability functions need to know the dependency label and relative distance between the dependent and the head. However, during parsing sometimes this head-binding can be very late. This means a right-headed word may need to wait very long for its right head, and so a big partial-parse queue is needed, while psychological evidence suggests that there is some overhead involved in processing every word and a word tends to attach locally. By modeling parsing cost we can first use a coarse proba</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>K Knight</author>
<author>K Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>In MT Summit IX. Intl. Assoc. for Machine Translation.</booktitle>
<contexts>
<context position="1814" citStr="Charniak et al 2003" startWordPosition="270" endWordPosition="273">r room for improvement: (1) A greedy algorithm without backtracking cannot ensure to find the optimal solution. In the course of left-to-right parsing, when further context is seen, the previous decisions may be wrong but a deterministic parser cannot correct it. The usual way of preventing early error “commitment” is to enable a k-best or beam-search strategy (Huang and Chiang 2005, Sagae and Lavie 2006). (2) A classifier based approach (e.g. using SVM or memory based learning) is usually linguistically naïve, to make it applicable to multiple languages. However, a few studies (Collins 1999, Charniak et al 2003, Galley et al 2006) have shown that linguistically sophisticated models can have a better accuracy at parsing, language modeling, and machine translation, among others. In this paper we explore ways to improve on the above-mentioned deterministic parsing model to overcome the two problems. The rest of the paper is organized as follows. Section 2 argues for a search strategy better at finding the optimal solution. In section 3 we built a series of linguistically richer models and show experimental results demonstrating their practical consequences. Finally we draw our conclusions and point out</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Eugene Charniak; K. Knight, and K.Yamada. 2003. Syntax-based language models for statistical machine translation. In MT Summit IX. Intl. Assoc. for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarsetofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarsetofine n-best parsing and maxent discriminative reranking. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Modelsfor Natural Language Parsing.</title>
<date>1999</date>
<tech>PhD</tech>
<institution>Dissertation, University of Pennsylvania.</institution>
<contexts>
<context position="1793" citStr="Collins 1999" startWordPosition="268" endWordPosition="269">t leave further room for improvement: (1) A greedy algorithm without backtracking cannot ensure to find the optimal solution. In the course of left-to-right parsing, when further context is seen, the previous decisions may be wrong but a deterministic parser cannot correct it. The usual way of preventing early error “commitment” is to enable a k-best or beam-search strategy (Huang and Chiang 2005, Sagae and Lavie 2006). (2) A classifier based approach (e.g. using SVM or memory based learning) is usually linguistically naïve, to make it applicable to multiple languages. However, a few studies (Collins 1999, Charniak et al 2003, Galley et al 2006) have shown that linguistically sophisticated models can have a better accuracy at parsing, language modeling, and machine translation, among others. In this paper we explore ways to improve on the above-mentioned deterministic parsing model to overcome the two problems. The rest of the paper is organized as follows. Section 2 argues for a search strategy better at finding the optimal solution. In section 3 we built a series of linguistically richer models and show experimental results demonstrating their practical consequences. Finally we draw our conc</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Modelsfor Natural Language Parsing. PhD Dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Parameter Estimation for Statistical Parsing Models: Theory and Practice of Distribution-Free Methods. In Harry Bunt el al, New Developments in Parsing</title>
<date>2004</date>
<publisher>Technology, Kluwer.</publisher>
<marker>Collins, 2004</marker>
<rawString>Michael Collins. 2004. Parameter Estimation for Statistical Parsing Models: Theory and Practice of Distribution-Free Methods. In Harry Bunt el al, New Developments in Parsing Technology, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A fundamental algorithm for dependency parsing.</title>
<date>2001</date>
<booktitle>Proceedings of the 39th Annual ACM Southeast Conference,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="851" citStr="Covington 2001" startWordPosition="119" endWordPosition="121"> Department of Computer Science Xiamen University, Xiamen 361005 ydchen@xmu.edu.cn Abstract We try to improve the classifier-based deterministic dependency parsing in two ways: by introducing a better search method based on a non-deterministic nbest algorithm and by devising a series of linguistically richer models. It is experimentally shown on a ConLL 2007 shared task that this results in a system with higher performance while still keeping it simple enough for an efficient implementation. 1 Introduction This work tries to improve the deterministic dependency parsing paradigm introduced in (Covington 2001, Nivre 2003, Nivre and Hall, 2005) where parsing is performed incrementally in a strict leftto-right order and a machine learned classifier is used to predict deterministically the next parser action. Although this approach is very simple, it achieved the state-of-art parsing accuracy. However, there are still some problems that leave further room for improvement: (1) A greedy algorithm without backtracking cannot ensure to find the optimal solution. In the course of left-to-right parsing, when further context is seen, the previous decisions may be wrong but a deterministic parser cannot corr</context>
</contexts>
<marker>Covington, 2001</marker>
<rawString>Michael A. Covington. 2001. A fundamental algorithm for dependency parsing. Proceedings of the 39th Annual ACM Southeast Conference, pp. 95-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
<author>Qiang Dong</author>
</authors>
<title>HowNet - a hybrid language and knowledge resource.</title>
<date>2003</date>
<booktitle>In Proceeding of Natural Language Processing and Knowledge Engineering.</booktitle>
<marker>Dong, Dong, 2003</marker>
<rawString>Zhendong Dong and Qiang Dong. 2003. HowNet - a hybrid language and knowledge resource. In Proceeding of Natural Language Processing and Knowledge Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Models. In</title>
<date>2006</date>
<booktitle>Proc. ACL-COLING.</booktitle>
<contexts>
<context position="1834" citStr="Galley et al 2006" startWordPosition="274" endWordPosition="277">t: (1) A greedy algorithm without backtracking cannot ensure to find the optimal solution. In the course of left-to-right parsing, when further context is seen, the previous decisions may be wrong but a deterministic parser cannot correct it. The usual way of preventing early error “commitment” is to enable a k-best or beam-search strategy (Huang and Chiang 2005, Sagae and Lavie 2006). (2) A classifier based approach (e.g. using SVM or memory based learning) is usually linguistically naïve, to make it applicable to multiple languages. However, a few studies (Collins 1999, Charniak et al 2003, Galley et al 2006) have shown that linguistically sophisticated models can have a better accuracy at parsing, language modeling, and machine translation, among others. In this paper we explore ways to improve on the above-mentioned deterministic parsing model to overcome the two problems. The rest of the paper is organized as follows. Section 2 argues for a search strategy better at finding the optimal solution. In section 3 we built a series of linguistically richer models and show experimental results demonstrating their practical consequences. Finally we draw our conclusions and point out areas to be explore</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang, and I. Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Models. In Proc. ACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaiying Liu</author>
</authors>
<title>Building a Chinese FrameNet.</title>
<date>2006</date>
<booktitle>In Proceeding of 25th anniversary of Chinese Information Processing Society of</booktitle>
<contexts>
<context position="6285" citStr="Liu 2006" startWordPosition="996" endWordPosition="997">tion to the basic algorithm of Nivre (2003). 3 Better Linguistic Modeling In our modeling we combine different linguistic models by using many probability functions: lm(y)=ΣlogP(wi,wj,x,y) =ΣW*log P where w are the trained weight vector and P is a vector of probability functions. In our system we considered the following functions: P1: function measuring the probability of a head and a dependent. This is the base function in most dependency parsing framework. P2: function calculating the subcategorization frame probability; P3: function calculating the semantic frame using a Chinese FrameNet (Liu 2006). P4: function measuring the semantic affinity between a head and a dependent using resources such as Hownet (Dong 2003). P5: Other Chinese specific probability functions defined on the features of the head, the dependents, the partial parse and the input. Model P2 is a probability function on pseudo subcategorization frames (as a concatenation of all the dependents’ labels) as we don’t know the distinction of arguments and adjuncts in the dependency Treebank. We used a Markovian subcategorization scheme with left and right STOP delimiters to ease the data sparseness. And as a first approximat</context>
</contexts>
<marker>Liu, 2006</marker>
<rawString>Kaiying Liu. 2006. Building a Chinese FrameNet. In Proceeding of 25th anniversary of Chinese Information Processing Society of China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005.</booktitle>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, Jun&apos;ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of ACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<pages>149--160</pages>
<contexts>
<context position="863" citStr="Nivre 2003" startWordPosition="122" endWordPosition="123">omputer Science Xiamen University, Xiamen 361005 ydchen@xmu.edu.cn Abstract We try to improve the classifier-based deterministic dependency parsing in two ways: by introducing a better search method based on a non-deterministic nbest algorithm and by devising a series of linguistically richer models. It is experimentally shown on a ConLL 2007 shared task that this results in a system with higher performance while still keeping it simple enough for an efficient implementation. 1 Introduction This work tries to improve the deterministic dependency parsing paradigm introduced in (Covington 2001, Nivre 2003, Nivre and Hall, 2005) where parsing is performed incrementally in a strict leftto-right order and a machine learned classifier is used to predict deterministically the next parser action. Although this approach is very simple, it achieved the state-of-art parsing accuracy. However, there are still some problems that leave further room for improvement: (1) A greedy algorithm without backtracking cannot ensure to find the optimal solution. In the course of left-to-right parsing, when further context is seen, the previous decisions may be wrong but a deterministic parser cannot correct it. The </context>
<context position="2530" citStr="Nivre (2003)" startWordPosition="386" endWordPosition="387">arsing, language modeling, and machine translation, among others. In this paper we explore ways to improve on the above-mentioned deterministic parsing model to overcome the two problems. The rest of the paper is organized as follows. Section 2 argues for a search strategy better at finding the optimal solution. In section 3 we built a series of linguistically richer models and show experimental results demonstrating their practical consequences. Finally we draw our conclusions and point out areas to be explored further. 2 Dependency Parsing Enhancements In the classifier-based approach as in Nivre (2003) a parse tree is produced by a series of actions similar to a left-to-right shift-reduce parser. The main source of errors in this method is the irrevocability of the parsing action and a wrong decision can therefore lead to further inaccuracies in later stages. So it cannot usually handle gardenpath sentences. Moreover, each action is usually predicted using only the local features of the words in a limited window, although dynamic features of the local context can be exploited (Carreras 2006). To remedy this situation, we just add a scoring function and a priority queue which records nbest p</context>
<context position="5719" citStr="Nivre (2003)" startWordPosition="912" endWordPosition="913">y one pass. Thus, we can strike a balance between accuracy, memory and speed. With a moderately-sized n (best partial results), we can reduce memory use and get higher speed to get a same accuracy. An added advantage is that this idea is also useful in other bottom-up parsing paradigms (not only in a dependency framework). In a word, our main innovation is the use of a parsing cost to influence the search paths, and the use of an evolving lm function to enable progressively better modeling. The nbest framework is general enough to make this a very simple modification to the basic algorithm of Nivre (2003). 3 Better Linguistic Modeling In our modeling we combine different linguistic models by using many probability functions: lm(y)=ΣlogP(wi,wj,x,y) =ΣW*log P where w are the trained weight vector and P is a vector of probability functions. In our system we considered the following functions: P1: function measuring the probability of a head and a dependent. This is the base function in most dependency parsing framework. P2: function calculating the subcategorization frame probability; P3: function calculating the semantic frame using a Chinese FrameNet (Liu 2006). P4: function measuring the seman</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of IWPT. 149-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
</authors>
<title>MaltParser: A Language-Independent System for Data-Driven Dependency Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth Workshop on Treebanks and Linguistic. Theories,</booktitle>
<pages>137--148</pages>
<location>Barcelona,</location>
<contexts>
<context position="886" citStr="Nivre and Hall, 2005" startWordPosition="124" endWordPosition="127">nce Xiamen University, Xiamen 361005 ydchen@xmu.edu.cn Abstract We try to improve the classifier-based deterministic dependency parsing in two ways: by introducing a better search method based on a non-deterministic nbest algorithm and by devising a series of linguistically richer models. It is experimentally shown on a ConLL 2007 shared task that this results in a system with higher performance while still keeping it simple enough for an efficient implementation. 1 Introduction This work tries to improve the deterministic dependency parsing paradigm introduced in (Covington 2001, Nivre 2003, Nivre and Hall, 2005) where parsing is performed incrementally in a strict leftto-right order and a machine learned classifier is used to predict deterministically the next parser action. Although this approach is very simple, it achieved the state-of-art parsing accuracy. However, there are still some problems that leave further room for improvement: (1) A greedy algorithm without backtracking cannot ensure to find the optimal solution. In the course of left-to-right parsing, when further context is seen, the previous decisions may be wrong but a deterministic parser cannot correct it. The usual way of preventing</context>
</contexts>
<marker>Nivre, Hall, 2005</marker>
<rawString>Joakim Nivre and Johan Hall. 2005. MaltParser: A Language-Independent System for Data-Driven Dependency Parsing. In Proceedings of the Fourth Workshop on Treebanks and Linguistic. Theories, Barcelona, 9-10 December 2005. 137-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>A best-first probabilistic shift-reduce parser.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1603" citStr="Sagae and Lavie 2006" startWordPosition="237" endWordPosition="240">lassifier is used to predict deterministically the next parser action. Although this approach is very simple, it achieved the state-of-art parsing accuracy. However, there are still some problems that leave further room for improvement: (1) A greedy algorithm without backtracking cannot ensure to find the optimal solution. In the course of left-to-right parsing, when further context is seen, the previous decisions may be wrong but a deterministic parser cannot correct it. The usual way of preventing early error “commitment” is to enable a k-best or beam-search strategy (Huang and Chiang 2005, Sagae and Lavie 2006). (2) A classifier based approach (e.g. using SVM or memory based learning) is usually linguistically naïve, to make it applicable to multiple languages. However, a few studies (Collins 1999, Charniak et al 2003, Galley et al 2006) have shown that linguistically sophisticated models can have a better accuracy at parsing, language modeling, and machine translation, among others. In this paper we explore ways to improve on the above-mentioned deterministic parsing model to overcome the two problems. The rest of the paper is organized as follows. Section 2 argues for a search strategy better at f</context>
<context position="3827" citStr="Sagae and Lavie 2006" startWordPosition="590" endWordPosition="593">e features of a partial parse. It can be decomposed into two subfunctions: score(a,y)=parsing_cost(a,y) + lm(y) where a is parsing actions and y is partial parses, and parsing cost (parsing_cost) is used to implement certain parsing preferences while the lingustic model score (lm) is usually modeled in the linguistic (in our case, dependency model) framework. 80 Proceedings of the 10th Conference on Parsing Technologies, pages 80–82, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics In the usual nbest or beam-search implementation (e.g. Huang and Chiang 2005, Sagae and Lavie 2006), only lm is present. We give justification of the first term as follows: Many probability functions need to know the dependency label and relative distance between the dependent and the head. However, during parsing sometimes this head-binding can be very late. This means a right-headed word may need to wait very long for its right head, and so a big partial-parse queue is needed, while psychological evidence suggests that there is some overhead involved in processing every word and a word tends to attach locally. By modeling parsing cost we can first use a coarse probability model to guide t</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Sagae, K. and Lavie, A. 2006 A best-first probabilistic shift-reduce parser. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>