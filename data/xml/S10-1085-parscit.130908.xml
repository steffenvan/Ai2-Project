<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035788">
<title confidence="0.994165">
JAIST: Clustering and ClassiÞcation based Approaches
for Japanese WSD
</title>
<author confidence="0.995816">
Kiyoaki Shirai Makoto Nakamura
</author>
<affiliation confidence="0.997323">
Japan Advanced Institute of Science and Technology
</affiliation>
<email confidence="0.996472">
{kshirai,mnakamur}@jaist.ac.jp
</email>
<sectionHeader confidence="0.994739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992075">
This paper reports about our three par-
ticipating systems in SemEval-2 Japanese
WSD task. The first one is a clustering
based method, which chooses a sense for,
not individual instances, but automatically
constructed clusters of instances. The sec-
ond one is a classification method, which
is an ordinary SVM classifier with simple
domain adaptation techniques. The last is
an ensemble of these two systems. Results
of the formal run shows the second system
is the best. Its precision is 0.7476.
</bodyText>
<sectionHeader confidence="0.998414" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996922472222222">
This paper reports about our systems in SemEval-
2 Japanese Word Sense Disambiguation (WSD)
task (Okumura et al., 2010). This task is a lexi-
cal sample task for Japanese WSD and has the fol-
lowing two characteristics. First, a balanced word-
sense tagged corpus is used for the task. Since it
consists of sub-corpora of several domains or gen-
res, domain adaptation might be required. Second,
the task takes into account not only the instances
having a sense in the given set but also the in-
stances having a sense not found in the set (called
‘new sense’). Participants are required to identify
new senses of words in this task.
The second characteristics of the task is mainly
considered in our system. A clustering based
approach is investigated to identify new senses.
Our system first constructs a set of clusters of
given word instances using unsupervised cluster-
ing techniques. This is motivated by the fact that
the new sense is not defined in the dictionary, and
sense induction without referring to the dictionary
would be required. Clusters obtained would be
sets of instances having the same sense, and some
of them would be new sense instances. Then each
cluster is judged whether instances in it have a new
sense or not. An ordinary classification-based ap-
proach is also considered. That is, WSD classifiers
are trained by a supervised learning algorithm.
Furthermore, simple techniques considering gen-
res of sub-corpora are incorporated into both our
clustering and classification based systems.
The paper continues as follows, Section 2 de-
scribes our three participating systems, JAIST-1,
JAIST-2 and JAIST-3. The results of these systems
are reported and discussed in Section 3. Finally we
conclude the paper in Section 4.
</bodyText>
<sectionHeader confidence="0.998259" genericHeader="introduction">
2 Systems
</sectionHeader>
<subsectionHeader confidence="0.993989">
2.1 JAIST-1: Clustering based WSD System
</subsectionHeader>
<bodyText confidence="0.99536625">
JAIST-1 was developed by a clustering based
method. The overview of the system is shown in
Figure 1. It consists of two procedures: (A) clus-
ters of word instances are constructed so that the
instances of the same sense are merged, (B) then
similarity between a cluster and a sense in a dic-
tionary is measured in order to determine senses
of instances in each cluster.
</bodyText>
<figure confidence="0.96631325">
Dictionary
(A) (B)
instance
(sentence)
</figure>
<figureCaption confidence="0.999976">
Figure 1: Overview of JAIST-1
</figureCaption>
<subsectionHeader confidence="0.949988">
2.1.1 Clustering of Word Instances
</subsectionHeader>
<bodyText confidence="0.99985975">
As previous work applying clustering techniques
for sense induction (Sch¨utze, 1998; Agirre and
Soroa, 2007), each instance is represented by a
feature vector. In JAIST-1, the following 4 vectors
are used for clustering.
Collocation Vector This vector reflects colloca-
tion including the target instance. Words or POSs
appearing just before and after the target instance
are used as features, i.e. they correspond to one di-
mension in the vector. The weight of each feature
is 1 if the feature exists for the instance, or 0 if not.
Context Vector The vector reflects words in the
context of the target instance. All content words
appearing in the context are used as features. The
window size of the context is set to 50. Further-
more, related words are also used as features to en-
</bodyText>
<figure confidence="0.979270923076923">
Corpus
【サービAl (service)
S 客に対するもてなし.M待.
1
help that people who work in a
shop give you
S 値引きなど、商売上の便宜.
2
help that is provided by a
business to customers
S 奉fr.
3
volunteer work
</figure>
<page confidence="0.986319">
379
</page>
<bodyText confidence="0.982885196078432">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 379–382,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
rich the information in the vector. Related words
are defined as follows: first topics of texts are au-
tomatically derived by Latent Dirichlet Allocation
(LDA) (Blei et al., 2003), then words which are the
most closely associated with each topic are formed
into a ‘related word set’. If one word in a related
word set appears in the context, other words in
that set also have a positive weight in the vector.
More concretely, the weight of each feature is de-
termined to be 1 if the word appears in the context
or 0.5 if the word does not appear but is in the re-
lated word set.
Association Vector Similarly to context vector,
this reflects words in the context of the target in-
stance, but data sparseness is alleviated in a differ-
ent manner. In advance, the co-occurrence matrix
A is constructed from a corpus. Each row and col-
umn in A corresponds to one of the most frequent
10,000 content words. Each element ai,j in the
matrix is P(wi|wj), conditional probability repre-
senting how likely it is that two words wi and wj
will occur in the same document. Now j-th col-
umn in A can be regarded as the co-occurrence
vector of wj, o(wj). Association vector is a nor-
malized vector of sum of o(wj) for all words in
the context.
Topic Vector Unlike other vectors, this vector re-
flects topics of texts. The topics zj automatically
derived by PLSI (Probabilistic Latent Semantic In-
dexing) are used as features. The weight for zj in
the vector is P(zj|di) estimated by Folding-in al-
gorithm (Hofmann, 1999), where di is the docu-
ment containing the instance. Topic vector is mo-
tivated by the well-known fact that word senses are
highly associated with the topics of documents.
Target instances are clustered by the agglomera-
tive clustering algorithm. Similarities between in-
stances are calculated by cosine measure of vec-
tors. Furthermore, pairs of instances in different
genre sub-corpora are treated as ‘cannot-link’, so
that they will not be merged into the same cluster.
Clustering procedure is stopped when the num-
ber of instances in a cluster become more than a
threshold Nc. Nc is set to 5 in the participating
system.
The clustering is performed 4 times using 4 dif-
ferent feature vectors. Then the best one is chosen
from the 4 sets of clusters obtained. A set of clus-
ter C (={Ci}) is evaluated by E(C)
</bodyText>
<equation confidence="0.996853">
E(C) = &amp; coh(Ci) (1)
</equation>
<bodyText confidence="0.999936">
where ‘cohesiveness’ coh(Ci) for each cluster Ci
is defined by (2).
</bodyText>
<equation confidence="0.990746833333333">
� |Ci|
1
coh(Ci) = rel-sim(�vij,�gi)
|Ci |j=1
sim(Vij,9i)
maxj sim(vij,gi) (2)
</equation>
<bodyText confidence="0.999513428571429">
vij is an instance vector in the cluster Ci, while gi
is an average vector of Ci. rel-sim(vij,gi) means
the relative similarity between the instance vector
and average vector. Intuitively, coh(Ci) evaluates
how likely instances in the cluster are similar each
other. C such that E(C) is maximum is chosen as
the final set of clusters.
</bodyText>
<subsubsectionHeader confidence="0.634171">
2.1.2 Similarity between Clusters and Senses
</subsubsectionHeader>
<bodyText confidence="0.999052714285714">
After clustering, similarity between a cluster Ci
and a sense Sj in the dictionary, sim(Ci, Sj), is
calculated for WSD. Ci and Sj are represented by
cluster vector ci and sense vector 9i respectively.
Then cosine measure between these two vectors is
calculated as sim(Ci, Sj).
The cluster vector ci is defined as (3):
</bodyText>
<equation confidence="0.9882895">
1 �
N eik∈Ci
</equation>
<bodyText confidence="0.99935475">
In (3), eik stands for an instance in the cluster Ci,
tl words appearing in the context of eik, o(tl) co-
occurrence vector of tl (similar one used in asso-
ciation vector), and N the constant for normaliza-
tion. So ci is similar to association vector, but the
co-occurrence vectors of words in the contexts of
all instances in the cluster are summed.
The sense vector sj is defined as in (4).
</bodyText>
<equation confidence="0.991654666666667">
�sj =
) (4)
l
</equation>
<bodyText confidence="0.999935416666667">
Dj stands for definition sentences of the sense Sj
in the Japanese dictionary Iwanami Kokugo Jiten
(the sense inventory in this task), while Ej a set of
example sentences of Sj. Here Ej includes both
example sentences from the dictionary and ones
excerpted from a sense-tagged corpus, the train-
ing data of this task. we is the parameter putting
more weight on words in example sentences than
in definition sentences. We set we = 2.0 through
the preliminary investigation.
Based on sim(Ci, Sj), the system judges
whether the cluster is a collection of new
</bodyText>
<equation confidence="0.9962985">
1
|Ci|
E
j=1
|Ci|
ci =
� o(tl) (3)
tl∈eik
N
1
⎛
⎝ 1] �
�o(tk) + we - o(t
k∈Dj tl∈Ej
</equation>
<page confidence="0.980121">
380
</page>
<bodyText confidence="0.999555380952381">
sense instances. Suppose that MaxSimi is
maxj sim(Ci, Sj), the maximum similarity be-
tween the cluster and the sense. If MaxSimi is
small, the cluster Ci is not similar to any defined
senses, so instances in Ci could have a new sense.
The system regards that the sense of instances in
Ci is new when MaxSimi is less than a thresh-
old Tns. Otherwise, it regards the sense of in-
stances in Ci as the most similar sense, Sj such
that j = arg maxj sim(Ci, Sj).
The threshold Tns for each target word is deter-
mined as follows. First the training data is equally
subdivided into two halves, the development data
Ddev and the training data Dtr. Next, JAIST-1 is
run for instances in Ddev, while example sentences
in Dtr are used as Ej in (4) when sense vectors are
constructed. For words where new sense instances
exist in Ddev, Tns is optimized for the accuracy
of new sense detection. For words where no new
sense instances are found in Ddev, Tns is deter-
mined by the minimum of MaxSimi as follows:
</bodyText>
<equation confidence="0.994286">
MaxSimi) x -y (5)
</equation>
<bodyText confidence="0.9769785">
Since even the cluster of which MaxSimi is min-
imum represents not a new but a defined sense, the
minimum of MaxSimi is decreased by -y. To de-
termine -y, the ratios
MaxSimi of clusters of new senses (6)
MaxSimi of clusters of defined senses
are investigated for 5 words1. Since we found the
ratios are more than 0.95, we set -y to 0.95.
</bodyText>
<subsectionHeader confidence="0.9833735">
2.2 JAIST-2: SVM ClassiÞer with Simple
Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999761">
Our second system JAIST-2 is the classification
based method. It is a WSD classifier trained by
Support Vector Machine (SVM). SVM is widely
used for various NLP tasks including Japanese
WSD (Shirai and Tamagaki, 2004). In this system,
new sense is treated as one of the sense classes.
Thus it would never choose “new sense” for any
instances when no new sense instance is found in
the training data. We used the LIBSVM package2
to train the SVM classifiers. Linear kernel is used
with default parameters.
The following conventional features of WSD
are used for training the SVM classifiers.
</bodyText>
<footnote confidence="0.9684148">
1Among 50 target words in this task, there exist new
sense instances of only ‘kanou’(possibility) in Ddev. So we
checked 4 more words, other than target words.
2http://www.csie.ntu.edu.tw/∼cjlin/
libsvm/
</footnote>
<equation confidence="0.8500905">
• W(0), W(−1), W(−2), W(+1), W(+2)
P(−1), P(−2), P(+1), P(+2)
</equation>
<bodyText confidence="0.99931325">
Words and their POSs appearing before or af-
ter a target instance. A number in parentheses
indicates the position of a word from a target
instance. W (0) means a target instance itself.
</bodyText>
<equation confidence="0.6908615">
• W(−2)&amp;W(−1),W(+1)&amp;W(+2),W(−1)&amp;W(+1)
P(−2)&amp;P(−1), P(+1)&amp;P(+2), P(−1)&amp;P(+1)
</equation>
<bodyText confidence="0.737628">
Pairs of words (or their POSs) near a target
instance.
</bodyText>
<listItem confidence="0.91832">
• Base form of content words appearing in the
context (bag-of-words).
</listItem>
<bodyText confidence="0.999951894736842">
The data used in this task is a set of documents
with 4 different genre codes: OC (Web page),
OW (white paper), PB (book) and PN (newspa-
per). The training data consists of documents of
3 genres OW, PB and PN, while the test data con-
tains all 4 genres. Considering domain adaptation,
each feature fi is represented as fi +g when SVM
classifiers are trained. g is one of the genre codes
{OW, PB, PN} if fi is derived from the docu-
ments of only one genre g in the training data, oth-
erwise g is ‘multi’. For instances in the test data,
only features fi+gt and fi+multi are used, where
gt is the genre code of the document of the target
instance. If gt is OC (which is not included in the
training data), however, all features are used. The
above method aims at distinguishing genre intrin-
sic features and improving the WSD performance
by excluding features which might be associated
with different genres.
</bodyText>
<subsectionHeader confidence="0.994819">
2.3 JAIST-3: Ensemble of Two Systems
</subsectionHeader>
<bodyText confidence="0.9998811">
The third system combines clustering based
method (JAIST-1) and classification based method
(JAIST-2). The basic idea is that JAIST-1 be used
only for reliable clusters, otherwise JAIST-2 is
used. Here ‘reliable cluster’ means a cluster such
that MaxSimi is high. The greater the similar-
ity between the cluster and the sense is, the more
likely the chosen sense is correct. Furthermore,
JAIST-1 is used for new sense detection. The de-
tailed procedure in JAIST-3 is:
</bodyText>
<listItem confidence="0.915802">
1. If JAIST-1 judges a cluster to be a collection
of new sense instances, output ‘new sense’
for instances in that cluster.
2. For instances in the top Ncl clusters of
MaxSimi,output senses chosen by JAIST-1.
3. Otherwise output senses chosen by JAIST-2.
</listItem>
<equation confidence="0.8375415">
Tns = (min
i
</equation>
<page confidence="0.986221">
381
</page>
<bodyText confidence="0.999786166666667">
For the optimization of Ncl, Ddev and Dtr, each
is a half of the training data described in Subsec-
tion 2.1, are used. Dtr is used for training SVM
classiÞers (JAIST-2). Then Ncl is determined so
that the precision of WSD on Ddev is optimized.
In the participating system, Ncl is set to 1.
</bodyText>
<sectionHeader confidence="0.998708" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999682142857143">
Table 1 shows the results of our participating sys-
tems and the baseline system MFS, which always
selects the most frequent sense in the training
data. The column WSD reveals the precision (P)
of word sense disambiguation, while the column
NSD shows accuracy (A), precision (P) and recall
(R) of new sense detection.
</bodyText>
<tableCaption confidence="0.99943">
Table 1: Results
</tableCaption>
<table confidence="0.997923666666667">
WSD A NSD R
P P
MFS 0.6896 0.9844 0 0
JAIST-1 0.6864 0.9512 0.0337 0.0769
JAIST-2 0.7476 0.9872 1 0.1795
JAIST-3 0.7208 0.9532 0.0851 0.2051
</table>
<bodyText confidence="0.997927612244898">
JAIST-1 is the clustering based method. Perfor-
mance of the clustering is also evaluated: Purity
was 0.9636, Inverse-Purity 0.1336 and F-measure
0.2333. Although this system was designed for
new sense detection, it seems not to work well.
It could correctly Þnd only three new sense in-
stances. The main reason is that there were few
instances of the new sense in the test data. Among
2,500 instances (50 instances of each word, for 50
target word), only 39 instances had the new sense.
Our system supposes that considerable number of
new sense instances exist in the corpus, and tries to
gather them into clusters. However, JAIST-1 was
able to construct only one cluster containing mul-
tiple new sense instances. The proposed method is
inadequate for new sense detection when the num-
ber of new sense instances is quite small.
For domain adaptation, features which are in-
trinsic to different genres were excluded for test
instances in JAIST-2. When we trained the system
using all features, its precision was 0.7516, which
is higher than that of JAIST-2. Thus our method
does not work at all. This might be caused by re-
moving features that were derived from different
genre sub-corpora, but effective for WSD. More
sophisticated ways to remove ineffective features
would be required.
JAIST-3 is the ensemble of JAIST-1 and JAIST-
2. Although a little improvement is found by com-
bining two different systems in our preliminary ex-
periments, however, the performance of JAIST-3
was worse than JAIST-2 because of the low per-
formance of JAIST-1. We compared WSD pre-
cision of three systems for 50 individual target
words, and found that JAIST-2 is almost always
the best. The only exceptional case was the target
word ‘ookii’(big). For this adjective, the precision
of JAIST-1, JAIST-2 and JAIST-3 were 0.74, 0.16
and 0.18, respectively. The precision of SVM clas-
siÞers (JAIST-2) is quite bad because of the differ-
ence of text genres. All 50 test instances of this
word were excerpted from Web sub-corpus, which
was not included in the training data. Furthermore,
word sense distributions of test and training data
were totally different. JAIST-1 works better in
such a case. Thus clustering based method might
be an alternative method for WSD when sense dis-
tribution in the test data is far from the training
data.
</bodyText>
<sectionHeader confidence="0.999682" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999974142857143">
The paper reports the participating systems in
SemEval-2 Japanese WSD task. Clustering based
method was designed for new sense detection,
however, it was ineffective when there were few
new sense instances. In future, we would like to
examine the performance of our method when it is
applied to a corpus including more new senses.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999662304347826">
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 7–12.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the SIGIR, pages 50–
57.
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese WSD. In Proceedings of the SemEval-
2010: 5th International Workshop on Semantic
Evaluations.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
Kiyoaki Shirai and Takayuki Tamagaki. 2004. Word
sense disambiguation using heterogeneous language
resources. In Proceedings of the First IJCNLP,
pages 614–619.
</reference>
<page confidence="0.998293">
382
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.550090">
<title confidence="0.9932965">Clustering and based Approaches for Japanese WSD</title>
<author confidence="0.8177915">Kiyoaki Shirai Makoto Nakamura Japan Advanced Institute of Science</author>
<author confidence="0.8177915">Technology</author>
<abstract confidence="0.990088076923077">This paper reports about our three participating systems in SemEval-2 Japanese task. The one is a clustering based method, which chooses a sense for, not individual instances, but automatically constructed clusters of instances. The secone is a method, which an ordinary SVM with simple domain adaptation techniques. The last is an ensemble of these two systems. Results of the formal run shows the second system is the best. Its precision is 0.7476.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="3064" citStr="Agirre and Soroa, 2007" startWordPosition="489" endWordPosition="492">s 2.1 JAIST-1: Clustering based WSD System JAIST-1 was developed by a clustering based method. The overview of the system is shown in Figure 1. It consists of two procedures: (A) clusters of word instances are constructed so that the instances of the same sense are merged, (B) then similarity between a cluster and a sense in a dictionary is measured in order to determine senses of instances in each cluster. Dictionary (A) (B) instance (sentence) Figure 1: Overview of JAIST-1 2.1.1 Clustering of Word Instances As previous work applying clustering techniques for sense induction (Sch¨utze, 1998; Agirre and Soroa, 2007), each instance is represented by a feature vector. In JAIST-1, the following 4 vectors are used for clustering. Collocation Vector This vector reflects collocation including the target instance. Words or POSs appearing just before and after the target instance are used as features, i.e. they correspond to one dimension in the vector. The weight of each feature is 1 if the feature exists for the instance, or 0 if not. Context Vector The vector reflects words in the context of the target instance. All content words appearing in the context are used as features. The window size of the context is</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="4272" citStr="Blei et al., 2003" startWordPosition="692" endWordPosition="695">context is set to 50. Furthermore, related words are also used as features to enCorpus 【サービAl (service) S 客に対するもてなし.M待. 1 help that people who work in a shop give you S 値引きなど、商売上の便宜. 2 help that is provided by a business to customers S 奉fr. 3 volunteer work 379 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 379–382, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics rich the information in the vector. Related words are defined as follows: first topics of texts are automatically derived by Latent Dirichlet Allocation (LDA) (Blei et al., 2003), then words which are the most closely associated with each topic are formed into a ‘related word set’. If one word in a related word set appears in the context, other words in that set also have a positive weight in the vector. More concretely, the weight of each feature is determined to be 1 if the word appears in the context or 0.5 if the word does not appear but is in the related word set. Association Vector Similarly to context vector, this reflects words in the context of the target instance, but data sparseness is alleviated in a different manner. In advance, the co-occurrence matrix A</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the SIGIR,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="5587" citStr="Hofmann, 1999" startWordPosition="933" endWordPosition="934">0 content words. Each element ai,j in the matrix is P(wi|wj), conditional probability representing how likely it is that two words wi and wj will occur in the same document. Now j-th column in A can be regarded as the co-occurrence vector of wj, o(wj). Association vector is a normalized vector of sum of o(wj) for all words in the context. Topic Vector Unlike other vectors, this vector reflects topics of texts. The topics zj automatically derived by PLSI (Probabilistic Latent Semantic Indexing) are used as features. The weight for zj in the vector is P(zj|di) estimated by Folding-in algorithm (Hofmann, 1999), where di is the document containing the instance. Topic vector is motivated by the well-known fact that word senses are highly associated with the topics of documents. Target instances are clustered by the agglomerative clustering algorithm. Similarities between instances are calculated by cosine measure of vectors. Furthermore, pairs of instances in different genre sub-corpora are treated as ‘cannot-link’, so that they will not be merged into the same cluster. Clustering procedure is stopped when the number of instances in a cluster become more than a threshold Nc. Nc is set to 5 in the par</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the SIGIR, pages 50– 57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Okumura</author>
<author>Kiyoaki Shirai</author>
<author>Kanako Komiya</author>
<author>Hikaru Yokono</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task: Japanese WSD. In Proceedings of the SemEval2010: 5th International Workshop on Semantic Evaluations.</booktitle>
<contexts>
<context position="818" citStr="Okumura et al., 2010" startWordPosition="119" endWordPosition="122">paper reports about our three participating systems in SemEval-2 Japanese WSD task. The first one is a clustering based method, which chooses a sense for, not individual instances, but automatically constructed clusters of instances. The second one is a classification method, which is an ordinary SVM classifier with simple domain adaptation techniques. The last is an ensemble of these two systems. Results of the formal run shows the second system is the best. Its precision is 0.7476. 1 Introduction This paper reports about our systems in SemEval2 Japanese Word Sense Disambiguation (WSD) task (Okumura et al., 2010). This task is a lexical sample task for Japanese WSD and has the following two characteristics. First, a balanced wordsense tagged corpus is used for the task. Since it consists of sub-corpora of several domains or genres, domain adaptation might be required. Second, the task takes into account not only the instances having a sense in the given set but also the instances having a sense not found in the set (called ‘new sense’). Participants are required to identify new senses of words in this task. The second characteristics of the task is mainly considered in our system. A clustering based a</context>
</contexts>
<marker>Okumura, Shirai, Komiya, Yokono, 2010</marker>
<rawString>Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, and Hikaru Yokono. 2010. Semeval-2010 task: Japanese WSD. In Proceedings of the SemEval2010: 5th International Workshop on Semantic Evaluations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
<author>Takayuki Tamagaki</author>
</authors>
<title>Word sense disambiguation using heterogeneous language resources.</title>
<date>2004</date>
<booktitle>In Proceedings of the First IJCNLP,</booktitle>
<pages>614--619</pages>
<contexts>
<context position="9935" citStr="Shirai and Tamagaki, 2004" startWordPosition="1707" endWordPosition="1710">xSimi) x -y (5) Since even the cluster of which MaxSimi is minimum represents not a new but a defined sense, the minimum of MaxSimi is decreased by -y. To determine -y, the ratios MaxSimi of clusters of new senses (6) MaxSimi of clusters of defined senses are investigated for 5 words1. Since we found the ratios are more than 0.95, we set -y to 0.95. 2.2 JAIST-2: SVM ClassiÞer with Simple Domain Adaptation Our second system JAIST-2 is the classification based method. It is a WSD classifier trained by Support Vector Machine (SVM). SVM is widely used for various NLP tasks including Japanese WSD (Shirai and Tamagaki, 2004). In this system, new sense is treated as one of the sense classes. Thus it would never choose “new sense” for any instances when no new sense instance is found in the training data. We used the LIBSVM package2 to train the SVM classifiers. Linear kernel is used with default parameters. The following conventional features of WSD are used for training the SVM classifiers. 1Among 50 target words in this task, there exist new sense instances of only ‘kanou’(possibility) in Ddev. So we checked 4 more words, other than target words. 2http://www.csie.ntu.edu.tw/∼cjlin/ libsvm/ • W(0), W(−1), W(−2), </context>
</contexts>
<marker>Shirai, Tamagaki, 2004</marker>
<rawString>Kiyoaki Shirai and Takayuki Tamagaki. 2004. Word sense disambiguation using heterogeneous language resources. In Proceedings of the First IJCNLP, pages 614–619.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>