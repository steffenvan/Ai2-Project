<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9971085">
Are Two Heads Better than One? Crowdsourced Translation via a
Two-Step Collaboration of Non-Professional Translators and Editors
</title>
<author confidence="0.994049">
Rui Yan, Mingkun Gao, Ellie Pavlick, and Chris Callison-Burch
</author>
<affiliation confidence="0.998738">
Computer and Information Science Department,
University of Pennsylvania, Philadelphia, PA 19104, U.S.A.
</affiliation>
<email confidence="0.996402">
{ruiyan,gmingkun,epavlick}@seas.upenn.edu, ccb@cis.upenn.edu
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840888888889">
Crowdsourcing is a viable mechanism for
creating training data for machine trans-
lation. It provides a low cost, fast turn-
around way of processing large volumes
of data. However, when compared to pro-
fessional translation, naive collection of
translations from non-professionals yields
low-quality results. Careful quality con-
trol is necessary for crowdsourcing to
work well. In this paper, we examine
the challenges of a two-step collaboration
process with translation and post-editing
by non-professionals. We develop graph-
based ranking models that automatically
select the best output from multiple redun-
dant versions of translations and edits, and
improves translation quality closer to pro-
fessionals.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998278683333334">
Statistical machine translation (SMT) systems are
trained using bilingual sentence-aligned parallel
corpora. Theoretically, SMT can be applied to
any language pair, but in practice it produces the
state-of-art results only for language pairs with
ample training data, like English-Arabic, English-
Chinese, French-English, etc. SMT gets stuck
in a severe bottleneck for many minority or ‘low
resource’ languages with insufficient data. This
drastically limits which languages SMT can be
successfully applied to. Because of this, collect-
ing parallel corpora for minor languages has be-
come an interesting research challenge. There are
various options for creating training data for new
language pairs. Past approaches have examined
harvesting translated documents from the web
(Resnik and Smith, 2003; Uszkoreit et al., 2010;
Smith et al., 2013), or discovering parallel frag-
ments from comparable corpora (Munteanu and
Marcu, 2005; Abdul-Rauf and Schwenk, 2009;
Smith et al., 2010). Until relatively recently, lit-
tle consideration has been given to creating par-
allel data from scratch. This is because the cost
of hiring professional translators is prohibitively
high. For instance, Germann (2001) hoped to hire
professional translators to create a modest sized
100,000 word Tamil-English parallel corpus, but
were stymied by the costs and the difficulty of
finding good translators for a short-term commit-
ment.
Recently, crowdsourcing has opened the possi-
bility of translating large amounts of text at low
cost using non-professional translators. Facebook
localized its web site into different languages us-
ing volunteers (TechCrunch, 2008). DuoLingo
turns translation into an educational game, and
translates web content using its language learners
(von Ahn, 2013).
Rather than relying on volunteers or gamifica-
tion, NLP research into crowdsourcing transla-
tion has focused on hiring workers on the Ama-
zon Mechanical Turk (MTurk) platform (Callison-
Burch, 2009). This setup presents unique chal-
lenges, since it typically involves non-professional
translators whose language skills are varied, and
since it sometimes involves participants who try
to cheat to get the small financial reward (Zaidan
and Callison-Burch, 2011). A natural approach
for trying to shore up the skills of weak bilinguals
is to pair them with a native speaker of the tar-
get language to edit their translations. We review
relevant research from NLP and human-computer
interaction (HCI) on collaborative translation pro-
cesses in Section 2. To sort good translations from
bad, researchers often solicit multiple, redundant
translations and then build models to try to predict
which translations are the best, or which transla-
tors tend to produce the highest quality transla-
tions.
The contributions of this paper are:
</bodyText>
<page confidence="0.958966">
1134
</page>
<note confidence="0.749286">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1134–1144,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<listItem confidence="0.9355804">
• An analysis of the difficulties posed by a two-
step collaboration between editors and trans-
lators in Mechanical Turk-style crowdsourc-
ing environments. Editors vary in quality,
and poor editing can be difficult to detect.
• A new graph-based algorithm for selecting
the best translation among multiple transla-
tions of the same input. This method takes
into account the collaborative relationship
between the translators and the editors.
</listItem>
<sectionHeader confidence="0.99974" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999971970588235">
In the HCI community, several researchers have
proposed protocols for collaborative translation
efforts (Morita and Ishida, 2009b; Morita and
Ishida, 2009a; Hu, 2009; Hu et al., 2010). These
have focused on an iterative collaboration between
monolingual speakers of the two languages, facil-
itated with a machine translation system. These
studies are similar to ours in that they rely on na-
tive speakers’ understanding of the target language
to correct the disfluencies in poor translations. In
our setup the poor translations are produced by
bilingual individuals who are weak in the target
language, and in their experiments the translations
are the output of a machine translation system.1
Another significant difference is that the HCI
studies assume cooperative participants. For in-
stance, Hu et al. (2010) recruited volunteers from
the International Children’s Digital Library (Hour-
cade et al., 2003) who were all well intentioned
and participated out a sense of altruism and to
build a good reputation among the other volunteer
translators at childrenslibrary.org. Our
setup uses anonymous crowd workers hired on
Mechanical Turk, whose motivation to participate
is financial. Bernstein et al. (2010) characterized
the problems with hiring editors via MTurk for a
word processing application. Workers were either
lazy (meaning they made only minimal edits) or
overly zealous (meaning they made many unnec-
essary edits). Bernstein et al. (2010) addressed
this problem with a three step find-fix-verify pro-
cess. In the first step, workers click on one word
or phrase that needed to be corrected. In the next
step, a separate group of workers proposed correc-
</bodyText>
<listItem confidence="0.5630356">
1A variety of HCI and NLP studies have confirmed the
efficacy of monolingual or bilingual individuals post-editing
of machine translation output (Callison-Burch, 2005; Koehn,
2010; Green et al., 2013). Past NLP work has also examined
automatic post-editing(Knight and Chander, 1994).
</listItem>
<bodyText confidence="0.999793490196079">
tions to problematic regions that had been identi-
fied by multiple workers in the first pass. In the
final step, other workers would validate whether
the proposed corrections were good.
Most NLP research into crowdsourcing has fo-
cused on Mechanical Turk, following pioneering
work by Snow et al. (2008) who showed that the
platform was a viable way of collecting data for a
wide variety of NLP tasks at low cost and in large
volumes. They further showed that non-expert an-
notations are similar to expert annotations when
many non-expert labelings for the same input
are aggregated, through simple voting or through
weighting votes based on how closely non-experts
matched experts on a small amount of calibra-
tion data. MTurk has subsequently been widely
adopted by the NLP community and used for an
extensive range of speech and language applica-
tions (Callison-Burch and Dredze, 2010).
Although hiring professional translators to cre-
ate bilingual training data for machine translation
systems has been deemed infeasible, Mechanical
Turk has provided a low cost way of creating large
volumes of translations (Callison-Burch, 2009;
Ambati and Vogel, 2010). For instance, Zbib et
al. (2012; Zbib et al. (2013) translated 1.5 mil-
lion words of Levine Arabic and Egyptian Arabic,
and showed that a statistical translation system
trained on the dialect data outperformed a system
trained on 100 times more MSA data. Post et al.
(2012) used MTurk to create parallel corpora for
six Indian languages for less than $0.01 per word.
MTurk workers translated more than half a million
words worth of Malayalam in less than a week.
Several researchers have examined the use of ac-
tive learning to further reduce the cost of transla-
tion (Ambati et al., 2010; Ambati, 2012; Blood-
good and Callison-Burch, 2010). Crowdsourcing
allowed real studies to be conducted whereas most
past active learning were simulated. Pavlick et al.
(2014) conducted a large-scale demographic study
of the languages spoken by workers on MTurk by
translating 10,000 words in each of 100 languages.
Chen and Dolan (2012) examined the steps neces-
sary to build a persistent multilingual workforce
on MTurk.
This paper is most closely related to previous
work by Zaidan and Callison-Burch (2011), who
showed that non-professional translators could ap-
proach the level of professional translators. They
solicited multiple redundant translations from dif-
</bodyText>
<page confidence="0.964915">
1135
</page>
<table confidence="0.7518455">
Urdu translator:
According to the territory’s people the pamphlets from
the Taaliban had been read in the announcements in all
the mosques of the Northern Wazeerastan.
English post-editor:
According to locals, the pamphlet released by the Taliban
was read out on the loudspeakers of all the mosques in
North Waziristan.
LDC professional:
According to the local people, the Taliban’s pamphlet
was read over the loudspeakers of all mosques in North
Waziristan.
</table>
<tableCaption confidence="0.998591">
Table 1: Different versions of translations.
</tableCaption>
<bodyText confidence="0.999905266666667">
ferent Turkers for a collection of Urdu sentences
that had been previously professionally translated
by the Linguistics Data Consortium. They built a
model to try to predict on a sentence-by-sentence
and Turker-by-Turker which was the best transla-
tion or translator. They also hired US-based Turk-
ers to edit the translations, since the translators
were largely based in Pakistan and exhibited er-
rors that are characteristic of speakers of English
as a language. Zaidan and Callison-Burch (2011)
observed only modest improvements when incor-
porating these edited translation into their model.
We attempt to analyze why this is, and we pro-
posed a new model to try to better leverage their
data.
</bodyText>
<sectionHeader confidence="0.949368" genericHeader="method">
3 Crowdsourcing Translation
</sectionHeader>
<bodyText confidence="0.977138052631579">
Setup We conduct our experiments using the
data collected by Zaidan and Callison-Burch
(2011). This data set consists 1,792 Urdu sen-
tences from a variety of news and online sources,
each paired with English translations provided by
non-professional translators on Mechanical Turk.
Each Urdu sentence was translated redundantly
by 3 distinct translators, and each translation was
edited by 3 separate (native English-speaking) ed-
itors to correct for grammatical and stylistic er-
rors. In total, this gives us 12 non-professional
English candidate sentences (3 unedited, 9 edited)
per original Urdu sentence. 52 different Turkers
took part in the translation task, each translating
138 sentences on average. In the editing task, 320
Turkers participated, averaging 56 sentences each.
For comparison, the data also includes 4 differ-
ent reference translations for each source sentence,
produced by professional translators.
Table 1 gives an example of an unedited trans-
lation, an edited translation, and a professional
translation for the same sentence. The transla-
tions provided by translators on MTurk are gen-
erally done conscientiously, preserving the mean-
ing of the source sentence, but typically con-
tain simple mistakes like misspellings, typos, and
awkward word choice. English-speaking editors,
despite having no knowledge of the source lan-
guage, are able to fix these errors. In this work,
we show that the collaboration design of two
heads– non-professional Urdu translators and non-
professional English editors– yields better trans-
lated output than would either one working in iso-
lation, and can better approximate the quality of
professional translators.
Analysis We know from inspection that trans-
lations seem to improve with editing (Table 1).
Given the data from MTurk, we explore whether
this is the case in general: Do all translations im-
prove with editing? To what extent does the in-
dividual translator and the individual editor effect
the quality of the final sentence?
Figure 1: Relationship between editor aggressive-
ness and effectiveness. Each point represents an
editor/translation pair. Aggressiveness (x-axis) is
measured as the TER between the pre-edit and
post-edit version of the translation, and effective-
ness (y-axis) is measured as the average amount
by which the editing reduces the translation’s
TERgold. While many editors make only a few
changes, those who make many changes can bring
the translation substantially closer to professional
quality.
We use translation edit rate (TER) as a mea-
sure of translation similarity. TER represents the
amount of change necessary to transform one sen-
tence into another, so a low TER means the two
</bodyText>
<page confidence="0.965244">
1136
</page>
<figure confidence="0.999161117647059">
0 TER9dd
0 TER9dd
0 TER9dd
Editor 0 TERgold 0.03 0.50
Editor 0 TERgold 0.01 0.03
-0.01
-0.03
Editor 0 TERgold -0.03 -0.01
-0.00
-0.04
-0.08
Editor 0 TERgold -0.64 -0.03
-0.01
-0.15
-0.30
0.3 0.9 0.2 0.3 0.2 0.2 0.1 0.2 0.0 0.1
Translation TERgold
</figure>
<figureCaption confidence="0.998758">
Figure 2: Effect of editing on translations of vary-
</figureCaption>
<bodyText confidence="0.966340666666667">
ing quality. Rows reflect bins of editors, with the
worse editors (those whose changes result in in-
creased TERgold) on the top and the most effective
editors (those whose changes result in the largest
reduction in TERgold) on the bottom. Bars re-
flect bins of translations, with the highest TERgold
translations on the left, and the lowest on the
right. We can see from the consistently negative
O TERgold in the bottom row that good editors are
able to improve both good and bad translations.
sentences are very similar. To capture the quality
(“professionalness”) of a translation, we take the
average TER of the translation against each of our
gold translations. That is, we define TERgold of
translation t as
</bodyText>
<equation confidence="0.99335075">
�4
1
TERgold = 4
Z=1
</equation>
<bodyText confidence="0.999705111111111">
where a lower TERgold is indicative of a higher
quality (more professional-sounding) translation.
We first look at editors along two dimensions:
their aggressiveness and their effectiveness. Some
editors may be very aggressive (they make many
changes to the original translation) but still be in-
effective (they fail to bring the quality of the trans-
lation closer to that of a professional). We measure
aggressiveness by looking at the TER between
the pre- and post-edited versions of each editor’s
translations; higher TER implies more aggressive
editing. To measure effectiveness, we look at the
change in TERgold that results from the editing;
negative OTERgold means the editor effectively
improved the quality of the translation, while pos-
itive OTERgold means the editing actually brought
the translation further from our gold standard.
Figure 1 shows the relationship between these
two qualities for individual editor/translation
pairs. We see that while most translations re-
quire only a few edits, there are a large number
of translations which improve substantially after
heavy editing. This trend conforms to our intu-
ition that editing is most useful when the transla-
tion has much room for improvement, and opens
the question of whether good editors can offer im-
provements to translations of all qualities.
To address this question, we split our transla-
tions into 5 bins, based on their TERgold. We also
split our editors into 5 bins, based on their effec-
tiveness (i.e. the average amount by which their
editing reduces TERgold). Figure 2 shows the de-
gree to which editors at each level are able to im-
prove the translations from each bin. We see that
good editors are able to make improvements to
translations of all qualities, but that good editing
has the greatest impact on lower quality transla-
tions. This result suggests that finding good ed-
itor/translator pairs, rather than good editors and
good translators in isolation, should produce the
best translations overall. Figure 3 gives an exam-
ple of how an initially medium-quality translation,
when combined with good editing, produces a bet-
ter result than the higher-quality translation paired
with mediocre editing.
</bodyText>
<sectionHeader confidence="0.985157" genericHeader="method">
4 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999422461538462">
The problem definition of the crowdsourcing
translation task is straightforward: given a set of
candidate translations for a source sentence, we
want to choose the best output translation.
This output translation is the result of the com-
bined translation and editing stages. Therefore,
our method operates over a heterogeneous net-
work that includes translators and post-editors as
well as the translated sentences that they pro-
duce. We frame the problem as follows. We form
two graphs: the first graph (GT) represents Turk-
ers (translator/editor pairs) as nodes; the second
graph (GC) represents candidate translated and
</bodyText>
<figure confidence="0.9975538">
0 TER9dd
0.02
0 TER9dd
0.07
0.05
0.02
0.01
0.00
0.01 Editor 0 TERgold -0.01 0.01
TER(goldZ, t) (1)
</figure>
<page confidence="0.642333">
1137
</page>
<figureCaption confidence="0.95036">
Figure 3: Three alternative translations (left) and the edited versions of each (right). Each edit on the
</figureCaption>
<bodyText confidence="0.966794694444444">
right was produced by a different editor. Order reflects the TERgold of each translation, with the lowest
TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be
easily improved through editing. In the above example, the middle-ranked translation (green) becomes
the best translation after being revised by a good editor.
post-edited sentences (henceforth “candidates”) as
nodes. These two graphs, GT and GC are com-
bined as subgraphs of a third graph (GTC). Edges
in GTC connect author pairs (nodes in GT) to the
candidate that they produced (nodes in GC). To-
gether, GT, GC, and GTC define a co-ranking
problem (Yan et al., 2012a; Yan et al., 2011b; Yan
et al., 2012b) with linkage establishment (Yan et
al., 2011a; Yan et al., 2012c), which we define for-
mally as follows.
Let G denote the heterogeneous graph with
nodes V and edges E. Let G = (V ,E) =
(VT, VC, ET, EC, ETC). G is divided into three
subgraphs, GT, GC, and GTC. GC = (VC, EC) is
a weighted undirected graph representing the can-
didates and their lexical relationships to one an-
other. Let VC denote a collection of translated
and edited candidates, and EC the lexical simi-
larity between the candidates (see Section 4.3 for
details). GT = (VT, ET) is a weighted undirected
graph representing collaborations between Turk-
ers. VT is the set of translator/editor pairs. Edges
ET connect translator/editor pairs in VT which
share a translator and/or editor. Each collabora-
tion (i.e. each node in VT) produces a candidate
(i.e. a node in VC). GTC = (VTC, ETC) is an
unweighted bipartite graph that ties GT and GC
together and represents “authorship”. The graph
G consists of nodes VTC = VT ∪ VC and edges
ETC connecting each candidate with its authoring
translator/post-editor pair. The three sub-networks
(GT, GC, and GTC) are illustrated in Figure 4.
</bodyText>
<subsectionHeader confidence="0.833835">
4.1 Inter-Graph Ranking
</subsectionHeader>
<bodyText confidence="0.999927033333333">
The framework includes three random walks, one
on GT, one on GC and one on GTC. A random
walk on a graph is a Markov chain, its states be-
ing the vertices of the graph. It can be described
by a stochastic square matrix, where the dimen-
sion is the number of vertices in the graph, and the
entries describe the transition probabilities from
one vertex to the next. The mutual reinforcement
framework couples the two random walks on GT
and GC that rank candidates and Turkers in iso-
lation. The ranking method allows us to obtain
a global ranking by taking into account the intra-
/inter-component dependencies. In the following
sections, we describe how we obtain the rankings
on GT and GC, and then move on to discuss how
the two are coupled.
Our algorithm aims to capture the following in-
tuitions. A candidate is important if 1) it is similar
to many of the other proposed candidates and 2)
it is authored by better qualified translators and/or
post-editors. Analogously, a translator/editor pair
is believed to be better qualified if 1) the editor
is collaborating with a good translator and vice
versa and 2) the pair has authored important candi-
dates. This ranking schema is actually a reinforced
process across the heterogeneous graphs. We use
two vectors c = [π(c)]|c|,,1 and t = [π(t)]|t|,,1 to
denote the saliency scores π(.) of candidates and
Turker pairs. The above-mentioned intuitions can
be formulated as follows:
</bodyText>
<listItem confidence="0.96868">
• Homogeneity. We use adjacency matrix
</listItem>
<page confidence="0.990504">
1138
</page>
<figureCaption confidence="0.829451166666667">
Figure 4: 2-step collaborative crowdsourcing translation model based on graph ranking framework in-
cluding three sub-networks. The undirected links between users denotes translation-editing collabora-
tion. The undirected links between candidate translations indicate lexical similarity between candidates.
A bipartite graph ties candidate and Turker networks together by authorship (to make the figure clearer,
some linkage is omitted). A dashed circle indicates the group of candidate translations for a single source
sentence to translate.
</figureCaption>
<bodyText confidence="0.91839">
[M]|c|x|c |to describe the homogeneous affinity
between candidates and [N]|t|x|t |to describe the
affinity between Turkers.
c oc MTc, t oc NTt (2)
where c = |VC |is the number of vertices in the
candidate graph and t = |VT  |is the number of ver-
tices in the Turker graph. The adjacency matrix
[M] denotes the transition probabilities between
candidates, and analogously matrix [N] denotes
the affinity between Turker collaboration pairs.
</bodyText>
<listItem confidence="0.9997766">
• Heterogeneity. We use an adjacency matrix
[ Wˆ]|c|x|t |and [ W¯]|t|x|c |to describe the authorship
between the output candidate and the producer
Turker pair from both of the candidate-to-Turker
and Turker-to-candidate perspectives.
</listItem>
<equation confidence="0.584699">
c oc WˆT t, t oc W¯T c (3)
</equation>
<bodyText confidence="0.967392304347826">
All affinity matrices will be defined in the next
section. By fusing the above equations, we can
have the following iterative calculation in matrix
forms. For numerical computation of the saliency
scores, the initial scores of all sentences and Turk-
ers are set to 1 and the following two steps are
alternated until convergence to select the best can-
didate.
Step 1: compute the saliency scores of candi-
dates, and then normalize using E-1 norm.
c(n) = c(n)/||c(n)||1
Step 2: compute the saliency scores of Turker
pairs, and then normalize using E-1 norm.
t(n) = t(n)/||t(n)||1
where A specifies the relative contributions to the
saliency score trade-off between the homogeneous
affinity and the heterogeneous affinity. In order
to guarantee the convergence of the iterative form,
we must force the transition matrix to be stochastic
and irreducible. To this end, we must make the
c and t column stochastic (Langville and Meyer,
2004). c and t are therefore normalized after each
iteration of Equation (4) and (5).
</bodyText>
<subsectionHeader confidence="0.862666">
4.2 Intra-Graph Ranking
</subsectionHeader>
<bodyText confidence="0.998434">
The standard PageRank algorithm starts from an
arbitrary node and randomly selects to either fol-
low a random out-going edge (considering the
weighted transition matrix) or to jump to a random
node (treating all nodes with equal probability).
</bodyText>
<equation confidence="0.9999435">
c(n) = (1 − A)MTc(n−1) + A Wˆt(n−1)
t(n) = (1 − A)NTt(n−1) + A W¯c(n−1)
</equation>
<page confidence="0.960022">
1139
</page>
<bodyText confidence="0.99964925">
In a simple random walk, it is assumed that all
nodes in the transitional matrix are equi-probable
before the walk starts. Then c and t are calculated
as:
</bodyText>
<equation confidence="0.8218934">
c = µMTc + (1 − µ) 1 (6)
|VC|
and
t = µNTt + (1 − µ) 1 (7)
|VT|
</equation>
<bodyText confidence="0.99994825">
where 1 is a vector with all elements equaling to 1
and the size is correspondent to the size of VC or
VT. µ is the damping factor usually set to 0.85, as
in the PageRank algorithm.
</bodyText>
<subsectionHeader confidence="0.996777">
4.3 Affinity Matrix Establishment
</subsectionHeader>
<bodyText confidence="0.994639310344828">
We introduce the affinity matrix calculation, in-
cluding homogeneous affinity (i.e., M, N) and
heterogeneous affinity (i.e., Wˆ, W¯).
As discussed, we model the collection of can-
didates as a weighted undirected graph, GC, in
which nodes in the graph represent candidate sen-
tences and edges represent lexical relatedness. We
define an edge’s weight to be the cosine similarity
between the candidates represented by the nodes
that it connects. The adjacency matrix M describes
such a graph, with each entry corresponding to the
weight of an edge.
where F(.) is the cosine similarity and c is a term
vector corresponding to a candidate. We treat a
candidate as a short document and weight each
term with if.Of (Manning et al., 2008), where if
is the term frequency and Of is the inverse docu-
ment frequency.
The Turker graph, GT, is an undirected graph
whose edges represent “collaboration.” Formally,
let ti and tj be two translator/editor pairs; we say
that pair ti “collaborates with” pair tj (and there-
fore, there is an edge between ti and tj) if ti and
tj share either a translator or an editor (or share
both a translator and an editor). Let the function
I(ti, tj) denote the number of “collaborations”
(#col) between ti and tj.
j) is an indicator function denot-
ing whether the candidate ci is generated by tj
</bodyText>
<equation confidence="0.862279">
I(ti,tj)Nij=E (10)
k I(ti,tk)
ETC(i,
:
</equation>
<bodyText confidence="0.8801805">
In the bipartite candidate-Turker graph GTC,
the edited sentences after tran
</bodyText>
<figure confidence="0.592737555555556">
slator/editor collab-
orations.
the BLEU scores achievable by professional tran
s-
lators.
F(ci,cj) =||ci |cj||
Mij =
Ek F(ci, ck)
(8)
F(ci, cj)
ci · cj
the entry
A(ci,tj)ˆW
Ek
ij=
A(ck,tj)
5 Evaluation
�
#col (eij ∈ ET )
I(ti, tj)
=
0 otherwise ,(9)
Then the adjacency matrix N is then defined as
�
1 (eij ∈ ETC)
A(ci, tj) = (11)
0 otherwise
</figure>
<bodyText confidence="0.907166590909091">
Through ETC we define the weight matrices
and
containing the conditional probabil-
ities of transitions from ci to tj and vice versa:
A(
i,
)
W
Ek
(12)
¯Wij
ˆWij,
c
j
ij=
A(ci,tk),
We are interested in testing our random walk
method, which incorporates information from
both the candidate translations and from the Turk-
ers. We want to test two versions of our pro-
posed collaborative co-ranking method: 1) based
on the unedited translations only and 2) based on
Metric Since we have four professional transla-
tion sets, we can calculate the Bilingual Evalu-
ation Understudy (BLEU) score (Papineni et al.,
2002) for one professional translator (P1) using
the other three (P2,3,4) as a reference set. We
repeat the process four times, scoring each pro-
fessional translator against the others, to calculate
the expected range of professional quality transla-
tion. In the following sections, we evaluate each of
our methods by calculating BLEU scores against
the same four sets of three reference translations.
Therefore, each number reported in our experi-
mental results is an average of four numbers, cor-
responding to the four possible ways of choosing 3
of the 4 reference sets. This allows us to compare
the BLEU score achieved by our methods against
Baselines As a naive baseline, we choose one
candidate translation at random for each input
Urdu sentence. To establish an upper bound for
our methods, and to determine if there exist high-
quality Turker translations at all
, we compute four
</bodyText>
<page confidence="0.960754">
1140
</page>
<table confidence="0.999763">
Reference (Avg.) 42.51
Oracle (Seg-Trans) 44.93
Oracle (Seg-Trans+Edit) 48.44
Oracle (Turker-Trans) 38.66
Oracle (Turker-Trans+Edit) 39.16
Random 30.52
Lowest TER 35.78
Graph Ranking (Trans) 38.88
Graph Ranking (Trans+Edit) 41.43
</table>
<tableCaption confidence="0.871146">
Table 2: Overall BLEU performance for all
methods (with and without post-editing). The
</tableCaption>
<bodyText confidence="0.980180315789474">
highlighted result indicates the best performance,
which is based on both candidate sentences and
Turker information.
oracle scores. The first oracle operates at the seg-
ment level on the sentences produced by transla-
tors only: for each source segment, we choose
from the translations the one that scores highest
(in terms of BLEU) against the reference sen-
tences. The second oracle is applied similarly,
but chooses from the candidates produced by the
collaboration of translator/post-editor pairs. The
third oracle operates at the worker level: for each
source segment, we choose from the translations
the one provided by the worker whose transla-
tions (over all sentences) score the highest on
average. The fourth oracle also operates at the
worker level, but selects from sentences produced
by translator/post-editor collaborations. These or-
acle methods represent ideal solutions under our
scenario. We also examine two voting-inspired
methods. The first method selects the translation
with the minimum average TER (Snover et al.,
2006) against the other translations; intuitively,
this would represent the “consensus” translation.
The second method selects the translation gen-
erated by the Turker who, on average, provides
translations with the minimum average TER.
Results A summary of our results in given in Ta-
ble 2. As expected, random selection yields bad
performance, with a BLEU score of 30.52. The
oracles indicate that there is usually an acceptable
translation from the Turkers for any given sen-
tence. Since the oracles select from a small group
of only 4 translations per source segment, they are
not overly optimistic, and rather reflect the true po-
tential of the collected translations. On average,
the reference translations give a score of 42.38. To
put this in perspective, the output of a state-of-the-
</bodyText>
<figureCaption confidence="0.598709">
Figure 5: Effect of candidate-Turker coupling (A)
on BLEU score.
</figureCaption>
<bodyText confidence="0.953129685714286">
art machine translation system (the syntax-based
variant of Joshua) achieves a score of 26.91, which
is reported in (Zaidan and Callison-Burch, 2011).
The approach which selects the translations with
the minimum average TER (Snover et al., 2006)
against the other three translations (the “consen-
sus” translation) achieves BLEU scores of 35.78.
Using the raw translations without post-editing,
our graph-based ranking method achieves a BLEU
score of 38.89, compared to Zaidan and Callison-
Burch (2011)’ s reported score of 28.13, which
they achieved using a linear feature-based classi-
fication. Their linear classifier achieved a reported
score of 39.062 when combining information from
both translators and editors. In contrast, our pro-
posed graph-based ranking framework achieves a
score of 41.43 when using the same information.
This boost in BLEU score confirms our intuition
that the hidden collaboration networks between
candidate translations and transltor/editor pairs are
indeed useful.
Parameter Tuning There are two parameters in
our experimental setups: µ controls the probability
of starting a new random walk and A controls the
coupling between the candidate and Turker sub-
graphs. We set the damping factor µ to 0.85, fol-
lowing the standard PageRank paradigm. In order
to determine a value for A, we used the average
BLEU, computed against the professional refer-
2Note that the data we used in our experiments are slightly
different, by discarding nearly 100 NULL sentences in the
raw data. We do not re-implement this baseline but report the
results from the paper directly. According to our experiments,
most of the results generated by baselines and oracles are very
close to the previously reported values.
</bodyText>
<page confidence="0.960434">
1141
</page>
<table confidence="0.9988656">
Plain ranking 38.89
w/o collaboration 38.88
Shared translator 41.38
Shared post-editor 41.29
Shared Turker 41.43
</table>
<tableCaption confidence="0.999911">
Table 3: Variations of all component settings.
</tableCaption>
<bodyText confidence="0.999962777777778">
ence translations, as a tuning metric. We experi-
mented with values of A ranging from 0 to 1, with
a step size of 0.05 (Figure 5). Small A values place
little emphasis on the candidate/Turker coupling,
whereas larger values rely more heavily on the co-
ranking. Overall, we observed better performance
with values within the range of 0.05-0.15. This
suggests that both sources of information– the can-
didate itself and its authors– are important for the
crowdsourcing translation task. In all of our re-
ported results, we used the A = 0.1.
Analysis We examine the relative contribution
of each component of our approach on the overall
performance. We first examine the centroid-based
ranking on the candidate sub-graph (GC) alone
to see the effect of voting among translated sen-
tences; we denote this strategy as plain ranking.
Then we incorporate the standard random walk on
the Turker graph (GT) to include the structural in-
formation but without yet including any collabo-
ration information; that is, we incorporate infor-
mation from GT and GC without including edges
linking the two together. The co-ranking paradigm
is exactly the same as the framework described in
Section 3.2, but with simplified structures.
Finally, we examine the two-step collaboration
based candidate-Turker graph using several varia-
tions on edge establishment. As before, the nodes
are the translator/post-editor working pairs. We
investigate three settings in which 1) edges con-
nect two nodes when they share only a transla-
tor, 2) edges connect two nodes when they share
only a post-editor, and 3) edges connect two nodes
when they share either a translator or a post-editor.
These results are summarized in Table 3.
Interestingly, we observe that when modeling
the linkage between the collaboration pairs, con-
necting Turker pairs which share either a transla-
tor or the post-editor achieves better performance
than connecting pairs that share only translators or
connecting pairs which share only editors. This
result supports the intuition that a denser collabo-
ration matrix will help propagate saliency to good
translators/post-editors and hence provides better
predictions for candidate quality.
</bodyText>
<sectionHeader confidence="0.994643" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999982190476191">
We have proposed an algorithm for using a two-
step collaboration between non-professional trans-
lators and post-editors to obtain professional-
quality translations. Our method, based on a
co-ranking model, selects the best crowdsourced
translation from a set of candidates, and is capable
of selecting translations which near professional
quality.
Crowdsourcing can play a pivotal role in fu-
ture efforts to create parallel translation datasets.
In addition to its benefits of cost and scalabil-
ity, crowdsourcing provides access to languages
that currently fall outside the scope of statistical
machine translation research. In future work on
crowdsourced translation, further benefits in qual-
ity improvement and cost reduction could stem
from 1) building ground truth data sets based on
high-quality Turkers’ translations and 2) identify-
ing when sufficient data has been collected for a
given input, to avoid soliciting unnecessary redun-
dant translations.
</bodyText>
<sectionHeader confidence="0.996923" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99984475">
This material is based on research sponsored by
a DARPA Computer Science Study Panel phase 3
award entitled “Crowdsourcing Translation” (con-
tract D12PC00368). The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as represent-
ing official policies or endorsements by DARPA
or the U.S. Government. This research was sup-
ported by the Johns Hopkins University Human
Language Technology Center of Excellence and
through gifts from Microsoft, Google and Face-
book.
</bodyText>
<sectionHeader confidence="0.99554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.611559666666667">
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
16–23, March.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation sys-
tems? In Workshop on Creating Speech and Lan-
guage Data with MTurk.
</reference>
<page confidence="0.971509">
1142
</page>
<reference confidence="0.998119523364486">
Vamshi Ambati, Stephan Vogel, and Jaime G Car-
bonell. 2010. Active learning and crowd-sourcing
for machine translation. In LREC, volume 11, pages
2169–2174. Citeseer.
Vamshi Ambati. 2012. Active Learning and Crowd-
sourcing for Machine Translation in Low Resource
Scenarios. Ph.D. thesis, Language Technologies In-
stitute, School of Computer Science, Carnegie Mel-
lon University, Pittsburgh, PA.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjrn Hartmann, Mark S. Ackerman, David R.
Karger, David Crowell, and Katrina Panovich.
2010. Soylent: a word processor with a crowd in-
side. In Proceedings of the ACM Symposium on
User Interface Software and Technology (UIST).
Michael Bloodgood and Chris Callison-Burch. 2010.
Large-scale cost-focused active learning for statisti-
cal machine translation. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon’s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 1–12,
June.
Chris Callison-Burch. 2005. Linear B system de-
scription for the 2005 NIST MT evaluation exercise.
In Proceedings of Machine Translation Evaluation
Workshop.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazon’s me-
chanical turk. In Proceedings of EMNLP.
David L. Chen and William B. Dolan. 2012. Build-
ing a persistent workforce on mechanical turk for
multilingual data collection. In Proceedings of the
Human Computer Interaction International Confer-
ence.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang
for the buck can we expect? In Proceedings of
the Workshop on Data-driven Methods in Machine
Translation - Volume 14, DMMT ’01, pages 1–8.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, CHI ’13, pages 439–448.
Juan Pablo Hourcade, Benjamin B Bederson, Allison
Druin, Anne Rose, Allison Farber, and Yoshifumi
Takayama. 2003. The international children’s digi-
tal library: viewing digital books online. Interacting
with Computers, 15(2):151–167.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration be-
tween monolingual users. In Proceedings of
ACM SIGKDD Workshop on Human Computation
(HCOMP).
Chang Hu, Benjamin B. Bederson, Philip Resnik, and
Yakov Kronrod. 2011. Monotrans2: A new human
computation system to support monolingual trans-
lation. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, CHI ’11,
pages 1133–1136.
Chang Hu. 2009. Collaborative translation by mono-
lingual users. In CHI ’09 Extended Abstracts on Hu-
man Factors in Computing Systems, CHI EA ’09,
pages 3105–3108.
Martin Kay. 1998. The proper place of men and ma-
chines in language translation. Machine Transla-
tion, 12(1/2):3–23, January.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In In Proceedings of
AAAI.
Philipp Koehn. 2010. Enabling monolingual transla-
tors: Post-editing vs. options. In HLT-NAACL’10,
pages 537–545, June.
Amy N Langville and Carl D Meyer. 2004. Deeper
inside pagerank. Internet Mathematics, 1(3):335–
380.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 133–137, July.
Annie Louis and Ani Nenkova. 2013. What makes
writing great? first experiments on article quality
prediction in the science journalism domain. Trans-
actions of Association for Computational Linguis-
tics.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Daisuke Morita and Toru Ishida. 2009a. Collaborative
translation by monolinguals with machine transla-
tors. In Proceedings of the 14th International Con-
ference on Intelligent User Interfaces, IUI ’09, pages
361–366.
Daisuke Morita and Toru Ishida. 2009b. Designing
protocols for collaborative translation. In Interna-
tional Conference on Principles of Practice in Multi-
Agent Systems (PRIMA-09), pages 17–32. Springer.
</reference>
<page confidence="0.638908">
1143
</page>
<reference confidence="0.999857375">
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Comput. Linguist.,
31(4):477–504, December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318.
Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev,
and Chris Callison-Burch. 2014. The language de-
mographics of Amazon Mechanical Turk. Transac-
tions of the Association for Computational Linguis-
tics (TACL), 2(Feb):79–92.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six Indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349–380, September.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
HLT-NAACL’10, pages 403–411.
Jason Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale paral-
lel text from the Common Crawl. In Proceedings of
the 2013 Conference of the Association for Compu-
tational Linguistics (ACL 2013), July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223–231.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’08, pages 254–263.
TechCrunch. 2008. Facebook taps users to create
translated versions of site, January.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ’10, pages 1101–
1109.
Luis von Ahn. 2013. Duolingo: Learn a language for
free while helping to translate the web. In Proceed-
ings of the 2013 International Conference on Intel-
ligent User Interfaces, IUI ’13, pages 1–2.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 433–443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolutionary
timeline summarization: A balanced optimization
framework via iterative substitution. In Proceed-
ings of the 34th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’11, pages 745–754.
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012a.
Tweet recommendation with graph co-ranking. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ’12, pages 516–525.
Rui Yan, Xiaojun Wan, Mirella Lapata, Wayne Xin
Zhao, Pu-Jen Cheng, and Xiaoming Li. 2012b.
Visualizing timelines: Evolutionary summarization
via iterative reinforcement between text and image
streams. In Proceedings of the 21st ACM Interna-
tional Conference on Information and Knowledge
Management, CIKM ’12, pages 275–284.
Rui Yan, Zi Yuan, Xiaojun Wan, Yan Zhang, and Xi-
aoming Li. 2012c. Hierarchical graph summariza-
tion: leveraging hybrid information through visible
and invisible linkage. In PAKDD’12, pages 97–108.
Springer.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1220–1229.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic di-
alects. In The 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, and John Makhoul. 2013.
Systematic comparison of professional and crowd-
sourced reference translations for machine transla-
tion. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia.
Xin Wayne Zhao, Yanwei Guo, Rui Yan, Yulan He,
and Xiaoming Li. 2013. Timeline generation with
social attention. In Proceedings of the 36th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’13,
pages 1061–1064.
</reference>
<page confidence="0.995768">
1144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.652727">
<title confidence="0.997478">Are Two Heads Better than One? Crowdsourced Translation via Two-Step Collaboration of Non-Professional Translators and Editors</title>
<author confidence="0.959488">Rui Yan</author>
<author confidence="0.959488">Mingkun Gao</author>
<author confidence="0.959488">Ellie Pavlick</author>
<author confidence="0.959488">Chris</author>
<affiliation confidence="0.963295">Computer and Information Science University of Pennsylvania, Philadelphia, PA 19104,</affiliation>
<email confidence="0.999209">ccb@cis.upenn.edu</email>
<abstract confidence="0.985861842105263">Crowdsourcing is a viable mechanism for creating training data for machine translation. It provides a low cost, fast turnaround way of processing large volumes of data. However, when compared to professional translation, naive collection of translations from non-professionals yields low-quality results. Careful quality control is necessary for crowdsourcing to work well. In this paper, we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals. We develop graphbased ranking models that automatically select the best output from multiple redundant versions of translations and edits, and improves translation quality closer to professionals.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sadaf Abdul-Rauf</author>
<author>Holger Schwenk</author>
</authors>
<title>On the use of comparable corpora to improve SMT performance.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2040" citStr="Abdul-Rauf and Schwenk, 2009" startWordPosition="279" endWordPosition="282">T gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook loc</context>
</contexts>
<marker>Abdul-Rauf, Schwenk, 2009</marker>
<rawString>Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the use of comparable corpora to improve SMT performance. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 16–23, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Stephan Vogel</author>
</authors>
<title>Can crowds build parallel corpora for machine translation systems?</title>
<date>2010</date>
<booktitle>In Workshop on Creating Speech and Language Data with MTurk.</booktitle>
<contexts>
<context position="7617" citStr="Ambati and Vogel, 2010" startWordPosition="1131" endWordPosition="1134">expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 201</context>
</contexts>
<marker>Ambati, Vogel, 2010</marker>
<rawString>Vamshi Ambati and Stephan Vogel. 2010. Can crowds build parallel corpora for machine translation systems? In Workshop on Creating Speech and Language Data with MTurk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Stephan Vogel</author>
<author>Jaime G Carbonell</author>
</authors>
<title>Active learning and crowd-sourcing for machine translation.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<volume>11</volume>
<pages>2169--2174</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="8204" citStr="Ambati et al., 2010" startWordPosition="1233" endWordPosition="1236">h, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional tra</context>
</contexts>
<marker>Ambati, Vogel, Carbonell, 2010</marker>
<rawString>Vamshi Ambati, Stephan Vogel, and Jaime G Carbonell. 2010. Active learning and crowd-sourcing for machine translation. In LREC, volume 11, pages 2169–2174. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
</authors>
<title>Active Learning and Crowdsourcing for Machine Translation in Low Resource Scenarios.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Language Technologies Institute, School of Computer Science, Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="8218" citStr="Ambati, 2012" startWordPosition="1237" endWordPosition="1238">ogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional translators. They</context>
</contexts>
<marker>Ambati, 2012</marker>
<rawString>Vamshi Ambati. 2012. Active Learning and Crowdsourcing for Machine Translation in Low Resource Scenarios. Ph.D. thesis, Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael S Bernstein</author>
<author>Greg Little</author>
<author>Robert C Miller</author>
<author>Bjrn Hartmann</author>
<author>Mark S Ackerman</author>
<author>David R Karger</author>
<author>David Crowell</author>
<author>Katrina Panovich</author>
</authors>
<title>Soylent: a word processor with a crowd inside.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST).</booktitle>
<contexts>
<context position="5727" citStr="Bernstein et al. (2010)" startWordPosition="834" endWordPosition="837">e target language, and in their experiments the translations are the output of a machine translation system.1 Another significant difference is that the HCI studies assume cooperative participants. For instance, Hu et al. (2010) recruited volunteers from the International Children’s Digital Library (Hourcade et al., 2003) who were all well intentioned and participated out a sense of altruism and to build a good reputation among the other volunteer translators at childrenslibrary.org. Our setup uses anonymous crowd workers hired on Mechanical Turk, whose motivation to participate is financial. Bernstein et al. (2010) characterized the problems with hiring editors via MTurk for a word processing application. Workers were either lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnecessary edits). Bernstein et al. (2010) addressed this problem with a three step find-fix-verify process. In the first step, workers click on one word or phrase that needed to be corrected. In the next step, a separate group of workers proposed correc1A variety of HCI and NLP studies have confirmed the efficacy of monolingual or bilingual individuals post-editing of machine translation output (</context>
</contexts>
<marker>Bernstein, Little, Miller, Hartmann, Ackerman, Karger, Crowell, Panovich, 2010</marker>
<rawString>Michael S. Bernstein, Greg Little, Robert C. Miller, Bjrn Hartmann, Mark S. Ackerman, David R. Karger, David Crowell, and Katrina Panovich. 2010. Soylent: a word processor with a crowd inside. In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Large-scale cost-focused active learning for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8255" citStr="Bloodgood and Callison-Burch, 2010" startWordPosition="1239" endWordPosition="1243">or instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional translators. They solicited multiple redundant transla</context>
</contexts>
<marker>Bloodgood, Callison-Burch, 2010</marker>
<rawString>Michael Bloodgood and Chris Callison-Burch. 2010. Large-scale cost-focused active learning for statistical machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="7348" citStr="Callison-Burch and Dredze, 2010" startWordPosition="1093" endWordPosition="1096">k, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations are similar to expert annotations when many non-expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian lan</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Linear B system description for the 2005 NIST MT evaluation exercise.</title>
<date>2005</date>
<booktitle>In Proceedings of Machine Translation Evaluation Workshop.</booktitle>
<contexts>
<context position="6347" citStr="Callison-Burch, 2005" startWordPosition="933" endWordPosition="934"> characterized the problems with hiring editors via MTurk for a word processing application. Workers were either lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnecessary edits). Bernstein et al. (2010) addressed this problem with a three step find-fix-verify process. In the first step, workers click on one word or phrase that needed to be corrected. In the next step, a separate group of workers proposed correc1A variety of HCI and NLP studies have confirmed the efficacy of monolingual or bilingual individuals post-editing of machine translation output (Callison-Burch, 2005; Koehn, 2010; Green et al., 2013). Past NLP work has also examined automatic post-editing(Knight and Chander, 1994). tions to problematic regions that had been identified by multiple workers in the first pass. In the final step, other workers would validate whether the proposed corrections were good. Most NLP research into crowdsourcing has focused on Mechanical Turk, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations a</context>
</contexts>
<marker>Callison-Burch, 2005</marker>
<rawString>Chris Callison-Burch. 2005. Linear B system description for the 2005 NIST MT evaluation exercise. In Proceedings of Machine Translation Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using amazon’s mechanical turk.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7592" citStr="Callison-Burch, 2009" startWordPosition="1129" endWordPosition="1130">tations when many non-expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati </context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using amazon’s mechanical turk. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>William B Dolan</author>
</authors>
<title>Building a persistent workforce on mechanical turk for multilingual data collection.</title>
<date>2012</date>
<booktitle>In Proceedings of the Human Computer Interaction International Conference.</booktitle>
<contexts>
<context position="8541" citStr="Chen and Dolan (2012)" startWordPosition="1284" endWordPosition="1287">l corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional translators. They solicited multiple redundant translations from dif1135 Urdu translator: According to the territory’s people the pamphlets from the Taaliban had been read in the announcements in all the mosques of the Northern Wazeerastan. English post-editor: According to locals, the pamphlet released by the Taliban was read out on the </context>
</contexts>
<marker>Chen, Dolan, 2012</marker>
<rawString>David L. Chen and William B. Dolan. 2012. Building a persistent workforce on mechanical turk for multilingual data collection. In Proceedings of the Human Computer Interaction International Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Building a statistical machine translation system from scratch: How much bang for the buck can we expect?</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Data-driven Methods in Machine Translation - Volume 14, DMMT ’01,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2277" citStr="Germann (2001)" startWordPosition="317" endWordPosition="318">become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site into different languages using volunteers (TechCrunch, 2008). DuoLingo turns translation into an educational game, and translates web content using its language learners (von Ahn, 2013). Rather than relying on volunte</context>
</contexts>
<marker>Germann, 2001</marker>
<rawString>Ulrich Germann. 2001. Building a statistical machine translation system from scratch: How much bang for the buck can we expect? In Proceedings of the Workshop on Data-driven Methods in Machine Translation - Volume 14, DMMT ’01, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Jeffrey Heer</author>
<author>Christopher D Manning</author>
</authors>
<title>The efficacy of human post-editing for language translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’13,</booktitle>
<pages>439--448</pages>
<contexts>
<context position="6381" citStr="Green et al., 2013" startWordPosition="937" endWordPosition="940">ring editors via MTurk for a word processing application. Workers were either lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnecessary edits). Bernstein et al. (2010) addressed this problem with a three step find-fix-verify process. In the first step, workers click on one word or phrase that needed to be corrected. In the next step, a separate group of workers proposed correc1A variety of HCI and NLP studies have confirmed the efficacy of monolingual or bilingual individuals post-editing of machine translation output (Callison-Burch, 2005; Koehn, 2010; Green et al., 2013). Past NLP work has also examined automatic post-editing(Knight and Chander, 1994). tions to problematic regions that had been identified by multiple workers in the first pass. In the final step, other workers would validate whether the proposed corrections were good. Most NLP research into crowdsourcing has focused on Mechanical Turk, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations are similar to expert annotations w</context>
</contexts>
<marker>Green, Heer, Manning, 2013</marker>
<rawString>Spence Green, Jeffrey Heer, and Christopher D. Manning. 2013. The efficacy of human post-editing for language translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’13, pages 439–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Pablo Hourcade</author>
<author>Benjamin B Bederson</author>
<author>Allison Druin</author>
<author>Anne Rose</author>
<author>Allison Farber</author>
<author>Yoshifumi Takayama</author>
</authors>
<title>The international children’s digital library: viewing digital books online.</title>
<date>2003</date>
<journal>Interacting with Computers,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="5427" citStr="Hourcade et al., 2003" startWordPosition="788" endWordPosition="792">, facilitated with a machine translation system. These studies are similar to ours in that they rely on native speakers’ understanding of the target language to correct the disfluencies in poor translations. In our setup the poor translations are produced by bilingual individuals who are weak in the target language, and in their experiments the translations are the output of a machine translation system.1 Another significant difference is that the HCI studies assume cooperative participants. For instance, Hu et al. (2010) recruited volunteers from the International Children’s Digital Library (Hourcade et al., 2003) who were all well intentioned and participated out a sense of altruism and to build a good reputation among the other volunteer translators at childrenslibrary.org. Our setup uses anonymous crowd workers hired on Mechanical Turk, whose motivation to participate is financial. Bernstein et al. (2010) characterized the problems with hiring editors via MTurk for a word processing application. Workers were either lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnecessary edits). Bernstein et al. (2010) addressed this problem with a three step find-fix-verify </context>
</contexts>
<marker>Hourcade, Bederson, Druin, Rose, Farber, Takayama, 2003</marker>
<rawString>Juan Pablo Hourcade, Benjamin B Bederson, Allison Druin, Anne Rose, Allison Farber, and Yoshifumi Takayama. 2003. The international children’s digital library: viewing digital books online. Interacting with Computers, 15(2):151–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Hu</author>
<author>Benjamin B Bederson</author>
<author>Philip Resnik</author>
</authors>
<title>Translation by iterative collaboration between monolingual users.</title>
<date>2010</date>
<booktitle>In Proceedings of ACM SIGKDD Workshop on Human Computation (HCOMP).</booktitle>
<contexts>
<context position="4705" citStr="Hu et al., 2010" startWordPosition="679" endWordPosition="682">e difficulties posed by a twostep collaboration between editors and translators in Mechanical Turk-style crowdsourcing environments. Editors vary in quality, and poor editing can be difficult to detect. • A new graph-based algorithm for selecting the best translation among multiple translations of the same input. This method takes into account the collaborative relationship between the translators and the editors. 2 Related work In the HCI community, several researchers have proposed protocols for collaborative translation efforts (Morita and Ishida, 2009b; Morita and Ishida, 2009a; Hu, 2009; Hu et al., 2010). These have focused on an iterative collaboration between monolingual speakers of the two languages, facilitated with a machine translation system. These studies are similar to ours in that they rely on native speakers’ understanding of the target language to correct the disfluencies in poor translations. In our setup the poor translations are produced by bilingual individuals who are weak in the target language, and in their experiments the translations are the output of a machine translation system.1 Another significant difference is that the HCI studies assume cooperative participants. For</context>
</contexts>
<marker>Hu, Bederson, Resnik, 2010</marker>
<rawString>Chang Hu, Benjamin B. Bederson, and Philip Resnik. 2010. Translation by iterative collaboration between monolingual users. In Proceedings of ACM SIGKDD Workshop on Human Computation (HCOMP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Hu</author>
<author>Benjamin B Bederson</author>
<author>Philip Resnik</author>
<author>Yakov Kronrod</author>
</authors>
<title>Monotrans2: A new human computation system to support monolingual translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’11,</booktitle>
<pages>1133--1136</pages>
<marker>Hu, Bederson, Resnik, Kronrod, 2011</marker>
<rawString>Chang Hu, Benjamin B. Bederson, Philip Resnik, and Yakov Kronrod. 2011. Monotrans2: A new human computation system to support monolingual translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’11, pages 1133–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Hu</author>
</authors>
<title>Collaborative translation by monolingual users.</title>
<date>2009</date>
<booktitle>In CHI ’09 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’09,</booktitle>
<pages>3105--3108</pages>
<contexts>
<context position="4687" citStr="Hu, 2009" startWordPosition="677" endWordPosition="678">ysis of the difficulties posed by a twostep collaboration between editors and translators in Mechanical Turk-style crowdsourcing environments. Editors vary in quality, and poor editing can be difficult to detect. • A new graph-based algorithm for selecting the best translation among multiple translations of the same input. This method takes into account the collaborative relationship between the translators and the editors. 2 Related work In the HCI community, several researchers have proposed protocols for collaborative translation efforts (Morita and Ishida, 2009b; Morita and Ishida, 2009a; Hu, 2009; Hu et al., 2010). These have focused on an iterative collaboration between monolingual speakers of the two languages, facilitated with a machine translation system. These studies are similar to ours in that they rely on native speakers’ understanding of the target language to correct the disfluencies in poor translations. In our setup the poor translations are produced by bilingual individuals who are weak in the target language, and in their experiments the translations are the output of a machine translation system.1 Another significant difference is that the HCI studies assume cooperative</context>
</contexts>
<marker>Hu, 2009</marker>
<rawString>Chang Hu. 2009. Collaborative translation by monolingual users. In CHI ’09 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’09, pages 3105–3108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>The proper place of men and machines in language translation.</title>
<date>1998</date>
<booktitle>Machine Translation,</booktitle>
<pages>12--1</pages>
<marker>Kay, 1998</marker>
<rawString>Martin Kay. 1998. The proper place of men and machines in language translation. Machine Translation, 12(1/2):3–23, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
</authors>
<title>Automated postediting of documents. In</title>
<date>1994</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="6463" citStr="Knight and Chander, 1994" startWordPosition="948" endWordPosition="951">r lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnecessary edits). Bernstein et al. (2010) addressed this problem with a three step find-fix-verify process. In the first step, workers click on one word or phrase that needed to be corrected. In the next step, a separate group of workers proposed correc1A variety of HCI and NLP studies have confirmed the efficacy of monolingual or bilingual individuals post-editing of machine translation output (Callison-Burch, 2005; Koehn, 2010; Green et al., 2013). Past NLP work has also examined automatic post-editing(Knight and Chander, 1994). tions to problematic regions that had been identified by multiple workers in the first pass. In the final step, other workers would validate whether the proposed corrections were good. Most NLP research into crowdsourcing has focused on Mechanical Turk, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations are similar to expert annotations when many non-expert labelings for the same input are aggregated, through simple vo</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Enabling monolingual translators: Post-editing vs. options.</title>
<date>2010</date>
<booktitle>In HLT-NAACL’10,</booktitle>
<pages>537--545</pages>
<contexts>
<context position="6360" citStr="Koehn, 2010" startWordPosition="935" endWordPosition="936">blems with hiring editors via MTurk for a word processing application. Workers were either lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnecessary edits). Bernstein et al. (2010) addressed this problem with a three step find-fix-verify process. In the first step, workers click on one word or phrase that needed to be corrected. In the next step, a separate group of workers proposed correc1A variety of HCI and NLP studies have confirmed the efficacy of monolingual or bilingual individuals post-editing of machine translation output (Callison-Burch, 2005; Koehn, 2010; Green et al., 2013). Past NLP work has also examined automatic post-editing(Knight and Chander, 1994). tions to problematic regions that had been identified by multiple workers in the first pass. In the final step, other workers would validate whether the proposed corrections were good. Most NLP research into crowdsourcing has focused on Mechanical Turk, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations are similar to</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Enabling monolingual translators: Post-editing vs. options. In HLT-NAACL’10, pages 537–545, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy N Langville</author>
<author>Carl D Meyer</author>
</authors>
<title>Deeper inside pagerank.</title>
<date>2004</date>
<journal>Internet Mathematics,</journal>
<volume>1</volume>
<issue>3</issue>
<pages>380</pages>
<contexts>
<context position="22347" citStr="Langville and Meyer, 2004" startWordPosition="3503" endWordPosition="3506">ted until convergence to select the best candidate. Step 1: compute the saliency scores of candidates, and then normalize using E-1 norm. c(n) = c(n)/||c(n)||1 Step 2: compute the saliency scores of Turker pairs, and then normalize using E-1 norm. t(n) = t(n)/||t(n)||1 where A specifies the relative contributions to the saliency score trade-off between the homogeneous affinity and the heterogeneous affinity. In order to guarantee the convergence of the iterative form, we must force the transition matrix to be stochastic and irreducible. To this end, we must make the c and t column stochastic (Langville and Meyer, 2004). c and t are therefore normalized after each iteration of Equation (4) and (5). 4.2 Intra-Graph Ranking The standard PageRank algorithm starts from an arbitrary node and randomly selects to either follow a random out-going edge (considering the weighted transition matrix) or to jump to a random node (treating all nodes with equal probability). c(n) = (1 − A)MTc(n−1) + A Wˆt(n−1) t(n) = (1 − A)NTt(n−1) + A W¯c(n−1) 1139 In a simple random walk, it is assumed that all nodes in the transitional matrix are equi-probable before the walk starts. Then c and t are calculated as: c = µMTc + (1 − µ) 1 </context>
</contexts>
<marker>Langville, Meyer, 2004</marker>
<rawString>Amy N Langville and Carl D Meyer. 2004. Deeper inside pagerank. Internet Mathematics, 1(3):335– 380.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Ann Irvine</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Ziyuan Wang</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua 2.0: A toolkit for parsing-based machine translation with syntax, semirings, discriminative training and other goodies.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>133--137</pages>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Irvine, Khudanpur, Schwartz, Thornton, Wang, Weese, Zaidan, 2010</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Ann Irvine, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Ziyuan Wang, Jonathan Weese, and Omar Zaidan. 2010. Joshua 2.0: A toolkit for parsing-based machine translation with syntax, semirings, discriminative training and other goodies. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 133–137, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>What makes writing great? first experiments on article quality prediction in the science journalism domain. Transactions of Association for Computational Linguistics.</title>
<date>2013</date>
<marker>Louis, Nenkova, 2013</marker>
<rawString>Annie Louis and Ani Nenkova. 2013. What makes writing great? first experiments on article quality prediction in the science journalism domain. Transactions of Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval, volume 1.</title>
<date>2008</date>
<publisher>Cambridge University Press Cambridge.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Morita</author>
<author>Toru Ishida</author>
</authors>
<title>Collaborative translation by monolinguals with machine translators.</title>
<date>2009</date>
<booktitle>In Proceedings of the 14th International Conference on Intelligent User Interfaces, IUI ’09,</booktitle>
<pages>361--366</pages>
<contexts>
<context position="4650" citStr="Morita and Ishida, 2009" startWordPosition="669" endWordPosition="672"> Association for Computational Linguistics • An analysis of the difficulties posed by a twostep collaboration between editors and translators in Mechanical Turk-style crowdsourcing environments. Editors vary in quality, and poor editing can be difficult to detect. • A new graph-based algorithm for selecting the best translation among multiple translations of the same input. This method takes into account the collaborative relationship between the translators and the editors. 2 Related work In the HCI community, several researchers have proposed protocols for collaborative translation efforts (Morita and Ishida, 2009b; Morita and Ishida, 2009a; Hu, 2009; Hu et al., 2010). These have focused on an iterative collaboration between monolingual speakers of the two languages, facilitated with a machine translation system. These studies are similar to ours in that they rely on native speakers’ understanding of the target language to correct the disfluencies in poor translations. In our setup the poor translations are produced by bilingual individuals who are weak in the target language, and in their experiments the translations are the output of a machine translation system.1 Another significant difference is th</context>
</contexts>
<marker>Morita, Ishida, 2009</marker>
<rawString>Daisuke Morita and Toru Ishida. 2009a. Collaborative translation by monolinguals with machine translators. In Proceedings of the 14th International Conference on Intelligent User Interfaces, IUI ’09, pages 361–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Morita</author>
<author>Toru Ishida</author>
</authors>
<title>Designing protocols for collaborative translation.</title>
<date>2009</date>
<booktitle>In International Conference on Principles of Practice in MultiAgent Systems (PRIMA-09),</booktitle>
<pages>17--32</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4650" citStr="Morita and Ishida, 2009" startWordPosition="669" endWordPosition="672"> Association for Computational Linguistics • An analysis of the difficulties posed by a twostep collaboration between editors and translators in Mechanical Turk-style crowdsourcing environments. Editors vary in quality, and poor editing can be difficult to detect. • A new graph-based algorithm for selecting the best translation among multiple translations of the same input. This method takes into account the collaborative relationship between the translators and the editors. 2 Related work In the HCI community, several researchers have proposed protocols for collaborative translation efforts (Morita and Ishida, 2009b; Morita and Ishida, 2009a; Hu, 2009; Hu et al., 2010). These have focused on an iterative collaboration between monolingual speakers of the two languages, facilitated with a machine translation system. These studies are similar to ours in that they rely on native speakers’ understanding of the target language to correct the disfluencies in poor translations. In our setup the poor translations are produced by bilingual individuals who are weak in the target language, and in their experiments the translations are the output of a machine translation system.1 Another significant difference is th</context>
</contexts>
<marker>Morita, Ishida, 2009</marker>
<rawString>Daisuke Morita and Toru Ishida. 2009b. Designing protocols for collaborative translation. In International Conference on Principles of Practice in MultiAgent Systems (PRIMA-09), pages 17–32. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="2010" citStr="Munteanu and Marcu, 2005" startWordPosition="275" endWordPosition="278">e, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professi</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Comput. Linguist., 31(4):477–504, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="25577" citStr="Papineni et al., 2002" startWordPosition="4076" endWordPosition="4079">(11) 0 otherwise Through ETC we define the weight matrices and containing the conditional probabilities of transitions from ci to tj and vice versa: A( i, ) W Ek (12) ¯Wij ˆWij, c j ij= A(ci,tk), We are interested in testing our random walk method, which incorporates information from both the candidate translations and from the Turkers. We want to test two versions of our proposed collaborative co-ranking method: 1) based on the unedited translations only and 2) based on Metric Since we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score (Papineni et al., 2002) for one professional translator (P1) using the other three (P2,3,4) as a reference set. We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation. In the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations. Therefore, each number reported in our experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets. This allows us to compare the BLEU </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellie Pavlick</author>
<author>Matt Post</author>
<author>Ann Irvine</author>
<author>Dmitry Kachaev</author>
<author>Chris Callison-Burch</author>
</authors>
<title>The language demographics of Amazon Mechanical Turk. Transactions of the Association for Computational Linguistics (TACL),</title>
<date>2014</date>
<pages>2--79</pages>
<contexts>
<context position="8379" citStr="Pavlick et al. (2014)" startWordPosition="1258" endWordPosition="1261">atistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional translators. They solicited multiple redundant translations from dif1135 Urdu translator: According to the territory’s people the pamphlets from the Taaliban had been read in the</context>
</contexts>
<marker>Pavlick, Post, Irvine, Kachaev, Callison-Burch, 2014</marker>
<rawString>Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev, and Chris Callison-Burch. 2014. The language demographics of Amazon Mechanical Turk. Transactions of the Association for Computational Linguistics (TACL), 2(Feb):79–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Constructing parallel corpora for six Indian languages via crowdsourcing.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="7891" citStr="Post et al. (2012)" startWordPosition="1178" endWordPosition="1181">range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 wor</context>
</contexts>
<marker>Post, Callison-Burch, Osborne, 2012</marker>
<rawString>Matt Post, Chris Callison-Burch, and Miles Osborne. 2012. Constructing parallel corpora for six Indian languages via crowdsourcing. In Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="1880" citStr="Resnik and Smith, 2003" startWordPosition="255" endWordPosition="258">actice it produces the state-of-art results only for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term </context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A. Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349–380, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Chris Quirk</author>
<author>Kristina Toutanova</author>
</authors>
<title>Extracting parallel sentences from comparable corpora using document level alignment.</title>
<date>2010</date>
<booktitle>In HLT-NAACL’10,</booktitle>
<pages>403--411</pages>
<contexts>
<context position="2061" citStr="Smith et al., 2010" startWordPosition="283" endWordPosition="286">eneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site i</context>
</contexts>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>Jason R. Smith, Chris Quirk, and Kristina Toutanova. 2010. Extracting parallel sentences from comparable corpora using document level alignment. In HLT-NAACL’10, pages 403–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Smith</author>
<author>Herve Saint-Amand</author>
<author>Magdalena Plamada</author>
<author>Philipp Koehn</author>
<author>Chris Callison-Burch</author>
<author>Adam Lopez</author>
</authors>
<title>Dirt cheap web-scale parallel text from the Common Crawl.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="1925" citStr="Smith et al., 2013" startWordPosition="263" endWordPosition="266"> for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opene</context>
</contexts>
<marker>Smith, Saint-Amand, Plamada, Koehn, Callison-Burch, Lopez, 2013</marker>
<rawString>Jason Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch, and Adam Lopez. 2013. Dirt cheap web-scale parallel text from the Common Crawl. In Proceedings of the 2013 Conference of the Association for Computational Linguistics (ACL 2013), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of association for machine translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="27816" citStr="Snover et al., 2006" startWordPosition="4421" endWordPosition="4424">om the candidates produced by the collaboration of translator/post-editor pairs. The third oracle operates at the worker level: for each source segment, we choose from the translations the one provided by the worker whose translations (over all sentences) score the highest on average. The fourth oracle also operates at the worker level, but selects from sentences produced by translator/post-editor collaborations. These oracle methods represent ideal solutions under our scenario. We also examine two voting-inspired methods. The first method selects the translation with the minimum average TER (Snover et al., 2006) against the other translations; intuitively, this would represent the “consensus” translation. The second method selects the translation generated by the Turker who, on average, provides translations with the minimum average TER. Results A summary of our results in given in Table 2. As expected, random selection yields bad performance, with a BLEU score of 30.52. The oracles indicate that there is usually an acceptable translation from the Turkers for any given sentence. Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of association for machine translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 254–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TechCrunch</author>
</authors>
<title>Facebook taps users to create translated versions of site,</title>
<date>2008</date>
<contexts>
<context position="2720" citStr="TechCrunch, 2008" startWordPosition="382" endWordPosition="383">eration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site into different languages using volunteers (TechCrunch, 2008). DuoLingo turns translation into an educational game, and translates web content using its language learners (von Ahn, 2013). Rather than relying on volunteers or gamification, NLP research into crowdsourcing translation has focused on hiring workers on the Amazon Mechanical Turk (MTurk) platform (CallisonBurch, 2009). This setup presents unique challenges, since it typically involves non-professional translators whose language skills are varied, and since it sometimes involves participants who try to cheat to get the small financial reward (Zaidan and Callison-Burch, 2011). A natural approac</context>
</contexts>
<marker>TechCrunch, 2008</marker>
<rawString>TechCrunch. 2008. Facebook taps users to create translated versions of site, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay M Ponte</author>
<author>Ashok C Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large scale parallel document mining for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>1101--1109</pages>
<contexts>
<context position="1904" citStr="Uszkoreit et al., 2010" startWordPosition="259" endWordPosition="262">tate-of-art results only for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, cr</context>
</contexts>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 1101– 1109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
</authors>
<title>Duolingo: Learn a language for free while helping to translate the web.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 International Conference on Intelligent User Interfaces, IUI ’13,</booktitle>
<pages>1--2</pages>
<marker>von Ahn, 2013</marker>
<rawString>Luis von Ahn. 2013. Duolingo: Learn a language for free while helping to translate the web. In Proceedings of the 2013 International Conference on Intelligent User Interfaces, IUI ’13, pages 1–2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Liang Kong</author>
<author>Congrui Huang</author>
<author>Xiaojun Wan</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Timeline generation through evolutionary trans-temporal summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>433--443</pages>
<contexts>
<context position="17525" citStr="Yan et al., 2011" startWordPosition="2710" endWordPosition="2713">anslation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GTC). Edges in GTC connect author pairs (nodes in GT) to the candidate that they produced (nodes in GC). Together, GT, GC, and GTC define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT, VC, ET, EC, ETC). G is divided into three subgraphs, GT, GC, and GTC. GC = (VC, EC) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT, ET) is a weighted undirected graph repre</context>
</contexts>
<marker>Yan, Kong, Huang, Wan, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xiaoming Li, and Yan Zhang. 2011a. Timeline generation through evolutionary trans-temporal summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 433–443.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Xiaojun Wan</author>
<author>Jahna Otterbacher</author>
<author>Liang Kong</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Evolutionary timeline summarization: A balanced optimization framework via iterative substitution.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’11,</booktitle>
<pages>745--754</pages>
<contexts>
<context position="17525" citStr="Yan et al., 2011" startWordPosition="2710" endWordPosition="2713">anslation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GTC). Edges in GTC connect author pairs (nodes in GT) to the candidate that they produced (nodes in GC). Together, GT, GC, and GTC define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT, VC, ET, EC, ETC). G is divided into three subgraphs, GT, GC, and GTC. GC = (VC, EC) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT, ET) is a weighted undirected graph repre</context>
</contexts>
<marker>Yan, Wan, Otterbacher, Kong, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, and Yan Zhang. 2011b. Evolutionary timeline summarization: A balanced optimization framework via iterative substitution. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’11, pages 745–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Mirella Lapata</author>
<author>Xiaoming Li</author>
</authors>
<title>Tweet recommendation with graph co-ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>516--525</pages>
<contexts>
<context position="17506" citStr="Yan et al., 2012" startWordPosition="2706" endWordPosition="2709"> TERgold of each translation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GTC). Edges in GTC connect author pairs (nodes in GT) to the candidate that they produced (nodes in GC). Together, GT, GC, and GTC define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT, VC, ET, EC, ETC). G is divided into three subgraphs, GT, GC, and GTC. GC = (VC, EC) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT, ET) is a weighted und</context>
</contexts>
<marker>Yan, Lapata, Li, 2012</marker>
<rawString>Rui Yan, Mirella Lapata, and Xiaoming Li. 2012a. Tweet recommendation with graph co-ranking. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 516–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Xiaojun Wan</author>
<author>Mirella Lapata</author>
<author>Wayne Xin Zhao</author>
<author>Pu-Jen Cheng</author>
<author>Xiaoming Li</author>
</authors>
<title>Visualizing timelines: Evolutionary summarization via iterative reinforcement between text and image streams.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12,</booktitle>
<pages>275--284</pages>
<contexts>
<context position="17506" citStr="Yan et al., 2012" startWordPosition="2706" endWordPosition="2709"> TERgold of each translation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GTC). Edges in GTC connect author pairs (nodes in GT) to the candidate that they produced (nodes in GC). Together, GT, GC, and GTC define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT, VC, ET, EC, ETC). G is divided into three subgraphs, GT, GC, and GTC. GC = (VC, EC) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT, ET) is a weighted und</context>
</contexts>
<marker>Yan, Wan, Lapata, Zhao, Cheng, Li, 2012</marker>
<rawString>Rui Yan, Xiaojun Wan, Mirella Lapata, Wayne Xin Zhao, Pu-Jen Cheng, and Xiaoming Li. 2012b. Visualizing timelines: Evolutionary summarization via iterative reinforcement between text and image streams. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12, pages 275–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Zi Yuan</author>
<author>Xiaojun Wan</author>
<author>Yan Zhang</author>
<author>Xiaoming Li</author>
</authors>
<title>Hierarchical graph summarization: leveraging hybrid information through visible and invisible linkage.</title>
<date>2012</date>
<booktitle>In PAKDD’12,</booktitle>
<pages>97--108</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17506" citStr="Yan et al., 2012" startWordPosition="2706" endWordPosition="2709"> TERgold of each translation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GTC). Edges in GTC connect author pairs (nodes in GT) to the candidate that they produced (nodes in GC). Together, GT, GC, and GTC define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT, VC, ET, EC, ETC). G is divided into three subgraphs, GT, GC, and GTC. GC = (VC, EC) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT, ET) is a weighted und</context>
</contexts>
<marker>Yan, Yuan, Wan, Zhang, Li, 2012</marker>
<rawString>Rui Yan, Zi Yuan, Xiaojun Wan, Yan Zhang, and Xiaoming Li. 2012c. Hierarchical graph summarization: leveraging hybrid information through visible and invisible linkage. In PAKDD’12, pages 97–108. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Crowdsourcing translation: Professional quality from non-professionals.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1220--1229</pages>
<contexts>
<context position="3301" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="465" endWordPosition="468">erent languages using volunteers (TechCrunch, 2008). DuoLingo turns translation into an educational game, and translates web content using its language learners (von Ahn, 2013). Rather than relying on volunteers or gamification, NLP research into crowdsourcing translation has focused on hiring workers on the Amazon Mechanical Turk (MTurk) platform (CallisonBurch, 2009). This setup presents unique challenges, since it typically involves non-professional translators whose language skills are varied, and since it sometimes involves participants who try to cheat to get the small financial reward (Zaidan and Callison-Burch, 2011). A natural approach for trying to shore up the skills of weak bilinguals is to pair them with a native speaker of the target language to edit their translations. We review relevant research from NLP and human-computer interaction (HCI) on collaborative translation processes in Section 2. To sort good translations from bad, researchers often solicit multiple, redundant translations and then build models to try to predict which translations are the best, or which translators tend to produce the highest quality translations. The contributions of this paper are: 1134 Proceedings of the 52nd Annua</context>
<context position="8713" citStr="Zaidan and Callison-Burch (2011)" startWordPosition="1311" endWordPosition="1314">Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional translators. They solicited multiple redundant translations from dif1135 Urdu translator: According to the territory’s people the pamphlets from the Taaliban had been read in the announcements in all the mosques of the Northern Wazeerastan. English post-editor: According to locals, the pamphlet released by the Taliban was read out on the loudspeakers of all the mosques in North Waziristan. LDC professional: According to the local people, the Taliban’s pamphlet was read over the loudspeakers of all mosques i</context>
<context position="10191" citStr="Zaidan and Callison-Burch (2011)" startWordPosition="1536" endWordPosition="1539">ntence-by-sentence and Turker-by-Turker which was the best translation or translator. They also hired US-based Turkers to edit the translations, since the translators were largely based in Pakistan and exhibited errors that are characteristic of speakers of English as a language. Zaidan and Callison-Burch (2011) observed only modest improvements when incorporating these edited translation into their model. We attempt to analyze why this is, and we proposed a new model to try to better leverage their data. 3 Crowdsourcing Translation Setup We conduct our experiments using the data collected by Zaidan and Callison-Burch (2011). This data set consists 1,792 Urdu sentences from a variety of news and online sources, each paired with English translations provided by non-professional translators on Mechanical Turk. Each Urdu sentence was translated redundantly by 3 distinct translators, and each translation was edited by 3 separate (native English-speaking) editors to correct for grammatical and stylistic errors. In total, this gives us 12 non-professional English candidate sentences (3 unedited, 9 edited) per original Urdu sentence. 52 different Turkers took part in the translation task, each translating 138 sentences </context>
<context position="28813" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="4579" endWordPosition="4582">. The oracles indicate that there is usually an acceptable translation from the Turkers for any given sentence. Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true potential of the collected translations. On average, the reference translations give a score of 42.38. To put this in perspective, the output of a state-of-theFigure 5: Effect of candidate-Turker coupling (A) on BLEU score. art machine translation system (the syntax-based variant of Joshua) achieves a score of 26.91, which is reported in (Zaidan and Callison-Burch, 2011). The approach which selects the translations with the minimum average TER (Snover et al., 2006) against the other three translations (the “consensus” translation) achieves BLEU scores of 35.78. Using the raw translations without post-editing, our graph-based ranking method achieves a BLEU score of 38.89, compared to Zaidan and CallisonBurch (2011)’ s reported score of 28.13, which they achieved using a linear feature-based classification. Their linear classifier achieved a reported score of 39.062 when combining information from both translators and editors. In contrast, our proposed graph-ba</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2011. Crowdsourcing translation: Professional quality from non-professionals. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1220–1229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rabih Zbib</author>
<author>Erika Malchiodi</author>
<author>Jacob Devlin</author>
<author>David Stallard</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
<author>Omar F Zaidan</author>
<author>Chris CallisonBurch</author>
</authors>
<title>Machine translation of Arabic dialects.</title>
<date>2012</date>
<booktitle>In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7650" citStr="Zbib et al. (2012" startWordPosition="1137" endWordPosition="1140"> aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, </context>
</contexts>
<marker>Zbib, Malchiodi, Devlin, Stallard, Matsoukas, Schwartz, Makhoul, Zaidan, CallisonBurch, 2012</marker>
<rawString>Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Makhoul, Omar F. Zaidan, and Chris CallisonBurch. 2012. Machine translation of Arabic dialects. In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rabih Zbib</author>
<author>Gretchen Markiewicz</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Systematic comparison of professional and crowdsourced reference translations for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="7670" citStr="Zbib et al. (2013)" startWordPosition="1141" endWordPosition="1144">h simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing</context>
</contexts>
<marker>Zbib, Markiewicz, Matsoukas, Schwartz, Makhoul, 2013</marker>
<rawString>Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas, Richard Schwartz, and John Makhoul. 2013. Systematic comparison of professional and crowdsourced reference translations for machine translation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Wayne Zhao</author>
<author>Yanwei Guo</author>
<author>Rui Yan</author>
<author>Yulan He</author>
<author>Xiaoming Li</author>
</authors>
<title>Timeline generation with social attention.</title>
<date>2013</date>
<booktitle>In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’13,</booktitle>
<pages>1061--1064</pages>
<marker>Zhao, Guo, Yan, He, Li, 2013</marker>
<rawString>Xin Wayne Zhao, Yanwei Guo, Rui Yan, Yulan He, and Xiaoming Li. 2013. Timeline generation with social attention. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’13, pages 1061–1064.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>