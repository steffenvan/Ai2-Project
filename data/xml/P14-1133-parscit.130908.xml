<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000105">
<title confidence="0.98645">
Semantic Parsing via Paraphrasing
</title>
<author confidence="0.999528">
Jonathan Berant Percy Liang
</author>
<affiliation confidence="0.999652">
Stanford University Stanford University
</affiliation>
<email confidence="0.997994">
joberant@stanford.edu pliang@cs.stanford.edu
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99961152173913">
A central challenge in semantic parsing is
handling the myriad ways in which knowl-
edge base predicates can be expressed.
Traditionally, semantic parsers are trained
primarily from text paired with knowledge
base information. Our goal is to exploit
the much larger amounts of raw text not
tied to any knowledge base. In this pa-
per, we turn semantic parsing on its head.
Given an input utterance, we first use a
simple method to deterministically gener-
ate a set of candidate logical forms with
a canonical realization in natural language
for each. Then, we use a paraphrase model
to choose the realization that best para-
phrases the input, and output the corre-
sponding logical form. We present two
simple paraphrase models, an association
model and a vector space model, and train
them jointly from question-answer pairs.
Our system PARASEMPRE improves state-
of-the-art accuracies on two recently re-
leased question-answering datasets.
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999860785714285">
We consider the semantic parsing problem of map-
ping natural language utterances into logical forms
to be executed on a knowledge base (KB) (Zelle
and Mooney, 1996; Zettlemoyer and Collins,
2005; Wong and Mooney, 2007; Kwiatkowski
et al., 2010). Scaling semantic parsers to large
knowledge bases has attracted substantial atten-
tion recently (Cai and Yates, 2013; Berant et al.,
2013; Kwiatkowski et al., 2013), since it drives
applications such as question answering (QA) and
information extraction (IE).
Semantic parsers need to somehow associate
natural language phrases with logical predicates,
e.g., they must learn that the constructions “What
</bodyText>
<figureCaption confidence="0.9985516">
Figure 1: Semantic parsing via paraphrasing: For each
candidate logical form (in red), we generate canonical utter-
ances (in purple). The model is trained to paraphrase the in-
put utterance (in green) into the canonical utterances associ-
ated with the correct denotation (in blue).
</figureCaption>
<bodyText confidence="0.996761">
does X do for a living?”, “What is X’s profes-
sion?”, and “Who is X?”, should all map to the
logical predicate Profession. To learn these map-
pings, traditional semantic parsers use data which
pairs natural language with the KB. However, this
leaves untapped a vast amount of text not related
to the KB. For instance, the utterances “Where is
ACL in 2014?” and “What is the location ofACL
2014?” cannot be used in traditional semantic
parsing methods, since the KB does not contain
an entity ACL2014, but this pair clearly contains
valuable linguistic information. As another refer-
ence point, out of 500,000 relations extracted by
the ReVerb Open IE system (Fader et al., 2011),
only about 10,000 can be aligned to Freebase (Be-
rant et al., 2013).
In this paper, we present a novel approach for
semantic parsing based on paraphrasing that can
exploit large amounts of text not covered by the
KB (Figure 1). Our approach targets factoid ques-
tions with a modest amount of compositionality.
Given an input utterance, we first use a simple de-
terministic procedure to construct a manageable
set of candidate logical forms (ideally, we would
generate canonical utterances for all possible logi-
cal forms, but this is intractable). Next, we heuris-
</bodyText>
<figure confidence="0.9902422">
paraphrase model
What party did Clay establish?
What political party founded by Henry Clay? ... What event involved the people Henry Clay?
Type.PoliticalParty fl Founder.HenryClay ... Type.Event fl Involved.HenryClay
Whig Party
</figure>
<page confidence="0.937052">
1415
</page>
<note confidence="0.9175355">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1415–1425,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.985934333333333">
Figure 2: The main challenge in semantic parsing is cop-
ing with the mismatch between language and the KB. (a)
Traditionally, semantic parsing maps utterances directly to
logical forms. (b) Kwiatkowski et al. (2013) map the utter-
ance to an underspecified logical form, and perform ontology
matching to handle the mismatch. (c) We approach the prob-
lem in the other direction, generating canonical utterances for
logical forms, and use paraphrase models to handle the mis-
match.
</figureCaption>
<bodyText confidence="0.998716">
tically generate canonical utterances for each log-
ical form based on the text descriptions of predi-
cates from the KB. Finally, we choose the canoni-
cal utterance that best paraphrases the input utter-
ance, and thereby the logical form that generated
it. We use two complementary paraphrase mod-
els: an association model based on aligned phrase
pairs extracted from a monolingual parallel cor-
pus, and a vector space model, which represents
each utterance as a vector and learns a similarity
score between them. The entire system is trained
jointly from question-answer pairs only.
Our work relates to recent lines of research
in semantic parsing and question answering.
Kwiatkowski et al. (2013) first maps utterances to
a domain-independent intermediate logical form,
and then performs ontology matching to produce
the final logical form. In some sense, we ap-
proach the problem from the opposite end, using
an intermediate utterance, which allows us to em-
ploy paraphrasing methods (Figure 2). Fader et
al. (2013) presented a QA system that maps ques-
tions onto simple queries against Open IE extrac-
tions, by learning paraphrases from a large mono-
lingual parallel corpus, and performing a single
paraphrasing step. We adopt the idea of using
paraphrasing for QA, but suggest a more general
paraphrase model and work against a formal KB
(Freebase).
We apply our semantic parser on two datasets:
WEBQUESTIONS (Berant et al., 2013), which
contains 5,810 question-answer pairs with
common questions asked by web users; and
FREE917 (Cai and Yates, 2013), which has
917 questions manually authored by annota-
tors. On WEBQUESTIONS, we obtain a relative
improvement of 12% in accuracy over the
state-of-the-art, and on FREE917 we match the
current best performing system. The source
code of our system PARASEMPRE is released
at http://www-nlp.stanford.edu/
software/sempre/.
</bodyText>
<sectionHeader confidence="0.983988" genericHeader="introduction">
2 Setup
</sectionHeader>
<bodyText confidence="0.94637240625">
Our task is as follows: Given (i) a knowledge
base K, and (ii) a training set of question-answer
pairs {(xi, yi)}ni=1, output a semantic parser that
maps new questions x to answers y via latent log-
ical forms z. Let £ denote a set of entities (e.g.,
BillGates), and let P denote a set of properties
(e.g., PlaceOfBirth). A knowledge base K is a
set of assertions (e1,p,e2) E £ x P x £ (e.g.,
(BillGates, PlaceOfBirth, Seattle)). We use
the Freebase KB (Google, 2013), which has 41M
entities, 19K properties, and 596M assertions.
To query the KB, we use a logical language
called simple A-DCS. In simple A-DCS, an
entity (e.g., Seattle) is a unary predicate
(i.e., a subset of £) denoting a singleton set
containing that entity. A property (which is a
binary predicate) can be joined with a unary
predicate; e.g., Founded.Microsoft denotes
the entities that are Microsoft founders. In
PlaceOfBirth.Seattle F1 Founded.Microsoft,
an intersection operator allows us to denote
the set of Seattle-born Microsoft founders.
A reverse operator reverses the order of ar-
guments: R[PlaceOfBirth].BillGates
denotes Bill Gates’s birthplace (in con-
trast to PlaceOfBirth.Seattle). Lastly,
count(Founded.Microsoft) denotes set cardinal-
ity, in this case, the number of Microsoft founders.
The denotation of a logical form z with respect to
a KB K is given by Qz�K. For a formal description
of simple A-DCS, see Liang (2013) and Berant et
al. (2013).
</bodyText>
<sectionHeader confidence="0.926758" genericHeader="method">
3 Model overview
</sectionHeader>
<bodyText confidence="0.961996166666667">
We now present the general framework for seman-
tic parsing via paraphrasing, including the model
and the learning algorithm. In Sections 4 and 5,
we provide the details of our implementation.
Canonical utterance construction Given an ut-
terance x and the KB, we construct a set of candi-
</bodyText>
<figure confidence="0.9851498">
ontology
matching
underspecified
logical
form
(Kwiatkowski et al. 2013)
paraphrase
canonical
utterance
(this work)
utterance
logical
form
direct
(traditional)
</figure>
<page confidence="0.993065">
1416
</page>
<bodyText confidence="0.999972">
date logical forms ix, and then for each z E ix
generate a small set of canonical natural language
utterances Cz. Our goal at this point is only to gen-
erate a manageable set of logical forms containing
the correct one, and then generate an appropriate
canonical utterance from it. This strategy is feasi-
ble in factoid QA where compositionality is low,
and so the size of ix is limited (Section 4).
Paraphrasing We score the canonical utter-
ances in Cz with respect to the input utterance x
using a paraphrase model, which offers two ad-
vantages. First, the paraphrase model is decoupled
from the KB, so we can train it from large text cor-
pora. Second, natural language utterances often do
not express predicates explicitly, e.g., the question
“What is Italy’s money?” expresses the binary
predicate CurrencyOf with a possessive construc-
tion. Paraphrasing methods are well-suited for
handling such text-to-text gaps. Our framework
accommodates any paraphrasing method, and in
this paper we propose an association model that
learns to associate natural language phrases that
co-occur frequently in a monolingual parallel cor-
pus, combined with a vector space model, which
learns to score the similarity between vector rep-
resentations of natural language utterances (Sec-
tion 5).
Model We define a discriminative log-linear
model that places a probability distribution over
pairs of logical forms and canonical utterances
(c, z), given an utterance x:
</bodyText>
<equation confidence="0.9931245">
pθ (c, z  |x) = �z�∈Z ecxpC{��x,c,z)T9}
xp{φ( z,)&gt;θ} ,
</equation>
<bodyText confidence="0.999696125">
where θ E Rb is the vector of parameters to be
learned, and φ(x, c, z) is a feature vector extracted
from the input utterance x, the canonical utterance
c, and the logical form z. Note that the candidate
set of logical forms ix and canonical utterances
Cx are constructed during the canonical utterance
construction phase.
The model score decomposes into two terms:
</bodyText>
<equation confidence="0.672786">
φ(x, c, z)&gt;θ = φpr(x, c)&gt;θpr + φlf(x,z)&gt;θlf,
</equation>
<bodyText confidence="0.997987333333333">
where the parameters θpr define the paraphrase
model (Section 5), which is based on features ex-
tracted from text only (the input and canonical ut-
terance). The parameters θlf correspond to seman-
tic parsing features based on the logical form and
input utterance, and are briefly described in this
section.
Many existing paraphrase models introduce la-
tent variables to describe the derivation of c from
x, e.g., with transformations (Heilman and Smith,
2010; Stern and Dagan, 2011) or alignments
(Haghighi et al., 2005; Das and Smith, 2009;
Chang et al., 2010). However, we opt for a sim-
pler paraphrase model without latent variables in
the interest of efficiency.
Logical form features The parameters θlf corre-
spond to the following features adopted from Be-
rant et al. (2013). For a logical form z, we extract
the size of its denotation Qz�K. We also add all bi-
nary predicates in z as features. Moreover, we ex-
tract a popularity feature for predicates based on
the number of instances they have in K. For Free-
base entities, we extract a popularity feature based
on the entity frequency in an entity linked subset
of Reverb (Lin et al., 2012). Lastly, Freebase for-
mulas have types (see Section 4), and we conjoin
the type of z with the first word of x, to capture the
correlation between a word (e.g., “where”) with
the Freebase type (e.g., Location).
Learning As our training data consists of
question-answer pairs (xi, yi), we maximize the
log-likelihood of the correct answer. The proba-
bility of an answer y is obtained by marginaliz-
ing over canonical utterances c and logical forms
z whose denotation is y. Formally, our objective
function O(θ) is as follows:
</bodyText>
<equation confidence="0.976506">
n
O(θ) = log pθ(yi  |xi) − λ11θ111,
i=1
pθ(y  |x) = E E pθ(c, z  |x).
z∈Zx:y=QzlK c∈Cz
</equation>
<bodyText confidence="0.9999495">
The strength λ of the L1 regularizer is set based
on cross-validation. We optimize the objective by
initializing the parameters θ to zero and running
AdaGrad (Duchi et al., 2010). We approximate
the set of pairs of logical forms and canonical ut-
terances with a beam of size 2,000.
</bodyText>
<sectionHeader confidence="0.945733" genericHeader="method">
4 Canonical utterance construction
</sectionHeader>
<bodyText confidence="0.999337">
We construct canonical utterances in two steps.
Given an input utterance x, we first construct a
set of logical forms ix, and then generate canon-
ical utterances from each z E ix. Both steps are
performed with a small and simple set of deter-
ministic rules, which suffices for our datasets, as
</bodyText>
<page confidence="0.966798">
1417
</page>
<bodyText confidence="0.993839931372549">
they consist of factoid questions with a modest
amount of compositional structure. We describe
these rules below for completeness. Due to its so-
porific effect though, we advise the reader to skim
it quickly.
Candidate logical forms We consider logical
forms defined by a set of templates, summarized
in Table 1. The basic template is a join of a bi-
nary and an entity, where a binary can either be
one property p.e (#1 in the table) or two proper-
ties p1.p2.e (#2). To handle cases of events in-
volving multiple arguments (e.g., “Who did Brad
Pitt play in Troy?”), we introduce the template
p.(p1.e1 u p2.e2) (#3), where the main event is
modified by more than one entity. Logical forms
can be further modified by a unary “filter”, e.g.,
the answer to “What composers spoke French?”
is a set of composers, i.e., a subset of all people
(#4). Lastly, we handle aggregation formulas for
utterances such as “How many teams are in the
NCAA?” (#5).
To construct candidate logical forms Zx for a
given utterance x, our strategy is to find an en-
tity in x and grow the logical form from that en-
tity. As we show later, this procedure actually pro-
duces a set with better coverage than construct-
ing logical forms recursively from spans of x, as
is done in traditional semantic parsing. Specifi-
cally, for every span of x, we take at most 10 en-
tities whose Freebase descriptions approximately
match the span. Then, we join each entity e with
all type-compatible1 binaries b, and add these log-
ical forms to Zx (#1 and #2).
To construct logical forms with multiple en-
tities (#3) we do the following: For any logical
form z = p.p1.e1, where p1 has type signa-
ture (t1, ∗), we look for other entities e2 that
were matched in x. Then, we add the logical
form p.(p1.e1 u p2.e2), if there exists a binary
p2 with a compatible type signature (t1, t2),
where t2 is one of e2’s types. For example, for
the logical form Character.Actor.BradPitt,
if we match the entity Troy in x, we obtain
Character.(Actor.BradPitt fl Film.Troy).
We further modify logical forms by intersecting
with a unary filter (#4): given a formula z with
some Freebase type (e.g., People), we look at
all Freebase sub-types t (e.g., Composer), and
1Entities in Freebase are associated with a set of types,
and properties have a type signature (t1, t2) We use these
types to compute an expected type t for any logical form z.
check whether one of their Freebase descriptions
(e.g., “composer”) appears in x. If so, we
add the formula Type.t u z to Zx. Finally, we
check whether x is an aggregation formula by
identifying whether it starts with phrases such as
“how many” or “number of” (#5).
On WEBQUESTIONS, this results in 645 for-
mulas per utterance on average. Clearly, we can
increase the expressivity of this step by expand-
ing the template set. For example, we could han-
dle superlative utterances (“What NBA player is
tallest?”) by adding a template with an argmax
operator.
Utterance generation While mapping general
language utterances to logical forms is hard, we
observe that it is much easier to generate a canoni-
cal natural language utterances of our choice given
a logical form. Table 2 summarizes the rules used
to generate canonical utterances from the template
p.e. Questions begin with a question word, are fol-
lowed by the Freebase description of the expected
answer type (d(t)), and followed by Freebase de-
scriptions of the entity (d(e)) and binary (d(p)).
To fill in auxiliary verbs, determiners, and prepo-
sitions, we parse the description d(p) into one of
NP, VP, PP, or NP VP. This determines the gen-
eration rule to be used.
Each Freebase property p has an explicit prop-
erty p&apos; equivalent to the reverse R[p] (e.g.,
ContainedBy and R[Contains]). For each logical
form z, we also generate using equivalent logical
forms where p is replaced with R[p&apos;]. Reversed
formulas have different generation rules, since en-
tities in these formulas are in the subject position
rather than object position.
We generate the description d(t) from the Free-
base description of the type of z (this handles #4).
For the template p1.p2.e (#2), we have a similar
set of rules, which depends on the syntax of d(p1)
and d(p2) and is omitted for brevity. The tem-
plate p.(p1.e1 u p2.e2) (#3) is generated by ap-
pending the prepositional phrase in d(e2), e.g,
“What character is the character of Brad Pitt in
Troy?”. Lastly, we choose the question phrase
“How many” for aggregation formulas (#5), and
“What” for all other formulas.
We also generate canonical utterances using
an alignment lexicon, released by Berant et al.
(2013), which maps text phrases to Freebase bi-
nary predicates. For a binary predicate b mapped
from text phrase d(b), we generate the utterance
</bodyText>
<page confidence="0.966367">
1418
</page>
<table confidence="0.999922285714286">
# Template Example Question
1 p.e Directed.TopGun Who directed Top Gun?
2 p1.p2.e Employment.EmployerOf.SteveBalmer Where does Steve Balmer work?
3 p.(p1.e1 flp2.e2) Character.(Actor.BradPitt fl Film.Troy) Who did Brad Pitt play in Troy?
4 Type.t fl z Type.Composer fl SpeakerOf.French What composers spoke French?
5 count(z) count(BoatDesigner.NatHerreshoff) How many ships were designed by
Nat Herreshoff?
</table>
<tableCaption confidence="0.960358">
Table 1: Logical form templates, where p, p1, p2 are Freebase properties, e, e1, e2 are Freebase entities, t is a Freebase type,
and z is a logical form.
</tableCaption>
<table confidence="0.999931333333333">
d(p) Categ. Rule Example
p.e NP WH d(t) has d(e) as NP ? What election contest has George Bush as winner?
VP WH d(t) (AUX) VP d(e) ? What radio station serves area New-York?
PP WH d(t) PP d(e) ? What beer from region Argentina?
NP VP WH d(t) VP the NP d(e) ? What mass transportation system served the area Berlin?
R(p).e NP WH d(t) is the NP of d(e) ? What location is the place of birth of Elvis Presley?
VP WH d(t) AUX d(e) VP ? Whatfilm is Brazil featured in?
PP WH d(t) d(e) PP ? What destination Spanish steps near travel destination?
NP VP WH NP is VP by d(e) ? What structure is designed by Herod?
</table>
<tableCaption confidence="0.979047666666667">
Table 2: Generation rules for templates of the formp.e and R[p].e based on the syntactic category of the property description.
Freebase descriptions for the type, entity, and property are denoted by d(t), d(e) and d(p) respectively. The surface form of the
auxiliary AUX is determined by the POS tag of the verb inside the VP tree.
</tableCaption>
<bodyText confidence="0.965378">
WH d(t) d(b) d(e) ?. On the WEBQUESTIONS
dataset, we generate an average of 1,423 canonical
utterances c per input utterance x. In Section 6,
we show that an even simpler method of gener-
ating canonical utterances by concatenating Free-
base descriptions hurts accuracy by only a modest
amount.
</bodyText>
<sectionHeader confidence="0.995549" genericHeader="method">
5 Paraphrasing
</sectionHeader>
<bodyText confidence="0.99988915">
Once the candidate set of logical forms paired with
canonical utterances is constructed, our problem
is reduced to scoring pairs (c, z) based on a para-
phrase model. The NLP paraphrase literature is
vast and ranges from simple methods employing
surface features (Wan et al., 2006), through vec-
tor space models (Socher et al., 2011), to latent
variable models (Das and Smith, 2009; Wang and
Manning, 2010; Stern and Dagan, 2011).
In this paper, we focus on two paraphrase mod-
els that emphasize simplicity and efficiency. This
is important since for each question-answer pair,
we consider thousands of canonical utterances as
potential paraphrases. In contrast, traditional para-
phrase detection (Dolan et al., 2004) and Recog-
nizing Textual Entailment (RTE) tasks (Dagan et
al., 2013) consider examples consisting of only a
single pair of candidate paraphrases.
Our paraphrase model decomposes into an as-
sociation model and a vector space model:
</bodyText>
<equation confidence="0.876469666666667">
φpr(x, c)Tθpr = φas(x, c)Tθas + φvs(x, c)Tθvs.
x : What type of music did Richard Wagner play
c : What is the musical genres of Richard Wagner
</equation>
<figureCaption confidence="0.9974245">
Figure 3: Token associations extracted for a paraphrase
pair. Blue and dashed (red and solid) indicate positive (neg-
ative) score. Line width is proportional to the absolute value
of the score.
</figureCaption>
<subsectionHeader confidence="0.985563">
5.1 Association model
</subsectionHeader>
<bodyText confidence="0.999839954545455">
The goal of the association model is to deter-
mine whether x and c contain phrases that are
likely to be paraphrases. Given an utterance x =
(x0, x1, .., xn−1), we denote by xi:j the span from
token i to token j. For each pair of utterances
(x, c), we go through all spans of x and c and
identify a set of pairs of potential paraphrases
(xi:j, ci,:j,), which we call associations. (We will
describe how associations are identified shortly.)
We then define features on each association; the
weighted combination of these features yields a
score. In this light, associations can be viewed
as soft paraphrase rules. Figure 3 presents exam-
ples of associations extracted from a paraphrase
pair and visualizes the learned scores. We can see
that our model learns a positive score for associ-
ating “type” with “genres”, and a negative score
for associating “is” with “play”.
We define associations in x and c primarily by
looking up phrase pairs in a phrase table con-
structed using the PARALEX corpus (Fader et al.,
2013). PARALEX is a large monolingual parallel
</bodyText>
<page confidence="0.943439">
1419
</page>
<table confidence="0.992613">
Category Description
Assoc. lemma(xi:j) ∧ lemma(ci0:j0)
pos(xi:j) ∧ pos(ci0:j0)
lemma(xi:j) = lemma(ci0:j0)?
pos(xi:j) = pos(ci0:j0)?
lemma(xi:j) and lemma(ci0:j0) are synonyms?
lemma(xi:j) and lemma(ci0:j0) are derivations?
Deletions Deleted lemma and POS tag
</table>
<tableCaption confidence="0.756376666666667">
Table 3: Full feature set in the association model. xi:j and
ci0:j0 denote spans from x and c. pos(xi:j) and lemma(xi:j)
denote the POS tag and lemma sequence of xi:j.
</tableCaption>
<bodyText confidence="0.999878571428572">
corpora, containing 18 million pairs of question
paraphrases from wikianswers.com, which
were tagged as having the same meaning by users.
PARALEX is suitable for our needs since it fo-
cuses on question paraphrases. For example, the
phrase “do for a living” occurs mostly in ques-
tions, and we can extract associations for this
phrase from PARALEX. Paraphrase pairs in PAR-
ALEX are word-aligned using standard machine
translation methods. We use the word alignments
to construct a phrase table by applying the con-
sistent phrase pair heuristic (Och and Ney, 2004)
to all 5-grams. This results in a phrase table with
approximately 1.3 million phrase pairs. We let A
denote this set of mined candidate associations.
For a pair (x, c), we also consider as candidate
associations the set B (represented implicitly),
which contains token pairs (xi, cis) such that xi
and cis share the same lemma, the same POS tag,
or are linked through a derivation link on WordNet
(Fellbaum, 1998). This allows us to learn para-
phrases for words that appear in our datasets but
are not covered by the phrase table, and to han-
dle nominalizations for phrase pairs such as “Who
designed the game of life?” and “What game de-
signer is the designer of the game of life?”.
Our model goes over all possible spans of x
and c and constructs all possible associations from
A and B. This results in many poor associations
(e.g., “play” and “the”), but as illustrated in Fig-
ure 3, we learn weights that discriminate good
from bad associations. Table 3 specifies the full
set of features. Note that unlike standard para-
phrase detection and RTE systems, we use lexi-
calized features, firing approximately 400,000 fea-
tures on WEBQUESTIONS. By extracting POS
features, we obtain soft syntactic rules, e.g., the
feature “JJ N ∧ N” indicates that omitting ad-
jectives before nouns is possible. Once associa-
tions are constructed, we mark tokens in x and c
that were not part of any association, and extract
deletion features for their lemmas and POS tags.
Thus, we learn that deleting pronouns is accept-
able, while deleting nouns is not.
To summarize, the association model links
phrases of two utterances in multiple overlapping
ways. During training, the model learns which
associations are characteristic of paraphrases and
which are not.
</bodyText>
<subsectionHeader confidence="0.983785">
5.2 Vector space model
</subsectionHeader>
<bodyText confidence="0.999929684210526">
The association model relies on having a good set
of candidate associations, but mining associations
suffers from coverage issues. We now introduce
a vector space (VS) model, which assigns a vec-
tor representation for each utterance, and learns a
scoring function that ranks paraphrase candidates.
We start by constructing vector representations
of words. We run the WORD2VEC tool (Mikolov et
al., 2013) on lower-cased Wikipedia text (1.59 bil-
lion tokens), using the CBOW model with a win-
dow of 5 and hierarchical softmax. We also ex-
periment with publicly released word embeddings
(Huang et al., 2012), which were trained using
both local and global context. Both result in k-
dimensional vectors (k = 50). Next, we construct
a vector vx E Rk for each utterance x by simply
averaging the vectors of all content words (nouns,
verbs, and adjectives) in x.
We can now estimate a paraphrase score for two
utterances x and c via a weighted combination of
the components of the vector representations:
where W E Rk×k is a parameter matrix. In terms
of our earlier notation, we have θvs = vec(W) and
φvs(x, c) = vec(vxvT c ), where vec(·) unrolls a ma-
trix into a vector. In Section 6, we experiment with
W equal to the identity matrix, constraining W to
be diagonal, and learning a full W matrix.
The VS model can identify correct paraphrases
in cases where it is hard to directly associate
phrases from x and c. For example, the answer
to “Where is made Kia car?” (from WEBQUES-
TIONS), is given by the canonical utterance “What
city is Kia motors a headquarters of?”. The as-
sociation model does not associate “made” and
“headquarters”, but the VS model is able to de-
termine that these utterances are semantically re-
lated. In other cases, the VS model cannot distin-
guish correct paraphrases from incorrect ones. For
</bodyText>
<table confidence="0.759902166666667">
T k wijvx,ivc,j
vx Wvc = i,j=1
1420
Dataset # examples # word types
FREE917 917 2,036
WEBQUESTIONS 5,810 4,525
</table>
<tableCaption confidence="0.999159">
Table 4: Statistics on WEBQUESTIONS and FREE917.
</tableCaption>
<bodyText confidence="0.999760857142857">
example, the association model identifies that the
paraphrase for “What type of music did Richard
Wagner Play?” is “What is the musical genres
of Richard Wagner?”, by relating phrases such as
“type of music” and “musical genres”. The VS
model ranks the canonical utterance “What com-
position has Richard Wagner as lyricist?” higher,
as this utterance is also in the music domain. Thus,
we combine the two models to benefit from their
complementary nature.
In summary, while the association model aligns
particular phrases to one another, the vector space
model provides a soft vector-based representation
for utterances.
</bodyText>
<sectionHeader confidence="0.995849" genericHeader="method">
6 Empirical evaluation
</sectionHeader>
<bodyText confidence="0.9997678">
In this section, we evaluate our system on WE-
BQUESTIONS and FREE917. After describing the
setup (Section 6.1), we present our main empirical
results and analyze the components of the system
(Section 6.2).
</bodyText>
<subsectionHeader confidence="0.991282">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.999906487804878">
We use the WEBQUESTIONS dataset (Berant et
al., 2013), which contains 5,810 question-answer
pairs. This dataset was created by crawling
questions through the Google Suggest API, and
then obtaining answers using Amazon Mechani-
cal Turk. We use the original train-test split, and
divide the training set into 3 random 80%–20%
splits for development. This dataset is character-
ized by questions that are commonly asked on the
web (and are not necessarily grammatical), such
as “What character did Natalie Portman play in
Star Wars?” and “What kind of money to take to
Bahamas?”.
The FREE917 dataset contains 917 questions,
authored by two annotators and annotated with
logical forms. This dataset contains questions on
rarer topics (for example, “What is the engine
in a 2010 Ferrari California?” and “What was
the cover price of the X-men Issue 1?”), but the
phrasing of questions tends to be more rigid com-
pared to WEBQUESTIONS. Table 4 provides some
statistics on the two datasets. Following Cai and
Yates (2013), we hold out 30% of the data for the
final test, and perform 3 random 80%-20% splits
of the training set for development. Since we train
from question-answer pairs, we collect answers by
executing the gold logical forms against Freebase.
We execute A-DCS queries by converting them
into SPARQL and executing them against a copy
of Freebase using the Virtuoso database engine.
We evaluate our system with accuracy, that is, the
proportion of questions we answer correctly. We
run all questions through the Stanford CoreNLP
pipeline (Toutanova and Manning, 2003; Finkel et
al., 2005; Klein and Manning, 2003).
We tuned the Li regularization strength, devel-
oped features, and ran analysis experiments on the
development set (averaging across random splits).
On WEBQUESTIONS, without Li regularization,
the number of non-zero features was 360K; Li
regularization brings it down to 17K.
</bodyText>
<subsectionHeader confidence="0.763989">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999836533333333">
We compare our system to Cai and Yates (2013)
(CY13), Berant et al. (2013) (BCFL13), and
Kwiatkowski et al. (2013) (KCAZ13). For
BCFL13, we obtained results using the SEMPRE
package2 and running Berant et al. (2013)’s sys-
tem on the datasets.
Table 5 presents results on the test set. We
achieve a substantial relative improvement of 12%
in accuracy on WEBQUESTIONS, and match the
best results on FREE917. Interestingly, our system
gets an oracle accuracy of 63% on WEBQUES-
TIONS compared to 48% obtained by BCFL13,
where the oracle accuracy is the fraction of ques-
tions for which at least one logical form in the
candidate set produced by the system is correct.
This demonstrates that our method for construct-
ing candidate logical forms is reasonable. To fur-
ther examine this, we ran BCFL13 on the devel-
opment set, allowing it to use only predicates from
logical forms suggested by our logical form con-
struction step. This improved oracle accuracy on
the development set to 64.5%, but accuracy was
32.2%. This shows that the improvement in accu-
racy should not be attributed only to better logical
form generation, but also to the paraphrase model.
We now perform more extensive analysis of our
system’s components and compare it to various
baselines.
Component ablation We ablate the association
model, the VS model, and the entire paraphrase
</bodyText>
<footnote confidence="0.974125">
2http://www-nlp.stanford.edu/software/sempre/
</footnote>
<page confidence="0.95151">
1421
</page>
<figure confidence="0.3747988">
FREE917 WEBQUESTIONS
CY13 59.0
BCFL13
KCAZ13
This work
</figure>
<tableCaption confidence="0.928335">
Table 5: Results on the test set.
</tableCaption>
<table confidence="0.977746583333333">
FREE917 WEBQUESTIONS
Our system 73.9 41.2
–VSM 71.0 40.5
–ASSOCIATION 52.7 35.3
–PARAPHRASE 31.8 21.3
SIMPLEGEN 73.4 40.4
Full matrix 52.7 35.3
Diagonal 50.4 30.6
Identity 50.7 30.4
JACCARD 69.7 31.3
EDIT 40.8 24.8
WDDC06 71.0 29.8
</table>
<tableCaption confidence="0.9926165">
Table 6: Results for ablations and baselines on develop-
ment set.
</tableCaption>
<bodyText confidence="0.976383">
model (using only logical form features). Table 5
shows that our full system obtains highest accu-
racy, and that removing the association model re-
sults in a much larger degradation compared to re-
moving the VS model.
Utterance generation Our system generates
relatively natural utterances from logical forms us-
ing simple rules based on Freebase descriptions
(Section 4). We now consider simply concate-
nating Freebase descriptions. For example, the
logical form R[PlaceOfBirth].ElvisPresley
would generate the utterance “What location Elvis
Presley place of birth?”. Row SIMPLEGEN in Ta-
ble 6 demonstrates that we still get good results in
this setup. This is expected given that our para-
phrase models are not sensitive to the syntactic
structure of the generated utterance.
VS model Our system learns parameters for a
full W matrix. We now examine results when
learning parameters for a full matrix W, a diago-
nal matrix W, and when setting W to be the iden-
tity matrix. Table 6 (third section) illustrates that
learning a full matrix substantially improves accu-
racy. Figure 4 gives an example for a correct para-
phrase pair, where the full matrix model boosts
the overall model score. Note that the full ma-
trix assigns a high score for the phrases “official
language” and “speak” compared to the simpler
models, but other pairs are less interpretable.
Baselines We also compared our system to the
following implemented baselines:
Diagonal do people czech republic speak
</bodyText>
<subsubsectionHeader confidence="0.645579">
Identity do people czech republic speak
</subsubsectionHeader>
<figureCaption confidence="0.9537744">
Figure 4: Values of the paraphrase score v iWvci, for all
content word tokens xi and cii, where W is an arbitrary full
matrix, a diagonal matrix, or the identity matrix. We omit
scores for the words “czech” and “republic” since they ap-
pear in all canonical utterances for this example.
</figureCaption>
<listItem confidence="0.9856958">
• JACCARD: We compute the Jaccard score
between the tokens of x and c and define
Opr(x, c) to be this single feature.
• EDIT: We compute the token edit distance
between x and c and define Opr(x, c) to be
this single feature.
• WDDC06: We re-implement 13 features
from Wan et al. (2006), who obtained close to
state-of-the-art performance on the Microsoft
Research paraphrase corpus.3
</listItem>
<bodyText confidence="0.997851375">
Table 6 demonstrates that we improve perfor-
mance over all baselines. Interestingly, JACCARD
and WDDC06 obtain reasonable performance
on FREE917 but perform much worse on WE-
BQUESTIONS. We surmise this is because ques-
tions in FREE917 were generated by annotators
prompted by Freebase facts, whereas questions
in WEBQUESTIONS originated independently of
Freebase. Thus, word choice in FREE917 is of-
ten close to the generated Freebase descriptions,
allowing simple baselines to perform well.
Error analysis We sampled examples from the
development set to examine the main reasons
PARASEMPRE makes errors. We notice that in
many cases the paraphrase model can be further
improved. For example, PARASEMPRE suggests
</bodyText>
<footnote confidence="0.9565645">
3We implement all features that do not require depen-
dency parsing.
</footnote>
<figure confidence="0.984188959183674">
Full do people czech republic speak
republic -8.71 12.47 -10.75
ofacal
8.09
7.81
2.58
-3.13
16.55
15.34 21.62 24.44
14.74
2.76
language
czech
0.7
3.86
0.67
5.21
8.13
6.72
9.69
ofacal 2.31
-0.72
1.88 0.27
-0.49
language 0.27 4.72
11.51 12.33 11
republic
-0.16
czech 1.4
ofacal 2.26
-1.41
0.89 0.07
-0.58
language 0.62 4.19
11.91 10.78 12.7
republic
-1.82
7.31
4.34
czech 2.88
5.42
9.44
62.0
68.0
68.5
–
35.7
–
39.9
</figure>
<page confidence="0.989937">
1422
</page>
<bodyText confidence="0.999601944444444">
that the best paraphrase for “What company did
Henry Ford work for?” is “What written work
novel by Henry Ford?” rather than “The em-
ployer of Henry Ford”, due to the exact match
of the word “work”. Another example is the
question “Where is the Nascar hall of fame?”,
where PARASEMPRE suggests that “What hall of
fame discipline has Nascar hall of fame as halls
of fame?” is the best canonical utterance. This
is because our simple model allows to associate
“hall offame” with the canonical utterance three
times. Entity recognition also accounts for many
errors, e.g., the entity chosen in “where was the
gallipoli campaign waged?” is Galipoli and not
GalipoliCampaign. Last, PARASEMPRE does not
handle temporal information, which causes errors
in questions like “Where did Harriet Tubman live
after the civil war?”
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999897630769231">
In this work, we approach the problem of seman-
tic parsing from a paraphrasing viewpoint. A
fundamental motivation and long standing goal
of the paraphrasing and RTE communities has
been to cast various semantic applications as para-
phrasing/textual entailment (Dagan et al., 2013).
While it has been shown that paraphrasing meth-
ods are useful for question answering (Harabagiu
and Hickl, 2006) and relation extraction (Romano
et al., 2006), this is, to the best of our knowledge,
the first paper to perform semantic parsing through
paraphrasing. Our paraphrase model emphasizes
simplicity and efficiency, but the framework is ag-
nostic to the internals of the paraphrase method.
On the semantic parsing side, our work is most
related to Kwiatkowski et al. (2013). The main
challenge in semantic parsing is coping with the
mismatch between language and the KB. In both
Kwiatkowski et al. (2013) and this work, an inter-
mediate representation is employed to handle the
mismatch, but while they use a logical represen-
tation, we opt for a text-based one. Our choice
allows us to benefit from the parallel monolingual
corpus PARALEX and from word vectors trained
on Wikipedia. We believe that our approach is
particularly suitable for scenarios such as factoid
question answering, where the space of logical
forms is somewhat constrained and a few gener-
ation rules suffice to reduce the problem to para-
phrasing.
Our work is also related to Fader et al. (2013),
who presented a paraphrase-driven question an-
swering system. One can view this work as a
generalization of Fader et al. along three dimen-
sions. First, Fader et al. use a KB over natu-
ral language extractions rather than a formal KB
and so querying the KB does not require a gener-
ation step – they paraphrase questions to KB en-
tries directly. Second, they suggest a particular
paraphrasing method that maps a test question to a
question for which the answer is already known in
a single step. We propose a general paraphrasing
framework and instantiate it with two paraphrase
models. Lastly, Fader et al. handle queries with
only one property and entity whereas we general-
ize to more types of logical forms.
Since our generated questions are passed to
a paraphrase model, we took a very simple ap-
proach, mostly ensuring that we preserved the se-
mantics of the utterance without striving for the
most fluent realization. Research on generation
(Dale et al., 2003; Reiter et al., 2005; Turner et
al., 2009; Piwek and Boyer, 2012) typically fo-
cuses on generating natural utterances for human
consumption, where fluency is important.
In conclusion, the main contribution of this pa-
per is a novel approach for semantic parsing based
on a simple generation procedure and a paraphrase
model. We achieve state-of-the-art results on two
recently released datasets. We believe that our ap-
proach opens a window of opportunity for learn-
ing semantic parsers from raw text not necessarily
related to the target KB. With more sophisticated
generation and paraphrase, we hope to tackle com-
positionally richer utterances.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999695">
We thank Kai Sheng Tai for performing the er-
ror analysis. Stanford University gratefully ac-
knowledges the support of the Defense Advanced
Research Projects Agency (DARPA) Deep Ex-
ploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL)
contract no. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government. The second author
is supported by a Google Faculty Research Award.
</bodyText>
<page confidence="0.973875">
1423
</page>
<sectionHeader confidence="0.989557" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873943396226">
J. Berant, A. Chou, R. Frostig, and P. Liang. 2013.
Semantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).
Q. Cai and A. Yates. 2013. Large-scale semantic pars-
ing via schema matching and lexicon extension. In
Association for Computational Linguistics (ACL).
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained la-
tent representations. In North American Association
for Computational Linguistics (NAACL).
I. Dagan, D. Roth, M. Sammons, and F. M. Zanzotto.
2013. Recognizing Textual Entailment: Models and
Applications. Morgan and Claypool Publishers.
R. Dale, S. Geldof, and J. Prost. 2003. Coral: using
natural language generation for navigational assis-
tance. In Australasian computer science conference,
pages 35–44.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition.
In Association for Computational Linguistics (ACL),
pages 468–476.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Inter-
national Conference on Computational Linguistics
(COLING).
J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In Conference on Learning Theory
(COLT).
A. Fader, S. Soderland, and O. Etzioni. 2011. Identi-
fying relations for open information extraction. In
Empirical Methods in Natural Language Processing
(EMNLP).
A. Fader, L. Zettlemoyer, and O. Etzioni. 2013.
Paraphrase-driven learning for open question an-
swering. In Association for Computational Linguis-
tics (ACL).
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Associ-
ation for Computational Linguistics (ACL), pages
363–370.
Google. 2013. Freebase data dumps (2013-06-
09). https://developers.google.com/
freebase/data.
A. Haghighi, A. Y. Ng, and C. D. Manning. 2005.
Robust textual inference via graph matching. In
Empirical Methods in Natural Language Processing
(EMNLP).
S. Harabagiu and A. Hickl. 2006. Methods for using
textual entailment in open-domain question answer-
ing. In Association for Computational Linguistics
(ACL).
M. Heilman and N. A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technology and North American Association
for Computational Linguistics (HLT/NAACL), pages
1011–1019.
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving word representations via global
context and multiple word prototypes. In Associa-
tion for Computational Linguistics (ACL).
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Association for Computational Lin-
guistics (ACL), pages 423–430.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order uni-
fication. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1223–1233.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).
P. Liang. 2013. Lambda dependency-based composi-
tional semantics. Technical report, ArXiv.
T. Lin, Mausam, and O. Etzioni. 2012. Entity linking
at web scale. In Knowledge Extraction Workshop
(AKBC-WEKEX).
T. Mikolov, K. Chen, G. Corrado, and Jeffrey. 2013.
Efficient estimation of word representations in vec-
tor space. Technical report, ArXiv.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30:417–449.
P. Piwek and K. E. Boyer. 2012. Varieties of question
generation: Introduction to this special issue. Dia-
logue and Discourse, 3:1–9.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137–
169.
L. Romano, M. kouylekov, I. Szpektor, I. Dagan,
and A. Lavelli. 2006. Investigating a generic
paraphrase-based approach for relation extraction.
In Proceedings of ECAL.
R. Socher, E. H. Huang, J. Pennin, C. D. Manning, and
A. Ng. 2011. Dynamic pooling and unfolding re-
cursive autoencoders for paraphrase detection. In
Advances in Neural Information Processing Systems
(NIPS), pages 801–809.
</reference>
<page confidence="0.882887">
1424
</page>
<reference confidence="0.999324028571429">
A. Stern and I. Dagan. 2011. A confidence model
for syntactically-motivated entailment proofs. In
Recent Advances in Natural Language Processing,
pages 455–462.
K. Toutanova and C. D. Manning. 2003. Feature-
rich part-of-speech tagging with a cyclic depen-
dency network. In Human Language Technology
and North American Association for Computational
Linguistics (HLT/NAACL).
R. Turner, Y. Sripada, and E. Reiter. 2009. Generating
approximate geographic descriptions. In European
Workshop on Natural Language Generation, pages
42–49.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using
dependency-based features to take the “para-farce”
out of paraphrase. In Australasian Language Tech-
nology Workshop.
M. Wang and C. D. Manning. 2010. Probabilistic tree-
edit models with structured latent variables for tex-
tual entailment and question answering. In The In-
ternational Conference on Computational Linguis-
tics, pages 1164–1172.
Y. W. Wong and R. J. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Association for Computational
Linguistics (ACL), pages 960–967.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming.
In Association for the Advancement of Artificial In-
telligence (AAAI), pages 1050–1055.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Un-
certainty in Artificial Intelligence (UAI), pages 658–
666.
</reference>
<page confidence="0.992391">
1425
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984189">
<title confidence="0.999822">Semantic Parsing via Paraphrasing</title>
<author confidence="0.999437">Jonathan Berant Percy Liang</author>
<affiliation confidence="0.999973">Stanford University Stanford University</affiliation>
<email confidence="0.999122">joberant@stanford.edupliang@cs.stanford.edu</email>
<abstract confidence="0.999404041666667">A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this paper, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best paraphrases the input, and output the corresponding logical form. We present two paraphrase models, an and a space and train them jointly from question-answer pairs. system stateof-the-art accuracies on two recently released question-answering datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>A Chou</author>
<author>R Frostig</author>
<author>P Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1483" citStr="Berant et al., 2013" startWordPosition="222" endWordPosition="225">rase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof-the-art accuracies on two recently released question-answering datasets. 1 Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). does X do for a living?”, “What is X’s profession?”, a</context>
<context position="2772" citStr="Berant et al., 2013" startWordPosition="430" endWordPosition="434">o learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instance, the utterances “Where is ACL in 2014?” and “What is the location ofACL 2014?” cannot be used in traditional semantic parsing methods, since the KB does not contain an entity ACL2014, but this pair clearly contains valuable linguistic information. As another reference point, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013). In this paper, we present a novel approach for semantic parsing based on paraphrasing that can exploit large amounts of text not covered by the KB (Figure 1). Our approach targets factoid questions with a modest amount of compositionality. Given an input utterance, we first use a simple deterministic procedure to construct a manageable set of candidate logical forms (ideally, we would generate canonical utterances for all possible logical forms, but this is intractable). Next, we heurisparaphrase model What party did Clay establish? What political party founded by Henry Clay? ... What event </context>
<context position="5594" citStr="Berant et al., 2013" startWordPosition="873" endWordPosition="876"> produce the final logical form. In some sense, we approach the problem from the opposite end, using an intermediate utterance, which allows us to employ paraphrasing methods (Figure 2). Fader et al. (2013) presented a QA system that maps questions onto simple queries against Open IE extractions, by learning paraphrases from a large monolingual parallel corpus, and performing a single paraphrasing step. We adopt the idea of using paraphrasing for QA, but suggest a more general paraphrase model and work against a formal KB (Freebase). We apply our semantic parser on two datasets: WEBQUESTIONS (Berant et al., 2013), which contains 5,810 question-answer pairs with common questions asked by web users; and FREE917 (Cai and Yates, 2013), which has 917 questions manually authored by annotators. On WEBQUESTIONS, we obtain a relative improvement of 12% in accuracy over the state-of-the-art, and on FREE917 we match the current best performing system. The source code of our system PARASEMPRE is released at http://www-nlp.stanford.edu/ software/sempre/. 2 Setup Our task is as follows: Given (i) a knowledge base K, and (ii) a training set of question-answer pairs {(xi, yi)}ni=1, output a semantic parser that maps </context>
<context position="7469" citStr="Berant et al. (2013)" startWordPosition="1172" endWordPosition="1175">., Founded.Microsoft denotes the entities that are Microsoft founders. In PlaceOfBirth.Seattle F1 Founded.Microsoft, an intersection operator allows us to denote the set of Seattle-born Microsoft founders. A reverse operator reverses the order of arguments: R[PlaceOfBirth].BillGates denotes Bill Gates’s birthplace (in contrast to PlaceOfBirth.Seattle). Lastly, count(Founded.Microsoft) denotes set cardinality, in this case, the number of Microsoft founders. The denotation of a logical form z with respect to a KB K is given by Qz�K. For a formal description of simple A-DCS, see Liang (2013) and Berant et al. (2013). 3 Model overview We now present the general framework for semantic parsing via paraphrasing, including the model and the learning algorithm. In Sections 4 and 5, we provide the details of our implementation. Canonical utterance construction Given an utterance x and the KB, we construct a set of candiontology matching underspecified logical form (Kwiatkowski et al. 2013) paraphrase canonical utterance (this work) utterance logical form direct (traditional) 1416 date logical forms ix, and then for each z E ix generate a small set of canonical natural language utterances Cz. Our goal at this po</context>
<context position="10619" citStr="Berant et al. (2013)" startWordPosition="1682" endWordPosition="1686">. The parameters θlf correspond to semantic parsing features based on the logical form and input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation Qz�K. We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 2012). Lastly, Freebase formulas have types (see Section 4), and we conjoin the type of z with the first word of x, to capture the correlation between a word (e.g., “where”) with the Freebase type (e.g., Location). Learning As our training</context>
<context position="16768" citStr="Berant et al. (2013)" startWordPosition="2757" endWordPosition="2760"> position. We generate the description d(t) from the Freebase description of the type of z (this handles #4). For the template p1.p2.e (#2), we have a similar set of rules, which depends on the syntax of d(p1) and d(p2) and is omitted for brevity. The template p.(p1.e1 u p2.e2) (#3) is generated by appending the prepositional phrase in d(e2), e.g, “What character is the character of Brad Pitt in Troy?”. Lastly, we choose the question phrase “How many” for aggregation formulas (#5), and “What” for all other formulas. We also generate canonical utterances using an alignment lexicon, released by Berant et al. (2013), which maps text phrases to Freebase binary predicates. For a binary predicate b mapped from text phrase d(b), we generate the utterance 1418 # Template Example Question 1 p.e Directed.TopGun Who directed Top Gun? 2 p1.p2.e Employment.EmployerOf.SteveBalmer Where does Steve Balmer work? 3 p.(p1.e1 flp2.e2) Character.(Actor.BradPitt fl Film.Troy) Who did Brad Pitt play in Troy? 4 Type.t fl z Type.Composer fl SpeakerOf.French What composers spoke French? 5 count(z) count(BoatDesigner.NatHerreshoff) How many ships were designed by Nat Herreshoff? Table 1: Logical form templates, where p, p1, p2 </context>
<context position="26699" citStr="Berant et al., 2013" startWordPosition="4422" endWordPosition="4425">has Richard Wagner as lyricist?” higher, as this utterance is also in the music domain. Thus, we combine the two models to benefit from their complementary nature. In summary, while the association model aligns particular phrases to one another, the vector space model provides a soft vector-based representation for utterances. 6 Empirical evaluation In this section, we evaluate our system on WEBQUESTIONS and FREE917. After describing the setup (Section 6.1), we present our main empirical results and analyze the components of the system (Section 6.2). 6.1 Setup We use the WEBQUESTIONS dataset (Berant et al., 2013), which contains 5,810 question-answer pairs. This dataset was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. We use the original train-test split, and divide the training set into 3 random 80%–20% splits for development. This dataset is characterized by questions that are commonly asked on the web (and are not necessarily grammatical), such as “What character did Natalie Portman play in Star Wars?” and “What kind of money to take to Bahamas?”. The FREE917 dataset contains 917 questions, authored by two annotators and anno</context>
<context position="28625" citStr="Berant et al. (2013)" startWordPosition="4731" endWordPosition="4734">e Virtuoso database engine. We evaluate our system with accuracy, that is, the proportion of questions we answer correctly. We run all questions through the Stanford CoreNLP pipeline (Toutanova and Manning, 2003; Finkel et al., 2005; Klein and Manning, 2003). We tuned the Li regularization strength, developed features, and ran analysis experiments on the development set (averaging across random splits). On WEBQUESTIONS, without Li regularization, the number of non-zero features was 360K; Li regularization brings it down to 17K. 6.2 Results We compare our system to Cai and Yates (2013) (CY13), Berant et al. (2013) (BCFL13), and Kwiatkowski et al. (2013) (KCAZ13). For BCFL13, we obtained results using the SEMPRE package2 and running Berant et al. (2013)’s system on the datasets. Table 5 presents results on the test set. We achieve a substantial relative improvement of 12% in accuracy on WEBQUESTIONS, and match the best results on FREE917. Interestingly, our system gets an oracle accuracy of 63% on WEBQUESTIONS compared to 48% obtained by BCFL13, where the oracle accuracy is the fraction of questions for which at least one logical form in the candidate set produced by the system is correct. This demonstr</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Cai</author>
<author>A Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1462" citStr="Cai and Yates, 2013" startWordPosition="218" endWordPosition="221">ent two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof-the-art accuracies on two recently released question-answering datasets. 1 Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). does X do for a living?”, “What i</context>
<context position="5714" citStr="Cai and Yates, 2013" startWordPosition="891" endWordPosition="894">erance, which allows us to employ paraphrasing methods (Figure 2). Fader et al. (2013) presented a QA system that maps questions onto simple queries against Open IE extractions, by learning paraphrases from a large monolingual parallel corpus, and performing a single paraphrasing step. We adopt the idea of using paraphrasing for QA, but suggest a more general paraphrase model and work against a formal KB (Freebase). We apply our semantic parser on two datasets: WEBQUESTIONS (Berant et al., 2013), which contains 5,810 question-answer pairs with common questions asked by web users; and FREE917 (Cai and Yates, 2013), which has 917 questions manually authored by annotators. On WEBQUESTIONS, we obtain a relative improvement of 12% in accuracy over the state-of-the-art, and on FREE917 we match the current best performing system. The source code of our system PARASEMPRE is released at http://www-nlp.stanford.edu/ software/sempre/. 2 Setup Our task is as follows: Given (i) a knowledge base K, and (ii) a training set of question-answer pairs {(xi, yi)}ni=1, output a semantic parser that maps new questions x to answers y via latent logical forms z. Let £ denote a set of entities (e.g., BillGates), and let P den</context>
<context position="27656" citStr="Cai and Yates (2013)" startWordPosition="4577" endWordPosition="4580">that are commonly asked on the web (and are not necessarily grammatical), such as “What character did Natalie Portman play in Star Wars?” and “What kind of money to take to Bahamas?”. The FREE917 dataset contains 917 questions, authored by two annotators and annotated with logical forms. This dataset contains questions on rarer topics (for example, “What is the engine in a 2010 Ferrari California?” and “What was the cover price of the X-men Issue 1?”), but the phrasing of questions tends to be more rigid compared to WEBQUESTIONS. Table 4 provides some statistics on the two datasets. Following Cai and Yates (2013), we hold out 30% of the data for the final test, and perform 3 random 80%-20% splits of the training set for development. Since we train from question-answer pairs, we collect answers by executing the gold logical forms against Freebase. We execute A-DCS queries by converting them into SPARQL and executing them against a copy of Freebase using the Virtuoso database engine. We evaluate our system with accuracy, that is, the proportion of questions we answer correctly. We run all questions through the Stanford CoreNLP pipeline (Toutanova and Manning, 2003; Finkel et al., 2005; Klein and Manning</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Q. Cai and A. Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>D Goldwasser</author>
<author>D Roth</author>
<author>V Srikumar</author>
</authors>
<title>Discriminative learning over constrained latent representations.</title>
<date>2010</date>
<booktitle>In North American Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="10403" citStr="Chang et al., 2010" startWordPosition="1647" endWordPosition="1650"> into two terms: φ(x, c, z)&gt;θ = φpr(x, c)&gt;θpr + φlf(x,z)&gt;θlf, where the parameters θpr define the paraphrase model (Section 5), which is based on features extracted from text only (the input and canonical utterance). The parameters θlf correspond to semantic parsing features based on the logical form and input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation Qz�K. We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 2012). Lastly, Freebase</context>
</contexts>
<marker>Chang, Goldwasser, Roth, Srikumar, 2010</marker>
<rawString>M. Chang, D. Goldwasser, D. Roth, and V. Srikumar. 2010. Discriminative learning over constrained latent representations. In North American Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>D Roth</author>
<author>M Sammons</author>
<author>F M Zanzotto</author>
</authors>
<title>Recognizing Textual Entailment: Models and Applications.</title>
<date>2013</date>
<publisher>Morgan and Claypool Publishers.</publisher>
<contexts>
<context position="19498" citStr="Dagan et al., 2013" startWordPosition="3217" endWordPosition="3220">el. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model decomposes into an association model and a vector space model: φpr(x, c)Tθpr = φas(x, c)Tθas + φvs(x, c)Tθvs. x : What type of music did Richard Wagner play c : What is the musical genres of Richard Wagner Figure 3: Token associations extracted for a paraphrase pair. Blue and dashed (red and solid) indicate positive (negative) score. Line width is proportional to the absolute value of the score. 5.1 Association model The goal of the association model is to determine whether x and c contain phrase</context>
<context position="34808" citStr="Dagan et al., 2013" startWordPosition="5737" endWordPosition="5740"> utterance three times. Entity recognition also accounts for many errors, e.g., the entity chosen in “where was the gallipoli campaign waged?” is Galipoli and not GalipoliCampaign. Last, PARASEMPRE does not handle temporal information, which causes errors in questions like “Where did Harriet Tubman live after the civil war?” 7 Discussion In this work, we approach the problem of semantic parsing from a paraphrasing viewpoint. A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment (Dagan et al., 2013). While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al., 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. Our paraphrase model emphasizes simplicity and efficiency, but the framework is agnostic to the internals of the paraphrase method. On the semantic parsing side, our work is most related to Kwiatkowski et al. (2013). The main challenge in semantic parsing is coping with the mismatch between language and the KB. In both Kwiatkowski e</context>
</contexts>
<marker>Dagan, Roth, Sammons, Zanzotto, 2013</marker>
<rawString>I. Dagan, D. Roth, M. Sammons, and F. M. Zanzotto. 2013. Recognizing Textual Entailment: Models and Applications. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>S Geldof</author>
<author>J Prost</author>
</authors>
<title>Coral: using natural language generation for navigational assistance.</title>
<date>2003</date>
<booktitle>In Australasian computer science conference,</booktitle>
<pages>35--44</pages>
<contexts>
<context position="36940" citStr="Dale et al., 2003" startWordPosition="6095" endWordPosition="6098">Second, they suggest a particular paraphrasing method that maps a test question to a question for which the answer is already known in a single step. We propose a general paraphrasing framework and instantiate it with two paraphrase models. Lastly, Fader et al. handle queries with only one property and entity whereas we generalize to more types of logical forms. Since our generated questions are passed to a paraphrase model, we took a very simple approach, mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization. Research on generation (Dale et al., 2003; Reiter et al., 2005; Turner et al., 2009; Piwek and Boyer, 2012) typically focuses on generating natural utterances for human consumption, where fluency is important. In conclusion, the main contribution of this paper is a novel approach for semantic parsing based on a simple generation procedure and a paraphrase model. We achieve state-of-the-art results on two recently released datasets. We believe that our approach opens a window of opportunity for learning semantic parsers from raw text not necessarily related to the target KB. With more sophisticated generation and paraphrase, we hope t</context>
</contexts>
<marker>Dale, Geldof, Prost, 2003</marker>
<rawString>R. Dale, S. Geldof, and J. Prost. 2003. Coral: using natural language generation for navigational assistance. In Australasian computer science conference, pages 35–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>468--476</pages>
<contexts>
<context position="10382" citStr="Das and Smith, 2009" startWordPosition="1643" endWordPosition="1646">odel score decomposes into two terms: φ(x, c, z)&gt;θ = φpr(x, c)&gt;θpr + φlf(x,z)&gt;θlf, where the parameters θpr define the paraphrase model (Section 5), which is based on features extracted from text only (the input and canonical utterance). The parameters θlf correspond to semantic parsing features based on the logical form and input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation Qz�K. We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 20</context>
<context position="19096" citStr="Das and Smith, 2009" startWordPosition="3156" endWordPosition="3159">verage of 1,423 canonical utterances c per input utterance x. In Section 6, we show that an even simpler method of generating canonical utterances by concatenating Freebase descriptions hurts accuracy by only a modest amount. 5 Paraphrasing Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model decomposes into an association model and a vector space model: φpr(x, c)Tθpr = φas(x, c)Tθas + φvs(</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>D. Das and N. A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Association for Computational Linguistics (ACL), pages 468–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="19430" citStr="Dolan et al., 2004" startWordPosition="3206" endWordPosition="3209">problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model decomposes into an association model and a vector space model: φpr(x, c)Tθpr = φas(x, c)Tθas + φvs(x, c)Tθvs. x : What type of music did Richard Wagner play c : What is the musical genres of Richard Wagner Figure 3: Token associations extracted for a paraphrase pair. Blue and dashed (red and solid) indicate positive (negative) score. Line width is proportional to the absolute value of the score. 5.1 Association model The goal of </context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>B. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2010</date>
<booktitle>In Conference on Learning Theory (COLT).</booktitle>
<contexts>
<context position="11769" citStr="Duchi et al., 2010" startWordPosition="1888" endWordPosition="1891">e”) with the Freebase type (e.g., Location). Learning As our training data consists of question-answer pairs (xi, yi), we maximize the log-likelihood of the correct answer. The probability of an answer y is obtained by marginalizing over canonical utterances c and logical forms z whose denotation is y. Formally, our objective function O(θ) is as follows: n O(θ) = log pθ(yi |xi) − λ11θ111, i=1 pθ(y |x) = E E pθ(c, z |x). z∈Zx:y=QzlK c∈Cz The strength λ of the L1 regularizer is set based on cross-validation. We optimize the objective by initializing the parameters θ to zero and running AdaGrad (Duchi et al., 2010). We approximate the set of pairs of logical forms and canonical utterances with a beam of size 2,000. 4 Canonical utterance construction We construct canonical utterances in two steps. Given an input utterance x, we first construct a set of logical forms ix, and then generate canonical utterances from each z E ix. Both steps are performed with a small and simple set of deterministic rules, which suffices for our datasets, as 1417 they consist of factoid questions with a modest amount of compositional structure. We describe these rules below for completeness. Due to its soporific effect though</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2010</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive subgradient methods for online learning and stochastic optimization. In Conference on Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2704" citStr="Fader et al., 2011" startWordPosition="418" endWordPosition="421"> “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instance, the utterances “Where is ACL in 2014?” and “What is the location ofACL 2014?” cannot be used in traditional semantic parsing methods, since the KB does not contain an entity ACL2014, but this pair clearly contains valuable linguistic information. As another reference point, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013). In this paper, we present a novel approach for semantic parsing based on paraphrasing that can exploit large amounts of text not covered by the KB (Figure 1). Our approach targets factoid questions with a modest amount of compositionality. Given an input utterance, we first use a simple deterministic procedure to construct a manageable set of candidate logical forms (ideally, we would generate canonical utterances for all possible logical forms, but this is intractable). Next, we heurisparaphrase model What party did Clay es</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying relations for open information extraction. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>L Zettlemoyer</author>
<author>O Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5180" citStr="Fader et al. (2013)" startWordPosition="805" endWordPosition="808">s, and a vector space model, which represents each utterance as a vector and learns a similarity score between them. The entire system is trained jointly from question-answer pairs only. Our work relates to recent lines of research in semantic parsing and question answering. Kwiatkowski et al. (2013) first maps utterances to a domain-independent intermediate logical form, and then performs ontology matching to produce the final logical form. In some sense, we approach the problem from the opposite end, using an intermediate utterance, which allows us to employ paraphrasing methods (Figure 2). Fader et al. (2013) presented a QA system that maps questions onto simple queries against Open IE extractions, by learning paraphrases from a large monolingual parallel corpus, and performing a single paraphrasing step. We adopt the idea of using paraphrasing for QA, but suggest a more general paraphrase model and work against a formal KB (Freebase). We apply our semantic parser on two datasets: WEBQUESTIONS (Berant et al., 2013), which contains 5,810 question-answer pairs with common questions asked by web users; and FREE917 (Cai and Yates, 2013), which has 917 questions manually authored by annotators. On WEBQ</context>
<context position="21030" citStr="Fader et al., 2013" startWordPosition="3482" endWordPosition="3485">ribe how associations are identified shortly.) We then define features on each association; the weighted combination of these features yields a score. In this light, associations can be viewed as soft paraphrase rules. Figure 3 presents examples of associations extracted from a paraphrase pair and visualizes the learned scores. We can see that our model learns a positive score for associating “type” with “genres”, and a negative score for associating “is” with “play”. We define associations in x and c primarily by looking up phrase pairs in a phrase table constructed using the PARALEX corpus (Fader et al., 2013). PARALEX is a large monolingual parallel 1419 Category Description Assoc. lemma(xi:j) ∧ lemma(ci0:j0) pos(xi:j) ∧ pos(ci0:j0) lemma(xi:j) = lemma(ci0:j0)? pos(xi:j) = pos(ci0:j0)? lemma(xi:j) and lemma(ci0:j0) are synonyms? lemma(xi:j) and lemma(ci0:j0) are derivations? Deletions Deleted lemma and POS tag Table 3: Full feature set in the association model. xi:j and ci0:j0 denote spans from x and c. pos(xi:j) and lemma(xi:j) denote the POS tag and lemma sequence of xi:j. corpora, containing 18 million pairs of question paraphrases from wikianswers.com, which were tagged as having the same mean</context>
<context position="35980" citStr="Fader et al. (2013)" startWordPosition="5930" endWordPosition="5933">ween language and the KB. In both Kwiatkowski et al. (2013) and this work, an intermediate representation is employed to handle the mismatch, but while they use a logical representation, we opt for a text-based one. Our choice allows us to benefit from the parallel monolingual corpus PARALEX and from word vectors trained on Wikipedia. We believe that our approach is particularly suitable for scenarios such as factoid question answering, where the space of logical forms is somewhat constrained and a few generation rules suffice to reduce the problem to paraphrasing. Our work is also related to Fader et al. (2013), who presented a paraphrase-driven question answering system. One can view this work as a generalization of Fader et al. along three dimensions. First, Fader et al. use a KB over natural language extractions rather than a formal KB and so querying the KB does not require a generation step – they paraphrase questions to KB entries directly. Second, they suggest a particular paraphrasing method that maps a test question to a question for which the answer is already known in a single step. We propose a general paraphrasing framework and instantiate it with two paraphrase models. Lastly, Fader et</context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>A. Fader, L. Zettlemoyer, and O. Etzioni. 2013. Paraphrase-driven learning for open question answering. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="22478" citStr="Fellbaum, 1998" startWordPosition="3713" endWordPosition="3714">pairs in PARALEX are word-aligned using standard machine translation methods. We use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) to all 5-grams. This results in a phrase table with approximately 1.3 million phrase pairs. We let A denote this set of mined candidate associations. For a pair (x, c), we also consider as candidate associations the set B (represented implicitly), which contains token pairs (xi, cis) such that xi and cis share the same lemma, the same POS tag, or are linked through a derivation link on WordNet (Fellbaum, 1998). This allows us to learn paraphrases for words that appear in our datasets but are not covered by the phrase table, and to handle nominalizations for phrase pairs such as “Who designed the game of life?” and “What game designer is the designer of the game of life?”. Our model goes over all possible spans of x and c and constructs all possible associations from A and B. This results in many poor associations (e.g., “play” and “the”), but as illustrated in Figure 3, we learn weights that discriminate good from bad associations. Table 3 specifies the full set of features. Note that unlike standa</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>363--370</pages>
<contexts>
<context position="28237" citStr="Finkel et al., 2005" startWordPosition="4671" endWordPosition="4674">tasets. Following Cai and Yates (2013), we hold out 30% of the data for the final test, and perform 3 random 80%-20% splits of the training set for development. Since we train from question-answer pairs, we collect answers by executing the gold logical forms against Freebase. We execute A-DCS queries by converting them into SPARQL and executing them against a copy of Freebase using the Virtuoso database engine. We evaluate our system with accuracy, that is, the proportion of questions we answer correctly. We run all questions through the Stanford CoreNLP pipeline (Toutanova and Manning, 2003; Finkel et al., 2005; Klein and Manning, 2003). We tuned the Li regularization strength, developed features, and ran analysis experiments on the development set (averaging across random splits). On WEBQUESTIONS, without Li regularization, the number of non-zero features was 360K; Li regularization brings it down to 17K. 6.2 Results We compare our system to Cai and Yates (2013) (CY13), Berant et al. (2013) (BCFL13), and Kwiatkowski et al. (2013) (KCAZ13). For BCFL13, we obtained results using the SEMPRE package2 and running Berant et al. (2013)’s system on the datasets. Table 5 presents results on the test set. We</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Association for Computational Linguistics (ACL), pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<title>Freebase data dumps (2013-06-09).</title>
<date>2013</date>
<note>https://developers.google.com/ freebase/data.</note>
<contexts>
<context position="6505" citStr="Google, 2013" startWordPosition="1025" endWordPosition="1026"> the current best performing system. The source code of our system PARASEMPRE is released at http://www-nlp.stanford.edu/ software/sempre/. 2 Setup Our task is as follows: Given (i) a knowledge base K, and (ii) a training set of question-answer pairs {(xi, yi)}ni=1, output a semantic parser that maps new questions x to answers y via latent logical forms z. Let £ denote a set of entities (e.g., BillGates), and let P denote a set of properties (e.g., PlaceOfBirth). A knowledge base K is a set of assertions (e1,p,e2) E £ x P x £ (e.g., (BillGates, PlaceOfBirth, Seattle)). We use the Freebase KB (Google, 2013), which has 41M entities, 19K properties, and 596M assertions. To query the KB, we use a logical language called simple A-DCS. In simple A-DCS, an entity (e.g., Seattle) is a unary predicate (i.e., a subset of £) denoting a singleton set containing that entity. A property (which is a binary predicate) can be joined with a unary predicate; e.g., Founded.Microsoft denotes the entities that are Microsoft founders. In PlaceOfBirth.Seattle F1 Founded.Microsoft, an intersection operator allows us to denote the set of Seattle-born Microsoft founders. A reverse operator reverses the order of arguments</context>
</contexts>
<marker>Google, 2013</marker>
<rawString>Google. 2013. Freebase data dumps (2013-06-09). https://developers.google.com/ freebase/data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="10361" citStr="Haghighi et al., 2005" startWordPosition="1639" endWordPosition="1642">nstruction phase. The model score decomposes into two terms: φ(x, c, z)&gt;θ = φpr(x, c)&gt;θpr + φlf(x,z)&gt;θlf, where the parameters θpr define the paraphrase model (Section 5), which is based on features extracted from text only (the input and canonical utterance). The parameters θlf correspond to semantic parsing features based on the logical form and input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation Qz�K. We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of R</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>A. Haghighi, A. Y. Ng, and C. D. Manning. 2005. Robust textual inference via graph matching. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
</authors>
<title>Methods for using textual entailment in open-domain question answering.</title>
<date>2006</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="34921" citStr="Harabagiu and Hickl, 2006" startWordPosition="5755" endWordPosition="5758">e was the gallipoli campaign waged?” is Galipoli and not GalipoliCampaign. Last, PARASEMPRE does not handle temporal information, which causes errors in questions like “Where did Harriet Tubman live after the civil war?” 7 Discussion In this work, we approach the problem of semantic parsing from a paraphrasing viewpoint. A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment (Dagan et al., 2013). While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al., 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. Our paraphrase model emphasizes simplicity and efficiency, but the framework is agnostic to the internals of the paraphrase method. On the semantic parsing side, our work is most related to Kwiatkowski et al. (2013). The main challenge in semantic parsing is coping with the mismatch between language and the KB. In both Kwiatkowski et al. (2013) and this work, an intermediate representation is employed to handle the mismatch, but while they use</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>S. Harabagiu and A. Hickl. 2006. Methods for using textual entailment in open-domain question answering. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL),</booktitle>
<pages>1011--1019</pages>
<contexts>
<context position="10300" citStr="Heilman and Smith, 2010" startWordPosition="1629" endWordPosition="1632">utterances Cx are constructed during the canonical utterance construction phase. The model score decomposes into two terms: φ(x, c, z)&gt;θ = φpr(x, c)&gt;θpr + φlf(x,z)&gt;θlf, where the parameters θpr define the paraphrase model (Section 5), which is based on features extracted from text only (the input and canonical utterance). The parameters θlf correspond to semantic parsing features based on the logical form and input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation Qz�K. We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature </context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>M. Heilman and N. A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL), pages 1011–1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Huang</author>
<author>R Socher</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="24423" citStr="Huang et al., 2012" startWordPosition="4035" endWordPosition="4038">Vector space model The association model relies on having a good set of candidate associations, but mining associations suffers from coverage issues. We now introduce a vector space (VS) model, which assigns a vector representation for each utterance, and learns a scoring function that ranks paraphrase candidates. We start by constructing vector representations of words. We run the WORD2VEC tool (Mikolov et al., 2013) on lower-cased Wikipedia text (1.59 billion tokens), using the CBOW model with a window of 5 and hierarchical softmax. We also experiment with publicly released word embeddings (Huang et al., 2012), which were trained using both local and global context. Both result in kdimensional vectors (k = 50). Next, we construct a vector vx E Rk for each utterance x by simply averaging the vectors of all content words (nouns, verbs, and adjectives) in x. We can now estimate a paraphrase score for two utterances x and c via a weighted combination of the components of the vector representations: where W E Rk×k is a parameter matrix. In terms of our earlier notation, we have θvs = vec(W) and φvs(x, c) = vec(vxvT c ), where vec(·) unrolls a matrix into a vector. In Section 6, we experiment with W equa</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>423--430</pages>
<contexts>
<context position="28263" citStr="Klein and Manning, 2003" startWordPosition="4675" endWordPosition="4678"> and Yates (2013), we hold out 30% of the data for the final test, and perform 3 random 80%-20% splits of the training set for development. Since we train from question-answer pairs, we collect answers by executing the gold logical forms against Freebase. We execute A-DCS queries by converting them into SPARQL and executing them against a copy of Freebase using the Virtuoso database engine. We evaluate our system with accuracy, that is, the proportion of questions we answer correctly. We run all questions through the Stanford CoreNLP pipeline (Toutanova and Manning, 2003; Finkel et al., 2005; Klein and Manning, 2003). We tuned the Li regularization strength, developed features, and ran analysis experiments on the development set (averaging across random splits). On WEBQUESTIONS, without Li regularization, the number of non-zero features was 360K; Li regularization brings it down to 17K. 6.2 Results We compare our system to Cai and Yates (2013) (CY13), Berant et al. (2013) (BCFL13), and Kwiatkowski et al. (2013) (KCAZ13). For BCFL13, we obtained results using the SEMPRE package2 and running Berant et al. (2013)’s system on the datasets. Table 5 presents results on the test set. We achieve a substantial rel</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. In Association for Computational Linguistics (ACL), pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1223--1233</pages>
<contexts>
<context position="1345" citStr="Kwiatkowski et al., 2010" startWordPosition="201" endWordPosition="204">aphrase model to choose the realization that best paraphrases the input, and output the corresponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof-the-art accuracies on two recently released question-answering datasets. 1 Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in gre</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Empirical Methods in Natural Language Processing (EMNLP), pages 1223–1233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>E Choi</author>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1510" citStr="Kwiatkowski et al., 2013" startWordPosition="226" endWordPosition="229">iation model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof-the-art accuracies on two recently released question-answering datasets. 1 Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all </context>
<context position="3910" citStr="Kwiatkowski et al. (2013)" startWordPosition="602" endWordPosition="605">t party did Clay establish? What political party founded by Henry Clay? ... What event involved the people Henry Clay? Type.PoliticalParty fl Founder.HenryClay ... Type.Event fl Involved.HenryClay Whig Party 1415 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1415–1425, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Figure 2: The main challenge in semantic parsing is coping with the mismatch between language and the KB. (a) Traditionally, semantic parsing maps utterances directly to logical forms. (b) Kwiatkowski et al. (2013) map the utterance to an underspecified logical form, and perform ontology matching to handle the mismatch. (c) We approach the problem in the other direction, generating canonical utterances for logical forms, and use paraphrase models to handle the mismatch. tically generate canonical utterances for each logical form based on the text descriptions of predicates from the KB. Finally, we choose the canonical utterance that best paraphrases the input utterance, and thereby the logical form that generated it. We use two complementary paraphrase models: an association model based on aligned phras</context>
<context position="7843" citStr="Kwiatkowski et al. 2013" startWordPosition="1232" endWordPosition="1235">ounded.Microsoft) denotes set cardinality, in this case, the number of Microsoft founders. The denotation of a logical form z with respect to a KB K is given by Qz�K. For a formal description of simple A-DCS, see Liang (2013) and Berant et al. (2013). 3 Model overview We now present the general framework for semantic parsing via paraphrasing, including the model and the learning algorithm. In Sections 4 and 5, we provide the details of our implementation. Canonical utterance construction Given an utterance x and the KB, we construct a set of candiontology matching underspecified logical form (Kwiatkowski et al. 2013) paraphrase canonical utterance (this work) utterance logical form direct (traditional) 1416 date logical forms ix, and then for each z E ix generate a small set of canonical natural language utterances Cz. Our goal at this point is only to generate a manageable set of logical forms containing the correct one, and then generate an appropriate canonical utterance from it. This strategy is feasible in factoid QA where compositionality is low, and so the size of ix is limited (Section 4). Paraphrasing We score the canonical utterances in Cz with respect to the input utterance x using a paraphrase</context>
<context position="28665" citStr="Kwiatkowski et al. (2013)" startWordPosition="4737" endWordPosition="4740">uate our system with accuracy, that is, the proportion of questions we answer correctly. We run all questions through the Stanford CoreNLP pipeline (Toutanova and Manning, 2003; Finkel et al., 2005; Klein and Manning, 2003). We tuned the Li regularization strength, developed features, and ran analysis experiments on the development set (averaging across random splits). On WEBQUESTIONS, without Li regularization, the number of non-zero features was 360K; Li regularization brings it down to 17K. 6.2 Results We compare our system to Cai and Yates (2013) (CY13), Berant et al. (2013) (BCFL13), and Kwiatkowski et al. (2013) (KCAZ13). For BCFL13, we obtained results using the SEMPRE package2 and running Berant et al. (2013)’s system on the datasets. Table 5 presents results on the test set. We achieve a substantial relative improvement of 12% in accuracy on WEBQUESTIONS, and match the best results on FREE917. Interestingly, our system gets an oracle accuracy of 63% on WEBQUESTIONS compared to 48% obtained by BCFL13, where the oracle accuracy is the fraction of questions for which at least one logical form in the candidate set produced by the system is correct. This demonstrates that our method for constructing ca</context>
<context position="35289" citStr="Kwiatkowski et al. (2013)" startWordPosition="5814" endWordPosition="5817">l of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment (Dagan et al., 2013). While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al., 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. Our paraphrase model emphasizes simplicity and efficiency, but the framework is agnostic to the internals of the paraphrase method. On the semantic parsing side, our work is most related to Kwiatkowski et al. (2013). The main challenge in semantic parsing is coping with the mismatch between language and the KB. In both Kwiatkowski et al. (2013) and this work, an intermediate representation is employed to handle the mismatch, but while they use a logical representation, we opt for a text-based one. Our choice allows us to benefit from the parallel monolingual corpus PARALEX and from word vectors trained on Wikipedia. We believe that our approach is particularly suitable for scenarios such as factoid question answering, where the space of logical forms is somewhat constrained and a few generation rules suf</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Lambda dependency-based compositional semantics.</title>
<date>2013</date>
<tech>Technical report, ArXiv.</tech>
<contexts>
<context position="7444" citStr="Liang (2013)" startWordPosition="1169" endWordPosition="1170">ry predicate; e.g., Founded.Microsoft denotes the entities that are Microsoft founders. In PlaceOfBirth.Seattle F1 Founded.Microsoft, an intersection operator allows us to denote the set of Seattle-born Microsoft founders. A reverse operator reverses the order of arguments: R[PlaceOfBirth].BillGates denotes Bill Gates’s birthplace (in contrast to PlaceOfBirth.Seattle). Lastly, count(Founded.Microsoft) denotes set cardinality, in this case, the number of Microsoft founders. The denotation of a logical form z with respect to a KB K is given by Qz�K. For a formal description of simple A-DCS, see Liang (2013) and Berant et al. (2013). 3 Model overview We now present the general framework for semantic parsing via paraphrasing, including the model and the learning algorithm. In Sections 4 and 5, we provide the details of our implementation. Canonical utterance construction Given an utterance x and the KB, we construct a set of candiontology matching underspecified logical form (Kwiatkowski et al. 2013) paraphrase canonical utterance (this work) utterance logical form direct (traditional) 1416 date logical forms ix, and then for each z E ix generate a small set of canonical natural language utterance</context>
</contexts>
<marker>Liang, 2013</marker>
<rawString>P. Liang. 2013. Lambda dependency-based compositional semantics. Technical report, ArXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lin</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>Entity linking at web scale.</title>
<date>2012</date>
<booktitle>In Knowledge Extraction Workshop (AKBC-WEKEX).</booktitle>
<contexts>
<context position="10985" citStr="Lin et al., 2012" startWordPosition="1751" endWordPosition="1754">nd Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation Qz�K. We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 2012). Lastly, Freebase formulas have types (see Section 4), and we conjoin the type of z with the first word of x, to capture the correlation between a word (e.g., “where”) with the Freebase type (e.g., Location). Learning As our training data consists of question-answer pairs (xi, yi), we maximize the log-likelihood of the correct answer. The probability of an answer y is obtained by marginalizing over canonical utterances c and logical forms z whose denotation is y. Formally, our objective function O(θ) is as follows: n O(θ) = log pθ(yi |xi) − λ11θ111, i=1 pθ(y |x) = E E pθ(c, z |x). z∈Zx:y=QzlK</context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>T. Lin, Mausam, and O. Etzioni. 2012. Entity linking at web scale. In Knowledge Extraction Workshop (AKBC-WEKEX).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>K Chen</author>
<author>G Corrado</author>
<author>Jeffrey</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<tech>Technical report, ArXiv.</tech>
<contexts>
<context position="24225" citStr="Mikolov et al., 2013" startWordPosition="4001" endWordPosition="4004">rize, the association model links phrases of two utterances in multiple overlapping ways. During training, the model learns which associations are characteristic of paraphrases and which are not. 5.2 Vector space model The association model relies on having a good set of candidate associations, but mining associations suffers from coverage issues. We now introduce a vector space (VS) model, which assigns a vector representation for each utterance, and learns a scoring function that ranks paraphrase candidates. We start by constructing vector representations of words. We run the WORD2VEC tool (Mikolov et al., 2013) on lower-cased Wikipedia text (1.59 billion tokens), using the CBOW model with a window of 5 and hierarchical softmax. We also experiment with publicly released word embeddings (Huang et al., 2012), which were trained using both local and global context. Both result in kdimensional vectors (k = 50). Next, we construct a vector vx E Rk for each utterance x by simply averaging the vectors of all content words (nouns, verbs, and adjectives) in x. We can now estimate a paraphrase score for two utterances x and c via a weighted combination of the components of the vector representations: where W E</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Jeffrey, 2013</marker>
<rawString>T. Mikolov, K. Chen, G. Corrado, and Jeffrey. 2013. Efficient estimation of word representations in vector space. Technical report, ArXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="22064" citStr="Och and Ney, 2004" startWordPosition="3640" endWordPosition="3643"> lemma(xi:j) denote the POS tag and lemma sequence of xi:j. corpora, containing 18 million pairs of question paraphrases from wikianswers.com, which were tagged as having the same meaning by users. PARALEX is suitable for our needs since it focuses on question paraphrases. For example, the phrase “do for a living” occurs mostly in questions, and we can extract associations for this phrase from PARALEX. Paraphrase pairs in PARALEX are word-aligned using standard machine translation methods. We use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) to all 5-grams. This results in a phrase table with approximately 1.3 million phrase pairs. We let A denote this set of mined candidate associations. For a pair (x, c), we also consider as candidate associations the set B (represented implicitly), which contains token pairs (xi, cis) such that xi and cis share the same lemma, the same POS tag, or are linked through a derivation link on WordNet (Fellbaum, 1998). This allows us to learn paraphrases for words that appear in our datasets but are not covered by the phrase table, and to handle nominalizations for phrase pairs such as “Who designed </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Piwek</author>
<author>K E Boyer</author>
</authors>
<title>Varieties of question generation: Introduction to this special issue. Dialogue and Discourse,</title>
<date>2012</date>
<pages>3--1</pages>
<contexts>
<context position="37006" citStr="Piwek and Boyer, 2012" startWordPosition="6107" endWordPosition="6110">ps a test question to a question for which the answer is already known in a single step. We propose a general paraphrasing framework and instantiate it with two paraphrase models. Lastly, Fader et al. handle queries with only one property and entity whereas we generalize to more types of logical forms. Since our generated questions are passed to a paraphrase model, we took a very simple approach, mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization. Research on generation (Dale et al., 2003; Reiter et al., 2005; Turner et al., 2009; Piwek and Boyer, 2012) typically focuses on generating natural utterances for human consumption, where fluency is important. In conclusion, the main contribution of this paper is a novel approach for semantic parsing based on a simple generation procedure and a paraphrase model. We achieve state-of-the-art results on two recently released datasets. We believe that our approach opens a window of opportunity for learning semantic parsers from raw text not necessarily related to the target KB. With more sophisticated generation and paraphrase, we hope to tackle compositionally richer utterances. Acknowledgments We tha</context>
</contexts>
<marker>Piwek, Boyer, 2012</marker>
<rawString>P. Piwek and K. E. Boyer. 2012. Varieties of question generation: Introduction to this special issue. Dialogue and Discourse, 3:1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
<author>J Hunter</author>
<author>J Yu</author>
<author>I Davy</author>
</authors>
<title>Choosing words in computer-generated weather forecasts.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>167</volume>
<pages>169</pages>
<contexts>
<context position="36961" citStr="Reiter et al., 2005" startWordPosition="6099" endWordPosition="6102">t a particular paraphrasing method that maps a test question to a question for which the answer is already known in a single step. We propose a general paraphrasing framework and instantiate it with two paraphrase models. Lastly, Fader et al. handle queries with only one property and entity whereas we generalize to more types of logical forms. Since our generated questions are passed to a paraphrase model, we took a very simple approach, mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization. Research on generation (Dale et al., 2003; Reiter et al., 2005; Turner et al., 2009; Piwek and Boyer, 2012) typically focuses on generating natural utterances for human consumption, where fluency is important. In conclusion, the main contribution of this paper is a novel approach for semantic parsing based on a simple generation procedure and a paraphrase model. We achieve state-of-the-art results on two recently released datasets. We believe that our approach opens a window of opportunity for learning semantic parsers from raw text not necessarily related to the target KB. With more sophisticated generation and paraphrase, we hope to tackle compositiona</context>
</contexts>
<marker>Reiter, Sripada, Hunter, Yu, Davy, 2005</marker>
<rawString>E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005. Choosing words in computer-generated weather forecasts. Artificial Intelligence, 167:137– 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Romano</author>
<author>M kouylekov</author>
<author>I Szpektor</author>
<author>I Dagan</author>
<author>A Lavelli</author>
</authors>
<title>Investigating a generic paraphrase-based approach for relation extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of ECAL.</booktitle>
<contexts>
<context position="34967" citStr="Romano et al., 2006" startWordPosition="5762" endWordPosition="5765"> not GalipoliCampaign. Last, PARASEMPRE does not handle temporal information, which causes errors in questions like “Where did Harriet Tubman live after the civil war?” 7 Discussion In this work, we approach the problem of semantic parsing from a paraphrasing viewpoint. A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment (Dagan et al., 2013). While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al., 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. Our paraphrase model emphasizes simplicity and efficiency, but the framework is agnostic to the internals of the paraphrase method. On the semantic parsing side, our work is most related to Kwiatkowski et al. (2013). The main challenge in semantic parsing is coping with the mismatch between language and the KB. In both Kwiatkowski et al. (2013) and this work, an intermediate representation is employed to handle the mismatch, but while they use a logical representation, we opt for a text-b</context>
</contexts>
<marker>Romano, kouylekov, Szpektor, Dagan, Lavelli, 2006</marker>
<rawString>L. Romano, M. kouylekov, I. Szpektor, I. Dagan, and A. Lavelli. 2006. Investigating a generic paraphrase-based approach for relation extraction. In Proceedings of ECAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>E H Huang</author>
<author>J Pennin</author>
<author>C D Manning</author>
<author>A Ng</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>801--809</pages>
<contexts>
<context position="19048" citStr="Socher et al., 2011" startWordPosition="3148" endWordPosition="3151"> ?. On the WEBQUESTIONS dataset, we generate an average of 1,423 canonical utterances c per input utterance x. In Section 6, we show that an even simpler method of generating canonical utterances by concatenating Freebase descriptions hurts accuracy by only a modest amount. 5 Paraphrasing Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model decomposes into an association model and a vector s</context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>R. Socher, E. H. Huang, J. Pennin, C. D. Manning, and A. Ng. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems (NIPS), pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stern</author>
<author>I Dagan</author>
</authors>
<title>A confidence model for syntactically-motivated entailment proofs.</title>
<date>2011</date>
<booktitle>In Recent Advances in Natural Language Processing,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="10324" citStr="Stern and Dagan, 2011" startWordPosition="1633" endWordPosition="1636">cted during the canonical utterance construction phase. The model score decomposes into two terms: φ(x, c, z)&gt;θ = φpr(x, c)&gt;θpr + φlf(x,z)&gt;θlf, where the parameters θpr define the paraphrase model (Section 5), which is based on features extracted from text only (the input and canonical utterance). The parameters θlf correspond to semantic parsing features based on the logical form and input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation Qz�K. We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity freq</context>
<context position="19144" citStr="Stern and Dagan, 2011" startWordPosition="3164" endWordPosition="3167">put utterance x. In Section 6, we show that an even simpler method of generating canonical utterances by concatenating Freebase descriptions hurts accuracy by only a modest amount. 5 Paraphrasing Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model decomposes into an association model and a vector space model: φpr(x, c)Tθpr = φas(x, c)Tθas + φvs(x, c)Tθvs. x : What type of music did Richard Wa</context>
</contexts>
<marker>Stern, Dagan, 2011</marker>
<rawString>A. Stern and I. Dagan. 2011. A confidence model for syntactically-motivated entailment proofs. In Recent Advances in Natural Language Processing, pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>Featurerich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="28216" citStr="Toutanova and Manning, 2003" startWordPosition="4667" endWordPosition="4670">some statistics on the two datasets. Following Cai and Yates (2013), we hold out 30% of the data for the final test, and perform 3 random 80%-20% splits of the training set for development. Since we train from question-answer pairs, we collect answers by executing the gold logical forms against Freebase. We execute A-DCS queries by converting them into SPARQL and executing them against a copy of Freebase using the Virtuoso database engine. We evaluate our system with accuracy, that is, the proportion of questions we answer correctly. We run all questions through the Stanford CoreNLP pipeline (Toutanova and Manning, 2003; Finkel et al., 2005; Klein and Manning, 2003). We tuned the Li regularization strength, developed features, and ran analysis experiments on the development set (averaging across random splits). On WEBQUESTIONS, without Li regularization, the number of non-zero features was 360K; Li regularization brings it down to 17K. 6.2 Results We compare our system to Cai and Yates (2013) (CY13), Berant et al. (2013) (BCFL13), and Kwiatkowski et al. (2013) (KCAZ13). For BCFL13, we obtained results using the SEMPRE package2 and running Berant et al. (2013)’s system on the datasets. Table 5 presents result</context>
</contexts>
<marker>Toutanova, Manning, 2003</marker>
<rawString>K. Toutanova and C. D. Manning. 2003. Featurerich part-of-speech tagging with a cyclic dependency network. In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Turner</author>
<author>Y Sripada</author>
<author>E Reiter</author>
</authors>
<title>Generating approximate geographic descriptions.</title>
<date>2009</date>
<booktitle>In European Workshop on Natural Language Generation,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="36982" citStr="Turner et al., 2009" startWordPosition="6103" endWordPosition="6106">rasing method that maps a test question to a question for which the answer is already known in a single step. We propose a general paraphrasing framework and instantiate it with two paraphrase models. Lastly, Fader et al. handle queries with only one property and entity whereas we generalize to more types of logical forms. Since our generated questions are passed to a paraphrase model, we took a very simple approach, mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization. Research on generation (Dale et al., 2003; Reiter et al., 2005; Turner et al., 2009; Piwek and Boyer, 2012) typically focuses on generating natural utterances for human consumption, where fluency is important. In conclusion, the main contribution of this paper is a novel approach for semantic parsing based on a simple generation procedure and a paraphrase model. We achieve state-of-the-art results on two recently released datasets. We believe that our approach opens a window of opportunity for learning semantic parsers from raw text not necessarily related to the target KB. With more sophisticated generation and paraphrase, we hope to tackle compositionally richer utterances</context>
</contexts>
<marker>Turner, Sripada, Reiter, 2009</marker>
<rawString>R. Turner, Y. Sripada, and E. Reiter. 2009. Generating approximate geographic descriptions. In European Workshop on Natural Language Generation, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wan</author>
<author>M Dras</author>
<author>R Dale</author>
<author>C Paris</author>
</authors>
<title>Using dependency-based features to take the “para-farce” out of paraphrase.</title>
<date>2006</date>
<booktitle>In Australasian Language Technology Workshop.</booktitle>
<contexts>
<context position="18997" citStr="Wan et al., 2006" startWordPosition="3139" endWordPosition="3142">f the verb inside the VP tree. WH d(t) d(b) d(e) ?. On the WEBQUESTIONS dataset, we generate an average of 1,423 canonical utterances c per input utterance x. In Section 6, we show that an even simpler method of generating canonical utterances by concatenating Freebase descriptions hurts accuracy by only a modest amount. 5 Paraphrasing Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model </context>
<context position="32406" citStr="Wan et al. (2006)" startWordPosition="5355" endWordPosition="5358">speak Identity do people czech republic speak Figure 4: Values of the paraphrase score v iWvci, for all content word tokens xi and cii, where W is an arbitrary full matrix, a diagonal matrix, or the identity matrix. We omit scores for the words “czech” and “republic” since they appear in all canonical utterances for this example. • JACCARD: We compute the Jaccard score between the tokens of x and c and define Opr(x, c) to be this single feature. • EDIT: We compute the token edit distance between x and c and define Opr(x, c) to be this single feature. • WDDC06: We re-implement 13 features from Wan et al. (2006), who obtained close to state-of-the-art performance on the Microsoft Research paraphrase corpus.3 Table 6 demonstrates that we improve performance over all baselines. Interestingly, JACCARD and WDDC06 obtain reasonable performance on FREE917 but perform much worse on WEBQUESTIONS. We surmise this is because questions in FREE917 were generated by annotators prompted by Freebase facts, whereas questions in WEBQUESTIONS originated independently of Freebase. Thus, word choice in FREE917 is often close to the generated Freebase descriptions, allowing simple baselines to perform well. Error analysi</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using dependency-based features to take the “para-farce” out of paraphrase. In Australasian Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>C D Manning</author>
</authors>
<title>Probabilistic treeedit models with structured latent variables for textual entailment and question answering.</title>
<date>2010</date>
<booktitle>In The International Conference on Computational Linguistics,</booktitle>
<pages>1164--1172</pages>
<contexts>
<context position="19120" citStr="Wang and Manning, 2010" startWordPosition="3160" endWordPosition="3163">ical utterances c per input utterance x. In Section 6, we show that an even simpler method of generating canonical utterances by concatenating Freebase descriptions hurts accuracy by only a modest amount. 5 Paraphrasing Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model decomposes into an association model and a vector space model: φpr(x, c)Tθpr = φas(x, c)Tθas + φvs(x, c)Tθvs. x : What type</context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>M. Wang and C. D. Manning. 2010. Probabilistic treeedit models with structured latent variables for textual entailment and question answering. In The International Conference on Computational Linguistics, pages 1164–1172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>960--967</pages>
<contexts>
<context position="1318" citStr="Wong and Mooney, 2007" startWordPosition="197" endWordPosition="200">ach. Then, we use a paraphrase model to choose the realization that best paraphrases the input, and output the corresponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof-the-art accuracies on two recently released question-answering datasets. 1 Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase </context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Association for Computational Linguistics (ACL), pages 960–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic proramming.</title>
<date>1996</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1264" citStr="Zelle and Mooney, 1996" startWordPosition="189" endWordPosition="192"> with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best paraphrases the input, and output the corresponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof-the-art accuracies on two recently released question-answering datasets. 1 Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utter</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic proramming. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>658--666</pages>
<contexts>
<context position="1295" citStr="Zettlemoyer and Collins, 2005" startWordPosition="193" endWordPosition="196">ation in natural language for each. Then, we use a paraphrase model to choose the realization that best paraphrases the input, and output the corresponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof-the-art accuracies on two recently released question-answering datasets. 1 Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Uncertainty in Artificial Intelligence (UAI), pages 658– 666.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>