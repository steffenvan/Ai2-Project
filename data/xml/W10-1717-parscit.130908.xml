<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.066493">
<title confidence="0.995546">
Lessons from NRC’s Portage System at WMT 2010
</title>
<author confidence="0.995642">
Samuel Larkin, Boxing Chen, George Foster, Ulrich Germann, Eric Joanis,
Howard Johnson, and Roland Kuhn
</author>
<affiliation confidence="0.8474175">
National Research Council of Canada (NRC)
Gatineau, Québec, Canada.
</affiliation>
<email confidence="0.83358">
Firstname.Lastname@cnrc-nrc.gc.ca
</email>
<sectionHeader confidence="0.992446" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9975593125">
NRC’s Portage system participated in the Eng-
lish-French (E-F) and French-English (F-E)
translation tasks of the ACL WMT 2010 eval-
uation. The most notable improvement over
earlier versions of Portage is an efficient im-
plementation of lattice MERT. While Portage
has typically performed well in Chinese to
English MT evaluations, most recently in the
NIST09 evaluation, our participation in WMT
2010 revealed some interesting differences be-
tween Chinese-English and E-F/F-E transla-
tion, and alerted us to certain weak spots in
our system. Most of this paper discusses the
problems we found in our system and ways of
fixing them. We learned several lessons that
we think will be of general interest.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999865434782609">
Portage, the statistical machine translation sys-
tem of the National Research Council of Canada
(NRC), is a two-pass phrase-based system. The
translation tasks to which it is most often applied
are Chinese to English, English to French (hen-
ceforth “E-F”), and French to English (hence-
forth “F-E”): in recent years we worked on Chi-
nese-English translation for the GALE project
and for NIST evaluations, and English and
French are Canada’s two official languages. In
WMT 2010, Portage scored 28.5 BLEU (un-
cased) for F-E, but only 27.0 BLEU (uncased)
for E-F. For both language pairs, Portage tru-
ecasing caused a loss of 1.4 BLEU; other WMT
systems typically lost around 1.0 BLEU after
truecasing. In Canada, about 80% of translations
between English and French are from English to
French, so we would have preferred better results
for that direction. This paper first describes the
version of Portage that participated in WMT
2010. It then analyzes problems with the system
and describes the solutions we found for some of
them.
</bodyText>
<sectionHeader confidence="0.896482" genericHeader="method">
2 Portage system description
</sectionHeader>
<subsectionHeader confidence="0.998082">
2.1 Core engine and training data
</subsectionHeader>
<bodyText confidence="0.999975606060606">
The NRC system uses a standard two-pass
phrase-based approach. Major features in the
first-pass loglinear model include phrase tables
derived from symmetrized IBM2 alignments and
symmetrized HMM alignments, a distance-based
distortion model, a lexicalized distortion model,
and language models (LMs) that can be either
static or else dynamic mixtures. Each phrase ta-
ble used was a merged one, created by separately
training an IBM2-based and an HMM-based
joint count table on the same data and then add-
ing the counts. Each includes relative frequency
estimates and lexical estimates (based on Zens
and Ney, 2004) of forward and backward condi-
tional probabilities. The lexicalized distortion
probabilities are also obtained by adding IBM2
and HMM counts. They involve 6 features (mo-
notone, swap and discontinuous features for fol-
lowing and preceding phrase) and are condi-
tioned on phrase pairs in a model similar to that
of Moses (Koehn et al., 2005); a MAP-based
backoff smoothing scheme is used to combat
data sparseness when estimating these probabili-
ties. Dynamic mixture LMs are linear mixtures
of ngram models trained on parallel sub-corpora
with weights set to minimize perplexity of the
current source text as described in (Foster and
Kuhn, 2007); henceforth, we’ll call them “dy-
namic LMs”.
Decoding uses the cube-pruning algorithm of
(Huang and Chiang, 2007) with a 7-word distor-
tion limit. Contrary to the usual implementation
of distortion limits, we allow a new phrase to end
</bodyText>
<page confidence="0.965034">
127
</page>
<note confidence="0.4532195">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99995271875">
more than 7 words past the first non-covered
word, as long as the new phrase starts within 7
words from the first non-covered word. Notwith-
standing the distortion limit, contiguous phrases
can always be swapped. Out-of-vocabulary
(OOV) source words are passed through un-
changed to the target. Loglinear weights are
tuned with Och&apos;s max-BLEU algorithm over lat-
tices (Macherey et al., 2008); more details about
lattice MERT are given in the next section. The
second pass rescores 1000-best lists produced by
the first pass, with additional features including
various LM and IBM-model probabilities; ngram,
length, and reordering posterior probabilities and
frequencies; and quote and parenthesis mismatch
indicators. To improve the quality of the maxima
found by MERT when using large sets of partial-
ly-overlapping rescoring features, we use greedy
feature selection, first expanding from a baseline
set, then pruning.
We restricted our training data to data that was
directly available through the workshop&apos;s web-
site; we didn’t use the LDC resources mentioned
on the website (e.g., French Gigaword, English
Gigaword). Below, “mono” refers to all mono-
lingual data (Europarl, news-commentary, and
shuffle); “mono” English is roughly three times
bigger than “mono” French (50.6 M lines in
“mono” English, 17.7 M lines in “mono” French).
“Domain” refers to all WMT parallel training
data except GigaFrEn (i.e., Europarl, news-
commentary, and UN).
</bodyText>
<subsectionHeader confidence="0.999255">
2.2 Preprocessing and postprocessing
</subsectionHeader>
<bodyText confidence="0.999975090909091">
We used our own English and French pre- and
post-processing tools, rather than those available
from the WMT web site. For training, all English
and French text is tokenized with a language-
specific tokenizer and then mapped to lowercase.
Truecasing uses an HMM approach, with lexical
probabilities derived from “mono” and transition
probabilities from a 3-gram LM trained on tru-
ecase “mono”. A subsequent rule-based pass ca-
pitalizes sentence-initial words. A final detokeni-
zation step undoes the tokenization.
</bodyText>
<subsectionHeader confidence="0.999183">
2.3 System configurations for WMT 2010
</subsectionHeader>
<bodyText confidence="0.998927857142857">
In the weeks preceding the evaluation, we tried
several ways of arranging the resources available
to us. We picked the configurations that gave the
highest BLEU scores on WMT2009 Newstest.
We found that tuning with lattice MERT rather
than N-best MERT allowed us to employ more
parameters and obtain better results.
</bodyText>
<listItem confidence="0.963163">
E-F system components:
1. Phrase table trained on “domain”;
2. Phrase table trained on GigaFrEn;
3. Lexicalized distortion model trained on
“domain”;
4. Distance-based distortion model;
5. 5-gram French LM trained on “mono”;
6. 4-gram LM trained on French half of
GigaFrEn;
7. Dynamic LM composed of 4 LMs, each
trained on the French half of a parallel
corpus (5-gram LM trained on “domain”,
4-gram LM on GigaFrEn, 5-gram LM on
news-commentary and 5-gram LM on
UN).
</listItem>
<bodyText confidence="0.9945975">
The F-E system is a mirror image of the E-F sys-
tem.
</bodyText>
<sectionHeader confidence="0.876211" genericHeader="method">
3 Details of lattice MERT (LMERT)
</sectionHeader>
<bodyText confidence="0.999993342857143">
Our system’s implementation of LMERT (Ma-
cherey et al., 2008) is the most notable recent
change in our system. As more and more features
are included in the loglinear model, especially if
they are correlated, N-best MERT (Och, 2003)
shows more and more instability, because of
convergence to local optima (Foster and Kuhn,
2009). We had been looking for methods that
promise more stability and better convergence.
LMERT seemed to fit the bill. It optimizes over
the complete lattice of candidate translations af-
ter a decoding run. This avoids some of the prob-
lems of N-best lists, which lack variety, leading
to poor local optima and the need for many de-
coder runs.
Though the algorithm is straightforward and is
highly parallelizable, attention must be paid to
space and time resource issues during implemen-
tation. Lattices output by our decoder were large
and needed to be shrunk dramatically for the al-
gorithm to function well. Fortunately, this could
be achieved via the finite state equivalence algo-
rithm for minimizing deterministic finite state
machines. The second helpful idea was to sepa-
rate out the features that were a function of the
phrase associated with an arc (e.g., translation
length and translation model probability fea-
tures). These features could then be stored in a
smaller phrase-feature table. Features associated
with language or distortion models could be han-
dled in a larger transition-feature table.
The above ideas, plus careful coding of data
structures, brought the memory footprint down
sufficiently to allow us to use complete lattices
from the decoder and optimize over the complete
</bodyText>
<page confidence="0.990868">
128
</page>
<bodyText confidence="0.999765">
development set for NIST09 Chinese-English.
However, combining lattices between decoder
runs again resulted in excessive memory re-
quirements. We achieved acceptable perfor-
mance by searching only the lattice from the lat-
est decoder run; perhaps information from earlier
runs, though critical for convergence in N-best
MERT, isn’t as important for LMERT.
Until a reviewer suggested it, we had not
thought of pruning lattices to a specified graph
density as a solution for our memory problems.
This is referred to in a single sentence in (Ma-
cherey et al., 2008), which does not specify its
implementation or its impact on performance,
and is an option of OpenFst (we didn’t use
OpenFst). We will certainly experiment with lat-
tice pruning in future.
Powell&apos;s algorithm (PA), which is at the core
of MERT, has good convergence when features
are mostly independent and do not depart much
from a simple coordinate search; it can run into
problems when there are many correlated fea-
tures (as with multiple translation and language
models). Figure 1 shows the kind of case where
PA works well. The contours of the function be-
ing optimized are relatively smooth, facilitating
learning of new search directions from gradients.
Figure 2 shows a more difficult case: there is
a single optimum, but noise dominates and PA
has difficulty finding new directions. Search of-
ten iterates over the original co-ordinates, miss-
ing optima that are nearby but in directions not
discoverable from local gradients. Probes in ran-
dom directions can do better than iteration over
the same directions (this is similar to the method
proposed for N-best MERT by Cer et al., 2008).
Each 1-dimensional MERT optimization is exact,
so if our probe stabs a region with better scores,
it will be discovered. Figures 1 and 2 only hint
at the problem: in reality, 2-dimensional search
isn’t a problem. The difficulties occur as the di-
mension grows: in high dimensions, it is more
important to get good directions and they are
harder to find.
For WMT 2010, we crafted a compromise
with the best properties of PA, yet allowing for a
more aggressive search in more directions. We
start with PA. As long as PA is adding new di-
rection vectors, it is continued. When PA stops
adding new directions, random rotation (ortho-
gonal transformation) of the coordinates is per-
formed and PA is restarted in the new space. PA
almost always fails to introduce new directions
within the new coordinates, then fails again, so
another set of random coordinates is chosen. This
process repeats until convergence. In future
work, we will look at incorporating random res-
tarts into the algorithm as additional insurance
against premature convergence.
Our LMERT implementation has room for
improvement: it may still run into over-fitting
problems with many correlated features. Howev-
er, during preparation for the evaluation, we no-
ticed that LMERT converged better than N-best
MERT, allowing models with more features and
higher BLEU to be chosen.
After the WMT submission, we discovered
that our LMERT implementation had a bug; our
submission was tuned with this buggy LMERT.
Comparison between our E-F submission tuned
with N-best MERT and the same system tuned
with bug-fixed LMERT shows BLEU gains of
+1.5-3.5 for LMERT (on dev, WMT2009, and
WMT2010, with no rescoring). However, N-best
MERT performed very poorly in this particular
case; we usually obtain a gain due to LMERT of
+0.2-1.0 (e.g., for the submitted F-E system).
</bodyText>
<figureCaption confidence="0.9868715">
Figure 1: Convergence for PA (Smooth Feature
Space)
Figure 2: Convergence for PA with Random Rotation
(Rough Feature Space)
</figureCaption>
<page confidence="0.995451">
129
</page>
<sectionHeader confidence="0.960049" genericHeader="evaluation">
4 Problems and Solutions
</sectionHeader>
<subsectionHeader confidence="0.990157">
4.1 Fixing LMERT
</subsectionHeader>
<bodyText confidence="0.999715142857143">
Just after the evaluation, we noticed a discrepan-
cy for E-F between BLEU scores computed dur-
ing LMERT optimization and scores from the 1-
best list immediately after decoding. Our
LMERT code had a bug that garbled any ac-
cented word in the version of the French refer-
ence in memory; previous LMERT experiments
had English as target language, so the bug hadn’t
showed up. The bug didn’t affect characters in
the 7-bit ASCII set, such as English ones, only
accented characters. Words in candidate transla-
tions were not garbled, so correct translations
with accents received a lower BLEU score than
they should have. As Table 1 shows, this bug
cost us about 0.5 BLEU for WMT 2010 E-F after
rescoring (according to NRC’s internal version
of BLEU, which differs slightly from WMT’s
BLEU). Despite this bug, the system tuned with
buggy LMERT (and submitted) was still better
than the best system we obtained with N-best
MERT. The bug didn’t affect F-E scores.
</bodyText>
<table confidence="0.99732475">
Dev WMT2009 WMT2010
LMERT (bug) 25.26 26.85 27.55
LMERT 25.43 26.89 28.07
(no bug)
</table>
<tableCaption confidence="0.999861">
Table 1: LMERT bug fix (E-F BLEU after rescoring)
</tableCaption>
<subsectionHeader confidence="0.994169">
4.2 Fixing odd translations
</subsectionHeader>
<bodyText confidence="0.999741891891892">
After the evaluation, we carefully studied the
system outputs on the WMT 2010 test data, par-
ticularly for E-F. Apart from truecasing errors,
we noticed two kinds of bad behaviour: transla-
tions of proper names and apparent passthrough
of English words to the French side.
Examples of E-F translations of proper names
from our WMT 2010 submission (each from a
different sentence):
Mr. Onderka → M. Roman, Lukáš Marvan → G.
Lukáš, Janey → The, Janette Tozer → Janette,
Aysel Tugluk → joints tugluk, Tawa Hallae →
Ottawa, Oleson → production, Alcobendas → ;
When the LMERT bug was fixed, some but
not all of these bad translations were corrected
(e.g., 3 of the 8 examples above were corrected).
Our system passes OOV words through un-
changed. Thus, the names above aren’t OOVs,
but words that occur rarely in the training data,
and for which bad alignments have a dispropor-
tionate effect. We realized that when a source
word begins with a capital, that may be a signal
that it should be passed through. We thus de-
signed a passthrough feature function that applies
to all capitalized forms not at the start of a sen-
tence (and also to forms at the sentence start if
they’re capitalized elsewhere). Sequences of one
or more capitalized forms are grouped into a
phrase suggestion (e.g., Barack Obama → bar-
rack obama) which competes with phrase table
entries and is assigned a weight by MERT.
The passthrough feature function yields a tiny
improvement over the E-F system with the bug-
fixed LMERT on the dev corpus (WMT2008):
+0.06 BLEU (without rescoring). It yields a larg-
er improvement on our test corpus: +0.27 BLEU
(without rescoring). Furthermore, it corrects all
the examples from the WMT 2010 test shown
above (after the LMERT bug fix 5 of the 8 ex-
amples above still had problems, but when the
passthrough function is incorporated all of them
go away). Though the BLEU gain is small, we
are happy to have almost eradicated this type of
error, which human beings find very annoying.
The opposite type of error is apparent pass-
through. For instance, “we’re” appeared 12 times
in the WMT 2010 test data, and was translated 6
times into French as “we’re” - even though better
translations had higher forward probabilities. The
source of the problem is the backward probabili-
ty P(E=“we’re”|F=“we’re”), which is 1.0; the
backward probabilities for valid French transla-
tions of “we’re” are lower. Because of the high
probability P(E=“we’re”|F=“we’re”) within the
loglinear combination, the decoder often chooses
“we’re” as the French translation of “we’re”.
The (E=“we’re”, F=“we’re”) pair in WMT
2010 phrase tables arose from two sentence pairs
where the “French” translation of an English sen-
tence is a copy of that English sentence. In both,
the original English sentence contains “we’re”.
Naturally, the English words on the “French”
side are word-aligned with their identical twins
on the English side. Generally, if the training
data has sentence pairs where the “French” sen-
tence contains words from the English sentence,
those words will get high backward probabilities
of being translated as themselves. This problem
may not show up as an apparent passthrough;
instead, it may cause MERT to lower the weight
of the backward probability component, thus
hurting performance.
We estimated English contamination of the
French side of the parallel training data by ma-
</bodyText>
<page confidence="0.992827">
130
</page>
<bodyText confidence="0.999846333333334">
nually inspecting a random sample of “French”
sentences containing common English function
words. Manual inspection is needed for accurate
estimation: a legitimate French sentence might
contain mostly English words if, e.g., it is short
and cites the title of an English work (this
wouldn’t count as contamination). The degree of
contamination is roughly 0.05% for Europarl,
0.5% for news-commentary, 0.5% for UN, and
1% for GigaFrEn (in these corpora the French is
also contaminated by other languages, particular-
ly German). Foreign contamination of English
for these corpora appears to be much less fre-
quent.
Contamination can take strange forms. We ex-
pected to see English sentences copied over in-
tact to the French side, and we did, but we did
not expect to see so many “French” sentences
that interleaved short English word sequences
with short French word sequences, apparently
because text with an English and a French col-
umn had been copied by taking lines from alter-
nate columns. We found many of these inter-
leaved “French” sentences, and found some of
them in exactly this form on the Web (i.e., the
corruption didn’t occur during WMT data collec-
tion). The details may not matter: whenever the
“French” training sentence contains words from
its English twin, there can be serious damage via
backward probabilities.
To test this hypothesis, we filtered all parallel
and monolingual training data for the E-F system
with a language guessing tool called text_cat
(Cavnar and Trenkle, 1994). From parallel data,
we filtered out sentence pairs whose French side
had a high probability of not being French; from
LM training data, sentences with a high non-
French probability. We set the filtering level by
inspecting the guesser’s assessment of news-
commentary sentences, choosing a rather aggres-
sive level that eliminated 0.7% of news-
commentary sentence pairs. We used the same
level to filter Europarl (0.8% of sentence pairs
removed), UN (3.4%), GigaFrEn (4.7%), and
“mono” (4.3% of sentences).
</bodyText>
<table confidence="0.998041">
Dev WMT2009 WMT2010
Baseline 25.23 26.47 27.72
Filtered 25.45 26.66 27.98
</table>
<tableCaption confidence="0.999087">
Table 2: Data filtering (E-F BLEU, no rescoring)
</tableCaption>
<bodyText confidence="0.9958485">
Table 2 shows the results: a small but consis-
tent gain (about +0.2 BLEU without rescoring).
We have not yet confirmed the hypothesis that
copies of source-language words in the paired
target sentence within training data can damage
system performance via backward probabilities.
</bodyText>
<subsectionHeader confidence="0.999426">
4.3 Fixing problems with LM training
</subsectionHeader>
<bodyText confidence="0.996194928571429">
Post-evaluation, we realized that our arrange-
ment of the training data for the LMs for both
language directions was flawed. The grouping
together of disparate corpora in “mono” and
“domain” didn’t allow higher-quality, truly in-
domain corpora to be weighted more heavily
(e.g., the news corpora should have higher
weights than Europarl, but they are lumped to-
gether in “mono”). There are also potentially
harmful overlaps between LMs (e.g., GigaFrEn
is used both inside and outside the dynamic LM).
We trained a new set of French LMs for the E-
F system, which replaced all the French LMs
(#5-7) described in section 2.3 in the E-F system:
</bodyText>
<listItem confidence="0.996659">
1. 5-gram LM trained on news-commentary
and shuffle;
2. Dynamic LM based on 4 5-gram LMs
</listItem>
<bodyText confidence="0.999383133333333">
trained on French side of parallel data
(LM trained on GigaFrEn, LM on UN,
LM on Europarl, and LM on news-
commentary).
We did not apply the passthrough function or
language filtering (section 4.2) to any of the
training data for any component (LMs, TMs, dis-
tortion models) of this system; we did use the
bug-fixed version of LMERT (section 4.1).
The experiments with these new French LMs
for the E-F system yielded a small decrease of
NRC BLEU on dev (-0.15) and small increases
on WMT Newstest 2009 and Newstest 2010
(+0.2 and +0.4 respectively without rescoring).
We didn’t do F-E experiments of this type.
</bodyText>
<subsectionHeader confidence="0.9989">
4.4 Pooling improvements
</subsectionHeader>
<bodyText confidence="0.999970125">
The improvements above were (individual un-
cased E-F BLEU gains without rescoring in
brackets): LMERT bug fix (about +0.5); pass-
through feature function (+0.1-0.3); language
filtering for French (+0.2). There was also a
small gain on test data by rearranging E-F LM
training data, though the loss on “dev” suggests
this may be a statistical fluctuation. We built
these four improvements into the evaluation E-F
system, along with quote normalization: in all
training and test data, diverse single quotes were
mapped onto the ascii single quote, and diverse
double quotes were mapped onto the ascii double
quote. The average result on WMT2009 and
WMT2010 was +1.7 BLEU points compared to
the original system, so there may be synergy be-
</bodyText>
<page confidence="0.99523">
131
</page>
<bodyText confidence="0.999922692307692">
tween the improvements. The original system
had gained +0.3 from rescoring, while the final
improved system only gained +0.1 from rescor-
ing: a post-evaluation rescored gain of +1.5.
An experiment in which we dropped lexica-
lized distortion from the improved system
showed that this component yields about +0.2
BLEU. Much earlier, when we were still training
systems with N-best MERT, incorporation of the
6-feature lexicalized distortion often caused
scores to go down (by as much as 2.8 BLEU).
This illustrates how LMERT can make incorpo-
ration of many more features worthwhile.
</bodyText>
<subsectionHeader confidence="0.998096">
4.5 Fixing truecasing
</subsectionHeader>
<bodyText confidence="0.999905894736842">
Our truecaser doesn’t work as well as truecasers
of other WMT groups: we lost 1.4 BLEU by tru-
ecasing in both language directions, while others
lost 1.0 or less. To improve our truecaser, we
tried: 1. Training it on all relevant data and 2.
Collecting 3-gram case-pattern statistics instead
of unigrams. Neither of these helped significant-
ly. One way of improving the truecaser would be
to let case information from source words influ-
ence the case of the corresponding target words.
Alternatively, one of the reviewers stated that
several labs involved in WMT have no separate
truecaser and simply train on truecase text. We
had previously tried this approach for NIST Chi-
nese-English and discarded it because of its poor
performance. We are currently re-trying it on
WMT data; if it works better than having a sepa-
rate truecaser, this was yet another area where
lessons from Chinese-English were misleading.
</bodyText>
<sectionHeader confidence="0.998859" genericHeader="conclusions">
5 Lessons
</sectionHeader>
<bodyText confidence="0.999983944444444">
LMERT is an improvement over N-best MERT.
The submitted system was one for which N-best
MERT happened to work very badly, so we got
ridiculously large gains of +1.5-3.5 BLEU for
non-buggy LMERT over N-best MERT. These
results are outliers: in experiments with similar
configurations, we typically get +0.2-1.0 for
LMERT over N-best MERT. Post-evaluation,
four minor improvements – a case-based pass-
through function, language filtering, LM rear-
rangement, and quote normalization – collective-
ly gave a nice improvement. Nothing we tried
helped truecaser performance significantly,
though we have some ideas on how to proceed.
We learned some lessons from WMT 2010.
Always test your system on the relevant lan-
guage pair. Our original version of LMERT was
developed on Chinese-English and worked well
there, but had a bug that surfaced only when the
target language had accents.
European language pairs are more porous to
information than Chinese-English. Our WMT
system reflected design decisions for Chinese-
English, and thus didn’t exploit case information
in the source: it passed through OOVs to the tar-
get, but didn’t pass through upper-case words
that are likely to be proper nouns.
It is beneficial to remove foreign-language
contamination from the training data.
When entering an evaluation one hasn’t parti-
cipated in for several years, always read system
papers from the previous year. Some of the
WMT 2008 system papers mention passthrough
of some non-OOVs, filtering out of noisy train-
ing data, and using the case of a source word to
predict the case of the corresponding target word.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999832375">
William Cavnar and John Trenkle. 1994. N-Gram-
Based Text Categorization. Proc. Symposium on
Document Analysis and Information Retrieval,
UNLV Publications/Reprographics, pp. 161-175.
Daniel Cer, Daniel Jurafsky, and Christopher D.
Manning. 2008. Regularization and search for min-
imum error rate training. Proc. Workshop on
SMT, pp. 26-34.
George Foster and Roland Kuhn. 2009. Stabilizing
Minimum Error Rate Training. Proc. Workshop
on SMT, pp. 242-249.
George Foster and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. Proc. Workshop on
SMT, pp. 128-135.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. Proc. ACL, pp. 144-151.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Transcription Evalua-
tion. MT Eval. Workshop.
Wolfgang Macherey, Franz Josef Och, Ignacio Thay-
er, and Jakob Uszkoreit. 2008. Lattice-based Min-
imum Error Rate Training for Statistical Machine-
Translation. Conf. EMNLP, pp. 725-734.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. Proc. ACL,
pp. 160-167.
Richard Zens and Hermann Ney. 2004. Improvements
in Phrase-Based Statistical Machine Translation.
Proc. HLT/NAACL, pp. 257-264.
</reference>
<page confidence="0.997729">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.335713">
<title confidence="0.48791">Lessons from NRC’s Portage System at WMT 2010</title>
<author confidence="0.840127">Samuel Larkin</author>
<author confidence="0.840127">Boxing Chen</author>
<author confidence="0.840127">George Foster</author>
<author confidence="0.840127">Ulrich Germann</author>
<author confidence="0.840127">Eric Howard Johnson</author>
<author confidence="0.840127">Roland</author>
<affiliation confidence="0.999492">National Research Council of Canada</affiliation>
<address confidence="0.970312">Gatineau, Québec, Canada.</address>
<email confidence="0.977202">Firstname.Lastname@cnrc-nrc.gc.ca</email>
<abstract confidence="0.999217117647059">NRC’s Portage system participated in the English-French (E-F) and French-English (F-E) translation tasks of the ACL WMT 2010 evaluation. The most notable improvement over earlier versions of Portage is an efficient implementation of lattice MERT. While Portage has typically performed well in Chinese to English MT evaluations, most recently in the NIST09 evaluation, our participation in WMT 2010 revealed some interesting differences between Chinese-English and E-F/F-E translation, and alerted us to certain weak spots in our system. Most of this paper discusses the problems we found in our system and ways of fixing them. We learned several lessons that we think will be of general interest.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William Cavnar</author>
<author>John Trenkle</author>
</authors>
<title>N-GramBased Text Categorization.</title>
<date>1994</date>
<booktitle>Proc. Symposium on Document Analysis and Information Retrieval, UNLV Publications/Reprographics,</booktitle>
<pages>161--175</pages>
<contexts>
<context position="17869" citStr="Cavnar and Trenkle, 1994" startWordPosition="2885" endWordPosition="2888">, apparently because text with an English and a French column had been copied by taking lines from alternate columns. We found many of these interleaved “French” sentences, and found some of them in exactly this form on the Web (i.e., the corruption didn’t occur during WMT data collection). The details may not matter: whenever the “French” training sentence contains words from its English twin, there can be serious damage via backward probabilities. To test this hypothesis, we filtered all parallel and monolingual training data for the E-F system with a language guessing tool called text_cat (Cavnar and Trenkle, 1994). From parallel data, we filtered out sentence pairs whose French side had a high probability of not being French; from LM training data, sentences with a high nonFrench probability. We set the filtering level by inspecting the guesser’s assessment of newscommentary sentences, choosing a rather aggressive level that eliminated 0.7% of newscommentary sentence pairs. We used the same level to filter Europarl (0.8% of sentence pairs removed), UN (3.4%), GigaFrEn (4.7%), and “mono” (4.3% of sentences). Dev WMT2009 WMT2010 Baseline 25.23 26.47 27.72 Filtered 25.45 26.66 27.98 Table 2: Data filterin</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William Cavnar and John Trenkle. 1994. N-GramBased Text Categorization. Proc. Symposium on Document Analysis and Information Retrieval, UNLV Publications/Reprographics, pp. 161-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Regularization and search for minimum error rate training.</title>
<date>2008</date>
<booktitle>Proc. Workshop on SMT,</booktitle>
<pages>26--34</pages>
<contexts>
<context position="9903" citStr="Cer et al., 2008" startWordPosition="1567" endWordPosition="1570">. Figure 1 shows the kind of case where PA works well. The contours of the function being optimized are relatively smooth, facilitating learning of new search directions from gradients. Figure 2 shows a more difficult case: there is a single optimum, but noise dominates and PA has difficulty finding new directions. Search often iterates over the original co-ordinates, missing optima that are nearby but in directions not discoverable from local gradients. Probes in random directions can do better than iteration over the same directions (this is similar to the method proposed for N-best MERT by Cer et al., 2008). Each 1-dimensional MERT optimization is exact, so if our probe stabs a region with better scores, it will be discovered. Figures 1 and 2 only hint at the problem: in reality, 2-dimensional search isn’t a problem. The difficulties occur as the dimension grows: in high dimensions, it is more important to get good directions and they are harder to find. For WMT 2010, we crafted a compromise with the best properties of PA, yet allowing for a more aggressive search in more directions. We start with PA. As long as PA is adding new direction vectors, it is continued. When PA stops adding new direct</context>
</contexts>
<marker>Cer, Jurafsky, Manning, 2008</marker>
<rawString>Daniel Cer, Daniel Jurafsky, and Christopher D. Manning. 2008. Regularization and search for minimum error rate training. Proc. Workshop on SMT, pp. 26-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Stabilizing Minimum Error Rate Training.</title>
<date>2009</date>
<booktitle>Proc. Workshop on SMT,</booktitle>
<pages>242--249</pages>
<contexts>
<context position="6962" citStr="Foster and Kuhn, 2009" startWordPosition="1089" endWordPosition="1092">alf of GigaFrEn; 7. Dynamic LM composed of 4 LMs, each trained on the French half of a parallel corpus (5-gram LM trained on “domain”, 4-gram LM on GigaFrEn, 5-gram LM on news-commentary and 5-gram LM on UN). The F-E system is a mirror image of the E-F system. 3 Details of lattice MERT (LMERT) Our system’s implementation of LMERT (Macherey et al., 2008) is the most notable recent change in our system. As more and more features are included in the loglinear model, especially if they are correlated, N-best MERT (Och, 2003) shows more and more instability, because of convergence to local optima (Foster and Kuhn, 2009). We had been looking for methods that promise more stability and better convergence. LMERT seemed to fit the bill. It optimizes over the complete lattice of candidate translations after a decoding run. This avoids some of the problems of N-best lists, which lack variety, leading to poor local optima and the need for many decoder runs. Though the algorithm is straightforward and is highly parallelizable, attention must be paid to space and time resource issues during implementation. Lattices output by our decoder were large and needed to be shrunk dramatically for the algorithm to function wel</context>
</contexts>
<marker>Foster, Kuhn, 2009</marker>
<rawString>George Foster and Roland Kuhn. 2009. Stabilizing Minimum Error Rate Training. Proc. Workshop on SMT, pp. 242-249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>MixtureModel Adaptation for SMT.</title>
<date>2007</date>
<booktitle>Proc. Workshop on SMT,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="3315" citStr="Foster and Kuhn, 2007" startWordPosition="518" endWordPosition="521">kward conditional probabilities. The lexicalized distortion probabilities are also obtained by adding IBM2 and HMM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, cont</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. MixtureModel Adaptation for SMT. Proc. Workshop on SMT, pp. 128-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest Rescoring: Faster Decoding with Integrated Language Models.</title>
<date>2007</date>
<booktitle>Proc. ACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="3428" citStr="Huang and Chiang, 2007" startWordPosition="535" endWordPosition="538">MM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, contiguous phrases can always be swapped. Out-of-vocabulary (OOV) source words are passed through unchanged to the ta</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. Proc. ACL, pp. 144-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>IWSLT Speech Transcription Evaluation.</title>
<date>2005</date>
<booktitle>Edinburgh System Description for the</booktitle>
<publisher>MT Eval. Workshop.</publisher>
<contexts>
<context position="3011" citStr="Koehn et al., 2005" startWordPosition="471" endWordPosition="474">lse dynamic mixtures. Each phrase table used was a merged one, created by separately training an IBM2-based and an HMM-based joint count table on the same data and then adding the counts. Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. The lexicalized distortion probabilities are also obtained by adding IBM2 and HMM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machi</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Transcription Evaluation. MT Eval. Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Josef Och</author>
<author>Ignacio Thayer</author>
<author>Jakob Uszkoreit</author>
</authors>
<date>2008</date>
<booktitle>Lattice-based Minimum Error Rate Training for Statistical MachineTranslation. Conf. EMNLP,</booktitle>
<pages>725--734</pages>
<contexts>
<context position="4129" citStr="Macherey et al., 2008" startWordPosition="643" endWordPosition="646">tion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, contiguous phrases can always be swapped. Out-of-vocabulary (OOV) source words are passed through unchanged to the target. Loglinear weights are tuned with Och&apos;s max-BLEU algorithm over lattices (Macherey et al., 2008); more details about lattice MERT are given in the next section. The second pass rescores 1000-best lists produced by the first pass, with additional features including various LM and IBM-model probabilities; ngram, length, and reordering posterior probabilities and frequencies; and quote and parenthesis mismatch indicators. To improve the quality of the maxima found by MERT when using large sets of partially-overlapping rescoring features, we use greedy feature selection, first expanding from a baseline set, then pruning. We restricted our training data to data that was directly available thr</context>
<context position="6695" citStr="Macherey et al., 2008" startWordPosition="1044" endWordPosition="1048"> results. E-F system components: 1. Phrase table trained on “domain”; 2. Phrase table trained on GigaFrEn; 3. Lexicalized distortion model trained on “domain”; 4. Distance-based distortion model; 5. 5-gram French LM trained on “mono”; 6. 4-gram LM trained on French half of GigaFrEn; 7. Dynamic LM composed of 4 LMs, each trained on the French half of a parallel corpus (5-gram LM trained on “domain”, 4-gram LM on GigaFrEn, 5-gram LM on news-commentary and 5-gram LM on UN). The F-E system is a mirror image of the E-F system. 3 Details of lattice MERT (LMERT) Our system’s implementation of LMERT (Macherey et al., 2008) is the most notable recent change in our system. As more and more features are included in the loglinear model, especially if they are correlated, N-best MERT (Och, 2003) shows more and more instability, because of convergence to local optima (Foster and Kuhn, 2009). We had been looking for methods that promise more stability and better convergence. LMERT seemed to fit the bill. It optimizes over the complete lattice of candidate translations after a decoding run. This avoids some of the problems of N-best lists, which lack variety, leading to poor local optima and the need for many decoder r</context>
<context position="8814" citStr="Macherey et al., 2008" startWordPosition="1385" endWordPosition="1389">te lattices from the decoder and optimize over the complete 128 development set for NIST09 Chinese-English. However, combining lattices between decoder runs again resulted in excessive memory requirements. We achieved acceptable performance by searching only the lattice from the latest decoder run; perhaps information from earlier runs, though critical for convergence in N-best MERT, isn’t as important for LMERT. Until a reviewer suggested it, we had not thought of pruning lattices to a specified graph density as a solution for our memory problems. This is referred to in a single sentence in (Macherey et al., 2008), which does not specify its implementation or its impact on performance, and is an option of OpenFst (we didn’t use OpenFst). We will certainly experiment with lattice pruning in future. Powell&apos;s algorithm (PA), which is at the core of MERT, has good convergence when features are mostly independent and do not depart much from a simple coordinate search; it can run into problems when there are many correlated features (as with multiple translation and language models). Figure 1 shows the kind of case where PA works well. The contours of the function being optimized are relatively smooth, facil</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>Wolfgang Macherey, Franz Josef Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based Minimum Error Rate Training for Statistical MachineTranslation. Conf. EMNLP, pp. 725-734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training</title>
<date>2003</date>
<booktitle>in Statistical Machine Translation. Proc. ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="6866" citStr="Och, 2003" startWordPosition="1076" endWordPosition="1077">rtion model; 5. 5-gram French LM trained on “mono”; 6. 4-gram LM trained on French half of GigaFrEn; 7. Dynamic LM composed of 4 LMs, each trained on the French half of a parallel corpus (5-gram LM trained on “domain”, 4-gram LM on GigaFrEn, 5-gram LM on news-commentary and 5-gram LM on UN). The F-E system is a mirror image of the E-F system. 3 Details of lattice MERT (LMERT) Our system’s implementation of LMERT (Macherey et al., 2008) is the most notable recent change in our system. As more and more features are included in the loglinear model, especially if they are correlated, N-best MERT (Och, 2003) shows more and more instability, because of convergence to local optima (Foster and Kuhn, 2009). We had been looking for methods that promise more stability and better convergence. LMERT seemed to fit the bill. It optimizes over the complete lattice of candidate translations after a decoding run. This avoids some of the problems of N-best lists, which lack variety, leading to poor local optima and the need for many decoder runs. Though the algorithm is straightforward and is highly parallelizable, attention must be paid to space and time resource issues during implementation. Lattices output </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. Proc. ACL, pp. 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<date>2004</date>
<booktitle>Improvements in Phrase-Based Statistical Machine Translation. Proc. HLT/NAACL,</booktitle>
<pages>257--264</pages>
<contexts>
<context position="2674" citStr="Zens and Ney, 2004" startWordPosition="416" endWordPosition="419">g data The NRC system uses a standard two-pass phrase-based approach. Major features in the first-pass loglinear model include phrase tables derived from symmetrized IBM2 alignments and symmetrized HMM alignments, a distance-based distortion model, a lexicalized distortion model, and language models (LMs) that can be either static or else dynamic mixtures. Each phrase table used was a merged one, created by separately training an IBM2-based and an HMM-based joint count table on the same data and then adding the counts. Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. The lexicalized distortion probabilities are also obtained by adding IBM2 and HMM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source tex</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in Phrase-Based Statistical Machine Translation. Proc. HLT/NAACL, pp. 257-264.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>