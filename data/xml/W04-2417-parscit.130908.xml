<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000197">
<title confidence="0.991536">
A transformation-based approach to argument labeling
</title>
<author confidence="0.915337">
Derrick Higgins
</author>
<affiliation confidence="0.648872">
Educational Testing Service
</affiliation>
<address confidence="0.641984333333333">
Mail Stop 12-R
Rosedale Road
Princeton, NJ 08541
</address>
<email confidence="0.996697">
dhiggins@ets.org
</email>
<sectionHeader confidence="0.995596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999771764705882">
This paper presents the results of applying
transformation-based learning (TBL) to the
problem of semantic role labeling. The great
advantage of the TBL paradigm is that it pro-
vides a simple learning framework in which the
parallel tasks of argument identification and ar-
gument labeling can mutually influence one an-
other. Semantic role labeling nevertheless dif-
fers from other tasks in which TBL has been
successfully applied, such as part-of-speech
tagging and named-entity recognition, because
of the large span of some arguments, the de-
pendence of argument labels on global infor-
mation, and the fact that core argument labels
are largely arbitrary. Consequently, some care
is needed in posing the task in a TBL frame-
work.
</bodyText>
<sectionHeader confidence="0.820271" genericHeader="method">
1 O verview
</sectionHeader>
<bodyText confidence="0.999961533333333">
In the closed challenge of the CoNLL shared task, the
system is charged with both identifying argument bound-
aries, and correctly labeling the arguments with the cor-
rect semantic role, without using a parser to suggest
candidate phrases. Transformation-based learning (Brill,
1995) is well-suited to simultaneously addressing this
dual task of identifying and labeling semantic arguments
of a predicate, because it allows intermediate hypothe-
ses to influence the ultimate decisions made. More con-
cretely, the category of an argument may decisively in-
fluence how the system places its boundaries, and con-
versely, the shape of an argument is an important factor
in predicting its category.
We treat the task as a word-by-word tagging problem,
using a variant of the IOB2 labeling scheme.
</bodyText>
<sectionHeader confidence="0.989241" genericHeader="method">
2 Transformation-based learning
</sectionHeader>
<bodyText confidence="0.998221777777778">
TBL is a general machine learning tool for assigning
classes to a sequence of observations. TBL induces a
set of transformational rules, which apply in sequence to
change the class assigned to observations which meet the
rules’ conditions.
We use the software package fnTBL to design
the model described here. This package, and the
TBL framework itself, are described in detail by
Ngai and Florian (2001).
</bodyText>
<sectionHeader confidence="0.992501" genericHeader="method">
3 Task Definition
</sectionHeader>
<bodyText confidence="0.999939818181818">
Defining the task of semantic role labeling in TBL terms
requires four basic steps. First, the problem has to be re-
duced to that of assigning an appropriate tag to each word
in a sentence. Second, we must define the features asso-
ciated with each word in the sentence, on which the trans-
formational rules will operate. Third, we must decide on
the exact forms the transformational rules will be allowed
to take (the rule templates). Finally, we must determine
a mapping from our word-by-word tag assignment to the
labeled bracketing used to identify semantic arguments in
the test data. Each of these steps is addressed below.
</bodyText>
<subsectionHeader confidence="0.999922">
3.1 Tagging scheme
</subsectionHeader>
<bodyText confidence="0.999723393939394">
The simplest way of representing the chunks of text
which correspond to semantic arguments is to use
some variant of the IOB tagging scheme (Sang and
Veenstra, 1999). This is the approach taken by
Hacioglu et al. (2003), who apply the IOB2 tagging
scheme in their word-by-word models, as shown in the
second row of Figure 1.
However, two aspects of the problem at hand make this
tag assignment difficult to use for TBL. First, semantic
argument chunks can be very large in size. An argu-
ment which contains a relative clause, for example, can
easily be longer than 20 words. Second, the label an ar-
gument is assigned is largely arbitrary, in the sense that
core argument labels (A0, A1, etc.) generally cannot be
assigned without some information external to the con-
stituent, such as the class of the predicate, or the identity
of other arguments which have already been assigned. So
using the IOB2 format, it might take a complicated se-
quence of TBL rules to completely re-tag, say, an A0 ar-
gument as A1. If this re-tagging is imperfectly achieved,
we are left with the difficult decision of how to interpret
the stranded I-A0 elements, and the problem that they
may incorrectly serve as an environment for other trans-
formational rules.
For this reason, we adopt a modified version of the
IOB2 scheme which is a compromise between addressing
the tasks of argument identification and argument label-
ing. The left boundary (B) tags indicate the label of the
argument, but the internal (I) tags are non-specific as to
argument label, as in the last row of Figure 1. This al-
lows a a single TBL rule to re-label an argument, while
still allowing for interleaving of TBL rules which affect
argument identification and labeling.
</bodyText>
<subsectionHeader confidence="0.998825">
3.2 Feature Coding
</subsectionHeader>
<bodyText confidence="0.994325775510204">
With each word in a sentence, we associate the following
features:
Word The word itself, normalized to lower-case.
Tag The word’s part-of-speech tag, as predicted by the
system of Gim´enez and M`arquez (2003).
Chunk The chunk label of the word, as predicted by the
system of Carreras and M`arquez (2003).
Entity The named-entity label of the word, as predicted
by the system of Chieu and Ng (2003).
L/R A feature indicating whether the word is to the left
(L) or right (R) of the target verb.
Indent This feature indicates the clause level of the cur-
rent word with respect to the target predicate. Us-
ing the clause boundaries predicted by the system
of Carreras and M`arquez (2003), we compute a fea-
ture based on the linguistic notion of c-command.&apos;
If both the predicate and the current word are in
the same basic clause, Indent=0. If the predicate c-
commands the current word, and the current word is
one clause level lower, Indent=1. If it is two clause
levels lower, Indent=2, and so on. If the c-command
relations are reversed, the indent levels are negative,
and if neither c-commands the other, Indent=‘NA’.
(Figure 2 illustrates how this feature is defined.) The
absolute value of the Indent feature is not permitted
to exceed 5.
is-PP A boolean feature indicating whether the word is
included within a base prepositional phrase. This is
&apos;A node a (reflexively) c-commands a node 3 iff there is a
node -y such that -y directly dominates a, and -y dominates 3.
Note that only clauses (S nodes) are considered in our applica-
tion described above.
true if its chunk tag is B-PP or I-PP, or if it is within
an NP chunk directly following a PP chunk.
PP-head If is-PP is true, this is the head of the preposi-
tional phrase; otherwise it is zero.
N-head The final nominal element of the next NP chunk
at the same indent level as the current word, if it
exists. For purposes of this feature, a possessive NP
chunk is combined with the following NP chunk.
Verb The target predicate under consideration.
V-Tag The POS tag of the target predicate.
V-Passive A boolean feature indicating whether the tar-
get verb is in the passive voice. This is determined
using a simple regular expression over the sentence.
Path As in (Pradhan et al., 2003), this feature is an or-
dered list of the chunk types intervening between the
target verb and the current word, with consecutive
NP chunks treated as one.
</bodyText>
<subsectionHeader confidence="0.997948">
3.3 Rule Templates
</subsectionHeader>
<bodyText confidence="0.9999725">
In order to define the space of rules searched by the TBL
algorithm, we must specify a set of rule templates, which
determine the form transformational rules may take. The
rule templates used in our system are 130 in number, and
fall into a small number of classes, as described below.
These rules all take the form f1 ... fn —&gt; label,11,
where f1 through fn are features of the current word w or
words in its environment, and usually include the current
(semantic argument) label assigned to w. The categoriza-
tion of rule templates below, then, basically amounts to a
list of the different feature sets which are used to predict
the argument label of each word.
The initial assignment of tags which is given to the
TBL algorithm is a very simple chunk-based assignment.
Every word is given the tag O (outside all semantic argu-
ments), except if it is within an NP chunk at Indent level
zero. In that case, the word is assigned the tag I if its
chunk label is I-NP, B-A0 if its chunk label is B-NP and
it is to the left of the verb, and B-A1 if its chunk label is
B-NP and it is to the right of the verb.
</bodyText>
<subsectionHeader confidence="0.638218">
3.3.1 Basic rules (10 total)
</subsectionHeader>
<bodyText confidence="0.99967">
The simplest class of rules simply change the current
word’s argument label based on its own local features,
including the current label, and the features L/R, Indent,
and Chunk.
</bodyText>
<subsectionHeader confidence="0.532311">
3.3.2 Basic rules using local context (29)
</subsectionHeader>
<bodyText confidence="0.99742675">
An expanded set of rules using all features of the cur-
rent word, as well as the argument labels of the current
and previous words. For example, the following rule will
change the label O to I within an NP chunk, if the initial
</bodyText>
<figure confidence="0.569061">
Argument boundaries [A1 The deal] [V collapsed] [AM-TMP on Friday] .
IOB2 [B-A1 The] [I-A1 deal] [B-V collapsed] [B-AM-TMP on] [I-AM-TMP Friday] [O .]
Modified scheme [B-A1 The] [I deal] [B-V collapsed] [B-AM-TMP on] [I Friday] [O .]
</figure>
<figureCaption confidence="0.994661">
Figure 1: Tag assignments for word-by-word semantic role assignment
</figureCaption>
<figure confidence="0.97001">
W V W V
indent = 1 indent = −1 indent = NA
</figure>
<figureCaption confidence="0.985481">
Figure 2: Sample values of Indent feature for different clause embeddings of a word W and target verb V
</figureCaption>
<equation confidence="0.843892375">
V W
portion of the chunk has already been marked as within a
semantic argument:
labelw0 = O
indentw0 = 0
chunkw0 = I-NP —&gt; labelw0 = I.
L/Rw0 = R
labelw_1 = I
</equation>
<subsectionHeader confidence="0.835866">
3.3.3 Lexically conditioned rules (14)
</subsectionHeader>
<bodyText confidence="0.9999465">
These rules change the argument label of the current
word based on the Word feature of the current or sur-
rounding words, in combination with argument labels and
chunk labels from the surrounding context. For example,
this rule marks the adverb back as a directional modifier
when it follows the target verb:
</bodyText>
<equation confidence="0.946752333333333">
labelw0 = O
chunkw0 = B-ADVP
wordw0 = back —&gt; labelw0 = B-AM-DIR.
labelw_1 = B-V
chunkw_1 = B-VP
3.3.4 Entity (24)
</equation>
<bodyText confidence="0.999766666666667">
These rules further add the named-entity tag of the cur-
rent, preceding, or following word to the basic and local-
context rules above.
</bodyText>
<subsectionHeader confidence="0.617855">
3.3.5 Verb tag (15)
</subsectionHeader>
<bodyText confidence="0.998322">
These rules add the POS tag of the predicate to the
basic and simpler local-context rules above.
</bodyText>
<subsectionHeader confidence="0.462967">
3.3.6 Verb-Noun dependency (9)
</subsectionHeader>
<bodyText confidence="0.998327333333333">
These rules allow the argument label of the current
word to be changed, based on its Verb and N-head fea-
tures,as well as other local features.
</bodyText>
<subsubsectionHeader confidence="0.459226">
3.3.7 Word-Noun dependency (3)
</subsubsectionHeader>
<bodyText confidence="0.9998745">
These rules allow the argument label of the current
word to be changed, based on its Word, N-head, Indent,
L/R, and Chunk features, as well as the argument labels
of adjacent words.
</bodyText>
<subsectionHeader confidence="0.670877">
3.3.8 Long-distance rules (6)
</subsectionHeader>
<bodyText confidence="0.9996337">
Because many of the dependencies involved in the se-
mantic role labeling task hold over the domain of the en-
tire sentence, we include a number of long-distance rules.
These rules allow the argument label to be changed de-
pending on the word’s current label, the features L/R, In-
dent, Verb, and the argument label of a word within 50 or
100 words of the current word. These rules are intended
to support generalizations like “if the current word is la-
beled A0, but there is already an A0 further to the left,
change it to I”.
</bodyText>
<subsectionHeader confidence="0.837892">
3.3.9 “Smoothing” rules (15)
</subsectionHeader>
<bodyText confidence="0.999874444444445">
Finally, there are a number of “smoothing” rules,
which are designed primarily to prevent I tags from
becoming stranded, so that arguments which contain a
large number of words can successfully be identified.
These rules allow the argument label of a word to be
changed based on the argument labels of the previous two
words, the next two words, and the chunk tags of these
words. This sample rule marks a word as being argument-
internal, if both its neighbors are already so marked:
</bodyText>
<equation confidence="0.97571475">
labelw_1 = I
labelw0 = O —&gt; labelw0 = I.
labelw1 = I
3.3.10 Path rules (5)
</equation>
<bodyText confidence="0.9998918">
Finally, we include a number of rule templates using
the highly-specific Path feature. These rules allow the ar-
gument label of a word to be changed based on its current
value, as well as the value of the feature Path in combi-
nation with L/R, Indent, V-Tag, Verb, and Word.
</bodyText>
<subsectionHeader confidence="0.948325">
3.4 Tag interpretation
</subsectionHeader>
<bodyText confidence="0.999980947368421">
The final step in our transformation-based approach to
semantic role labeling is to map the word-by word IOB
tags predicted by the TBL model back to the format of the
original data set, which marks only argument boundaries,
so that we can calculate precision and recall statistics for
each argument type. The simplest method of performing
this mapping is to consider an argument as consisting of
an initial labeled boundary tag (such as B-A0, followed
by zero or more argument-internal (I) tags, ignoring any-
thing which does not conform to this structure (in partic-
ular, strings of Is with no initial boundary marker).
In fact, this method works quite well, and it is used for
the results reported below.
Finally, there is a post-processing step in which adjucts
may be re-labeled if the same sequence of words is found
as an adjunct in the training data, and always bears the
same role. This affected fewer than twenty labels on the
development data, and added only about 0.1 to the overall
f-measure.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999976777777778">
The results on the test section of the CoNLL 2004 data
are presented in Table 1 below. The overall result, an f-
score of 60.66, is considerably below results reported for
systems using a parser on a comparable data set. How-
ever, it is a reasonable result given the simplicity of our
system, which does not make use of the additional infor-
mation found in the PropBank frames themselves.
It is an interesting question to what extent our re-
sults depend on the use of the Path feature (which
Pradhan et al. (2003) found to be essential to their mod-
els’ performance). Since this Path feature is also likely
to be one of the model’s most brittle features, depend-
ing heavily on the accuracy of the syntactic analysis, we
might hope that the system does not depend too heav-
ily on it. In fact, the overall f-score on the development
set drops from 62.75 to 61.33 when the Path feature is
removed, suggestig that it is not essential to our model,
though it does help performance to some extent.
</bodyText>
<sectionHeader confidence="0.972238" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.750236333333333">
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case study
in part-of-speech tagging. Computational Linguistics,
21(4):543–565.
Xavier Carreras and Lluis M`arquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
</bodyText>
<reference confidence="0.953029555555556">
Proceedings of RANLP 2003.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings of CoNLL 2003.
Jes´us Gim´enez and Lluis M`arquez. 2003. Fast and accu-
rate part-of-speech tagging: the SVM approach revis-
ited. In Proceedings ofRANLP 2003.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Dan Jurafsky. 2003. Shallow semantic
</reference>
<table confidence="0.999978620689655">
Precision Recall F0=1
Overall 64.17% 57.52% 60.66
A0 72.48% 68.94% 70.67
A1 63.57% 61.88% 62.72
A2 51.32% 40.90% 45.52
A3 51.58% 32.67% 40.00
A4 36.07% 44.00% 39.64
A5 0.00% 0.00% 0.00
AM-ADV 41.08% 32.25% 36.13
AM-CAU 63.33% 38.78% 48.10
AM-DIR 31.58% 24.00% 27.27
AM-DIS 56.93% 53.99% 55.42
AM-EXT 70.00% 50.00% 58.33
AM-LOC 26.34% 21.49% 23.67
AM-MNR 46.90% 26.67% 34.00
AM-MOD 96.24% 91.10% 93.60
AM-NEG 90.98% 95.28% 93.08
AM-PNC 37.93% 12.94% 19.30
AM-PRD 0.00% 0.00% 0.00
AM-TMP 51.81% 38.42% 44.12
R-A0 82.00% 77.36% 79.61
R-A1 78.26% 51.43% 62.07
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AM-LOC 50.00% 25.00% 33.33
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 100.00% 7.14% 13.33
V 98.15% 98.15% 98.15
</table>
<tableCaption confidence="0.999821">
Table 1: Results on test set: closed challenge
</tableCaption>
<bodyText confidence="0.716541166666667">
parsing using support vector machines. Technical Re-
port CSLR-2003-01, Center for Spoken Language Re-
search, University of Colorado at Boulder.
Grace Ngai and Radu Florian. 2001. Transformation-
based learning in the fast lane. In Proceedings of
NAACL 2001, pages 40–47, June.
</bodyText>
<reference confidence="0.990755">
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2003. Support vector learning for semantic argument
classification. Technical Report CSLR-2003-03, Cen-
ter for Spoken Language Research, University of Col-
orado at Boulder.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In Proceedings of EACL 1999,
pages 173–179.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.011584">
<title confidence="0.999656">A transformation-based approach to argument labeling</title>
<author confidence="0.811146">Derrick</author>
<affiliation confidence="0.622556">Educational Testing</affiliation>
<address confidence="0.526559666666667">Mail Stop Rosedale Princeton, NJ</address>
<email confidence="0.990128">dhiggins@ets.org</email>
<abstract confidence="0.992994447098977">This paper presents the results of applying transformation-based learning (TBL) to the problem of semantic role labeling. The great advantage of the TBL paradigm is that it provides a simple learning framework in which the parallel tasks of argument identification and argument labeling can mutually influence one another. Semantic role labeling nevertheless differs from other tasks in which TBL has been successfully applied, such as part-of-speech tagging and named-entity recognition, because of the large span of some arguments, the dependence of argument labels on global information, and the fact that core argument labels are largely arbitrary. Consequently, some care is needed in posing the task in a TBL framework. 1 O verview In the closed challenge of the CoNLL shared task, the system is charged with both identifying argument boundaries, and correctly labeling the arguments with the correct semantic role, without using a parser to suggest candidate phrases. Transformation-based learning (Brill, 1995) is well-suited to simultaneously addressing this dual task of identifying and labeling semantic arguments of a predicate, because it allows intermediate hypotheses to influence the ultimate decisions made. More concretely, the category of an argument may decisively influence how the system places its boundaries, and conversely, the shape of an argument is an important factor in predicting its category. We treat the task as a word-by-word tagging problem, a variant of the scheme. 2 Transformation-based learning TBL is a general machine learning tool for assigning classes to a sequence of observations. TBL induces a set of transformational rules, which apply in sequence to change the class assigned to observations which meet the rules’ conditions. use the software package design the model described here. This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). 3 Task Definition Defining the task of semantic role labeling in TBL terms requires four basic steps. First, the problem has to be reduced to that of assigning an appropriate tag to each word in a sentence. Second, we must define the features associated with each word in the sentence, on which the transformational rules will operate. Third, we must decide on the exact forms the transformational rules will be allowed take (the Finally, we must determine a mapping from our word-by-word tag assignment to the labeled bracketing used to identify semantic arguments in the test data. Each of these steps is addressed below. 3.1 Tagging scheme The simplest way of representing the chunks of text which correspond to semantic arguments is to use some variant of the IOB tagging scheme (Sang and Veenstra, 1999). This is the approach taken Hacioglu et al. (2003), who apply the IOB2 tagging scheme in their word-by-word models, as shown in the second row of Figure 1. However, two aspects of the problem at hand make this tag assignment difficult to use for TBL. First, semantic argument chunks can be very large in size. An argument which contains a relative clause, for example, can easily be longer than 20 words. Second, the label an argument is assigned is largely arbitrary, in the sense that argument labels etc.) generally cannot be assigned without some information external to the constituent, such as the class of the predicate, or the identity of other arguments which have already been assigned. So the IOB2 format, it might take a complicated seof TBL rules to completely re-tag, say, an aras If this re-tagging is imperfectly achieved, we are left with the difficult decision of how to interpret stranded and the problem that they may incorrectly serve as an environment for other transformational rules. For this reason, we adopt a modified version of the IOB2 scheme which is a compromise between addressing tasks of identification label- The left boundary (B) tags indicate the label of the argument, but the internal (I) tags are non-specific as to argument label, as in the last row of Figure 1. This allows a a single TBL rule to re-label an argument, while still allowing for interleaving of TBL rules which affect argument identification and labeling. 3.2 Feature Coding With each word in a sentence, we associate the following features: word itself, normalized to lower-case. word’s part-of-speech tag, as predicted by the system of Gim´enez and M`arquez (2003). chunk label of the word, as predicted by the system of Carreras and M`arquez (2003). named-entity label of the word, as predicted by the system of Chieu and Ng (2003). feature indicating whether the word is to the left (L) or right (R) of the target verb. feature indicates the clause level of the current word with respect to the target predicate. Using the clause boundaries predicted by the system of Carreras and M`arquez (2003), we compute a feabased on the linguistic notion of If both the predicate and the current word are in same basic clause, If the predicate ccommands the current word, and the current word is clause level lower, If it is two clause lower, and so on. If the c-command relations are reversed, the indent levels are negative, if neither c-commands the other, (Figure 2 illustrates how this feature is defined.) The value of the is not permitted to exceed 5. boolean feature indicating whether the word is included within a base prepositional phrase. This is node c-commands a node there is a that dominates and Note that only clauses (S nodes) are considered in our application described above. if its chunk tag is or if it is within an NP chunk directly following a PP chunk. true, this is the head of the prepositional phrase; otherwise it is zero. final nominal element of the next NP chunk at the same indent level as the current word, if it exists. For purposes of this feature, a possessive NP chunk is combined with the following NP chunk. target predicate under consideration. POS tag of the target predicate. boolean feature indicating whether the target verb is in the passive voice. This is determined using a simple regular expression over the sentence. in (Pradhan et al., 2003), this feature is an ordered list of the chunk types intervening between the target verb and the current word, with consecutive NP chunks treated as one. 3.3 Rule Templates In order to define the space of rules searched by the TBL algorithm, we must specify a set of rule templates, which determine the form transformational rules may take. The rule templates used in our system are 130 in number, and fall into a small number of classes, as described below. rules all take the form ... —&gt; through are features of the current word words in its environment, and usually include the current argument) label assigned to The categorization of rule templates below, then, basically amounts to a list of the different feature sets which are used to predict the argument label of each word. The initial assignment of tags which is given to the TBL algorithm is a very simple chunk-based assignment. word is given the tag all semantic arguexcept if it is within an NP chunk at In that case, the word is assigned the tag its label is its chunk label is is to the left of the verb, and its chunk label is it is to the right of the verb. 3.3.1 Basic rules (10 total) The simplest class of rules simply change the current word’s argument label based on its own local features, the current label, and the features 3.3.2 Basic rules using local context (29) An expanded set of rules using all features of the current word, as well as the argument labels of the current and previous words. For example, the following rule will the label an NP chunk, if the initial Argument boundaries deal] Friday] . IOB2 The]deal] collapsed] on]Friday] Modified scheme collapsed] Figure 1: Tag assignments for word-by-word semantic role assignment W V W V indent = 1 indent = −1 indent = NA 2: Sample values of for different clause embeddings of a word target verb V W portion of the chunk has already been marked as within a semantic argument: = O = 0 = = R I 3.3.3 Lexically conditioned rules (14) These rules change the argument label of the current based on the of the current or surrounding words, in combination with argument labels and chunk labels from the surrounding context. For example, rule marks the adverb a directional modifier when it follows the target verb: = O = = back = 3.3.4 Entity (24) These rules further add the named-entity tag of the current, preceding, or following word to the basic and localcontext rules above. 3.3.5 Verb tag (15) These rules add the POS tag of the predicate to the basic and simpler local-context rules above. 3.3.6 Verb-Noun dependency (9) These rules allow the argument label of the current to be changed, based on its features,as well as other local features. 3.3.7 Word-Noun dependency (3) These rules allow the argument label of the current to be changed, based on its and as well as the argument labels of adjacent words. 3.3.8 Long-distance rules (6) Because many of the dependencies involved in the semantic role labeling task hold over the domain of the entire sentence, we include a number of long-distance rules. These rules allow the argument label to be changed deon the word’s current label, the features Inand the argument label of a word within 50 or 100 words of the current word. These rules are intended to support generalizations like “if the current word is labut there is already an to the left, it to 3.3.9 “Smoothing” rules (15) Finally, there are a number of “smoothing” rules, are designed primarily to prevent from becoming stranded, so that arguments which contain a large number of words can successfully be identified. These rules allow the argument label of a word to be changed based on the argument labels of the previous two words, the next two words, and the chunk tags of these words. This sample rule marks a word as being argumentinternal, if both its neighbors are already so marked: I = O = = I 3.3.10 Path rules (5) Finally, we include a number of rule templates using highly-specific These rules allow the argument label of a word to be changed based on its current as well as the value of the feature combiwith and 3.4 Tag interpretation The final step in our transformation-based approach to semantic role labeling is to map the word-by word IOB tags predicted by the TBL model back to the format of the original data set, which marks only argument boundaries, so that we can calculate precision and recall statistics for each argument type. The simplest method of performing this mapping is to consider an argument as consisting of initial labeled boundary tag (such as followed zero or more argument-internal tags, ignoring anything which does not conform to this structure (in particstrings of with no initial boundary marker). In fact, this method works quite well, and it is used for the results reported below. Finally, there is a post-processing step in which adjucts may be re-labeled if the same sequence of words is found as an adjunct in the training data, and always bears the same role. This affected fewer than twenty labels on the data, and added only about the overall f-measure. 4 Results The results on the test section of the CoNLL 2004 data are presented in Table 1 below. The overall result, an fof is considerably below results reported for systems using a parser on a comparable data set. However, it is a reasonable result given the simplicity of our system, which does not make use of the additional information found in the PropBank frames themselves. It is an interesting question to what extent our redepend on the use of the (which Pradhan et al. (2003) found to be essential to their modperformance). Since this is also likely to be one of the model’s most brittle features, depending heavily on the accuracy of the syntactic analysis, we might hope that the system does not depend too heavily on it. In fact, the overall f-score on the development drops from the is removed, suggestig that it is not essential to our model, though it does help performance to some extent. References Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study part-of-speech tagging. 21(4):543–565. Xavier Carreras and Lluis M`arquez. 2003. Phrase recognition by filtering and ranking with perceptrons. In of RANLP Hai Leong Chieu and Hwee Tou Ng. 2003. Named entity recognition with a maximum entropy approach. In of CoNLL Jes´us Gim´enez and Lluis M`arquez. 2003. Fast and accurate part-of-speech tagging: the SVM approach revis-</abstract>
<note confidence="0.934891479166667">In ofRANLP Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H. Martin, and Dan Jurafsky. 2003. Shallow semantic Precision Recall Overall 64.17% 57.52% 60.66 A0 72.48% 68.94% 70.67 A1 63.57% 61.88% 62.72 A2 51.32% 40.90% 45.52 A3 51.58% 32.67% 40.00 A4 36.07% 44.00% 39.64 A5 0.00% 0.00% 0.00 AM-ADV 41.08% 32.25% 36.13 AM-CAU 63.33% 38.78% 48.10 AM-DIR 31.58% 24.00% 27.27 AM-DIS 56.93% 53.99% 55.42 AM-EXT 70.00% 50.00% 58.33 AM-LOC 26.34% 21.49% 23.67 AM-MNR 46.90% 26.67% 34.00 AM-MOD 96.24% 91.10% 93.60 AM-NEG 90.98% 95.28% 93.08 AM-PNC 37.93% 12.94% 19.30 AM-PRD 0.00% 0.00% 0.00 AM-TMP 51.81% 38.42% 44.12 R-A0 82.00% 77.36% 79.61 R-A1 78.26% 51.43% 62.07 R-A2 100.00% 22.22% 36.36 R-A3 0.00% 0.00% 0.00 R-AM-LOC 50.00% 25.00% 33.33 R-AM-MNR 0.00% 0.00% 0.00 R-AM-PNC 0.00% 0.00% 0.00 R-AM-TMP 100.00% 7.14% 13.33 V 98.15% 98.15% 98.15 Table 1: Results on test set: closed challenge parsing using support vector machines. Technical Report CSLR-2003-01, Center for Spoken Language Research, University of Colorado at Boulder. Grace Ngai and Radu Florian. 2001. Transformationlearning in the fast lane. In of pages 40–47, June. Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2003. Support vector learning for semantic argument classification. Technical Report CSLR-2003-03, Center for Spoken Language Research, University of Colorado at Boulder. Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Reptext chunks. In of EACL pages 173–179.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2003</date>
<booktitle>Proceedings of RANLP</booktitle>
<contexts>
<context position="3015" citStr="(2003)" startWordPosition="481" endWordPosition="481">sentence, on which the transformational rules will operate. Third, we must decide on the exact forms the transformational rules will be allowed to take (the rule templates). Finally, we must determine a mapping from our word-by-word tag assignment to the labeled bracketing used to identify semantic arguments in the test data. Each of these steps is addressed below. 3.1 Tagging scheme The simplest way of representing the chunks of text which correspond to semantic arguments is to use some variant of the IOB tagging scheme (Sang and Veenstra, 1999). This is the approach taken by Hacioglu et al. (2003), who apply the IOB2 tagging scheme in their word-by-word models, as shown in the second row of Figure 1. However, two aspects of the problem at hand make this tag assignment difficult to use for TBL. First, semantic argument chunks can be very large in size. An argument which contains a relative clause, for example, can easily be longer than 20 words. Second, the label an argument is assigned is largely arbitrary, in the sense that core argument labels (A0, A1, etc.) generally cannot be assigned without some information external to the constituent, such as the class of the predicate, or the i</context>
<context position="4744" citStr="(2003)" startWordPosition="776" endWordPosition="776">the tasks of argument identification and argument labeling. The left boundary (B) tags indicate the label of the argument, but the internal (I) tags are non-specific as to argument label, as in the last row of Figure 1. This allows a a single TBL rule to re-label an argument, while still allowing for interleaving of TBL rules which affect argument identification and labeling. 3.2 Feature Coding With each word in a sentence, we associate the following features: Word The word itself, normalized to lower-case. Tag The word’s part-of-speech tag, as predicted by the system of Gim´enez and M`arquez (2003). Chunk The chunk label of the word, as predicted by the system of Carreras and M`arquez (2003). Entity The named-entity label of the word, as predicted by the system of Chieu and Ng (2003). L/R A feature indicating whether the word is to the left (L) or right (R) of the target verb. Indent This feature indicates the clause level of the current word with respect to the target predicate. Using the clause boundaries predicted by the system of Carreras and M`arquez (2003), we compute a feature based on the linguistic notion of c-command.&apos; If both the predicate and the current word are in the same</context>
<context position="13191" citStr="(2003)" startWordPosition="2287" endWordPosition="2287">enty labels on the development data, and added only about 0.1 to the overall f-measure. 4 Results The results on the test section of the CoNLL 2004 data are presented in Table 1 below. The overall result, an fscore of 60.66, is considerably below results reported for systems using a parser on a comparable data set. However, it is a reasonable result given the simplicity of our system, which does not make use of the additional information found in the PropBank frames themselves. It is an interesting question to what extent our results depend on the use of the Path feature (which Pradhan et al. (2003) found to be essential to their models’ performance). Since this Path feature is also likely to be one of the model’s most brittle features, depending heavily on the accuracy of the syntactic analysis, we might hope that the system does not depend too heavily on it. In fact, the overall f-score on the development set drops from 62.75 to 61.33 when the Path feature is removed, suggestig that it is not essential to our model, though it does help performance to some extent. References Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in par</context>
</contexts>
<marker>2003</marker>
<rawString>Proceedings of RANLP 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named entity recognition with a maximum entropy approach.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<contexts>
<context position="4933" citStr="Chieu and Ng (2003)" startWordPosition="807" endWordPosition="810">ment label, as in the last row of Figure 1. This allows a a single TBL rule to re-label an argument, while still allowing for interleaving of TBL rules which affect argument identification and labeling. 3.2 Feature Coding With each word in a sentence, we associate the following features: Word The word itself, normalized to lower-case. Tag The word’s part-of-speech tag, as predicted by the system of Gim´enez and M`arquez (2003). Chunk The chunk label of the word, as predicted by the system of Carreras and M`arquez (2003). Entity The named-entity label of the word, as predicted by the system of Chieu and Ng (2003). L/R A feature indicating whether the word is to the left (L) or right (R) of the target verb. Indent This feature indicates the clause level of the current word with respect to the target predicate. Using the clause boundaries predicted by the system of Carreras and M`arquez (2003), we compute a feature based on the linguistic notion of c-command.&apos; If both the predicate and the current word are in the same basic clause, Indent=0. If the predicate ccommands the current word, and the current word is one clause level lower, Indent=1. If it is two clause levels lower, Indent=2, and so on. If the</context>
</contexts>
<marker>Chieu, Ng, 2003</marker>
<rawString>Hai Leong Chieu and Hwee Tou Ng. 2003. Named entity recognition with a maximum entropy approach. In Proceedings of CoNLL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Fast and accurate part-of-speech tagging: the SVM approach revisited.</title>
<date>2003</date>
<booktitle>In Proceedings ofRANLP</booktitle>
<marker>Gim´enez, M`arquez, 2003</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2003. Fast and accurate part-of-speech tagging: the SVM approach revisited. In Proceedings ofRANLP 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<date>2003</date>
<note>Shallow semantic</note>
<contexts>
<context position="3015" citStr="Hacioglu et al. (2003)" startWordPosition="478" endWordPosition="481">ach word in the sentence, on which the transformational rules will operate. Third, we must decide on the exact forms the transformational rules will be allowed to take (the rule templates). Finally, we must determine a mapping from our word-by-word tag assignment to the labeled bracketing used to identify semantic arguments in the test data. Each of these steps is addressed below. 3.1 Tagging scheme The simplest way of representing the chunks of text which correspond to semantic arguments is to use some variant of the IOB tagging scheme (Sang and Veenstra, 1999). This is the approach taken by Hacioglu et al. (2003), who apply the IOB2 tagging scheme in their word-by-word models, as shown in the second row of Figure 1. However, two aspects of the problem at hand make this tag assignment difficult to use for TBL. First, semantic argument chunks can be very large in size. An argument which contains a relative clause, for example, can easily be longer than 20 words. Second, the label an argument is assigned is largely arbitrary, in the sense that core argument labels (A0, A1, etc.) generally cannot be assigned without some information external to the constituent, such as the class of the predicate, or the i</context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H. Martin, and Dan Jurafsky. 2003. Shallow semantic</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2003</date>
<tech>Technical Report CSLR-2003-03,</tech>
<institution>Center for Spoken Language Research, University of Colorado at Boulder.</institution>
<contexts>
<context position="6772" citStr="Pradhan et al., 2003" startWordPosition="1133" endWordPosition="1136">chunk directly following a PP chunk. PP-head If is-PP is true, this is the head of the prepositional phrase; otherwise it is zero. N-head The final nominal element of the next NP chunk at the same indent level as the current word, if it exists. For purposes of this feature, a possessive NP chunk is combined with the following NP chunk. Verb The target predicate under consideration. V-Tag The POS tag of the target predicate. V-Passive A boolean feature indicating whether the target verb is in the passive voice. This is determined using a simple regular expression over the sentence. Path As in (Pradhan et al., 2003), this feature is an ordered list of the chunk types intervening between the target verb and the current word, with consecutive NP chunks treated as one. 3.3 Rule Templates In order to define the space of rules searched by the TBL algorithm, we must specify a set of rule templates, which determine the form transformational rules may take. The rule templates used in our system are 130 in number, and fall into a small number of classes, as described below. These rules all take the form f1 ... fn —&gt; label,11, where f1 through fn are features of the current word w or words in its environment, and </context>
<context position="13191" citStr="Pradhan et al. (2003)" startWordPosition="2284" endWordPosition="2287">d fewer than twenty labels on the development data, and added only about 0.1 to the overall f-measure. 4 Results The results on the test section of the CoNLL 2004 data are presented in Table 1 below. The overall result, an fscore of 60.66, is considerably below results reported for systems using a parser on a comparable data set. However, it is a reasonable result given the simplicity of our system, which does not make use of the additional information found in the PropBank frames themselves. It is an interesting question to what extent our results depend on the use of the Path feature (which Pradhan et al. (2003) found to be essential to their models’ performance). Since this Path feature is also likely to be one of the model’s most brittle features, depending heavily on the accuracy of the syntactic analysis, we might hope that the system does not depend too heavily on it. In fact, the overall f-score on the development set drops from 62.75 to 61.33 when the Path feature is removed, suggestig that it is not essential to our model, though it does help performance to some extent. References Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in par</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2003. Support vector learning for semantic argument classification. Technical Report CSLR-2003-03, Center for Spoken Language Research, University of Colorado at Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Jorn Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>173--179</pages>
<contexts>
<context position="2961" citStr="Sang and Veenstra, 1999" startWordPosition="468" endWordPosition="471">e. Second, we must define the features associated with each word in the sentence, on which the transformational rules will operate. Third, we must decide on the exact forms the transformational rules will be allowed to take (the rule templates). Finally, we must determine a mapping from our word-by-word tag assignment to the labeled bracketing used to identify semantic arguments in the test data. Each of these steps is addressed below. 3.1 Tagging scheme The simplest way of representing the chunks of text which correspond to semantic arguments is to use some variant of the IOB tagging scheme (Sang and Veenstra, 1999). This is the approach taken by Hacioglu et al. (2003), who apply the IOB2 tagging scheme in their word-by-word models, as shown in the second row of Figure 1. However, two aspects of the problem at hand make this tag assignment difficult to use for TBL. First, semantic argument chunks can be very large in size. An argument which contains a relative clause, for example, can easily be longer than 20 words. Second, the label an argument is assigned is largely arbitrary, in the sense that core argument labels (A0, A1, etc.) generally cannot be assigned without some information external to the con</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Representing text chunks. In Proceedings of EACL 1999, pages 173–179.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>