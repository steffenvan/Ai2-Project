<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009049">
<title confidence="0.991762">
Learning Probabilistic Synchronous CFGs for Phrase-based Translation
</title>
<author confidence="0.963652">
Markos Mylonakis Khalil Sima’an
</author>
<affiliation confidence="0.938702">
ILLC ILLC
University of Amsterdam University of Amsterdam
</affiliation>
<email confidence="0.988917">
m.mylonakis@uva.nl k.simaan@uva.nl
</email>
<sectionHeader confidence="0.997244" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999479535714286">
Probabilistic phrase-based synchronous
grammars are now considered promis-
ing devices for statistical machine transla-
tion because they can express reordering
phenomena between pairs of languages.
Learning these hierarchical, probabilistic
devices from parallel corpora constitutes a
major challenge, because of multiple la-
tent model variables as well as the risk
of data overfitting. This paper presents
an effective method for learning a family
of particular interest to MT, binary Syn-
chronous Context-Free Grammars with in-
verted/monotone orientation (a.k.a. Bi-
nary ITG). A second contribution con-
cerns devising a lexicalized phrase re-
ordering mechanism that has complimen-
tary strengths to Chiang’s model. The
latter conditions reordering decisions on
the surrounding lexical context of phrases,
whereas our mechanism works with the
lexical content of phrase pairs (akin to
standard phrase-based systems). Surpris-
ingly, our experiments on French-English
data show that our learning method ap-
plied to far simpler models exhibits per-
formance indistinguishable from the Hiero
system.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999790925">
A fundamental problem in phrase-based machine
translation concerns the learning of a probabilistic
synchronous context-free grammar (SCFG) over
phrase pairs from an input parallel corpus. Chi-
ang’s Hiero system (Chiang, 2007) exemplifies
the gains to be had by combining phrase-based
translation (Och and Ney, 2004) with the hierar-
chical reordering capabilities of SCFGs, particu-
larly originating from Binary Inversion Transduc-
tion Grammars (BITG) (Wu, 1997). Yet, exist-
ing empirical work is largely based on successful
heuristic techniques, and the learning of Hiero-like
BITG/SCFG remains an unsolved problem,
The difficulty of this problem stems from the
need for simultaneously learning of two kinds of
preferences (see Fig.1) (1) lexical translation prob-
abilities (P((e, f)  |X)) of source (f) and target
(e) phrase pairs, and (2) phrase reordering prefer-
ences of a target string relative to a source string,
expressed in synchronous productions probabil-
ities (for monotone or switching productions).
Theoretically speaking, both kinds of preferences
may involve latent structure relative to the paral-
lel corpus. The mapping between source-target
sentence pairs can be expressed in terms of la-
tent phrase segmentations and latent word/phrase-
alignments, and the hierarchical phrase reorder-
ing can be expressed in terms of latent binary
synchronous hierarchical structures (cf. Fig. 1).
But each of these three kinds of latent structures
may be made explicit using external resources:
word-alignment could be considered solved us-
ing Giza++ (Och and Ney, 2003)), phrase pairs
can be obtained from these word-alignments (Och
and Ney, 2004), and the hierarchical synchronous
structure can be grown over source/target linguis-
tic syntactic trees output by an existing parser.
The Joint Phrase Translation Model (Marcu and
Wong, 2002) constitutes a specific case, albeit
without the hierarchical, synchronous reordering
</bodyText>
<equation confidence="0.8689405">
Start 5 —* X1 / X1 (1)
Monotone X —* X1 X2 /X1 X2 (2)
Switching X —* X1 X2 /X2 X1 (3)
Emission X —* e / f (4)
</equation>
<figureCaption confidence="0.998598">
Figure 1: A phrase-pair SCFG (BITG)
</figureCaption>
<page confidence="0.955955">
117
</page>
<note confidence="0.9559305">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117–125,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998944190476191">
component. Other existing work, e.g. (Chiang,
2007), assumes the word-alignments are given in
the parallel corpus, but the problem of learning
phrase translation probabilities is usually avoided
by using surface counts of phrase pairs (Koehn et
al., 2003). The problem of learning the hierar-
chical, synchronous grammar reordering rules is
oftentimes addressed as a learning problem in its
own right assuming all the rest is given (Blunsom
et al., 2008b).
A small number of efforts has been dedicated
to the simultaneous learning of the probabilities
of phrase translation pairs as well as hierarchi-
cal reordering, e.g., (DeNero et al., 2008; Zhang
et al., 2008; Blunsom et al., 2009). Of these,
some concentrate on evaluating word-alignment,
directly such as (Zhang et al., 2008) or indirectly
by evaluating a heuristically trained hierarchical
translation system from sampled phrasal align-
ments (Blunsom et al., 2009). However, very
few evaluate on actual translation performance of
induced synchronous grammars (DeNero et al.,
2008). In the majority of cases, the Hiero system,
which constitutes the yardstick by which hierar-
chical systems are measured, remains superior in
translation performance, see e.g. (DeNero et al.,
2008).
This paper tackles the problem of learning gen-
erative BITG models as translation models assum-
ing latent segmentation and latent reordering: this
is the most similar setting to the training of Hiero.
Unlike all other work that heuristically selects a
subset of phrase pairs, we start out from an SCFG
that works with all phrase pairs in the training set
and concentrate on the aspects of learning. This
learning problem is fraught with the risks of over-
fitting and can easily result in inadequate reorder-
ing preferences (see e.g. (DeNero et al., 2006)).
Almost instantly, we find that the translation
performance of all-phrase probabilistic SCFGs
learned in this setting crucially depends on the in-
terplay between two aspects of learning:
</bodyText>
<listItem confidence="0.985973714285714">
• Defining a more constrained parameter
space, where the reordering productions
are phrase-lexicalised and made sensitive to
neighbouring reorderings, and
• Defining an objective function that effec-
tively smoothes the maximum-likelihood cri-
terion.
</listItem>
<bodyText confidence="0.998441395348837">
One contribution of this paper is in devis-
ing an effective, data-driven smoothed Maximum-
Likelihood that can cope with a model working
with all phrase pair SCFGs. This builds upon
our previous work on estimating parameters of a
”bag-of-phrases” model for Machine Translation
(Mylonakis and Sima’an, 2008). However, learn-
ing SCFGs poses significant novel challenges, the
core of which lies on the hierarchical nature of a
stochastic SCFG translation model and the rele-
vant additional layer of latent structure. We ad-
dress these issues in this work. Another important
contribution is in defining a lexicalised reorder-
ing component within BITG that captures order
divergences orthogonal to Chiang’s model (Chi-
ang, 2007) but somewhat akin to Phrase-Based
Statistical Machine Translation reordering models
(Koehn et al., 2003).
Our analysis shows that the learning difficul-
ties can be attributed to a rather weak generative
model. Yet, our best system exhibits Hiero-level
performance on French-English Europarl data us-
ing an SCFG-based decoder (Li et al., 2009). Our
findings should be insightful for others attempting
to make the leap from shallow phrase-based sys-
tems to hierarchical SCFG-based translation mod-
els using learning methods, as opposed to heuris-
tics.
The rest of the paper is structured as follows.
Section 2 briefly introduces the SCFG formalism
and discusses its adoption in the context of Statis-
tical Machine Translation (SMT). In section 3, we
consider some of the pitfalls of stochastic SCFG
grammar learning and address them by introduc-
ing a novel learning objective and algorithm. In
the section that follows we browse through latent
translation structure choices, while in section 5 we
present our empirical experiments on evaluating
the induced stochastic SCFGs on a translation task
and compare their performance with a hierarchical
translation baseline. We close with a comparison
of related work and a final discussion including fu-
ture research directions.
</bodyText>
<sectionHeader confidence="0.9804505" genericHeader="introduction">
2 Synchronous Grammars for Machine
Translation
</sectionHeader>
<bodyText confidence="0.9895295">
Synchronous Context Free Grammars (SCFGs)
provide an appealing formalism to describe the
translation process, which explains the generation
of parallel strings recursively and allows capturing
long-range reordering phenomena. Formally, an
SCFG G is defined as the tuple (N, E, F, R, 5),
</bodyText>
<page confidence="0.997332">
118
</page>
<bodyText confidence="0.999993791666667">
where N is the finite set of non-terminals with
S E N the start symbol, F and E are finite sets
of words for the source and target language and R
is a finite set of rewrite rules. Every rule expands
a left-hand side non-terminal to a right-hand side
pair of strings, a source language string over the
vocabulary FUN and a target language string over
E U N. The number of non-terminals in the two
strings is equal and the rule is complemented with
a mapping between them.
String pairs in the language of the SCFG are
those with a valid derivation, consisting of a se-
quence of rule applications, starting from S and
recursively expanding the linked non-terminals at
the right-hand side of rules. Stochastic SCFGs
augment every rule in R with a probability, under
the constraint that probabilities of rules with the
same left-hand side sum up to one. The probabil-
ity of each derived string pair is then the product
of the probabilities of rules used in the derivation.
Unless otherwise stated, for the rest of the paper
when we refer to SCFGs we will be pointing to
their stochastic extension.
The rank of an SCFG is defined as the maxi-
mum number of non-terminals in a grammar’s rule
right-hand side. Contrary to monolingual Context
Free Grammars, there does not always exist a con-
version of an SCFG of a higher rank to one of a
lower rank with the same language of string pairs.
For this, most machine translation applications fo-
cus on SCFGs of rank two (binary SCFGs), or
binarisable ones witch can be converted to a bi-
nary SCFG, given that these seem to cover most
of the translation phenomena encountered in lan-
guage pairs (Wu, 1997) and the related processing
algorithms are less demanding computationally.
Although SCFGS were initially introduced for
machine translation as a stochastic word-based
translation process in the form of the Inversion-
Transduction Grammar (Wu, 1997), they were ac-
tually able to offer state-of-the-art performance in
their latter phrase-based implementation by Chi-
ang (Chiang, 2005). Chiang’s Hiero hierarchi-
cal translation system is based on a synchronous
grammar with a single non-terminal X covering
all learned phrase-pairs. Beginning from the start
symbol S, an initial phrase-span structure is con-
structed monotonically using a simple ‘glue gram-
</bodyText>
<equation confidence="0.929264666666667">
mar’:
S —*S1 X2 / S1 X2
S —*X1 / X1
</equation>
<bodyText confidence="0.999816333333333">
The true power of the system lies in expanding
these initial phrase-spans with a set of hierarchi-
cal translation rules, which allow conditioning re-
ordering decisions based on lexical context. For
the French to English language pair, some exam-
ples would be:
</bodyText>
<equation confidence="0.98300775">
S —* X1 economiques / financial X1
S —* cette X1 de X2 / this X1 X2
S —* politique X1 commune de X2 /
X20 s common X1 policy
</equation>
<bodyText confidence="0.987343333333333">
Further work builds on the Hiero grammar to ex-
pand it with constituency syntax motivated non-
terminals (Zollmann and Venugopal, 2006).
</bodyText>
<sectionHeader confidence="0.95092" genericHeader="method">
3 Synchronous Grammar Learning
</sectionHeader>
<bodyText confidence="0.999969">
The learning of phrase-based stochastic SCFGs
with a Maximum Likelihood objective is exposed
to overfitting as other all-fragment models such as
Phrase-Based SMT (PBSMT) (Marcu and Wong,
2002; DeNero et al., 2006) and Data-Oriented
Parsing (DOP) (Bod et al., 2003; Zollmann and
Sima’an, 2006). Maximum Likelihood Estima-
tion (MLE) returns degenerate grammar estimates
that memorise well the parallel training corpus but
generalise poorly to unseen data.
The bias-variance decomposition of the gener-
alisation error Err sheds light on this learning
problem. For an estimator p� with training data D,
Err can be expressed as the expected Kullback-
Leibler (KL) divergence between the target distri-
bution q and that the estimate P. This error decom-
poses into bias and variance terms (Heskes, 1998):
Bias is the KL-divergence between q and the mean
estimate over all training data p� = EDp(D). Vari-
ance is the expected divergence between the av-
erage estimate and the estimator’s actual choice.
MLE estimators for all-fragment models are zero-
biased with zero divergence between the average
estimate and the true data distribution. In contrast,
their variance is unboundedly large, leading to un-
bounded generalisation error on unseen cases.
</bodyText>
<equation confidence="0.979499333333333">
bias
z } |{
Err = KL(q, p) +
variance
z }|{
EDKL(p, p) (5)
</equation>
<page confidence="0.991281">
119
</page>
<subsectionHeader confidence="0.989689">
3.1 Cross Validated MLE
</subsectionHeader>
<bodyText confidence="0.999936022222222">
A well-known method for estimating generalisa-
tion error is k-fold Cross-Validation (CV) (Hastie
et al., 2001). By partitioning the training data D
into k parts Hk1, we estimate Err as the expected
error over all 1 G i G k, when testing on Hi with
a model trained by MLE on the rest of the data
D−i = Uj6�iHj.
Here we use CV to leverage the bias-variance
trade-off for learning stochastic all-phrase SCFGs.
Given an input all-phrase SCFG grammar with
phrase-pairs extracted from the training data, we
maximise training data likelihood (MLE) subject
to CV smoothing: for each data part Hi (1 G i G
k), we consider only derivations which employ
grammar rules extracted from the rest of the data
D−i. Other work (Mylonakis and Sima’an, 2008)
has also explored MLE under CV for a “bag-of-
phrases model” that does not deal with reordering
preferences, does not employ latent hierarchical
structure and works with a non-hierarchical de-
coder, and partially considers the sparsity issues
that arise within CV training. The present paper
deals with these issues.
Because of the latent segmentation and hi-
erarchical variables, CV-smoothed MLE cannot
be solved analytically and we devise a CV in-
stance of the Expectation-Maximization (EM) al-
gorithm, with an implementation based on a syn-
chronous version of the Inside-Outside algorithm
(see Fig. 2). For each word-aligned sentence pair
in a partition Hi, the set of eligible derivations (de-
noted D−i) are those that can be built using only
phrase-pairs and productions found in D−i. An es-
sential part of the learning process involves defin-
ing the grammar extractor G(D), a function from
data to an all-phrase SCFG. We will discuss vari-
ous extractors in section 4.
Our CV-EM algorithm is an EM instance, guar-
anteeing convergence and a non-decreasing CV-
smoothed data likelihood after each iteration. The
running time remains O(n6), where n is input
length, but by considering only derivation spans
which do not cross word-alignment points, this
runs in reasonable times for relatively large cor-
pora.
</bodyText>
<subsectionHeader confidence="0.998266">
3.2 Bayesian Aspects of CV-MLE
</subsectionHeader>
<bodyText confidence="0.97336225">
Beside being an estimator, the CV-MLE learning
algorithm has the added value of being a grammar
learner focusing on reducing generalisation error,
INPUT: Word-aligned parallel training data D
Grammar extractor G
The number of parts k to partition D
OUTPUT: SCFG G with rule probabilities p
Partition training data D into parts H1, ... , Hk.
</bodyText>
<figure confidence="0.982055642857143">
For 1 G i G k do
Extract grammar rules set Gi = G(Hi)
Initialise G = UiGi, p0 uniform
Letj=0
Repeat
Letj=j+1
E-step:
For 1 G i G k do
Calculate expected counts given G, pj−1,
for derivations D−i of Hi
using rules from Uk6�iG(k)
M-step: set pj to ML estimate given
expected counts
Until convergence
</figure>
<figureCaption confidence="0.9453455">
Figure 2: The CV Expectation Maximization al-
gorithm
</figureCaption>
<bodyText confidence="0.989204">
in the sense that probabilities of grammar produc-
tions should reflect the frequency with which these
productions are expected to be used for translating
future data. Additionally, since the CV criterion
prohibits for every data point derivations that use
rules only extracted from the same data part, such
rules are assigned zero probabilities in the final es-
timate and are effectively excluded from the gram-
mar. In this way, the algorithm ‘shapes’ the input
grammar, concentrating probability mass on pro-
ductions that are likely to be used with future data.
One view point of CV-MLE is that each par-
tition D−i and Hi induces a prior probability
Prior(7r; D−i) on every parameter assignment 7r,
obtained from D−i. This prior assigns zero prob-
ability to all 7r parameter sets with non-zero prob-
abilities for rules not in G(D−i), and uniformly
distributes probability to the rest of the parameter
sets. In light of this, the CV-MLE objective can be
written as follows:
�arg max Prior(7r; D−i) x P(Hi 7r) (6)
π
i
This data-driven prior aims to directly favour pa-
rameter sets which are expected to better gener-
alise according to the CV criterion, without rely-
ing on arbitrary constraints such as limiting the
</bodyText>
<page confidence="0.976744">
120
</page>
<bodyText confidence="0.9999605">
length of phrase pairs in the right-hand side of
grammar rules. Furthermore, other frequently em-
ployed priors such as the Dirichlet distribution and
the Dirichlet Process promote better generalising
rule probability distributions based on externally
set hyperparameter values, whose selection is fre-
quently sensitive in terms of language pairs, or
even the training corpus itself. In contrast, the CV-
MLE prior aims for a data-driven Bayesian model,
focusing on getting information from the data, in-
stead of imposing external human knowledge on
them (see also (Mackay and Petoy, 1995)).
</bodyText>
<subsectionHeader confidence="0.998192">
3.3 Smoothing the Model
</subsectionHeader>
<bodyText confidence="0.999938090909091">
One remaining wrinkle in the CV-EM scheme is
the treatment of boundary cases. There will often
be sentence-pairs in Hi, that cannot be fully de-
rived by the grammar extracted from the rest of the
data D−i either because of (1) ‘unknown’ words
(i.e. not appearing in other parts of the CV parti-
tion) or (2) complicated combinations of adjacent
word-alignments. We employ external smoothing
of the grammar, prior to learning.
Our solution is to extend the SCFG extracted
from D−i with new emission productions deriving
the ‘unknown’ phrase-pairs (i.e., found in Hi but
not in D−i). Crucially, the probabilities of these
productions are drawn from a fixed smoothing dis-
tribution, i.e., they remain constant throughout es-
timation. Our smoothing distribution of phrase-
pairs for all pre-terminals considers source-target
phrase lengths drawn from a Poisson distribution
with unit mean, drawing subsequently the words
of each of the phrases uniformly from the vocab-
ulary of each language, similar to (Blunsom et al.,
2009).
</bodyText>
<equation confidence="0.993634666666667">
psMooth(f�e) = V |f|
f V|e|
e
</equation>
<bodyText confidence="0.9999405">
Since the smoothing distribution puts stronger
preference on shorter phrase-pairs and avoids
competing with the ‘known’ phrase-pairs, it leads
the learner to prefer using as little as possible such
smoothing rules, covering only the phrase-pairs
required to complete full derivations.
</bodyText>
<sectionHeader confidence="0.983586" genericHeader="method">
4 Parameter Spaces and Grammar
Extractors
</sectionHeader>
<bodyText confidence="0.999899764705882">
A Grammar Extractor (GE) plays a major role in
our probabilistic SCFG learning pipeline. A GE is
a function from a word-aligned parallel corpus to a
probabilistic SCFG model. Together with the con-
straints that render a proper probabilistic SCFG1,
this defines the parameter space.
The extractors used in this paper create SCFGs
productions of two different kinds: (a) hierarchi-
cal synchronous productions that define the space
of possible derivations up to the level of the SCFG
pre-terminals, and (2) the phrase-pair emission
rules that expand the pre-terminals to phrase-pairs
of varying lengths. Given the word-alignments,
the set of phrase-pairs extracted is the set of all
translational equivalents (without length upper-
bound) under the word-alignment as defined in
(Och and Ney, 2004; Koehn et al., 2003).
Below we focus on the two grammar extrac-
tors employed in our experiments. We start out
from the most generic, BITG-like formulation,
and aim at incremental refinement of the hierar-
chical productions in order to capture relevant,
content-based phrase-pair reordering preferences
in the training data.
Single non-terminal SCFG This is a phrase-
based binary SCFG grammar employing a single
non-terminal X covering each extracted phrase-
pair. The other productions consist of monotone
and switching expansions of phrase-pair spans
covered by X. Finally, the whole sentence-pair is
considered to be covered by X. We will call this
‘plain SCFG’ extractor. See Fig. 1.
Lexicalised Reordering SCFG One weakness
of the plain SCFG is that the reordering deci-
sions in the derivations are made without reference
to lexical content of the phrases; this is because
all phrase-pairs are covered by the same non-
terminal. As a refinement, we propose a gram-
mar extractor that aims at modelling the reordering
behaviour of phrase-pairs by taking their content
into account. This time, the X non-terminal is re-
served for phrase-pairs and spans which will take
part in monotonic productions only. Two fresh
non-terminals, X5L and X5R, are used for cov-
ering phrase-pairs that participate in order switch-
ing with other, adjacent phrase-pairs. The non-
terminal X5L covers phrase-pairs which appear
first in the source language order, and the latter
those which follow them. The grammar rules pro-
duced by this GE, dubbed ‘switch grammar’, are
listed in Fig. 3.
</bodyText>
<footnote confidence="0.8321185">
1The sum of productions that have the same left-hand la-
bel must be one.
</footnote>
<equation confidence="0.8802332">
ppoisson(|f|; 1) ppoisson(|e|; 1)
121
Start S —* X1 /X1
Monotone Expansion
X — *X1 X2 /X1 X2
XSL —* X1 X2 / X1 X2
XSR —* X1 X2 /X1 X2
Switching Expansion
X —* XSL1 XSR2 /XSR2 XSL1
XSL —* XSL1 XSR2 /XSR2 XSL1
XSR —* XSL1 XSR2 /XSR2 XSL1
Phrase-Pair Emission
X —* e/f
XSL —* e / f
XSR —* e / f
</equation>
<figureCaption confidence="0.994706">
Figure 3: Lexicalised-Reordering SCFG
</figureCaption>
<bodyText confidence="0.999971153846154">
The reordering information captured by the
switch grammar is in a sense orthogonal to that
of Hiero-like systems utilising rules such as those
listed in section 2. Hiero rules encode hier-
archical reordering patterns based on surround-
ing context. In contrast, the switch grammar
models the reordering preferences of the phrase-
pairs themselves, similarly to the monotone-swap-
discontinuous reordering models of Phrase-based
SMT models (Koehn et al., 2003). Furthermore, it
strives to match pairs of such preferences, combin-
ing together phrase-pairs with compatible reorder-
ing preferences.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999476">
In this section we proceed to integrate our esti-
mates within an SCFG-based decoder. We subse-
quently evaluate our performance in relation to a
state-of-the-art Hiero baseline on a French to En-
glish translation task.
</bodyText>
<subsectionHeader confidence="0.985701">
5.1 Decoding
</subsectionHeader>
<bodyText confidence="0.999964">
The joint model of bilingual string derivations pro-
vided by the learned SCFG grammar can be used
for translation given a input source sentence, since
arg max, p(e|f) = arg max, p(e, f). We use our
learned stochastic SCFG grammar with the decod-
ing component of the Joshua SCFG toolkit (Li
et al., 2009). The full translation model inter-
polates log-linearly the probability of a grammar
derivation together with the language model prob-
ability of the target string. The model is further
smoothed, similarly to phrase-based models and
the Hiero system, with smoothing features φZ such
as the lexical translation scores of the phrase-pairs
involved and rule usage penalties. As usual with
statistical translation, we aim for retrieving the tar-
get sentence e corresponding to the most probable
derivation D =*=&gt;. (f, e) with rules r, with:
</bodyText>
<equation confidence="0.8103195">
p(D) a p(e)-l�—ps,fg(e, f)-lscf9 ri ri φZ(r)-li
Z rED
</equation>
<bodyText confidence="0.9930205">
The interpolation weights are tuned using Mini-
mum Error Rate Training (Och, 2003).
</bodyText>
<subsectionHeader confidence="0.926343">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999857205128205">
We test empirically the learner’s output gram-
mars for translating from French to English, us-
ing k = 5 for the Cross Validation data partition-
ing. The training material is a GIZA++ word-
aligned corpus of 200K sentence-pairs from the
Europarl corpus (Koehn, 2005), with our devel-
opment and test parallel corpora of 2K sentence-
pairs stemming from the same source. Train-
ing the grammar parameters until convergence de-
mands around 6 hours on an 8-core 2.26 GHz Intel
Xeon system. Decoding employs a 4-gram lan-
guage model, trained on English Europarl data of
19.5M words, smoothed using modified Kneser-
Ney discounting (Chen and Goodman, 1998), and
lexical translation smoothing features based on the
GIZA++ alignments.
In a sense, the real baseline to which we might
compare against should be a system employing the
MLE estimate for the grammar extracted from the
whole training corpus. However, as we have al-
ready discussed, this assigns zero probability to all
sentence-pairs outside of the training data and is
subsequently bound to perform extremely poorly,
as decoding would then completely rely on the
smoothing features. Instead, we opt to compare
against a hierarchical translation baseline provided
by the Joshua toolkit, trained and tuned on the
same data as our learning algorithm. The grammar
used by the baseline is much richer than the ones
learned by our algorithm, also employing rules
which translate with context, as shown in section
2. Nevertheless, since it is not clear how the re-
ordering rules probabilities of a grammar similar
to the ones we use could be trained heuristically,
we choose to relate the performance of our learned
stochastic SCFG grammars to the particular, state-
of-the-art in SCFG-based translation, system.
Table 1 presents the translation performance re-
sults of our systems and the baseline. On first
</bodyText>
<page confidence="0.990443">
122
</page>
<table confidence="0.999841">
System Lexical BLEU
Smoothing
joshua-baseline No 27.79
plain scfg No 28.04
switch scfg No 28.48
joshua-baseline Yes 29.96
plain scfg Yes 29.75
switch scfg Yes 29.88
</table>
<tableCaption confidence="0.81867">
Table 1: Empirical results, with and without addi-
tional lexical translation smoothing features dur-
ing decoding
</tableCaption>
<bodyText confidence="0.9986945">
observation, it is evident that our learning algo-
rithm outputs stochastic SCFGs which manage to
generalise, avoiding the degenerate behaviour of
plain MLE training for these models. Given the
notoriety of the estimation process, this is note-
worthy on its own. Having a learning algorithm
at hand which realises in a reasonable extent the
potential of each stochastic grammar design (as
implemented in the relevant grammar extractors),
we can now compare between the two grammar
extractors used in our experiments. The results
table highlights the importance of conditioning
the reordering process on lexical grounds. The
plain grammar with the single phrase-pair non-
terminal cannot accomplish this and achieves a
lower BLEU score. On the other hand, the switch
SCFG allows such conditioning. The learner takes
advantage of this feature to output a grammar
which performs better in taking reordering deci-
sions, something that is reflected in both the actual
translations as well as the BLEU score achieved.
Furthermore, our results highlight the impor-
tance of the smoothing decoding features. The
unsmoothed baseline system itself scores consid-
erably less when employing solely the heuristic
translation score. Our unsmoothed switch gram-
mar decoding setup improves on the baseline by
a considerable difference of 0.7 BLEU. Subse-
quently, when adding the smoothing lexical trans-
lation features, both systems record a significant
increase in performance, reaching comparable lev-
els of performance.
The degenerate behaviour of MLE for SCFGs
can be greatly limited by constraining ourselves
to grammars employing minimal phrase-pairs
; phrase-pairs which cannot be further broken
down into smaller ones according to the word-
alignment. One could argue that it is enough to
perform plain MLE with such minimal phrase-pair
SCFGs, instead of using our more elaborate learn-
ing algorithm with phrase-pairs of all lengths. To
investigate this, for our final experiment we used
a plain MLE estimate of the switch grammar to
translate, limiting the grammar’s phrase-pair emis-
sion rules to only those which involve minimal
phrase-pairs. The very low score of 17.82 BLEU
(without lexical smoothing) not only highlights
the performance gains of using longer phrase-pairs
in hierarchical translation models, but most impor-
tantly provides a strong incentive to address the
overfitting behaviour of MLE estimators for such
models, instead of avoiding it.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999314428571429">
Most learning of phrase-based models, e.g.,
(Marcu and Wong, 2002; DeNero et al., 2006;
Mylonakis and Sima’an, 2008), works without hi-
erarchical components (i.e., not based on the ex-
plicit learning of an SCFG/BITG). These learning
problems pose other kinds of learning challenges
than the ones posed by explicit learning of SCFGs.
Chiang’s original work (Chiang, 2007) is also re-
lated. Yet, the learning problem is not expressed in
terms of an explicit objective function because sur-
face heuristic counts are used. It has been very dif-
ficult to match the performance of Chiang’s model
without use of these heuristic counts.
A somewhat related work, (Blunsom et al.,
2008b), attempts learning new non-terminal labels
for synchronous productions in order to improve
translation. This work differs substantially from
our work because it employs a heuristic estimate
for the phrase pair probabilities, thereby concen-
trating on a different learning problem: that of re-
fining the grammar symbols. Our approach might
also benefit from such a refinement but we do not
attempt this problem here. In contrast, (Blunsom
et al., 2008a) works with the expanded phrase pair
set of (Chiang, 2005), formulating an exponential
model and concentrating on marginalising out the
latent segmentation variables. Again, the learning
problem is rather different from ours. Similarly,
the work in (Zhang et al., 2008) reports on a multi-
stage model, without a latent segmentation vari-
able, but with a strong prior preferring sparse esti-
mates embedded in a Variational Bayes (VB) esti-
mator. This work concentrates the efforts on prun-
ing both the space of phrase pairs and the space of
(ITG) analyses.
</bodyText>
<page confidence="0.996516">
123
</page>
<bodyText confidence="0.9999360625">
To the best of our knowledge, this work is the
first to attempt learning probabilistic phrase-based
BITGs as translation models in a setting where
both a phrase segmentation component and a hi-
erarchical reordering component are assumed la-
tent variables. Like this work, (Mylonakis and
Sima’an, 2008; DeNero et al., 2008) also employ
an all-phrases model. Our paper shows that it is
possible to train such huge grammars under itera-
tive schemes like CV-EM, without need for sam-
pling or pruning. At the surface of it, our CV-
EM estimator is also a kind of Bayesian learner,
but in reality it is a more specific form of regu-
larisation, similar to smoothing techniques used in
language modelling (Chen and Goodman, 1998;
Mackay and Petoy, 1995).
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="discussions">
7 Discussion and Future Research
</sectionHeader>
<bodyText confidence="0.999850627450981">
Phrase-based stochastic SCFGs provide a rich for-
malism to express translation phenomena, which
has been shown to offer competitive performance
in practice. Since learning SCFGs for machine
translation has proven notoriously difficult, most
successful SCFG models for SMT rely on rules ex-
tracted from word-alignment patterns and heuris-
tically computed rule scores, with the impact and
the limits imposed by these choices yet unknown.
Some of the reasons behind the challenges of
SCFG learning can be traced back to the introduc-
tion of latent variables at different, competing lev-
els: word and phrase-alignment as well as hier-
archical reordering structure, with larger phrase-
pairs reducing the need for extensive reordering
structure and vice versa. While imposing priors
such as the often used Dirichlet distribution or the
Dirichlet Process provides a method to overcome
these pitfalls, we believe that the data-driven reg-
ularisation employed in this work provides an ef-
fective alternative to them, focusing more on the
data instead of importing generic external human
knowledge.
We believe that this work makes a significant
step towards learning synchronous grammars for
SMT. This is an objective not only worthy be-
cause of promises of increased performance, but,
most importantly, also by increasing the depth of
our understanding on SCFGs as vehicles of latent
translation structures. Our usage of the induced
grammars directly for translation, instead of an in-
termediate task such as phrase-alignment, aims ex-
actly at this.
While the latent structures that we explored
in this paper were relatively simple in compar-
ison with Hiero-like SCFGs, they take a differ-
ent, content-driven approach on learning reorder-
ing preferences than the context-driven approach
of Hiero. We believe that these approaches are not
merely orthogonal, but could also prove comple-
mentary. Taking advantage of the possible syner-
gies between content and context-driven reorder-
ing learning is an appealing direction of future re-
search. This is particularly promising for other
language pairs, such as Chinese to English, where
Hiero-like grammars have been shown to perform
particularly well.
Acknowledgments: Both authors are supported
by a VIDI grant (nr. 639.022.604) from The
Netherlands Organization for Scientific Research
(NWO).
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99962878125">
P. Blunsom, T. Cohn, and M. Osborne. 2008a. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proceedings of ACL-08: HLT,
pages 200–208. Association for Computational Lin-
guistics.
Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008b. Bayesian synchronous grammar induction.
In Advances in Neural Information Processing Sys-
tems 21, Vancouver, Canada, December.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009. A gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
47th Annual Meeting of the Association of Compu-
tational Linguistics, Singapore, August. Association
for Computational Linguistics.
R. Bod, R. Scha, and K. Sima’an, editors. 2003. Data
Oriented Parsing. CSLI Publications, Stanford Uni-
versity, Stanford, California, USA.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Harvard University, August.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263–270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why generative phrase models underperform sur-
face heuristics. In Proceedings on the Workshop on
Statistical Machine Translation, pages 31–38, New
York City. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.982903">
124
</page>
<reference confidence="0.999758313432836">
John DeNero, Alexandre Bouchard-Cˆot´e, and Dan
Klein. 2008. Sampling alignment structure un-
der a Bayesian translation model. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 314–323, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
T. Hastie, R. Tibshirani, and J. H. Friedman. 2001. The
Elements of Statistical Learning. Springer.
Tom Heskes. 1998. Bias/variance decompositions for
likelihood-based estimators. Neural Computation,
10:1425–1433.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In HLT-NAACL 2003.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In MT Summit 2005.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An open source toolkit for parsing-based machine
translation. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 135–139,
Athens, Greece, March. Association for Computa-
tional Linguistics.
David J. C. Mackay and Linda C. Bauman Petoy. 1995.
A hierarchical dirichlet language model. Natural
Language Engineering, 1:1–19.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proceedings of Empirical methods in natural lan-
guage processing, pages 133–139. Association for
Computational Linguistics.
Markos Mylonakis and Khalil Sima’an. 2008. Phrase
translation probabilities with itg priors and smooth-
ing as learning objective. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 630–639, Honolulu, USA,
October.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–403.
H. Zhang, Ch. Quirk, R. C. Moore, and D. Gildea.
2008. Bayesian learning of non-compositional
phrases with synchronous parsing. In Proceedings
of ACL-08: HLT, pages 97–105, Columbus, Ohio,
June. Association for Computational Linguistics.
A. Zollmann and K. Sima’an. 2006. An efficient
and consistent estimator for data-oriented parsing.
Journal ofAutomata, Languages and Combinatorics
(JALC), 10 (2005) Number 2/3:367–388.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138–141, New York City,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.998496">
125
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.917777">
<title confidence="0.999995">Learning Probabilistic Synchronous CFGs for Phrase-based Translation</title>
<author confidence="0.999852">Markos Mylonakis Khalil Sima’an</author>
<affiliation confidence="0.999226">ILLC ILLC University of Amsterdam University of Amsterdam</affiliation>
<email confidence="0.965264">m.mylonakis@uva.nlk.simaan@uva.nl</email>
<abstract confidence="0.998310551724138">Probabilistic phrase-based synchronous grammars are now considered promising devices for statistical machine translation because they can express reordering phenomena between pairs of languages. Learning these hierarchical, probabilistic devices from parallel corpora constitutes a major challenge, because of multiple latent model variables as well as the risk of data overfitting. This paper presents an effective method for learning a family of particular interest to MT, binary Synchronous Context-Free Grammars with inverted/monotone orientation (a.k.a. Binary ITG). A second contribution concerns devising a lexicalized phrase reordering mechanism that has complimentary strengths to Chiang’s model. The latter conditions reordering decisions on the surrounding lexical context of phrases, whereas our mechanism works with the lexical content of phrase pairs (akin to standard phrase-based systems). Surprisingly, our experiments on French-English data show that our learning method applied to far simpler models exhibits performance indistinguishable from the Hiero system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>200--208</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3998" citStr="Blunsom et al., 2008" startWordPosition="585" endWordPosition="588"> the Fourteenth Conference on Computational Natural Language Learning, pages 117–125, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics component. Other existing work, e.g. (Chiang, 2007), assumes the word-alignments are given in the parallel corpus, but the problem of learning phrase translation probabilities is usually avoided by using surface counts of phrase pairs (Koehn et al., 2003). The problem of learning the hierarchical, synchronous grammar reordering rules is oftentimes addressed as a learning problem in its own right assuming all the rest is given (Blunsom et al., 2008b). A small number of efforts has been dedicated to the simultaneous learning of the probabilities of phrase translation pairs as well as hierarchical reordering, e.g., (DeNero et al., 2008; Zhang et al., 2008; Blunsom et al., 2009). Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al., 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al., 2009). However, very few evaluate on actual translation performance of induced synchronous grammars (DeNero et al., 2008). In the majority</context>
<context position="28007" citStr="Blunsom et al., 2008" startWordPosition="4453" endWordPosition="4456"> and Wong, 2002; DeNero et al., 2006; Mylonakis and Sima’an, 2008), works without hierarchical components (i.e., not based on the explicit learning of an SCFG/BITG). These learning problems pose other kinds of learning challenges than the ones posed by explicit learning of SCFGs. Chiang’s original work (Chiang, 2007) is also related. Yet, the learning problem is not expressed in terms of an explicit objective function because surface heuristic counts are used. It has been very difficult to match the performance of Chiang’s model without use of these heuristic counts. A somewhat related work, (Blunsom et al., 2008b), attempts learning new non-terminal labels for synchronous productions in order to improve translation. This work differs substantially from our work because it employs a heuristic estimate for the phrase pair probabilities, thereby concentrating on a different learning problem: that of refining the grammar symbols. Our approach might also benefit from such a refinement but we do not attempt this problem here. In contrast, (Blunsom et al., 2008a) works with the expanded phrase pair set of (Chiang, 2005), formulating an exponential model and concentrating on marginalising out the latent segm</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>P. Blunsom, T. Cohn, and M. Osborne. 2008a. A discriminative latent variable model for statistical machine translation. In Proceedings of ACL-08: HLT, pages 200–208. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 21,</booktitle>
<location>Vancouver, Canada,</location>
<contexts>
<context position="3998" citStr="Blunsom et al., 2008" startWordPosition="585" endWordPosition="588"> the Fourteenth Conference on Computational Natural Language Learning, pages 117–125, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics component. Other existing work, e.g. (Chiang, 2007), assumes the word-alignments are given in the parallel corpus, but the problem of learning phrase translation probabilities is usually avoided by using surface counts of phrase pairs (Koehn et al., 2003). The problem of learning the hierarchical, synchronous grammar reordering rules is oftentimes addressed as a learning problem in its own right assuming all the rest is given (Blunsom et al., 2008b). A small number of efforts has been dedicated to the simultaneous learning of the probabilities of phrase translation pairs as well as hierarchical reordering, e.g., (DeNero et al., 2008; Zhang et al., 2008; Blunsom et al., 2009). Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al., 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al., 2009). However, very few evaluate on actual translation performance of induced synchronous grammars (DeNero et al., 2008). In the majority</context>
<context position="28007" citStr="Blunsom et al., 2008" startWordPosition="4453" endWordPosition="4456"> and Wong, 2002; DeNero et al., 2006; Mylonakis and Sima’an, 2008), works without hierarchical components (i.e., not based on the explicit learning of an SCFG/BITG). These learning problems pose other kinds of learning challenges than the ones posed by explicit learning of SCFGs. Chiang’s original work (Chiang, 2007) is also related. Yet, the learning problem is not expressed in terms of an explicit objective function because surface heuristic counts are used. It has been very difficult to match the performance of Chiang’s model without use of these heuristic counts. A somewhat related work, (Blunsom et al., 2008b), attempts learning new non-terminal labels for synchronous productions in order to improve translation. This work differs substantially from our work because it employs a heuristic estimate for the phrase pair probabilities, thereby concentrating on a different learning problem: that of refining the grammar symbols. Our approach might also benefit from such a refinement but we do not attempt this problem here. In contrast, (Blunsom et al., 2008a) works with the expanded phrase pair set of (Chiang, 2005), formulating an exponential model and concentrating on marginalising out the latent segm</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008b. Bayesian synchronous grammar induction. In Advances in Neural Information Processing Systems 21, Vancouver, Canada, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association of Computational Linguistics, Singapore,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4230" citStr="Blunsom et al., 2009" startWordPosition="623" endWordPosition="626">he word-alignments are given in the parallel corpus, but the problem of learning phrase translation probabilities is usually avoided by using surface counts of phrase pairs (Koehn et al., 2003). The problem of learning the hierarchical, synchronous grammar reordering rules is oftentimes addressed as a learning problem in its own right assuming all the rest is given (Blunsom et al., 2008b). A small number of efforts has been dedicated to the simultaneous learning of the probabilities of phrase translation pairs as well as hierarchical reordering, e.g., (DeNero et al., 2008; Zhang et al., 2008; Blunsom et al., 2009). Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al., 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al., 2009). However, very few evaluate on actual translation performance of induced synchronous grammars (DeNero et al., 2008). In the majority of cases, the Hiero system, which constitutes the yardstick by which hierarchical systems are measured, remains superior in translation performance, see e.g. (DeNero et al., 2008). This paper tackles the problem of learning generat</context>
<context position="17855" citStr="Blunsom et al., 2009" startWordPosition="2834" endWordPosition="2837">e grammar, prior to learning. Our solution is to extend the SCFG extracted from D−i with new emission productions deriving the ‘unknown’ phrase-pairs (i.e., found in Hi but not in D−i). Crucially, the probabilities of these productions are drawn from a fixed smoothing distribution, i.e., they remain constant throughout estimation. Our smoothing distribution of phrasepairs for all pre-terminals considers source-target phrase lengths drawn from a Poisson distribution with unit mean, drawing subsequently the words of each of the phrases uniformly from the vocabulary of each language, similar to (Blunsom et al., 2009). psMooth(f�e) = V |f| f V|e| e Since the smoothing distribution puts stronger preference on shorter phrase-pairs and avoids competing with the ‘known’ phrase-pairs, it leads the learner to prefer using as little as possible such smoothing rules, covering only the phrase-pairs required to complete full derivations. 4 Parameter Spaces and Grammar Extractors A Grammar Extractor (GE) plays a major role in our probabilistic SCFG learning pipeline. A GE is a function from a word-aligned parallel corpus to a probabilistic SCFG model. Together with the constraints that render a proper probabilistic S</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proceedings of the 47th Annual Meeting of the Association of Computational Linguistics, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
<author>R Scha</author>
<author>K Sima’an</author>
<author>editors</author>
</authors>
<title>Data Oriented Parsing.</title>
<date>2003</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford University, Stanford, California, USA.</location>
<marker>Bod, Scha, Sima’an, editors, 2003</marker>
<rawString>R. Bod, R. Scha, and K. Sima’an, editors. 2003. Data Oriented Parsing. CSLI Publications, Stanford University, Stanford, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University,</institution>
<contexts>
<context position="23407" citStr="Chen and Goodman, 1998" startWordPosition="3734" endWordPosition="3737">rically the learner’s output grammars for translating from French to English, using k = 5 for the Cross Validation data partitioning. The training material is a GIZA++ wordaligned corpus of 200K sentence-pairs from the Europarl corpus (Koehn, 2005), with our development and test parallel corpora of 2K sentencepairs stemming from the same source. Training the grammar parameters until convergence demands around 6 hours on an 8-core 2.26 GHz Intel Xeon system. Decoding employs a 4-gram language model, trained on English Europarl data of 19.5M words, smoothed using modified KneserNey discounting (Chen and Goodman, 1998), and lexical translation smoothing features based on the GIZA++ alignments. In a sense, the real baseline to which we might compare against should be a system employing the MLE estimate for the grammar extracted from the whole training corpus. However, as we have already discussed, this assigns zero probability to all sentence-pairs outside of the training data and is subsequently bound to perform extremely poorly, as decoding would then completely rely on the smoothing features. Instead, we opt to compare against a hierarchical translation baseline provided by the Joshua toolkit, trained and</context>
<context position="29728" citStr="Chen and Goodman, 1998" startWordPosition="4733" endWordPosition="4736">se-based BITGs as translation models in a setting where both a phrase segmentation component and a hierarchical reordering component are assumed latent variables. Like this work, (Mylonakis and Sima’an, 2008; DeNero et al., 2008) also employ an all-phrases model. Our paper shows that it is possible to train such huge grammars under iterative schemes like CV-EM, without need for sampling or pruning. At the surface of it, our CVEM estimator is also a kind of Bayesian learner, but in reality it is a more specific form of regularisation, similar to smoothing techniques used in language modelling (Chen and Goodman, 1998; Mackay and Petoy, 1995). 7 Discussion and Future Research Phrase-based stochastic SCFGs provide a rich formalism to express translation phenomena, which has been shown to offer competitive performance in practice. Since learning SCFGs for machine translation has proven notoriously difficult, most successful SCFG models for SMT rely on rules extracted from word-alignment patterns and heuristically computed rule scores, with the impact and the limits imposed by these choices yet unknown. Some of the reasons behind the challenges of SCFG learning can be traced back to the introduction of latent</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="10095" citStr="Chiang, 2005" startWordPosition="1557" endWordPosition="1558"> translation applications focus on SCFGs of rank two (binary SCFGs), or binarisable ones witch can be converted to a binary SCFG, given that these seem to cover most of the translation phenomena encountered in language pairs (Wu, 1997) and the related processing algorithms are less demanding computationally. Although SCFGS were initially introduced for machine translation as a stochastic word-based translation process in the form of the InversionTransduction Grammar (Wu, 1997), they were actually able to offer state-of-the-art performance in their latter phrase-based implementation by Chiang (Chiang, 2005). Chiang’s Hiero hierarchical translation system is based on a synchronous grammar with a single non-terminal X covering all learned phrase-pairs. Beginning from the start symbol S, an initial phrase-span structure is constructed monotonically using a simple ‘glue grammar’: S —*S1 X2 / S1 X2 S —*X1 / X1 The true power of the system lies in expanding these initial phrase-spans with a set of hierarchical translation rules, which allow conditioning reordering decisions based on lexical context. For the French to English language pair, some examples would be: S —* X1 economiques / financial X1 S —</context>
<context position="28518" citStr="Chiang, 2005" startWordPosition="4534" endWordPosition="4535">f Chiang’s model without use of these heuristic counts. A somewhat related work, (Blunsom et al., 2008b), attempts learning new non-terminal labels for synchronous productions in order to improve translation. This work differs substantially from our work because it employs a heuristic estimate for the phrase pair probabilities, thereby concentrating on a different learning problem: that of refining the grammar symbols. Our approach might also benefit from such a refinement but we do not attempt this problem here. In contrast, (Blunsom et al., 2008a) works with the expanded phrase pair set of (Chiang, 2005), formulating an exponential model and concentrating on marginalising out the latent segmentation variables. Again, the learning problem is rather different from ours. Similarly, the work in (Zhang et al., 2008) reports on a multistage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator. This work concentrates the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. 123 To the best of our knowledge, this work is the first to attempt learning probabilistic phrase-based BIT</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL 2005, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<contexts>
<context position="1523" citStr="Chiang, 2007" startWordPosition="207" endWordPosition="208">he latter conditions reordering decisions on the surrounding lexical context of phrases, whereas our mechanism works with the lexical content of phrase pairs (akin to standard phrase-based systems). Surprisingly, our experiments on French-English data show that our learning method applied to far simpler models exhibits performance indistinguishable from the Hiero system. 1 Introduction A fundamental problem in phrase-based machine translation concerns the learning of a probabilistic synchronous context-free grammar (SCFG) over phrase pairs from an input parallel corpus. Chiang’s Hiero system (Chiang, 2007) exemplifies the gains to be had by combining phrase-based translation (Och and Ney, 2004) with the hierarchical reordering capabilities of SCFGs, particularly originating from Binary Inversion Transduction Grammars (BITG) (Wu, 1997). Yet, existing empirical work is largely based on successful heuristic techniques, and the learning of Hiero-like BITG/SCFG remains an unsolved problem, The difficulty of this problem stems from the need for simultaneously learning of two kinds of preferences (see Fig.1) (1) lexical translation probabilities (P((e, f) |X)) of source (f) and target (e) phrase pairs</context>
<context position="3598" citStr="Chiang, 2007" startWordPosition="524" endWordPosition="525"> be grown over source/target linguistic syntactic trees output by an existing parser. The Joint Phrase Translation Model (Marcu and Wong, 2002) constitutes a specific case, albeit without the hierarchical, synchronous reordering Start 5 —* X1 / X1 (1) Monotone X —* X1 X2 /X1 X2 (2) Switching X —* X1 X2 /X2 X1 (3) Emission X —* e / f (4) Figure 1: A phrase-pair SCFG (BITG) 117 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117–125, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics component. Other existing work, e.g. (Chiang, 2007), assumes the word-alignments are given in the parallel corpus, but the problem of learning phrase translation probabilities is usually avoided by using surface counts of phrase pairs (Koehn et al., 2003). The problem of learning the hierarchical, synchronous grammar reordering rules is oftentimes addressed as a learning problem in its own right assuming all the rest is given (Blunsom et al., 2008b). A small number of efforts has been dedicated to the simultaneous learning of the probabilities of phrase translation pairs as well as hierarchical reordering, e.g., (DeNero et al., 2008; Zhang et </context>
<context position="6481" citStr="Chiang, 2007" startWordPosition="967" endWordPosition="969">t can cope with a model working with all phrase pair SCFGs. This builds upon our previous work on estimating parameters of a ”bag-of-phrases” model for Machine Translation (Mylonakis and Sima’an, 2008). However, learning SCFGs poses significant novel challenges, the core of which lies on the hierarchical nature of a stochastic SCFG translation model and the relevant additional layer of latent structure. We address these issues in this work. Another important contribution is in defining a lexicalised reordering component within BITG that captures order divergences orthogonal to Chiang’s model (Chiang, 2007) but somewhat akin to Phrase-Based Statistical Machine Translation reordering models (Koehn et al., 2003). Our analysis shows that the learning difficulties can be attributed to a rather weak generative model. Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al., 2009). Our findings should be insightful for others attempting to make the leap from shallow phrase-based systems to hierarchical SCFG-based translation models using learning methods, as opposed to heuristics. The rest of the paper is structured as follows. Sectio</context>
<context position="27705" citStr="Chiang, 2007" startWordPosition="4403" endWordPosition="4404">e performance gains of using longer phrase-pairs in hierarchical translation models, but most importantly provides a strong incentive to address the overfitting behaviour of MLE estimators for such models, instead of avoiding it. 6 Related work Most learning of phrase-based models, e.g., (Marcu and Wong, 2002; DeNero et al., 2006; Mylonakis and Sima’an, 2008), works without hierarchical components (i.e., not based on the explicit learning of an SCFG/BITG). These learning problems pose other kinds of learning challenges than the ones posed by explicit learning of SCFGs. Chiang’s original work (Chiang, 2007) is also related. Yet, the learning problem is not expressed in terms of an explicit objective function because surface heuristic counts are used. It has been very difficult to match the performance of Chiang’s model without use of these heuristic counts. A somewhat related work, (Blunsom et al., 2008b), attempts learning new non-terminal labels for synchronous productions in order to improve translation. This work differs substantially from our work because it employs a heuristic estimate for the phrase pair probabilities, thereby concentrating on a different learning problem: that of refinin</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Gillick</author>
<author>J Zhang</author>
<author>D Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<institution>City. Association for Computational Linguistics.</institution>
<location>New York</location>
<contexts>
<context position="5330" citStr="DeNero et al., 2006" startWordPosition="795" endWordPosition="798"> superior in translation performance, see e.g. (DeNero et al., 2008). This paper tackles the problem of learning generative BITG models as translation models assuming latent segmentation and latent reordering: this is the most similar setting to the training of Hiero. Unlike all other work that heuristically selects a subset of phrase pairs, we start out from an SCFG that works with all phrase pairs in the training set and concentrate on the aspects of learning. This learning problem is fraught with the risks of overfitting and can easily result in inadequate reordering preferences (see e.g. (DeNero et al., 2006)). Almost instantly, we find that the translation performance of all-phrase probabilistic SCFGs learned in this setting crucially depends on the interplay between two aspects of learning: • Defining a more constrained parameter space, where the reordering productions are phrase-lexicalised and made sensitive to neighbouring reorderings, and • Defining an objective function that effectively smoothes the maximum-likelihood criterion. One contribution of this paper is in devising an effective, data-driven smoothed MaximumLikelihood that can cope with a model working with all phrase pair SCFGs. Th</context>
<context position="11160" citStr="DeNero et al., 2006" startWordPosition="1735" endWordPosition="1738">ning reordering decisions based on lexical context. For the French to English language pair, some examples would be: S —* X1 economiques / financial X1 S —* cette X1 de X2 / this X1 X2 S —* politique X1 commune de X2 / X20 s common X1 policy Further work builds on the Hiero grammar to expand it with constituency syntax motivated nonterminals (Zollmann and Venugopal, 2006). 3 Synchronous Grammar Learning The learning of phrase-based stochastic SCFGs with a Maximum Likelihood objective is exposed to overfitting as other all-fragment models such as Phrase-Based SMT (PBSMT) (Marcu and Wong, 2002; DeNero et al., 2006) and Data-Oriented Parsing (DOP) (Bod et al., 2003; Zollmann and Sima’an, 2006). Maximum Likelihood Estimation (MLE) returns degenerate grammar estimates that memorise well the parallel training corpus but generalise poorly to unseen data. The bias-variance decomposition of the generalisation error Err sheds light on this learning problem. For an estimator p� with training data D, Err can be expressed as the expected KullbackLeibler (KL) divergence between the target distribution q and that the estimate P. This error decomposes into bias and variance terms (Heskes, 1998): Bias is the KL-diverg</context>
<context position="27423" citStr="DeNero et al., 2006" startWordPosition="4358" endWordPosition="4361">estigate this, for our final experiment we used a plain MLE estimate of the switch grammar to translate, limiting the grammar’s phrase-pair emission rules to only those which involve minimal phrase-pairs. The very low score of 17.82 BLEU (without lexical smoothing) not only highlights the performance gains of using longer phrase-pairs in hierarchical translation models, but most importantly provides a strong incentive to address the overfitting behaviour of MLE estimators for such models, instead of avoiding it. 6 Related work Most learning of phrase-based models, e.g., (Marcu and Wong, 2002; DeNero et al., 2006; Mylonakis and Sima’an, 2008), works without hierarchical components (i.e., not based on the explicit learning of an SCFG/BITG). These learning problems pose other kinds of learning challenges than the ones posed by explicit learning of SCFGs. Chiang’s original work (Chiang, 2007) is also related. Yet, the learning problem is not expressed in terms of an explicit objective function because surface heuristic counts are used. It has been very difficult to match the performance of Chiang’s model without use of these heuristic counts. A somewhat related work, (Blunsom et al., 2008b), attempts lea</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006. Why generative phrase models underperform surface heuristics. In Proceedings on the Workshop on Statistical Machine Translation, pages 31–38, New York City. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>314--323</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314–323, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
<author>J H Friedman</author>
</authors>
<title>The Elements of Statistical Learning.</title>
<date>2001</date>
<publisher>Springer.</publisher>
<contexts>
<context position="12374" citStr="Hastie et al., 2001" startWordPosition="1930" endWordPosition="1933">divergence between q and the mean estimate over all training data p� = EDp(D). Variance is the expected divergence between the average estimate and the estimator’s actual choice. MLE estimators for all-fragment models are zerobiased with zero divergence between the average estimate and the true data distribution. In contrast, their variance is unboundedly large, leading to unbounded generalisation error on unseen cases. bias z } |{ Err = KL(q, p) + variance z }|{ EDKL(p, p) (5) 119 3.1 Cross Validated MLE A well-known method for estimating generalisation error is k-fold Cross-Validation (CV) (Hastie et al., 2001). By partitioning the training data D into k parts Hk1, we estimate Err as the expected error over all 1 G i G k, when testing on Hi with a model trained by MLE on the rest of the data D−i = Uj6�iHj. Here we use CV to leverage the bias-variance trade-off for learning stochastic all-phrase SCFGs. Given an input all-phrase SCFG grammar with phrase-pairs extracted from the training data, we maximise training data likelihood (MLE) subject to CV smoothing: for each data part Hi (1 G i G k), we consider only derivations which employ grammar rules extracted from the rest of the data D−i. Other work (</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2001</marker>
<rawString>T. Hastie, R. Tibshirani, and J. H. Friedman. 2001. The Elements of Statistical Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Heskes</author>
</authors>
<title>Bias/variance decompositions for likelihood-based estimators.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<pages>10--1425</pages>
<contexts>
<context position="11737" citStr="Heskes, 1998" startWordPosition="1827" endWordPosition="1828">and Wong, 2002; DeNero et al., 2006) and Data-Oriented Parsing (DOP) (Bod et al., 2003; Zollmann and Sima’an, 2006). Maximum Likelihood Estimation (MLE) returns degenerate grammar estimates that memorise well the parallel training corpus but generalise poorly to unseen data. The bias-variance decomposition of the generalisation error Err sheds light on this learning problem. For an estimator p� with training data D, Err can be expressed as the expected KullbackLeibler (KL) divergence between the target distribution q and that the estimate P. This error decomposes into bias and variance terms (Heskes, 1998): Bias is the KL-divergence between q and the mean estimate over all training data p� = EDp(D). Variance is the expected divergence between the average estimate and the estimator’s actual choice. MLE estimators for all-fragment models are zerobiased with zero divergence between the average estimate and the true data distribution. In contrast, their variance is unboundedly large, leading to unbounded generalisation error on unseen cases. bias z } |{ Err = KL(q, p) + variance z }|{ EDKL(p, p) (5) 119 3.1 Cross Validated MLE A well-known method for estimating generalisation error is k-fold Cross-</context>
</contexts>
<marker>Heskes, 1998</marker>
<rawString>Tom Heskes. 1998. Bias/variance decompositions for likelihood-based estimators. Neural Computation, 10:1425–1433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL</booktitle>
<contexts>
<context position="3802" citStr="Koehn et al., 2003" startWordPosition="553" endWordPosition="556">chical, synchronous reordering Start 5 —* X1 / X1 (1) Monotone X —* X1 X2 /X1 X2 (2) Switching X —* X1 X2 /X2 X1 (3) Emission X —* e / f (4) Figure 1: A phrase-pair SCFG (BITG) 117 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117–125, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics component. Other existing work, e.g. (Chiang, 2007), assumes the word-alignments are given in the parallel corpus, but the problem of learning phrase translation probabilities is usually avoided by using surface counts of phrase pairs (Koehn et al., 2003). The problem of learning the hierarchical, synchronous grammar reordering rules is oftentimes addressed as a learning problem in its own right assuming all the rest is given (Blunsom et al., 2008b). A small number of efforts has been dedicated to the simultaneous learning of the probabilities of phrase translation pairs as well as hierarchical reordering, e.g., (DeNero et al., 2008; Zhang et al., 2008; Blunsom et al., 2009). Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al., 2008) or indirectly by evaluating a heuristically trained hierarchical translatio</context>
<context position="6586" citStr="Koehn et al., 2003" startWordPosition="980" endWordPosition="983">stimating parameters of a ”bag-of-phrases” model for Machine Translation (Mylonakis and Sima’an, 2008). However, learning SCFGs poses significant novel challenges, the core of which lies on the hierarchical nature of a stochastic SCFG translation model and the relevant additional layer of latent structure. We address these issues in this work. Another important contribution is in defining a lexicalised reordering component within BITG that captures order divergences orthogonal to Chiang’s model (Chiang, 2007) but somewhat akin to Phrase-Based Statistical Machine Translation reordering models (Koehn et al., 2003). Our analysis shows that the learning difficulties can be attributed to a rather weak generative model. Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al., 2009). Our findings should be insightful for others attempting to make the leap from shallow phrase-based systems to hierarchical SCFG-based translation models using learning methods, as opposed to heuristics. The rest of the paper is structured as follows. Section 2 briefly introduces the SCFG formalism and discusses its adoption in the context of Statistical Machin</context>
<context position="19024" citStr="Koehn et al., 2003" startWordPosition="3013" endWordPosition="3016">e constraints that render a proper probabilistic SCFG1, this defines the parameter space. The extractors used in this paper create SCFGs productions of two different kinds: (a) hierarchical synchronous productions that define the space of possible derivations up to the level of the SCFG pre-terminals, and (2) the phrase-pair emission rules that expand the pre-terminals to phrase-pairs of varying lengths. Given the word-alignments, the set of phrase-pairs extracted is the set of all translational equivalents (without length upperbound) under the word-alignment as defined in (Och and Ney, 2004; Koehn et al., 2003). Below we focus on the two grammar extractors employed in our experiments. We start out from the most generic, BITG-like formulation, and aim at incremental refinement of the hierarchical productions in order to capture relevant, content-based phrase-pair reordering preferences in the training data. Single non-terminal SCFG This is a phrasebased binary SCFG grammar employing a single non-terminal X covering each extracted phrasepair. The other productions consist of monotone and switching expansions of phrase-pair spans covered by X. Finally, the whole sentence-pair is considered to be covere</context>
<context position="21413" citStr="Koehn et al., 2003" startWordPosition="3410" endWordPosition="3413"> XSR2 /XSR2 XSL1 XSL —* XSL1 XSR2 /XSR2 XSL1 XSR —* XSL1 XSR2 /XSR2 XSL1 Phrase-Pair Emission X —* e/f XSL —* e / f XSR —* e / f Figure 3: Lexicalised-Reordering SCFG The reordering information captured by the switch grammar is in a sense orthogonal to that of Hiero-like systems utilising rules such as those listed in section 2. Hiero rules encode hierarchical reordering patterns based on surrounding context. In contrast, the switch grammar models the reordering preferences of the phrasepairs themselves, similarly to the monotone-swapdiscontinuous reordering models of Phrase-based SMT models (Koehn et al., 2003). Furthermore, it strives to match pairs of such preferences, combining together phrase-pairs with compatible reordering preferences. 5 Experiments In this section we proceed to integrate our estimates within an SCFG-based decoder. We subsequently evaluate our performance in relation to a state-of-the-art Hiero baseline on a French to English translation task. 5.1 Decoding The joint model of bilingual string derivations provided by the learned SCFG grammar can be used for translation given a input source sentence, since arg max, p(e|f) = arg max, p(e, f). We use our learned stochastic SCFG gra</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="23032" citStr="Koehn, 2005" startWordPosition="3674" endWordPosition="3675">irs involved and rule usage penalties. As usual with statistical translation, we aim for retrieving the target sentence e corresponding to the most probable derivation D =*=&gt;. (f, e) with rules r, with: p(D) a p(e)-l�—ps,fg(e, f)-lscf9 ri ri φZ(r)-li Z rED The interpolation weights are tuned using Minimum Error Rate Training (Och, 2003). 5.2 Results We test empirically the learner’s output grammars for translating from French to English, using k = 5 for the Cross Validation data partitioning. The training material is a GIZA++ wordaligned corpus of 200K sentence-pairs from the Europarl corpus (Koehn, 2005), with our development and test parallel corpora of 2K sentencepairs stemming from the same source. Training the grammar parameters until convergence demands around 6 hours on an 8-core 2.26 GHz Intel Xeon system. Decoding employs a 4-gram language model, trained on English Europarl data of 19.5M words, smoothed using modified KneserNey discounting (Chen and Goodman, 1998), and lexical translation smoothing features based on the GIZA++ alignments. In a sense, the real baseline to which we might compare against should be a system employing the MLE estimate for the grammar extracted from the who</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In MT Summit 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="6822" citStr="Li et al., 2009" startWordPosition="1017" endWordPosition="1020">slation model and the relevant additional layer of latent structure. We address these issues in this work. Another important contribution is in defining a lexicalised reordering component within BITG that captures order divergences orthogonal to Chiang’s model (Chiang, 2007) but somewhat akin to Phrase-Based Statistical Machine Translation reordering models (Koehn et al., 2003). Our analysis shows that the learning difficulties can be attributed to a rather weak generative model. Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al., 2009). Our findings should be insightful for others attempting to make the leap from shallow phrase-based systems to hierarchical SCFG-based translation models using learning methods, as opposed to heuristics. The rest of the paper is structured as follows. Section 2 briefly introduces the SCFG formalism and discusses its adoption in the context of Statistical Machine Translation (SMT). In section 3, we consider some of the pitfalls of stochastic SCFG grammar learning and address them by introducing a novel learning objective and algorithm. In the section that follows we browse through latent trans</context>
<context position="22090" citStr="Li et al., 2009" startWordPosition="3521" endWordPosition="3524"> combining together phrase-pairs with compatible reordering preferences. 5 Experiments In this section we proceed to integrate our estimates within an SCFG-based decoder. We subsequently evaluate our performance in relation to a state-of-the-art Hiero baseline on a French to English translation task. 5.1 Decoding The joint model of bilingual string derivations provided by the learned SCFG grammar can be used for translation given a input source sentence, since arg max, p(e|f) = arg max, p(e, f). We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit (Li et al., 2009). The full translation model interpolates log-linearly the probability of a grammar derivation together with the language model probability of the target string. The model is further smoothed, similarly to phrase-based models and the Hiero system, with smoothing features φZ such as the lexical translation scores of the phrase-pairs involved and rule usage penalties. As usual with statistical translation, we aim for retrieving the target sentence e corresponding to the most probable derivation D =*=&gt;. (f, e) with rules r, with: p(D) a p(e)-l�—ps,fg(e, f)-lscf9 ri ri φZ(r)-li Z rED The interpola</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135–139, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C Mackay</author>
<author>Linda C Bauman Petoy</author>
</authors>
<title>A hierarchical dirichlet language model.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<pages>1--1</pages>
<contexts>
<context position="16814" citStr="Mackay and Petoy, 1995" startWordPosition="2668" endWordPosition="2671">ry constraints such as limiting the 120 length of phrase pairs in the right-hand side of grammar rules. Furthermore, other frequently employed priors such as the Dirichlet distribution and the Dirichlet Process promote better generalising rule probability distributions based on externally set hyperparameter values, whose selection is frequently sensitive in terms of language pairs, or even the training corpus itself. In contrast, the CVMLE prior aims for a data-driven Bayesian model, focusing on getting information from the data, instead of imposing external human knowledge on them (see also (Mackay and Petoy, 1995)). 3.3 Smoothing the Model One remaining wrinkle in the CV-EM scheme is the treatment of boundary cases. There will often be sentence-pairs in Hi, that cannot be fully derived by the grammar extracted from the rest of the data D−i either because of (1) ‘unknown’ words (i.e. not appearing in other parts of the CV partition) or (2) complicated combinations of adjacent word-alignments. We employ external smoothing of the grammar, prior to learning. Our solution is to extend the SCFG extracted from D−i with new emission productions deriving the ‘unknown’ phrase-pairs (i.e., found in Hi but not in </context>
<context position="29753" citStr="Mackay and Petoy, 1995" startWordPosition="4737" endWordPosition="4740">ation models in a setting where both a phrase segmentation component and a hierarchical reordering component are assumed latent variables. Like this work, (Mylonakis and Sima’an, 2008; DeNero et al., 2008) also employ an all-phrases model. Our paper shows that it is possible to train such huge grammars under iterative schemes like CV-EM, without need for sampling or pruning. At the surface of it, our CVEM estimator is also a kind of Bayesian learner, but in reality it is a more specific form of regularisation, similar to smoothing techniques used in language modelling (Chen and Goodman, 1998; Mackay and Petoy, 1995). 7 Discussion and Future Research Phrase-based stochastic SCFGs provide a rich formalism to express translation phenomena, which has been shown to offer competitive performance in practice. Since learning SCFGs for machine translation has proven notoriously difficult, most successful SCFG models for SMT rely on rules extracted from word-alignment patterns and heuristically computed rule scores, with the impact and the limits imposed by these choices yet unknown. Some of the reasons behind the challenges of SCFG learning can be traced back to the introduction of latent variables at different, </context>
</contexts>
<marker>Mackay, Petoy, 1995</marker>
<rawString>David J. C. Mackay and Linda C. Bauman Petoy. 1995. A hierarchical dirichlet language model. Natural Language Engineering, 1:1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of Empirical methods in natural language processing,</booktitle>
<pages>133--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3128" citStr="Marcu and Wong, 2002" startWordPosition="446" endWordPosition="449">segmentations and latent word/phrasealignments, and the hierarchical phrase reordering can be expressed in terms of latent binary synchronous hierarchical structures (cf. Fig. 1). But each of these three kinds of latent structures may be made explicit using external resources: word-alignment could be considered solved using Giza++ (Och and Ney, 2003)), phrase pairs can be obtained from these word-alignments (Och and Ney, 2004), and the hierarchical synchronous structure can be grown over source/target linguistic syntactic trees output by an existing parser. The Joint Phrase Translation Model (Marcu and Wong, 2002) constitutes a specific case, albeit without the hierarchical, synchronous reordering Start 5 —* X1 / X1 (1) Monotone X —* X1 X2 /X1 X2 (2) Switching X —* X1 X2 /X2 X1 (3) Emission X —* e / f (4) Figure 1: A phrase-pair SCFG (BITG) 117 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117–125, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics component. Other existing work, e.g. (Chiang, 2007), assumes the word-alignments are given in the parallel corpus, but the problem of learning phrase translation probabilities is us</context>
<context position="11138" citStr="Marcu and Wong, 2002" startWordPosition="1731" endWordPosition="1734">, which allow conditioning reordering decisions based on lexical context. For the French to English language pair, some examples would be: S —* X1 economiques / financial X1 S —* cette X1 de X2 / this X1 X2 S —* politique X1 commune de X2 / X20 s common X1 policy Further work builds on the Hiero grammar to expand it with constituency syntax motivated nonterminals (Zollmann and Venugopal, 2006). 3 Synchronous Grammar Learning The learning of phrase-based stochastic SCFGs with a Maximum Likelihood objective is exposed to overfitting as other all-fragment models such as Phrase-Based SMT (PBSMT) (Marcu and Wong, 2002; DeNero et al., 2006) and Data-Oriented Parsing (DOP) (Bod et al., 2003; Zollmann and Sima’an, 2006). Maximum Likelihood Estimation (MLE) returns degenerate grammar estimates that memorise well the parallel training corpus but generalise poorly to unseen data. The bias-variance decomposition of the generalisation error Err sheds light on this learning problem. For an estimator p� with training data D, Err can be expressed as the expected KullbackLeibler (KL) divergence between the target distribution q and that the estimate P. This error decomposes into bias and variance terms (Heskes, 1998):</context>
<context position="27402" citStr="Marcu and Wong, 2002" startWordPosition="4354" endWordPosition="4357">of all lengths. To investigate this, for our final experiment we used a plain MLE estimate of the switch grammar to translate, limiting the grammar’s phrase-pair emission rules to only those which involve minimal phrase-pairs. The very low score of 17.82 BLEU (without lexical smoothing) not only highlights the performance gains of using longer phrase-pairs in hierarchical translation models, but most importantly provides a strong incentive to address the overfitting behaviour of MLE estimators for such models, instead of avoiding it. 6 Related work Most learning of phrase-based models, e.g., (Marcu and Wong, 2002; DeNero et al., 2006; Mylonakis and Sima’an, 2008), works without hierarchical components (i.e., not based on the explicit learning of an SCFG/BITG). These learning problems pose other kinds of learning challenges than the ones posed by explicit learning of SCFGs. Chiang’s original work (Chiang, 2007) is also related. Yet, the learning problem is not expressed in terms of an explicit objective function because surface heuristic counts are used. It has been very difficult to match the performance of Chiang’s model without use of these heuristic counts. A somewhat related work, (Blunsom et al.,</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu and W. Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of Empirical methods in natural language processing, pages 133–139. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Phrase translation probabilities with itg priors and smoothing as learning objective.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>630--639</pages>
<location>Honolulu, USA,</location>
<marker>Mylonakis, Sima’an, 2008</marker>
<rawString>Markos Mylonakis and Khalil Sima’an. 2008. Phrase translation probabilities with itg priors and smoothing as learning objective. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 630–639, Honolulu, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2859" citStr="Och and Ney, 2003" startWordPosition="406" endWordPosition="409">ctions probabilities (for monotone or switching productions). Theoretically speaking, both kinds of preferences may involve latent structure relative to the parallel corpus. The mapping between source-target sentence pairs can be expressed in terms of latent phrase segmentations and latent word/phrasealignments, and the hierarchical phrase reordering can be expressed in terms of latent binary synchronous hierarchical structures (cf. Fig. 1). But each of these three kinds of latent structures may be made explicit using external resources: word-alignment could be considered solved using Giza++ (Och and Ney, 2003)), phrase pairs can be obtained from these word-alignments (Och and Ney, 2004), and the hierarchical synchronous structure can be grown over source/target linguistic syntactic trees output by an existing parser. The Joint Phrase Translation Model (Marcu and Wong, 2002) constitutes a specific case, albeit without the hierarchical, synchronous reordering Start 5 —* X1 / X1 (1) Monotone X —* X1 X2 /X1 X2 (2) Switching X —* X1 X2 /X2 X1 (3) Emission X —* e / f (4) Figure 1: A phrase-pair SCFG (BITG) 117 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117–</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1613" citStr="Och and Ney, 2004" startWordPosition="219" endWordPosition="222">es, whereas our mechanism works with the lexical content of phrase pairs (akin to standard phrase-based systems). Surprisingly, our experiments on French-English data show that our learning method applied to far simpler models exhibits performance indistinguishable from the Hiero system. 1 Introduction A fundamental problem in phrase-based machine translation concerns the learning of a probabilistic synchronous context-free grammar (SCFG) over phrase pairs from an input parallel corpus. Chiang’s Hiero system (Chiang, 2007) exemplifies the gains to be had by combining phrase-based translation (Och and Ney, 2004) with the hierarchical reordering capabilities of SCFGs, particularly originating from Binary Inversion Transduction Grammars (BITG) (Wu, 1997). Yet, existing empirical work is largely based on successful heuristic techniques, and the learning of Hiero-like BITG/SCFG remains an unsolved problem, The difficulty of this problem stems from the need for simultaneously learning of two kinds of preferences (see Fig.1) (1) lexical translation probabilities (P((e, f) |X)) of source (f) and target (e) phrase pairs, and (2) phrase reordering preferences of a target string relative to a source string, ex</context>
<context position="2937" citStr="Och and Ney, 2004" startWordPosition="418" endWordPosition="421">eaking, both kinds of preferences may involve latent structure relative to the parallel corpus. The mapping between source-target sentence pairs can be expressed in terms of latent phrase segmentations and latent word/phrasealignments, and the hierarchical phrase reordering can be expressed in terms of latent binary synchronous hierarchical structures (cf. Fig. 1). But each of these three kinds of latent structures may be made explicit using external resources: word-alignment could be considered solved using Giza++ (Och and Ney, 2003)), phrase pairs can be obtained from these word-alignments (Och and Ney, 2004), and the hierarchical synchronous structure can be grown over source/target linguistic syntactic trees output by an existing parser. The Joint Phrase Translation Model (Marcu and Wong, 2002) constitutes a specific case, albeit without the hierarchical, synchronous reordering Start 5 —* X1 / X1 (1) Monotone X —* X1 X2 /X1 X2 (2) Switching X —* X1 X2 /X2 X1 (3) Emission X —* e / f (4) Figure 1: A phrase-pair SCFG (BITG) 117 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117–125, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Li</context>
<context position="19003" citStr="Och and Ney, 2004" startWordPosition="3009" endWordPosition="3012">l. Together with the constraints that render a proper probabilistic SCFG1, this defines the parameter space. The extractors used in this paper create SCFGs productions of two different kinds: (a) hierarchical synchronous productions that define the space of possible derivations up to the level of the SCFG pre-terminals, and (2) the phrase-pair emission rules that expand the pre-terminals to phrase-pairs of varying lengths. Given the word-alignments, the set of phrase-pairs extracted is the set of all translational equivalents (without length upperbound) under the word-alignment as defined in (Och and Ney, 2004; Koehn et al., 2003). Below we focus on the two grammar extractors employed in our experiments. We start out from the most generic, BITG-like formulation, and aim at incremental refinement of the hierarchical productions in order to capture relevant, content-based phrase-pair reordering preferences in the training data. Single non-terminal SCFG This is a phrasebased binary SCFG grammar employing a single non-terminal X covering each extracted phrasepair. The other productions consist of monotone and switching expansions of phrase-pair spans covered by X. Finally, the whole sentence-pair is co</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="22758" citStr="Och, 2003" startWordPosition="3628" endWordPosition="3629"> probability of a grammar derivation together with the language model probability of the target string. The model is further smoothed, similarly to phrase-based models and the Hiero system, with smoothing features φZ such as the lexical translation scores of the phrase-pairs involved and rule usage penalties. As usual with statistical translation, we aim for retrieving the target sentence e corresponding to the most probable derivation D =*=&gt;. (f, e) with rules r, with: p(D) a p(e)-l�—ps,fg(e, f)-lscf9 ri ri φZ(r)-li Z rED The interpolation weights are tuned using Minimum Error Rate Training (Och, 2003). 5.2 Results We test empirically the learner’s output grammars for translating from French to English, using k = 5 for the Cross Validation data partitioning. The training material is a GIZA++ wordaligned corpus of 200K sentence-pairs from the Europarl corpus (Koehn, 2005), with our development and test parallel corpora of 2K sentencepairs stemming from the same source. Training the grammar parameters until convergence demands around 6 hours on an 8-core 2.26 GHz Intel Xeon system. Decoding employs a 4-gram language model, trained on English Europarl data of 19.5M words, smoothed using modifi</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1756" citStr="Wu, 1997" startWordPosition="241" endWordPosition="242">h-English data show that our learning method applied to far simpler models exhibits performance indistinguishable from the Hiero system. 1 Introduction A fundamental problem in phrase-based machine translation concerns the learning of a probabilistic synchronous context-free grammar (SCFG) over phrase pairs from an input parallel corpus. Chiang’s Hiero system (Chiang, 2007) exemplifies the gains to be had by combining phrase-based translation (Och and Ney, 2004) with the hierarchical reordering capabilities of SCFGs, particularly originating from Binary Inversion Transduction Grammars (BITG) (Wu, 1997). Yet, existing empirical work is largely based on successful heuristic techniques, and the learning of Hiero-like BITG/SCFG remains an unsolved problem, The difficulty of this problem stems from the need for simultaneously learning of two kinds of preferences (see Fig.1) (1) lexical translation probabilities (P((e, f) |X)) of source (f) and target (e) phrase pairs, and (2) phrase reordering preferences of a target string relative to a source string, expressed in synchronous productions probabilities (for monotone or switching productions). Theoretically speaking, both kinds of preferences may</context>
<context position="9717" citStr="Wu, 1997" startWordPosition="1505" endWordPosition="1506">refer to SCFGs we will be pointing to their stochastic extension. The rank of an SCFG is defined as the maximum number of non-terminals in a grammar’s rule right-hand side. Contrary to monolingual Context Free Grammars, there does not always exist a conversion of an SCFG of a higher rank to one of a lower rank with the same language of string pairs. For this, most machine translation applications focus on SCFGs of rank two (binary SCFGs), or binarisable ones witch can be converted to a binary SCFG, given that these seem to cover most of the translation phenomena encountered in language pairs (Wu, 1997) and the related processing algorithms are less demanding computationally. Although SCFGS were initially introduced for machine translation as a stochastic word-based translation process in the form of the InversionTransduction Grammar (Wu, 1997), they were actually able to offer state-of-the-art performance in their latter phrase-based implementation by Chiang (Chiang, 2005). Chiang’s Hiero hierarchical translation system is based on a synchronous grammar with a single non-terminal X covering all learned phrase-pairs. Beginning from the start symbol S, an initial phrase-span structure is cons</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore Quirk</author>
<author>D Gildea</author>
</authors>
<title>Bayesian learning of non-compositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>97--105</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<marker>Quirk, Gildea, 2008</marker>
<rawString>H. Zhang, Ch. Quirk, R. C. Moore, and D. Gildea. 2008. Bayesian learning of non-compositional phrases with synchronous parsing. In Proceedings of ACL-08: HLT, pages 97–105, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>K Sima’an</author>
</authors>
<title>An efficient and consistent estimator for data-oriented parsing.</title>
<date>2006</date>
<journal>Journal ofAutomata, Languages and Combinatorics (JALC),</journal>
<volume>10</volume>
<pages>2--3</pages>
<marker>Zollmann, Sima’an, 2006</marker>
<rawString>A. Zollmann and K. Sima’an. 2006. An efficient and consistent estimator for data-oriented parsing. Journal ofAutomata, Languages and Combinatorics (JALC), 10 (2005) Number 2/3:367–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="10914" citStr="Zollmann and Venugopal, 2006" startWordPosition="1699" endWordPosition="1702">al phrase-span structure is constructed monotonically using a simple ‘glue grammar’: S —*S1 X2 / S1 X2 S —*X1 / X1 The true power of the system lies in expanding these initial phrase-spans with a set of hierarchical translation rules, which allow conditioning reordering decisions based on lexical context. For the French to English language pair, some examples would be: S —* X1 economiques / financial X1 S —* cette X1 de X2 / this X1 X2 S —* politique X1 commune de X2 / X20 s common X1 policy Further work builds on the Hiero grammar to expand it with constituency syntax motivated nonterminals (Zollmann and Venugopal, 2006). 3 Synchronous Grammar Learning The learning of phrase-based stochastic SCFGs with a Maximum Likelihood objective is exposed to overfitting as other all-fragment models such as Phrase-Based SMT (PBSMT) (Marcu and Wong, 2002; DeNero et al., 2006) and Data-Oriented Parsing (DOP) (Bod et al., 2003; Zollmann and Sima’an, 2006). Maximum Likelihood Estimation (MLE) returns degenerate grammar estimates that memorise well the parallel training corpus but generalise poorly to unseen data. The bias-variance decomposition of the generalisation error Err sheds light on this learning problem. For an estim</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine Translation, pages 138–141, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>