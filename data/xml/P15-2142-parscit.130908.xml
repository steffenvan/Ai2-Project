<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002016">
<title confidence="0.997591">
Generative Incremental Dependency Parsing with Neural Networks
</title>
<author confidence="0.999578">
Jan Buys1 and Phil Blunsom1,2
</author>
<affiliation confidence="0.999708">
1Department of Computer Science, University of Oxford 2Google DeepMind
</affiliation>
<email confidence="0.998881">
{jan.buys,phil.blunsom}@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.994802" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999907307692308">
We propose a neural network model for
scalable generative transition-based de-
pendency parsing. A probability distri-
bution over both sentences and transi-
tion sequences is parameterised by a feed-
forward neural network. The model sur-
passes the accuracy and speed of previ-
ous generative dependency parsers, reach-
ing 91.1% UAS. Perplexity results show
a strong improvement over n-gram lan-
guage models, opening the way to the ef-
ficient integration of syntax into neural
models for language generation.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999712389830509">
Transition-based dependency parsers that perform
incremental local inference with a discrimina-
tive classifier offer an appealing trade-off be-
tween speed and accuracy (Nivre, 2008; Zhang
and Nivre, 2011; Choi and Mccallum, 2013).
Recently neural network transition-based depen-
dency parsers have been shown to give state-of-
the-art performance (Chen and Manning, 2014;
Dyer et al., 2015; Weiss et al., 2015). However,
the downstream integration of syntactic structure
in language understanding and generation tasks is
often done heuristically.
Neural networks have also been shown to be
powerful generative models for language mod-
elling (Bengio et al., 2003; Mikolov et al., 2010)
and machine translation (Kalchbrenner and Blun-
som, 2013; Devlin et al., 2014; Sutskever et al.,
2014). However, currently these models lack
awareness of syntax, which limits their ability to
include longer-distance dependencies even when
potentially unbounded contexts are used.
In this paper we propose a generative model for
incremental parsing that offers an efficient way to
incorporate syntactic information into a generative
model. It relies on the strength of neural networks
to overcome sparsity in the long conditioning con-
texts required for an accurate model, while also of-
fering a principled approach to learn dependency-
based word representations (Levy and Goldberg,
2014; Bansal et al., 2014).
Generative models for graph-based dependency
parsing (Eisner, 1996; Wallach et al., 2008) are
much less accurate than their discriminative coun-
terparts. Syntactic language models based on
PCFGs (Roark, 2001; Charniak, 2001) and incre-
mental parsing (Chelba and Jelinek, 2000; Emami
and Jelinek, 2005) have been proposed for speech
recognition and machine translation. However,
these models are also limited in either scalability,
expressiveness, or both. A generative transition-
based dependency parser based on recurrent neu-
ral networks (Titov and Henderson, 2007) obtains
high accuracy, but training and decoding is pro-
hibitively expensive.
We perform efficient linear-time decoding with
a particle filtering-based beam-search method
where derivations after pruned after every word
generation and the beam size depends on the un-
certainty in the model (Buys and Blunsom, 2015).
The model obtains 91.1% UAS on the WSJ,
which is 0.2% UAS better than the previous high-
est accuracy generative dependency parser (Titov
and Henderson, 2007), while also being much
more efficient. As a language model its perplex-
ity reaches 111.8, a 23% reduction over an n-
gram baseline, when combining supervised train-
ing with unsupervised fine-tuning. Finally, we find
that the model is able to generate sentences that
display both local and syntactic coherence.
</bodyText>
<page confidence="0.935403">
863
</page>
<bodyText confidence="0.297106">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 863–869,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.959078" genericHeader="method">
2 Generative Transition-based Parsing
</sectionHeader>
<bodyText confidence="0.9988836875">
Our parsing model is based on transition-based
arc-standard projective dependency parsing (Nivre
and Scholz, 2004). The generative formulation
is similar to previous generative transition-based
parsers (Titov and Henderson, 2007; Cohen et al.,
2011; Buys and Blunsom, 2015), and also related
to the joint tagging and parsing model of Bohnet
and Nivre (2012).
The model predicts a sequence of parsing tran-
sitions: A shift transition generates a word (and
its POS tag), while a reduce transition adds an arc
(i, l, j), where i is the head node, j the dependent
and l is the dependency label.
The joint probability distribution over a sen-
tence with words w1:n, tags t1:n and a transition
sequence a1:2n is defined as
</bodyText>
<equation confidence="0.9908175">
Hn (p(ti|hmi)p(wi|ti, hmi)
i=1
</equation>
<bodyText confidence="0.999307379310345">
where mi is the number of transitions that have
been performed when (ti, wi) is shifted and hj is
the conditioning context at the jth transition.
A parser configuration (a, Q, A) for sentence s
consists of a stack a of indices in s, an index Q to
the next word to be generated, and a set of arcs A.
The stack elements are referred to as a1,... , a|a|,
where a1 is the top element. For any node a,
lc1(a) refers to the leftmost child of a in A, and
rc1(a) to its rightmost child. A root node is added
to the beginning of the sentence, and the head
word of the sentence (we assume there is only one)
is the dependent of the root.
The initial configuration is ([], 0, 0), while A
terminal configuration is reached when Q &gt; |s|
and |a |= 1.
The transition types are shift, left-arc and right-
arc. Shift generates the next word of the sentence
and pushes it on the stack. Left-arc adds an arc
(a1, l, a2) and removes a2 from the stack. Right-
arc adds (a2, l, a1) and pops a1.
The parsing strategy adds arcs bottom-up. In
a valid transition sequence the last transition is a
right-arc from the root to the head word, and the
root node is not involved in any other dependen-
cies. We use an oracle to extract transition se-
quences from the training data: The oracle prefers
reduce over shift transitions when both may lead
to a valid derivation.
</bodyText>
<table confidence="0.875932166666667">
Order Elements
1 a1, a2, a3, a4
2 lc1(a1), rc1(a1), lc1(a2), rc1(a2)
lc2(a1), rc2(a1), lc2(a2), rc2(a2)
3 lc1(lc1(a1)),rc1(rc1(a1))
lc1(lc1(a2)),rc1(rc1(a2))
</table>
<tableCaption confidence="0.985171666666667">
Table 1: Conditioning context elements for neural
network input: First, second and third order de-
pendencies are used.
</tableCaption>
<sectionHeader confidence="0.999126" genericHeader="method">
3 Neural Network Model
</sectionHeader>
<bodyText confidence="0.995404260869565">
Our probability model is based on neural net-
work language models with distributed represen-
tations (Bengio et al., 2003; Mnih and Hinton,
2007), as well as feed-forward neural network
models for transition-based dependency pars-
ing (Chen and Manning, 2014; Weiss et al., 2015).
We estimate the distributions p(ti|hi), p(wi|ti, hi)
and p(aj|hj) with neural networks with shared in-
put and hidden layers but separate output layers.
The templates for the conditioning context used
are defined in Table 1. In the templates we ob-
tain sentence indexes, which are then mapped to
the corresponding words, tags and labels (for the
dependencies of 2nd and 3rd order elements). The
neural network allows us to include a large number
of elements without suffering from sparsity.
In the input layer we make use of additive rep-
resentations (Botha and Blunsom, 2014) so that
for each word input position i we can include the
word type, tag and other features, and learn input
representations for each of these. Each context
feature f has an input representation qf E RD.
The composite representation is computed as qi =
</bodyText>
<equation confidence="0.944501166666667">
E
f∈µ(wi) qf, where µ(wi) are the word features.
The hidden layer is then defined as
L
φ(h) = g( Cjqhj),
j=1
</equation>
<bodyText confidence="0.999957444444444">
where Cj E RDxD are transformation matrices
defined for each position in sequence h, L = |h|
and g is a (usually non-linear) activation function
applied element-wise. The matrices Cj can be ap-
proximated to be diagonal to reduce the number
of model parameters and speed up the model by
avoiding expensive matrix multiplications.
For the output layer predicting the next transi-
tion a, the hidden layer is mapped with a scoring
</bodyText>
<figure confidence="0.662554666666667">
mi+1 )p(aj|hj) ,
H
j=mi+1
</figure>
<page confidence="0.768834">
864
</page>
<bodyText confidence="0.7678788">
function
X(a, h) = kTa φ(h) + ea,
where ka is the transition output representation
and ea is the bias weight. The score is normalised
with the soft-max function:
</bodyText>
<equation confidence="0.983448">
exp(X(a, h))
p(a|h) =
Ea/EA exp(X(al, h)).
</equation>
<bodyText confidence="0.949793909090909">
The output layer for predicting the next tag has
a similar form, using the scoring function
τ(t, h) = tTt φ(h) + ot
for tag representation tt and bias ot.
The probability p(w|t, h) can be estimated
similarly. However, to reduce the computa-
tional cost of normalising over the entire vocab-
ulary, we factorize the probability as P(w|h) =
P(c|t, h)P(w|c, t, h), where c = c(w) is the
unique class of word w. For each c, let F(c) be
the set of words in that class. The vocabulary is
</bodyText>
<equation confidence="0.592414">
V
</equation>
<bodyText confidence="0.999905615384616">
clustered into approximately |V  |classes using
Brown clustering (Brown et al., 1992), reducing
the number of items to sum over in the normal-
isation factor from O(|V |) to O(V|V |). Class-
based factorization has been shown to be an effec-
tive strategy in normalizing neural language mod-
els (Baltescu and Blunsom, 2015),
The class prediction score is defined as
ψ(c, h) = sT c φ(h) + dc, where sc ∈ RD is the
output weight vector for class c and dc is the class
bias weight. The output layer then consists of a
softmax function for p(c|h) and another softmax
for the word prediction
</bodyText>
<equation confidence="0.989456333333333">
exp(`b(w, h))
p(w|c, h) =
Ew&apos;EΓ(c) exp(`b(w&apos;, h)),
</equation>
<bodyText confidence="0.999956461538462">
where `b(w, h) = rTwφ(h)+bw is the word scoring
function with output word representation rw and
bias weight bw.
The model is trained with minibatch stochas-
tic gradient descent (SGD) with Adagrad (Duchi
et al., 2011) and L2 regularisation, to minimise
the negative log likelihood of the joint distribu-
tion over parsed training sentences. For our ex-
periments we train the model while the training
objective improves, and choose the parameters of
the iteration with the best development set accu-
racy (early stopping). The model obtains high ac-
curacy with only a few training iterations.
</bodyText>
<sectionHeader confidence="0.995823" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.999985210526316">
Beam-search decoders for transition-based pars-
ing (Zhang and Clark, 2008) keep a beam of par-
tial derivations, advancing each derivation by one
transition at a time. When the size of the beam
exceeds a set threshold, the lowest-scoring deriva-
tions are removed. However, in an incremental
generative model we need to compare derivations
with the same number of words shifted, rather than
transitions performed. To let the decoding time re-
main linear, we also need to bound the total num-
ber of reduce transitions that can be performed
over all derivations between two shift transitions.
To achieve this, we use a decoding method re-
cently proposed for generative incremental pars-
ing (Buys and Blunsom, 2015) based on particle
filtering (Doucet et al., 2001), a sequential Monte
Carlo sampling method.
In the algorithm, a fixed number of particles
are divided among the partial derivations in the
beam. Suppose i words have been shifted in all the
derivations on the beam. To predict the next tran-
sition from derivation dj, its particles are divided
according to p(a|h). In practice, adding only shift
and the most likely reduce transition leads to al-
most no accuracy loss. After all the derivations
have been advanced to shift word i+1, a selection
step is performed: The number of particles of each
derivation is redistributed according to its proba-
bility, weighted by its current number of particles.
Some derivations may be assigned 0 particles, in
which case they are removed.
The particle filtering method lets the beam size
depend of the uncertainty of the model, somewhat
similar to Choi and Mccallum (2013), while fixing
the total number of particles constrains the decod-
ing time to be linear. The particle filter also allow
us to sample outputs, and to marginalise over the
syntax when generating.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999971833333333">
We evaluate our model for parsing and language
modelling on the English Penn Treebank (Marcus
et al., 1993) WSJ parsing setup1. Constituency
trees are converted to projective CoNLL syntac-
tic dependencies (Johansson and Nugues, 2007)
with the LTH converter2. For some experiments
</bodyText>
<footnote confidence="0.994262333333333">
1Training on sections 02-21, development on section 22,
and testing on section 23.
2http://nlp.cs.lth.se/software/treebank converter/
</footnote>
<page confidence="0.985432">
865
</page>
<table confidence="0.9996438">
Activation UAS LAS
linear 88.40 86.48
rectifier 89.99 88.31
tanh 90.91 89.22
sigmoid 91.48 89.94
</table>
<tableCaption confidence="0.87854">
Table 2: Parsing accuracies using different neural
network activation functions.
</tableCaption>
<table confidence="0.9995952">
Model UAS LAS
Wallach et al. (2008) 85.7 -
Titov and Henderson (2007) 90.93 89.42
NN-GenDP 91.11 89.41
Chen and Manning (2014) 92.0 90.7
</table>
<tableCaption confidence="0.901519">
Table 3: Parsing accuracies for dependency
parsers on the WSJ test set, CoNLL dependencies.
</tableCaption>
<bodyText confidence="0.998232913043478">
we also use the Stanford dependency representa-
tion (De Marneffe and Manning, 2008) (SD)3.
Our neural network implementation is partly
based on the OxLM neural language modelling
framework (Baltescu et al., 2014). The model pa-
rameters are initialised randomly by drawing from
a Gaussian distribution with mean 0 and variance
0.1, except for the bias weights, which are ini-
tialised by the unigram distributions of their out-
put. We use minibatches of size 128, the L2 regu-
larization parameter is 10, and the word represen-
tation and hidden layer of size is 256. The Ada-
grad learning rate is initialised to 0.05.
POS tags for the development and test sets are
obtained with the Stanford POS tagger (Toutanova
et al., 2003), with 97.5% test set accuracy. Words
that occur only once in the training data are treated
as unknown words. Unknown words are replaced
by tokens representing morphological surface fea-
tures (based on capitalization, numbers, punctua-
tion and common suffixes) similar to those used
in the implementation of generative constituency
parsers (Klein and Manning, 2003).
</bodyText>
<subsectionHeader confidence="0.999229">
5.1 Parsing results
</subsectionHeader>
<bodyText confidence="0.999972733333333">
We report unlabelled attachment score (UAS) and
labelled attachment score (LAS) in our results,
excluding punctuation. On the development set,
we consider the effect of the choice of activation
function (Table 2), finding that a sigmoid activa-
tion (logistic function) performs best, following by
tanh. Under our training setup the model can ob-
tain up to 91.0 UAS after only 1 training iteration,
thereby performing pure online learning.
We found that including third order depen-
dencies in the conditioning context performs just
0.1% UAS better than including only first and sec-
ond order dependencies. Including additional ele-
ments does not improve performance further. The
model can obtain 91.18 UAS, 89.02 LAS when
</bodyText>
<footnote confidence="0.997575">
3Converted with version 3.4.1 of the Stanford parser,
available at http::/nlp.stanford.edu/software/lex-parser.shtml.
</footnote>
<bodyText confidence="0.999694913043478">
trained only on words, not POS tags. Dependency
parsers that do not use distributed representations
tend to rely much more on the tags.
Test set results comparing generative depen-
dency parsers are given in Table 3 (our model is
refered to as NN-GenDP). The graph-based gen-
erative baseline (Wallach et al., 2008), parame-
terised by Pitman-Yor Processes, is quite weak.
Our model outperforms the generative model of
Titov and Henderson (2007), which we retrained
on our dataset, by 0.2%, despite that model be-
ing able to condition on arbitrary-sized contexts.
The decoding speed of our model is around 20 sen-
tences per second, against less than 1 sentence per
second for Titov and Henderson’s model. Using
diagonal transformation matrices further increases
our model’s speed, but reduces parsing accuracy.
On the Stanford dependency representation our
model obtains 90.63% UAS, 88.27% LAS. Al-
though this performance is promising, it is still
below the discriminative neural network models of
Dyer et al. (2015) and Weiss et al. (2015), who ob-
tained 93.1% UAS and 94.0% UAS respectively.
</bodyText>
<subsectionHeader confidence="0.999557">
5.2 Language modelling
</subsectionHeader>
<bodyText confidence="0.999986066666667">
We also evaluate our parser as a language model,
on the same WSJ data used for the parsing eval-
uation4. We perform unlabelled parsing, as ex-
periments show that including labels in the con-
ditioning context has a very small impact on per-
formance. Neither do we use POS tags, as they
are too expensive to predict in language genera-
tion applications.
Perplexity results on the WSJ are given in Ta-
ble 4. As baselines we report results on modified
Knesser-Ney (Kneser and Ney, 1995) and neu-
ral network 5-gram models. For our dependency-
based language models we report perplexities
based on the most likely parse found by the de-
coder, which gives an upper bound on the true
</bodyText>
<footnote confidence="0.996107333333333">
4However instead of using multiple unknown word
classes, we replace all numbers by 0 and have a single un-
known word token.
</footnote>
<page confidence="0.996879">
866
</page>
<bodyText confidence="0.9735888">
the u.s. union board said revenue rose 11 % to $ NUM million , or $ NUM a share.
mr. bush has UNK-ed a plan to buy the company for $ NUM to NUM million, or $ NUM a share.
the plan was UNK-ed by the board ’s decision to sell its $ NUM million UNK loan loan funds.
in stocks coming months, china ’s NUM shares rose 10 cents to $ NUM million , or $ NUM a share.
in the case, mr. bush said it will sell the company business UNK concern to buy the company.
it was NUM common shares in addition, with $ NUM million , or $ NUM a share, according to mr. bush.
in the first quarter , 1989 shares closed yesterday at $ NUM , mr. bush has increased the plan .
last year ’s retrenchment price index index rose 11 cents to $ NUM million , or $ NUM million is asked .
last year earlier, net income rose 11 million % to $ NUM million , or 91 cents a share.
the u.s. union has UNK-ed $ NUM million , or 22 cents a share, in 1990 , payable nov. 9 .
</bodyText>
<tableCaption confidence="0.994168">
Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
</tableCaption>
<table confidence="0.8590948">
Model Perplexity
KN 5-gram 145.7
NN 5-gram 142.5
NN-GenDP 132.2
NN-GenDP + unsup 111.8
</table>
<tableCaption confidence="0.61772">
Table 4: WSJ Language modelling test results.
</tableCaption>
<bodyText confidence="0.998851452380952">
We compare our model, with and without unsu-
pervised tuning, to n-gram baselines.
value of the model perplexity.
First we only perform standard supervised train-
ing with the model - this already leads to an im-
provement of 10 perplexity points over the neu-
ral n-gram model. Second we consider a train-
ing setup where we first perform 5 supervised it-
erations, and then perform unsupervised training,
treating the transition sequence as latent. For each
minibatch parse trees are sampled with a parti-
cle filter. This approach further improves the per-
plexity to 111.8, a 23% reduction relative to the
Knesser-Ney model.
The unsupervised training stage lets the parsing
accuracy fall from 91.48 to 89.49 UAS. We pos-
tulate that the model is learning to make small ad-
justments to favour of parsing structures that ex-
plain the data better than the annotated parse trees,
leading to the improvement in perplexity.
To test the scalability of our model, we also
trained it on a larger unannotated corpus – a sub-
set (of around 7 million words) of the billion word
language modeling benchmark dataset (Chelba et
al., 2013). After training the model on the WSJ,
we parsed the unannotated data with the model,
and continued to train on the obtained parses.
We observed a small increase in perplexity, from
203.5 for a neural n-gram model to 200.7 for the
generative dependency model. We expect larger
improvements when training on more data and
with more sophisticated inference.
To evaluate our generative model qualitatively,
we perform unconstrained generation of sentences
(and parse trees) from the model, and found that
sentences display a higher degree of syntactic co-
herence than sentences generated by an n-gram
model. See Table 5 for examples generated by the
model. The highest-scoring sentences of length 20
or more are given, from 1000 samples generated.
Note that the generation includes unknown word
tokens (here NUM, UNK and UNK-ed are used).
</bodyText>
<sectionHeader confidence="0.996639" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999879571428571">
We presented an incremental generative depen-
dency parser that can obtain accuracies competi-
tive with discriminative models. The same model
can be applied as an efficient syntactic language
model, and for future work it should be integrated
into language generation tasks such as machine
translation.
</bodyText>
<sectionHeader confidence="0.971578" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997032">
We acknowledge the financial support of the Ox-
ford Clarendon Fund and the Skye Foundation.
</bodyText>
<sectionHeader confidence="0.997544" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994990714285714">
Paul Baltescu and Phil Blunsom. 2015. Pragmatic neu-
ral language modelling in machine translation. In
Proceedings of NAACL-HTL, pages 820–829.
Paul Baltescu, Phil Blunsom, and Hieu Hoang. 2014.
Oxlm: A neural language modelling framework for
machine translation. The Prague Bulletin of Mathe-
matical Linguistics, 102(1):81–92, October.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the ACL.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
</reference>
<page confidence="0.992051">
867
</page>
<reference confidence="0.988580704761905">
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of EMNLP-CONLL, pages 1455–1465.
Jan A. Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In Proceedings of the 31st International
Conference on Machine Learning (ICML).
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467–479.
Jan Buys and Phil Blunsom. 2015. A Bayesian model
for generative transition-based dependency parsing.
arXiv preprint arXiv:1506.04334.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of ACL, pages
124–131. Association for Computational Linguis-
tics.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech &amp; Lan-
guage, 14(4):283–332.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. Tech-
nical report, Google.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of EMNLP.
Jinho D. Choi and Andrew Mccallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of ACL.
Shay B. Cohen, Carlos G´omez-Rodr´ıguez, and Giorgio
Satta. 2011. Exact inference for generative proba-
bilistic non-projective dependency parsing. In Pro-
ceedings of EMNLP, pages 1234–1245.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1–8.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of ACL,
pages 1370–1380.
Arnaud Doucet, Nando De Freitas, and Neil Gordon.
2001. Sequential Monte Carlo methods in practice.
Springer.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Joural of Machine
Learning Research, 12:2121–2159.
Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of ACL 2015.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of COLING, pages 340–345.
Ahmad Emami and Frederick Jelinek. 2005. A neural
syntactic language model. Machine Learning, 60(1-
3):195–227.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In 16th Nordic Conference of Computa-
tional Linguistics, pages 105–112, Tartu, Estonia.
Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. In Proceedings
of EMNLP, pages 1700–1709.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL,
pages 423–430.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
ICASSP, volume 1, pages 181–184. IEEE.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of ACL:
Short Papers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th International Conference
on Machine learning, pages 641–648.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational linguistics,
27(2):249–276.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.
</reference>
<page confidence="0.981787">
868
</page>
<reference confidence="0.99892284">
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the Tenth International Conference
on Parsing Technologies, pages 144–155.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL, pages 173–180.
Hanna M Wallach, Charles Sutton, and Andrew Mc-
Callum. 2008. Bayesian modeling of dependency
trees using hierarchical Pitman-Yor priors. In ICML
Workshop on Prior Knowledge for Text and Lan-
guage Processing.
David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural net-
work transition-based parsing. In Proceedings of
ACL 2015.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of EMNLP, pages 562–571.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings ofACL-HLT: Short papers, pages 188–
193.
</reference>
<page confidence="0.998908">
869
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805898">
<title confidence="0.998773">Generative Incremental Dependency Parsing with Neural Networks</title>
<affiliation confidence="0.822144">of Computer Science, University of Oxford</affiliation>
<abstract confidence="0.999577928571429">We propose a neural network model for scalable generative transition-based dependency parsing. A probability distribution over both sentences and transition sequences is parameterised by a feedforward neural network. The model surpasses the accuracy and speed of previous generative dependency parsers, reach- UAS. Perplexity results show strong improvement over language models, opening the way to the efficient integration of syntax into neural models for language generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul Baltescu</author>
<author>Phil Blunsom</author>
</authors>
<title>Pragmatic neural language modelling in machine translation.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL-HTL,</booktitle>
<pages>820--829</pages>
<contexts>
<context position="8806" citStr="Baltescu and Blunsom, 2015" startWordPosition="1425" endWordPosition="1428">ty p(w|t, h) can be estimated similarly. However, to reduce the computational cost of normalising over the entire vocabulary, we factorize the probability as P(w|h) = P(c|t, h)P(w|c, t, h), where c = c(w) is the unique class of word w. For each c, let F(c) be the set of words in that class. The vocabulary is V clustered into approximately |V |classes using Brown clustering (Brown et al., 1992), reducing the number of items to sum over in the normalisation factor from O(|V |) to O(V|V |). Classbased factorization has been shown to be an effective strategy in normalizing neural language models (Baltescu and Blunsom, 2015), The class prediction score is defined as ψ(c, h) = sT c φ(h) + dc, where sc ∈ RD is the output weight vector for class c and dc is the class bias weight. The output layer then consists of a softmax function for p(c|h) and another softmax for the word prediction exp(`b(w, h)) p(w|c, h) = Ew&apos;EΓ(c) exp(`b(w&apos;, h)), where `b(w, h) = rTwφ(h)+bw is the word scoring function with output word representation rw and bias weight bw. The model is trained with minibatch stochastic gradient descent (SGD) with Adagrad (Duchi et al., 2011) and L2 regularisation, to minimise the negative log likelihood of the</context>
</contexts>
<marker>Baltescu, Blunsom, 2015</marker>
<rawString>Paul Baltescu and Phil Blunsom. 2015. Pragmatic neural language modelling in machine translation. In Proceedings of NAACL-HTL, pages 820–829.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Baltescu</author>
<author>Phil Blunsom</author>
<author>Hieu Hoang</author>
</authors>
<title>Oxlm: A neural language modelling framework for machine translation.</title>
<date>2014</date>
<journal>The Prague Bulletin of Mathematical Linguistics,</journal>
<volume>102</volume>
<issue>1</issue>
<contexts>
<context position="12568" citStr="Baltescu et al., 2014" startWordPosition="2034" endWordPosition="2037">865 Activation UAS LAS linear 88.40 86.48 rectifier 89.99 88.31 tanh 90.91 89.22 sigmoid 91.48 89.94 Table 2: Parsing accuracies using different neural network activation functions. Model UAS LAS Wallach et al. (2008) 85.7 - Titov and Henderson (2007) 90.93 89.42 NN-GenDP 91.11 89.41 Chen and Manning (2014) 92.0 90.7 Table 3: Parsing accuracies for dependency parsers on the WSJ test set, CoNLL dependencies. we also use the Stanford dependency representation (De Marneffe and Manning, 2008) (SD)3. Our neural network implementation is partly based on the OxLM neural language modelling framework (Baltescu et al., 2014). The model parameters are initialised randomly by drawing from a Gaussian distribution with mean 0 and variance 0.1, except for the bias weights, which are initialised by the unigram distributions of their output. We use minibatches of size 128, the L2 regularization parameter is 10, and the word representation and hidden layer of size is 256. The Adagrad learning rate is initialised to 0.05. POS tags for the development and test sets are obtained with the Stanford POS tagger (Toutanova et al., 2003), with 97.5% test set accuracy. Words that occur only once in the training data are treated as</context>
</contexts>
<marker>Baltescu, Blunsom, Hoang, 2014</marker>
<rawString>Paul Baltescu, Phil Blunsom, and Hieu Hoang. 2014. Oxlm: A neural language modelling framework for machine translation. The Prague Bulletin of Mathematical Linguistics, 102(1):81–92, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="2102" citStr="Bansal et al., 2014" startWordPosition="303" endWordPosition="306">ver et al., 2014). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this paper we propose a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations (Levy and Goldberg, 2014; Bansal et al., 2014). Generative models for graph-based dependency parsing (Eisner, 1996; Wallach et al., 2008) are much less accurate than their discriminative counterparts. Syntactic language models based on PCFGs (Roark, 2001; Charniak, 2001) and incremental parsing (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transitionbased dependency parser based on recurrent neural networks (Titov and Henderson, 2007) obtains high accuracy, but trainin</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1374" citStr="Bengio et al., 2003" startWordPosition="193" endWordPosition="196"> that perform incremental local inference with a discriminative classifier offer an appealing trade-off between speed and accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this paper we propose a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while </context>
<context position="6258" citStr="Bengio et al., 2003" startWordPosition="982" endWordPosition="985">t involved in any other dependencies. We use an oracle to extract transition sequences from the training data: The oracle prefers reduce over shift transitions when both may lead to a valid derivation. Order Elements 1 a1, a2, a3, a4 2 lc1(a1), rc1(a1), lc1(a2), rc1(a2) lc2(a1), rc2(a1), lc2(a2), rc2(a2) 3 lc1(lc1(a1)),rc1(rc1(a1)) lc1(lc1(a2)),rc1(rc1(a2)) Table 1: Conditioning context elements for neural network input: First, second and third order dependencies are used. 3 Neural Network Model Our probability model is based on neural network language models with distributed representations (Bengio et al., 2003; Mnih and Hinton, 2007), as well as feed-forward neural network models for transition-based dependency parsing (Chen and Manning, 2014; Weiss et al., 2015). We estimate the distributions p(ti|hi), p(wi|ti, hi) and p(aj|hj) with neural networks with shared input and hidden layers but separate output layers. The templates for the conditioning context used are defined in Table 1. In the templates we obtain sentence indexes, which are then mapped to the corresponding words, tags and labels (for the dependencies of 2nd and 3rd order elements). The neural network allows us to include a large number</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CONLL,</booktitle>
<pages>1455--1465</pages>
<contexts>
<context position="4119" citStr="Bohnet and Nivre (2012)" startWordPosition="597" endWordPosition="600">sociation for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 863–869, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Generative Transition-based Parsing Our parsing model is based on transition-based arc-standard projective dependency parsing (Nivre and Scholz, 2004). The generative formulation is similar to previous generative transition-based parsers (Titov and Henderson, 2007; Cohen et al., 2011; Buys and Blunsom, 2015), and also related to the joint tagging and parsing model of Bohnet and Nivre (2012). The model predicts a sequence of parsing transitions: A shift transition generates a word (and its POS tag), while a reduce transition adds an arc (i, l, j), where i is the head node, j the dependent and l is the dependency label. The joint probability distribution over a sentence with words w1:n, tags t1:n and a transition sequence a1:2n is defined as Hn (p(ti|hmi)p(wi|ti, hmi) i=1 where mi is the number of transitions that have been performed when (ti, wi) is shifted and hj is the conditioning context at the jth transition. A parser configuration (a, Q, A) for sentence s consists of a stac</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of EMNLP-CONLL, pages 1455–1465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan A Botha</author>
<author>Phil Blunsom</author>
</authors>
<title>Compositional morphology for word representations and language modelling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="6988" citStr="Botha and Blunsom, 2014" startWordPosition="1100" endWordPosition="1103">rsing (Chen and Manning, 2014; Weiss et al., 2015). We estimate the distributions p(ti|hi), p(wi|ti, hi) and p(aj|hj) with neural networks with shared input and hidden layers but separate output layers. The templates for the conditioning context used are defined in Table 1. In the templates we obtain sentence indexes, which are then mapped to the corresponding words, tags and labels (for the dependencies of 2nd and 3rd order elements). The neural network allows us to include a large number of elements without suffering from sparsity. In the input layer we make use of additive representations (Botha and Blunsom, 2014) so that for each word input position i we can include the word type, tag and other features, and learn input representations for each of these. Each context feature f has an input representation qf E RD. The composite representation is computed as qi = E f∈µ(wi) qf, where µ(wi) are the word features. The hidden layer is then defined as L φ(h) = g( Cjqhj), j=1 where Cj E RDxD are transformation matrices defined for each position in sequence h, L = |h| and g is a (usually non-linear) activation function applied element-wise. The matrices Cj can be approximated to be diagonal to reduce the numbe</context>
</contexts>
<marker>Botha, Blunsom, 2014</marker>
<rawString>Jan A. Botha and Phil Blunsom. 2014. Compositional morphology for word representations and language modelling. In Proceedings of the 31st International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="8575" citStr="Brown et al., 1992" startWordPosition="1384" endWordPosition="1387">x function: exp(X(a, h)) p(a|h) = Ea/EA exp(X(al, h)). The output layer for predicting the next tag has a similar form, using the scoring function τ(t, h) = tTt φ(h) + ot for tag representation tt and bias ot. The probability p(w|t, h) can be estimated similarly. However, to reduce the computational cost of normalising over the entire vocabulary, we factorize the probability as P(w|h) = P(c|t, h)P(w|c, t, h), where c = c(w) is the unique class of word w. For each c, let F(c) be the set of words in that class. The vocabulary is V clustered into approximately |V |classes using Brown clustering (Brown et al., 1992), reducing the number of items to sum over in the normalisation factor from O(|V |) to O(V|V |). Classbased factorization has been shown to be an effective strategy in normalizing neural language models (Baltescu and Blunsom, 2015), The class prediction score is defined as ψ(c, h) = sT c φ(h) + dc, where sc ∈ RD is the output weight vector for class c and dc is the class bias weight. The output layer then consists of a softmax function for p(c|h) and another softmax for the word prediction exp(`b(w, h)) p(w|c, h) = Ew&apos;EΓ(c) exp(`b(w&apos;, h)), where `b(w, h) = rTwφ(h)+bw is the word scoring functi</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Buys</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model for generative transition-based dependency parsing. arXiv preprint arXiv:1506.04334.</title>
<date>2015</date>
<contexts>
<context position="2979" citStr="Buys and Blunsom, 2015" startWordPosition="429" endWordPosition="432">lba and Jelinek, 2000; Emami and Jelinek, 2005) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transitionbased dependency parser based on recurrent neural networks (Titov and Henderson, 2007) obtains high accuracy, but training and decoding is prohibitively expensive. We perform efficient linear-time decoding with a particle filtering-based beam-search method where derivations after pruned after every word generation and the beam size depends on the uncertainty in the model (Buys and Blunsom, 2015). The model obtains 91.1% UAS on the WSJ, which is 0.2% UAS better than the previous highest accuracy generative dependency parser (Titov and Henderson, 2007), while also being much more efficient. As a language model its perplexity reaches 111.8, a 23% reduction over an ngram baseline, when combining supervised training with unsupervised fine-tuning. Finally, we find that the model is able to generate sentences that display both local and syntactic coherence. 863 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference </context>
<context position="10419" citStr="Buys and Blunsom, 2015" startWordPosition="1697" endWordPosition="1700"> beam of partial derivations, advancing each derivation by one transition at a time. When the size of the beam exceeds a set threshold, the lowest-scoring derivations are removed. However, in an incremental generative model we need to compare derivations with the same number of words shifted, rather than transitions performed. To let the decoding time remain linear, we also need to bound the total number of reduce transitions that can be performed over all derivations between two shift transitions. To achieve this, we use a decoding method recently proposed for generative incremental parsing (Buys and Blunsom, 2015) based on particle filtering (Doucet et al., 2001), a sequential Monte Carlo sampling method. In the algorithm, a fixed number of particles are divided among the partial derivations in the beam. Suppose i words have been shifted in all the derivations on the beam. To predict the next transition from derivation dj, its particles are divided according to p(a|h). In practice, adding only shift and the most likely reduce transition leads to almost no accuracy loss. After all the derivations have been advanced to shift word i+1, a selection step is performed: The number of particles of each derivat</context>
</contexts>
<marker>Buys, Blunsom, 2015</marker>
<rawString>Jan Buys and Phil Blunsom. 2015. A Bayesian model for generative transition-based dependency parsing. arXiv preprint arXiv:1506.04334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>124--131</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2327" citStr="Charniak, 2001" startWordPosition="336" endWordPosition="337">ve model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations (Levy and Goldberg, 2014; Bansal et al., 2014). Generative models for graph-based dependency parsing (Eisner, 1996; Wallach et al., 2008) are much less accurate than their discriminative counterparts. Syntactic language models based on PCFGs (Roark, 2001; Charniak, 2001) and incremental parsing (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transitionbased dependency parser based on recurrent neural networks (Titov and Henderson, 2007) obtains high accuracy, but training and decoding is prohibitively expensive. We perform efficient linear-time decoding with a particle filtering-based beam-search method where derivations after pruned after every word generation and the beam size depends on t</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of ACL, pages 124–131. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="2377" citStr="Chelba and Jelinek, 2000" startWordPosition="342" endWordPosition="345">ers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations (Levy and Goldberg, 2014; Bansal et al., 2014). Generative models for graph-based dependency parsing (Eisner, 1996; Wallach et al., 2008) are much less accurate than their discriminative counterparts. Syntactic language models based on PCFGs (Roark, 2001; Charniak, 2001) and incremental parsing (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transitionbased dependency parser based on recurrent neural networks (Titov and Henderson, 2007) obtains high accuracy, but training and decoding is prohibitively expensive. We perform efficient linear-time decoding with a particle filtering-based beam-search method where derivations after pruned after every word generation and the beam size depends on the uncertainty in the model (Buys and Blunsom, 201</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. Computer Speech &amp; Language, 14(4):283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Tomas Mikolov</author>
<author>Mike Schuster</author>
<author>Qi Ge</author>
<author>Thorsten Brants</author>
<author>Phillipp Koehn</author>
<author>Tony Robinson</author>
</authors>
<title>One billion word benchmark for measuring progress in statistical language modeling.</title>
<date>2013</date>
<tech>Technical report, Google.</tech>
<contexts>
<context position="18460" citStr="Chelba et al., 2013" startWordPosition="3047" endWordPosition="3050"> a particle filter. This approach further improves the perplexity to 111.8, a 23% reduction relative to the Knesser-Ney model. The unsupervised training stage lets the parsing accuracy fall from 91.48 to 89.49 UAS. We postulate that the model is learning to make small adjustments to favour of parsing structures that explain the data better than the annotated parse trees, leading to the improvement in perplexity. To test the scalability of our model, we also trained it on a larger unannotated corpus – a subset (of around 7 million words) of the billion word language modeling benchmark dataset (Chelba et al., 2013). After training the model on the WSJ, we parsed the unannotated data with the model, and continued to train on the obtained parses. We observed a small increase in perplexity, from 203.5 for a neural n-gram model to 200.7 for the generative dependency model. We expect larger improvements when training on more data and with more sophisticated inference. To evaluate our generative model qualitatively, we perform unconstrained generation of sentences (and parse trees) from the model, and found that sentences display a higher degree of syntactic coherence than sentences generated by an n-gram mod</context>
</contexts>
<marker>Chelba, Mikolov, Schuster, Ge, Brants, Koehn, Robinson, 2013</marker>
<rawString>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. Technical report, Google.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1084" citStr="Chen and Manning, 2014" startWordPosition="149" endWordPosition="152"> speed of previous generative dependency parsers, reaching 91.1% UAS. Perplexity results show a strong improvement over n-gram language models, opening the way to the efficient integration of syntax into neural models for language generation. 1 Introduction Transition-based dependency parsers that perform incremental local inference with a discriminative classifier offer an appealing trade-off between speed and accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this</context>
<context position="6393" citStr="Chen and Manning, 2014" startWordPosition="1002" endWordPosition="1005">duce over shift transitions when both may lead to a valid derivation. Order Elements 1 a1, a2, a3, a4 2 lc1(a1), rc1(a1), lc1(a2), rc1(a2) lc2(a1), rc2(a1), lc2(a2), rc2(a2) 3 lc1(lc1(a1)),rc1(rc1(a1)) lc1(lc1(a2)),rc1(rc1(a2)) Table 1: Conditioning context elements for neural network input: First, second and third order dependencies are used. 3 Neural Network Model Our probability model is based on neural network language models with distributed representations (Bengio et al., 2003; Mnih and Hinton, 2007), as well as feed-forward neural network models for transition-based dependency parsing (Chen and Manning, 2014; Weiss et al., 2015). We estimate the distributions p(ti|hi), p(wi|ti, hi) and p(aj|hj) with neural networks with shared input and hidden layers but separate output layers. The templates for the conditioning context used are defined in Table 1. In the templates we obtain sentence indexes, which are then mapped to the corresponding words, tags and labels (for the dependencies of 2nd and 3rd order elements). The neural network allows us to include a large number of elements without suffering from sparsity. In the input layer we make use of additive representations (Botha and Blunsom, 2014) so t</context>
<context position="12254" citStr="Chen and Manning (2014)" startWordPosition="1986" endWordPosition="1989">l., 1993) WSJ parsing setup1. Constituency trees are converted to projective CoNLL syntactic dependencies (Johansson and Nugues, 2007) with the LTH converter2. For some experiments 1Training on sections 02-21, development on section 22, and testing on section 23. 2http://nlp.cs.lth.se/software/treebank converter/ 865 Activation UAS LAS linear 88.40 86.48 rectifier 89.99 88.31 tanh 90.91 89.22 sigmoid 91.48 89.94 Table 2: Parsing accuracies using different neural network activation functions. Model UAS LAS Wallach et al. (2008) 85.7 - Titov and Henderson (2007) 90.93 89.42 NN-GenDP 91.11 89.41 Chen and Manning (2014) 92.0 90.7 Table 3: Parsing accuracies for dependency parsers on the WSJ test set, CoNLL dependencies. we also use the Stanford dependency representation (De Marneffe and Manning, 2008) (SD)3. Our neural network implementation is partly based on the OxLM neural language modelling framework (Baltescu et al., 2014). The model parameters are initialised randomly by drawing from a Gaussian distribution with mean 0 and variance 0.1, except for the bias weights, which are initialised by the unigram distributions of their output. We use minibatches of size 128, the L2 regularization parameter is 10, </context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Andrew Mccallum</author>
</authors>
<title>Transition-based dependency parsing with selectional branching.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="947" citStr="Choi and Mccallum, 2013" startWordPosition="130" endWordPosition="133">ibution over both sentences and transition sequences is parameterised by a feedforward neural network. The model surpasses the accuracy and speed of previous generative dependency parsers, reaching 91.1% UAS. Perplexity results show a strong improvement over n-gram language models, opening the way to the efficient integration of syntax into neural models for language generation. 1 Introduction Transition-based dependency parsers that perform incremental local inference with a discriminative classifier offer an appealing trade-off between speed and accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). However, currently these models lack awareness</context>
<context position="11326" citStr="Choi and Mccallum (2013)" startWordPosition="1847" endWordPosition="1850">t transition from derivation dj, its particles are divided according to p(a|h). In practice, adding only shift and the most likely reduce transition leads to almost no accuracy loss. After all the derivations have been advanced to shift word i+1, a selection step is performed: The number of particles of each derivation is redistributed according to its probability, weighted by its current number of particles. Some derivations may be assigned 0 particles, in which case they are removed. The particle filtering method lets the beam size depend of the uncertainty of the model, somewhat similar to Choi and Mccallum (2013), while fixing the total number of particles constrains the decoding time to be linear. The particle filter also allow us to sample outputs, and to marginalise over the syntax when generating. 5 Experiments We evaluate our model for parsing and language modelling on the English Penn Treebank (Marcus et al., 1993) WSJ parsing setup1. Constituency trees are converted to projective CoNLL syntactic dependencies (Johansson and Nugues, 2007) with the LTH converter2. For some experiments 1Training on sections 02-21, development on section 22, and testing on section 23. 2http://nlp.cs.lth.se/software/</context>
</contexts>
<marker>Choi, Mccallum, 2013</marker>
<rawString>Jinho D. Choi and Andrew Mccallum. 2013. Transition-based dependency parsing with selectional branching. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Carlos G´omez-Rodr´ıguez</author>
<author>Giorgio Satta</author>
</authors>
<title>Exact inference for generative probabilistic non-projective dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1234--1245</pages>
<marker>Cohen, G´omez-Rodr´ıguez, Satta, 2011</marker>
<rawString>Shay B. Cohen, Carlos G´omez-Rodr´ıguez, and Giorgio Satta. 2011. Exact inference for generative probabilistic non-projective dependency parsing. In Proceedings of EMNLP, pages 1234–1245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D Manning. 2008. The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1370--1380</pages>
<contexts>
<context position="1474" citStr="Devlin et al., 2014" startWordPosition="209" endWordPosition="212">off between speed and accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this paper we propose a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations (Levy and Goldberg</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of ACL, pages 1370–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnaud Doucet</author>
<author>Nando De Freitas</author>
<author>Neil Gordon</author>
</authors>
<title>Sequential Monte Carlo methods in practice.</title>
<date>2001</date>
<publisher>Springer.</publisher>
<marker>Doucet, De Freitas, Gordon, 2001</marker>
<rawString>Arnaud Doucet, Nando De Freitas, and Neil Gordon. 2001. Sequential Monte Carlo methods in practice. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Joural of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="9336" citStr="Duchi et al., 2011" startWordPosition="1521" endWordPosition="1524"> an effective strategy in normalizing neural language models (Baltescu and Blunsom, 2015), The class prediction score is defined as ψ(c, h) = sT c φ(h) + dc, where sc ∈ RD is the output weight vector for class c and dc is the class bias weight. The output layer then consists of a softmax function for p(c|h) and another softmax for the word prediction exp(`b(w, h)) p(w|c, h) = Ew&apos;EΓ(c) exp(`b(w&apos;, h)), where `b(w, h) = rTwφ(h)+bw is the word scoring function with output word representation rw and bias weight bw. The model is trained with minibatch stochastic gradient descent (SGD) with Adagrad (Duchi et al., 2011) and L2 regularisation, to minimise the negative log likelihood of the joint distribution over parsed training sentences. For our experiments we train the model while the training objective improves, and choose the parameters of the iteration with the best development set accuracy (early stopping). The model obtains high accuracy with only a few training iterations. 4 Decoding Beam-search decoders for transition-based parsing (Zhang and Clark, 2008) keep a beam of partial derivations, advancing each derivation by one transition at a time. When the size of the beam exceeds a set threshold, the </context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Joural of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Miguel Ballesteros</author>
<author>Wang Ling</author>
<author>Austin Matthews</author>
<author>Noah A Smith</author>
</authors>
<title>Transitionbased dependency parsing with stack long shortterm memory.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1103" citStr="Dyer et al., 2015" startWordPosition="153" endWordPosition="156">ative dependency parsers, reaching 91.1% UAS. Perplexity results show a strong improvement over n-gram language models, opening the way to the efficient integration of syntax into neural models for language generation. 1 Introduction Transition-based dependency parsers that perform incremental local inference with a discriminative classifier offer an appealing trade-off between speed and accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this paper we propose a</context>
<context position="15300" citStr="Dyer et al. (2015)" startWordPosition="2463" endWordPosition="2466">the generative model of Titov and Henderson (2007), which we retrained on our dataset, by 0.2%, despite that model being able to condition on arbitrary-sized contexts. The decoding speed of our model is around 20 sentences per second, against less than 1 sentence per second for Titov and Henderson’s model. Using diagonal transformation matrices further increases our model’s speed, but reduces parsing accuracy. On the Stanford dependency representation our model obtains 90.63% UAS, 88.27% LAS. Although this performance is promising, it is still below the discriminative neural network models of Dyer et al. (2015) and Weiss et al. (2015), who obtained 93.1% UAS and 94.0% UAS respectively. 5.2 Language modelling We also evaluate our parser as a language model, on the same WSJ data used for the parsing evaluation4. We perform unlabelled parsing, as experiments show that including labels in the conditioning context has a very small impact on performance. Neither do we use POS tags, as they are too expensive to predict in language generation applications. Perplexity results on the WSJ are given in Table 4. As baselines we report results on modified Knesser-Ney (Kneser and Ney, 1995) and neural network 5-gr</context>
</contexts>
<marker>Dyer, Ballesteros, Ling, Matthews, Smith, 2015</marker>
<rawString>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transitionbased dependency parsing with stack long shortterm memory. In Proceedings of ACL 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="2170" citStr="Eisner, 1996" startWordPosition="313" endWordPosition="314"> which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this paper we propose a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations (Levy and Goldberg, 2014; Bansal et al., 2014). Generative models for graph-based dependency parsing (Eisner, 1996; Wallach et al., 2008) are much less accurate than their discriminative counterparts. Syntactic language models based on PCFGs (Roark, 2001; Charniak, 2001) and incremental parsing (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transitionbased dependency parser based on recurrent neural networks (Titov and Henderson, 2007) obtains high accuracy, but training and decoding is prohibitively expensive. We perform efficient line</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Frederick Jelinek</author>
</authors>
<title>A neural syntactic language model.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="2403" citStr="Emami and Jelinek, 2005" startWordPosition="346" endWordPosition="349">corporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations (Levy and Goldberg, 2014; Bansal et al., 2014). Generative models for graph-based dependency parsing (Eisner, 1996; Wallach et al., 2008) are much less accurate than their discriminative counterparts. Syntactic language models based on PCFGs (Roark, 2001; Charniak, 2001) and incremental parsing (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transitionbased dependency parser based on recurrent neural networks (Titov and Henderson, 2007) obtains high accuracy, but training and decoding is prohibitively expensive. We perform efficient linear-time decoding with a particle filtering-based beam-search method where derivations after pruned after every word generation and the beam size depends on the uncertainty in the model (Buys and Blunsom, 2015). The model obtains 91.1</context>
</contexts>
<marker>Emami, Jelinek, 2005</marker>
<rawString>Ahmad Emami and Frederick Jelinek. 2005. A neural syntactic language model. Machine Learning, 60(1-3):195–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In 16th Nordic Conference of Computational Linguistics,</booktitle>
<pages>105--112</pages>
<location>Tartu, Estonia.</location>
<contexts>
<context position="11765" citStr="Johansson and Nugues, 2007" startWordPosition="1916" endWordPosition="1919"> assigned 0 particles, in which case they are removed. The particle filtering method lets the beam size depend of the uncertainty of the model, somewhat similar to Choi and Mccallum (2013), while fixing the total number of particles constrains the decoding time to be linear. The particle filter also allow us to sample outputs, and to marginalise over the syntax when generating. 5 Experiments We evaluate our model for parsing and language modelling on the English Penn Treebank (Marcus et al., 1993) WSJ parsing setup1. Constituency trees are converted to projective CoNLL syntactic dependencies (Johansson and Nugues, 2007) with the LTH converter2. For some experiments 1Training on sections 02-21, development on section 22, and testing on section 23. 2http://nlp.cs.lth.se/software/treebank converter/ 865 Activation UAS LAS linear 88.40 86.48 rectifier 89.99 88.31 tanh 90.91 89.22 sigmoid 91.48 89.94 Table 2: Parsing accuracies using different neural network activation functions. Model UAS LAS Wallach et al. (2008) 85.7 - Titov and Henderson (2007) 90.93 89.42 NN-GenDP 91.11 89.41 Chen and Manning (2014) 92.0 90.7 Table 3: Parsing accuracies for dependency parsers on the WSJ test set, CoNLL dependencies. we also </context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In 16th Nordic Conference of Computational Linguistics, pages 105–112, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="1453" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="204" endWordPosition="208">sifier offer an appealing trade-off between speed and accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this paper we propose a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representatio</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of EMNLP, pages 1700–1709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="13437" citStr="Klein and Manning, 2003" startWordPosition="2177" endWordPosition="2180">, the L2 regularization parameter is 10, and the word representation and hidden layer of size is 256. The Adagrad learning rate is initialised to 0.05. POS tags for the development and test sets are obtained with the Stanford POS tagger (Toutanova et al., 2003), with 97.5% test set accuracy. Words that occur only once in the training data are treated as unknown words. Unknown words are replaced by tokens representing morphological surface features (based on capitalization, numbers, punctuation and common suffixes) similar to those used in the implementation of generative constituency parsers (Klein and Manning, 2003). 5.1 Parsing results We report unlabelled attachment score (UAS) and labelled attachment score (LAS) in our results, excluding punctuation. On the development set, we consider the effect of the choice of activation function (Table 2), finding that a sigmoid activation (logistic function) performs best, following by tanh. Under our training setup the model can obtain up to 91.0 UAS after only 1 training iteration, thereby performing pure online learning. We found that including third order dependencies in the conditioning context performs just 0.1% UAS better than including only first and seco</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In ICASSP,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="15876" citStr="Kneser and Ney, 1995" startWordPosition="2565" endWordPosition="2568">e neural network models of Dyer et al. (2015) and Weiss et al. (2015), who obtained 93.1% UAS and 94.0% UAS respectively. 5.2 Language modelling We also evaluate our parser as a language model, on the same WSJ data used for the parsing evaluation4. We perform unlabelled parsing, as experiments show that including labels in the conditioning context has a very small impact on performance. Neither do we use POS tags, as they are too expensive to predict in language generation applications. Perplexity results on the WSJ are given in Table 4. As baselines we report results on modified Knesser-Ney (Kneser and Ney, 1995) and neural network 5-gram models. For our dependencybased language models we report perplexities based on the most likely parse found by the decoder, which gives an upper bound on the true 4However instead of using multiple unknown word classes, we replace all numbers by 0 and have a single unknown word token. 866 the u.s. union board said revenue rose 11 % to $ NUM million , or $ NUM a share. mr. bush has UNK-ed a plan to buy the company for $ NUM to NUM million, or $ NUM a share. the plan was UNK-ed by the board ’s decision to sell its $ NUM million UNK loan loan funds. in stocks coming mon</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In ICASSP, volume 1, pages 181–184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL: Short Papers.</booktitle>
<contexts>
<context position="2080" citStr="Levy and Goldberg, 2014" startWordPosition="299" endWordPosition="302">vlin et al., 2014; Sutskever et al., 2014). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this paper we propose a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations (Levy and Goldberg, 2014; Bansal et al., 2014). Generative models for graph-based dependency parsing (Eisner, 1996; Wallach et al., 2008) are much less accurate than their discriminative counterparts. Syntactic language models based on PCFGs (Roark, 2001; Charniak, 2001) and incremental parsing (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transitionbased dependency parser based on recurrent neural networks (Titov and Henderson, 2007) obtains high</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of ACL: Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="11640" citStr="Marcus et al., 1993" startWordPosition="1899" endWordPosition="1902">on is redistributed according to its probability, weighted by its current number of particles. Some derivations may be assigned 0 particles, in which case they are removed. The particle filtering method lets the beam size depend of the uncertainty of the model, somewhat similar to Choi and Mccallum (2013), while fixing the total number of particles constrains the decoding time to be linear. The particle filter also allow us to sample outputs, and to marginalise over the syntax when generating. 5 Experiments We evaluate our model for parsing and language modelling on the English Penn Treebank (Marcus et al., 1993) WSJ parsing setup1. Constituency trees are converted to projective CoNLL syntactic dependencies (Johansson and Nugues, 2007) with the LTH converter2. For some experiments 1Training on sections 02-21, development on section 22, and testing on section 23. 2http://nlp.cs.lth.se/software/treebank converter/ 865 Activation UAS LAS linear 88.40 86.48 rectifier 89.99 88.31 tanh 90.91 89.22 sigmoid 91.48 89.94 Table 2: Parsing accuracies using different neural network activation functions. Model UAS LAS Wallach et al. (2008) 85.7 - Titov and Henderson (2007) 90.93 89.42 NN-GenDP 91.11 89.41 Chen and </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine learning,</booktitle>
<pages>641--648</pages>
<contexts>
<context position="6282" citStr="Mnih and Hinton, 2007" startWordPosition="986" endWordPosition="989">er dependencies. We use an oracle to extract transition sequences from the training data: The oracle prefers reduce over shift transitions when both may lead to a valid derivation. Order Elements 1 a1, a2, a3, a4 2 lc1(a1), rc1(a1), lc1(a2), rc1(a2) lc2(a1), rc2(a1), lc2(a2), rc2(a2) 3 lc1(lc1(a1)),rc1(rc1(a1)) lc1(lc1(a2)),rc1(rc1(a2)) Table 1: Conditioning context elements for neural network input: First, second and third order dependencies are used. 3 Neural Network Model Our probability model is based on neural network language models with distributed representations (Bengio et al., 2003; Mnih and Hinton, 2007), as well as feed-forward neural network models for transition-based dependency parsing (Chen and Manning, 2014; Weiss et al., 2015). We estimate the distributions p(ti|hi), p(wi|ti, hi) and p(aj|hj) with neural networks with shared input and hidden layers but separate output layers. The templates for the conditioning context used are defined in Table 1. In the templates we obtain sentence indexes, which are then mapped to the corresponding words, tags and labels (for the dependencies of 2nd and 3rd order elements). The neural network allows us to include a large number of elements without suf</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th International Conference on Machine learning, pages 641–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="3876" citStr="Nivre and Scholz, 2004" startWordPosition="560" endWordPosition="563">ram baseline, when combining supervised training with unsupervised fine-tuning. Finally, we find that the model is able to generate sentences that display both local and syntactic coherence. 863 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 863–869, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Generative Transition-based Parsing Our parsing model is based on transition-based arc-standard projective dependency parsing (Nivre and Scholz, 2004). The generative formulation is similar to previous generative transition-based parsers (Titov and Henderson, 2007; Cohen et al., 2011; Buys and Blunsom, 2015), and also related to the joint tagging and parsing model of Bohnet and Nivre (2012). The model predicts a sequence of parsing transitions: A shift transition generates a word (and its POS tag), while a reduce transition adds an arc (i, l, j), where i is the head node, j the dependent and l is the dependency label. The joint probability distribution over a sentence with words w1:n, tags t1:n and a transition sequence a1:2n is defined as </context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="898" citStr="Nivre, 2008" startWordPosition="124" endWordPosition="125">endency parsing. A probability distribution over both sentences and transition sequences is parameterised by a feedforward neural network. The model surpasses the accuracy and speed of previous generative dependency parsers, reaching 91.1% UAS. Perplexity results show a strong improvement over n-gram language models, opening the way to the efficient integration of syntax into neural models for language generation. 1 Introduction Transition-based dependency parsers that perform incremental local inference with a discriminative classifier offer an appealing trade-off between speed and accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<booktitle>Computational linguistics,</booktitle>
<pages>27--2</pages>
<contexts>
<context position="2310" citStr="Roark, 2001" startWordPosition="334" endWordPosition="335">se a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations (Levy and Goldberg, 2014; Bansal et al., 2014). Generative models for graph-based dependency parsing (Eisner, 1996; Wallach et al., 2008) are much less accurate than their discriminative counterparts. Syntactic language models based on PCFGs (Roark, 2001; Charniak, 2001) and incremental parsing (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transitionbased dependency parser based on recurrent neural networks (Titov and Henderson, 2007) obtains high accuracy, but training and decoding is prohibitively expensive. We perform efficient linear-time decoding with a particle filtering-based beam-search method where derivations after pruned after every word generation and the beam </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="1499" citStr="Sutskever et al., 2014" startWordPosition="213" endWordPosition="216"> accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this paper we propose a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations (Levy and Goldberg, 2014; Bansal et al., 20</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>A latent variable model for generative dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Tenth International Conference on Parsing Technologies,</booktitle>
<pages>144--155</pages>
<contexts>
<context position="2667" citStr="Titov and Henderson, 2007" startWordPosition="383" endWordPosition="386">epresentations (Levy and Goldberg, 2014; Bansal et al., 2014). Generative models for graph-based dependency parsing (Eisner, 1996; Wallach et al., 2008) are much less accurate than their discriminative counterparts. Syntactic language models based on PCFGs (Roark, 2001; Charniak, 2001) and incremental parsing (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transitionbased dependency parser based on recurrent neural networks (Titov and Henderson, 2007) obtains high accuracy, but training and decoding is prohibitively expensive. We perform efficient linear-time decoding with a particle filtering-based beam-search method where derivations after pruned after every word generation and the beam size depends on the uncertainty in the model (Buys and Blunsom, 2015). The model obtains 91.1% UAS on the WSJ, which is 0.2% UAS better than the previous highest accuracy generative dependency parser (Titov and Henderson, 2007), while also being much more efficient. As a language model its perplexity reaches 111.8, a 23% reduction over an ngram baseline, </context>
<context position="3990" citStr="Titov and Henderson, 2007" startWordPosition="574" endWordPosition="577">is able to generate sentences that display both local and syntactic coherence. 863 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 863–869, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Generative Transition-based Parsing Our parsing model is based on transition-based arc-standard projective dependency parsing (Nivre and Scholz, 2004). The generative formulation is similar to previous generative transition-based parsers (Titov and Henderson, 2007; Cohen et al., 2011; Buys and Blunsom, 2015), and also related to the joint tagging and parsing model of Bohnet and Nivre (2012). The model predicts a sequence of parsing transitions: A shift transition generates a word (and its POS tag), while a reduce transition adds an arc (i, l, j), where i is the head node, j the dependent and l is the dependency label. The joint probability distribution over a sentence with words w1:n, tags t1:n and a transition sequence a1:2n is defined as Hn (p(ti|hmi)p(wi|ti, hmi) i=1 where mi is the number of transitions that have been performed when (ti, wi) is shi</context>
<context position="12197" citStr="Titov and Henderson (2007)" startWordPosition="1977" endWordPosition="1980">language modelling on the English Penn Treebank (Marcus et al., 1993) WSJ parsing setup1. Constituency trees are converted to projective CoNLL syntactic dependencies (Johansson and Nugues, 2007) with the LTH converter2. For some experiments 1Training on sections 02-21, development on section 22, and testing on section 23. 2http://nlp.cs.lth.se/software/treebank converter/ 865 Activation UAS LAS linear 88.40 86.48 rectifier 89.99 88.31 tanh 90.91 89.22 sigmoid 91.48 89.94 Table 2: Parsing accuracies using different neural network activation functions. Model UAS LAS Wallach et al. (2008) 85.7 - Titov and Henderson (2007) 90.93 89.42 NN-GenDP 91.11 89.41 Chen and Manning (2014) 92.0 90.7 Table 3: Parsing accuracies for dependency parsers on the WSJ test set, CoNLL dependencies. we also use the Stanford dependency representation (De Marneffe and Manning, 2008) (SD)3. Our neural network implementation is partly based on the OxLM neural language modelling framework (Baltescu et al., 2014). The model parameters are initialised randomly by drawing from a Gaussian distribution with mean 0 and variance 0.1, except for the bias weights, which are initialised by the unigram distributions of their output. We use minibat</context>
<context position="14732" citStr="Titov and Henderson (2007)" startWordPosition="2374" endWordPosition="2377"> performance further. The model can obtain 91.18 UAS, 89.02 LAS when 3Converted with version 3.4.1 of the Stanford parser, available at http::/nlp.stanford.edu/software/lex-parser.shtml. trained only on words, not POS tags. Dependency parsers that do not use distributed representations tend to rely much more on the tags. Test set results comparing generative dependency parsers are given in Table 3 (our model is refered to as NN-GenDP). The graph-based generative baseline (Wallach et al., 2008), parameterised by Pitman-Yor Processes, is quite weak. Our model outperforms the generative model of Titov and Henderson (2007), which we retrained on our dataset, by 0.2%, despite that model being able to condition on arbitrary-sized contexts. The decoding speed of our model is around 20 sentences per second, against less than 1 sentence per second for Titov and Henderson’s model. Using diagonal transformation matrices further increases our model’s speed, but reduces parsing accuracy. On the Stanford dependency representation our model obtains 90.63% UAS, 88.27% LAS. Although this performance is promising, it is still below the discriminative neural network models of Dyer et al. (2015) and Weiss et al. (2015), who ob</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007. A latent variable model for generative dependency parsing. In Proceedings of the Tenth International Conference on Parsing Technologies, pages 144–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="13074" citStr="Toutanova et al., 2003" startWordPosition="2123" endWordPosition="2126">neural network implementation is partly based on the OxLM neural language modelling framework (Baltescu et al., 2014). The model parameters are initialised randomly by drawing from a Gaussian distribution with mean 0 and variance 0.1, except for the bias weights, which are initialised by the unigram distributions of their output. We use minibatches of size 128, the L2 regularization parameter is 10, and the word representation and hidden layer of size is 256. The Adagrad learning rate is initialised to 0.05. POS tags for the development and test sets are obtained with the Stanford POS tagger (Toutanova et al., 2003), with 97.5% test set accuracy. Words that occur only once in the training data are treated as unknown words. Unknown words are replaced by tokens representing morphological surface features (based on capitalization, numbers, punctuation and common suffixes) similar to those used in the implementation of generative constituency parsers (Klein and Manning, 2003). 5.1 Parsing results We report unlabelled attachment score (UAS) and labelled attachment score (LAS) in our results, excluding punctuation. On the development set, we consider the effect of the choice of activation function (Table 2), f</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Bayesian modeling of dependency trees using hierarchical Pitman-Yor priors.</title>
<date>2008</date>
<booktitle>In ICML Workshop on Prior Knowledge for Text and Language Processing.</booktitle>
<contexts>
<context position="2193" citStr="Wallach et al., 2008" startWordPosition="315" endWordPosition="318">their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this paper we propose a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning contexts required for an accurate model, while also offering a principled approach to learn dependencybased word representations (Levy and Goldberg, 2014; Bansal et al., 2014). Generative models for graph-based dependency parsing (Eisner, 1996; Wallach et al., 2008) are much less accurate than their discriminative counterparts. Syntactic language models based on PCFGs (Roark, 2001; Charniak, 2001) and incremental parsing (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transitionbased dependency parser based on recurrent neural networks (Titov and Henderson, 2007) obtains high accuracy, but training and decoding is prohibitively expensive. We perform efficient linear-time decoding with a</context>
<context position="12163" citStr="Wallach et al. (2008)" startWordPosition="1971" endWordPosition="1974">te our model for parsing and language modelling on the English Penn Treebank (Marcus et al., 1993) WSJ parsing setup1. Constituency trees are converted to projective CoNLL syntactic dependencies (Johansson and Nugues, 2007) with the LTH converter2. For some experiments 1Training on sections 02-21, development on section 22, and testing on section 23. 2http://nlp.cs.lth.se/software/treebank converter/ 865 Activation UAS LAS linear 88.40 86.48 rectifier 89.99 88.31 tanh 90.91 89.22 sigmoid 91.48 89.94 Table 2: Parsing accuracies using different neural network activation functions. Model UAS LAS Wallach et al. (2008) 85.7 - Titov and Henderson (2007) 90.93 89.42 NN-GenDP 91.11 89.41 Chen and Manning (2014) 92.0 90.7 Table 3: Parsing accuracies for dependency parsers on the WSJ test set, CoNLL dependencies. we also use the Stanford dependency representation (De Marneffe and Manning, 2008) (SD)3. Our neural network implementation is partly based on the OxLM neural language modelling framework (Baltescu et al., 2014). The model parameters are initialised randomly by drawing from a Gaussian distribution with mean 0 and variance 0.1, except for the bias weights, which are initialised by the unigram distributio</context>
<context position="14604" citStr="Wallach et al., 2008" startWordPosition="2355" endWordPosition="2358">ust 0.1% UAS better than including only first and second order dependencies. Including additional elements does not improve performance further. The model can obtain 91.18 UAS, 89.02 LAS when 3Converted with version 3.4.1 of the Stanford parser, available at http::/nlp.stanford.edu/software/lex-parser.shtml. trained only on words, not POS tags. Dependency parsers that do not use distributed representations tend to rely much more on the tags. Test set results comparing generative dependency parsers are given in Table 3 (our model is refered to as NN-GenDP). The graph-based generative baseline (Wallach et al., 2008), parameterised by Pitman-Yor Processes, is quite weak. Our model outperforms the generative model of Titov and Henderson (2007), which we retrained on our dataset, by 0.2%, despite that model being able to condition on arbitrary-sized contexts. The decoding speed of our model is around 20 sentences per second, against less than 1 sentence per second for Titov and Henderson’s model. Using diagonal transformation matrices further increases our model’s speed, but reduces parsing accuracy. On the Stanford dependency representation our model obtains 90.63% UAS, 88.27% LAS. Although this performanc</context>
</contexts>
<marker>Wallach, Sutton, McCallum, 2008</marker>
<rawString>Hanna M Wallach, Charles Sutton, and Andrew McCallum. 2008. Bayesian modeling of dependency trees using hierarchical Pitman-Yor priors. In ICML Workshop on Prior Knowledge for Text and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Weiss</author>
<author>Chris Alberti</author>
<author>Michael Collins</author>
<author>Slav Petrov</author>
</authors>
<title>Structured training for neural network transition-based parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1124" citStr="Weiss et al., 2015" startWordPosition="157" endWordPosition="160">rsers, reaching 91.1% UAS. Perplexity results show a strong improvement over n-gram language models, opening the way to the efficient integration of syntax into neural models for language generation. 1 Introduction Transition-based dependency parsers that perform incremental local inference with a discriminative classifier offer an appealing trade-off between speed and accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. In this paper we propose a generative model for</context>
<context position="6414" citStr="Weiss et al., 2015" startWordPosition="1006" endWordPosition="1009">ons when both may lead to a valid derivation. Order Elements 1 a1, a2, a3, a4 2 lc1(a1), rc1(a1), lc1(a2), rc1(a2) lc2(a1), rc2(a1), lc2(a2), rc2(a2) 3 lc1(lc1(a1)),rc1(rc1(a1)) lc1(lc1(a2)),rc1(rc1(a2)) Table 1: Conditioning context elements for neural network input: First, second and third order dependencies are used. 3 Neural Network Model Our probability model is based on neural network language models with distributed representations (Bengio et al., 2003; Mnih and Hinton, 2007), as well as feed-forward neural network models for transition-based dependency parsing (Chen and Manning, 2014; Weiss et al., 2015). We estimate the distributions p(ti|hi), p(wi|ti, hi) and p(aj|hj) with neural networks with shared input and hidden layers but separate output layers. The templates for the conditioning context used are defined in Table 1. In the templates we obtain sentence indexes, which are then mapped to the corresponding words, tags and labels (for the dependencies of 2nd and 3rd order elements). The neural network allows us to include a large number of elements without suffering from sparsity. In the input layer we make use of additive representations (Botha and Blunsom, 2014) so that for each word inp</context>
<context position="15324" citStr="Weiss et al. (2015)" startWordPosition="2468" endWordPosition="2471"> Titov and Henderson (2007), which we retrained on our dataset, by 0.2%, despite that model being able to condition on arbitrary-sized contexts. The decoding speed of our model is around 20 sentences per second, against less than 1 sentence per second for Titov and Henderson’s model. Using diagonal transformation matrices further increases our model’s speed, but reduces parsing accuracy. On the Stanford dependency representation our model obtains 90.63% UAS, 88.27% LAS. Although this performance is promising, it is still below the discriminative neural network models of Dyer et al. (2015) and Weiss et al. (2015), who obtained 93.1% UAS and 94.0% UAS respectively. 5.2 Language modelling We also evaluate our parser as a language model, on the same WSJ data used for the parsing evaluation4. We perform unlabelled parsing, as experiments show that including labels in the conditioning context has a very small impact on performance. Neither do we use POS tags, as they are too expensive to predict in language generation applications. Perplexity results on the WSJ are given in Table 4. As baselines we report results on modified Knesser-Ney (Kneser and Ney, 1995) and neural network 5-gram models. For our depen</context>
</contexts>
<marker>Weiss, Alberti, Collins, Petrov, 2015</marker>
<rawString>David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. 2015. Structured training for neural network transition-based parsing. In Proceedings of ACL 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>562--571</pages>
<contexts>
<context position="9789" citStr="Zhang and Clark, 2008" startWordPosition="1592" endWordPosition="1595">g function with output word representation rw and bias weight bw. The model is trained with minibatch stochastic gradient descent (SGD) with Adagrad (Duchi et al., 2011) and L2 regularisation, to minimise the negative log likelihood of the joint distribution over parsed training sentences. For our experiments we train the model while the training objective improves, and choose the parameters of the iteration with the best development set accuracy (early stopping). The model obtains high accuracy with only a few training iterations. 4 Decoding Beam-search decoders for transition-based parsing (Zhang and Clark, 2008) keep a beam of partial derivations, advancing each derivation by one transition at a time. When the size of the beam exceeds a set threshold, the lowest-scoring derivations are removed. However, in an incremental generative model we need to compare derivations with the same number of words shifted, rather than transitions performed. To let the decoding time remain linear, we also need to bound the total number of reduce transitions that can be performed over all derivations between two shift transitions. To achieve this, we use a decoding method recently proposed for generative incremental pa</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of EMNLP, pages 562–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL-HLT: Short papers,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="921" citStr="Zhang and Nivre, 2011" startWordPosition="126" endWordPosition="129">ng. A probability distribution over both sentences and transition sequences is parameterised by a feedforward neural network. The model surpasses the accuracy and speed of previous generative dependency parsers, reaching 91.1% UAS. Perplexity results show a strong improvement over n-gram language models, opening the way to the efficient integration of syntax into neural models for language generation. 1 Introduction Transition-based dependency parsers that perform incremental local inference with a discriminative classifier offer an appealing trade-off between speed and accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). However, currently t</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings ofACL-HLT: Short papers, pages 188– 193.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>