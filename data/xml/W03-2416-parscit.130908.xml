<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029003">
<title confidence="0.981951">
Stretching the TEI: Converting the Genia Corpus
</title>
<author confidence="0.996495">
Tomai Erjavec Jin-Dong Kim Tomoko Ohta
</author>
<affiliation confidence="0.794153">
Department of Intelligent Systems CREST Department of Information Science
Jo2ef Stefan Institute, Ljubljana Japan Science and University of Tokyo
Technology Corporation
</affiliation>
<author confidence="0.788304">
Yuka Tateisi Jun-ichi Tsujii
</author>
<sectionHeader confidence="0.7659825" genericHeader="abstract">
CREST JSTC CREST JSTC
Abstract
</sectionHeader>
<bodyText confidence="0.999929315789474">
The paper discusses the application of
the Text Encoding Initiative Guidelines
to a linguistically annotated corpus. The
recently released GENIA corpus Ver-
sion 3.0 contains 2,000 abstracts taken
from the MEDLINE database, and has
almost 100,000 hand-annotated terms,
which are marked for semantic class
from the accompanying ontology. The
paper introduces and shows how to ap-
ply the TEI, which has become a de-
facto standard in corpus encoding, to
this corpus. It overviews the history
of TEI, including recent and expected
developments, and then turns to imple-
menting a TEl parametrisation and con-
version for the GENIA corpus. Dis-
cussed are some problems and choices
that arise in this process.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999835522727273">
Text mining from biological literature is emerg-
ing as one of the main issues in bioinformatics re-
search, a huge and thriving field. Natural language
processing methods could significantly raise the
potential of utilising this literature, with applica-
tions ranging from intelligent searches to auto-
matic discovery of scientific theories. Yet, while
NLP techniques are relatively domain-portable,
reference materials, e.g., corpora, are not. The
lack of a large annotated corpus of biological texts
can thus be seen as a major bottleneck for apply-
ing NLP techniques to bioinformatics. This was
the reason behind the compilation of the GENIA
corpus (Ohta et al., 2002).
In this paper we show how to develop a stan-
dardised encoding for such a resource, and for oth-
ers of its kind. For this we use the Text Encoding
Initiative Guidelines P4 (Sperberg-McQueen and
Burnard, 2002), and specify a constructive map-
ping, i.e., an XSLT stylesheet, to the developed
encoding.
The motivation for this re-encoding is that TEl
is well-designed and widely accepted architecture,
which has been often used for annotating language
corpora, and by porting to it, GENIA can gain
new insights into possible encoding practices and
maybe make the corpus better suited for inter-
change. As the transformation to TEl is fully auto-
matic, there is also no need to abandon the original
markup format of GENIA, which, as it has been
crafted specially for the corpus, provides a tighter
encoding than can be possible with the more gen-
eral TEI.
The paper thus proposes the creation of a practi-
cal annotation scheme for linguistically annotated
(biomedical) corpora, the conversion to which is
automatic and supports consistency checking and
validation. The paper also serves as a guide to
parametrising TEl and overviews its modules that
might be useful for encoding linguistically anno-
tated corpora; here we also discuss the shortcom-
ings and expected developments of these modules.
The paper is structured as follows: Section 2 in-
troduces the GENIA corpus; Section 3 turns to the
</bodyText>
<page confidence="0.99578">
117
</page>
<bodyText confidence="0.99991525">
TEl and gives its history, pros and cons of using it,
and the method of parametrising TEl for particu-
lar projects; Section 4 discusses such a parametri-
sation for GENIA; Section 5 reviews the structure
of the corpus, giving in parallel the original and
the TEl encodings and explains the conversion of
the corpus to TEI; finally, Section 6 offers some
conclusions and directions for further work.
</bodyText>
<sectionHeader confidence="0.947679" genericHeader="method">
2 The GENIA Corpus
</sectionHeader>
<bodyText confidence="0.999283128205128">
The GENIA corpus (Ohta et al., 2002) is being
developed in the scope of the GENIA project,
which seeks to develop information extraction
techniques for scientific texts using NLP technol-
ogy. The corpus consists of semantically anno-
tated published abstracts from the biomedical do-
main. The corpus is a collection of articles ex-
tracted from the on-line MEDLINE abstracts (U.S.
National Center for Biotechnology Information,
http://wwwncbi.nlm.nih.gov/, PubMed database).
Since the focus of the corpus is on biological re-
actions concerning transcription factors in human
blood cells, articles were selected that contain the
MeSH terms human, blood cell and transcription
factor.
For those not familiar with the field we should
note that the articles are composed largely of struc-
turally very complex technical terms, and are al-
most incomprehensible to a layperson. A typical
heading e.g., reads IL-2 gene expression and NF-
kappa B activation through CD28 requires reac-
tive oxygen production by 5-lipoxygenase.
The main value of the GENIA corpus comes
from its annotation: all the abstracts and their ti-
tles have been marked-up by two domain experts
for biologically meaningful terms, and these terms
have been semantically annotated with descriptors
from the GENIA ontology.
The GENIA ontology is a taxonomy of, cur-
rently, 47 biologically relevant nominal categories,
such as body part, virus, or RNA domain or region;
the taxonomy has 35 terminal categories.
The terms of the corpus are semantically de-
fined as those sentence constituents that can be
categorised using the terminal categories from the
ontology. Syntactically such constituents are quite
varied: they include qualifiers and can be recur-
sive.
The GENIA corpus is encoded in the Genia
Project Markup Language. The GPML is an XML
DTD (Kim et al., 2001) where each article con-
tains its MEDLINE ID, title and abstract. The
texts of the abstracts are segmented into sentences,
and these contain the constituents with their se-
mantic classification. Examples will be given in
Section 5.
The GENIA ontology is provided together with
the GENIA corpus and is encoded in DAML+OIL
(http://www.daml.org/), the standard XML-based
ontology description language.
A suite of supporting tools has been devel-
oped or tuned for the GENIA corpus and GPML:
the term annotation is performed with the XML-
Mind editor; an XPath-based concordancer has
been developed for searching the corpus; and CSS
stylesheets are available for browsing it.
The GENIA corpus V2.1 has been released in
August 2002 and is the prototype version: it con-
tains 670 abstracts (cca 160,000 words) annotated
for terms, and also tokenised and marked for PoS.
PoS tagging has been performed automatically
and later — to an extent — hand validated. In this
version, the GPML DTD and resource organisa-
tion was also more complex than presented above:
each article could contain local resources that in-
cluded the article-specific ontology as well as a
lexicon, which mediated between the text and the
ontology.
In December 2002, Version 3.0 has been re-
leased. It consists of 2,000 abstracts with over
400,000 words and more than 90,000 marked-up
terms. The structure has also been simplified,
without the local resources. This version has not
yet been marked-up with tokens or PoS informa-
tion.
The GENIA corpus is available free of
charge from the GENIA project homepage, at
http://www-tsujii.is.s.u-to o.acip/GENIAL
</bodyText>
<sectionHeader confidence="0.979271" genericHeader="method">
3 The Text Encoding Initiative
</sectionHeader>
<bodyText confidence="0.998977166666667">
The Text Encoding Initiative was established in
1987 as a systematised attempt to develop a fully
general text encoding model and set of encoding
conventions based upon it, suitable for process-
ing and analysis of any type of text, in any lan-
guage, and intended to serve the increasing range
</bodyText>
<page confidence="0.997335">
118
</page>
<bodyText confidence="0.999963117647059">
of existing (and potential) applications and uses.
The TEl Guidelines for Electronic Text Encod-
ing and Interchange, were first published in April
1994 in two substantial green volumes, known as
TEl P3. In May 1999, a revised edition of TEl P3
was produced, correcting several typographic and
other errors. In December 2000 the TEl Consor-
tium (http://wwwtei-c.orgn was set up to maintain
and develop the TEl standard; the Consortium is
managed by a Board of Directors, and its techni-
cal work is overseen by an elected Council.
In 2002, the TEl Consortium announced avail-
ability of a major revision of TEl P3, the TEl P4
(Sperberg-McQueen and Burnard, 2002) the ob-
ject of which is to provide equal support for XML
and SGML applications using the TEl scheme.
The revisions needed to make TEl P4 have been
deliberately restricted to error correction only,
with a view to ensuring that documents conform-
ing to TEl P3 will not become illegal when pro-
cessed with TEl P4. For GENIA, we are using the
XML-compatible version of TEl P4.
In producing P4, many possibilities for other,
more fundamental changes have been identified.
With the establishment of the TEl Council, it be-
came possible to agree on a programme of work
to enhance and modify the Guidelines more funda-
mentally over the coming years. TEl P5 will be the
next full revision of the Guidelines. The work on
P5 has now started, and the date of its appearance
will likely be in 2004. There are currently several
TEl Working Groups addressing various parts of
the Guidelines that need attention; some of these
will be mentioned in the following sections.
More than 80 projects spanning over 30 lan-
guages have so far made use of the TEl guide-
lines, producing such diverse resources as com-
puter corpora, medieval literature, text-critical edi-
tions of classical works, dictionaries, and library
catalogues. TEl has also been influential in cor-
pus encoding, where the best known example is
probably the British National Corpus. However,
while the TEl has been extensively used for an-
notating PoS tagged corpora, it been less popular
for encoding texts used by the the Information Re-
trieval/Extraction community; there, a number of
non-TEI initiatives have taken the lead in encod-
ing, say, ontologies or inter-document linking. As
will be seen, the content and original encoding of
GENIA pose a number of challenges in the con-
version to TEI.
</bodyText>
<subsectionHeader confidence="0.999556">
3.1 Pros and cons of using TEl
</subsectionHeader>
<bodyText confidence="0.99169143902439">
Why, if a corpus is already encoded in XML using
a home-grown DTD, to re-encoded it in TEl at all?
One reasons is certainly the validation aspect of
the exercise: re-coding a corpus, or any other re-
source, reveals hidden (and in practice incorrect)
assumptions about its structure. Re-coding to a
standard recommendation also forces the corpus
designers to face issues which might have been
overlooked in the original design.
There are also other advantages of using TEl as
the interchange format: (1) it is a wide-coverage,
well-designed (modular and extensible), widely
accepted and well-maintained architecture; (2) it
provides extensive documentation, which com-
prises not only the Guidelines but also papers and
documentation (best practices) of various projects;
(3) it offers community support via the tei-1 public
discussion list; (4) various TEI-dedicated software
already exists, and more is likely to become avail-
able; and (5) using it contributes to the adoption of
open standards and recommendations.
However, using a very general recommenda-
tion which tries to cater for any possible situation
brings with it also several disadvantages:
Tag abuse TEl might not have elements / at-
tributes with the exact meaning we require.
This results in a tendency to misuse tags for
purposes they were not meant for; however,
it is a case of individual judgement to decide
whether to (slightly) abuse a tag, or to imple-
ment a local extension to add the attribute or
element required.
Tag bloat Being a general purpose recommen-
dation, TEl can — almost by definition —
never be optimal for a specific application.
Thus a custom developed DTD will be leaner,
have less (redundant) tags and simpler con-
tent models.
TEl for humanities While the Guidelines cover
a vast range of text types and annotations,
they are maybe the least developed for &amp;quot;high
</bodyText>
<page confidence="0.995029">
119
</page>
<bodyText confidence="0.9999538">
level&amp;quot; NLP applications or have failed to
keep abreast of &amp;quot;cutting-edge&amp;quot; initiatives. As
will be seen, critical areas are the encoding of
ontologies, of lexical databases and of feature
structures.
</bodyText>
<subsectionHeader confidence="0.999873">
3.2 Building a TEl DTD
</subsectionHeader>
<bodyText confidence="0.999974068965517">
The TEl Guidelines (Sperberg-McQueen and
Burnard, 2002) consist of the formal part, which
is a set of SGML/XML DTD fragments, and the
documentation, which explains the rationale be-
hind the elements available in these fragments, as
well as giving overall information about the struc-
ture of the TEI.
The formal SGML/XML part of TEl comes as a
set of DTD fragments or tagsets. A TEl DTD for
a particular application is then constructed by se-
lecting an appropriate combination of such tagsets.
TEl distinguishes the following types of tagsets:
(1) the core tagset, which is always included with-
out any special action by the encoder; (2) the base
tagsets, which are the basic building blocks for
specific text types; exactly one base must be se-
lected by the encoder, unless one of the combined
bases is used; (3) the additional tagsets, which
define extra tags useful for particular purposes; all
additional tagsets are compatible with all bases
and with each other; (4) user defined tagsets,
which give the possibility of extending and over-
riding the definitions provided in the TEl tagset.
While a project-particular XML DTD can be
constructed by including and ignoring the TEl
DTD fragments directly, it is also possible for eas-
ier processing to build a one-file DTD with the
help of the on-line TEl Pizza Chef service, avail-
able from the TEl web site.
</bodyText>
<sectionHeader confidence="0.935406" genericHeader="method">
4 Parametrising TEl for GENIA
</sectionHeader>
<bodyText confidence="0.9999313">
This section gives our proposal on how to encode
GENIA, and in general, ontology-annotated cor-
pora, in the TEI. A number of tagsets could prove
useful in the long term, and we have chosen a
parametrisation of TEl that collects not only those
that we consider necessary for the current version
of GENIA, but also some that might prove of ser-
vice in the future. Furthermore, we support the
encoding of both version 2.1 and 3.0 of the cor-
pus. The resulting DTD is thus very generous in
</bodyText>
<figure confidence="0.9601125">
&lt;!DOCTYPE teiCorpus.2 SYSTEM
&amp;quot;http://www.tei-c.org/P4X/DTD/tei2.dtd&amp;quot;
[&lt;!ENTITY % TEI.prose &amp;quot;INCLUDE&amp;quot;›
&lt;!ENTITY % TEI.general &amp;quot;INCLUDE&amp;quot;&gt;
&lt;!ENTITY % TEI.dictionaries &amp;quot;INCLUDE&amp;quot;›
&lt;!ENTITY % TEI.terminology &amp;quot;INCLUDE&amp;quot;›
&lt;!ENTITY % TEI.linking &amp;quot;INCLUDE&amp;quot;›
&lt;!ENTITY % TEI.analysis &amp;quot;INCLUDE&amp;quot;›
&lt;!ENTITY % TEI.fs &amp;quot;INCLUDE&amp;quot;›
&lt;!ENTITY % TEI.corpus &amp;quot;INCLUDE&amp;quot;›
&lt;!ENTITY % TEI.XML &amp;quot;INCLUDE&amp;quot;›
&lt;!ENTITY % TEI.extensions.ent SYSTEM
&apos;geniaex.ent&apos;›
&lt;!ENTITY % TEI.extensions.dtd SYSTEM
&apos;geniaex.dtd&apos;›
]&gt;
</figure>
<figureCaption confidence="0.999985">
Figure 1: The XML TEl prolog for GENIA
</figureCaption>
<bodyText confidence="0.999378666666667">
what kinds of data it caters for. We give in Fig-
ure 1 the XML prolog of the TEl encoded corpus
that defines our parametrisation of TEI.
</bodyText>
<subsectionHeader confidence="0.991484">
4.1 TEI.prose
</subsectionHeader>
<bodyText confidence="0.999986142857143">
The base tagset does not declare many elements
but rather inherits all of the TEl core, which in-
cludes the TEl header, and text elements. A TEl
document will typically have as its root element
(TEI.2) which is composed of the (teiHeader),
followed by the (text).
The TEl header describes an encoded work so
that the text (corpus) itself, its source, its encoding,
and its revisions are all thoroughly documented.
TEI.prose also contains elements and attributes for
describing text structure, e.g. (div) for text di-
vision, (p) for paragraph, (head) for text header,
etc. The tagset is therefore useful for encoding the
gross structure of the corpus texts.
</bodyText>
<subsectionHeader confidence="0.995845">
4.2 TEI.general
</subsectionHeader>
<bodyText confidence="0.999951727272727">
This combined base allows the combination of
base tagsets, with the proviso, that each appear
within its own division, (div). We use it to cir-
cumvent the requirement that a TEl DTD should
contain only one base tagset.
This option was necessary for the V2.1 version
of the GENIA corpus, where each article could
contain not only the text of the abstract but also
a local lexicon and ontology, as each of these is
modelled using a different base tagset, as is ex-
plained next.
</bodyText>
<page confidence="0.973103">
120
</page>
<subsectionHeader confidence="0.99104">
4.3 TEI.dictionaries
</subsectionHeader>
<bodyText confidence="0.999905263157895">
This base tagset is oriented toward printed dic-
tionaries. While the GENIA lexicon for Version
2.1 (Version 3.0 does not include a lexicon) is sig-
nificantly different from a printed dictionary, this
tagset does offer, at the current depth of encoding,
elements which are suitable for expressing the de-
sired lexical markup. In particular, each lexical
entry is encoded in the (entry) and the form of the
entry in the (form) elements.
However, this is only a stop-gap measure. As
TEl does not have any working group devoted to
lexica, it might be better to look for an lexicon in-
terchange encoding further afield, say to the Open
Lexicon Interchange Format, http://www.olif.net/.
Another option, which directly builds on the TEI,
is the CONCEDE Lexica Database Model (Erjavec
et al., 2000).
Version 3.0 of the corpus does not use lexica, so
this issue has been, for the time being, put aside.
</bodyText>
<subsectionHeader confidence="0.998025">
4.4 TEI.terminology
</subsectionHeader>
<bodyText confidence="0.999952058823529">
This base tagset is used for encoding terminologi-
cal databases, which we, for V2.1, used to encode
local ontologies; this is the closest TEl comes to
offering a base useful for encoding general ontolo-
gies.
As is noted in the P4 Guidelines themselves,
the TEl chapter on encoding terminology has
been rendered obsolete in several respects, chiefly
as a result of the publication of MARTIF, the
ISO 12200:1999 standard &amp;quot;Machine-readable ter-
minology interchange format&amp;quot;.
Version 3.0 of the corpus does, in any case, not
use local ontologies. Furthermore, the structure of
the ontology has been in V3.0 explicated, which
is why we now use a simpler encoding that stores
it the corpus TEl header, as will be discussed in
Section 5.4.
</bodyText>
<subsectionHeader confidence="0.997113">
4.5 TEI.corpus
</subsectionHeader>
<bodyText confidence="0.999964333333333">
This additional tagset introduces a new root ele-
ment, (teiCorpus.2), which comprises a (corpus)
header and a series of (TEI.2) elements. The
TEI.corpus tagset also extends the certain header
elements to provide more detailed descriptions of
the corpus material.
</bodyText>
<subsectionHeader confidence="0.993185">
4.6 TELlinking
</subsectionHeader>
<bodyText confidence="0.9999855">
This additional tagset provides mechanisms for
linking, segmentation, and alignment. The el-
ements provided here enable links to be made
e.g., between the articles and their source URLs,
or between concepts and their hypernyms.
It should be noted that while the TEl treatment
of external pointers had been very influential, it
was overtaken and made obsolete by newer rec-
ommendations. However, the TEl does have a
Working Group on Stand-Off Markup, XLink and
XPointer, which should produce new TEl encod-
ing recommendations for this area in 2003.
</bodyText>
<subsectionHeader confidence="0.993703">
4.7 TEI.analysis
</subsectionHeader>
<bodyText confidence="0.999945888888889">
This additional tagset is used for associating sim-
ple linguistic analyses and interpretations with text
elements. It can be used to annotate words, (4),
clauses, (c/), and sentences, (s) with dedicated
tags, as well as arbitrary and possibly nested seg-
ments with the (seg). Such elements can be, via at-
tributes, associated with their analyses. This tagset
has proved very popular for PoS-annotated cor-
pora.
</bodyText>
<subsectionHeader confidence="0.995985">
4.8 TEI.fs
</subsectionHeader>
<bodyText confidence="0.999984047619048">
This additional tagset is used to mark-up the text
with feature structures. In addition, the TEl Fea-
ture Structure Declaration is provided, which is
an auxiliary DTD, for defining feature values and
names, their descriptions and constraints on valid
feature structures.
While the current versions of the GENIA corpus
do not use elements from this tagset, we included
it for future reference. Namely, the corpus should
eventually have markup for deep syntactic analy-
ses, possibly in the HPSG framework. For this,
a feature-structure encoding is necessary, and the
TEl tagset offers a venue for experimentation.
The current TEl proposal has some disadvan-
tages; foremost, there are no known application of
this tagset we could find. The proposal is also tai-
lored toward GPSG rather than HPSG and has no
support for a type hierarchy or co-indexing. Fi-
nally, there are no mechanisms (apart from the
DTD) for checking validity of specified feature
structures.
</bodyText>
<page confidence="0.995875">
121
</page>
<bodyText confidence="0.9999748125">
But there also exist reasons for pursuing the
possibility of using TEl for encoding feature-
structures. First, there are, to our knowledge, no
other (much less better) standardised efforts to en-
code them. Also, there exists now a number of
unification-based parsers with large grammars and
lexica, and the beginnings of feature-structure an-
notated corpora (e.g., BulTreeBank), so the field
might be ready to start exchanging the resources.
Finally, the TEI.fs is being taken forward under the
auspices of the ISO Technical Committee 37 (Ter-
minology and Other Language Resources). Work-
ing Group 4 of TC37 (Language Resource Man-
agement) is in the process of forming a group to
oversee the development of TEI.fs into an interna-
tional standard.
</bodyText>
<subsectionHeader confidence="0.87459">
4.9 TEI.XML
</subsectionHeader>
<bodyText confidence="0.96722780952381">
TEl P4 allows both standard SGML and XML en-
codings. Including the TEI.XML option indicates
that the target DTD is to be expressed in XML.
4.10 TEI.extensions.ent
The file gives, for each element sanctioned by the
chosen modules, whether we include or ignore it
in our parametrisation. While this is not strictly
necessary (without any such specification, all the
elements would be included) we thought it wise
to constrain the content models somewhat, to re-
duce the bewildering variety of choices that the
TEl otherwise offers. Also, such an entity exten-
sion file gives the complete list of all the TEl el-
ements that are allowed (and disallowed) in GE-
NIA, which might prove useful for documentation
purposes.
4.11 TEI.extensions.dtd
This file specifies the changes we have made to
TEl elements. We have e.g., added the un l attribute
to (xptr) and (xref) and tagging attributes to word
and punctuation elements.
</bodyText>
<sectionHeader confidence="0.863048" genericHeader="method">
5 GENIA in TEl
</sectionHeader>
<bodyText confidence="0.999974285714286">
With the TEl DTD in place, it is possible to spec-
ify the mapping between the original GPML en-
coding and the TEl one. Formally, the mapping
is an XSLT stylesheet, as further discussed below.
Here we present this translation by giving exam-
ples of the input and output encodings and com-
paring the two.
</bodyText>
<subsectionHeader confidence="0.997194">
5.1 Corpus structure
</subsectionHeader>
<bodyText confidence="0.999944933333333">
As shown in Figure 2, the most noticeable differ-
ence between GPML and TEl is, apart from the re-
naming of elements, the addition of headers to the
corpus and texts. In the GENIA (teiHeader) we
give e.g., the name, address, availability, sampling
description, and, for each abstract&apos;s (sourceDesc),
two (xptr)s: the first gives the URL of the HTML
article in the MEDLINE database, while the sec-
ond is the URL of the article in the original XML.
It should be noted that we use a locally defined unl
attribute for specifying the value of the pointer.
In V2.1 the local resources of the article,
namely the ontology and the lexicon are in the
TEl encoding contained in two separate (div) el-
ements; this is not used in V3.0.
</bodyText>
<subsectionHeader confidence="0.992632">
5.2 Term annotation
</subsectionHeader>
<bodyText confidence="0.999986090909091">
As shown in Figure 3, the text of the abstract is
first analysed into sentences, and these into con-
stituents (terms), which use an attribute to point to
the appropriate GENIA ontology class.
The main difference in the two encodings, apart
from renaming, are the uses of the attributes ana
and function on clauses. Whereas in GPML, the
sem attribute can hold either the pointer to the se-
mantic class, or an expression, it is in TEl the ana
that holds the # IDREF, while function contains
the complex expressions.
</bodyText>
<subsectionHeader confidence="0.988212">
5.3 Token annotation
</subsectionHeader>
<bodyText confidence="0.999988923076923">
For V1.1 we have also annotated the GPML ver-
sion of the corpus with LTG tools (Grover et al.,
2002). In short, the corpus is tokenised, and then
part-of-speech tagged with two taggers, each one
using a different tagset, and the nouns and verbs
lemmatised. Additionally, the deverbal nominali-
sations are assigned their verbal stems.
The conversion to TEl is also able to handle this
additional markup, by using the TEI.analysis mod-
ule. The word and punctuation tokens are encoded
as (w) and (c) elements respectively, which are
further marked with type and lemma and the lo-
cally defined cl, c2 and vstem.
</bodyText>
<page confidence="0.976805">
122
</page>
<figure confidence="0.99957685">
&lt;!DOCTYPE set SYSTEM &amp;quot;gpml.dtd&amp;quot;›
&lt;set&gt;
&lt;article&gt;
&lt;articleinfo&gt;&lt;bibliomisc&gt;
*MEDLINE ID*
&lt;/bibliomisc&gt;&lt;/articleinfo&gt;
&lt;title&gt;
*Title_of_article*
&lt;/title&gt;
&lt;abstract&gt;
*Abstract of article*
&lt;/abstract&gt;
&lt;localresource&gt; &lt;!--V2.1--&gt;
&lt;imports *Ontology REF*&amp;quot;/&gt;
*Local_ontology*
*Local_lexicon*
&lt;/localresource&gt;
&lt;/article&gt;
*More_articles*
&lt;/set&gt;
&lt;!DOCTYPE teiCorpus.2 SYSTEM &amp;quot;genia-tei.dtd&amp;quot;›
&lt;TEIcorpus.2&gt;
&lt;teiHeader type=&amp;quot;corpus&amp;quot;›
*Corpus_header*&lt;/teiHeader&gt;
&lt;TEI.2 id=&amp;quot;*MEDLINE ID*&amp;quot;&gt;
&lt;teiHeader type=&amp;quot;text&amp;quot;›
*Article_header*&lt;/teiHeader&gt;
&lt;text&gt;&lt;body&gt;
&lt;div type=&amp;quot;abstract&amp;quot;›
&lt;head&gt;*Title_of_article*&lt;/head&gt;
&lt;p&gt;*Abstract of article*&lt;/P&gt;
&lt;/div&gt;
&lt;div type=&amp;quot;ontology&amp;quot;› &lt;!--V2.1--&gt;
*Local ontology/TEI.terminology*
&lt;Idly&gt;
&lt;div type=&amp;quot;lexiconfl&gt; &lt;!--V2.1--&gt;
*Local lexicon/TEI.dictionaries*
&lt;Idly&gt;
&lt;/body&gt;&lt;/text&gt;&lt;/TEI.2&gt;
*More articles*&lt;/TEIcorpus.2&gt;
</figure>
<figureCaption confidence="0.991706">
Figure 2: The GPML and TEl structure of the corpus
</figureCaption>
<figure confidence="0.997758857142857">
&lt;cons sem=&amp;quot;(AND G#other name G#other name)&amp;quot;&gt;
&lt;cons&gt;Cellular&lt;/cons&gt; and
&lt;cons&gt;molecular&lt;/cons&gt;
&lt;cons&gt;mechanisms&lt;/cons&gt;
&lt;/cons&gt; of
&lt;cons sem=&amp;quot;G#other_name&amp;quot;&gt;IFN-gamma
production&lt;/cons&gt; induced by ...
&lt;cl function=&amp;quot; (AND G.other name
G.other_name)&amp;quot; ana=&amp;quot;G.other_name&amp;quot;›
&lt;c1&gt;Cellular&lt;/cl&gt; and &lt;cl&gt;molecular&lt;/cl&gt;
&lt;cl&gt;mechanisms&lt;/cl&gt;
&lt;/d&gt; of
&lt;cl ana=&amp;quot;G.other_name&amp;quot;&gt;IFN-gamma
production&lt;/cl&gt; induced by...
</figure>
<figureCaption confidence="0.999933">
Figure 3: The GPML and TEl encoding of terms
</figureCaption>
<subsectionHeader confidence="0.979391">
5.4 The ontology
</subsectionHeader>
<bodyText confidence="0.999966">
One of the more interesting questions in recoding
GENIA in TEl was how to encode the ontology;
in V2.1 it could be included in the local resources
and this was modelled in TELterminology. How-
ever, this choice has proved too complex and out
of touch with current practices. As shown in the
left side of Figure 4 the ontology is in V3.0 en-
coded in a separate document, conforming to the
OIL+DAML specification. This, inter alia, means
that that XML file heavily relies on XML Names-
paces and the RDF recommendation.
As currently the GENIA ontology can be mod-
elled by a taxonomy, we have now translated it to
the TEl (taxonomy) element, which is contained
in the (classDecl) of the header (encodingDesc).
The TEl defines this element as &amp;quot;[the classifica-
tion declaration] contains one or more taxonomies
defining any classificatory codes used elsewhere
in the text&amp;quot;, i.e., exactly suited for our purposes.
There are quite substantial differences between
the two encodings: the DAML+OIL models class
inclusion with links, while the TEl does it as XML
element inclusion. This is certainly the simpler
and more robust solution, but requires that the on-
tology is a taxonomy, i.e., tree structured. The sec-
ond difference is in the status of the identifiers:
in DAML+OIL they are general #CDATA links,
which need a separate (XLink/XPointer) mecha-
nisms for their resolution. In TEl they are XML
ID attributes, and can rely on the XML parser to
resolve them. While this is a simpler solution, it
does support document-internal reference only.
</bodyText>
<subsectionHeader confidence="0.992367">
5.5 Conversion of GPML to TEl
</subsectionHeader>
<bodyText confidence="0.999742666666667">
Because the source format of GENIA will remain
the simpler GPML, it is imperative to have an au-
tomatic procedure for converting to the TEl inter-
</bodyText>
<page confidence="0.993099">
123
</page>
<figure confidence="0.999409272727273">
&lt;daml:Class rdf:ID =&amp;quot;source&amp;quot;&gt;&lt;/daml:Class&gt; &lt;taxonomy id=&amp;quot;G.taxonomy&amp;quot;›
&lt;daml:Class rdf:ID =&amp;quot;natural&amp;quot;› &lt;category id=&amp;quot;G.source&amp;quot;›
&lt;rdfs:subClassOf rdf:resource=&amp;quot;#source&amp;quot;/&gt; &lt;catDesc&gt;biological source&lt;/catDesc&gt;
&lt;/daml:Class&gt; &lt;category id=&amp;quot;G.natural&amp;quot;›
&lt;daml:Class rdf:ID =&amp;quot;organism&amp;quot;› &lt;catDesc&gt;natural&lt;/catDesc&gt;
&lt;rdfs:subClassOf rdf:resource=&amp;quot;#natural&amp;quot;/ &lt;category id=&amp;quot;G.organismn&gt;
&lt;/daml:Class&gt; &lt;catDesc&gt;organism&lt;/catDesc&gt;
&lt;daml:Class rdf:ID =&amp;quot;multi_cell&amp;quot;› &lt;category id=&amp;quot;G.multi_cell&amp;quot;›
&lt;rdfs:subClassOf rdf:resource=&amp;quot;#organism&amp;quot; /&gt; &lt;catDesc&gt;multi-cellular&lt;/catDesc&gt;
&lt;/daml:Class&gt; &lt;/category&gt;
• • •
</figure>
<figureCaption confidence="0.999991">
Figure 4: The GENIA DAML+OIL and TEl ontology
</figureCaption>
<bodyText confidence="0.9996141875">
change format. The translation process takes ad-
vantage of the fact that both the input and out-
put are encoded in XML, which makes it possible
to use the XSL Transformation Language, XSLT
that defines a standard declarative specification of
transformations between XML documents. There
also exist a number of free XSLT processors; we
used Daniel Veillard&apos;s x s ltpr oc
The transformation is written as a XSLT
stylesheet, which makes reference to two docu-
ments: the GENIA ontology in TEl and the tem-
plate for the corpus header. The stylesheet then
resolves the GPML encoded corpus into TEI. The
translation of the corpus is thus fully automatic,
except for the taxonomy, which was translated by
hand.
</bodyText>
<sectionHeader confidence="0.999582" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999958555555556">
The paper proposed a TEl encoding for GENIA
and specified a mapping from the Genia markup
language to this encoding. The conversion has
been implemented in XSLT, and both the PoS
marked up version with local resources, as well
as the larger but structurally simpler version 3.0
have been translated to our XML paramterisation
of TEl P4. The parametrisation (DTD) and the
XSLT stylesheets are, together with a report docu-
ment them, available at http://nl.ijs.si/et/genial.
We have attempted to survey the TEl modules
that can be useful for encoding a wide variety of
linguistically annotated corpora and to comment
on the areas of the TEl where the Guidelines need
attention. The paper, it is hoped, can thus serve as
a blueprint for parametrising TEl for diverse cor-
pus resources.
As for GENIA, the corpus should in the future
gather more complex annotations, say for chunk-
ing and parsing. Interesting is also the inclusion
of other knowledge sources into the corpus, say of
Medical Subject Headings (MeSH), Unified Medi-
cal Language System (UMLS), International Clas-
sification of Disease (ICD), etc. The place of these
annotations in the corpus will have to be consid-
ered, and their linking to the existing information
determined.
</bodyText>
<sectionHeader confidence="0.998049" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999511583333333">
Tomak Erjavec, Roger Evans, Nancy Ide, and Adam
Kilgarriff. 2000. The Concede Model for Lexi-
cal Databases. In Second International Conference
on Language Resources and Evaluation, LREC&apos;00,
pages 355-362, Paris. ELRA.
Claire Grover, Ewan Klein, Alex Lascarides,
and Maria Lapata. 2002. XML-based NLP
Tools for Analysing and Annotating Med-
ical Language. In 2nd Workshop on NLP
and XML (CoLing Workshop NLPXML-2002).
http://www.ltg.ed.ac.uk/softwarefitt/.
Jin-Dong Kim, Tomoko Ohta, and Jun-ichi Tsujii.
2001. XML-based linguistic annotation of corpus.
In Proceedings of the first NLP and XML Workshop,
pages 44-53.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.
The GENIA Corpus: an Annotated Research Ab-
stract Corpus in Molecular Biology Domain. In Pro-
ceedings of the Human Language Technology Con-
ference, to appear.
C. M. Sperberg-McQueen and Lou Burnard, editors.
2002. Guidelines for Electronic Text Encoding and
Interchange, The XML Version of the TEl Guide-
lines. The TEl Consortium. http://www.tei-c.org/.
</reference>
<page confidence="0.998291">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.541258">
<title confidence="0.999699">Stretching the TEI: Converting the Genia Corpus</title>
<author confidence="0.997838">Tomai Erjavec Jin-Dong Kim Tomoko Ohta</author>
<affiliation confidence="0.885378">Department of Intelligent Systems CREST Department of Information Science Jo2ef Stefan Institute, Ljubljana Japan Science and University of Tokyo Technology Corporation</affiliation>
<author confidence="0.868518">Tateisi Tsujii</author>
<affiliation confidence="0.983712">CREST JSTC CREST JSTC</affiliation>
<abstract confidence="0.99876265">The paper discusses the application of the Text Encoding Initiative Guidelines to a linguistically annotated corpus. The recently released GENIA corpus Version 3.0 contains 2,000 abstracts taken from the MEDLINE database, and has almost 100,000 hand-annotated terms, which are marked for semantic class from the accompanying ontology. The paper introduces and shows how to apply the TEI, which has become a defacto standard in corpus encoding, to this corpus. It overviews the history of TEI, including recent and expected developments, and then turns to implementing a TEl parametrisation and conversion for the GENIA corpus. Discussed are some problems and choices that arise in this process.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tomak Erjavec</author>
<author>Roger Evans</author>
<author>Nancy Ide</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The Concede Model for Lexical Databases.</title>
<date>2000</date>
<booktitle>In Second International Conference on Language Resources and Evaluation, LREC&apos;00,</booktitle>
<pages>355--362</pages>
<location>Paris. ELRA.</location>
<contexts>
<context position="16220" citStr="Erjavec et al., 2000" startWordPosition="2635" endWordPosition="2638"> from a printed dictionary, this tagset does offer, at the current depth of encoding, elements which are suitable for expressing the desired lexical markup. In particular, each lexical entry is encoded in the (entry) and the form of the entry in the (form) elements. However, this is only a stop-gap measure. As TEl does not have any working group devoted to lexica, it might be better to look for an lexicon interchange encoding further afield, say to the Open Lexicon Interchange Format, http://www.olif.net/. Another option, which directly builds on the TEI, is the CONCEDE Lexica Database Model (Erjavec et al., 2000). Version 3.0 of the corpus does not use lexica, so this issue has been, for the time being, put aside. 4.4 TEI.terminology This base tagset is used for encoding terminological databases, which we, for V2.1, used to encode local ontologies; this is the closest TEl comes to offering a base useful for encoding general ontologies. As is noted in the P4 Guidelines themselves, the TEl chapter on encoding terminology has been rendered obsolete in several respects, chiefly as a result of the publication of MARTIF, the ISO 12200:1999 standard &amp;quot;Machine-readable terminology interchange format&amp;quot;. Version </context>
</contexts>
<marker>Erjavec, Evans, Ide, Kilgarriff, 2000</marker>
<rawString>Tomak Erjavec, Roger Evans, Nancy Ide, and Adam Kilgarriff. 2000. The Concede Model for Lexical Databases. In Second International Conference on Language Resources and Evaluation, LREC&apos;00, pages 355-362, Paris. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Ewan Klein</author>
<author>Alex Lascarides</author>
<author>Maria Lapata</author>
</authors>
<title>XML-based NLP Tools for Analysing and Annotating Medical Language.</title>
<date>2002</date>
<booktitle>In 2nd Workshop on NLP and XML (CoLing Workshop NLPXML-2002). http://www.ltg.ed.ac.uk/softwarefitt/.</booktitle>
<contexts>
<context position="22673" citStr="Grover et al., 2002" startWordPosition="3710" endWordPosition="3713">Figure 3, the text of the abstract is first analysed into sentences, and these into constituents (terms), which use an attribute to point to the appropriate GENIA ontology class. The main difference in the two encodings, apart from renaming, are the uses of the attributes ana and function on clauses. Whereas in GPML, the sem attribute can hold either the pointer to the semantic class, or an expression, it is in TEl the ana that holds the # IDREF, while function contains the complex expressions. 5.3 Token annotation For V1.1 we have also annotated the GPML version of the corpus with LTG tools (Grover et al., 2002). In short, the corpus is tokenised, and then part-of-speech tagged with two taggers, each one using a different tagset, and the nouns and verbs lemmatised. Additionally, the deverbal nominalisations are assigned their verbal stems. The conversion to TEl is also able to handle this additional markup, by using the TEI.analysis module. The word and punctuation tokens are encoded as (w) and (c) elements respectively, which are further marked with type and lemma and the locally defined cl, c2 and vstem. 122 &lt;!DOCTYPE set SYSTEM &amp;quot;gpml.dtd&amp;quot;› &lt;set&gt; &lt;article&gt; &lt;articleinfo&gt;&lt;bibliomisc&gt; *MEDLINE ID* &lt;/b</context>
</contexts>
<marker>Grover, Klein, Lascarides, Lapata, 2002</marker>
<rawString>Claire Grover, Ewan Klein, Alex Lascarides, and Maria Lapata. 2002. XML-based NLP Tools for Analysing and Annotating Medical Language. In 2nd Workshop on NLP and XML (CoLing Workshop NLPXML-2002). http://www.ltg.ed.ac.uk/softwarefitt/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Jun-ichi Tsujii</author>
</authors>
<title>XML-based linguistic annotation of corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the first NLP and XML Workshop,</booktitle>
<pages>44--53</pages>
<contexts>
<context position="5305" citStr="Kim et al., 2001" startWordPosition="839" endWordPosition="842"> have been semantically annotated with descriptors from the GENIA ontology. The GENIA ontology is a taxonomy of, currently, 47 biologically relevant nominal categories, such as body part, virus, or RNA domain or region; the taxonomy has 35 terminal categories. The terms of the corpus are semantically defined as those sentence constituents that can be categorised using the terminal categories from the ontology. Syntactically such constituents are quite varied: they include qualifiers and can be recursive. The GENIA corpus is encoded in the Genia Project Markup Language. The GPML is an XML DTD (Kim et al., 2001) where each article contains its MEDLINE ID, title and abstract. The texts of the abstracts are segmented into sentences, and these contain the constituents with their semantic classification. Examples will be given in Section 5. The GENIA ontology is provided together with the GENIA corpus and is encoded in DAML+OIL (http://www.daml.org/), the standard XML-based ontology description language. A suite of supporting tools has been developed or tuned for the GENIA corpus and GPML: the term annotation is performed with the XMLMind editor; an XPath-based concordancer has been developed for searchi</context>
</contexts>
<marker>Kim, Ohta, Tsujii, 2001</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, and Jun-ichi Tsujii. 2001. XML-based linguistic annotation of corpus. In Proceedings of the first NLP and XML Workshop, pages 44-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Jin-Dong Kim</author>
</authors>
<title>The GENIA Corpus: an Annotated Research Abstract Corpus in Molecular Biology Domain.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<note>to appear.</note>
<contexts>
<context position="1691" citStr="Ohta et al., 2002" startWordPosition="255" endWordPosition="258">ing as one of the main issues in bioinformatics research, a huge and thriving field. Natural language processing methods could significantly raise the potential of utilising this literature, with applications ranging from intelligent searches to automatic discovery of scientific theories. Yet, while NLP techniques are relatively domain-portable, reference materials, e.g., corpora, are not. The lack of a large annotated corpus of biological texts can thus be seen as a major bottleneck for applying NLP techniques to bioinformatics. This was the reason behind the compilation of the GENIA corpus (Ohta et al., 2002). In this paper we show how to develop a standardised encoding for such a resource, and for others of its kind. For this we use the Text Encoding Initiative Guidelines P4 (Sperberg-McQueen and Burnard, 2002), and specify a constructive mapping, i.e., an XSLT stylesheet, to the developed encoding. The motivation for this re-encoding is that TEl is well-designed and widely accepted architecture, which has been often used for annotating language corpora, and by porting to it, GENIA can gain new insights into possible encoding practices and maybe make the corpus better suited for interchange. As t</context>
<context position="3529" citStr="Ohta et al., 2002" startWordPosition="562" endWordPosition="565">uss the shortcomings and expected developments of these modules. The paper is structured as follows: Section 2 introduces the GENIA corpus; Section 3 turns to the 117 TEl and gives its history, pros and cons of using it, and the method of parametrising TEl for particular projects; Section 4 discusses such a parametrisation for GENIA; Section 5 reviews the structure of the corpus, giving in parallel the original and the TEl encodings and explains the conversion of the corpus to TEI; finally, Section 6 offers some conclusions and directions for further work. 2 The GENIA Corpus The GENIA corpus (Ohta et al., 2002) is being developed in the scope of the GENIA project, which seeks to develop information extraction techniques for scientific texts using NLP technology. The corpus consists of semantically annotated published abstracts from the biomedical domain. The corpus is a collection of articles extracted from the on-line MEDLINE abstracts (U.S. National Center for Biotechnology Information, http://wwwncbi.nlm.nih.gov/, PubMed database). Since the focus of the corpus is on biological reactions concerning transcription factors in human blood cells, articles were selected that contain the MeSH terms huma</context>
</contexts>
<marker>Ohta, Tateisi, Kim, 2002</marker>
<rawString>Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002. The GENIA Corpus: an Annotated Research Abstract Corpus in Molecular Biology Domain. In Proceedings of the Human Language Technology Conference, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Sperberg-McQueen</author>
<author>Lou Burnard</author>
<author>editors</author>
</authors>
<date>2002</date>
<booktitle>Guidelines for Electronic Text Encoding and Interchange, The XML Version of the TEl Guidelines. The TEl Consortium. http://www.tei-c.org/.</booktitle>
<marker>Sperberg-McQueen, Burnard, editors, 2002</marker>
<rawString>C. M. Sperberg-McQueen and Lou Burnard, editors. 2002. Guidelines for Electronic Text Encoding and Interchange, The XML Version of the TEl Guidelines. The TEl Consortium. http://www.tei-c.org/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>