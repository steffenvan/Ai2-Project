<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000964">
<title confidence="0.997967">
Combining Semantic and Temporal Constraints for Multimodal Integra-
tion in Conversation Systems
</title>
<author confidence="0.975725">
Joyce Y. Chai Pengyu Hong Michelle X. Zhou
</author>
<affiliation confidence="0.971197666666667">
Department of Computer Department of Statistics IBM T. J. Watson Research
Science and Engineering Harvard University Center
Michigan State University Cambridge, MA 02138 19 Skyline Drive
</affiliation>
<address confidence="0.897619">
East Lansing, MI 48864 hong@stat.harvard.edu Hawthorne, NY 10532
</address>
<email confidence="0.997923">
jchai@cse.msu.edu mzhou@us.ibm.com
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999780727272727">
In a multimodal conversation, user refer-
ring patterns could be complex, involving
multiple referring expressions from
speech utterances and multiple gestures.
To resolve those references, multimodal
integration based on semantic constraints
is insufficient. In this paper, we describe a
graph-based probabilistic approach that
simultaneously combines both semantic
and temporal constraints to achieve a high
performance.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999833846153846">
Multimodal conversation systems allow users to
converse with systems through multiple modalities
such as speech, gesture and gaze (Cohen et al.,
1996; Wahlster, 1998). In such an environment,
not only are more interaction modalities available,
but also richer contexts are established during the
interaction. Understanding user inputs, for
example, what users refer to is important. Previous
work on multimodal reference resolution includes
the use of a focus space model (Neal et al., 1998),
the centering framework (Zancanaro et al., 1997),
context factors (Huls et al., 1995), and rules
(Kehler 2000). These previous approaches focus
</bodyText>
<note confidence="0.959646666666667">
G1 G2 G3 Total
no gest. one gest mul. gest. Num
S1: no expression 2 1 0 3
S2: one expression 12 117 2 131
S3: mul. expressions 1 6 15 22
Total Num 15 124 17 156
</note>
<bodyText confidence="0.999887416666667">
on semantics constraints without fully addressing
temporal constraints. In a user study1, we found
that the majority of user referring behavior
involved one referring expression and one gesture
(as in [S2, G2] in Table 1). The earlier approaches
worked well for these types of references.
However, we found that 14.1% of the inputs were
complex, which involved multiple referring
expressions from speech utterances and multiple
gestures (S3 in Table 1). To resolve those complex
references, we have to not only apply semantic
constraints, but also apply temporal constraints at
the same time.
For example, Figure 1 shows three inputs where
the number of referring expressions is the same
and the number of gestures is the same. The speech
utterances and gestures are aligned along the time
axis. The first case (Figure 1a) and the second case
(Figure 1b) have the same speech utterance but
different temporal alignment between the gestures
and the speech input. The second case and the third
case (Figure 1c) have a similar alignment, but the
third case provides an additional constraint on the
number of referents (from the word “two”).
Although all three cases are similar, but the
objects they refer to are quite different in each
case. In the first case, most likely “this” refers to
the house selected by the first point gesture and
“these houses” refers to two houses selected by the
other two gestures. In the second case, “this” most
likely refers to the highlighted house on the display
and “these houses” refer to three houses selected
by the gestures. In the third case, “this” most likely
refers to the house selected by the first point
gesture and “these two houses” refers to two
houses selected by the other two point gestures.
</bodyText>
<tableCaption confidence="0.997208">
Table 1: Referring patterns from the user study 1 We are developing a system that helps users find real estate
</tableCaption>
<table confidence="0.886927636363636">
properties. So here we use real estate as the testing domain.
Resolving these complex cases requires
Speech input: Compare this with these houses.
Gesture input: t...t t... ............
Time
Speech input: Compare this with these houses.
Gesture input: t...t.........t... ......
Time
Speech input: Compare this with these two houses.
Gesture input: ..t...t.........t... ......
Time
</table>
<figureCaption confidence="0.958279">
Figure 1. Three multimodal inputs under the same
interaction context. The timings of the point gestures
are denoted by “t”.
</figureCaption>
<bodyText confidence="0.99763">
simultaneously satisfying semantic constraints
from inputs and the interaction contexts, and the
temporal constraints between speech and gesture.
</bodyText>
<sectionHeader confidence="0.990053" genericHeader="introduction">
2 Graph-based Approach
</sectionHeader>
<bodyText confidence="0.999998170212766">
We use a probabilistic approach based on attrib-
uted relational graphs (ARGs) to combine semantic
and temporal constraints for reference resolution.
First, ARGs can adequately capture the semantic
and temporal information (for both referring ex-
pressions and potential referents). Second, the
graph match mechanism allows a simultaneous
application of temporal constraints and semantic
constraints. Specifically, we use two attributed re-
lational graphs (ARGs). One graph corresponds to
all referring expressions in the speech utterances,
called the referring graph. The other graph corre-
sponds to all potential referents (either coming
from gestures or contexts), called the referent
graph. By finding the best match between the re-
ferring graph and the referent graph, we can find
the most possible referent(s) to each referring ex-
pression.
An ARG consists of a set of nodes and a set of
edges. For example, Figure 2(a) is the referring
graph for the speech utterance in Figure 1(c).
There are two nodes corresponding to two refer-
ring expressions “this” and “these two houses” re-
spectively. Each node encodes the semantic and
temporal information of the corresponding refer-
ring expression such as the semantic type of the
potential referent, the number, the start and end
time the expression was uttered, etc. The edge be-
tween two nodes indicates the semantic and tempo-
ral relations between these two expressions.
Similarly, Figure 2(b) is the referent graph for the
input in Figure 1(c). This referent graph consists of
four sub-graphs. Three sub-graphs correspond to
three gestures respectively. Each node in these sub-
graphs corresponds to one object selected by the
gesture. Each node encodes the semantic and tem-
poral information of the selected object, as well as
the probability this object is actually selected.
There is also a sub-graph corresponding to the in-
teraction context. Each node in this sub-graph
represents an object in the focus in the last interac-
tion turn. The sub-graphs are connected via seman-
tic type and temporal relations.
With the ARG representations described above,
the reference resolution problem becomes match-
ing the referent graph with the referring graph.
Suppose we have two graphs to be matched:
</bodyText>
<listItem confidence="0.989082333333333">
• The referent graph Gc = ({ax}, {rxy}), where {ax}
is the node list and {rxy} is the edge list. The
edge rxy connects nodes ax and ay.
</listItem>
<figureCaption confidence="0.997457333333333">
Figure 2. The ARG representation for references in Figure
1(c). (a) The referring graph (b) The referent graph, where
dashed rectangles represent sub-graphs.
</figureCaption>
<figure confidence="0.791718959183674">
Surface: “this”
Base: Unknown
Number: 1
Begin Time: 32264270
End Time: 32264273
Node 1
Relation: 1
Direction: Node1 → Node2
Temporal: Preceding
Semantic type: Same
Surface: “these two houses”
Base: House
Number: 2
Begin Time: 32264381
End Time: 32264398
Node 2
Sub-graph of the
interaction context
Relation 5
Direction: Node 1 → Node 4
Temporal: Preceding
Semantic Type: Same
Node 8
Node 1
Node 4
Node 6
Node 2
Node3 Node 5
Node 7
Sub-graph of the
2nd point gesture
Sub-graph of the
3rd point gesture
Node 3
Base: Town
Identifier: 1
Attr: {Area, Population ...}
Begin Time: 32264365
End Time: 32264366
Prob: 0.3321
Sub-graph of the
1st point gesture
Node 2
Base: House
Identifier: 4
Attr: {Price, Size, ...}
Begin Time: 32264365
End Time: 32264366
Prob: 0.4356
</figure>
<listItem confidence="0.533018">
• The referring graph Gs = 〈{αm}, {γmn}〉, where
{αm} is the node list and {γmn} is the edge list.
</listItem>
<bodyText confidence="0.948767">
The edge γmn connects nodes αm and αn.
The match process is to maximize the following
function:
</bodyText>
<equation confidence="0.993155692307692">
Q G G
( , ) = ∑ ∑ P a
( , ) ( , )
α ζ α
a
c s x m x m x m
P a
( , ) ( , ) ( , )
P a
x m
α y n
α ψ γ
rxy mn
</equation>
<bodyText confidence="0.999286277777778">
with respect to P(ax,αm), the matching probabili-
ties between the referent node ax and the referring
node αm.
The function Q(Gc,Gs) measures the degree of
the overall match between the referent graph and
the referring graph. This function not only consid-
ers the similarities between nodes as indicated by
the function ζ(ax,αm), but also considers the simi-
larities between edges as indicated by the function
ψ(rxy,γmn). Both node similarity and edge similarity
functions are further defined by a combination of
semantic and temporal constraints. For example,
ζ(ax,αm)=Sem(ax,αm)Tem(ax,αm), where Sem(ax,αm)
measures the semantic compatibility by determin-
ing whether the semantic categories of ax and αm
are the same, whether their attributes are compati-
ble, and so on. Tem(ax,αm) measures the temporal
alignment and is empirically defined as follows:
</bodyText>
<figure confidence="0.849128631578947">
 exp−  |time(ax) − time(αm)  |, 

2000
from context
When the algorithm converges,
gives us
the matching probabilities. Details are descri
P(ax,αm)
bed in
a separate paper.
3 Discussion
23 3
67275921 67277343 39218 10000 0 0 255
0.28571 70 23 2 2 2 67275921 67277343 39218 10000
1
0 0 255 1.
speech input:
compare_67273821this_67274160House_67274490with_67275547 this_67275847
House_67276096
</figure>
<figureCaption confidence="0.999146">
Figure 3. Gesture and speech data
</figureCaption>
<bodyText confidence="0.999535375">
“compare
house”.
to incorporate spatial constraints.
system assigned time stamps to each recognized
word in the utterance, and each gesture. Figure 3
shows an example of an input that consisted of two
gesture inputs and a speech utterance
this house with this
The first two lines rep-
resent two gestures. Each line gives information
about when the gesture started and ended, as well
as the selected objects with their probabilities.
These data provided us information on how the
speech and gesture were aligned (to the accuracy
of milliseconds). These data will help us further
validate the temporal compatibility function used
in the matching process.
We described an approach that uses graph
matching algorithm to combine semantic and tem-
poral constraints for reference resolution. The
study showed that this approach worked quite well
(93% accuracy) when the referring expressions
were correctly recognized by the ASR. In the fu-
ture, we plan
</bodyText>
<subsubsectionHeader confidence="0.725955">
Proceedings
</subsubsectionHeader>
<reference confidence="0.995725222222222">
31-40.
S. Gold and A. Rangarajan. 1996. A graduated
assignment algorithm for graph matching, IEEE
Trans. Pattern Analysis and Machine Intelligence,
vol. 18, no.
C. Huls, E. Bos, and W.
1995. Automatic
Referent Resolution of Deictic and Anaphoric
Expressions. Computational Linguistics, 21(1):59-79.
A. Kehler. 2000. Cognitive Status and Form of
Reference in Multimodal Human-Computer
Interaction, Proceedings
685-689.
J. G. Neal, C. Y. Thielman, Z. Dobes, S. M. Haller, and
S. C. Shapiro. 1998. Natural Language with
Integrated Deictic and Graphic Gestures. Intelligent
User Interfaces, M. Maybury and W. Wahlster
38-51.
W. H. Tsai and K. S. Fu. 1979. Error-correcting
isomorphism of attributed relational graphs for pattern
of ACM Multimedia,
4. 377−388.
Classen.
of AAAI’00.
(eds.),
analysis. IEEE Trans. Sys., Man and Cyb., vol. 9,
757−768.
</reference>
<bodyText confidence="0.793882375">
To maximize (1), we modified the graduated as-
signment algorithm (Gold and Rangarajan, 1996).
Duri
ng the study, we collected 156 inputs. The
Input received on port 3334: 67275921 67277343 2 69
Input received on port 3334: 67278140 67279078 2 71
24 4 1 2 67278140 67279078 797 10000 255 0 0 0.74545
72 24 3 2 2 67278140 67279078 797 10000 0 0 255 1.
</bodyText>
<sectionHeader confidence="0.995335" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996618166666667">
P. Cohen, M. Johnston, D. McGee, S. Oviatt, J. Pittman,
I. Smith, L. Chen, and J.Clow. 1996. Quickset:
Multimodal Interaction for Distributed Applications.
W. Wahlster. 1998. User and Discourse Models for
Multimodal Communication, Intelligent User
Interfaces, M. Maybury and W. Wahlster
(eds.), 359-
370.
M. Zancanaro, O. Stock, and C. Strapparava. 1997.
Multimodal Interaction for Information Access:
Exploiting Cohesion. Computational Intelligence
13(7):439-464.
</reference>
<figure confidence="0.9876164">
+
∑ ∑ ∑ ∑
x y m n
(1)
Tem (ax,αm ) =  

0.
1,
axis
ax is from gesture
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.478374">
<title confidence="0.9991205">Combining Semantic and Temporal Constraints for Multimodal Integration in Conversation Systems</title>
<author confidence="0.977687">Joyce Y Chai Department of Computer Science</author>
<author confidence="0.977687">Engineering Michigan State University East Lansing</author>
<author confidence="0.977687">MI Pengyu Michelle X jchaicse msu edu Department of IBM T J Watson</author>
<affiliation confidence="0.52987">Harvard 19 Skyline</affiliation>
<address confidence="0.999128">Cambridge, MA 02138 Hawthorne, NY 10532</address>
<email confidence="0.998418">hong@stat.harvard.edumzhou@us.ibm.com</email>
<abstract confidence="0.993111333333333">In a multimodal conversation, user referring patterns could be complex, involving multiple referring expressions from speech utterances and multiple gestures. To resolve those references, multimodal integration based on semantic constraints is insufficient. In this paper, we describe a graph-based probabilistic approach that simultaneously combines both semantic and temporal constraints to achieve a high performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<pages>31--40</pages>
<marker></marker>
<rawString>31-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gold</author>
<author>A Rangarajan</author>
</authors>
<title>A graduated assignment algorithm for graph matching,</title>
<date>1996</date>
<journal>IEEE Trans. Pattern Analysis and Machine Intelligence,</journal>
<volume>18</volume>
<pages>no.</pages>
<marker>Gold, Rangarajan, 1996</marker>
<rawString>S. Gold and A. Rangarajan. 1996. A graduated assignment algorithm for graph matching, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 18, no.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Huls</author>
<author>E Bos</author>
<author>W</author>
</authors>
<title>Automatic Referent Resolution of Deictic and Anaphoric Expressions.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--1</pages>
<contexts>
<context position="1448" citStr="Huls et al., 1995" startWordPosition="196" endWordPosition="199">ieve a high performance. 1 Introduction Multimodal conversation systems allow users to converse with systems through multiple modalities such as speech, gesture and gaze (Cohen et al., 1996; Wahlster, 1998). In such an environment, not only are more interaction modalities available, but also richer contexts are established during the interaction. Understanding user inputs, for example, what users refer to is important. Previous work on multimodal reference resolution includes the use of a focus space model (Neal et al., 1998), the centering framework (Zancanaro et al., 1997), context factors (Huls et al., 1995), and rules (Kehler 2000). These previous approaches focus G1 G2 G3 Total no gest. one gest mul. gest. Num S1: no expression 2 1 0 3 S2: one expression 12 117 2 131 S3: mul. expressions 1 6 15 22 Total Num 15 124 17 156 on semantics constraints without fully addressing temporal constraints. In a user study1, we found that the majority of user referring behavior involved one referring expression and one gesture (as in [S2, G2] in Table 1). The earlier approaches worked well for these types of references. However, we found that 14.1% of the inputs were complex, which involved multiple referring </context>
</contexts>
<marker>Huls, Bos, W, 1995</marker>
<rawString>C. Huls, E. Bos, and W. 1995. Automatic Referent Resolution of Deictic and Anaphoric Expressions. Computational Linguistics, 21(1):59-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
</authors>
<title>Cognitive Status and Form of Reference in Multimodal Human-Computer Interaction,</title>
<date>2000</date>
<booktitle>Proceedings</booktitle>
<pages>685--689</pages>
<contexts>
<context position="1473" citStr="Kehler 2000" startWordPosition="202" endWordPosition="203">oduction Multimodal conversation systems allow users to converse with systems through multiple modalities such as speech, gesture and gaze (Cohen et al., 1996; Wahlster, 1998). In such an environment, not only are more interaction modalities available, but also richer contexts are established during the interaction. Understanding user inputs, for example, what users refer to is important. Previous work on multimodal reference resolution includes the use of a focus space model (Neal et al., 1998), the centering framework (Zancanaro et al., 1997), context factors (Huls et al., 1995), and rules (Kehler 2000). These previous approaches focus G1 G2 G3 Total no gest. one gest mul. gest. Num S1: no expression 2 1 0 3 S2: one expression 12 117 2 131 S3: mul. expressions 1 6 15 22 Total Num 15 124 17 156 on semantics constraints without fully addressing temporal constraints. In a user study1, we found that the majority of user referring behavior involved one referring expression and one gesture (as in [S2, G2] in Table 1). The earlier approaches worked well for these types of references. However, we found that 14.1% of the inputs were complex, which involved multiple referring expressions from speech u</context>
</contexts>
<marker>Kehler, 2000</marker>
<rawString>A. Kehler. 2000. Cognitive Status and Form of Reference in Multimodal Human-Computer Interaction, Proceedings 685-689.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J G Neal</author>
<author>C Y Thielman</author>
<author>Z Dobes</author>
<author>S M Haller</author>
</authors>
<location>and</location>
<marker>Neal, Thielman, Dobes, Haller, </marker>
<rawString>J. G. Neal, C. Y. Thielman, Z. Dobes, S. M. Haller, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Shapiro</author>
</authors>
<title>Natural Language with Integrated Deictic and Graphic Gestures.</title>
<date>1998</date>
<booktitle>Intelligent User Interfaces, M. Maybury and W. Wahlster</booktitle>
<pages>38--51</pages>
<marker>Shapiro, 1998</marker>
<rawString>S. C. Shapiro. 1998. Natural Language with Integrated Deictic and Graphic Gestures. Intelligent User Interfaces, M. Maybury and W. Wahlster 38-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Tsai</author>
<author>K S Fu</author>
</authors>
<title>Error-correcting isomorphism of attributed relational graphs for pattern of</title>
<date>1979</date>
<journal>ACM Multimedia,</journal>
<volume>4</volume>
<pages>377--388</pages>
<marker>Tsai, Fu, 1979</marker>
<rawString>W. H. Tsai and K. S. Fu. 1979. Error-correcting isomorphism of attributed relational graphs for pattern of ACM Multimedia, 4. 377−388.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Classen</author>
</authors>
<note>of AAAI’00.</note>
<marker>Classen, </marker>
<rawString>Classen. of AAAI’00.</rawString>
</citation>
<citation valid="false">
<journal>IEEE Trans. Sys., Man and Cyb.,</journal>
<volume>9</volume>
<pages>757--768</pages>
<editor>(eds.), analysis.</editor>
<marker></marker>
<rawString>(eds.), analysis. IEEE Trans. Sys., Man and Cyb., vol. 9, 757−768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cohen</author>
<author>M Johnston</author>
<author>D McGee</author>
<author>S Oviatt</author>
<author>J Pittman</author>
<author>I Smith</author>
<author>L Chen</author>
<author>J Clow</author>
</authors>
<title>Quickset: Multimodal Interaction for Distributed Applications.</title>
<date>1996</date>
<contexts>
<context position="1019" citStr="Cohen et al., 1996" startWordPosition="132" endWordPosition="135">su.edu mzhou@us.ibm.com Abstract In a multimodal conversation, user referring patterns could be complex, involving multiple referring expressions from speech utterances and multiple gestures. To resolve those references, multimodal integration based on semantic constraints is insufficient. In this paper, we describe a graph-based probabilistic approach that simultaneously combines both semantic and temporal constraints to achieve a high performance. 1 Introduction Multimodal conversation systems allow users to converse with systems through multiple modalities such as speech, gesture and gaze (Cohen et al., 1996; Wahlster, 1998). In such an environment, not only are more interaction modalities available, but also richer contexts are established during the interaction. Understanding user inputs, for example, what users refer to is important. Previous work on multimodal reference resolution includes the use of a focus space model (Neal et al., 1998), the centering framework (Zancanaro et al., 1997), context factors (Huls et al., 1995), and rules (Kehler 2000). These previous approaches focus G1 G2 G3 Total no gest. one gest mul. gest. Num S1: no expression 2 1 0 3 S2: one expression 12 117 2 131 S3: mu</context>
</contexts>
<marker>Cohen, Johnston, McGee, Oviatt, Pittman, Smith, Chen, Clow, 1996</marker>
<rawString>P. Cohen, M. Johnston, D. McGee, S. Oviatt, J. Pittman, I. Smith, L. Chen, and J.Clow. 1996. Quickset: Multimodal Interaction for Distributed Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>User and Discourse Models for Multimodal Communication, Intelligent</title>
<date>1998</date>
<pages>359--370</pages>
<editor>User Interfaces, M. Maybury and W. Wahlster (eds.),</editor>
<contexts>
<context position="1036" citStr="Wahlster, 1998" startWordPosition="136" endWordPosition="137">com Abstract In a multimodal conversation, user referring patterns could be complex, involving multiple referring expressions from speech utterances and multiple gestures. To resolve those references, multimodal integration based on semantic constraints is insufficient. In this paper, we describe a graph-based probabilistic approach that simultaneously combines both semantic and temporal constraints to achieve a high performance. 1 Introduction Multimodal conversation systems allow users to converse with systems through multiple modalities such as speech, gesture and gaze (Cohen et al., 1996; Wahlster, 1998). In such an environment, not only are more interaction modalities available, but also richer contexts are established during the interaction. Understanding user inputs, for example, what users refer to is important. Previous work on multimodal reference resolution includes the use of a focus space model (Neal et al., 1998), the centering framework (Zancanaro et al., 1997), context factors (Huls et al., 1995), and rules (Kehler 2000). These previous approaches focus G1 G2 G3 Total no gest. one gest mul. gest. Num S1: no expression 2 1 0 3 S2: one expression 12 117 2 131 S3: mul. expressions 1 </context>
</contexts>
<marker>Wahlster, 1998</marker>
<rawString>W. Wahlster. 1998. User and Discourse Models for Multimodal Communication, Intelligent User Interfaces, M. Maybury and W. Wahlster (eds.), 359-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zancanaro</author>
<author>O Stock</author>
<author>C Strapparava</author>
</authors>
<title>Multimodal Interaction for Information Access: Exploiting Cohesion.</title>
<date>1997</date>
<journal>Computational Intelligence</journal>
<pages>13--7</pages>
<contexts>
<context position="1411" citStr="Zancanaro et al., 1997" startWordPosition="190" endWordPosition="193">h semantic and temporal constraints to achieve a high performance. 1 Introduction Multimodal conversation systems allow users to converse with systems through multiple modalities such as speech, gesture and gaze (Cohen et al., 1996; Wahlster, 1998). In such an environment, not only are more interaction modalities available, but also richer contexts are established during the interaction. Understanding user inputs, for example, what users refer to is important. Previous work on multimodal reference resolution includes the use of a focus space model (Neal et al., 1998), the centering framework (Zancanaro et al., 1997), context factors (Huls et al., 1995), and rules (Kehler 2000). These previous approaches focus G1 G2 G3 Total no gest. one gest mul. gest. Num S1: no expression 2 1 0 3 S2: one expression 12 117 2 131 S3: mul. expressions 1 6 15 22 Total Num 15 124 17 156 on semantics constraints without fully addressing temporal constraints. In a user study1, we found that the majority of user referring behavior involved one referring expression and one gesture (as in [S2, G2] in Table 1). The earlier approaches worked well for these types of references. However, we found that 14.1% of the inputs were comple</context>
</contexts>
<marker>Zancanaro, Stock, Strapparava, 1997</marker>
<rawString>M. Zancanaro, O. Stock, and C. Strapparava. 1997. Multimodal Interaction for Information Access: Exploiting Cohesion. Computational Intelligence 13(7):439-464.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>