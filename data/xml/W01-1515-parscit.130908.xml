<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001804">
<title confidence="0.981464">
Annotation Tools Based on the Annotation Graph API
</title>
<author confidence="0.999717">
Steven Bird, Kazuaki Maeda, Xiaoyi Ma and Haejoong Lee
</author>
<affiliation confidence="0.995972">
Linguistic Data Consortium, University of Pennsylvania
</affiliation>
<address confidence="0.66273">
3615 Market Street, Suite 200, Philadelphia, PA 19104-2608, USA
</address>
<email confidence="0.99944">
{sb,maeda,xma,haejoong}@ldc.upenn.edu
</email>
<sectionHeader confidence="0.995652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987165">
Annotation graphs provide an efficient
and expressive data model for linguistic
annotations of time-series data. This
paper reports progress on a complete
open-source software infrastructure
supporting the rapid development of
tools for transcribing and annotating
time-series data. This general-
purpose infrastructure uses annotation
graphs as the underlying model, and
allows developers to quickly create
special-purpose annotation tools using
common components. An application
programming interface, an I/O library,
and graphical user interfaces are
described. Our experience has shown
us that it is a straightforward task to
create new special-purpose annotation
tools based on this general-purpose
infrastructure.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999790272727273">
In the past, standardized file formats and coding
practices have greatly facilitated data sharing and
software reuse. Yet it has so far proved impossible
to work out universally agreed formats and codes
for linguistic annotation. We contend that this is a
vain hope, and that the interests of sharing and
reuse are better served by agreeing on the data
models and interfaces.
Annotation graphs (AGs) provide an efficient
and expressive data model for linguistic anno-
tations of time-series data (Bird and Liberman,
</bodyText>
<figureCaption confidence="0.998561">
Figure 1: Architecture for Annotation Systems
</figureCaption>
<bodyText confidence="0.996258884615385">
2001). Recently, the LDC has been develop-
ing a complete software infrastructure supporting
the rapid development of tools for transcribing
and annotating time-series data, in cooperation
with NIST and MITRE as part of the ATLAS
project, and with the developers of other widely
used annotation systems, Transcriber and Emu
(Bird et al., 2000; Barras et al., 2001; Cassidy and
Harrington, 2001).
The infrastructure is being used in the devel-
opment of a series of annotation tools at the Lin-
guistic Data Consortium. Two tools are shown in
the paper: one for dialogue annotation and one
for interlinear transcription. In both cases, the
transcriptions are time-aligned to a digital audio
signal.
This paper will cover the following points: the
application programming interfaces for manipu-
lating annotation graph data and importing data
from other formats; the model of inter-component
communication which permits easy reuse of soft-
ware components; and the design of the graphical
user interfaces.
Once a pair of anchors have been created it
is possible to create an annotation which spans
them:
</bodyText>
<sectionHeader confidence="0.989635" genericHeader="introduction">
2 Architecture
</sectionHeader>
<subsectionHeader confidence="0.999289">
2.1 General architecture
</subsectionHeader>
<bodyText confidence="0.999741307692308">
Figure 1 shows the architecture of the tools
currently being developed. Annotation tools,
such as the ones discussed below, must provide
graphical user interface components for signal
visualization and annotation. The communication
between components is handled through an
extensible event language. An application
programming interface for annotation graphs
has been developed to support well-formed
operations on annotation graphs. This permits
applications to abstract away from file format
issues, and deal with annotations purely at the
logical level.
</bodyText>
<equation confidence="0.96061975">
CreateAnnotation(&amp;quot;agSet12:ag5&amp;quot;,
&amp;quot;agSet12:ag5:anchor34&amp;quot;,
&amp;quot;agSet12:ag5:anchor35&amp;quot;,
&amp;quot;phonetic&amp;quot; );
</equation>
<bodyText confidence="0.9545253">
This call will construct an annotation
object and return an identifier for it, e.g.
agSet12:ag5:annotation41. We can now add
features to this annotation:
SetFeature(&amp;quot;agSet12:ag5:annotation41&amp;quot;,
&amp;quot;date&amp;quot;, &amp;quot;1999-07-02&amp;quot; );
The implementation maintains indexes on all
the features, and also on the temporal information
and graph structure, permitting efficient search
using a family of functions such as:
</bodyText>
<equation confidence="0.6576035">
GetAnnotationSetByFeature(
&amp;quot;agSet12:ag5&amp;quot;, &amp;quot;date&amp;quot;, &amp;quot;1999-07-02&amp;quot;);
</equation>
<subsectionHeader confidence="0.99711">
2.2 The annotation graph API
</subsectionHeader>
<bodyText confidence="0.94926952">
The application programming interface provides
access to internal objects (signals, anchors,
annotations etc) using identifiers, represented
as formatted strings. For example, an AG
identifier is qualified with an AGSet identifier:
AGSetId:AGId. Annotations and anchors are
doubly qualified: AGSetId:AGId:AnnotationId,
AGSetId:AGId:AnchorId. Thus, the identifier
encodes the unique membership of an object in
the containing objects.
We demonstrate the behavior of the API with
a series of simple examples. Suppose we have
already constructed an AG and now wish to create
a new anchor. We might have the following API
call:
CreateAnchor(&amp;quot;agSet12:ag5&amp;quot;, 15.234, &amp;quot;sec&amp;quot;);
This call would construct a new anchor object
and return its identifier: agSet12:ag5:anchor34.
Alternatively, if we already have an anchor iden-
tifier that we wish to use for the new anchor (e.g.
because we are reading previously created anno-
tation data from a file and do not wish to assign
new identifiers), then we could have the following
API call:
CreateAnchor(&amp;quot;agset12:ag5:anchor34&amp;quot;,
</bodyText>
<sectionHeader confidence="0.369093" genericHeader="method">
15.234, &amp;quot;sec&amp;quot;);
</sectionHeader>
<subsectionHeader confidence="0.988547">
2.3 A file I/O library
</subsectionHeader>
<bodyText confidence="0.999938714285714">
A file I/O library (AG-FIO) supports input and
output of AG data to existing formats. Formats
currently supported by the AG-FIO library
include the TIMIT, BU, Treebank, AIF (ATLAS
Interchange Format), Switchboard and BAS
Partitur formats. In time, the library will handle
all widely-used signal annotation formats.
</bodyText>
<subsectionHeader confidence="0.854942">
2.4 Inter-component communication
</subsectionHeader>
<bodyText confidence="0.981669833333333">
Figure 2 shows the structure of an annotation tool
in terms of components and their communication.
The main program is typically a small script
which sets up the widgets and provides callback
functions to handle widget events. In this
example there are four other components which
</bodyText>
<figure confidence="0.724293166666667">
Main program - a small script
AG-FIO-API
File input
/ output
Waveform
display
AG-GUI-API AG-API
Transcription
editor
Internal
representation
AG-GUI-API
</figure>
<figureCaption confidence="0.7171955">
This call will return agset12:ag5:anchor34.
Figure 2: The Structure of an Annotation Tool
</figureCaption>
<figure confidence="0.99742">
Main program
Update
User types Control-G Update Display
Internal Representation
</figure>
<figureCaption confidence="0.999853">
Figure 3: Inter-component Communication
</figureCaption>
<bodyText confidence="0.99988236">
are reused by several annotation tools. The AG
and AG-FIO components have already been
described. The waveform display component (of
which there may be multiple instances) receives
instructions to pan and zoom, to play a segment
of audio data, and so on. The transcription
editor is an annotation component which is
specialized for a particular coding task. Most tool
customization is accomplished by substituting for
this component.
Both GUI components and the main program
support a common API for transmitting and
receiving events. For example, GUI components
have a notion of a “current region” — the
timespan which is currently in focus. A
waveform component can change an annotation
component’s idea of the current region by
sending a SetRegion event (Figure 3). The
same event can also be used in the reverse
direction. The main program routes the events
between GUI components, calling the annotation
graph API to update the internal representation as
needed. With this communication mechanism, it
is straightforward to add new commands, specific
to the annotation task.
</bodyText>
<subsectionHeader confidence="0.996409">
2.5 Reuse of software components
</subsectionHeader>
<bodyText confidence="0.999508454545454">
The architecture described in this paper allows
rapid development of special-purpose annotation
tools using common components. In particular,
our model of inter-component communication
facilitates reuse of software components.
The annotation tools described in the next
section are not intended for general purpose
annotation/transcription tasks; the goal is not
to create an “emacs for linguistic annotation”.
Instead, they are special-purpose tools based on
the general purpose infrastructure. These GUI
</bodyText>
<figureCaption confidence="0.8683785">
Figure 4: Dialogue Annotation Tool for the
TRAINS/DAMSL Corpus
</figureCaption>
<bodyText confidence="0.9545355">
components can be modified or replaced when
building new special-purpose tools.
</bodyText>
<sectionHeader confidence="0.855057" genericHeader="method">
3 Graphical User Interfaces
</sectionHeader>
<subsectionHeader confidence="0.998851">
3.1 A spreadsheet component
</subsectionHeader>
<bodyText confidence="0.999985368421053">
Dialogue annotation typically consists of assign-
ing a field-structured record to each utterance in
each speaker turn. A key challenge is to handle
overlapping turns and back-channel cues without
disrupting the structure of individual speaker con-
tributions. The tool side-steps these problems by
permitting utterances to be independently aligned
to a (multi-channel) recording. The records are
displayed in a spreadsheet; clicking on a row of
the spreadsheet causes the corresponding extent
of audio signal to be highlighted. As an extended
recording is played back, annotated sections are
highlighted, in both the waveform and spread-
sheet displays.
Figure 4 shows the tool with a section of the
TRAINS/DAMSL corpus (Jurafsky et al., 1997).
Note that the highlighted segment in the audio
channel corresponds to the highlighted annotation
in the spreadsheet.
</bodyText>
<subsectionHeader confidence="0.998707">
3.2 An interlinear transcription component
</subsectionHeader>
<bodyText confidence="0.9965526">
Interlinear text is a kind of text in which
each word is annotated with phonological,
morphological and syntactic information
(displayed under the word) and each sentence
is annotated with a free translation. Our tool
</bodyText>
<figure confidence="0.862542">
Waveform display
AG-API
Transcription editor
SetRegion t1 t2 AG::SetAnchorOffset
SetRegion t1 t2
</figure>
<figureCaption confidence="0.999177">
Figure 5: Interlinear Transcription Tool
</figureCaption>
<bodyText confidence="0.999822416666667">
permits interlinear transcription aligned to a
primary audio signal, for greater accuracy and
accountability. Whole words and sub-parts of
words can be easily aligned with the audio.
Clicking on a piece of the annotation causes
the corresponding extent of audio signal to be
highlighted. As an extended recording is played
back, annotated sections are highlighted (both
waveform and interlinear text displays).
The screenshot in Figure 5 shows the tool with
some interlinear text from Mawu (a Manding lan-
guage of the Ivory Coast, West Africa).
</bodyText>
<subsectionHeader confidence="0.992082">
3.3 A waveform display component
</subsectionHeader>
<bodyText confidence="0.9999382">
The tools described above utilize WaveSurfer
and Snack (Sj¨olander, 2000; Sj¨olander and
Beskow, 2000). We have developed a plug-in
for WaveSurfer to support the inter-component
communication described in this paper.
</bodyText>
<sectionHeader confidence="0.923386" genericHeader="method">
4 Available Software and Future Work
</sectionHeader>
<bodyText confidence="0.9999000625">
The Annotation Graph Toolkit, version 1.0, con-
tains a complete implementation of the annota-
tion graph model, import filters for several for-
mats, loading/storing data to an annotation server
(MySQL), application programming interfaces in
C++ and Tcl/tk, and example annotation tools for
dialogue, ethology and interlinear text. The sup-
ported formats are: xlabel, TIMIT, BAS Parti-
tur, Penn Treebank, Switchboard, LDC Callhome,
CSV and AIF level 0. All software is distributed
under an open source license, and is available
from http://www.ldc.upenn.edu/AG/.
Future work will provide Python and Perl inter-
faces, more supported formats, a query language
and interpreter, a multichannel transcription tool,
and a client/server model.
</bodyText>
<sectionHeader confidence="0.9977" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999973625">
This paper has described a comprehensive infras-
tructure for developing annotation tools based on
annotation graphs. Our experience has shown us
that it is a simple matter to construct new special-
purpose annotation tools using high-level soft-
ware components. The tools can be quickly cre-
ated and deployed, and replaced by new versions
as annotation tasks evolve.
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.624211">
This material is based upon work supported by the
National Science Foundation under Grant Nos.
9978056, 9980009, and 9983258.
</bodyText>
<sectionHeader confidence="0.967851" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999786678571428">
Claude Barras, Edouard Geoffrois, Zhibiao Wu, and Mark
Liberman. 2001. Transcriber: development and use of a
tool for assisting speech corpora production. Speech
Communication, 33:5–22.
Steven Bird and Mark Liberman. 2001. A formal
framework for linguistic annotation. Speech
Communication, 33:23–60.
Steven Bird, David Day, John Garofolo, John Henderson,
Chris Laprun, and Mark Liberman. 2000. ATLAS: A
flexible and extensible architecture for linguistic annotation.
In Proceedings of the Second International Conference on
Language Resources and Evaluation. Paris: European
Language Resources Association.
Steve Cassidy and Jonathan Harrington. 2001. Multi-level
annotation of speech: An overview of the emu speech
database management system. Speech Communication,
33:61–77.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca.
1997. Switchboard SWBD-DAMSL Labeling Project
Coder’s Manual, Draft 13. Technical Report 97-02,
University of Colorado Institute of Cognitive Science.
[stripe.colorado.edu/˜jurafsky/manual.august1.html].
K˚are Sj¨olander and Jonas Beskow. 2000. Wavesurfer – an
open source speech tool. In Proceedings of the 6th
International Conference on Spoken Language Processing.
http://www.speech.kth.se/wavesurfer/.
K˚are Sj¨olander. 2000. The Snack sound toolkit.
http://www.speech.kth.se/snack/.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.858989">
<title confidence="0.999696">Annotation Tools Based on the Annotation Graph API</title>
<author confidence="0.958206">Steven Bird</author>
<author confidence="0.958206">Kazuaki Maeda</author>
<author confidence="0.958206">Xiaoyi Ma</author>
<author confidence="0.958206">Haejoong</author>
<affiliation confidence="0.96564">Linguistic Data Consortium, University of</affiliation>
<address confidence="0.980361">3615 Market Street, Suite 200, Philadelphia, PA 19104-2608,</address>
<abstract confidence="0.996259047619048">Annotation graphs provide an efficient and expressive data model for linguistic annotations of time-series data. This paper reports progress on a complete open-source software infrastructure supporting the rapid development of tools for transcribing and annotating data. This purpose infrastructure uses annotation graphs as the underlying model, and allows developers to quickly create special-purpose annotation tools using common components. An application programming interface, an I/O library, and graphical user interfaces are described. Our experience has shown us that it is a straightforward task to create new special-purpose annotation tools based on this general-purpose infrastructure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Claude Barras</author>
<author>Edouard Geoffrois</author>
<author>Zhibiao Wu</author>
<author>Mark Liberman</author>
</authors>
<title>Transcriber: development and use of a tool for assisting speech corpora production.</title>
<date>2001</date>
<journal>Speech Communication,</journal>
<pages>33--5</pages>
<contexts>
<context position="1926" citStr="Barras et al., 2001" startWordPosition="272" endWordPosition="275">nd reuse are better served by agreeing on the data models and interfaces. Annotation graphs (AGs) provide an efficient and expressive data model for linguistic annotations of time-series data (Bird and Liberman, Figure 1: Architecture for Annotation Systems 2001). Recently, the LDC has been developing a complete software infrastructure supporting the rapid development of tools for transcribing and annotating time-series data, in cooperation with NIST and MITRE as part of the ATLAS project, and with the developers of other widely used annotation systems, Transcriber and Emu (Bird et al., 2000; Barras et al., 2001; Cassidy and Harrington, 2001). The infrastructure is being used in the development of a series of annotation tools at the Linguistic Data Consortium. Two tools are shown in the paper: one for dialogue annotation and one for interlinear transcription. In both cases, the transcriptions are time-aligned to a digital audio signal. This paper will cover the following points: the application programming interfaces for manipulating annotation graph data and importing data from other formats; the model of inter-component communication which permits easy reuse of software components; and the design o</context>
</contexts>
<marker>Barras, Geoffrois, Wu, Liberman, 2001</marker>
<rawString>Claude Barras, Edouard Geoffrois, Zhibiao Wu, and Mark Liberman. 2001. Transcriber: development and use of a tool for assisting speech corpora production. Speech Communication, 33:5–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Mark Liberman</author>
</authors>
<title>A formal framework for linguistic annotation.</title>
<date>2001</date>
<journal>Speech Communication,</journal>
<pages>33--23</pages>
<marker>Bird, Liberman, 2001</marker>
<rawString>Steven Bird and Mark Liberman. 2001. A formal framework for linguistic annotation. Speech Communication, 33:23–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>David Day</author>
<author>John Garofolo</author>
<author>John Henderson</author>
<author>Chris Laprun</author>
<author>Mark Liberman</author>
</authors>
<title>ATLAS: A flexible and extensible architecture for linguistic annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources</booktitle>
<contexts>
<context position="1905" citStr="Bird et al., 2000" startWordPosition="268" endWordPosition="271">erests of sharing and reuse are better served by agreeing on the data models and interfaces. Annotation graphs (AGs) provide an efficient and expressive data model for linguistic annotations of time-series data (Bird and Liberman, Figure 1: Architecture for Annotation Systems 2001). Recently, the LDC has been developing a complete software infrastructure supporting the rapid development of tools for transcribing and annotating time-series data, in cooperation with NIST and MITRE as part of the ATLAS project, and with the developers of other widely used annotation systems, Transcriber and Emu (Bird et al., 2000; Barras et al., 2001; Cassidy and Harrington, 2001). The infrastructure is being used in the development of a series of annotation tools at the Linguistic Data Consortium. Two tools are shown in the paper: one for dialogue annotation and one for interlinear transcription. In both cases, the transcriptions are time-aligned to a digital audio signal. This paper will cover the following points: the application programming interfaces for manipulating annotation graph data and importing data from other formats; the model of inter-component communication which permits easy reuse of software compone</context>
</contexts>
<marker>Bird, Day, Garofolo, Henderson, Laprun, Liberman, 2000</marker>
<rawString>Steven Bird, David Day, John Garofolo, John Henderson, Chris Laprun, and Mark Liberman. 2000. ATLAS: A flexible and extensible architecture for linguistic annotation. In Proceedings of the Second International Conference on Language Resources and Evaluation. Paris: European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Cassidy</author>
<author>Jonathan Harrington</author>
</authors>
<title>Multi-level annotation of speech: An overview of the emu speech database management system.</title>
<date>2001</date>
<journal>Speech Communication,</journal>
<pages>33--61</pages>
<contexts>
<context position="1957" citStr="Cassidy and Harrington, 2001" startWordPosition="276" endWordPosition="279">erved by agreeing on the data models and interfaces. Annotation graphs (AGs) provide an efficient and expressive data model for linguistic annotations of time-series data (Bird and Liberman, Figure 1: Architecture for Annotation Systems 2001). Recently, the LDC has been developing a complete software infrastructure supporting the rapid development of tools for transcribing and annotating time-series data, in cooperation with NIST and MITRE as part of the ATLAS project, and with the developers of other widely used annotation systems, Transcriber and Emu (Bird et al., 2000; Barras et al., 2001; Cassidy and Harrington, 2001). The infrastructure is being used in the development of a series of annotation tools at the Linguistic Data Consortium. Two tools are shown in the paper: one for dialogue annotation and one for interlinear transcription. In both cases, the transcriptions are time-aligned to a digital audio signal. This paper will cover the following points: the application programming interfaces for manipulating annotation graph data and importing data from other formats; the model of inter-component communication which permits easy reuse of software components; and the design of the graphical user interfaces</context>
</contexts>
<marker>Cassidy, Harrington, 2001</marker>
<rawString>Steve Cassidy and Jonathan Harrington. 2001. Multi-level annotation of speech: An overview of the emu speech database management system. Speech Communication, 33:61–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Debra Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL Labeling Project Coder’s Manual, Draft 13.</title>
<date>1997</date>
<tech>Technical Report 97-02,</tech>
<institution>University of Colorado Institute of Cognitive Science.</institution>
<contexts>
<context position="8482" citStr="Jurafsky et al., 1997" startWordPosition="1222" endWordPosition="1225">. A key challenge is to handle overlapping turns and back-channel cues without disrupting the structure of individual speaker contributions. The tool side-steps these problems by permitting utterances to be independently aligned to a (multi-channel) recording. The records are displayed in a spreadsheet; clicking on a row of the spreadsheet causes the corresponding extent of audio signal to be highlighted. As an extended recording is played back, annotated sections are highlighted, in both the waveform and spreadsheet displays. Figure 4 shows the tool with a section of the TRAINS/DAMSL corpus (Jurafsky et al., 1997). Note that the highlighted segment in the audio channel corresponds to the highlighted annotation in the spreadsheet. 3.2 An interlinear transcription component Interlinear text is a kind of text in which each word is annotated with phonological, morphological and syntactic information (displayed under the word) and each sentence is annotated with a free translation. Our tool Waveform display AG-API Transcription editor SetRegion t1 t2 AG::SetAnchorOffset SetRegion t1 t2 Figure 5: Interlinear Transcription Tool permits interlinear transcription aligned to a primary audio signal, for greater a</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca. 1997. Switchboard SWBD-DAMSL Labeling Project Coder’s Manual, Draft 13. Technical Report 97-02, University of Colorado Institute of Cognitive Science. [stripe.colorado.edu/˜jurafsky/manual.august1.html].</rawString>
</citation>
<citation valid="true">
<authors>
<author>K˚are Sj¨olander</author>
<author>Jonas Beskow</author>
</authors>
<title>Wavesurfer – an open source speech tool.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Conference on Spoken Language Processing. http://www.speech.kth.se/wavesurfer/.</booktitle>
<marker>Sj¨olander, Beskow, 2000</marker>
<rawString>K˚are Sj¨olander and Jonas Beskow. 2000. Wavesurfer – an open source speech tool. In Proceedings of the 6th International Conference on Spoken Language Processing. http://www.speech.kth.se/wavesurfer/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K˚are Sj¨olander</author>
</authors>
<title>The Snack sound toolkit.</title>
<date>2000</date>
<note>http://www.speech.kth.se/snack/.</note>
<marker>Sj¨olander, 2000</marker>
<rawString>K˚are Sj¨olander. 2000. The Snack sound toolkit. http://www.speech.kth.se/snack/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>