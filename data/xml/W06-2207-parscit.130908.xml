<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000792">
<title confidence="0.996747">
A Hybrid Approach for the Acquisition of
Information Extraction Patterns
</title>
<author confidence="0.947037">
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno
</author>
<affiliation confidence="0.951403">
Technical University of Catalunya
</affiliation>
<address confidence="0.947569">
Barcelona, Spain
</address>
<email confidence="0.999667">
{surdeanu,turmo,ageno}@lsi.upc.edu
</email>
<sectionHeader confidence="0.996799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996433266666667">
In this paper we present a hybrid ap-
proach for the acquisition of syntactico-
semantic patterns from raw text. Our
approach co-trains a decision list learner
whose feature space covers the set of all
syntactico-semantic patterns with an Ex-
pectation Maximization clustering algo-
rithm that uses the text words as attributes.
We show that the combination of the two
methods always outperforms the decision
list learner alone. Furthermore, using a
modular architecture we investigate sev-
eral algorithms for pattern ranking, the
most important component of the decision
list learner.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992688543859649">
Traditionally, Information Extraction (IE) identi-
fies domain-specific events, entities, and relations
among entities and/or events with the goals of:
populating relational databases, providing event-
level indexing in news stories, feeding link discov-
ery applications, etcetera.
By and large the identification and selective ex-
traction of relevant information is built around a
set of domain-specific linguistic patterns. For ex-
ample, for a “financial market change” domain
one relevant pattern is &lt;NOUN fall MONEY
to MONEY&gt;. When this pattern is matched on
the text “London gold fell $4.70 to $308.35”, a
change of $4.70 is detected for the financial in-
strument “London gold”.
Domain-specific patterns are either hand-
crafted or acquired automatically (Riloff, 1996;
Yangarber et al., 2000; Yangarber, 2003; Steven-
son and Greenwood, 2005). To minimize annota-
tion costs, some of the latter approaches use lightly
supervised bootstrapping algorithms that require
as input only a small set of documents annotated
with their corresponding category label. The focus
of this paper is to improve such lightly supervised
pattern acquisition methods. Moreover, we focus
on robust bootstrapping algorithms that can han-
dle real-world document collections, which con-
tain many domains.
Although a rich literature covers bootstrap-
ping methods applied to natural language prob-
lems (Yarowsky, 1995; Riloff, 1996; Collins and
Singer, 1999; Yangarber et al., 2000; Yangar-
ber, 2003; Abney, 2004) several questions remain
unanswered when these methods are applied to
syntactic or semantic pattern acquisition. In this
paper we answer two of these questions:
(1) Can pattern acquisition be improved with
text categorization techniques?
Bootstrapping-based pattern acquisition algo-
rithms can also be regarded as incremental text
categorization (TC), since in each iteration docu-
ments containing certain patterns are assigned the
corresponding category label. Although TC is ob-
viously not the main goal of pattern acquisition
methodologies, it is nevertheless an integral part of
the learning algorithm: each iteration of the acqui-
sition algorithm depends on the previous assign-
ments of category labels to documents. Hence, if
the quality of the TC solution proposed is bad, the
quality of the acquired patterns will suffer.
Motivated by this observation, we introduce a
co-training-based algorithm (Blum and Mitchell,
1998) that uses a text categorization algorithm as
reinforcement for pattern acquisition. We show,
using both a direct and an indirect evaluation, that
the combination of the two methodologies always
improves the quality of the acquired patterns.
</bodyText>
<page confidence="0.997956">
48
</page>
<bodyText confidence="0.987345">
(2) Which pattern selection strategy is best?
While most bootstrapping-based algorithms fol-
low the same framework, they vary significantly
in what they consider the most relevant patterns in
each bootstrapping iteration. Several approaches
have been proposed in the context of word sense
disambiguation (Yarowsky, 1995), named entity
(NE) classification (Collins and Singer, 1999),
pattern acquisition for IE (Riloff, 1996; Yangarber,
2003), or dimensionality reduction for text catego-
rization (TC) (Yang and Pedersen, 1997). How-
ever, it is not clear which selection approach is
the best for the acquisition of syntactico-semantic
patterns. To answer this question, we have im-
plemented a modular pattern acquisition architec-
ture where several of these ranking strategies are
implemented and evaluated. The empirical study
presented in this paper shows that a strategy previ-
ously proposed for feature ranking for NE recogni-
tion outperforms algorithms designed specifically
for pattern acquisition.
The paper is organized as follows: Sec-
tion 2 introduces the bootstrapping framework
used throughout the paper. Section 3 introduces
the data collections. Section 4 describes the di-
rect and indirect evaluation procedures. Section 5
introduces a detailed empirical evaluation of the
proposed system. Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.967884" genericHeader="method">
2 The Pattern Acquisition Framework
</sectionHeader>
<bodyText confidence="0.999787153846154">
In this section we introduce a modular pattern ac-
quisition framework that co-trains two different
views of the document collection: the first view
uses the collection words to train a text categoriza-
tion algorithm, while the second view bootstraps
a decision list learner that uses all syntactico-
semantic patterns as features. The rules acquired
by the latter algorithm, of the form p → y, where
p is a pattern and y is a domain label, are the out-
put of the overall system. The system can be cus-
tomized with several pattern selection strategies
that dramatically influence the quality and order
of the acquired rules.
</bodyText>
<subsectionHeader confidence="0.998664">
2.1 Co-training Text Categorization and
Pattern Acquisition
</subsectionHeader>
<bodyText confidence="0.999700236363636">
Given two views of a classification task, co-
training (Blum and Mitchell, 1998) bootstraps a
separate classifier for each view as follows: (1)
it initializes both classifiers with the same small
amount of labeled data (i.e. seed documents in our
case); (2) it repeatedly trains both classifiers us-
ing the currently labeled data; and (3) after each
learning iteration, the two classifiers share all or a
subset of the newly labeled examples (documents
in our particular case).
The intuition is that each classifier provides
new, informative labeled data to the other classi-
fier. If the two views are conditional independent
and the two classifiers generally agree on unla-
beled data they will have low generalization error.
In this paper we focus on a “naive” co-training ap-
proach, which trains a different classifier in each
iteration and feeds its newly labeled examples to
the other classifier. This approach was shown to
perform well on real-world natural language prob-
lems (Collins and Singer, 1999).
Figure 1 illustrates the co-training framework
used in this paper. The feature space of the
first view contains only lexical information, i.e.
the collection words, and uses as classifier Ex-
pectation Maximization (EM) (Dempster et al.,
1977). EM is actually a class of iterative algo-
rithms that find maximum likelihood estimates of
parameters using probabilistic models over incom-
plete data (e.g. both labeled and unlabeled docu-
ments) (Dempster et al., 1977). EM was theoret-
ically proven to converge to a local maximum of
the parameters’ log likelihood. Furthermore, em-
pirical experiments showed that EM has excellent
performance for lightly-supervised text classifica-
tion (Nigam et al., 2000). The EM algorithm used
in this paper estimates its model parameters us-
ing the Naive Bayes (NB) assumptions, similarly
to (Nigam et al., 2000). From this point further,
we refer to this instance of the EM algorithm as
NB-EM.
The feature space of the second view contains
the syntactico-semantic patterns, generated using
the procedure detailed in Section 3.2. The second
learner is the actual pattern acquisition algorithm
implemented as a bootstrapped decision list clas-
sifier.
The co-training algorithm introduced in this pa-
per interleaves one iteration of the NB-EM algo-
rithm with one iteration of the pattern acquisition
algorithm. If one classifier converges faster (e.g.
NB-EM typically converges in under 20 iterations,
whereas the acquisition algorithms learns new pat-
terns for hundreds of iterations) we continue boot-
strapping the other classifier alone.
</bodyText>
<subsectionHeader confidence="0.997498">
2.2 The Text Categorization Algorithm
</subsectionHeader>
<bodyText confidence="0.997685">
The parameters of the generative NB model, ˆB, in-
clude the probability of seeing a given category,
</bodyText>
<page confidence="0.997733">
49
</page>
<figure confidence="0.999529708333334">
Yes
No
Pattern
acquisition
iteration
Pattern
acquisition
terminated?
Yes
Ranking
method
Initialize
pattern
acquisition
NB−EM
converged?
No
Patterns
Initialize
NB−EM
NB−EM
Iteration
Labeled seed documents
Unlabeled documents
</figure>
<figureCaption confidence="0.999438">
Figure 1: Co-training framework for pattern acquisition.
</figureCaption>
<listItem confidence="0.999078533333334">
1. Initialization:
• Initialize the set of labeled examples with n la-
beled seed documents of the form (di, yi). yi is
the label of the ith document di. Each docu-
ment di contains a set of patterns {pi1,pi2, ...,pim}.
• Initialize the list of learned rules R = {}.
2. Loop:
• For each label y, select a small set of pattern
rules r = p → y, r ∈/ R.
• Append all selected rules r to R.
• For all non-seed documents d that contain a
pattern in R, set label(d) = arg maxp,y strength(p, y).
3. Termination condition:
• Stop if no rules selected or maximum number
of iterations reached.
</listItem>
<figureCaption confidence="0.99076">
Figure 2: Pattern acquisition meta algorithm
</figureCaption>
<bodyText confidence="0.999915571428571">
P(c|ˆB), and the probability of seeing a word given
a category, P(w|c; ˆB). We calculate both simi-
larly to Nigam (2000). Using these parameters,
the word independence assumption typical to the
Naive Bayes model, and the Bayes rule, the prob-
ability that a document d has a given category c is
calculated as:
</bodyText>
<equation confidence="0.9924605">
P(c|d; ˆθ) = P(c|
= P(c|ˆθ)Π�d|1P(wi|c; ˆθ)
Eqdl
P(cj |ˆθAn 1P(wi |cj; ˆθ) (2)
</equation>
<subsectionHeader confidence="0.99272">
2.3 The Pattern Acquisition Algorithm
</subsectionHeader>
<bodyText confidence="0.999940477272728">
The lightly-supervised pattern acquisition algo-
rithm iteratively learns domain-specific IE pat-
terns from a small set of labeled documents and
a much larger set of unlabeled documents. Dur-
ing each learning iteration, the algorithm acquires
a new set of patterns and labels more documents
based on the new evidence. The algorithm output
is a list R of rules p → y, where p is a pattern
in the set of patterns P, and y a category label in
Y = {1...k}, k being the number of categories in
the document collection. The list of acquired rules
R is sorted in descending order of rule importance
to guarantee that the most relevant rules are ac-
cessed first. This generic bootstrapping algorithm
is formalized in Figure 2.
Previous studies called the class of algorithms
illustrated in Figure 2 “cautious” or “sequential”
because in each iteration they acquire 1 or a small
set of rules (Abney, 2004; Collins and Singer,
1999). This strategy stops the algorithm from be-
ing over-confident, an important restriction for an
algorithm that learns from large amounts of unla-
beled data. This approach was empirically shown
to perform better than a method that in each itera-
tion acquires all rules that match a certain criterion
(e.g. the corresponding rule has a strength over a
certain threshold).
The key element where most instances of this
algorithm vary is the select procedure, which de-
cides which rules are acquired in each iteration.
Although several selection strategies have been
previously proposed for various NLP problems, to
our knowledge no existing study performs an em-
pirical analysis of such strategies in the context of
acquisition of IE patterns. For this reason, we im-
plement several selection methods in our system
(described in Section 2.4) and evaluate their per-
formance in Section 5.
The label of each collection document is given
by the strength of its patterns. Similarly to (Collins
and Singer, 1999; Yarowsky, 1995), we define the
strength of a pattern p in a category y as the pre-
cision of p in the set of documents labeled with
category y, estimated using Laplace smoothing:
</bodyText>
<equation confidence="0.9996395">
strength(p, y) = count(p, y) + � (3)
count(p) + k�
</equation>
<bodyText confidence="0.999969866666667">
where count(p, y) is the number of documents la-
beled y containing pattern p, count(p) is the over-
all number of labeled documents containing p, and
k is the number of domains. For all experiments
presented here we used c = 1.
Another point where acquisition algorithms dif-
fer is the initialization procedure: some start with a
small number of hand-labeled documents (Riloff,
1996), as illustrated in Figure 2, while others start
with a set of seed rules (Yangarber et al., 2000;
Yangarber, 2003). However, these approaches are
conceptually similar: the seed rules are simply
used to generate the seed documents.
This paper focuses on the framework introduced
in Figure 2 for two reasons: (a) “cautious” al-
</bodyText>
<equation confidence="0.9736705">
(1)
ˆθ)
ˆθ)P(d|c; ˆθ)
P(d|
</equation>
<page confidence="0.946185">
50
</page>
<bodyText confidence="0.999756">
gorithms were shown to perform best for several
NLP problems (including acquisition of IE pat-
terns), and (b) it has nice theoretical properties:
Abney (2004) showed that, regardless of the selec-
tion procedure, “sequential” bootstrapping algo-
rithms converge to a local minimum of K, where
K is an upper bound of the negative log likelihood
of the data. Obviously, the quality of the local
minimum discovered is highly dependent of the
selection procedure, which is why we believe an
evaluation of several pattern selection strategies is
important.
</bodyText>
<subsectionHeader confidence="0.995776">
2.4 Selection Criteria
</subsectionHeader>
<bodyText confidence="0.98612792">
The pattern selection component, i.e. the select
procedure of the algorithm in Figure 2, consists of
the following: (a) for each category y all patterns
p are sorted in descending order of their scores in
the current category, score(p, y), and (b) for each
category the top k patterns are selected. For all
experiments in this paper we have used k = 3.
We provide four different implementations for the
pattern scoring function score(p, y) according to
four different selection criteria.
Criterion 1: Riloff
This selection criterion was developed specifically
for the pattern acquisition task (Riloff, 1996) and
has been used in several other pattern acquisition
systems (Yangarber et al., 2000; Yangarber, 2003;
Stevenson and Greenwood, 2005). The intuition
behind it is that a qualitative pattern is yielded by a
compromise between pattern precision (which is a
good indicator of relevance) and pattern frequency
(which is a good indicator of coverage). Further-
more, the criterion considers only patterns that are
positively correlated with the corresponding cate-
gory, i.e. their precision is higher than 50%. The
Riloff score of a pattern p in a category y is for-
malized as:
</bodyText>
<equation confidence="0.9988416">
prec(p, y) log(count(p, y)),
score(p, y) = if prec(p, y) &gt; 0.5; (4)
0, otherwise.
prec(p, y) = count(p, y) (5)
count(p)
</equation>
<bodyText confidence="0.9430205">
where prec(p, y) is the raw precision of pattern p
in the set of documents labeled with category y.
Criterion 2: Collins
This criterion was used in a lightly-supervised NE
recognizer (Collins and Singer, 1999). Unlike the
previous criterion, which combines relevance and
frequency in the same scoring function, Collins
considers only patterns whose raw precision is
over a hard threshold T and ranks them by their
global coverage:
</bodyText>
<equation confidence="0.938713333333333">
f count(p), if prec(p, y) &gt; T;
score(p, y) = Sl (6)
0, otherwise.
</equation>
<bodyText confidence="0.988304071428571">
Similarly to (Collins and Singer, 1999) we used
T = 0.95 for all experiments reported here.
Criterion 3: x2 (Chi)
The x2 score measures the lack of independence
between a pattern p and a category y. It is com-
puted using a two-way contingency table of p and
y, where a is the number of times p and y co-occur,
b is the number of times p occurs without y, c is
the number of times y occurs without p, and d is
the number of times neither p nor y occur. The
number of documents in the collection is n. Sim-
ilarly to the first criterion, we consider only pat-
terns positively correlated with the corresponding
category:
</bodyText>
<equation confidence="0.992192666666667">
�
X2(p, y), if prec(p, y) &gt; 0.5;
score(p, y) = (7)
0, otherwise.
n(ad − cb)2 (8)
(a + c)(b + d)(a + b)(c + d)
</equation>
<bodyText confidence="0.9668302">
The x2 statistic was previously reported to be
the best feature selection strategy for text catego-
rization (Yang and Pedersen, 1997).
Criterion 4: Mutual Information (MI)
Mutual information is a well known information
theory criterion that measures the independence of
two variables, in our case a pattern p and a cate-
gory y (Yang and Pedersen, 1997). Using the same
contingency table introduced above, the MI crite-
rion is estimated as:
</bodyText>
<equation confidence="0.9832825">
score = 1 MI (p, y), if prec(p, y) &gt; 0.5;9
�, y) l 0, otherwise. ( )
P(p n y)
MI(p, y) = log (10)
P(p) X P(y)
na
≈ log (11)
(a + c)(a + b)
</equation>
<sectionHeader confidence="0.998739" genericHeader="method">
3 The Data
</sectionHeader>
<subsectionHeader confidence="0.998226">
3.1 The Document Collections
</subsectionHeader>
<bodyText confidence="0.999557444444444">
For all experiments reported in this paper we used
the following three document collections: (a) the
AP collection is the Associated Press (year 1999)
subset of the AQUAINT collection (LDC catalog
number LDC2002T31); (b) the LATIMES collec-
tion is the Los Angeles Times subset of the TREC-
5 collection1; and (c) the REUTERS collection is
the by now classic Reuters-21578 text categoriza-
tion collection2.
</bodyText>
<footnote confidence="0.9998105">
1http://trec.nist.gov/data/docs eng.html
2http://trec.nist.gov/data/reuters/reuters.html
</footnote>
<equation confidence="0.91301">
X2(p, y) =
</equation>
<page confidence="0.984775">
51
</page>
<table confidence="0.9221245">
Collection # of docs # of categories # of words # of patterns
AP 5000 7 24812 140852
LATIMES 5000 8 29659 69429
REUTERS 9035 10 12905 36608
</table>
<tableCaption confidence="0.999802">
Table 1: Document collections used in the evaluation
</tableCaption>
<bodyText confidence="0.8817155">
Similarly to previous work, for the REUTERS
collection we used the ModApte split and selected
the ten most frequent categories (Nigam et al.,
2000). Due to memory limitations on our test ma-
chines, we reduced the size of the AP and LA-
TIMES collections to their first 5,000 documents
(the complete collections contain over 100,000
documents).
The collection words were pre-processed as fol-
lows: (i) stop words and numbers were discarded;
(ii) all words were converted to lower case; and
(iii) terms that appear in a single document were
removed. Table 1 lists the collection characteris-
tics after pre-processing.
</bodyText>
<subsectionHeader confidence="0.989383">
3.2 Pattern Generation
</subsectionHeader>
<bodyText confidence="0.999971384615385">
In order to extract the set of patterns available in
a document, each collection document undergoes
the following processing steps: (a) we recognize
and classify named entities3, and (b) we generate
full parse trees of all document sentences using a
probabilistic context-free parser.
Following the above processing steps, we ex-
tract Subject-Verb-Object (SVO) tuples using a se-
ries of heuristics, e.g.: (a) nouns preceding active
verbs are subjects, (b) nouns directly attached to a
verb phrase are objects, (c) nouns attached to the
verb phrase through a prepositional attachment are
indirect objects. Each tuple element is replaced
with either its head word, if its head word is not
included in a NE, or with the NE category oth-
erwise. For indirect objects we additionally store
the accompanying preposition. Lastly, each tuple
containing more than two elements is generalized
by maintaining only subsets of two and three of its
elements and replacing the others with a wildcard.
Table 2 lists the patterns extracted from one
sample sentence. As Table 2 hints, the system
generates a large number of candidate patterns. It
is the task of the pattern acquisition algorithm to
extract only the relevant ones from this complex
search space.
</bodyText>
<sectionHeader confidence="0.998132" genericHeader="method">
4 The Evaluation Procedures
</sectionHeader>
<subsectionHeader confidence="0.995323">
4.1 The Indirect Evaluation Procedure
</subsectionHeader>
<bodyText confidence="0.998724">
The goal of our evaluation procedure is to measure
the quality of the acquired patterns. Intuitively,
</bodyText>
<tableCaption confidence="0.6571655">
3We identify six categories: persons, locations, organiza-
tions, other names, temporal and numerical expressions.
</tableCaption>
<table confidence="0.996384909090909">
Text The Minnesota Vikings beat the Arizona
Cardinals in yesterday’s game.
Patterns s(ORG) v(beat)
v(beat) o(ORG)
s(ORG) o(ORG)
v(beat) io(in game)
s(ORG) io(in game)
o(ORG) io(in game)
s(ORG) v(beat) o(ORG)
s(ORG) v(beat) io(in game)
v(beat) o(ORG) io(in game)
</table>
<tableCaption confidence="0.991056666666667">
Table 2: Patterns extracted from one sample sentence. s
stands for subject, v for verb, o for object, and io for indirect
object.
</tableCaption>
<bodyText confidence="0.999761928571429">
the learned patterns should have high coverage and
low ambiguity. We indirectly measure the quality
of the acquired patterns using a text categorization
strategy: we feed the acquired rules to a decision-
list classifier, which is then used to classify a new
set of documents. The classifier assigns to each
document the category label given by the first rule
whose pattern matches. Since we expect higher-
quality patterns to appear higher in the rule list,
the decision-list classifier never changes the cate-
gory of an already-labeled document.
The quality of the generated classification is
measured using micro-averaged precision and re-
call:
</bodyText>
<equation confidence="0.9996645">
�q i=1 TruePositivesi
P _ (12)
�qi= 1(TruePositivesi + FalsePositivesi)
�q i=1 TruePositivesi
R _ (13)
�qi= 1(TruePositivesi + FalseNegativesi)
</equation>
<bodyText confidence="0.999623708333333">
where q is the number of categories in the docu-
ment collection.
For all experiments and all collections with the
exception of REUTERS, which has a standard
document split for training and testing, we used 5-
fold cross validation: we randomly partitioned the
collections into 5 sets of equal sizes, and reserved
a different one for testing in each fold.
We have chosen this evaluation strategy because
this indirect approach was shown to correlate well
with a direct evaluation, where the learned patterns
were used to customize an IE system (Yangarber
et al., 2000). For this reason, much of the fol-
lowing work on pattern acquisition has used this
approach as a de facto evaluation standard (Yan-
garber, 2003; Stevenson and Greenwood, 2005).
Furthermore, given the high number of domains
and patterns (we evaluate on 25 domains), an eval-
uation by human experts is extremely costly. Nev-
ertheless, to show that the proposed indirect eval-
uation correlates well with a direct evaluation, two
human experts have evaluated the patterns in sev-
eral domains. The direct evaluation procedure is
described next.
</bodyText>
<page confidence="0.996219">
52
</page>
<subsectionHeader confidence="0.902718">
4.2 The Direct Evaluation Procedure
</subsectionHeader>
<bodyText confidence="0.999773344827586">
The task of manually deciding whether an ac-
quired pattern is relevant or not for a given domain
is not trivial, mainly due to the ambiguity of the
patterns. Thus, this process should be carried out
by more than one expert, so that the relevance of
the ambiguous patterns can be agreed upon. For
example, the patterns s(ORG) v(score) o(goal) and
s(PER) v(lead) io(with point) are clearly relevant
only for the sports domain, whereas the patterns
v(sign) io(as agent) and o(title) io(in DATE) might
be regarded as relevant for other domains as well.
The specific procedure to manually evaluate the
patterns is the following: (1) two experts sepa-
rately evaluate the acquired patterns for the con-
sidered domains and collections; and (2) the re-
sults of both evaluations are compared. For any
disagreement, we have opted for a strict evalua-
tion: all the occurrences of the corresponding pat-
tern are looked up in the collection and, whenever
at least one pattern occurrence belongs to a docu-
ment assigned to a different domain than the do-
main in question, the pattern will be considered as
not relevant.
Both the ambiguity and the high number of
the extracted patterns have prevented us from per-
forming an exhaustive direct evaluation. For this
reason, only the top (most relevant) 100 patterns
have been evaluated for one domain per collection.
The results are detailed in Section 5.2.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="evaluation">
5 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.992216">
5.1 Indirect Evaluation
</subsectionHeader>
<bodyText confidence="0.992928428571429">
For a better understanding of the proposed ap-
proach we perform an incremental evaluation:
first, we evaluate only the various pattern selection
criteria described in Section 2.4 by disabling the
NB-EM component. Second, using the best selec-
tion criteria, we evaluate the complete co-training
system.
In both experiments we initialize the system
with high-precision manually-selected seed rules
which yield seed documents with a coverage of
10% of the training partitions. The remaining 90%
of the training documents are maintained unla-
beled. For all experiments we used a maximum of
400 bootstrapping iterations. The acquired rules
are fed to the decision list classifier which assigns
category labels to the documents in the test parti-
tions.
Evaluation of the pattern selection criteria
Figure 3 illustrates the precision/recall charts
of the four algorithms as the number of patterns
made available to the decision list classifier in-
creases. All charts show precision/recall points
starting after 100 learning iterations with 100-
iteration increments. It is immediately obvious
that the Collins selection criterion performs sig-
nificantly better than the other three criteria. For
the same recall point, Collins yields a classifica-
tion model with much higher precision, with dif-
ferences ranging from 5% in the REUTERS col-
lection to 20% in the AP collection.
Theorem 5 in (Abney, 2002) provides a theo-
retical explanation for these results: if certain in-
dependence conditions between the classifier rules
are satisfied and the precision of each rule is larger
than a threshold T, then the precision of the final
classifier is larger than T. Although the rule inde-
pendence conditions are certainly not satisfied in
our real-world evaluation, the above theorem in-
dicates that there is a strong relation between the
precision of the classifier rules on labeled data and
the precision of the final classifier. Our results pro-
vide the empirical proof that controling the preci-
sion of the acquired rules (i.e. the Collins crite-
rion) is important.
The Collins criterion controls the recall of the
learned model by favoring rules with high fre-
quency in the collection. However, since the other
two criteria do not use a high precision thresh-
old, they will acquire more rules, which translates
in better recall. For two out of the three collec-
tions, Riloff and Chi obtain a slightly better recall,
about 2% higher than Collins’, albeit with a much
lower precision. We do not consider this an im-
portant advantage: in the next section we show
that co-training with the NB-EM component fur-
ther boosts the precision and recall of the Collins-
based acquisition algorithm.
The MI criterion performs the worst of the four
evaluated criteria. A clue for this behavior lies in
the following equivalent form for MI: MI(p, y) =
log P(p|y)−log P(p). This formula indicates that,
for patterns with equal conditional probabilities
P(p|y), MI assigns higher scores to patterns with
lower frequency. This is not the desired behavior
in a TC-oriented system.
Evaluation of the co-training system
Figure 4 compares the performance of the
stand-alone pattern acquisition algorithm (“boot-
strapping”) with the performance of the acquisi-
tion algorithm trained in the co-training environ-
</bodyText>
<page confidence="0.990931">
53
</page>
<figure confidence="0.99929087755102">
0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55
Recall
0.1 0.15 0.2 0.25 0.3 0.35 0.4
Recall
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45
Recall
collins
riloff
chi
mi
collins
riloff
chi
mi
Precision 0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
Precision 0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
Precision 0.95
0.9
0.85
0.8
0.75
0.7
0.65
collins
riloff
chi
mi
(a) (b) (c)
</figure>
<figureCaption confidence="0.7879605">
Figure 3: Performance of the pattern acquisition algorithm for various pattern selection strategies and multiple collections:
(a) AP, (b) LATIMES, and (c) REUTERS
</figureCaption>
<bodyText confidence="0.999809210526316">
ment (“co-training”). For both setups we used the
best pattern selection criterion for pattern acqui-
sition, i.e. the Collins criterion. To put things in
perspective, we also depict the performance ob-
tained with a baseline system, i.e. the system con-
figured to use the Riloff pattern selection criterion
and without the NB-EM algorithm (“baseline”).
To our knowledge, this system, or a variation of
it, is the current state-of-the-art in pattern acqui-
sition (Riloff, 1996; Yangarber et al., 2000; Yan-
garber, 2003; Stevenson and Greenwood, 2005).
All algorithms were initialized with the same seed
rules and had access to all documents.
Figure 4 shows that the quality of the learned
patterns always improves if the pattern acquisi-
tion algorithm is “reinforced” with EM. For the
same recall point, the patterns acquired in the
co-training environment yield classification mod-
els with precision (generally) much larger than
the models generated by the pattern acquisition
algorithm alone. When using the same pat-
tern acquisition criterion, e.g. Collins, the dif-
ferences between the co-training approach and
the stand-alone pattern acquisition method (“boot-
strapping”) range from 2-3% in the REUTERS
collection to 20% in the LATIMES collection.
These results support our intuition that the sparse
pattern space is insufficient to generate good clas-
sification models, which directly influences the
quality of all acquired patterns.
Furthermore, due to the increased coverage of
the lexicalized collection views, the patterns ac-
quired in the co-training setup generally have bet-
ter recall, up to 11% higher in the LATIMES col-
lection.
Lastly, the comparison of our best system (“co-
training”) against the current state-of-the-art (our
“baseline”) draws an even more dramatic picture:
</bodyText>
<table confidence="0.998989428571429">
Collection Domain Relevant Relevant Initial
patterns patterns inter-expert
baseline co-training agreement
AP Sports 22% 68% 84%
LATIMES Financial 67% 76% 70%
REUTERS Corporate 38% 46% 66%
Acquisitions
</table>
<tableCaption confidence="0.982893">
Table 3: Percentage of relevant patterns for one domain per
collection by the baseline system (Riloff) and the co-training
system.
</tableCaption>
<bodyText confidence="0.999288">
for the same recall point, the co-training system
obtains a precision up to 35% higher for AP and
LATIMES, and up to 10% higher for REUTERS.
</bodyText>
<subsectionHeader confidence="0.99424">
5.2 Direct Evaluation
</subsectionHeader>
<bodyText confidence="0.99999072">
As stated in Section 4.2, two experts have man-
ually evaluated the top 100 acquired patterns for
one different domain in each of the three collec-
tions. The three corresponding domains have been
selected intending to deal with different degrees of
ambiguity, which are reflected in the initial inter-
expert agreement. Any disagreement between ex-
perts is solved using the algorithm introduced in
Section 4.2. Table 3 shows the results of this di-
rect evaluation. The co-training approach outper-
forms the baseline for all three collections. Con-
cretely, improvements of 9% and 8% are achieved
for the Financial and the Corporate Acquisitions
domains, and 46%, by far the largest difference, is
found for the Sports domain in AP. Table 4 lists
the top 20 patterns extracted by both approaches
in the latter domain. It can be observed that for
the baseline, only the top 4 patterns are relevant,
the rest being extremely general patterns. On the
other hand, the quality of the patterns acquired by
our approach is much higher: all the patterns are
relevant to the domain, although 7 out of the 20
might be considered ambiguous and according to
the criterion defined in Section 4.2 have been eval-
uated as not relevant.
</bodyText>
<page confidence="0.985997">
54
</page>
<figure confidence="0.999002833333333">
0.3 0.35 0.4 0.45 0.5 0.55 0.6
Recall
0.2 0.25 0.3 0.35 0.4 0.45
Recall
0.15 0.2 0.25 0.3 0.35 0.4 0.45
Recall
co-training
bootstrapping
baseline
co-training
bootstrapping
baseline
Precision 0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
Precision 0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
Precision 0.95
0.9
0.85
0.8
0.75
0.7
co-training
bootstrapping
baseline
(a) (b) (c)
</figure>
<figureCaption confidence="0.9527615">
Figure 4: Comparison of the bootstrapping pattern acquisition algorithm with the co-training approach: (a) AP, (b) LATIMES,
and (c) REUTERS
</figureCaption>
<equation confidence="0.883903857142857">
Baseline Co-training
s(he) o(game) v(win) o(title)
v(miss) o(game) s(I) v(play)
v(play) o(game) s(he) v(game)
v(play) io(in LOC) s(we) v(play)
v(go) o(be) v(miss) o(game)
s(he) v(be) s(he) v(coach)
s(that) v(be) v(lose) o(game)
s(I) v(be) s(I) o(play)
s(it) v(go) o(be) v(make) o(play)
s(it) v(be) v(play) io(in game)
s(I) v(think) v(want) o(play)
s(I) v(know) v(win) o(MISC)
s(I) v(want) s(he) o(player)
s(there) v(be) v(start) o(game)
s(we)v(do) s(PER) o(contract)
v(do) o(it) s(we) o(play)
s(it) o(be) s(team) v(win)
s(we) v(are) v(rush) io(for yard)
s(we) v(go) s(we) o(team)
s(PER) o(DATE) v(win) o(Bowl)
</equation>
<tableCaption confidence="0.904129">
Table 4: Top 20 patterns acquired from the Sports domain
by the baseline system (Riloff) and the co-training system for
the AP collection. The correct patterns are in bold.
</tableCaption>
<sectionHeader confidence="0.999307" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999968333333333">
This paper introduces a hybrid, lightly-supervised
method for the acquisition of syntactico-semantic
patterns for Information Extraction. Our approach
co-trains a decision list learner whose feature
space covers the set of all syntactico-semantic
patterns with an Expectation Maximization clus-
tering algorithm that uses the text words as at-
tributes. Furthermore, we customize the decision
list learner with up to four criteria for pattern se-
lection, which is the most important component of
the acquisition algorithm.
For the evaluation of the proposed approach we
have used both an indirect evaluation based on
Text Categorization and a direct evaluation where
human experts evaluated the quality of the gener-
ated patterns. Our results indicate that co-training
the Expectation Maximization algorithm with the
decision list learner tailored to acquire only high
precision patterns is by far the best solution. For
the same recall point, the proposed method in-
creases the precision of the generated models up
to 35% from the previous state of the art. Further-
more, the combination of the two feature spaces
(words and patterns) also increases the coverage
of the acquired patterns. The direct evaluation of
the acquired patterns by the human experts vali-
dates these results.
</bodyText>
<sectionHeader confidence="0.999204" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999665225">
S. Abney. 2002. Bootstrapping. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics.
S. Abney. 2004. Understanding the Yarowsky algorithm.
Computational Linguistics, 30(3).
A. Blum and T. Mitchell. 1998. Combining labeled and un-
labeled data with co-training. In Proceedings of the 11th
Annual Conference on Computational Learning Theory.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of EMNLP.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Series B,
39(1).
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000.
Text classification from labeled and unlabeled documents
using EM. Machine Learning, 39(2/3).
E. Riloff. 1996. Automatically generating extraction patterns
from untagged text. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence (AAAI-96).
M. Stevenson and M. Greenwood. 2005. A semantic ap-
proach to ie pattern induction. In Proceedings of the 43rd
Meeting of the Association for Computational Linguistics.
Y. Yang and J. O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proceed-
ings of the Fourteenth International Conference on Ma-
chine Learning.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hutunen.
2000. Automatic acquisition of domain knowledge for in-
formation extraction. In Proceedings of the 18th Interna-
tional Conference of Computational Linguistics (COLING
2000).
R. Yangarber. 2003. Counter-training in discovery of se-
mantic patterns. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics (ACL
2003).
D. Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Proceedings of the
33rd Annual Meeting of the Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.999057">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685555">
<title confidence="0.9994365">A Hybrid Approach for the Acquisition Information Extraction Patterns</title>
<author confidence="0.973473">Mihai Surdeanu</author>
<author confidence="0.973473">Jordi Turmo</author>
<author confidence="0.973473">Alicia</author>
<affiliation confidence="0.996456">Technical University of</affiliation>
<address confidence="0.734667">Barcelona,</address>
<abstract confidence="0.99748675">In this paper we present a hybrid approach for the acquisition of syntacticosemantic patterns from raw text. Our approach co-trains a decision list learner whose feature space covers the set of all syntactico-semantic patterns with an Expectation Maximization clustering algorithm that uses the text words as attributes. We show that the combination of the two methods always outperforms the decision list learner alone. Furthermore, using a modular architecture we investigate several algorithms for pattern ranking, the most important component of the decision list learner.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<date>2002</date>
<booktitle>Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24088" citStr="Abney, 2002" startWordPosition="3860" endWordPosition="3861">tion criteria Figure 3 illustrates the precision/recall charts of the four algorithms as the number of patterns made available to the decision list classifier increases. All charts show precision/recall points starting after 100 learning iterations with 100- iteration increments. It is immediately obvious that the Collins selection criterion performs significantly better than the other three criteria. For the same recall point, Collins yields a classification model with much higher precision, with differences ranging from 5% in the REUTERS collection to 20% in the AP collection. Theorem 5 in (Abney, 2002) provides a theoretical explanation for these results: if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T, then the precision of the final classifier is larger than T. Although the rule independence conditions are certainly not satisfied in our real-world evaluation, the above theorem indicates that there is a strong relation between the precision of the classifier rules on labeled data and the precision of the final classifier. Our results provide the empirical proof that controling the precision of the acq</context>
</contexts>
<marker>Abney, 2002</marker>
<rawString>S. Abney. 2002. Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Understanding the Yarowsky algorithm.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="2280" citStr="Abney, 2004" startWordPosition="332" endWordPosition="333">osts, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff, 1996; Collins and Singer, 1999; Yangarber et al., 2000; Yangarber, 2003; Abney, 2004) several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition. In this paper we answer two of these questions: (1) Can pattern acquisition be improved with text categorization techniques? Bootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (TC), since in each iteration documents containing certain patterns are assigned the corresponding category label. Although TC is obviously not the main goal of pattern acquisition methodologies, it is nevertheless an integral part of the learning algori</context>
<context position="10410" citStr="Abney, 2004" startWordPosition="1617" endWordPosition="1618">cuments based on the new evidence. The algorithm output is a list R of rules p → y, where p is a pattern in the set of patterns P, and y a category label in Y = {1...k}, k being the number of categories in the document collection. The list of acquired rules R is sorted in descending order of rule importance to guarantee that the most relevant rules are accessed first. This generic bootstrapping algorithm is formalized in Figure 2. Previous studies called the class of algorithms illustrated in Figure 2 “cautious” or “sequential” because in each iteration they acquire 1 or a small set of rules (Abney, 2004; Collins and Singer, 1999). This strategy stops the algorithm from being over-confident, an important restriction for an algorithm that learns from large amounts of unlabeled data. This approach was empirically shown to perform better than a method that in each iteration acquires all rules that match a certain criterion (e.g. the corresponding rule has a strength over a certain threshold). The key element where most instances of this algorithm vary is the select procedure, which decides which rules are acquired in each iteration. Although several selection strategies have been previously prop</context>
<context position="12553" citStr="Abney (2004)" startWordPosition="1974" endWordPosition="1975">he initialization procedure: some start with a small number of hand-labeled documents (Riloff, 1996), as illustrated in Figure 2, while others start with a set of seed rules (Yangarber et al., 2000; Yangarber, 2003). However, these approaches are conceptually similar: the seed rules are simply used to generate the seed documents. This paper focuses on the framework introduced in Figure 2 for two reasons: (a) “cautious” al(1) ˆθ) ˆθ)P(d|c; ˆθ) P(d| 50 gorithms were shown to perform best for several NLP problems (including acquisition of IE patterns), and (b) it has nice theoretical properties: Abney (2004) showed that, regardless of the selection procedure, “sequential” bootstrapping algorithms converge to a local minimum of K, where K is an upper bound of the negative log likelihood of the data. Obviously, the quality of the local minimum discovered is highly dependent of the selection procedure, which is why we believe an evaluation of several pattern selection strategies is important. 2.4 Selection Criteria The pattern selection component, i.e. the select procedure of the algorithm in Figure 2, consists of the following: (a) for each category y all patterns p are sorted in descending order o</context>
</contexts>
<marker>Abney, 2004</marker>
<rawString>S. Abney. 2004. Understanding the Yarowsky algorithm. Computational Linguistics, 30(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory.</booktitle>
<contexts>
<context position="3205" citStr="Blum and Mitchell, 1998" startWordPosition="466" endWordPosition="469"> regarded as incremental text categorization (TC), since in each iteration documents containing certain patterns are assigned the corresponding category label. Although TC is obviously not the main goal of pattern acquisition methodologies, it is nevertheless an integral part of the learning algorithm: each iteration of the acquisition algorithm depends on the previous assignments of category labels to documents. Hence, if the quality of the TC solution proposed is bad, the quality of the acquired patterns will suffer. Motivated by this observation, we introduce a co-training-based algorithm (Blum and Mitchell, 1998) that uses a text categorization algorithm as reinforcement for pattern acquisition. We show, using both a direct and an indirect evaluation, that the combination of the two methodologies always improves the quality of the acquired patterns. 48 (2) Which pattern selection strategy is best? While most bootstrapping-based algorithms follow the same framework, they vary significantly in what they consider the most relevant patterns in each bootstrapping iteration. Several approaches have been proposed in the context of word sense disambiguation (Yarowsky, 1995), named entity (NE) classification (</context>
<context position="5568" citStr="Blum and Mitchell, 1998" startWordPosition="825" endWordPosition="828">the first view uses the collection words to train a text categorization algorithm, while the second view bootstraps a decision list learner that uses all syntacticosemantic patterns as features. The rules acquired by the latter algorithm, of the form p → y, where p is a pattern and y is a domain label, are the output of the overall system. The system can be customized with several pattern selection strategies that dramatically influence the quality and order of the acquired rules. 2.1 Co-training Text Categorization and Pattern Acquisition Given two views of a classification task, cotraining (Blum and Mitchell, 1998) bootstraps a separate classifier for each view as follows: (1) it initializes both classifiers with the same small amount of labeled data (i.e. seed documents in our case); (2) it repeatedly trains both classifiers using the currently labeled data; and (3) after each learning iteration, the two classifiers share all or a subset of the newly labeled examples (documents in our particular case). The intuition is that each classifier provides new, informative labeled data to the other classifier. If the two views are conditional independent and the two classifiers generally agree on unlabeled dat</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2225" citStr="Collins and Singer, 1999" startWordPosition="321" endWordPosition="324">ber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff, 1996; Collins and Singer, 1999; Yangarber et al., 2000; Yangarber, 2003; Abney, 2004) several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition. In this paper we answer two of these questions: (1) Can pattern acquisition be improved with text categorization techniques? Bootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (TC), since in each iteration documents containing certain patterns are assigned the corresponding category label. Although TC is obviously not the main goal of pattern acquisition methodologies, it </context>
<context position="3830" citStr="Collins and Singer, 1999" startWordPosition="556" endWordPosition="559"> that uses a text categorization algorithm as reinforcement for pattern acquisition. We show, using both a direct and an indirect evaluation, that the combination of the two methodologies always improves the quality of the acquired patterns. 48 (2) Which pattern selection strategy is best? While most bootstrapping-based algorithms follow the same framework, they vary significantly in what they consider the most relevant patterns in each bootstrapping iteration. Several approaches have been proposed in the context of word sense disambiguation (Yarowsky, 1995), named entity (NE) classification (Collins and Singer, 1999), pattern acquisition for IE (Riloff, 1996; Yangarber, 2003), or dimensionality reduction for text categorization (TC) (Yang and Pedersen, 1997). However, it is not clear which selection approach is the best for the acquisition of syntactico-semantic patterns. To answer this question, we have implemented a modular pattern acquisition architecture where several of these ranking strategies are implemented and evaluated. The empirical study presented in this paper shows that a strategy previously proposed for feature ranking for NE recognition outperforms algorithms designed specifically for patt</context>
<context position="6491" citStr="Collins and Singer, 1999" startWordPosition="974" endWordPosition="977">lassifiers share all or a subset of the newly labeled examples (documents in our particular case). The intuition is that each classifier provides new, informative labeled data to the other classifier. If the two views are conditional independent and the two classifiers generally agree on unlabeled data they will have low generalization error. In this paper we focus on a “naive” co-training approach, which trains a different classifier in each iteration and feeds its newly labeled examples to the other classifier. This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999). Figure 1 illustrates the co-training framework used in this paper. The feature space of the first view contains only lexical information, i.e. the collection words, and uses as classifier Expectation Maximization (EM) (Dempster et al., 1977). EM is actually a class of iterative algorithms that find maximum likelihood estimates of parameters using probabilistic models over incomplete data (e.g. both labeled and unlabeled documents) (Dempster et al., 1977). EM was theoretically proven to converge to a local maximum of the parameters’ log likelihood. Furthermore, empirical experiments showed th</context>
<context position="10437" citStr="Collins and Singer, 1999" startWordPosition="1619" endWordPosition="1622"> on the new evidence. The algorithm output is a list R of rules p → y, where p is a pattern in the set of patterns P, and y a category label in Y = {1...k}, k being the number of categories in the document collection. The list of acquired rules R is sorted in descending order of rule importance to guarantee that the most relevant rules are accessed first. This generic bootstrapping algorithm is formalized in Figure 2. Previous studies called the class of algorithms illustrated in Figure 2 “cautious” or “sequential” because in each iteration they acquire 1 or a small set of rules (Abney, 2004; Collins and Singer, 1999). This strategy stops the algorithm from being over-confident, an important restriction for an algorithm that learns from large amounts of unlabeled data. This approach was empirically shown to perform better than a method that in each iteration acquires all rules that match a certain criterion (e.g. the corresponding rule has a strength over a certain threshold). The key element where most instances of this algorithm vary is the select procedure, which decides which rules are acquired in each iteration. Although several selection strategies have been previously proposed for various NLP proble</context>
<context position="14473" citStr="Collins and Singer, 1999" startWordPosition="2283" endWordPosition="2286">relevance) and pattern frequency (which is a good indicator of coverage). Furthermore, the criterion considers only patterns that are positively correlated with the corresponding category, i.e. their precision is higher than 50%. The Riloff score of a pattern p in a category y is formalized as: prec(p, y) log(count(p, y)), score(p, y) = if prec(p, y) &gt; 0.5; (4) 0, otherwise. prec(p, y) = count(p, y) (5) count(p) where prec(p, y) is the raw precision of pattern p in the set of documents labeled with category y. Criterion 2: Collins This criterion was used in a lightly-supervised NE recognizer (Collins and Singer, 1999). Unlike the previous criterion, which combines relevance and frequency in the same scoring function, Collins considers only patterns whose raw precision is over a hard threshold T and ranks them by their global coverage: f count(p), if prec(p, y) &gt; T; score(p, y) = Sl (6) 0, otherwise. Similarly to (Collins and Singer, 1999) we used T = 0.95 for all experiments reported here. Criterion 3: x2 (Chi) The x2 score measures the lack of independence between a pattern p and a category y. It is computed using a two-way contingency table of p and y, where a is the number of times p and y co-occur, b i</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="6734" citStr="Dempster et al., 1977" startWordPosition="1011" endWordPosition="1014">and the two classifiers generally agree on unlabeled data they will have low generalization error. In this paper we focus on a “naive” co-training approach, which trains a different classifier in each iteration and feeds its newly labeled examples to the other classifier. This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999). Figure 1 illustrates the co-training framework used in this paper. The feature space of the first view contains only lexical information, i.e. the collection words, and uses as classifier Expectation Maximization (EM) (Dempster et al., 1977). EM is actually a class of iterative algorithms that find maximum likelihood estimates of parameters using probabilistic models over incomplete data (e.g. both labeled and unlabeled documents) (Dempster et al., 1977). EM was theoretically proven to converge to a local maximum of the parameters’ log likelihood. Furthermore, empirical experiments showed that EM has excellent performance for lightly-supervised text classification (Nigam et al., 2000). The EM algorithm used in this paper estimates its model parameters using the Naive Bayes (NB) assumptions, similarly to (Nigam et al., 2000). From</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="7186" citStr="Nigam et al., 2000" startWordPosition="1080" endWordPosition="1083">ture space of the first view contains only lexical information, i.e. the collection words, and uses as classifier Expectation Maximization (EM) (Dempster et al., 1977). EM is actually a class of iterative algorithms that find maximum likelihood estimates of parameters using probabilistic models over incomplete data (e.g. both labeled and unlabeled documents) (Dempster et al., 1977). EM was theoretically proven to converge to a local maximum of the parameters’ log likelihood. Furthermore, empirical experiments showed that EM has excellent performance for lightly-supervised text classification (Nigam et al., 2000). The EM algorithm used in this paper estimates its model parameters using the Naive Bayes (NB) assumptions, similarly to (Nigam et al., 2000). From this point further, we refer to this instance of the EM algorithm as NB-EM. The feature space of the second view contains the syntactico-semantic patterns, generated using the procedure detailed in Section 3.2. The second learner is the actual pattern acquisition algorithm implemented as a bootstrapped decision list classifier. The co-training algorithm introduced in this paper interleaves one iteration of the NB-EM algorithm with one iteration of</context>
<context position="16946" citStr="Nigam et al., 2000" startWordPosition="2723" endWordPosition="2726">ATIMES collection is the Los Angeles Times subset of the TREC5 collection1; and (c) the REUTERS collection is the by now classic Reuters-21578 text categorization collection2. 1http://trec.nist.gov/data/docs eng.html 2http://trec.nist.gov/data/reuters/reuters.html X2(p, y) = 51 Collection # of docs # of categories # of words # of patterns AP 5000 7 24812 140852 LATIMES 5000 8 29659 69429 REUTERS 9035 10 12905 36608 Table 1: Document collections used in the evaluation Similarly to previous work, for the REUTERS collection we used the ModApte split and selected the ten most frequent categories (Nigam et al., 2000). Due to memory limitations on our test machines, we reduced the size of the AP and LATIMES collections to their first 5,000 documents (the complete collections contain over 100,000 documents). The collection words were pre-processed as follows: (i) stop words and numbers were discarded; (ii) all words were converted to lower case; and (iii) terms that appear in a single document were removed. Table 1 lists the collection characteristics after pre-processing. 3.2 Pattern Generation In order to extract the set of patterns available in a document, each collection document undergoes the following</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96).</booktitle>
<contexts>
<context position="1569" citStr="Riloff, 1996" startWordPosition="226" endWordPosition="227">opulating relational databases, providing eventlevel indexing in news stories, feeding link discovery applications, etcetera. By and large the identification and selective extraction of relevant information is built around a set of domain-specific linguistic patterns. For example, for a “financial market change” domain one relevant pattern is &lt;NOUN fall MONEY to MONEY&gt;. When this pattern is matched on the text “London gold fell $4.70 to $308.35”, a change of $4.70 is detected for the financial instrument “London gold”. Domain-specific patterns are either handcrafted or acquired automatically (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems</context>
<context position="3872" citStr="Riloff, 1996" startWordPosition="564" endWordPosition="565">ment for pattern acquisition. We show, using both a direct and an indirect evaluation, that the combination of the two methodologies always improves the quality of the acquired patterns. 48 (2) Which pattern selection strategy is best? While most bootstrapping-based algorithms follow the same framework, they vary significantly in what they consider the most relevant patterns in each bootstrapping iteration. Several approaches have been proposed in the context of word sense disambiguation (Yarowsky, 1995), named entity (NE) classification (Collins and Singer, 1999), pattern acquisition for IE (Riloff, 1996; Yangarber, 2003), or dimensionality reduction for text categorization (TC) (Yang and Pedersen, 1997). However, it is not clear which selection approach is the best for the acquisition of syntactico-semantic patterns. To answer this question, we have implemented a modular pattern acquisition architecture where several of these ranking strategies are implemented and evaluated. The empirical study presented in this paper shows that a strategy previously proposed for feature ranking for NE recognition outperforms algorithms designed specifically for pattern acquisition. The paper is organized as</context>
<context position="12041" citStr="Riloff, 1996" startWordPosition="1890" endWordPosition="1891">1999; Yarowsky, 1995), we define the strength of a pattern p in a category y as the precision of p in the set of documents labeled with category y, estimated using Laplace smoothing: strength(p, y) = count(p, y) + � (3) count(p) + k� where count(p, y) is the number of documents labeled y containing pattern p, count(p) is the overall number of labeled documents containing p, and k is the number of domains. For all experiments presented here we used c = 1. Another point where acquisition algorithms differ is the initialization procedure: some start with a small number of hand-labeled documents (Riloff, 1996), as illustrated in Figure 2, while others start with a set of seed rules (Yangarber et al., 2000; Yangarber, 2003). However, these approaches are conceptually similar: the seed rules are simply used to generate the seed documents. This paper focuses on the framework introduced in Figure 2 for two reasons: (a) “cautious” al(1) ˆθ) ˆθ)P(d|c; ˆθ) P(d| 50 gorithms were shown to perform best for several NLP problems (including acquisition of IE patterns), and (b) it has nice theoretical properties: Abney (2004) showed that, regardless of the selection procedure, “sequential” bootstrapping algorith</context>
<context position="13573" citStr="Riloff, 1996" startWordPosition="2136" endWordPosition="2137">teria The pattern selection component, i.e. the select procedure of the algorithm in Figure 2, consists of the following: (a) for each category y all patterns p are sorted in descending order of their scores in the current category, score(p, y), and (b) for each category the top k patterns are selected. For all experiments in this paper we have used k = 3. We provide four different implementations for the pattern scoring function score(p, y) according to four different selection criteria. Criterion 1: Riloff This selection criterion was developed specifically for the pattern acquisition task (Riloff, 1996) and has been used in several other pattern acquisition systems (Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). The intuition behind it is that a qualitative pattern is yielded by a compromise between pattern precision (which is a good indicator of relevance) and pattern frequency (which is a good indicator of coverage). Furthermore, the criterion considers only patterns that are positively correlated with the corresponding category, i.e. their precision is higher than 50%. The Riloff score of a pattern p in a category y is formalized as: prec(p, y) log(count(p, y)), </context>
<context position="26965" citStr="Riloff, 1996" startWordPosition="4336" endWordPosition="4337"> of the pattern acquisition algorithm for various pattern selection strategies and multiple collections: (a) AP, (b) LATIMES, and (c) REUTERS ment (“co-training”). For both setups we used the best pattern selection criterion for pattern acquisition, i.e. the Collins criterion. To put things in perspective, we also depict the performance obtained with a baseline system, i.e. the system configured to use the Riloff pattern selection criterion and without the NB-EM algorithm (“baseline”). To our knowledge, this system, or a variation of it, is the current state-of-the-art in pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). All algorithms were initialized with the same seed rules and had access to all documents. Figure 4 shows that the quality of the learned patterns always improves if the pattern acquisition algorithm is “reinforced” with EM. For the same recall point, the patterns acquired in the co-training environment yield classification models with precision (generally) much larger than the models generated by the pattern acquisition algorithm alone. When using the same pattern acquisition criterion, e.g. Collins, the differences bet</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>M Greenwood</author>
</authors>
<title>A semantic approach to ie pattern induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1642" citStr="Stevenson and Greenwood, 2005" startWordPosition="234" endWordPosition="238">dexing in news stories, feeding link discovery applications, etcetera. By and large the identification and selective extraction of relevant information is built around a set of domain-specific linguistic patterns. For example, for a “financial market change” domain one relevant pattern is &lt;NOUN fall MONEY to MONEY&gt;. When this pattern is matched on the text “London gold fell $4.70 to $308.35”, a change of $4.70 is detected for the financial instrument “London gold”. Domain-specific patterns are either handcrafted or acquired automatically (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff, 1996; Collins and Singer, 1999; Yangarber et al</context>
<context position="13709" citStr="Stevenson and Greenwood, 2005" startWordPosition="2154" endWordPosition="2157">g: (a) for each category y all patterns p are sorted in descending order of their scores in the current category, score(p, y), and (b) for each category the top k patterns are selected. For all experiments in this paper we have used k = 3. We provide four different implementations for the pattern scoring function score(p, y) according to four different selection criteria. Criterion 1: Riloff This selection criterion was developed specifically for the pattern acquisition task (Riloff, 1996) and has been used in several other pattern acquisition systems (Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). The intuition behind it is that a qualitative pattern is yielded by a compromise between pattern precision (which is a good indicator of relevance) and pattern frequency (which is a good indicator of coverage). Furthermore, the criterion considers only patterns that are positively correlated with the corresponding category, i.e. their precision is higher than 50%. The Riloff score of a pattern p in a category y is formalized as: prec(p, y) log(count(p, y)), score(p, y) = if prec(p, y) &gt; 0.5; (4) 0, otherwise. prec(p, y) = count(p, y) (5) count(p) where prec(p, y) is the raw precision of patt</context>
<context position="20872" citStr="Stevenson and Greenwood, 2005" startWordPosition="3344" endWordPosition="3347">he exception of REUTERS, which has a standard document split for training and testing, we used 5- fold cross validation: we randomly partitioned the collections into 5 sets of equal sizes, and reserved a different one for testing in each fold. We have chosen this evaluation strategy because this indirect approach was shown to correlate well with a direct evaluation, where the learned patterns were used to customize an IE system (Yangarber et al., 2000). For this reason, much of the following work on pattern acquisition has used this approach as a de facto evaluation standard (Yangarber, 2003; Stevenson and Greenwood, 2005). Furthermore, given the high number of domains and patterns (we evaluate on 25 domains), an evaluation by human experts is extremely costly. Nevertheless, to show that the proposed indirect evaluation correlates well with a direct evaluation, two human experts have evaluated the patterns in several domains. The direct evaluation procedure is described next. 52 4.2 The Direct Evaluation Procedure The task of manually deciding whether an acquired pattern is relevant or not for a given domain is not trivial, mainly due to the ambiguity of the patterns. Thus, this process should be carried out by</context>
<context position="27038" citStr="Stevenson and Greenwood, 2005" startWordPosition="4345" endWordPosition="4348">rn selection strategies and multiple collections: (a) AP, (b) LATIMES, and (c) REUTERS ment (“co-training”). For both setups we used the best pattern selection criterion for pattern acquisition, i.e. the Collins criterion. To put things in perspective, we also depict the performance obtained with a baseline system, i.e. the system configured to use the Riloff pattern selection criterion and without the NB-EM algorithm (“baseline”). To our knowledge, this system, or a variation of it, is the current state-of-the-art in pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). All algorithms were initialized with the same seed rules and had access to all documents. Figure 4 shows that the quality of the learned patterns always improves if the pattern acquisition algorithm is “reinforced” with EM. For the same recall point, the patterns acquired in the co-training environment yield classification models with precision (generally) much larger than the models generated by the pattern acquisition algorithm alone. When using the same pattern acquisition criterion, e.g. Collins, the differences between the co-training approach and the stand-alone pattern acquisition met</context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>M. Stevenson and M. Greenwood. 2005. A semantic approach to ie pattern induction. In Proceedings of the 43rd Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3974" citStr="Yang and Pedersen, 1997" startWordPosition="576" endWordPosition="579">t the combination of the two methodologies always improves the quality of the acquired patterns. 48 (2) Which pattern selection strategy is best? While most bootstrapping-based algorithms follow the same framework, they vary significantly in what they consider the most relevant patterns in each bootstrapping iteration. Several approaches have been proposed in the context of word sense disambiguation (Yarowsky, 1995), named entity (NE) classification (Collins and Singer, 1999), pattern acquisition for IE (Riloff, 1996; Yangarber, 2003), or dimensionality reduction for text categorization (TC) (Yang and Pedersen, 1997). However, it is not clear which selection approach is the best for the acquisition of syntactico-semantic patterns. To answer this question, we have implemented a modular pattern acquisition architecture where several of these ranking strategies are implemented and evaluated. The empirical study presented in this paper shows that a strategy previously proposed for feature ranking for NE recognition outperforms algorithms designed specifically for pattern acquisition. The paper is organized as follows: Section 2 introduces the bootstrapping framework used throughout the paper. Section 3 introd</context>
<context position="15617" citStr="Yang and Pedersen, 1997" startWordPosition="2497" endWordPosition="2500">ntingency table of p and y, where a is the number of times p and y co-occur, b is the number of times p occurs without y, c is the number of times y occurs without p, and d is the number of times neither p nor y occur. The number of documents in the collection is n. Similarly to the first criterion, we consider only patterns positively correlated with the corresponding category: � X2(p, y), if prec(p, y) &gt; 0.5; score(p, y) = (7) 0, otherwise. n(ad − cb)2 (8) (a + c)(b + d)(a + b)(c + d) The x2 statistic was previously reported to be the best feature selection strategy for text categorization (Yang and Pedersen, 1997). Criterion 4: Mutual Information (MI) Mutual information is a well known information theory criterion that measures the independence of two variables, in our case a pattern p and a category y (Yang and Pedersen, 1997). Using the same contingency table introduced above, the MI criterion is estimated as: score = 1 MI (p, y), if prec(p, y) &gt; 0.5;9 �, y) l 0, otherwise. ( ) P(p n y) MI(p, y) = log (10) P(p) X P(y) na ≈ log (11) (a + c)(a + b) 3 The Data 3.1 The Document Collections For all experiments reported in this paper we used the following three document collections: (a) the AP collection i</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Y. Yang and J. O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of the Fourteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
<author>R Grishman</author>
<author>P Tapanainen</author>
<author>S Hutunen</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference of Computational Linguistics (COLING</booktitle>
<contexts>
<context position="1593" citStr="Yangarber et al., 2000" startWordPosition="228" endWordPosition="231">tional databases, providing eventlevel indexing in news stories, feeding link discovery applications, etcetera. By and large the identification and selective extraction of relevant information is built around a set of domain-specific linguistic patterns. For example, for a “financial market change” domain one relevant pattern is &lt;NOUN fall MONEY to MONEY&gt;. When this pattern is matched on the text “London gold fell $4.70 to $308.35”, a change of $4.70 is detected for the financial instrument “London gold”. Domain-specific patterns are either handcrafted or acquired automatically (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff</context>
<context position="12138" citStr="Yangarber et al., 2000" startWordPosition="1906" endWordPosition="1909">ision of p in the set of documents labeled with category y, estimated using Laplace smoothing: strength(p, y) = count(p, y) + � (3) count(p) + k� where count(p, y) is the number of documents labeled y containing pattern p, count(p) is the overall number of labeled documents containing p, and k is the number of domains. For all experiments presented here we used c = 1. Another point where acquisition algorithms differ is the initialization procedure: some start with a small number of hand-labeled documents (Riloff, 1996), as illustrated in Figure 2, while others start with a set of seed rules (Yangarber et al., 2000; Yangarber, 2003). However, these approaches are conceptually similar: the seed rules are simply used to generate the seed documents. This paper focuses on the framework introduced in Figure 2 for two reasons: (a) “cautious” al(1) ˆθ) ˆθ)P(d|c; ˆθ) P(d| 50 gorithms were shown to perform best for several NLP problems (including acquisition of IE patterns), and (b) it has nice theoretical properties: Abney (2004) showed that, regardless of the selection procedure, “sequential” bootstrapping algorithms converge to a local minimum of K, where K is an upper bound of the negative log likelihood of </context>
<context position="13660" citStr="Yangarber et al., 2000" startWordPosition="2148" endWordPosition="2151">thm in Figure 2, consists of the following: (a) for each category y all patterns p are sorted in descending order of their scores in the current category, score(p, y), and (b) for each category the top k patterns are selected. For all experiments in this paper we have used k = 3. We provide four different implementations for the pattern scoring function score(p, y) according to four different selection criteria. Criterion 1: Riloff This selection criterion was developed specifically for the pattern acquisition task (Riloff, 1996) and has been used in several other pattern acquisition systems (Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). The intuition behind it is that a qualitative pattern is yielded by a compromise between pattern precision (which is a good indicator of relevance) and pattern frequency (which is a good indicator of coverage). Furthermore, the criterion considers only patterns that are positively correlated with the corresponding category, i.e. their precision is higher than 50%. The Riloff score of a pattern p in a category y is formalized as: prec(p, y) log(count(p, y)), score(p, y) = if prec(p, y) &gt; 0.5; (4) 0, otherwise. prec(p, y) = count(p, y) (5) count</context>
<context position="20698" citStr="Yangarber et al., 2000" startWordPosition="3315" endWordPosition="3318">tivesi R _ (13) �qi= 1(TruePositivesi + FalseNegativesi) where q is the number of categories in the document collection. For all experiments and all collections with the exception of REUTERS, which has a standard document split for training and testing, we used 5- fold cross validation: we randomly partitioned the collections into 5 sets of equal sizes, and reserved a different one for testing in each fold. We have chosen this evaluation strategy because this indirect approach was shown to correlate well with a direct evaluation, where the learned patterns were used to customize an IE system (Yangarber et al., 2000). For this reason, much of the following work on pattern acquisition has used this approach as a de facto evaluation standard (Yangarber, 2003; Stevenson and Greenwood, 2005). Furthermore, given the high number of domains and patterns (we evaluate on 25 domains), an evaluation by human experts is extremely costly. Nevertheless, to show that the proposed indirect evaluation correlates well with a direct evaluation, two human experts have evaluated the patterns in several domains. The direct evaluation procedure is described next. 52 4.2 The Direct Evaluation Procedure The task of manually decid</context>
<context position="26989" citStr="Yangarber et al., 2000" startWordPosition="4338" endWordPosition="4341">n acquisition algorithm for various pattern selection strategies and multiple collections: (a) AP, (b) LATIMES, and (c) REUTERS ment (“co-training”). For both setups we used the best pattern selection criterion for pattern acquisition, i.e. the Collins criterion. To put things in perspective, we also depict the performance obtained with a baseline system, i.e. the system configured to use the Riloff pattern selection criterion and without the NB-EM algorithm (“baseline”). To our knowledge, this system, or a variation of it, is the current state-of-the-art in pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). All algorithms were initialized with the same seed rules and had access to all documents. Figure 4 shows that the quality of the learned patterns always improves if the pattern acquisition algorithm is “reinforced” with EM. For the same recall point, the patterns acquired in the co-training environment yield classification models with precision (generally) much larger than the models generated by the pattern acquisition algorithm alone. When using the same pattern acquisition criterion, e.g. Collins, the differences between the co-training app</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Hutunen, 2000</marker>
<rawString>R. Yangarber, R. Grishman, P. Tapanainen, and S. Hutunen. 2000. Automatic acquisition of domain knowledge for information extraction. In Proceedings of the 18th International Conference of Computational Linguistics (COLING 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
</authors>
<title>Counter-training in discovery of semantic patterns.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="1610" citStr="Yangarber, 2003" startWordPosition="232" endWordPosition="233">ing eventlevel indexing in news stories, feeding link discovery applications, etcetera. By and large the identification and selective extraction of relevant information is built around a set of domain-specific linguistic patterns. For example, for a “financial market change” domain one relevant pattern is &lt;NOUN fall MONEY to MONEY&gt;. When this pattern is matched on the text “London gold fell $4.70 to $308.35”, a change of $4.70 is detected for the financial instrument “London gold”. Domain-specific patterns are either handcrafted or acquired automatically (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff, 1996; Collins a</context>
<context position="3890" citStr="Yangarber, 2003" startWordPosition="566" endWordPosition="567">rn acquisition. We show, using both a direct and an indirect evaluation, that the combination of the two methodologies always improves the quality of the acquired patterns. 48 (2) Which pattern selection strategy is best? While most bootstrapping-based algorithms follow the same framework, they vary significantly in what they consider the most relevant patterns in each bootstrapping iteration. Several approaches have been proposed in the context of word sense disambiguation (Yarowsky, 1995), named entity (NE) classification (Collins and Singer, 1999), pattern acquisition for IE (Riloff, 1996; Yangarber, 2003), or dimensionality reduction for text categorization (TC) (Yang and Pedersen, 1997). However, it is not clear which selection approach is the best for the acquisition of syntactico-semantic patterns. To answer this question, we have implemented a modular pattern acquisition architecture where several of these ranking strategies are implemented and evaluated. The empirical study presented in this paper shows that a strategy previously proposed for feature ranking for NE recognition outperforms algorithms designed specifically for pattern acquisition. The paper is organized as follows: Section </context>
<context position="12156" citStr="Yangarber, 2003" startWordPosition="1910" endWordPosition="1911"> documents labeled with category y, estimated using Laplace smoothing: strength(p, y) = count(p, y) + � (3) count(p) + k� where count(p, y) is the number of documents labeled y containing pattern p, count(p) is the overall number of labeled documents containing p, and k is the number of domains. For all experiments presented here we used c = 1. Another point where acquisition algorithms differ is the initialization procedure: some start with a small number of hand-labeled documents (Riloff, 1996), as illustrated in Figure 2, while others start with a set of seed rules (Yangarber et al., 2000; Yangarber, 2003). However, these approaches are conceptually similar: the seed rules are simply used to generate the seed documents. This paper focuses on the framework introduced in Figure 2 for two reasons: (a) “cautious” al(1) ˆθ) ˆθ)P(d|c; ˆθ) P(d| 50 gorithms were shown to perform best for several NLP problems (including acquisition of IE patterns), and (b) it has nice theoretical properties: Abney (2004) showed that, regardless of the selection procedure, “sequential” bootstrapping algorithms converge to a local minimum of K, where K is an upper bound of the negative log likelihood of the data. Obviousl</context>
<context position="13677" citStr="Yangarber, 2003" startWordPosition="2152" endWordPosition="2153">s of the following: (a) for each category y all patterns p are sorted in descending order of their scores in the current category, score(p, y), and (b) for each category the top k patterns are selected. For all experiments in this paper we have used k = 3. We provide four different implementations for the pattern scoring function score(p, y) according to four different selection criteria. Criterion 1: Riloff This selection criterion was developed specifically for the pattern acquisition task (Riloff, 1996) and has been used in several other pattern acquisition systems (Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). The intuition behind it is that a qualitative pattern is yielded by a compromise between pattern precision (which is a good indicator of relevance) and pattern frequency (which is a good indicator of coverage). Furthermore, the criterion considers only patterns that are positively correlated with the corresponding category, i.e. their precision is higher than 50%. The Riloff score of a pattern p in a category y is formalized as: prec(p, y) log(count(p, y)), score(p, y) = if prec(p, y) &gt; 0.5; (4) 0, otherwise. prec(p, y) = count(p, y) (5) count(p) where prec(p,</context>
<context position="20840" citStr="Yangarber, 2003" startWordPosition="3341" endWordPosition="3343">ollections with the exception of REUTERS, which has a standard document split for training and testing, we used 5- fold cross validation: we randomly partitioned the collections into 5 sets of equal sizes, and reserved a different one for testing in each fold. We have chosen this evaluation strategy because this indirect approach was shown to correlate well with a direct evaluation, where the learned patterns were used to customize an IE system (Yangarber et al., 2000). For this reason, much of the following work on pattern acquisition has used this approach as a de facto evaluation standard (Yangarber, 2003; Stevenson and Greenwood, 2005). Furthermore, given the high number of domains and patterns (we evaluate on 25 domains), an evaluation by human experts is extremely costly. Nevertheless, to show that the proposed indirect evaluation correlates well with a direct evaluation, two human experts have evaluated the patterns in several domains. The direct evaluation procedure is described next. 52 4.2 The Direct Evaluation Procedure The task of manually deciding whether an acquired pattern is relevant or not for a given domain is not trivial, mainly due to the ambiguity of the patterns. Thus, this </context>
<context position="27006" citStr="Yangarber, 2003" startWordPosition="4342" endWordPosition="4344">for various pattern selection strategies and multiple collections: (a) AP, (b) LATIMES, and (c) REUTERS ment (“co-training”). For both setups we used the best pattern selection criterion for pattern acquisition, i.e. the Collins criterion. To put things in perspective, we also depict the performance obtained with a baseline system, i.e. the system configured to use the Riloff pattern selection criterion and without the NB-EM algorithm (“baseline”). To our knowledge, this system, or a variation of it, is the current state-of-the-art in pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). All algorithms were initialized with the same seed rules and had access to all documents. Figure 4 shows that the quality of the learned patterns always improves if the pattern acquisition algorithm is “reinforced” with EM. For the same recall point, the patterns acquired in the co-training environment yield classification models with precision (generally) much larger than the models generated by the pattern acquisition algorithm alone. When using the same pattern acquisition criterion, e.g. Collins, the differences between the co-training approach and the sta</context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>R. Yangarber. 2003. Counter-training in discovery of semantic patterns. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2185" citStr="Yarowsky, 1995" startWordPosition="317" endWordPosition="318">Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff, 1996; Collins and Singer, 1999; Yangarber et al., 2000; Yangarber, 2003; Abney, 2004) several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition. In this paper we answer two of these questions: (1) Can pattern acquisition be improved with text categorization techniques? Bootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (TC), since in each iteration documents containing certain patterns are assigned the corresponding category label. Although TC is obviously not the main goal o</context>
<context position="3769" citStr="Yarowsky, 1995" startWordPosition="550" endWordPosition="551">-training-based algorithm (Blum and Mitchell, 1998) that uses a text categorization algorithm as reinforcement for pattern acquisition. We show, using both a direct and an indirect evaluation, that the combination of the two methodologies always improves the quality of the acquired patterns. 48 (2) Which pattern selection strategy is best? While most bootstrapping-based algorithms follow the same framework, they vary significantly in what they consider the most relevant patterns in each bootstrapping iteration. Several approaches have been proposed in the context of word sense disambiguation (Yarowsky, 1995), named entity (NE) classification (Collins and Singer, 1999), pattern acquisition for IE (Riloff, 1996; Yangarber, 2003), or dimensionality reduction for text categorization (TC) (Yang and Pedersen, 1997). However, it is not clear which selection approach is the best for the acquisition of syntactico-semantic patterns. To answer this question, we have implemented a modular pattern acquisition architecture where several of these ranking strategies are implemented and evaluated. The empirical study presented in this paper shows that a strategy previously proposed for feature ranking for NE reco</context>
<context position="11449" citStr="Yarowsky, 1995" startWordPosition="1784" endWordPosition="1785">tances of this algorithm vary is the select procedure, which decides which rules are acquired in each iteration. Although several selection strategies have been previously proposed for various NLP problems, to our knowledge no existing study performs an empirical analysis of such strategies in the context of acquisition of IE patterns. For this reason, we implement several selection methods in our system (described in Section 2.4) and evaluate their performance in Section 5. The label of each collection document is given by the strength of its patterns. Similarly to (Collins and Singer, 1999; Yarowsky, 1995), we define the strength of a pattern p in a category y as the precision of p in the set of documents labeled with category y, estimated using Laplace smoothing: strength(p, y) = count(p, y) + � (3) count(p) + k� where count(p, y) is the number of documents labeled y containing pattern p, count(p) is the overall number of labeled documents containing p, and k is the number of domains. For all experiments presented here we used c = 1. Another point where acquisition algorithms differ is the initialization procedure: some start with a small number of hand-labeled documents (Riloff, 1996), as ill</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>