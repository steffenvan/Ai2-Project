<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000271">
<title confidence="0.988237">
Detecting Hedge Cues and their Scopes with Average Perceptron
</title>
<author confidence="0.998968">
Feng Ji, Xipeng Qiu, Xuanjing Huang
</author>
<affiliation confidence="0.99368">
Fudan University
</affiliation>
<email confidence="0.994019">
{fengji,xpqiu,xjhuang}@fudan.edu.cn
</email>
<sectionHeader confidence="0.997325" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998604375">
In this paper, we proposed a hedge de-
tection method with average perceptron,
which was used in the closed challenge
in CoNLL-2010 Shared Task. There are
two subtasks: (1) detecting uncertain sen-
tences and (2) identifying the in-sentence
scopes of hedge cues. We use the unified
learning algorithm for both subtasks since
that the hedge score of sentence can be de-
composed into scores of the words, espe-
cially the hedge words. On the biomedical
corpus, our methods achieved F-measure
with 77.86% in detecting in-domain un-
certain sentences, 77.44% in recognizing
hedge cues, and 19.27% in identifying the
scopes.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999847654545454">
Detecting hedged information in biomedical lit-
eratures has received considerable interest in the
biomedical natural language processing (NLP)
community recently. Hedge information indicates
that authors do not or cannot back up their opin-
ions or statements with facts (Szarvas et al., 2008),
which exists in many natural language texts, such
as webpages or blogs, as well as biomedical liter-
atures.
For many NLP applications, such as question
answering and information extraction, the infor-
mation extracted from hedge sentences would be
harmful to their final performances. Therefore,
the hedge or speculative information should be
detected in advance, and dealt with different ap-
proaches or discarded directly.
In CoNLL-2010 Shared Task (Farkas et al.,
2010), there are two different level subtasks: de-
tecting sentences containing uncertainty and iden-
tifying the in-sentence scopes of hedge cues.
For example, in the following sentence:
These results suggest that the IRE motif
in the ALAS mRNA is functional and
imply that translation of the mRNA is
controlled by cellular iron availability
during erythropoiesis.
The words suggest and imply indicate that the
statements are not supported with facts.
In the first subtask, the sentence is considered
as uncertainty.
In the second subtask, suggest and imply are
identified as hedge cues, while the consecutive
blocks suggest that the IRE motif in the ALAS
mRNA is functional and imply that translation of
the mRNA is controlled by cellular iron availabil-
ity during erythropoiesis are recognized as their
corresponding scopes.
In this paper, we proposed a hedge detec-
tion method with average perceptron (Collins,
2002), which was used in the closed challenges in
CoNLL-2010 Shared Task (Farkas et al., 2010).
Our motivation is to use a unified model to de-
tect two level hedge information (word-level and
sentence-level) and the model is easily expanded
to joint learning of two subtasks. Since that the
hedge score of sentence can be decomposed into
scores of the words, especially the hedge words,
we chosen linear classifier in our method and used
average perceptron as the training algorithm.
The rest of the paper is organized as follows. In
Section 2, a brief review of related works is pre-
sented. Then, we describe our method in Section
3. Experiments and results are presented in the
section 4. Finally, the conclusion will be presented
in Section 5.
</bodyText>
<sectionHeader confidence="0.999811" genericHeader="introduction">
2 Related works
</sectionHeader>
<bodyText confidence="0.9997745">
Although the concept of hedge information has
been introduced in linguistic community for a
long time, researches on automatic hedge detec-
tion emerged from machine learning or compu-
</bodyText>
<page confidence="0.991547">
32
</page>
<bodyText confidence="0.972276279069767">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 32–39,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
tational linguistic perspective in recent years. In
this section, we give a brief review on the related
works.
For speculative sentences detection, Medlock
and Briscoe (2007) report their approach based
on weakly supervised learning. In their method,
a statistical model is initially derived from a seed
corpus, and then iteratively modified by augment-
ing the training dataset with unlabeled samples
according the posterior probability. They only
employ bag-of-words features. On the public
biomedical dataset1, their experiments achieve the
performance of 0.76 in BEP (break even point).
Although they also introduced more linguistic fea-
tures, such as part-of-speech (POS), lemma and
bigram (Medlock, 2008), there are no significant
improvements.
In Ganter and Strube (2009), the same task on
Wikipedia is presented. In their system, score of a
sentence is defined as a normalized tangent value
of the sum of scores over all words in the sentence.
Shallow linguistic features are introduced in their
experiments.
Morante and Daelemans (2009) present their re-
search on identifying hedge cues and their scopes.
Their system consists of several classifiers and
works in two phases, first identifying the hedge
cues in a sentence and secondly finding the full
scope for each hedge cue. In the first phase, they
use IGTREE algorithm to train a classifier with
3 categories. In the second phase, three different
classifiers are trained to find the first token and last
token of in-sentence scope and finally combined
into a meta classifier. The experiments shown
that their system achieves an F1 of nearly 0.85
of identifying hedge cues in the abstracts sub cor-
pus, while nearly 0.79 of finding the scopes with
predicted hedge cues. More experiments could
be found in their paper (Morante and Daelemans,
2009). They also provide a detail statistics on
hedge cues in BioScope corpus2.
</bodyText>
<sectionHeader confidence="0.998962" genericHeader="method">
3 Hedge detection with average
perceptron
</sectionHeader>
<subsectionHeader confidence="0.999973">
3.1 Detecting uncertain sentences
</subsectionHeader>
<bodyText confidence="0.9996885">
The first subtask is to identify sentences con-
taining uncertainty information. In particular,
</bodyText>
<footnote confidence="0.99539125">
1http://www.benmedlock.co.uk/
hedgeclassif.html
2http://www.inf.u-szeged.hu/rgai/
bioscope
</footnote>
<bodyText confidence="0.952407">
this subtask is a binary classification problem at
sentence-level.
We define the score of sentence as the confi-
dence that the sentence contains uncertainty infor-
mation.
The score can be decomposed as the sum of the
scores of all words in the sentence,
</bodyText>
<equation confidence="0.993112333333333">
� �
S(x, y) = s(xi, y) =
xiEX xiEX
</equation>
<bodyText confidence="0.998326375">
where, x denotes a sentence and xi is the i-
th word in the sentence x, φ(xi, y) is a sparse
high-dimensional binary feature vector of word xi.
y ∈ {uncertain, certain} is the category of the
sentence. For instance, in the example sentence,
if current word is suggest while the category of
this sentence is uncertain, the following feature is
hired,
</bodyText>
<equation confidence="0.98452375">
�
xi= ‘ ‘suggest”
1, if y=‘‘uncertain”
0, otherwise
</equation>
<bodyText confidence="0.985874">
where n is feature index.
This representation is commonly used in struc-
tured learning algorithms. We can combine the
features into a sparse feature vector Φ(x, y) =
Ei φ(xi, y).
</bodyText>
<equation confidence="0.9466475">
S(x, y) = wTΦ(x, y) = � wTφ(xi, y)
xiEX
</equation>
<bodyText confidence="0.999732">
In the predicting phase, we assign x to the cate-
gory with the highest score,
</bodyText>
<equation confidence="0.9241295">
y∗ = arg max wT Φ(x, y)
y
</equation>
<bodyText confidence="0.9999730625">
We learn the parameters w with online learning
framework. The most common online learner is
the perceptron (Duda et al., 2001). It adjusts pa-
rameters w when a misclassification occurs. Al-
though this framework is very simple, it has been
shown that the algorithm converges in a finite
number of iterations if the data is linearly separa-
ble. Moreover, much less training time is required
in practice than the batch learning methods, such
as support vector machine (SVM) or conditional
maximum entropy (CME).
Here we employ a variant perceptron algorithm
to train the model, which is commonly named
average perceptron since it averages parameters
w across iterations. This algorithm is first pro-
posed in Collins (2002). Many experiments of
</bodyText>
<equation confidence="0.9990515">
wTφ(xi, y)
φn(xi, y) =
</equation>
<page confidence="0.970795">
33
</page>
<bodyText confidence="0.999928461538462">
NLP problems demonstrate better generalization
performance than non averaged parameters. More
theoretical proofs can be found in Collins (2002).
Different from the standard average perceptron al-
gorithm, we slightly modify the average strategy.
The reason to this modification is that the origi-
nal algorithm is slow since parameters accumulate
across all iterations. In order to keep fast training
speed and avoid overfitting at the same time, we
make a slight change of the parameters accumu-
lation strategy, which occurs only after each iter-
ation over the training data finished. Our training
algorithm is shown in Algorithm 1.
</bodyText>
<equation confidence="0.909845">
input : training data set:
(xn, yn), n = 1, ··· , N,
</equation>
<bodyText confidence="0.652390666666667">
parameters: average number: K,
maximum iteration number: T.
output: average weight: cw
</bodyText>
<equation confidence="0.841759428571428">
Initialize: cw ← 0,;
for k = 0···K − 1 do
w0 ← 0 ;
fort = 0···T − 1 do
receive an example (xt, yt);
predict: ˆyt = arg maxy wTt Φ(xt, y) ;
if ˆyt =6 yt then
wt+, = wt+Φ(xt, yt)−Φ(xt, ˆyt)
end
end
cw = cw + wT ;
end
cw = cw/K ;
Algorithm 1: Average Perceptron algorithm
</equation>
<bodyText confidence="0.999319375">
Binary context features are extracted from 6
predefined patterns, which are shown in Figure 1.
By using these patterns, we can easily obtain the
complicate features. As in the previous example,
if the current word is suggest, then a new com-
pound feature could be extracted in the form of
w−, =results//w0 =suggest by employing the pat-
tern w−,w0. // is the separate symbol.
</bodyText>
<subsectionHeader confidence="0.999834">
3.2 Identifying hedge cues and their scopes
</subsectionHeader>
<bodyText confidence="0.9998975">
Our approach for the second subtask consists of
two phases: (1) identifying hedge cues in a sen-
tence, then (2) recognizing their corresponding
scopes.
</bodyText>
<subsectionHeader confidence="0.894731">
3.2.1 Identifying hedge cues
</subsectionHeader>
<bodyText confidence="0.97078">
Hedge cues are the most important clues for de-
termining whether a sentence contains uncertain
</bodyText>
<listItem confidence="0.999666333333333">
• unigram: w0,p0
• bigram: w0w,, w0p0, p0p,
• trigram: w−,w0w,
</listItem>
<figureCaption confidence="0.61553">
Figure 1: Patterns employed in the sentence-level
</figureCaption>
<bodyText confidence="0.612542666666667">
hedge detection. Here w denotes single word, p is
part of speech, and the subscript denotes the rela-
tive offset compared with current position.
</bodyText>
<listItem confidence="0.999114666666667">
• unigram: w−2, w−,, w0, w,, w2, p0
• bigram: w−,w0, w0w,, w0p0, p−,p0, p0p,
• trigram: w−,w0w,
</listItem>
<figureCaption confidence="0.9624935">
Figure 2: Patterns employed in the word-level
hedge detection.
</figureCaption>
<bodyText confidence="0.99991">
information. Therefore in this phase, we treat the
problem of identifying hedge cues as a classifica-
tion problem. Each word in a sentence would be
predicted a category indicating whether this word
is a hedge cue word or not. In the previous ex-
ample, there are two different hedge cues in the
sentence (show in bold manner). Words suggest
and imply are assigned with the category CUE de-
noting hedge cue word, while other words are as-
signed with label O denoting non hedge cue word.
In our system, this module is much similar to
the module of detecting uncertain sentences. The
only difference is that this phase is word level. So
that each training sample in this phase is a word,
while in detecting speculative sentences training
sample is a sentence. The training algorithm is the
same as the algorithm shown in Algorithm 1. 12
predefined patterns of context features are shown
in Figure 2.
</bodyText>
<subsectionHeader confidence="0.919415">
3.2.2 Recognizing in-sentence scopes
</subsectionHeader>
<bodyText confidence="0.999246444444445">
After identifying the hedge cues in the first phase,
we need to recognize their corresponding in-
sentence scopes, which means the boundary of
scope should be found within the same sentence.
We consider this problem as a word-cue pair
classification problem, where word is any word
in a sentence and cue is the identified hedge cue
word. Similar to the previous phase, a word-level
linear classifier is trained to predict whether each
</bodyText>
<page confidence="0.995495">
34
</page>
<bodyText confidence="0.99979745">
word-cue pair in a sentence is in the scope of the
hedge cue.
Besides base context features used in the pre-
vious phase, we introduce additional syntactic de-
pendency features. These features are generated
by a first-order projective dependency parser (Mc-
Donald et al., 2005), and listed in Figure 3.
The scopes of hedge cues are always covering
a consecutive block of words including the hedge
cue itself. The ideal method should recognize only
one consecutive block for each hedge cue. How-
ever, our classifier cannot work so well. Therefore,
we apply a simple strategy to process the output
of the classifier. The simple strategy is to find a
maximum consecutive sequence which covers the
hedge cue. If a sentence is considered to contain
several hedge cues, we simply combine the con-
secutive sequences, which have at least one com-
mon word, to a large block and assign it to the
relative hedge cues.
</bodyText>
<sectionHeader confidence="0.99981" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.996311833333333">
In this section, we report our experiments on
datasets of CoNLL-2010 shared tasks, including
the official results and our experimental results
when developing the system.
Our system architecture is shown in Figure 4,
which consists of the following modules.
</bodyText>
<listItem confidence="0.949041333333333">
1. corpus preprocess module, which employs a
tokenizer to normalize the corpus;
2. sentence detection module, which uses a bi-
nary sentence-level classifier to determine
whether a sentence contains uncertainty in-
formation;
3. hedge cues detection module, which identi-
fies which words in a sentence are the hedge
cues, we train a binary word-level classifier;
4. cue scope recognition module, which recog-
nizes the corresponding scope for each hedge
cue by another word-level classifier.
</listItem>
<bodyText confidence="0.9974118">
Our experimental results are obtained on the
training datasets by 10-fold cross validation. The
maximum iteration number for training the aver-
age perceptron is set to 20. Our system is imple-
mented with Java3.
</bodyText>
<footnote confidence="0.927724">
3http://code.google.com/p/fudannlp/
</footnote>
<table confidence="0.999559777777778">
biomedical Wikipedia
#sentences 14541 11111
#words 382274 247328
#hedge sentences 2620 2484
%hedge sentences 0.18 0.22
#hedge cues 3378 3133
average number 1.29 1.26
average cue length 1.14 2.45
av. scope length 15.42 -
</table>
<tableCaption confidence="0.976003">
Table 1: Statistical information on annotated cor-
pus.
</tableCaption>
<subsectionHeader confidence="0.944507">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999922666666667">
In CoNLL-2010 Shared Task, two different
datasets are provided to develop the system: (1)
biological abstracts and full articles from the Bio-
Scope corpus, (2) paragraphs from Wikipedia. Be-
sides manually annotated datasets, three corre-
sponding unlabeled datasets are also allowed for
the closed challenges. But we have not employed
any unlabeled datasets in our system.
A preliminary statistics can be found in Ta-
ble 1. We make no distinction between sen-
tences from abstracts or full articles in biomedi-
cal dataset. From Table 1, most sentences are cer-
tainty while about 18% sentences in biomedical
dataset and 22% in Wikipedia dataset are spec-
ulative. On the average, there exists nearly 1.29
hedge cues per sentence in biomedical dataset and
1.26 in Wikipedia. The average length of hedge
cues varies in these two corpus. In biomedical
dataset, hedge cues are nearly one word, but more
than two words in Wikipedia. On average, the
scope of hedge cue covers 15.42 words.
</bodyText>
<subsectionHeader confidence="0.995348">
4.2 Corpus preprocess
</subsectionHeader>
<bodyText confidence="0.999981555555556">
The sentence are processed with a maximum-
entropy part-of-speech tagger4 (Toutanova et al.,
2003), in which a rule-based tokenzier is used to
separate punctuations or other symbols from reg-
ular words. Moreover, we train a first-order pro-
jective dependency parser with MSTParser5 (Mc-
Donald et al., 2005) on the standard WSJ training
corpus, which is converted from constituent trees
to dependency trees by several heuristic rules6.
</bodyText>
<footnote confidence="0.9982285">
4http://nlp.stanford.edu/software/
tagger.shtml
5http://www.seas.upenn.edu/-strctlrn/
MSTParser/MSTParser.html
6http://w3.msi.vxu.se/-nivre/research/
Penn2Malt.html
</footnote>
<page confidence="0.998627">
35
</page>
<listItem confidence="0.998165058823529">
• word-cue pair: current word and the hedge cue word pair,
• word-cue POS pair: POS pair of current word and the hedge cue word,
• path of POS: path of POS from current word to the hedge cue word along dependency
tree,
• path of dependency: relation path of dependency from current word to the hedge cue
word along dependency tree,
• POS of hedge cue word+direction: POS of hedge cue word with the direction to the
current word. Here direction can be “LEFT” if the hedge cue is on the left to the current
word, or “RIGHT” on the right,
• tree depth: depth of current in the corresponding dependency tree,
• surface distance: surface distance between current word and the hedge cue word. The
value of this feature is always 10 in the case of surface distance greater than 10,
• surface distance+tree depth: combination of surface distance and tree depth
• number of punctuations: number of punctuations between current word and the hedge
cue word,
• number of punctuations + tree depth: combination of number of punctuations and tree
depth
</listItem>
<figureCaption confidence="0.890658">
Figure 3: Additional features used in recognizing in-sentence scope
</figureCaption>
<subsectionHeader confidence="0.990085">
4.3 Uncertain sentences detection
</subsectionHeader>
<bodyText confidence="0.999752909090909">
In the first subtask, we carry out the experiments
within domain and cross domains. As previously
mentioned, we do not use the unlabeled datasets
and make no distinction between abstracts and full
articles in biomedical dataset. This means we
train the models only with the official annotated
datasets. The model for cross-domain is trained
on the combination of annotated biomedical and
Wikipedia datasets.
In this subtask, evaluation is carried out on the
sentence level and F-measure of uncertainty sen-
tences is employed as the chief metric.
Table 2 shows the results within domain. Af-
ter 10-fold cross validation over training dataset,
we achieve 84.39% of F1-measure on biomedical
while 56.06% on Wikipedia.
We analyzed the low performance of our sub-
mission result on Wikipedia. The possible rea-
son is our careless work when dealing with the
trained model file. Therefore we retrain a model
for Wikipedia and the performance is listed on the
bottom line (Wikipedia*) in Table 2.
</bodyText>
<table confidence="0.999367875">
Dataset Precision Recall F1
10-fold cross validation
biomedical 91.03 78.66 84.39
Wikipedia 66.54 48.43 56.06
official evaluation
biomedical 79.45 76.33 77.86
Wikipedia 94.23 6.58 1.23
Wikipedia* 82.19 32.86 46.95
</table>
<tableCaption confidence="0.85555">
Table 2: Results for in-domain uncertain sentences
detection
</tableCaption>
<bodyText confidence="0.9848238">
Table 3 shows the results across domains. We
split each annotated dataset into 10 folds. Then
training dataset is combined by individually draw-
ing 9 folds out from the split datasets and the
rests are used as the test data. On biomedical
dataset, F1-measure gets to 79.24% while 56.16%
on Wikipedia dataset. Compared with the results
within domain, over 5% performance decreases
from 84.39% to 79.24% on biomedical, but a
slightly increase on Wikipedia.
</bodyText>
<page confidence="0.997846">
36
</page>
<figureCaption confidence="0.997218">
Figure 4: System architecture of our system
</figureCaption>
<table confidence="0.999761285714286">
Dataset Precision Recall F1
10-fold cross validation
biomedical 87.86 72.16 79.24
Wikipedia 67.78 47.95 56.16
official evaluation
biomedical 62.81 79.11 70.03
Wikipedia 62.66 55.28 58.74
</table>
<tableCaption confidence="0.9678275">
Table 3: Results for across-domain uncertain sen-
tences detection
</tableCaption>
<table confidence="0.999891666666667">
Dataset Precision Recall F1
10-fold cross validation(word-level)
biomedical 90.15 84.43 87.19
Wikipedia 57.74 39.81 47.13
official evaluation(phrase-level)
biomedical 78.7 76.22 77.44
</table>
<tableCaption confidence="0.9123095">
Table 6: Results for in-domain hedge cue identifi-
cation
</tableCaption>
<subsectionHeader confidence="0.912398">
4.3.1 Results analysis
</subsectionHeader>
<bodyText confidence="0.999976333333333">
We investigate the weights of internal features and
found that the words, which have no uncertainty
information, also play the significant roles to pre-
dict the uncertainty of the sentence.
Intuitively, the words without uncertainty infor-
mation should just have negligible effect and the
corresponding features should have low weights.
However, this ideal case is difficult to reached by
learning algorithm due to the sparsity of data.
In Table 4, we list the top 10 words involved
in features with the largest weights for each cate-
gory. These words are ranked by the accumulative
scores of their related features.
In Table 5, we list the top 10 POS involved in
features with the largest weight for each category.
</bodyText>
<subsectionHeader confidence="0.998141">
4.4 Hedge cue identification
</subsectionHeader>
<bodyText confidence="0.999968333333333">
Hedge cues identification is one module for the
second subtask, we also analyze the performance
on this module.
Since we treat this problem as a binary classi-
fication problem, we evaluate F-measure of hedge
cue words. The results are listed in Table 6.
We have to point out that our evaluation is
based on word while official evaluation is based
on phrase. That means our results would seem
to be higher than the official results, especially on
Wikipedia dataset because average length of hedge
cues in Wikipedia dataset is more than 2 words.
</bodyText>
<subsectionHeader confidence="0.843653">
4.4.1 Result Analysis
</subsectionHeader>
<bodyText confidence="0.99997425">
We classify the results into four categories: false
negative, false positive, true positive and true neg-
ative. We found that most mistakes are made be-
cause of polysemy and collocation.
In Table 7, we list top 10 words for each cate-
gory. For the false results, the words are difficult to
distinguish without its context in the correspond-
ing sentence.
</bodyText>
<subsectionHeader confidence="0.981664">
4.5 Scopes recognition
</subsectionHeader>
<bodyText confidence="0.999658142857143">
For recognizing the in-sentence scopes, F-measure
is also used to evaluate the performance of the
word-cue pair classifier. The results using gold
hedge cues are shown in Table 8. From the re-
sults, F-measure achieves respectively 70.44% and
75.94% when individually using the base context
features extracted by 12 predefined patterns (see
</bodyText>
<figureCaption confidence="0.890073">
Figure 1) and syntactic dependency features (see
Figure 3), while 79.55% when using all features.
</figureCaption>
<bodyText confidence="0.955537">
The results imply that syntactic dependency
</bodyText>
<page confidence="0.997892">
37
</page>
<bodyText confidence="0.938377416666667">
biomedical Wikipedia cross domain
uncertain certain uncertain certain uncertain certain
whether show probably the other suggest show
may demonstrate some often whether used to
suggest will many patients probably was
likely can one of another indicate CFS
indicate role believed days appear demonstrate
possible found possibly CFS putative the other
putative human considered are some of all
appear known to such as any other thought ‘:’
thought report several western possibly people
potential evidence said to pop likely could not
</bodyText>
<tableCaption confidence="0.998274">
Table 4: Top 10 significant words in detecting uncertain sentences
</tableCaption>
<table confidence="0.999708571428572">
biomedical Wikipedia cross domain
uncertain certain uncertain certain uncertain certain
MD SYM RB VBZ JJS SYM
VBG PRP JJS CD RBS ‘:’
VB NN RBS ‘:’ RB JJR
VBZ CD FW WRB EX WDT
IN WDT VBP PRP CC CD
</table>
<tableCaption confidence="0.995223">
Table 5: Top 5 significant POS in detecting uncertain sentences
</tableCaption>
<table confidence="0.999831714285714">
Dataset Precision Recall F1
base context features
biomedical 66.04 75.48 70.44
syntactic dependency features
biomedical 93.77 63.05 75.94
all features
biomedical 78.72 80.41 79.55
</table>
<tableCaption confidence="0.895661">
Table 8: Results for scopes recognizing with gold
hedge cues (word-level)
features contribute more benefits to recognize
scopes than surface context features.
Official results evaluated at block level are also
listed in Table 9.
</tableCaption>
<table confidence="0.9096105">
dataset Precision Recall F1
biomedical 21.87 17.23 19.27
</table>
<tableCaption confidence="0.783567">
Table 9: Official results for scopes recognizing
(block level)
</tableCaption>
<bodyText confidence="0.999893545454546">
From Table 9 and the official result on hedge
cue identification in Table 6, we believe that our
post-processing strategy would be responsible for
the low performance on recognizing scopes. Our
strategy is to find a maximum consecutive block
covering the corresponding hedge cue. This strat-
egy cannot do well with the complex scope struc-
ture. For example, a scope is covered by another
scope. Therefore, our system would generate a
block covering all hedge cues if there exists more
than one hedge cues in a sentence.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991">
We present our implemented system for CoNLL-
2010 Shared Task in this paper. We introduce
syntactic dependency features when recognizing
hedge scopes and employ average perceptron al-
gorithm to train the models. On the biomedi-
cal corpus, our system achieves F-measure with
77.86% in detecting uncertain sentences, 77.44%
in recognizing hedge cues, and 19.27% in identi-
fying the scopes.
Although some results are low and beyond our
expectations, we believe that our system can be at
least improved within the following fields. Firstly,
we would experiment on other kinds of features,
such as chunk or named entities in biomedical.
Secondly, the unlabeled datasets would be ex-
plored in the future.
</bodyText>
<page confidence="0.998355">
38
</page>
<table confidence="0.872736909090909">
False Negative False Positive True Positive True Negative
support considered suggesting chemiluminescence
of potential may rhinitis
demonstrate or proposal leukemogenic
a hope might ribosomal
postulate indicates indicating bp
supports expected likely nc82
good can appear intronic/exonic
advocates should possible large
implicated either speculate allele
putative idea whether end
</table>
<tableCaption confidence="0.994096">
Table 7: Top 10 words with the largest scores for each category in hedge cue identification
</tableCaption>
<sectionHeader confidence="0.999466" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.897926">
This work was funded by Chinese NSF (Grant
No. 60673038), 973 Program (Grant No.
2010CB327906, and Shanghai Committee of Sci-
ence and Technology(Grant No. 09511501404).
</bodyText>
<sectionHeader confidence="0.994397" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996243148148148">
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing, pages 1–8. Associa-
tion for Computational Linguistics.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2001. Pattern classification. Wiley, New York.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1–12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173–176. Association for Com-
putational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the ACL, pages
91–98. Association for Computational Linguistics.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992–999. Association for Computational Lin-
guistics.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41(4):636–654.
Roser Morante and Walter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the Workshop on BioNLP, pages 28–36.
Association for Computational Linguistics.
Gy¨orgy Szarvas, Veronika Vincze, Rich´ard Farkas, and
J´anos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of the Workshop
on Current Trends in Biomedical Natural Language
Processing, pages 38–45. Association for Computa-
tional Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.999531">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.838369">
<title confidence="0.999373">Detecting Hedge Cues and their Scopes with Average Perceptron</title>
<author confidence="0.855251">Feng Ji</author>
<author confidence="0.855251">Xipeng Qiu</author>
<author confidence="0.855251">Xuanjing</author>
<affiliation confidence="0.995409">Fudan University</affiliation>
<abstract confidence="0.998882529411765">In this paper, we proposed a hedge detection method with average perceptron, which was used in the closed challenge in CoNLL-2010 Shared Task. There are two subtasks: (1) detecting uncertain sentences and (2) identifying the in-sentence scopes of hedge cues. We use the unified learning algorithm for both subtasks since that the hedge score of sentence can be decomposed into scores of the words, especially the hedge words. On the biomedical corpus, our methods achieved F-measure with 77.86% in detecting in-domain uncertain sentences, 77.44% in recognizing hedge cues, and 19.27% in identifying the scopes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2454" citStr="Collins, 2002" startWordPosition="373" endWordPosition="374">ntrolled by cellular iron availability during erythropoiesis. The words suggest and imply indicate that the statements are not supported with facts. In the first subtask, the sentence is considered as uncertainty. In the second subtask, suggest and imply are identified as hedge cues, while the consecutive blocks suggest that the IRE motif in the ALAS mRNA is functional and imply that translation of the mRNA is controlled by cellular iron availability during erythropoiesis are recognized as their corresponding scopes. In this paper, we proposed a hedge detection method with average perceptron (Collins, 2002), which was used in the closed challenges in CoNLL-2010 Shared Task (Farkas et al., 2010). Our motivation is to use a unified model to detect two level hedge information (word-level and sentence-level) and the model is easily expanded to joint learning of two subtasks. Since that the hedge score of sentence can be decomposed into scores of the words, especially the hedge words, we chosen linear classifier in our method and used average perceptron as the training algorithm. The rest of the paper is organized as follows. In Section 2, a brief review of related works is presented. Then, we descri</context>
<context position="7451" citStr="Collins (2002)" startWordPosition="1182" endWordPosition="1183">ptron (Duda et al., 2001). It adjusts parameters w when a misclassification occurs. Although this framework is very simple, it has been shown that the algorithm converges in a finite number of iterations if the data is linearly separable. Moreover, much less training time is required in practice than the batch learning methods, such as support vector machine (SVM) or conditional maximum entropy (CME). Here we employ a variant perceptron algorithm to train the model, which is commonly named average perceptron since it averages parameters w across iterations. This algorithm is first proposed in Collins (2002). Many experiments of wTφ(xi, y) φn(xi, y) = 33 NLP problems demonstrate better generalization performance than non averaged parameters. More theoretical proofs can be found in Collins (2002). Different from the standard average perceptron algorithm, we slightly modify the average strategy. The reason to this modification is that the original algorithm is slow since parameters accumulate across all iterations. In order to keep fast training speed and avoid overfitting at the same time, we make a slight change of the parameters accumulation strategy, which occurs only after each iteration over </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard O Duda</author>
<author>Peter E Hart</author>
<author>David G Stork</author>
</authors>
<title>Pattern classification.</title>
<date>2001</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="6862" citStr="Duda et al., 2001" startWordPosition="1085" endWordPosition="1088">sentence, if current word is suggest while the category of this sentence is uncertain, the following feature is hired, � xi= ‘ ‘suggest” 1, if y=‘‘uncertain” 0, otherwise where n is feature index. This representation is commonly used in structured learning algorithms. We can combine the features into a sparse feature vector Φ(x, y) = Ei φ(xi, y). S(x, y) = wTΦ(x, y) = � wTφ(xi, y) xiEX In the predicting phase, we assign x to the category with the highest score, y∗ = arg max wT Φ(x, y) y We learn the parameters w with online learning framework. The most common online learner is the perceptron (Duda et al., 2001). It adjusts parameters w when a misclassification occurs. Although this framework is very simple, it has been shown that the algorithm converges in a finite number of iterations if the data is linearly separable. Moreover, much less training time is required in practice than the batch learning methods, such as support vector machine (SVM) or conditional maximum entropy (CME). Here we employ a variant perceptron algorithm to train the model, which is commonly named average perceptron since it averages parameters w across iterations. This algorithm is first proposed in Collins (2002). Many expe</context>
</contexts>
<marker>Duda, Hart, Stork, 2001</marker>
<rawString>Richard O. Duda, Peter E. Hart, and David G. Stork. 2001. Pattern classification. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 1–12, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viola Ganter</author>
<author>Michael Strube</author>
</authors>
<title>Finding hedges by chasing weasels: hedge detection using Wikipedia tags and shallow linguistic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>173--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4363" citStr="Ganter and Strube (2009)" startWordPosition="666" endWordPosition="669">lock and Briscoe (2007) report their approach based on weakly supervised learning. In their method, a statistical model is initially derived from a seed corpus, and then iteratively modified by augmenting the training dataset with unlabeled samples according the posterior probability. They only employ bag-of-words features. On the public biomedical dataset1, their experiments achieve the performance of 0.76 in BEP (break even point). Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008), there are no significant improvements. In Ganter and Strube (2009), the same task on Wikipedia is presented. In their system, score of a sentence is defined as a normalized tangent value of the sum of scores over all words in the sentence. Shallow linguistic features are introduced in their experiments. Morante and Daelemans (2009) present their research on identifying hedge cues and their scopes. Their system consists of several classifiers and works in two phases, first identifying the hedge cues in a sentence and secondly finding the full scope for each hedge cue. In the first phase, they use IGTREE algorithm to train a classifier with 3 categories. In th</context>
</contexts>
<marker>Ganter, Strube, 2009</marker>
<rawString>Viola Ganter and Michael Strube. 2009. Finding hedges by chasing weasels: hedge detection using Wikipedia tags and shallow linguistic features. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173–176. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11281" citStr="McDonald et al., 2005" startWordPosition="1834" endWordPosition="1838">onding insentence scopes, which means the boundary of scope should be found within the same sentence. We consider this problem as a word-cue pair classification problem, where word is any word in a sentence and cue is the identified hedge cue word. Similar to the previous phase, a word-level linear classifier is trained to predict whether each 34 word-cue pair in a sentence is in the scope of the hedge cue. Besides base context features used in the previous phase, we introduce additional syntactic dependency features. These features are generated by a first-order projective dependency parser (McDonald et al., 2005), and listed in Figure 3. The scopes of hedge cues are always covering a consecutive block of words including the hedge cue itself. The ideal method should recognize only one consecutive block for each hedge cue. However, our classifier cannot work so well. Therefore, we apply a simple strategy to process the output of the classifier. The simple strategy is to find a maximum consecutive sequence which covers the hedge cue. If a sentence is considered to contain several hedge cues, we simply combine the consecutive sequences, which have at least one common word, to a large block and assign it t</context>
<context position="14491" citStr="McDonald et al., 2005" startWordPosition="2347" endWordPosition="2351">e exists nearly 1.29 hedge cues per sentence in biomedical dataset and 1.26 in Wikipedia. The average length of hedge cues varies in these two corpus. In biomedical dataset, hedge cues are nearly one word, but more than two words in Wikipedia. On average, the scope of hedge cue covers 15.42 words. 4.2 Corpus preprocess The sentence are processed with a maximumentropy part-of-speech tagger4 (Toutanova et al., 2003), in which a rule-based tokenzier is used to separate punctuations or other symbols from regular words. Moreover, we train a first-order projective dependency parser with MSTParser5 (McDonald et al., 2005) on the standard WSJ training corpus, which is converted from constituent trees to dependency trees by several heuristic rules6. 4http://nlp.stanford.edu/software/ tagger.shtml 5http://www.seas.upenn.edu/-strctlrn/ MSTParser/MSTParser.html 6http://w3.msi.vxu.se/-nivre/research/ Penn2Malt.html 35 • word-cue pair: current word and the hedge cue word pair, • word-cue POS pair: POS pair of current word and the hedge cue word, • path of POS: path of POS from current word to the hedge cue word along dependency tree, • path of dependency: relation path of dependency from current word to the hedge cue</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the ACL, pages 91–98. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>992--999</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3762" citStr="Medlock and Briscoe (2007)" startWordPosition="579" endWordPosition="582">. Finally, the conclusion will be presented in Section 5. 2 Related works Although the concept of hedge information has been introduced in linguistic community for a long time, researches on automatic hedge detection emerged from machine learning or compu32 Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 32–39, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics tational linguistic perspective in recent years. In this section, we give a brief review on the related works. For speculative sentences detection, Medlock and Briscoe (2007) report their approach based on weakly supervised learning. In their method, a statistical model is initially derived from a seed corpus, and then iteratively modified by augmenting the training dataset with unlabeled samples according the posterior probability. They only employ bag-of-words features. On the public biomedical dataset1, their experiments achieve the performance of 0.76 in BEP (break even point). Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008), there are no significant improvements. In Ganter and Strube (2009</context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock and Ted Briscoe. 2007. Weakly supervised learning for hedge classification in scientific literature. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
</authors>
<title>Exploring hedge identification in biomedical literature.</title>
<date>2008</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>41</volume>
<issue>4</issue>
<contexts>
<context position="4295" citStr="Medlock, 2008" startWordPosition="658" endWordPosition="659">he related works. For speculative sentences detection, Medlock and Briscoe (2007) report their approach based on weakly supervised learning. In their method, a statistical model is initially derived from a seed corpus, and then iteratively modified by augmenting the training dataset with unlabeled samples according the posterior probability. They only employ bag-of-words features. On the public biomedical dataset1, their experiments achieve the performance of 0.76 in BEP (break even point). Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008), there are no significant improvements. In Ganter and Strube (2009), the same task on Wikipedia is presented. In their system, score of a sentence is defined as a normalized tangent value of the sum of scores over all words in the sentence. Shallow linguistic features are introduced in their experiments. Morante and Daelemans (2009) present their research on identifying hedge cues and their scopes. Their system consists of several classifiers and works in two phases, first identifying the hedge cues in a sentence and secondly finding the full scope for each hedge cue. In the first phase, they</context>
</contexts>
<marker>Medlock, 2008</marker>
<rawString>Ben Medlock. 2008. Exploring hedge identification in biomedical literature. Journal of Biomedical Informatics, 41(4):636–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the scope of hedge cues in biomedical texts.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on BioNLP,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4630" citStr="Morante and Daelemans (2009)" startWordPosition="710" endWordPosition="713">posterior probability. They only employ bag-of-words features. On the public biomedical dataset1, their experiments achieve the performance of 0.76 in BEP (break even point). Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008), there are no significant improvements. In Ganter and Strube (2009), the same task on Wikipedia is presented. In their system, score of a sentence is defined as a normalized tangent value of the sum of scores over all words in the sentence. Shallow linguistic features are introduced in their experiments. Morante and Daelemans (2009) present their research on identifying hedge cues and their scopes. Their system consists of several classifiers and works in two phases, first identifying the hedge cues in a sentence and secondly finding the full scope for each hedge cue. In the first phase, they use IGTREE algorithm to train a classifier with 3 categories. In the second phase, three different classifiers are trained to find the first token and last token of in-sentence scope and finally combined into a meta classifier. The experiments shown that their system achieves an F1 of nearly 0.85 of identifying hedge cues in the abs</context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. Learning the scope of hedge cues in biomedical texts. In Proceedings of the Workshop on BioNLP, pages 28–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
<author>Veronika Vincze</author>
<author>Rich´ard Farkas</author>
<author>J´anos Csirik</author>
</authors>
<title>The BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,</booktitle>
<pages>38--45</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1076" citStr="Szarvas et al., 2008" startWordPosition="158" endWordPosition="161">m for both subtasks since that the hedge score of sentence can be decomposed into scores of the words, especially the hedge words. On the biomedical corpus, our methods achieved F-measure with 77.86% in detecting in-domain uncertain sentences, 77.44% in recognizing hedge cues, and 19.27% in identifying the scopes. 1 Introduction Detecting hedged information in biomedical literatures has received considerable interest in the biomedical natural language processing (NLP) community recently. Hedge information indicates that authors do not or cannot back up their opinions or statements with facts (Szarvas et al., 2008), which exists in many natural language texts, such as webpages or blogs, as well as biomedical literatures. For many NLP applications, such as question answering and information extraction, the information extracted from hedge sentences would be harmful to their final performances. Therefore, the hedge or speculative information should be detected in advance, and dealt with different approaches or discarded directly. In CoNLL-2010 Shared Task (Farkas et al., 2010), there are two different level subtasks: detecting sentences containing uncertainty and identifying the in-sentence scopes of hedg</context>
</contexts>
<marker>Szarvas, Vincze, Farkas, Csirik, 2008</marker>
<rawString>Gy¨orgy Szarvas, Veronika Vincze, Rich´ard Farkas, and J´anos Csirik. 2008. The BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts. In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 38–45. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14286" citStr="Toutanova et al., 2003" startWordPosition="2315" endWordPosition="2318">bstracts or full articles in biomedical dataset. From Table 1, most sentences are certainty while about 18% sentences in biomedical dataset and 22% in Wikipedia dataset are speculative. On the average, there exists nearly 1.29 hedge cues per sentence in biomedical dataset and 1.26 in Wikipedia. The average length of hedge cues varies in these two corpus. In biomedical dataset, hedge cues are nearly one word, but more than two words in Wikipedia. On average, the scope of hedge cue covers 15.42 words. 4.2 Corpus preprocess The sentence are processed with a maximumentropy part-of-speech tagger4 (Toutanova et al., 2003), in which a rule-based tokenzier is used to separate punctuations or other symbols from regular words. Moreover, we train a first-order projective dependency parser with MSTParser5 (McDonald et al., 2005) on the standard WSJ training corpus, which is converted from constituent trees to dependency trees by several heuristic rules6. 4http://nlp.stanford.edu/software/ tagger.shtml 5http://www.seas.upenn.edu/-strctlrn/ MSTParser/MSTParser.html 6http://w3.msi.vxu.se/-nivre/research/ Penn2Malt.html 35 • word-cue pair: current word and the hedge cue word pair, • word-cue POS pair: POS pair of curren</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>