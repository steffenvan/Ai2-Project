<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006006">
<title confidence="0.971962">
The Cunei Machine Translation Platform for WMT ’10
</title>
<author confidence="0.769051">
Aaron B. Phillips
</author>
<affiliation confidence="0.784734">
Carnegie Mellon
Pittsburgh, USA.
</affiliation>
<email confidence="0.979708">
aphillips@cmu.edu
</email>
<sectionHeader confidence="0.993332" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993835">
This paper describes the Cunei Machine
Translation Platform and how it was used
in the WMT ’10 German to English and
Czech to English translation tasks.
</bodyText>
<sectionHeader confidence="0.9836575" genericHeader="keywords">
1 The Cunei Machine Translation
Platform
</sectionHeader>
<bodyText confidence="0.999887">
The Cunei Machine Translation Platform (Phillips
and Brown, 2009) is open-source software
and freely available at http://www.cunei.
org/. Like Moses (Koehn et al., 2007) and
Joshua (Li et al., 2009), Cunei provides a statisti-
cal decoder that combines partial translations (ei-
ther phase pairs or grammar rules) in order to com-
pose a coherent sentence in the target language.
What makes Cunei unique is that it models the
translation task with a non-parametric model that
assesses the relevance of each translation instance.
The process begins by encoding in a lattice all
possible contiguous phrases from the input.1 For
each source phrase in the lattice, Cunei locates in-
stances of it in the corpus and then identifies the
aligned target phrase(s). This much is standard to
most data-driven MT systems. The typical step at
this stage is to model a phrase pair by computing
relative frequencies over the collection of transla-
tion instances. This model for the phrase pair will
never change and knowledge of the translation in-
stances can subsequently be discarded. In contrast
to using a phrase pair as the basic unit of modeling,
Cunei models each translation instance. A dis-
tance function, represented by a log-linear model,
scores the relevance of each translation instance.
Our model then sums the scores of translation in-
stances that predict the same target hypothesis.
The advantage of this approach is that it pro-
vides a flexible framework for novel sources of
</bodyText>
<footnote confidence="0.892322666666667">
1Cunei offers limited support for non-contiguous phrases,
similar in concept to grammar rules, but this setting was dis-
abled in our experiments.
</footnote>
<bodyText confidence="0.999973944444445">
information. The non-parametric model still uses
information gleaned over all translation instances,
but it permits us to define a distance function that
operates over one translation instance at a time.
This enables us to score a wide-variety of informa-
tion represented by the translation instance with
respect to the input and the target hypothesis un-
der consideration. For example, we could compute
how similar one translation instance’s parse tree or
morpho-syntactic information is to the input. Fur-
thermore, this information will vary throughout
the corpus with some translation instances exhibit-
ing higher similarity to the input. Our approach
captures that these instances are more relevant and
they will have a larger effect on the model. For
the WMT ’10 task, we exploited instance-specific
context and alignment features which will be dis-
cussed in more detail below.
</bodyText>
<subsectionHeader confidence="0.911977">
1.1 Formalism
</subsectionHeader>
<bodyText confidence="0.999846619047619">
Cunei’s model is a hybrid between the approaches
of Statistical MT and Example-Based MT. A typ-
ical SMT model will score a phrase pair with
source s, target t, log features 0, and weights
A using a log-linear model, as shown in Equa-
tion 1 of Figure 1. There is no prototypical model
for EBMT, but Equation 2 demonstrates a reason-
able framework where evidence for the phrase pair
is accumulated over all instances of translation.
Each instance of translation from the corpus has
a source s&apos; and target t&apos;. In the most limited case
s = s&apos; and t = t&apos;, but typically an EBMT sys-
tem will have some notion of similarity and use
instances of translation that do not exactly match
the input.
Cunei’s model is defined in such a way that we
maintain the distance function 0(s, s&apos;, t&apos;, t) from
the EBMT model, but compute it in a much more
efficient manner. In particular, we remove the real-
space summation within a logarithm that makes it
impractical to tune model weights. However, our
</bodyText>
<page confidence="0.982768">
149
</page>
<note confidence="0.4486355">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 149–154,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<equation confidence="0.998779">
�score(s,t) = AkOk(s, t) (1)
k
�score(s, t) = ln eEk λkφk(s,s0,t0,t) (2)
s0,t0
�score(s,t) = S + ` E(s0,t0)∈C Ok(s, s0, t0, t)eEi λiφi(s,s0,t0,t) �(3)
k ^k E� i λiφi(s,s0,t0,t)
(s0,t0)∈C e
</equation>
<figureCaption confidence="0.993124">
Figure 1: Translation model scores according to SMT (1), EBMT (2), and Cunei (2)
</figureCaption>
<bodyText confidence="0.986027833333333">
model preserves the first-order derivative of Equa-
tion 2, which is useful during optimization to lo-
cally approximate the hypothesis space. While the
inner term initially appears complex, it is simply
the expectation of each feature under the distribu-
tion of translation instances and can be efficiently
computed with an online update. Last, the in-
troduction of S, a slack variable, is necessary to
additionally ensure that the score of this model
is equal to Equation 2. Specifying the model in
this manner ties together the two different mod-
eling approaches pursued by SMT and EBMT;
the SMT model of Equation 1 is merely a spe-
cial case of our model when the features for all
instances of a translation are constant such that
Ok(s, s0, t0, t) = Ok(s, t) bs0, t0.
Indeed, this distinction illuminates the primary
advantage of our model. Each feature is calcu-
lated particular to one translation instance in the
corpus and each translation instance is scored in-
dividually. The model is then responsible for ag-
gregating knowledge across multiple instances of
translation. Unlike the SMT model, our aggregate
model does not maintain feature independence.
Each instance of translation represents a joint set
of features. The higher the score of a translation
instance, the more all its features inform the ag-
gregate model. Thus, our model is biased toward
feature values that represent relevant translation
instances.
</bodyText>
<subsectionHeader confidence="0.975417">
1.2 Context
</subsectionHeader>
<bodyText confidence="0.999864375">
Not all translations found in a corpus are equally
useful. Often, when dealing with data of vary-
ing quality, training a SMT system on all of the
data degrades performance. A common work-
around is to perform some sort of sub-sampling
that selects a small quantity of novel phrase pairs
from the large out-of-domain corpus such that they
do not overwhelm the number of phrase pairs ex-
tracted from the smaller in-domain corpus.
Instead of building our model from a heuristic
sub-sample, we utilize Cunei’s modeling approach
to explicitly identify the relevance of each transla-
tion instance. We add features to the model that
identify when a translation instance occurs within
the same context as the input. This permits us to
train on all available data by dynamically weight-
ing each instance of a translation.
First, we capture the broader context or genre of
a translation instance by comparing the document
in the corpus from which it was extracted to the
input document. These documents are modeled as
a bag of words, and we use common document-
level distance metrics from the field of information
retrieval. Specifically, we implement as features
document-level precision, recall, cosine distance
and Jensen-Shannon distance (Lin, 1991).
In order to capture local, intra-sentential con-
text, we compare the words immediately to the left
and right of each translation instance with the in-
put. We add one feature that counts the total num-
ber of adjacent words that match the input and a
second feature that penalizes translation instances
whose adjacent context only (or mostly) occurs in
one direction. As a variation on the same concept,
we also add four binary features that indicate when
a unigram or bigram match is present on the left or
right hand side.
The corpus in which an instance is located can
also substantially alter the style of a translation.
For example, both the German to English and the
Czech to English corpora consisted of in-domain
News Commenary and out-of-domain Europarl
text. When creating the index, Cunei stores the
name of the corpus that is associated with each
sentence. From this information we create a set
of binary features for each instance of translation
that indicate from which corpus the instance origi-
nated. The weights for these origin features can be
</bodyText>
<page confidence="0.98759">
150
</page>
<bodyText confidence="0.9975155">
conceived as mixture weights specifying the rele-
vance of each corpus.
</bodyText>
<subsectionHeader confidence="0.995543">
1.3 Alignment
</subsectionHeader>
<bodyText confidence="0.999994377358491">
After a match is found on the source-side of the
corpus, Cunei must determine the target phrase to
which it aligns. The phrase alignment is treated as
a hidden variable and not specified during train-
ing. Ideally, the full alignment process would
be carried out dynamically at run-time. Unfor-
tunately, even a simple word alignment such as
IBM Model-1 is too expensive. Instead, we run a
word aligner offline and our on-line phrase align-
ment computes features over the the word align-
ments. The phrase alignment features are then
components of the model for each translation in-
stance. While the calculations are not exactly the
same, conceptually this work is modeled after (Vo-
gel, 2005).
For each source-side match in the corpus, an
alignment matrix is loaded for the complete sen-
tence in which the match resides. This align-
ment matrix contains scores for all word corre-
spondences in the sentence pair and can be created
using GIZA++ (Och and Ney, 2003) or the Berke-
ley aligner (Liang et al., 2006). Intuitively, when a
source phrase is aligned to a target phrase, this im-
plies that the remainder of the source sentence that
is not specified by the source phrase is aligned to
the remainder of the target sentence not specified
by the target phrase. Separate features compute
the probability that the word alignments for to-
kens within the phrase are concentrated within the
phrase boundaries and that the word alignments
for tokens outside the phrase are concentrated out-
side the phrase boundaries. In addition, words
with no alignment links or weak alignments links
demonstrate uncertainty in modeling. To capture
this effect, we incorporate two more features that
count the number of uncertain alignments present
in the source phrase and the target phrase.
The features described above assess the phrase
alignment likelihood for a particular translation in-
stance. Because they operate over all the word
alignments present in a sentence, the alignment
scores are contextual and usually vary from in-
stance to instance. As the model weights change,
so too will the phrase alignment scores. Each
source phrase is modeled as having some proba-
bility of aligning to every possible target phrase
within a given sentence. However, it is not prac-
tical to compute all possible phrase alignments,
so we extract translation instances using only a
few high-scoring phrase alignments for each oc-
currence of a source phrase in the corpus.2 As dis-
cussed previously, these extracted translation in-
stances form the basic modeling unit in Cunei.
</bodyText>
<subsectionHeader confidence="0.991638">
1.4 Optimization
</subsectionHeader>
<bodyText confidence="0.985371424242424">
Cunei’s built-in optimization code closely follows
the approach of (Smith and Eisner, 2006), which
minimizes the expectation of the loss function over
the distribution of translations present in the n-
best list. Following (Smith and Eisner, 2006), we
implemented log(BLEU) as the loss function such
that the objective function can be decomposed as
the expected value of BLEU’s brevity penalty and
the expected value of BLEU’s precision score.
The optimization process slowly anneals the dis-
tribution of the n-best list in order to avoid local
minima. This begins with a near uniform distribu-
tion of translations and eventually reaches a distri-
bution where, for each sentence, nearly all of the
probability mass resides on the top translation (and
corresponds closely with the actual 1-best BLEU
score). In addition, Cunei supports the ability to
decode sentences toward a particular set of refer-
ences. This is used to prime the optimization pro-
cess in the first iteration with high-scoring, obtain-
able translations.
2 The WMT ’10 Translation Task
For the WMT ’10 Translation Task we built two
systems. The first translated from German to En-
glish and was trained with the provided News
Commentary and Europarl (Koehn, 2005) corpora.
The second system translated from Czech to En-
glish and used the CzEng 0.9 corpus (Bojar and
ˇZabokrtsk´y, 2009), which is a collection of many
different texts and includes the Europarl. To val-
idate our results, we also trained a Moses system
with the same corpus, alignments, and language
model.
</bodyText>
<subsectionHeader confidence="0.979794">
2.1 Corpus Preparation
</subsectionHeader>
<bodyText confidence="0.999704">
A large number of hand-crafted regular expres-
sions were used to remove noise (control char-
acters, null bytes, etc.), normalize (hard spaces
vs. soft spaces, different forms of quotations,
</bodyText>
<footnote confidence="0.979049">
2This is controlled by a score ratio that typically selects
2-6 translation instances per occurrence of a source phrase.
</footnote>
<page confidence="0.995873">
151
</page>
<bodyText confidence="0.9997875">
render XML codes as characters, etc.), and tok-
enize (abbreviations, numbers, punctuation, etc.).
However, these rules are fairly generic and appli-
cable to most Western languages. In particular,
we did not perform any morphologically-sensitive
segmentation. From the clean text we calculated
the expected word and character ratios between
the source language and the target language. Then
we proceeded to remove sentence pairs according
to the following heuristics:
</bodyText>
<listItem confidence="0.901401875">
• A sentence exceeded 125 words
• A sentence exceeded 1,000 characters
• The square of the difference between the
actual and expected words divided by the
square of the standard deviation exceeded 5
• The square of the difference between the ac-
tual and expected characters divided by the
square of the standard deviation exceeded 5
</listItem>
<bodyText confidence="0.999252636363636">
All of these processing routines are included as
part of the Cunei distribution and are configurable
options. An overview of the resulting corpora is
shown in Table 1.
Finally, we used the GIZA++ toolkit (Och and
Ney, 2003) to induce word alignments in both di-
rections for each language pair. The resulting cor-
pus and word alignments were provided to Moses
and Cunei for training. Each system used their
respective phrase extraction and model estimation
routines.
</bodyText>
<subsectionHeader confidence="0.975742">
2.2 Language Model
</subsectionHeader>
<bodyText confidence="0.997149727272727">
We intentionally selected two language pairs that
translated into English so that we could share one
language model between them. We used the large
monolingual English News text made available
through the workshop and augmented this with
the Xinhua and AFP sections of the English Gi-
gaword corpus (Parker and others, 2009). In all,
approximately one billion words of English text
were fed to the SRILM toolkit (Stolcke, 2002) to
construct a single English 5-gram language model
with Kneser-Ney smoothing.
</bodyText>
<subsectionHeader confidence="0.938517">
2.3 Experiments
</subsectionHeader>
<bodyText confidence="0.999980764705883">
The newswire evaluation sets from the prior two
years were selected as development data. 636 sen-
tences were sampled from WMT ’09 for tuning
and all 2,051 sentences from WMT ’08 were re-
served for testing. Finally, a blind evaluation was
also performed with the new WMT ’10 test set.
All systems were tuned toward BLEU (Papineni
et al., 2002) and all evaluation metrics were run
on lowercased, tokenized text.
The results in Table 2 and Table 3 show the per-
formance of Cunei3 against the Moses system we
also built with the same data. The first Cunei sys-
tem we built included all the alignment features
discussed in §1.3. These per-instance alignment
features are essential to Cunei’s run-time phrase
extraction and cannot be disabled. The second,
and complete, system added to this all the context
features described in §1.2. Cunei, in general, per-
forms significantly better than Moses in German
and is competitive with Moses in Czech. However,
we hoped to see a larger gain from the addition of
the context features.
In order to better understand our results and see
if there was greater potential for the context fea-
tures, we selectively added a few of the features at
a time to the German system. These experiments
are reported in Table 4. What is interesting here
is that most subsets of context features did better
than the whole and none degraded the baseline (at
least according to BLEU) on the test sets. We did
not expect a fully additive gain from the combina-
tion, as many of the context features do represent
different ways of capturing the same phenomena.
However, we were still surprised to find an appar-
ently detrimental interaction among the full set of
context features.
Theoretically adding new features should only
improve a system as a feature can always by ig-
nored by assigning it a weight of zero. How-
ever, new features expand the hypothesis space
and provide the model with more degrees of free-
dom which may make it easier to get stuck in lo-
cal minima. While the gradient-based, annealing
method for optimization that we use tends work
better than MERT (Och, 2003), it is still suscep-
tible to these issues. Indeed, the variation on the
tuning set–while relatively inconsequential–is ev-
idence that this is occurring and that we have not
found the global optimum. Further investigation is
necessary into the interaction between the context
features and techniques for robust optimization.
</bodyText>
<footnote confidence="0.991813666666667">
3These results have been updated since the official
WMT ’10 submission as a result of minor bug-fixes and code
improvements to Cunei.
</footnote>
<page confidence="0.994114">
152
</page>
<figure confidence="0.981604416666667">
German
English
Czech
English
Tokens
Sentences
41,245,188
43,064,069
63,776,164
72,325,831
1574044
6181270
</figure>
<tableCaption confidence="0.991528">
Table 1: Corpus Statistics
</tableCaption>
<subsectionHeader confidence="0.981193">
2.4 Conclusion
</subsectionHeader>
<bodyText confidence="0.9999808">
We used the Cunei Machine Translation Platform
to build German to English and Czech to English
systems for the WMT ’10 evaluation. In both
systems we experimented with per-instance align-
ment and context features. Our addition of the
context features resulted in only minor improve-
ment, but a deeper analysis of the individual fea-
tures suggests greater potential. Overall, Cunei
performed strongly in our evaluation against a
comparable Moses system. We acknowledge that
the actual features we selected are not particu-
larly novel. Instead, the importance of this work
is the simplicity with which instance-specific fea-
tures can be jointly modeled and integrated within
Cunei as a result of its unique modeling approach.
</bodyText>
<sectionHeader confidence="0.989082" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999498">
The author would like to thank Ralf Brown for
providing suggestions and feedback on this paper.
</bodyText>
<sectionHeader confidence="0.982142" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.959401463768116">
Ondˇrej Bojar and Zdenˇek ˇZabokrtsk´y. 2009.
Czeng0.9: Large parallel treebank with rich annota-
tion. Prague Bulletin of Mathematical Linguistics,
92.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, pages 177–
180, Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X Proceedings (mts, 2005), pages 79–
86.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An open source toolkit for parsing-based machine
translation. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 135–139,
Athens, Greece, March.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 104–111, New York City,
USA, June.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1):145–151, January.
2005. Phuket, Thailand, September.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
USA, July.
Robert Parker et al. 2009. English gigaword fourth
edition.
Aaron B. Phillips and Ralf D. Brown. 2009. Cunei
machine translation platform: System description.
In Mikel L. Forcada and Andy Way, editors, Pro-
ceedings of the 3rd Workshop on Example-Based
Machine Translation, pages 29–36, Dublin, Ireland,
November.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 787–794, Sydney, Australia, July.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In 7th International Conference
on Spoken Language Processing, pages 901–904,
Denver, USA, September.
Stephan Vogel. 2005. Pesa: Phrase pair extraction as
sentence splitting. In Machine Translation Summit
X Proceedings (mts, 2005), pages 251–258.
</reference>
<page confidence="0.99615">
153
</page>
<figure confidence="0.998374156862746">
0.6224
0.6054
0.5984
0.6313
0.6198
0.6003
TER
TER
0.6224
0.6172
0.6202
0.6206
0.6208
0.6209
TER
Meteor
Meteor
0.5676
0.5573
0.5575
0.5617
0.5665
0.5591
Meteor
Blind Test
Blind Test
0.5574
0.5564
0.5609
0.5579
0.5608
0.5573
Blind Test
6.8464
6.7916
6.8829
6.5657
6.6355
6.6467
NIST
NIST
6.7336
6.6656
6.6900
6.6719
6.6647
6.6355
NIST
0.2210
0.2214
0.2097
0.2297
0.2315
BLEU
BLEU
0.2291
0.2210
0.2244
0.2237
0.2235
0.2228
BLEU
0.2221
0.6430
0.6422
0.6362
0.6408
0.6523
0.6391
TER
TER
0.6430
0.6434
0.6456
0.6410
0.6391
0.6431
TER
Table 4: Breakdown of Context Features in German to English
Development Test
Development Test
Meteor
Table 2: Overview of German to English Evaluations
Meteor
0.5330
0.5342
0.5344
0.5398
0.5425
0.5361
Table 3: Overview of Czech to English Evaluations
Development Test
Meteor
0.5342
0.5354
0.5324
0.5329
0.5361
0.5361
6.2802
6.3574
6.4116
6.3639
6.3753
6.4391
NIST
NIST
6.3639
6.4154
6.3984
6.3598
6.3498
6.4183
NIST
0.2046
0.2127
0.2058
BLEU
0.2125
BLEU
0.2065
0.2041
0.2150
0.2134
0.2125
0.2147
0.2137
0.2145
BLEU
0.6362
0.6170
0.6128
0.6475
0.6375
0.6125
TER
TER
0.6402
0.6410
0.6422
0.6376
0.6375
0.6353
TER
Development Tuning
Development Tuning
Meteor
Meteor
0.5286
0.5326
0.5536
0.5567
0.5555
Development Tuning
0.5331
Meteor
0.5326
0.5370
0.5310
0.5338
0.5305
0.5315
5.9156
6.2634
6.2802
5.9847
6.1969
6.0021
NIST
NIST
6.0080
5.9514
5.9764
5.9847
5.9648
6.0233
NIST
0.1916
0.2022
0.2206
0.2170
0.2018
BLEU
BLEU
0.2141
0.2010
0.2002
0.2018
0.1987
0.2007
BLEU
0.2011
Cunei with Alignment &amp; Context
Cunei with Alignment &amp; Context
+ Adjacent Length &amp; Skew
+ Doc Precision &amp; Recall
+ Doc Cosine &amp; JSD
+ Adjacent N-grams
Cunei with Alignment
Cunei with Alignment
+ Origins
Cunei
Moses
Moses
</figure>
<page confidence="0.998679">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.974068">The Cunei Machine Translation Platform for WMT ’10</title>
<author confidence="0.999929">Aaron B Phillips</author>
<affiliation confidence="0.999114">Carnegie Mellon</affiliation>
<address confidence="0.850881">Pittsburgh, USA.</address>
<email confidence="0.999807">aphillips@cmu.edu</email>
<abstract confidence="0.993278399449036">This paper describes the Cunei Machine Translation Platform and how it was used in the WMT ’10 German to English and Czech to English translation tasks. 1 The Cunei Machine Translation Platform The Cunei Machine Translation Platform (Phillips and Brown, 2009) is open-source software freely available at Like Moses (Koehn et al., 2007) and Joshua (Li et al., 2009), Cunei provides a statistical decoder that combines partial translations (either phase pairs or grammar rules) in order to compose a coherent sentence in the target language. What makes Cunei unique is that it models the translation task with a non-parametric model that assesses the relevance of each translation instance. The process begins by encoding in a lattice all contiguous phrases from the For each source phrase in the lattice, Cunei locates instances of it in the corpus and then identifies the aligned target phrase(s). This much is standard to most data-driven MT systems. The typical step at this stage is to model a phrase pair by computing relative frequencies over the collection of translation instances. This model for the phrase pair will never change and knowledge of the translation instances can subsequently be discarded. In contrast to using a phrase pair as the basic unit of modeling, Cunei models each translation instance. A distance function, represented by a log-linear model, scores the relevance of each translation instance. Our model then sums the scores of translation instances that predict the same target hypothesis. The advantage of this approach is that it provides a flexible framework for novel sources of offers limited support for non-contiguous phrases, similar in concept to grammar rules, but this setting was disabled in our experiments. information. The non-parametric model still uses information gleaned over all translation instances, but it permits us to define a distance function that operates over one translation instance at a time. This enables us to score a wide-variety of information represented by the translation instance with respect to the input and the target hypothesis under consideration. For example, we could compute how similar one translation instance’s parse tree or morpho-syntactic information is to the input. Furthermore, this information will vary throughout the corpus with some translation instances exhibiting higher similarity to the input. Our approach captures that these instances are more relevant and they will have a larger effect on the model. For the WMT ’10 task, we exploited instance-specific context and alignment features which will be discussed in more detail below. 1.1 Formalism Cunei’s model is a hybrid between the approaches of Statistical MT and Example-Based MT. A typical SMT model will score a phrase pair with target log features and weights a log-linear model, as shown in Equation 1 of Figure 1. There is no prototypical model for EBMT, but Equation 2 demonstrates a reasonable framework where evidence for the phrase pair is accumulated over all instances of translation. Each instance of translation from the corpus has source and target In the most limited case and but typically an EBMT system will have some notion of similarity and use instances of translation that do not exactly match the input. Cunei’s model is defined in such a way that we the distance function the EBMT model, but compute it in a much more efficient manner. In particular, we remove the realspace summation within a logarithm that makes it impractical to tune model weights. However, our 149 of the Joint 5th Workshop on Statistical Machine Translation and pages Sweden, 15-16 July 2010. Association for Computational Linguistics = k = ln = k Figure 1: Translation model scores according to SMT (1), EBMT (2), and Cunei (2) model preserves the first-order derivative of Equation 2, which is useful during optimization to locally approximate the hypothesis space. While the inner term initially appears complex, it is simply the expectation of each feature under the distribution of translation instances and can be efficiently computed with an online update. Last, the inof a slack variable, is necessary to additionally ensure that the score of this model is equal to Equation 2. Specifying the model in this manner ties together the two different modeling approaches pursued by SMT and EBMT; the SMT model of Equation 1 is merely a special case of our model when the features for all instances of a translation are constant such that = Indeed, this distinction illuminates the primary advantage of our model. Each feature is calculated particular to one translation instance in the corpus and each translation instance is scored individually. The model is then responsible for aggregating knowledge across multiple instances of translation. Unlike the SMT model, our aggregate model does not maintain feature independence. instance of translation represents a of features. The higher the score of a translation the more features inform the aggregate model. Thus, our model is biased toward feature values that represent relevant translation instances. 1.2 Context Not all translations found in a corpus are equally useful. Often, when dealing with data of varying quality, training a SMT system on all of the A common workaround is to perform some sort of sub-sampling that selects a small quantity of novel phrase pairs from the large out-of-domain corpus such that they not overwhelm the number of phrase pairs extracted from the smaller in-domain corpus. Instead of building our model from a heuristic sub-sample, we utilize Cunei’s modeling approach to explicitly identify the relevance of each translation instance. We add features to the model that identify when a translation instance occurs within the same context as the input. This permits us to on data by dynamically weighting each instance of a translation. First, we capture the broader context or genre of a translation instance by comparing the document in the corpus from which it was extracted to the input document. These documents are modeled as a bag of words, and we use common documentlevel distance metrics from the field of information retrieval. Specifically, we implement as features document-level precision, recall, cosine distance and Jensen-Shannon distance (Lin, 1991). In order to capture local, intra-sentential context, we compare the words immediately to the left and right of each translation instance with the input. We add one feature that counts the total number of adjacent words that match the input and a second feature that penalizes translation instances whose adjacent context only (or mostly) occurs in one direction. As a variation on the same concept, we also add four binary features that indicate when is present on the side. The corpus in which an instance is located can also substantially alter the style of a translation. For example, both the German to English and the Czech to English corpora consisted of in-domain News Commenary and out-of-domain Europarl text. When creating the index, Cunei stores the name of the corpus that is associated with each sentence. From this information we create a set of binary features for each instance of translation that indicate from which corpus the instance originated. The weights for these origin features can be 150 conceived as mixture weights specifying the relevance of each corpus. 1.3 Alignment After a match is found on the source-side of the corpus, Cunei must determine the target phrase to which it aligns. The phrase alignment is treated as a hidden variable and not specified during training. Ideally, the full alignment process would be carried out dynamically at run-time. Unfortunately, even a simple word alignment such as IBM Model-1 is too expensive. Instead, we run a word aligner offline and our on-line phrase alignment computes features over the the word alignments. The phrase alignment features are then components of the model for each translation instance. While the calculations are not exactly the same, conceptually this work is modeled after (Vogel, 2005). For each source-side match in the corpus, an alignment matrix is loaded for the complete sentence in which the match resides. This alignment matrix contains scores for all word correspondences in the sentence pair and can be created using GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006). Intuitively, when a source phrase is aligned to a target phrase, this implies that the remainder of the source sentence that is not specified by the source phrase is aligned to the remainder of the target sentence not specified by the target phrase. Separate features compute the probability that the word alignments for tokens within the phrase are concentrated within the phrase boundaries and that the word alignments for tokens outside the phrase are concentrated outside the phrase boundaries. In addition, words with no alignment links or weak alignments links demonstrate uncertainty in modeling. To capture this effect, we incorporate two more features that count the number of uncertain alignments present in the source phrase and the target phrase. The features described above assess the phrase alignment likelihood for a particular translation instance. Because they operate over all the word alignments present in a sentence, the alignment scores are contextual and usually vary from instance to instance. As the model weights change, so too will the phrase alignment scores. Each source phrase is modeled as having some probability of aligning to every possible target phrase within a given sentence. However, it is not practical to compute all possible phrase alignments, so we extract translation instances using only a few high-scoring phrase alignments for each ocof a source phrase in the As discussed previously, these extracted translation instances form the basic modeling unit in Cunei. 1.4 Optimization Cunei’s built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over distribution of translations present in the best list. Following (Smith and Eisner, 2006), we the loss function such that the objective function can be decomposed as the expected value of BLEU’s brevity penalty and the expected value of BLEU’s precision score. The optimization process slowly anneals the disof the list in order to avoid local minima. This begins with a near uniform distribution of translations and eventually reaches a distribution where, for each sentence, nearly all of the probability mass resides on the top translation (and corresponds closely with the actual 1-best BLEU score). In addition, Cunei supports the ability to sentences particular set of references. This is used to prime the optimization process in the first iteration with high-scoring, obtainable translations. 2 The WMT ’10 Translation Task For the WMT ’10 Translation Task we built two systems. The first translated from German to English and was trained with the provided News Commentary and Europarl (Koehn, 2005) corpora. The second system translated from Czech to English and used the CzEng 0.9 corpus (Bojar and 2009), which is a collection of many different texts and includes the Europarl. To validate our results, we also trained a Moses system with the same corpus, alignments, and language model. 2.1 Corpus Preparation A large number of hand-crafted regular expressions were used to remove noise (control characters, null bytes, etc.), normalize (hard spaces vs. soft spaces, different forms of quotations, is controlled by a score ratio that typically selects 2-6 translation instances per occurrence of a source phrase. 151 render XML codes as characters, etc.), and tokenize (abbreviations, numbers, punctuation, etc.). However, these rules are fairly generic and applicable to most Western languages. In particular, we did not perform any morphologically-sensitive segmentation. From the clean text we calculated the expected word and character ratios between the source language and the target language. Then we proceeded to remove sentence pairs according to the following heuristics: • A sentence exceeded 125 words • A sentence exceeded 1,000 characters • The square of the difference between the actual and expected words divided by the square of the standard deviation exceeded 5 • The square of the difference between the actual and expected characters divided by the square of the standard deviation exceeded 5 All of these processing routines are included as part of the Cunei distribution and are configurable options. An overview of the resulting corpora is shown in Table 1. Finally, we used the GIZA++ toolkit (Och and Ney, 2003) to induce word alignments in both directions for each language pair. The resulting corpus and word alignments were provided to Moses and Cunei for training. Each system used their respective phrase extraction and model estimation routines. 2.2 Language Model We intentionally selected two language pairs that translated into English so that we could share one language model between them. We used the large monolingual English News text made available through the workshop and augmented this with the Xinhua and AFP sections of the English Gigaword corpus (Parker and others, 2009). In all, approximately one billion words of English text were fed to the SRILM toolkit (Stolcke, 2002) to construct a single English 5-gram language model with Kneser-Ney smoothing. 2.3 Experiments The newswire evaluation sets from the prior two years were selected as development data. 636 sentences were sampled from WMT ’09 for tuning and all 2,051 sentences from WMT ’08 were reserved for testing. Finally, a blind evaluation was also performed with the new WMT ’10 test set. All systems were tuned toward BLEU (Papineni et al., 2002) and all evaluation metrics were run on lowercased, tokenized text. The results in Table 2 and Table 3 show the perof against the Moses system we also built with the same data. The first Cunei system we built included all the alignment features in These per-instance alignment features are essential to Cunei’s run-time phrase extraction and cannot be disabled. The second, and complete, system added to this all the context described in Cunei, in general, performs significantly better than Moses in German and is competitive with Moses in Czech. However, we hoped to see a larger gain from the addition of the context features. In order to better understand our results and see if there was greater potential for the context features, we selectively added a few of the features at a time to the German system. These experiments are reported in Table 4. What is interesting here is that most subsets of context features did better than the whole and none degraded the baseline (at least according to BLEU) on the test sets. We did not expect a fully additive gain from the combination, as many of the context features do represent different ways of capturing the same phenomena. However, we were still surprised to find an apparamong the full set of context features. Theoretically adding new features should only improve a system as a feature can always by ignored by assigning it a weight of zero. However, new features expand the hypothesis space and provide the model with more degrees of freedom which may make it easier to get stuck in local minima. While the gradient-based, annealing method for optimization that we use tends work better than MERT (Och, 2003), it is still susceptible to these issues. Indeed, the variation on the tuning set–while relatively inconsequential–is evidence that this is occurring and that we have not found the global optimum. Further investigation is necessary into the interaction between the context features and techniques for robust optimization. results have been updated since the official WMT ’10 submission as a result of minor bug-fixes and code improvements to Cunei.</abstract>
<note confidence="0.388891285714286">152 German English Czech English Tokens Sentences</note>
<address confidence="0.828343333333333">41,245,188 43,064,069 63,776,164 72,325,831 1574044 6181270</address>
<note confidence="0.826617">Table 1: Corpus Statistics</note>
<abstract confidence="0.9607651875">2.4 Conclusion We used the Cunei Machine Translation Platform to build German to English and Czech to English systems for the WMT ’10 evaluation. In both systems we experimented with per-instance alignment and context features. Our addition of the context features resulted in only minor improvement, but a deeper analysis of the individual features suggests greater potential. Overall, Cunei performed strongly in our evaluation against a comparable Moses system. We acknowledge that the actual features we selected are not particularly novel. Instead, the importance of this work is the simplicity with which instance-specific features can be jointly modeled and integrated within Cunei as a result of its unique modeling approach.</abstract>
<note confidence="0.889544888888889">Acknowledgements The author would like to thank Ralf Brown for providing suggestions and feedback on this paper. References Bojar and Zdenˇek 2009. Czeng0.9: Large parallel treebank with rich annota- Bulletin of Mathematical 92. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris</note>
<author confidence="0.737093333333333">Marcello Federico Callison-Burch</author>
<author confidence="0.737093333333333">Nicola Bertoldi</author>
<author confidence="0.737093333333333">Brooke Cowan</author>
<author confidence="0.737093333333333">Wade Shen</author>
<author confidence="0.737093333333333">Christine Moran</author>
<author confidence="0.737093333333333">Richard Zens</author>
<author confidence="0.737093333333333">Chris Dyer</author>
<author confidence="0.737093333333333">Ondrej Bojar</author>
<author confidence="0.737093333333333">Alexandra</author>
<note confidence="0.887273758064516">Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Asfor Computational pages 177– 180, Prague, Czech Republic, June. Philipp Koehn. 2005. Europarl: A parallel corpus for machine translation. In Transla- Summit X Proceedings 2005), pages 79– 86. Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine In of the Fourth Workshop Statistical Machine pages 135–139, Athens, Greece, March. Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignby agreement. In of the Human Language Technology Conference of the North American Chapter of the Association for Computapages 104–111, New York City, USA, June. Jianhua Lin. 1991. Divergence measures based on the entropy. Transactions on Information 37(1):145–151, January. 2005. Phuket, Thailand, September. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment 29(1):19–51. Franz Josef Och. 2003. Minimum error rate trainin statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association Computational pages 160–167, Sapporo, Japan, July. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalof machine translation. In of the 40th Annual Meeting of the Association for Compages 311–318, Philadelphia, USA, July. Robert Parker et al. 2009. English gigaword fourth edition. Aaron B. Phillips and Ralf D. Brown. 2009. Cunei machine translation platform: System description. Mikel L. Forcada and Andy Way, editors, Proceedings of the 3rd Workshop on Example-Based pages 29–36, Dublin, Ireland, November. David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguispages 787–794, Sydney, Australia, July. Andreas Stolcke. 2002. Srilm an extensible language toolkit. In International Conference Spoken Language pages 901–904, Denver, USA, September. Stephan Vogel. 2005. Pesa: Phrase pair extraction as splitting. In Translation Summit Proceedings 2005), pages 251–258. 153</note>
<address confidence="0.6766875">0.6224 0.6054 0.5984 0.6313 0.6198 0.6003</address>
<email confidence="0.650054">TER</email>
<affiliation confidence="0.781206">TER</affiliation>
<address confidence="0.8562446">0.6224 0.6172 0.6202 0.6206 0.6208</address>
<date confidence="0.479746">0.6209</date>
<title confidence="0.707402666666667">TER Meteor Meteor</title>
<address confidence="0.6532735">0.5676 0.5573 0.5575 0.5617 0.5665 0.5591</address>
<title confidence="0.861594">Meteor</title>
<author confidence="0.8205185">Blind Test Blind Test</author>
<address confidence="0.835054">0.5574 0.5564 0.5609 0.5579 0.5608 0.5573</address>
<author confidence="0.688276">Blind Test</author>
<address confidence="0.817103333333333">6.8464 6.7916 6.8829 6.5657 6.6355 6.6467</address>
<affiliation confidence="0.7691915">NIST NIST</affiliation>
<address confidence="0.75873025">6.7336 6.6656 6.6900 6.6719 6.6647 6.6355 NIST 0.2210 0.2214 0.2097 0.2297 0.2315</address>
<email confidence="0.662062">BLEU</email>
<affiliation confidence="0.745066">BLEU</affiliation>
<address confidence="0.823288166666667">0.2291 0.2210 0.2244 0.2237 0.2235 0.2228</address>
<affiliation confidence="0.469155">BLEU</affiliation>
<address confidence="0.781880714285714">0.2221 0.6430 0.6422 0.6362 0.6408 0.6523 0.6391</address>
<email confidence="0.605101">TER</email>
<affiliation confidence="0.687894">TER</affiliation>
<address confidence="0.8470238">0.6430 0.6434 0.6456 0.6410 0.6391</address>
<date confidence="0.495534">0.6431</date>
<title confidence="0.740026857142857">TER Table 4: Breakdown of Context Features in German to English Development Test Development Test Meteor Table 2: Overview of German to English Evaluations Meteor</title>
<address confidence="0.7839764">0.5330 0.5342 0.5344 0.5398 0.5425</address>
<date confidence="0.522102">0.5361</date>
<title confidence="0.528073">Table 3: Overview of Czech to English Evaluations Development Test Meteor</title>
<address confidence="0.834299083333333">0.5342 0.5354 0.5324 0.5329 0.5361 0.5361 6.2802 6.3574 6.4116 6.3639 6.3753 6.4391</address>
<affiliation confidence="0.805945">NIST NIST</affiliation>
<address confidence="0.829860333333333">6.3639 6.4154 6.3984 6.3598 6.3498 6.4183</address>
<affiliation confidence="0.730257">NIST</affiliation>
<address confidence="0.762556333333333">0.2046 0.2127 0.2058</address>
<email confidence="0.456694">BLEU</email>
<date confidence="0.223425">0.2125</date>
<affiliation confidence="0.275515">BLEU</affiliation>
<address confidence="0.776107933333333">0.2065 0.2041 0.2150 0.2134 0.2125 0.2147 0.2137 0.2145 BLEU 0.6362 0.6170 0.6128 0.6475 0.6375 0.6125</address>
<email confidence="0.724522">TER</email>
<affiliation confidence="0.769314">TER</affiliation>
<address confidence="0.795950333333333">0.6402 0.6410 0.6422 0.6376 0.6375 0.6353</address>
<email confidence="0.425264">TER</email>
<title confidence="0.91931825">Development Tuning Development Tuning Meteor Meteor</title>
<address confidence="0.787652428571429">0.5286 0.5326 0.5536 0.5567 0.5555 Development Tuning 0.5331</address>
<email confidence="0.493927">Meteor</email>
<address confidence="0.86209075">0.5326 0.5370 0.5310 0.5338 0.5305 0.5315 5.9156 6.2634 6.2802 5.9847 6.1969 6.0021</address>
<affiliation confidence="0.740814">NIST NIST</affiliation>
<address confidence="0.874391166666667">6.0080 5.9514 5.9764 5.9847 5.9648 6.0233</address>
<affiliation confidence="0.732843">NIST</affiliation>
<address confidence="0.8589254">0.1916 0.2022 0.2206 0.2170 0.2018</address>
<email confidence="0.672402">BLEU</email>
<affiliation confidence="0.766826">BLEU</affiliation>
<address confidence="0.7434175">0.2141 0.2010 0.2002 0.2018 0.1987 0.2007</address>
<email confidence="0.443116">BLEU</email>
<date confidence="0.268848">0.2011</date>
<note confidence="0.8273005">Cunei with Alignment &amp; Context Cunei with Alignment &amp; Context</note>
<title confidence="0.853655666666667">Adjacent Length &amp; Skew + Doc Precision &amp; Recall + Doc Cosine &amp; JSD + Adjacent N-grams Cunei with Alignment Cunei with Alignment + Origins Cunei Moses</title>
<author confidence="0.699487">Moses</author>
<intro confidence="0.341207">154</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<date>2009</date>
<marker>Bojar, ˇZabokrtsk´y, 2009</marker>
<rawString>Ondˇrej Bojar and Zdenˇek ˇZabokrtsk´y. 2009.</rawString>
</citation>
<citation valid="false">
<title>Czeng0.9: Large parallel treebank with rich annotation.</title>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>92</volume>
<marker></marker>
<rawString>Czeng0.9: Large parallel treebank with rich annotation. Prague Bulletin of Mathematical Linguistics, 92.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 177– 180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X Proceedings (mts,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="11834" citStr="Koehn, 2005" startWordPosition="1938" endWordPosition="1939">s and eventually reaches a distribution where, for each sentence, nearly all of the probability mass resides on the top translation (and corresponds closely with the actual 1-best BLEU score). In addition, Cunei supports the ability to decode sentences toward a particular set of references. This is used to prime the optimization process in the first iteration with high-scoring, obtainable translations. 2 The WMT ’10 Translation Task For the WMT ’10 Translation Task we built two systems. The first translated from German to English and was trained with the provided News Commentary and Europarl (Koehn, 2005) corpora. The second system translated from Czech to English and used the CzEng 0.9 corpus (Bojar and ˇZabokrtsk´y, 2009), which is a collection of many different texts and includes the Europarl. To validate our results, we also trained a Moses system with the same corpus, alignments, and language model. 2.1 Corpus Preparation A large number of hand-crafted regular expressions were used to remove noise (control characters, null bytes, etc.), normalize (hard spaces vs. soft spaces, different forms of quotations, 2This is controlled by a score ratio that typically selects 2-6 translation instanc</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Machine Translation Summit X Proceedings (mts, 2005), pages 79– 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<location>Athens, Greece,</location>
<marker>Li, Callison-Burch, Dyer, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135–139, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>New York City, USA,</location>
<contexts>
<context position="9066" citStr="Liang et al., 2006" startWordPosition="1492" endWordPosition="1495">we run a word aligner offline and our on-line phrase alignment computes features over the the word alignments. The phrase alignment features are then components of the model for each translation instance. While the calculations are not exactly the same, conceptually this work is modeled after (Vogel, 2005). For each source-side match in the corpus, an alignment matrix is loaded for the complete sentence in which the match resides. This alignment matrix contains scores for all word correspondences in the sentence pair and can be created using GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006). Intuitively, when a source phrase is aligned to a target phrase, this implies that the remainder of the source sentence that is not specified by the source phrase is aligned to the remainder of the target sentence not specified by the target phrase. Separate features compute the probability that the word alignments for tokens within the phrase are concentrated within the phrase boundaries and that the word alignments for tokens outside the phrase are concentrated outside the phrase boundaries. In addition, words with no alignment links or weak alignments links demonstrate uncertainty in mode</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 104–111, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the shannon entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>1</issue>
<location>Phuket, Thailand,</location>
<contexts>
<context position="6924" citStr="Lin, 1991" startWordPosition="1130" endWordPosition="1131">ify when a translation instance occurs within the same context as the input. This permits us to train on all available data by dynamically weighting each instance of a translation. First, we capture the broader context or genre of a translation instance by comparing the document in the corpus from which it was extracted to the input document. These documents are modeled as a bag of words, and we use common documentlevel distance metrics from the field of information retrieval. Specifically, we implement as features document-level precision, recall, cosine distance and Jensen-Shannon distance (Lin, 1991). In order to capture local, intra-sentential context, we compare the words immediately to the left and right of each translation instance with the input. We add one feature that counts the total number of adjacent words that match the input and a second feature that penalizes translation instances whose adjacent context only (or mostly) occurs in one direction. As a variation on the same concept, we also add four binary features that indicate when a unigram or bigram match is present on the left or right hand side. The corpus in which an instance is located can also substantially alter the st</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on the shannon entropy. IEEE Transactions on Information Theory, 37(1):145–151, January. 2005. Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9021" citStr="Och and Ney, 2003" startWordPosition="1483" endWordPosition="1486">h as IBM Model-1 is too expensive. Instead, we run a word aligner offline and our on-line phrase alignment computes features over the the word alignments. The phrase alignment features are then components of the model for each translation instance. While the calculations are not exactly the same, conceptually this work is modeled after (Vogel, 2005). For each source-side match in the corpus, an alignment matrix is loaded for the complete sentence in which the match resides. This alignment matrix contains scores for all word correspondences in the sentence pair and can be created using GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006). Intuitively, when a source phrase is aligned to a target phrase, this implies that the remainder of the source sentence that is not specified by the source phrase is aligned to the remainder of the target sentence not specified by the target phrase. Separate features compute the probability that the word alignments for tokens within the phrase are concentrated within the phrase boundaries and that the word alignments for tokens outside the phrase are concentrated outside the phrase boundaries. In addition, words with no alignment links or weak ali</context>
<context position="13496" citStr="Och and Ney, 2003" startWordPosition="2201" endWordPosition="2204">o remove sentence pairs according to the following heuristics: • A sentence exceeded 125 words • A sentence exceeded 1,000 characters • The square of the difference between the actual and expected words divided by the square of the standard deviation exceeded 5 • The square of the difference between the actual and expected characters divided by the square of the standard deviation exceeded 5 All of these processing routines are included as part of the Cunei distribution and are configurable options. An overview of the resulting corpora is shown in Table 1. Finally, we used the GIZA++ toolkit (Och and Ney, 2003) to induce word alignments in both directions for each language pair. The resulting corpus and word alignments were provided to Moses and Cunei for training. Each system used their respective phrase extraction and model estimation routines. 2.2 Language Model We intentionally selected two language pairs that translated into English so that we could share one language model between them. We used the large monolingual English News text made available through the workshop and augmented this with the Xinhua and AFP sections of the English Gigaword corpus (Parker and others, 2009). In all, approxim</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="16362" citStr="Och, 2003" startWordPosition="2687" endWordPosition="2688">s many of the context features do represent different ways of capturing the same phenomena. However, we were still surprised to find an apparently detrimental interaction among the full set of context features. Theoretically adding new features should only improve a system as a feature can always by ignored by assigning it a weight of zero. However, new features expand the hypothesis space and provide the model with more degrees of freedom which may make it easier to get stuck in local minima. While the gradient-based, annealing method for optimization that we use tends work better than MERT (Och, 2003), it is still susceptible to these issues. Indeed, the variation on the tuning set–while relatively inconsequential–is evidence that this is occurring and that we have not found the global optimum. Further investigation is necessary into the interaction between the context features and techniques for robust optimization. 3These results have been updated since the official WMT ’10 submission as a result of minor bug-fixes and code improvements to Cunei. 152 German English Czech English Tokens Sentences 41,245,188 43,064,069 63,776,164 72,325,831 1574044 6181270 Table 1: Corpus Statistics 2.4 Co</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="14617" citStr="Papineni et al., 2002" startWordPosition="2384" endWordPosition="2387">he Xinhua and AFP sections of the English Gigaword corpus (Parker and others, 2009). In all, approximately one billion words of English text were fed to the SRILM toolkit (Stolcke, 2002) to construct a single English 5-gram language model with Kneser-Ney smoothing. 2.3 Experiments The newswire evaluation sets from the prior two years were selected as development data. 636 sentences were sampled from WMT ’09 for tuning and all 2,051 sentences from WMT ’08 were reserved for testing. Finally, a blind evaluation was also performed with the new WMT ’10 test set. All systems were tuned toward BLEU (Papineni et al., 2002) and all evaluation metrics were run on lowercased, tokenized text. The results in Table 2 and Table 3 show the performance of Cunei3 against the Moses system we also built with the same data. The first Cunei system we built included all the alignment features discussed in §1.3. These per-instance alignment features are essential to Cunei’s run-time phrase extraction and cannot be disabled. The second, and complete, system added to this all the context features described in §1.2. Cunei, in general, performs significantly better than Moses in German and is competitive with Moses in Czech. Howev</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
</authors>
<date>2009</date>
<note>English gigaword fourth edition.</note>
<marker>Parker, 2009</marker>
<rawString>Robert Parker et al. 2009. English gigaword fourth edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron B Phillips</author>
<author>Ralf D Brown</author>
</authors>
<title>Cunei machine translation platform: System description.</title>
<date>2009</date>
<booktitle>Proceedings of the 3rd Workshop on Example-Based Machine Translation,</booktitle>
<pages>29--36</pages>
<editor>In Mikel L. Forcada and Andy Way, editors,</editor>
<location>Dublin, Ireland,</location>
<marker>Phillips, Brown, 2009</marker>
<rawString>Aaron B. Phillips and Ralf D. Brown. 2009. Cunei machine translation platform: System description. In Mikel L. Forcada and Andy Way, editors, Proceedings of the 3rd Workshop on Example-Based Machine Translation, pages 29–36, Dublin, Ireland, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>787--794</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="10704" citStr="Smith and Eisner, 2006" startWordPosition="1753" endWordPosition="1756">ance. As the model weights change, so too will the phrase alignment scores. Each source phrase is modeled as having some probability of aligning to every possible target phrase within a given sentence. However, it is not practical to compute all possible phrase alignments, so we extract translation instances using only a few high-scoring phrase alignments for each occurrence of a source phrase in the corpus.2 As discussed previously, these extracted translation instances form the basic modeling unit in Cunei. 1.4 Optimization Cunei’s built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the nbest list. Following (Smith and Eisner, 2006), we implemented log(BLEU) as the loss function such that the objective function can be decomposed as the expected value of BLEU’s brevity penalty and the expected value of BLEU’s precision score. The optimization process slowly anneals the distribution of the n-best list in order to avoid local minima. This begins with a near uniform distribution of translations and eventually reaches a distribution where, for each sentence, nearly all of th</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 787–794, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In 7th International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, USA,</location>
<contexts>
<context position="14181" citStr="Stolcke, 2002" startWordPosition="2313" endWordPosition="2314">he resulting corpus and word alignments were provided to Moses and Cunei for training. Each system used their respective phrase extraction and model estimation routines. 2.2 Language Model We intentionally selected two language pairs that translated into English so that we could share one language model between them. We used the large monolingual English News text made available through the workshop and augmented this with the Xinhua and AFP sections of the English Gigaword corpus (Parker and others, 2009). In all, approximately one billion words of English text were fed to the SRILM toolkit (Stolcke, 2002) to construct a single English 5-gram language model with Kneser-Ney smoothing. 2.3 Experiments The newswire evaluation sets from the prior two years were selected as development data. 636 sentences were sampled from WMT ’09 for tuning and all 2,051 sentences from WMT ’08 were reserved for testing. Finally, a blind evaluation was also performed with the new WMT ’10 test set. All systems were tuned toward BLEU (Papineni et al., 2002) and all evaluation metrics were run on lowercased, tokenized text. The results in Table 2 and Table 3 show the performance of Cunei3 against the Moses system we al</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In 7th International Conference on Spoken Language Processing, pages 901–904, Denver, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
</authors>
<title>Pesa: Phrase pair extraction as sentence splitting.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X Proceedings (mts,</booktitle>
<pages>251--258</pages>
<contexts>
<context position="8754" citStr="Vogel, 2005" startWordPosition="1437" endWordPosition="1439">rmine the target phrase to which it aligns. The phrase alignment is treated as a hidden variable and not specified during training. Ideally, the full alignment process would be carried out dynamically at run-time. Unfortunately, even a simple word alignment such as IBM Model-1 is too expensive. Instead, we run a word aligner offline and our on-line phrase alignment computes features over the the word alignments. The phrase alignment features are then components of the model for each translation instance. While the calculations are not exactly the same, conceptually this work is modeled after (Vogel, 2005). For each source-side match in the corpus, an alignment matrix is loaded for the complete sentence in which the match resides. This alignment matrix contains scores for all word correspondences in the sentence pair and can be created using GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006). Intuitively, when a source phrase is aligned to a target phrase, this implies that the remainder of the source sentence that is not specified by the source phrase is aligned to the remainder of the target sentence not specified by the target phrase. Separate features compute the probab</context>
</contexts>
<marker>Vogel, 2005</marker>
<rawString>Stephan Vogel. 2005. Pesa: Phrase pair extraction as sentence splitting. In Machine Translation Summit X Proceedings (mts, 2005), pages 251–258.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>