<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000162">
<title confidence="0.997688">
A Deep Learning Approach to Machine Transliteration
</title>
<author confidence="0.833622">
Thomas Deselaers and Saˇsa Hasan and Oliver Bender and Hermann Ney
</author>
<affiliation confidence="0.744563">
Human Language Technology and Pattern Recognition Group – RWTH Aachen University
</affiliation>
<email confidence="0.958761">
&lt;surname&gt;@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.981195" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999002294117647">
In this paper we present a novel translit-
eration technique which is based on deep
belief networks. Common approaches
use finite state machines or other meth-
ods similar to conventional machine trans-
lation. Instead of using conventional NLP
techniques, the approach presented here
builds on deep belief networks, a tech-
nique which was shown to work well for
other machine learning problems. We
show that deep belief networks have cer-
tain properties which are very interesting
for transliteration and possibly also for
translation and that a combination with
conventional techniques leads to an im-
provement over both components on an
Arabic-English transliteration task.
</bodyText>
<sectionHeader confidence="0.998803" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999909600000001">
Transliteration, i.e. the transcription of words such
as proper nouns from one language into another or,
more commonly from one alphabet into another, is
an important subtask of machine translation (MT)
in order to obtain high quality output.
We present a new technique for transliteration
which is based on deep belief networks (DBNs),
a well studied approach in machine learning.
Transliteration can in principle be considered to be
a small-scale translation problem and, thus, some
ideas presented here can be transferred to the ma-
chine translation domain as well.
Transliteration has been in use in machine trans-
lation systems, e.g. Russian-English, since the ex-
istence of the field of machine translation. How-
ever, to our knowledge it was first studied as a
machine learning problem by Knight and Graehl
(1998) using probabilistic finite-state transducers.
Subsequently, the performance of this system was
greatly improved by combining different spelling
and phonetic models (Al-Onaizan and Knight,
2002). Huang et al. (2004) construct a proba-
bilistic Chinese-English edit model as part of a
larger alignment solution using a heuristic boot-
strapped procedure. Freitag and Khadivi (2007)
propose a technique which combines conventional
MT methods with a single layer perceptron.
In contrast to these methods which strongly
build on top of well-established natural language
processing (NLP) techniques, we propose an al-
ternative model. Our new model is based on deep
belief networks which have been shown to work
well in other machine learning and pattern recog-
nition areas (cf. Section 2). Since translation and
transliteration are closely related and translitera-
tion can be considered a translation problem on the
character level, we discuss various methods from
both domains which are related to the proposed
approach in the following.
Neural networks have been used in NLP in
the past, e.g. for machine translation (Asunci´on
Casta˜no et al., 1997) and constituent parsing
(Titov and Henderson, 2007). However, it might
not be straight-forward to obtain good results us-
ing neural networks in this domain. In general,
when training a neural network, one has to choose
the structure of the neural network which involves
certain trade-offs. If a small network with no hid-
den layer is chosen, it can be efficiently trained
but has very limited representational power, and
may be unable to learn the relationships between
the source and the target language. The DBN ap-
proach alleviates some of the problems that com-
monly occur when working with neural networks:
1. they allow for efficient training due to a good
initialisation of the individual layers. 2. Overfit-
ting problems are addressed by creating generative
models which are later refined discriminatively. 3.
The network structure is clearly defined and only a
few structure parameters have to be set. 4. DBNs
can be interpreted as Bayesian probabilistic gener-
ative models.
Recently, Collobert and Weston (2008) pro-
posed a technique which applies a convolutional
DBN to a multi-task learning NLP problem. Their
approach is able to address POS tagging, chunk-
ing, named entity tagging, semantic role and simi-
lar word identification in one model. Our model is
similar to this approach in that it uses the same ma-
chine learning techniques but the encoding and the
</bodyText>
<note confidence="0.9378545">
Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 233–241,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998602">
233
</page>
<bodyText confidence="0.9999538">
processing is done differently. First, we learn two
independent generative models, one for the source
input and one for the target output. Then, these
two models are combined into a source-to-target
encoding/decoding system (cf. Section 2).
Regarding that the target is generated and not
searched in a space of hypotheses (e.g. in a word
graph), our approach is similar to the approach
presented by Bangalore et al. (2007) who present
an MT system where the set of words of the tar-
get sentence is generated based on the full source
sentence and then a finite-state approach is used to
reorder the words. Opposed to this approach we
do not only generate the letters/words in the target
sentence but we generate the full sentence with or-
dering.
We evaluate the proposed methods on an
Arabic-English transliteration task where Arabic
city names have to be transcribed into the equiva-
lent English spelling.
</bodyText>
<sectionHeader confidence="0.9705945" genericHeader="method">
2 Deep Belief Networks for
Transliteration
</sectionHeader>
<bodyText confidence="0.999437464285714">
Although DBNs are thoroughly described in the
literature, e.g. (Hinton et al., 2006), we give a short
overview on the ideas and techniques and intro-
duce our notation.
Deep architectures in machine learning and ar-
tificial intelligence are becoming more and more
popular after an efficient training algorithm has
been proposed (Hinton et al., 2006), although the
idea is known for some years (Ackley et al., 1985).
Deep belief networks consist of multiple layers of
restricted Boltzmann machines (RBMs). It was
shown that DBNs can be used for dimensionality
reduction of images and text documents (Hinton
and Salakhutdinow, 2006) and for language mod-
elling (Mnih and Hinton, 2007). Recently, DBNs
were also used successfully in image retrieval to
create very compact but meaningful representa-
tions of a huge set of images (nearly 13 million)
for retrieval (Torralba et al., 2008).
DBNs are built from RBMs by first training an
RBM on the input data. A second RBM is built
on the output of the first one and so on until a
sufficiently deep architecture is created. RBMs
are stochastic generative artificial neural networks
with restricted connectivity. From a theoretical
viewpoint, RBMs are interesting because they are
able to discover complex regularities and find no-
table features in data (Ackley et al., 1985).
</bodyText>
<figureCaption confidence="0.9777775">
Figure 1: A schematic representation of our DBN
for transliteration.
</figureCaption>
<bodyText confidence="0.999925285714286">
Hinton and Salakhutdinow (2006) present a
deep belief network to learn a tiny representation
of its inputs and to reconstruct the input with high
accuracy which is demonstrated for images and
textual documents. Here, we use DBNs similarly:
first, we learn encoders for the source and tar-
get words respectively and then connect these two
through a joint layer to map between the two lan-
guages. This joint layer is trained in the same way
as the top-level neurons in the deep belief classi-
fier from (Hinton et al., 2006).
In Figure 1, a schematic view of our DBN for
transliteration is shown. On the left and on the
right are encoders for the source and target words
respectively. To transliterate a source word, it is
passed through the layers of the network. First, it
traverses through the source encoder on the left,
then it passes into the joint layer, finally travers-
ing down through the target encoder. Each layer
consists of a set of neurons receiving the output
of the preceding layer as input. The first layers in
the source and target encoders consist of 51 and
T1 neurons, respectively; the second layers have
52 and T2 nodes, and the third layers have 53 and
T3 nodes, respectively. A joint layer with J nodes
connects the source and the target encoders.
Here, the number of nodes in the individual lay-
ers are the most important parameters. The more
</bodyText>
<page confidence="0.996034">
234
</page>
<bodyText confidence="0.9998381">
nodes a layer has, the more information can be
conveyed through it, but the harder the training:
the amount of data needed for training and thus
the computation time required is exponential in the
size of the network (Ackley et al., 1985).
To transliterate a source word, it is first encoded
as a DF-dimensional binary vector 5F (cf. Sec-
tion 2.1) and then fed into the first layer of the
source encoder. The 51-dimensional output vec-
tor OS1 of the first layer is computed as
</bodyText>
<equation confidence="0.932106">
OS1 ← 1/ exp (1 + wS15F + bS1) , (1)
</equation>
<bodyText confidence="0.99881775">
where wS1 is a 51 × DF-dimensional weight ma-
trix and bS1 is an 51-dimensional bias vector.
The output of each layer is used as input to the
next layer as follows:
</bodyText>
<equation confidence="0.9991635">
OS2 ← 1/ exp (1 + wS2OS1 + bS2) , (2)
OS3 ← 1/ exp (1 + wS3OS2 + bS3) . (3)
</equation>
<bodyText confidence="0.999905833333333">
After the source encoder has been traversed, the
joint layer is reached which processes the data
twice: once using the input from the source en-
coder to get a state of the hidden neurons OSJ and
then to infer an output state OJT as input to the
topmost level of the output encoder
</bodyText>
<equation confidence="0.6409875">
OSJ ← 1/ exp (1 + wSJOS3 + bSJ) , (4)
OJT ← 1/ exp (1 + wJTOSJ + bJT) . (5)
</equation>
<bodyText confidence="0.962139">
This output vector is decoded by traversing down-
wards through the output encoder:
</bodyText>
<equation confidence="0.996397">
OT3 ← 1/ exp (1 + wT3OJT + bT3) ,
OT2 ← 1/ exp (1 + wT2OT3 + bT2) ,
OT1 ← wT1OT2 + bT1,
</equation>
<bodyText confidence="0.999230833333333">
where OT1 is a vector encoding a word in the tar-
get language.
Note that this model is intrinsically bidirec-
tional since the individual RBMs are bidirectional
models and thus it is possible to transliterate from
source to target and vice versa.
</bodyText>
<subsectionHeader confidence="0.996391">
2.1 Source and Target Encoding
</subsectionHeader>
<bodyText confidence="0.99998875">
A problem with DBNs and transliteration is the
data representation. The input and output data are
commonly sequences of varying length but a DBN
expects input data of constant length. To repre-
sent a source or target language word, it is con-
verted into a sparse binary vector of dimensional-
ity DF = |F |· J or DE = |E |· I, respectively,
where |F |and |E |are the sizes of the alphabets
and I and J are the lengths of the longest words.
If a word is shorter than this, a padding letter w0
is used to fill the spaces. This encoding is depicted
in the bottom part of Figure 1.
Since the output vector of the DBN is not bi-
nary, we infer the maximum a posterior hypothe-
sis by selecting the letter with the highest output
value for each position.
</bodyText>
<subsectionHeader confidence="0.999257">
2.2 Training Method
</subsectionHeader>
<bodyText confidence="0.99866535">
For the training, we follow the method proposed
in (Hinton et al., 2006). To find a good starting
point for backpropagation on the whole network,
each of the RBMs is trained individually. First, we
learn the generative encoders for the source and
target words, i.e. the weights wS1 and wT1, respec-
tively. Therefore, each of the layers is trained as a
restricted Boltzmann machine, such that it learns
to generate the input vectors with high probability,
i.e. the weights are learned such that the data val-
ues have low values of the trained cost function.
After learning a layer, the activity vectors of the
hidden units, as obtained from the real training
data, are used as input data for the next layer. This
can be repeated to learn as many hidden layers as
desired. After learning multiple hidden layers in
this way, the whole network can be viewed as a
single, multi-layer generative model and each ad-
ditional hidden layer improves a lower bound on
the probability that the multi-layer model would
generate the training data (Hinton et al., 2006).
For each language, the output of the first layer is
used as input to learn the weights of the next lay-
ers wS2 and wT2. The same procedure is repeated
to learn wS3 and wT3. Note that so far no con-
nection between the individual letters in the two
alphabets is created but each encoder only learns
feature functions to represent the space of possi-
ble source and target words. Then, the weights
for the joined layer are learned using concatenated
outputs of the top layers of the source and target
encoders to find an initial set of weights wSJ and
wJT .
After each of the layers has been trained in-
dividually, backpropagation is performed on the
whole network to tune the weights and to learn the
connections between both languages. We use the
average squared error over the output vectors be-
tween reference and inferred words as the training
criterion. For the training, we split the training
</bodyText>
<page confidence="0.988096">
235
</page>
<bodyText confidence="0.999980538461539">
data into batches of 100 randomly selected words
and allow for 10 training iterations of the individ-
ual layers and up to 200 training iterations for the
backpropagation. Currently, we only optimise the
parameters for the source to target direction and
thus do not retain the bidirectionality1.
Thus, the whole training procedure consists of
4 phases. First, an autoencoder for the source
words is learnt. Second, an autoencoder for
the target words is learnt. Third, these autoen-
coders are connected by a top connecting layer,
and finally backpropagation is performed over the
whole network for fine-tuning of the weights.
</bodyText>
<subsectionHeader confidence="0.999966">
2.3 Creation of n-Best Lists
</subsectionHeader>
<bodyText confidence="0.980099791666667">
N-best lists are a common means for combination
of several systems in natural language processing
and for rescoring. In this section, we describe how
a set of hypotheses can be created for a given in-
put. Although these hypotheses are not n-best lists
because they have not been obtained from a search
process, they can be used similarly and can bet-
ter be compared to randomly sampled ‘good’ hy-
potheses from a full word-graph.
Since the values of the nodes in the individ-
ual layers are probabilities for this particular node
to be activated, it is possible to sample a set of
states from the distribution for the individual lay-
ers, which is called Gibbs sampling (Geman and
Geman, 1984). This sampling can be used to cre-
ate several hypotheses for a given input sentence,
and this set of hypotheses can be used similar to
an n-best list.
The layer in which the Gibbs sampling is done
can in principle be chosen arbitrarily. However,
we believe it is natural to sample in either the first
layer, the joint layer, or the last layer. Sampling in
the first layer leads to different features traversing
the full network. Sampling in the joint layer only
affects the generation of the target sentence, and
sampling in the last layer is equal to directly sam-
pling from the distribution of target hypotheses.
Conventional Gibbs sampling has a very strong
impact on the outcome of the network because the
smoothness of the distributions and the encoding
of similar matches is entirely lost. Therefore, we
use a weak variant of Gibbs sampling. Instead of
replacing the states’ probabilities with fully dis-
cretely sampled states, we keep the probabilities
1Note that it is easily possible to extend the backpropaga-
tion to include both directions, but to keep the computational
demands lower we decided to start with only one direction.
and add a fraction of a sampled state, effectively
modifying the probabilities to give a slightly bet-
ter score to the last sampled state. Let p be the
D-dimensional vector of probabilities for D nodes
in an RBM to be on. Normal Gibbs sampling
would sample a D-dimensional vector 5 contain-
ing a state for each node from this distribution.
Instead of replacing the vector p with 5, we use
p&apos; &lt;-- p + e5, leading to smoother changes than
conventional Gibbs sampling. This process can
easily be repeated to obtain multiple hypotheses.
</bodyText>
<sectionHeader confidence="0.998448" genericHeader="method">
3 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999989947368421">
In this section we present experimental results for
an Arabic-English transliteration task. For evalu-
ation we use the character error rate (CER) which
is the commonly used word error rate (WER) on
character level.
We use a corpus of 10,084 personal names in
Arabic and their transliterated English ASCII rep-
resentation (LDC corpus LDC2005G02). The
Arabic names are written in the usual way, i.e.
lacking vowels and diacritics. 1,000 names were
randomly sampled for development and evalua-
tion, respectively (Freitag and Khadivi, 2007).
The vocabulary of the source language is 33 and
the target language has 30 different characters
(including the padding character). The longest
word on both sides consists of 14 characters,
thus the feature vector on the source side is 462-
dimensional and the feature vector on the target
side is 420-dimensional.
</bodyText>
<subsectionHeader confidence="0.997625">
3.1 Network Structure
</subsectionHeader>
<bodyText confidence="0.999669882352941">
First, we evaluate how the structure of the network
should be chosen. For these experiments, we fixed
the numbers of layers and the size of the bottom
layers in the target and source encoder and evalu-
ate different network structures and the size of the
joint layer.
The experiments we performed are described in
Table 1. The top part of the table gives the results
for different network structures. We compare net-
works with increasing layer sizes, identical layer
sizes, and decreasing layer sizes. It can be seen
that decreasing layer sizes leads to the best results.
In these experiments, we choose the number of
nodes in the joint layer to be three times as large
as the topmost encoder layers.
In the bottom part, we kept most of the network
structure fixed and only vary the number of nodes
</bodyText>
<page confidence="0.998331">
236
</page>
<tableCaption confidence="0.7752285">
Table 1: Transliteration experiments using differ-
ent network structures.
</tableCaption>
<table confidence="0.978421125">
number of nodes CER [%]
51,T1 52,T2 53,T3 J train dev eval
400 500 600 1800 0.3 27.2 28.1
400 400 400 1200 0.7 26.1 25.2
400 350 300 900 1.8 25.1 24.3
400 350 300 1000 1.7 24.8 24.0
400 350 300 1500 1.3 24.1 22.7
400 350 300 2000 0.2 24.2 23.5
</table>
<figure confidence="0.9860904375">
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
train
dev
test
50 200 300 400 500 600 1000
network size
CER [%]
</figure>
<bodyText confidence="0.999736">
in the joint layer. Here, a small number of nodes
leads to suboptimal performance and a very high
number of nodes leads to overfitting which can be
seen in nearly perfect performance on the training
data and an increasing CER on the development
and eval data.
</bodyText>
<subsectionHeader confidence="0.993683">
3.2 Network Size
</subsectionHeader>
<bodyText confidence="0.9997519">
Next, we evaluate systems with different numbers
of nodes. Therefore, we start from the best param-
eters (400-350-300-1500) from the previous sec-
tion and scale the number of nodes in the individ-
ual layers by a certain factor, i.e. factor 1.5 leads
to (600-525-450-2250).
In Figure 2 and Table 2, the results from the
experimental evaluation on the transliteration task
are given. The network size denotes the number
of nodes in the bottom layers of the source and the
target encoder (i.e. 51 and T1) and the other layers
are chosen according to the results from the exper-
iments presented in the previous section.
The results show that small networks perform
badly, the optimal performance is reached with
medium sized networks of 400-600 nodes in the
bottom layers, and larger networks perform worse,
which is probably due to overfitting.
For comparison, we give results for a state-of-
the-art phrase-based MT system applied on the
character level with default system parameters (la-
belled as ‘PBT untuned’), and the same system,
where all scaling factors were tuned on dev data
(labelled as ‘PBT tuned’). The tuned phrase-based
MT system clearly outperforms our approach.
Additionally, we perform an experiment with
a standard multi-layer perceptron. Therefore, we
choose the network structure with 400-350-300-
1500 nodes, initialised these randomly and trained
the entire network with backpropagation training.
</bodyText>
<figureCaption confidence="0.7971565">
Figure 2: Results for the Arabic-English translit-
eration task depending on the network size.
</figureCaption>
<bodyText confidence="0.999850555555555">
The results (line ‘MLP-400’ in Table 2) of this ex-
periment are far worse than any of the other re-
sults, which shows that, apart from the convenient
theoretical interpretation, the creation of the DBN
as described is a suitable method to train the sys-
tem. The reason for the large difference is likely
the bad initialisation of the network and the fact
that the backpropagation algorithm gets stuck in a
local optimum at this point.
</bodyText>
<subsectionHeader confidence="0.999553">
3.3 Reordering capabilities
</subsectionHeader>
<bodyText confidence="0.999909538461538">
Although reordering is not an issue in transliter-
ation, the proposed model has certain properties
which we investigated and where interesting prop-
erties can be observed.
To investigate the performance under adverse
reordering conditions, we also perform an exper-
iment with reversed ordering of the target letters
(i.e. a word w = c1, c2, ... , cJ is now written
cJ, cJ_1, ... , c1). Since the DBN is fully sym-
metric, i.e. each input node is connected with each
output node in the same way and vice versa, the
DBN result is not changed except for some minor
numerical differences due to random initialisation.
Indeed, the DBN obtained is nearly identical ex-
cept for a changed ordering of the weights in the
joint layer, and if desired it is possible to construct
a DBN for reverse-order target language from a
fully trained DBN by permuting the weights.
On the same setup an experiment with our
phrase-based decoder has been performed and
here the performance is strongly decreased (bot-
tom line of Table 2). The phrase-based MT sys-
tem for this experiment used a reordering with
IBM block-limit constraints with distortion lim-
its and all default parameters were reasonably
tuned. We observed that the position-independent
</bodyText>
<page confidence="0.996991">
237
</page>
<tableCaption confidence="0.40631">
Table 2: Results for the Arabic-English translit-
eration task depending on the network size and a
comparison with state of the art results using con-
ventional phrase-based machine translation tech-
niques
</tableCaption>
<figure confidence="0.9487305">
network CER [%]
size
50
100
200
300
400
500
600
1000
MLP-400
untuned PBT
tuned PBT
(Freitag and Khadivi, 2007)
</figure>
<figureCaption confidence="0.591231">
reversed task: PBT
</figureCaption>
<bodyText confidence="0.9964638">
error rate of the phrase-based MT system is hardly
changed which also underlines that, in principle,
the phrase-based MT system is currently better but
that under adverse reordering conditions the DBN
system has some advantages.
</bodyText>
<subsectionHeader confidence="0.923952">
3.4 N-Best Lists
</subsectionHeader>
<bodyText confidence="0.99998025">
As described above, different possibilities to cre-
ate n-best lists exists. Starting from the system
with 400-350-300-1500 nodes, we evaluate the
creation of n-best lists in the first source layer, the
joint layer, and the last target layer. Therefore,
we create n best lists with up to 10 hypotheses
(sometimes, we have less due to duplicates after
sampling, on the average we have 8.3 hypotheses
per sequence), and evaluate the oracle error rate.
In Table 3 it can be observed that sampling in the
first layer leads to the best oracle error rates. The
baseline performance (first best) for this system is
24.1% CER on the development data, and 22.7%
CER on the eval data, which can be improved by
nearly 10% absolute using the oracle from a 10-
best list.
</bodyText>
<subsectionHeader confidence="0.847737">
3.5 Rescoring
</subsectionHeader>
<bodyText confidence="0.999896">
Using the n-best list sampled in the first source
layer, we also perform rescoring experiments.
Therefore, we rescore the transliteration hypothe-
</bodyText>
<tableCaption confidence="0.983769">
Table 3: Oracle character error rates on 10-best
</tableCaption>
<table confidence="0.9039647">
lists.
sampling layer oracle CER [%]
dev eval
51 15.8 14.8
joint layer 17.5 16.4
T1 18.7 18.2
CER [%]
System dev eval
DBN w/o rescoring 24.1 22.7
w/ rescoring 21.3 20.1
</table>
<tableCaption confidence="0.996302">
Table 4: Results from the rescoring experiments
</tableCaption>
<bodyText confidence="0.9624976">
and fusion with the phrase-based MT system.
ses (after truncating the padding letters w0) with
additional models, which are commonly used in
MT, and which we have trained on the training
data:
</bodyText>
<listItem confidence="0.679196666666667">
• IBM model 1 lexical probabilities modelling
the probability for a target sequence given a
source sequence
</listItem>
<table confidence="0.993317666666667">
hIBM1(fJ1 , eI1)=− log ⎛ ⎞
YJ
1 XI
⎝ p(fj|ei) ⎠
(I + 1)J
j=1 i=0
</table>
<listItem confidence="0.9820795">
• m-gram language model over the letter se-
quences
</listItem>
<equation confidence="0.959795">
hLM(eI1) = − log
</equation>
<bodyText confidence="0.928719692307692">
with m being the size of the m-gram, we
choose m = 9.
• sequence length model (commonly referred
to as word penalty).
Then, these models are fused in a log-linear model
(Och and Ney, 2002), and we tune the model scal-
ing factors discriminatively on the development n-
best list using the downhill simplex algorithm. Re-
sults from the rescoring experiments are given in
Table 4.
The performance of the DBN system is im-
proved on the dev data from 24.1% to 21.3% CER
and on the eval data from 22.7% to 20.1% CER.
</bodyText>
<table confidence="0.866200833333333">
train dev eval
35.8 43.7 43.6
26.4 36.3 35.8
5.8 25.2 24.3
3.9 24.3 24.4
1.3 24.1 22.7
1.2 22.9 22.8
1.0 24.1 22.6
0.2 26.6 24.4
22.0 64.1 63.2
4.9 23.3 23.6
2.2 12.9 13.3
n/a 11.1 11.1
13.0 35.2 35.7
p(ei|ei−1
i−m+1),
YI
i=1
</table>
<page confidence="0.990325">
238
</page>
<subsectionHeader confidence="0.9968315">
3.6 Application Within a System
Combination Framework
</subsectionHeader>
<bodyText confidence="0.998734797979798">
Although being clearly outperformed by the
phrase-based MT system, we applied the translit-
eration candidates generated by the DBN ap-
proach within a system combination framework.
Motivated by the fact that the DBN approach
differs decisively from the other statistical ap-
proaches we applied to the machine transliteration
task, we wanted to investigate the potential ben-
efit of the diverse nature of the DBN transliter-
ations. Taking the transliteration candidates ob-
tained from another study which was intended to
perform a comparison of various statistical ap-
proaches to the transliteration task, we performed
the system combination as is customary in speech
recognition, i.e. following the Recognizer Output
Voting Error Reduction (ROVER) approach (Fis-
cus, 1997).
The following methods were investigated:
(Monotone) Phrase-based MT on character level:
A state-of-the-art phrase-based SMT system
(Zens and Ney, 2004) was used for name
transliteration, i.e. translation of characters
instead of words. No reordering model
was employed due to the monotonicity
of the transliteration task, and the model
scaling factors were tuned on maximum
transliteration accuracy.
Data-driven grapheme-to-phoneme conversion:
In Grapheme-to-Phoneme conversion (G2P),
or phonetic transcription, we seek the most
likely pronunciation (phoneme sequence)
for a given orthographic form (sequence of
letters). Then, a grapheme-phoneme joint
multi-gram, or graphone for short, is a pair
of a letter sequence and a phoneme sequence
of possibly different length (Bisani and Ney,
2008). The model training is done in two
steps: First, maximum likelihood is used to
infer the graphones. Second, the input is
segmented into a stream of graphones and
absolute discounting with leaving-one-out
is applied to estimate the actual M-gram
model. Interpreting the characters of the
English target names as phonemes, we used
the G2P toolkit of (Bisani and Ney, 2008) to
transliterate the Arabic names.
Position-wise maximum entropy models / CRFs:
The segmentation as provided by the G2P
model is used and “null words” are inserted
such that the transliteration task can be
interpreted as a classical tagging task (e.g.
POS, conceptual tagging, etc.). This means
that we seek for a one-to-one mapping and
define feature functions to model the pos-
terior probability. Maximum entropy (ME)
models are defined position-wise, whereas
conditional random fields (CRFs) consider
full sequences. Both models were trained
according to the maximum class posterior
criterion. We used an ME tagger (Bender et
al., 2003) and the freely available CRF++
toolkit.2
Results for each of the individual systems and
different combinations are given in Table 5. As
expected, the DBN transliterations cannot keep up
with the other approaches. The additional models
(G2P, CRF and ME) perform slightly better than
the PBT method. If we look at combinations of
systems without the DBN approach, we observe
only marginal improvements of around 0.1-0.2%
CER. Interestingly, a combination of all 4 models
(PBT, G2P, ME, CRF) works as good as individual
3-way combinations (the same 11.9% on dev are
obtained). This can be interpreted as a potential
“similarity” of the approaches. Adding e.g. ME to
a combination of PBT, G2P and CRF does not im-
prove results because the transliteration hypothe-
ses are too similar. If we simply put together all
5 systems including DBN with equal weights, we
have a similar trend. Since all systems are equally
weighted and at least 3 of the systems are similar
in individual performance (G2P, ME, CRF have all
around 12% CER on the tested data sets), the DBN
approach does not get a large impact on overall
performance.
If we drop similar systems and tune for 3-way
combinations, we observe a large reduction in
CER if DBN comes into play. Compared to the
best individual system of 12% CER, we now ar-
rive at a CER of 10.9% for a combination of PBT,
CRF and DBN which is significantly better than
each of the individual methods. Our interpreta-
tion of this is that the DBN system has different
hypotheses compared to all other systems and that
the hypotheses from the other systems are too sim-
ilar to be apt for combination. So, although DBN
is much worse than the other approaches, it obvi-
ously helps in the system combination. Using the
rescored variant of the DBN transliterations from
</bodyText>
<footnote confidence="0.977855">
2http://crfpp.sourceforge.net/
</footnote>
<page confidence="0.987745">
239
</page>
<table confidence="0.999058272727273">
CER [%]
System dev eval
DBN 24.1 22.7
PBT 12.9 13.3
G2P 12.2 12.1
ME 12.3 12.4
CRF 12.0 12.0
ROVER 11.9 11.8
best setting w/o DBN
5-way equal weights 11.7 11.9
best setting w/ DBN 10.9 10.9
</table>
<tableCaption confidence="0.9962615">
Table 5: Results from the individual methods in-
vestigated versus ROVER combination.
</tableCaption>
<bodyText confidence="0.899929">
Section 3.5, performance is similar to the one ob-
tained for the DBN baseline.
</bodyText>
<sectionHeader confidence="0.998491" genericHeader="evaluation">
4 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999996464285714">
We have presented a novel method for machine
transliteration based on DBNs, which despite not
having competitive results can be an important ad-
ditional cue for system combination setups. The
DBN model has some immediate advantages: the
model is in principle fully bidirectional and is
based on sound and valid theories from machine
learning. Instead of common techniques which
are based on finite-state machines or phrase-based
machine translation, the proposed system does not
rely on word alignments and beam-search decod-
ing and has interesting properties regarding the re-
ordering of sequences. We have experimentally
evaluated the network structure and size, reorder-
ing capabilities, the creation of multiple hypothe-
ses, and rescoring and combination with other
transliteration approaches. It was shown that, al-
beit the approach cannot compete with the cur-
rent state of the art, deep belief networks might
be a learning framework with some potential for
transliteration. It was also shown that the pro-
posed method is suited for combination with dif-
ferent state-of-the-art systems and that improve-
ments over the single models can be obtained
in a ROVER-like setting. Furthermore, adding
DBN-based transliterations, although individually
far behind the other approaches, significantly im-
proves the overall results by 1% absolute.
</bodyText>
<sectionHeader confidence="0.75764" genericHeader="conclusions">
Outlook
</sectionHeader>
<bodyText confidence="0.999868888888889">
In the future we plan to investigate several details
of the proposed model: we will exploit the inher-
ent bidirectionality, further investigate the struc-
ture of the model, such as the number of layers
and the numbers of nodes in the individual lay-
ers. Also, it is important to improve the efficiency
of our implementation to allow for working on
larger datasets and obtain more competitive re-
sults. Furthermore, we are planning to investigate
convolutional input layers for transliteration and
use a translation approach analogous to the one
proposed by Collobert and Weston (2008) in or-
der to allow for the incorporation of reorderings,
language models, and to be able to work on larger
tasks.
Acknowledgement. We would like to thank Ge-
offrey Hinton for providing the Matlab Code ac-
companying (Hinton and Salakhutdinow, 2006).
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99990772972973">
D. Ackley, G. Hinton, and T. Sejnowski. 1985. A
learning algorithm for Boltzmann machines. Cog-
nitive Science, 9(1):147–169.
Y. Al-Onaizan and K. Knight. 2002. Machine
transliteration of names in Arabic text. In ACL
2002 Workshop on Computationaal Approaches to
Semitic Languages.
M. Asunci´on Casta˜no, F. Casacuberta, and E. Vidal.
1997. Machine translation using neural networks
and finite-state models. In Theoretical and Method-
ological Issues in Machine Translation (TMI), pages
160–167, Santa Fe, NM, USA, July.
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Sta-
tistical machine translation through global lexical
selection and sentence reconstruction. In Annual
Meeting of the Association for Computational Lin-
gustic (ACL), pages 152–159, Prague, Czech Repub-
lic.
O. Bender, F. J. Och, and H. Ney. 2003. Maxi-
mum entropy models for named entity recognition.
In Proc. 7th Conf. on Computational Natural Lan-
guage Learning (CoNLL), pages 148–151, Edmon-
ton, Canada, May.
M. Bisani and H. Ney. 2008. Joint-sequence models
for grapheme-to-phoneme conversion. Speech Com-
munication, 50(5):434–451, May.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, Helsinki, Finn-
land, July.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 347–354, Santa Barbara, CA, USA, Decem-
ber.
</reference>
<page confidence="0.94831">
240
</page>
<reference confidence="0.999858795454546">
D. Freitag and S. Khadivi. 2007. A sequence align-
ment model based on the averaged perceptron. In
Conference on Empirical methods in Natural Lan-
guage Processing, pages 238–247, Prague, Czech
Republic, June.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transaction on Pattern Analysis and
Machine Intelligence, 6(6):721–741, November.
G. Hinton and R. R. Salakhutdinow. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313:504–507, July.
G. Hinton, S. Osindero, and Y.-W. Teh. 2006. A
fast learning algorithm for deep belief nets. Neural
Computation, 18:1527–1554.
F. Huang, S. Vogel, and A. Waibel. 2004. Improving
named entity translation combining phonetic and se-
mantic similarities. In HLT-NAACL.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(2).
A. Mnih and G. Hinton. 2007. Three new graphical
models for statistical language modelling. In ICML
’07: International Conference on Machine Learn-
ing, pages 641–648, New York, NY, USA. ACM.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Annual Meeting of the As-
soc. for Computational Linguistics, pages 295–302,
Philadelphia, PA, USA, July.
I. Titov and J. Henderson. 2007. Constituent parsing
with incremental sigmoid belief networks. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 632–639,
Prague, Czech Republic, June.
A. Torralba, R. Fergus, and Y. Weiss. 2008. Small
codes and large image databases for recognition. In
IEEE Conference on Computer Vision and Pattern
Recognition, Anchorage, AK, USA, June.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: HLT-NAACL
2004, pages 257–264, Boston, MA, May.
</reference>
<page confidence="0.998105">
241
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.561965">
<title confidence="0.999924">A Deep Learning Approach to Machine Transliteration</title>
<author confidence="0.785157">Deselaers Hasan Bender Human Language Technology</author>
<author confidence="0.785157">Pattern Recognition Group</author>
<email confidence="0.994929"><surname>@cs.rwth-aachen.de</email>
<abstract confidence="0.998195277777778">In this paper we present a novel transliteration technique which is based on deep belief networks. Common approaches use finite state machines or other methods similar to conventional machine translation. Instead of using conventional NLP techniques, the approach presented here builds on deep belief networks, a technique which was shown to work well for other machine learning problems. We show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an Arabic-English transliteration task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Ackley</author>
<author>G Hinton</author>
<author>T Sejnowski</author>
</authors>
<title>A learning algorithm for Boltzmann machines.</title>
<date>1985</date>
<journal>Cognitive Science,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="5778" citStr="Ackley et al., 1985" startWordPosition="905" endWordPosition="908">ring. We evaluate the proposed methods on an Arabic-English transliteration task where Arabic city names have to be transcribed into the equivalent English spelling. 2 Deep Belief Networks for Transliteration Although DBNs are thoroughly described in the literature, e.g. (Hinton et al., 2006), we give a short overview on the ideas and techniques and introduce our notation. Deep architectures in machine learning and artificial intelligence are becoming more and more popular after an efficient training algorithm has been proposed (Hinton et al., 2006), although the idea is known for some years (Ackley et al., 1985). Deep belief networks consist of multiple layers of restricted Boltzmann machines (RBMs). It was shown that DBNs can be used for dimensionality reduction of images and text documents (Hinton and Salakhutdinow, 2006) and for language modelling (Mnih and Hinton, 2007). Recently, DBNs were also used successfully in image retrieval to create very compact but meaningful representations of a huge set of images (nearly 13 million) for retrieval (Torralba et al., 2008). DBNs are built from RBMs by first training an RBM on the input data. A second RBM is built on the output of the first one and so on </context>
<context position="8352" citStr="Ackley et al., 1985" startWordPosition="1342" endWordPosition="1345">ng layer as input. The first layers in the source and target encoders consist of 51 and T1 neurons, respectively; the second layers have 52 and T2 nodes, and the third layers have 53 and T3 nodes, respectively. A joint layer with J nodes connects the source and the target encoders. Here, the number of nodes in the individual layers are the most important parameters. The more 234 nodes a layer has, the more information can be conveyed through it, but the harder the training: the amount of data needed for training and thus the computation time required is exponential in the size of the network (Ackley et al., 1985). To transliterate a source word, it is first encoded as a DF-dimensional binary vector 5F (cf. Section 2.1) and then fed into the first layer of the source encoder. The 51-dimensional output vector OS1 of the first layer is computed as OS1 ← 1/ exp (1 + wS15F + bS1) , (1) where wS1 is a 51 × DF-dimensional weight matrix and bS1 is an 51-dimensional bias vector. The output of each layer is used as input to the next layer as follows: OS2 ← 1/ exp (1 + wS2OS1 + bS2) , (2) OS3 ← 1/ exp (1 + wS3OS2 + bS3) . (3) After the source encoder has been traversed, the joint layer is reached which processes</context>
</contexts>
<marker>Ackley, Hinton, Sejnowski, 1985</marker>
<rawString>D. Ackley, G. Hinton, and T. Sejnowski. 1985. A learning algorithm for Boltzmann machines. Cognitive Science, 9(1):147–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Machine transliteration of names in Arabic text.</title>
<date>2002</date>
<booktitle>In ACL 2002 Workshop on Computationaal Approaches to Semitic Languages.</booktitle>
<contexts>
<context position="1929" citStr="Al-Onaizan and Knight, 2002" startWordPosition="286" endWordPosition="289"> Transliteration can in principle be considered to be a small-scale translation problem and, thus, some ideas presented here can be transferred to the machine translation domain as well. Transliteration has been in use in machine translation systems, e.g. Russian-English, since the existence of the field of machine translation. However, to our knowledge it was first studied as a machine learning problem by Knight and Graehl (1998) using probabilistic finite-state transducers. Subsequently, the performance of this system was greatly improved by combining different spelling and phonetic models (Al-Onaizan and Knight, 2002). Huang et al. (2004) construct a probabilistic Chinese-English edit model as part of a larger alignment solution using a heuristic bootstrapped procedure. Freitag and Khadivi (2007) propose a technique which combines conventional MT methods with a single layer perceptron. In contrast to these methods which strongly build on top of well-established natural language processing (NLP) techniques, we propose an alternative model. Our new model is based on deep belief networks which have been shown to work well in other machine learning and pattern recognition areas (cf. Section 2). Since translati</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Y. Al-Onaizan and K. Knight. 2002. Machine transliteration of names in Arabic text. In ACL 2002 Workshop on Computationaal Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Asunci´on Casta˜no</author>
<author>F Casacuberta</author>
<author>E Vidal</author>
</authors>
<title>Machine translation using neural networks and finite-state models.</title>
<date>1997</date>
<booktitle>In Theoretical and Methodological Issues in Machine Translation (TMI),</booktitle>
<pages>160--167</pages>
<location>Santa Fe, NM, USA,</location>
<marker>Casta˜no, Casacuberta, Vidal, 1997</marker>
<rawString>M. Asunci´on Casta˜no, F. Casacuberta, and E. Vidal. 1997. Machine translation using neural networks and finite-state models. In Theoretical and Methodological Issues in Machine Translation (TMI), pages 160–167, Santa Fe, NM, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>P Haffner</author>
<author>S Kanthak</author>
</authors>
<title>Statistical machine translation through global lexical selection and sentence reconstruction.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Lingustic (ACL),</booktitle>
<pages>152--159</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4843" citStr="Bangalore et al. (2007)" startWordPosition="750" endWordPosition="753">he encoding and the Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 233–241, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 233 processing is done differently. First, we learn two independent generative models, one for the source input and one for the target output. Then, these two models are combined into a source-to-target encoding/decoding system (cf. Section 2). Regarding that the target is generated and not searched in a space of hypotheses (e.g. in a word graph), our approach is similar to the approach presented by Bangalore et al. (2007) who present an MT system where the set of words of the target sentence is generated based on the full source sentence and then a finite-state approach is used to reorder the words. Opposed to this approach we do not only generate the letters/words in the target sentence but we generate the full sentence with ordering. We evaluate the proposed methods on an Arabic-English transliteration task where Arabic city names have to be transcribed into the equivalent English spelling. 2 Deep Belief Networks for Transliteration Although DBNs are thoroughly described in the literature, e.g. (Hinton et al</context>
</contexts>
<marker>Bangalore, Haffner, Kanthak, 2007</marker>
<rawString>S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statistical machine translation through global lexical selection and sentence reconstruction. In Annual Meeting of the Association for Computational Lingustic (ACL), pages 152–159, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bender</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Maximum entropy models for named entity recognition.</title>
<date>2003</date>
<booktitle>In Proc. 7th Conf. on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>148--151</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="26516" citStr="Bender et al., 2003" startWordPosition="4447" endWordPosition="4450">he Arabic names. Position-wise maximum entropy models / CRFs: The segmentation as provided by the G2P model is used and “null words” are inserted such that the transliteration task can be interpreted as a classical tagging task (e.g. POS, conceptual tagging, etc.). This means that we seek for a one-to-one mapping and define feature functions to model the posterior probability. Maximum entropy (ME) models are defined position-wise, whereas conditional random fields (CRFs) consider full sequences. Both models were trained according to the maximum class posterior criterion. We used an ME tagger (Bender et al., 2003) and the freely available CRF++ toolkit.2 Results for each of the individual systems and different combinations are given in Table 5. As expected, the DBN transliterations cannot keep up with the other approaches. The additional models (G2P, CRF and ME) perform slightly better than the PBT method. If we look at combinations of systems without the DBN approach, we observe only marginal improvements of around 0.1-0.2% CER. Interestingly, a combination of all 4 models (PBT, G2P, ME, CRF) works as good as individual 3-way combinations (the same 11.9% on dev are obtained). This can be interpreted a</context>
</contexts>
<marker>Bender, Och, Ney, 2003</marker>
<rawString>O. Bender, F. J. Och, and H. Ney. 2003. Maximum entropy models for named entity recognition. In Proc. 7th Conf. on Computational Natural Language Learning (CoNLL), pages 148–151, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bisani</author>
<author>H Ney</author>
</authors>
<title>Joint-sequence models for grapheme-to-phoneme conversion.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>5</issue>
<contexts>
<context position="25506" citStr="Bisani and Ney, 2008" startWordPosition="4289" endWordPosition="4292">ansliteration, i.e. translation of characters instead of words. No reordering model was employed due to the monotonicity of the transliteration task, and the model scaling factors were tuned on maximum transliteration accuracy. Data-driven grapheme-to-phoneme conversion: In Grapheme-to-Phoneme conversion (G2P), or phonetic transcription, we seek the most likely pronunciation (phoneme sequence) for a given orthographic form (sequence of letters). Then, a grapheme-phoneme joint multi-gram, or graphone for short, is a pair of a letter sequence and a phoneme sequence of possibly different length (Bisani and Ney, 2008). The model training is done in two steps: First, maximum likelihood is used to infer the graphones. Second, the input is segmented into a stream of graphones and absolute discounting with leaving-one-out is applied to estimate the actual M-gram model. Interpreting the characters of the English target names as phonemes, we used the G2P toolkit of (Bisani and Ney, 2008) to transliterate the Arabic names. Position-wise maximum entropy models / CRFs: The segmentation as provided by the G2P model is used and “null words” are inserted such that the transliteration task can be interpreted as a class</context>
</contexts>
<marker>Bisani, Ney, 2008</marker>
<rawString>M. Bisani and H. Ney. 2008. Joint-sequence models for grapheme-to-phoneme conversion. Speech Communication, 50(5):434–451, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning,</booktitle>
<location>Helsinki, Finnland,</location>
<contexts>
<context position="3891" citStr="Collobert and Weston (2008)" startWordPosition="597" endWordPosition="600">ed representational power, and may be unable to learn the relationships between the source and the target language. The DBN approach alleviates some of the problems that commonly occur when working with neural networks: 1. they allow for efficient training due to a good initialisation of the individual layers. 2. Overfitting problems are addressed by creating generative models which are later refined discriminatively. 3. The network structure is clearly defined and only a few structure parameters have to be set. 4. DBNs can be interpreted as Bayesian probabilistic generative models. Recently, Collobert and Weston (2008) proposed a technique which applies a convolutional DBN to a multi-task learning NLP problem. Their approach is able to address POS tagging, chunking, named entity tagging, semantic role and similar word identification in one model. Our model is similar to this approach in that it uses the same machine learning techniques but the encoding and the Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 233–241, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 233 processing is done differently. First, we learn two independent genera</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, Helsinki, Finnland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER).</title>
<date>1997</date>
<booktitle>In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),</booktitle>
<pages>347--354</pages>
<location>Santa Barbara, CA, USA,</location>
<contexts>
<context position="24711" citStr="Fiscus, 1997" startWordPosition="4180" endWordPosition="4182">stem combination framework. Motivated by the fact that the DBN approach differs decisively from the other statistical approaches we applied to the machine transliteration task, we wanted to investigate the potential benefit of the diverse nature of the DBN transliterations. Taking the transliteration candidates obtained from another study which was intended to perform a comparison of various statistical approaches to the transliteration task, we performed the system combination as is customary in speech recognition, i.e. following the Recognizer Output Voting Error Reduction (ROVER) approach (Fiscus, 1997). The following methods were investigated: (Monotone) Phrase-based MT on character level: A state-of-the-art phrase-based SMT system (Zens and Ney, 2004) was used for name transliteration, i.e. translation of characters instead of words. No reordering model was employed due to the monotonicity of the transliteration task, and the model scaling factors were tuned on maximum transliteration accuracy. Data-driven grapheme-to-phoneme conversion: In Grapheme-to-Phoneme conversion (G2P), or phonetic transcription, we seek the most likely pronunciation (phoneme sequence) for a given orthographic form</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>J. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER). In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 347–354, Santa Barbara, CA, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
<author>S Khadivi</author>
</authors>
<title>A sequence alignment model based on the averaged perceptron.</title>
<date>2007</date>
<booktitle>In Conference on Empirical methods in Natural Language Processing,</booktitle>
<pages>238--247</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2111" citStr="Freitag and Khadivi (2007)" startWordPosition="314" endWordPosition="317">l. Transliteration has been in use in machine translation systems, e.g. Russian-English, since the existence of the field of machine translation. However, to our knowledge it was first studied as a machine learning problem by Knight and Graehl (1998) using probabilistic finite-state transducers. Subsequently, the performance of this system was greatly improved by combining different spelling and phonetic models (Al-Onaizan and Knight, 2002). Huang et al. (2004) construct a probabilistic Chinese-English edit model as part of a larger alignment solution using a heuristic bootstrapped procedure. Freitag and Khadivi (2007) propose a technique which combines conventional MT methods with a single layer perceptron. In contrast to these methods which strongly build on top of well-established natural language processing (NLP) techniques, we propose an alternative model. Our new model is based on deep belief networks which have been shown to work well in other machine learning and pattern recognition areas (cf. Section 2). Since translation and transliteration are closely related and transliteration can be considered a translation problem on the character level, we discuss various methods from both domains which are </context>
<context position="15939" citStr="Freitag and Khadivi, 2007" startWordPosition="2690" endWordPosition="2693">This process can easily be repeated to obtain multiple hypotheses. 3 Experimental Evaluation In this section we present experimental results for an Arabic-English transliteration task. For evaluation we use the character error rate (CER) which is the commonly used word error rate (WER) on character level. We use a corpus of 10,084 personal names in Arabic and their transliterated English ASCII representation (LDC corpus LDC2005G02). The Arabic names are written in the usual way, i.e. lacking vowels and diacritics. 1,000 names were randomly sampled for development and evaluation, respectively (Freitag and Khadivi, 2007). The vocabulary of the source language is 33 and the target language has 30 different characters (including the padding character). The longest word on both sides consists of 14 characters, thus the feature vector on the source side is 462- dimensional and the feature vector on the target side is 420-dimensional. 3.1 Network Structure First, we evaluate how the structure of the network should be chosen. For these experiments, we fixed the numbers of layers and the size of the bottom layers in the target and source encoder and evaluate different network structures and the size of the joint lay</context>
<context position="21277" citStr="Freitag and Khadivi, 2007" startWordPosition="3594" endWordPosition="3597">coder has been performed and here the performance is strongly decreased (bottom line of Table 2). The phrase-based MT system for this experiment used a reordering with IBM block-limit constraints with distortion limits and all default parameters were reasonably tuned. We observed that the position-independent 237 Table 2: Results for the Arabic-English transliteration task depending on the network size and a comparison with state of the art results using conventional phrase-based machine translation techniques network CER [%] size 50 100 200 300 400 500 600 1000 MLP-400 untuned PBT tuned PBT (Freitag and Khadivi, 2007) reversed task: PBT error rate of the phrase-based MT system is hardly changed which also underlines that, in principle, the phrase-based MT system is currently better but that under adverse reordering conditions the DBN system has some advantages. 3.4 N-Best Lists As described above, different possibilities to create n-best lists exists. Starting from the system with 400-350-300-1500 nodes, we evaluate the creation of n-best lists in the first source layer, the joint layer, and the last target layer. Therefore, we create n best lists with up to 10 hypotheses (sometimes, we have less due to du</context>
</contexts>
<marker>Freitag, Khadivi, 2007</marker>
<rawString>D. Freitag and S. Khadivi. 2007. A sequence alignment model based on the averaged perceptron. In Conference on Empirical methods in Natural Language Processing, pages 238–247, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transaction on Pattern Analysis and Machine Intelligence,</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="13711" citStr="Geman and Geman, 1984" startWordPosition="2320" endWordPosition="2323">ems in natural language processing and for rescoring. In this section, we describe how a set of hypotheses can be created for a given input. Although these hypotheses are not n-best lists because they have not been obtained from a search process, they can be used similarly and can better be compared to randomly sampled ‘good’ hypotheses from a full word-graph. Since the values of the nodes in the individual layers are probabilities for this particular node to be activated, it is possible to sample a set of states from the distribution for the individual layers, which is called Gibbs sampling (Geman and Geman, 1984). This sampling can be used to create several hypotheses for a given input sentence, and this set of hypotheses can be used similar to an n-best list. The layer in which the Gibbs sampling is done can in principle be chosen arbitrarily. However, we believe it is natural to sample in either the first layer, the joint layer, or the last layer. Sampling in the first layer leads to different features traversing the full network. Sampling in the joint layer only affects the generation of the target sentence, and sampling in the last layer is equal to directly sampling from the distribution of targe</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transaction on Pattern Analysis and Machine Intelligence, 6(6):721–741, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
<author>R R Salakhutdinow</author>
</authors>
<title>Reducing the dimensionality of data with neural networks.</title>
<date>2006</date>
<journal>Science,</journal>
<pages>313--504</pages>
<contexts>
<context position="5994" citStr="Hinton and Salakhutdinow, 2006" startWordPosition="937" endWordPosition="940">ion Although DBNs are thoroughly described in the literature, e.g. (Hinton et al., 2006), we give a short overview on the ideas and techniques and introduce our notation. Deep architectures in machine learning and artificial intelligence are becoming more and more popular after an efficient training algorithm has been proposed (Hinton et al., 2006), although the idea is known for some years (Ackley et al., 1985). Deep belief networks consist of multiple layers of restricted Boltzmann machines (RBMs). It was shown that DBNs can be used for dimensionality reduction of images and text documents (Hinton and Salakhutdinow, 2006) and for language modelling (Mnih and Hinton, 2007). Recently, DBNs were also used successfully in image retrieval to create very compact but meaningful representations of a huge set of images (nearly 13 million) for retrieval (Torralba et al., 2008). DBNs are built from RBMs by first training an RBM on the input data. A second RBM is built on the output of the first one and so on until a sufficiently deep architecture is created. RBMs are stochastic generative artificial neural networks with restricted connectivity. From a theoretical viewpoint, RBMs are interesting because they are able to d</context>
</contexts>
<marker>Hinton, Salakhutdinow, 2006</marker>
<rawString>G. Hinton and R. R. Salakhutdinow. 2006. Reducing the dimensionality of data with neural networks. Science, 313:504–507, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
<author>S Osindero</author>
<author>Y-W Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<journal>Neural Computation,</journal>
<pages>18--1527</pages>
<contexts>
<context position="5451" citStr="Hinton et al., 2006" startWordPosition="851" endWordPosition="854">t al. (2007) who present an MT system where the set of words of the target sentence is generated based on the full source sentence and then a finite-state approach is used to reorder the words. Opposed to this approach we do not only generate the letters/words in the target sentence but we generate the full sentence with ordering. We evaluate the proposed methods on an Arabic-English transliteration task where Arabic city names have to be transcribed into the equivalent English spelling. 2 Deep Belief Networks for Transliteration Although DBNs are thoroughly described in the literature, e.g. (Hinton et al., 2006), we give a short overview on the ideas and techniques and introduce our notation. Deep architectures in machine learning and artificial intelligence are becoming more and more popular after an efficient training algorithm has been proposed (Hinton et al., 2006), although the idea is known for some years (Ackley et al., 1985). Deep belief networks consist of multiple layers of restricted Boltzmann machines (RBMs). It was shown that DBNs can be used for dimensionality reduction of images and text documents (Hinton and Salakhutdinow, 2006) and for language modelling (Mnih and Hinton, 2007). Rece</context>
<context position="7267" citStr="Hinton et al., 2006" startWordPosition="1150" endWordPosition="1153">s in data (Ackley et al., 1985). Figure 1: A schematic representation of our DBN for transliteration. Hinton and Salakhutdinow (2006) present a deep belief network to learn a tiny representation of its inputs and to reconstruct the input with high accuracy which is demonstrated for images and textual documents. Here, we use DBNs similarly: first, we learn encoders for the source and target words respectively and then connect these two through a joint layer to map between the two languages. This joint layer is trained in the same way as the top-level neurons in the deep belief classifier from (Hinton et al., 2006). In Figure 1, a schematic view of our DBN for transliteration is shown. On the left and on the right are encoders for the source and target words respectively. To transliterate a source word, it is passed through the layers of the network. First, it traverses through the source encoder on the left, then it passes into the joint layer, finally traversing down through the target encoder. Each layer consists of a set of neurons receiving the output of the preceding layer as input. The first layers in the source and target encoders consist of 51 and T1 neurons, respectively; the second layers hav</context>
<context position="10499" citStr="Hinton et al., 2006" startWordPosition="1769" endWordPosition="1772">et language word, it is converted into a sparse binary vector of dimensionality DF = |F |· J or DE = |E |· I, respectively, where |F |and |E |are the sizes of the alphabets and I and J are the lengths of the longest words. If a word is shorter than this, a padding letter w0 is used to fill the spaces. This encoding is depicted in the bottom part of Figure 1. Since the output vector of the DBN is not binary, we infer the maximum a posterior hypothesis by selecting the letter with the highest output value for each position. 2.2 Training Method For the training, we follow the method proposed in (Hinton et al., 2006). To find a good starting point for backpropagation on the whole network, each of the RBMs is trained individually. First, we learn the generative encoders for the source and target words, i.e. the weights wS1 and wT1, respectively. Therefore, each of the layers is trained as a restricted Boltzmann machine, such that it learns to generate the input vectors with high probability, i.e. the weights are learned such that the data values have low values of the trained cost function. After learning a layer, the activity vectors of the hidden units, as obtained from the real training data, are used a</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>G. Hinton, S. Osindero, and Y.-W. Teh. 2006. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Improving named entity translation combining phonetic and semantic similarities.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="1950" citStr="Huang et al. (2004)" startWordPosition="290" endWordPosition="293">ple be considered to be a small-scale translation problem and, thus, some ideas presented here can be transferred to the machine translation domain as well. Transliteration has been in use in machine translation systems, e.g. Russian-English, since the existence of the field of machine translation. However, to our knowledge it was first studied as a machine learning problem by Knight and Graehl (1998) using probabilistic finite-state transducers. Subsequently, the performance of this system was greatly improved by combining different spelling and phonetic models (Al-Onaizan and Knight, 2002). Huang et al. (2004) construct a probabilistic Chinese-English edit model as part of a larger alignment solution using a heuristic bootstrapped procedure. Freitag and Khadivi (2007) propose a technique which combines conventional MT methods with a single layer perceptron. In contrast to these methods which strongly build on top of well-established natural language processing (NLP) techniques, we propose an alternative model. Our new model is based on deep belief networks which have been shown to work well in other machine learning and pattern recognition areas (cf. Section 2). Since translation and transliteratio</context>
</contexts>
<marker>Huang, Vogel, Waibel, 2004</marker>
<rawString>F. Huang, S. Vogel, and A. Waibel. 2004. Improving named entity translation combining phonetic and semantic similarities. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="1735" citStr="Knight and Graehl (1998)" startWordPosition="262" endWordPosition="265">lation (MT) in order to obtain high quality output. We present a new technique for transliteration which is based on deep belief networks (DBNs), a well studied approach in machine learning. Transliteration can in principle be considered to be a small-scale translation problem and, thus, some ideas presented here can be transferred to the machine translation domain as well. Transliteration has been in use in machine translation systems, e.g. Russian-English, since the existence of the field of machine translation. However, to our knowledge it was first studied as a machine learning problem by Knight and Graehl (1998) using probabilistic finite-state transducers. Subsequently, the performance of this system was greatly improved by combining different spelling and phonetic models (Al-Onaizan and Knight, 2002). Huang et al. (2004) construct a probabilistic Chinese-English edit model as part of a larger alignment solution using a heuristic bootstrapped procedure. Freitag and Khadivi (2007) propose a technique which combines conventional MT methods with a single layer perceptron. In contrast to these methods which strongly build on top of well-established natural language processing (NLP) techniques, we propos</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>K. Knight and J. Graehl. 1998. Machine transliteration. Computational Linguistics, 24(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In ICML ’07: International Conference on Machine Learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6045" citStr="Mnih and Hinton, 2007" startWordPosition="946" endWordPosition="949">, e.g. (Hinton et al., 2006), we give a short overview on the ideas and techniques and introduce our notation. Deep architectures in machine learning and artificial intelligence are becoming more and more popular after an efficient training algorithm has been proposed (Hinton et al., 2006), although the idea is known for some years (Ackley et al., 1985). Deep belief networks consist of multiple layers of restricted Boltzmann machines (RBMs). It was shown that DBNs can be used for dimensionality reduction of images and text documents (Hinton and Salakhutdinow, 2006) and for language modelling (Mnih and Hinton, 2007). Recently, DBNs were also used successfully in image retrieval to create very compact but meaningful representations of a huge set of images (nearly 13 million) for retrieval (Torralba et al., 2008). DBNs are built from RBMs by first training an RBM on the input data. A second RBM is built on the output of the first one and so on until a sufficiently deep architecture is created. RBMs are stochastic generative artificial neural networks with restricted connectivity. From a theoretical viewpoint, RBMs are interesting because they are able to discover complex regularities and find notable featu</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>A. Mnih and G. Hinton. 2007. Three new graphical models for statistical language modelling. In ICML ’07: International Conference on Machine Learning, pages 641–648, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="23346" citStr="Och and Ney, 2002" startWordPosition="3951" endWordPosition="3954"> fusion with the phrase-based MT system. ses (after truncating the padding letters w0) with additional models, which are commonly used in MT, and which we have trained on the training data: • IBM model 1 lexical probabilities modelling the probability for a target sequence given a source sequence hIBM1(fJ1 , eI1)=− log ⎛ ⎞ YJ 1 XI ⎝ p(fj|ei) ⎠ (I + 1)J j=1 i=0 • m-gram language model over the letter sequences hLM(eI1) = − log with m being the size of the m-gram, we choose m = 9. • sequence length model (commonly referred to as word penalty). Then, these models are fused in a log-linear model (Och and Ney, 2002), and we tune the model scaling factors discriminatively on the development nbest list using the downhill simplex algorithm. Results from the rescoring experiments are given in Table 4. The performance of the DBN system is improved on the dev data from 24.1% to 21.3% CER and on the eval data from 22.7% to 20.1% CER. train dev eval 35.8 43.7 43.6 26.4 36.3 35.8 5.8 25.2 24.3 3.9 24.3 24.4 1.3 24.1 22.7 1.2 22.9 22.8 1.0 24.1 22.6 0.2 26.6 24.4 22.0 64.1 63.2 4.9 23.3 23.6 2.2 12.9 13.3 n/a 11.1 11.1 13.0 35.2 35.7 p(ei|ei−1 i−m+1), YI i=1 238 3.6 Application Within a System Combination Framewor</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Annual Meeting of the Assoc. for Computational Linguistics, pages 295–302, Philadelphia, PA, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>Constituent parsing with incremental sigmoid belief networks.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>632--639</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2927" citStr="Titov and Henderson, 2007" startWordPosition="441" endWordPosition="444">essing (NLP) techniques, we propose an alternative model. Our new model is based on deep belief networks which have been shown to work well in other machine learning and pattern recognition areas (cf. Section 2). Since translation and transliteration are closely related and transliteration can be considered a translation problem on the character level, we discuss various methods from both domains which are related to the proposed approach in the following. Neural networks have been used in NLP in the past, e.g. for machine translation (Asunci´on Casta˜no et al., 1997) and constituent parsing (Titov and Henderson, 2007). However, it might not be straight-forward to obtain good results using neural networks in this domain. In general, when training a neural network, one has to choose the structure of the neural network which involves certain trade-offs. If a small network with no hidden layer is chosen, it can be efficiently trained but has very limited representational power, and may be unable to learn the relationships between the source and the target language. The DBN approach alleviates some of the problems that commonly occur when working with neural networks: 1. they allow for efficient training due to</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>I. Titov and J. Henderson. 2007. Constituent parsing with incremental sigmoid belief networks. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 632–639, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Torralba</author>
<author>R Fergus</author>
<author>Y Weiss</author>
</authors>
<title>Small codes and large image databases for recognition.</title>
<date>2008</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<location>Anchorage, AK, USA,</location>
<contexts>
<context position="6244" citStr="Torralba et al., 2008" startWordPosition="978" endWordPosition="981"> more popular after an efficient training algorithm has been proposed (Hinton et al., 2006), although the idea is known for some years (Ackley et al., 1985). Deep belief networks consist of multiple layers of restricted Boltzmann machines (RBMs). It was shown that DBNs can be used for dimensionality reduction of images and text documents (Hinton and Salakhutdinow, 2006) and for language modelling (Mnih and Hinton, 2007). Recently, DBNs were also used successfully in image retrieval to create very compact but meaningful representations of a huge set of images (nearly 13 million) for retrieval (Torralba et al., 2008). DBNs are built from RBMs by first training an RBM on the input data. A second RBM is built on the output of the first one and so on until a sufficiently deep architecture is created. RBMs are stochastic generative artificial neural networks with restricted connectivity. From a theoretical viewpoint, RBMs are interesting because they are able to discover complex regularities and find notable features in data (Ackley et al., 1985). Figure 1: A schematic representation of our DBN for transliteration. Hinton and Salakhutdinow (2006) present a deep belief network to learn a tiny representation of</context>
</contexts>
<marker>Torralba, Fergus, Weiss, 2008</marker>
<rawString>A. Torralba, R. Fergus, and Y. Weiss. 2008. Small codes and large image databases for recognition. In IEEE Conference on Computer Vision and Pattern Recognition, Anchorage, AK, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improvements in phrasebased statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL</booktitle>
<pages>257--264</pages>
<location>Boston, MA,</location>
<contexts>
<context position="24864" citStr="Zens and Ney, 2004" startWordPosition="4199" endWordPosition="4202">machine transliteration task, we wanted to investigate the potential benefit of the diverse nature of the DBN transliterations. Taking the transliteration candidates obtained from another study which was intended to perform a comparison of various statistical approaches to the transliteration task, we performed the system combination as is customary in speech recognition, i.e. following the Recognizer Output Voting Error Reduction (ROVER) approach (Fiscus, 1997). The following methods were investigated: (Monotone) Phrase-based MT on character level: A state-of-the-art phrase-based SMT system (Zens and Ney, 2004) was used for name transliteration, i.e. translation of characters instead of words. No reordering model was employed due to the monotonicity of the transliteration task, and the model scaling factors were tuned on maximum transliteration accuracy. Data-driven grapheme-to-phoneme conversion: In Grapheme-to-Phoneme conversion (G2P), or phonetic transcription, we seek the most likely pronunciation (phoneme sequence) for a given orthographic form (sequence of letters). Then, a grapheme-phoneme joint multi-gram, or graphone for short, is a pair of a letter sequence and a phoneme sequence of possib</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>R. Zens and H. Ney. 2004. Improvements in phrasebased statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 257–264, Boston, MA, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>