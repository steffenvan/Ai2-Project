<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.96218">
Weighted parsing of trees
</title>
<author confidence="0.972168">
Mark-Jan Nederhof
</author>
<affiliation confidence="0.735164">
School of Computer Science, University of St Andrews
North Haugh, St Andrews, KY16 9SX, Scotland
</affiliation>
<sectionHeader confidence="0.967076" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999764428571429">
We show how parsing of trees can be for-
malized in terms of the intersection of two
tree languages. The focus is on weighted
regular tree grammars and weighted tree
adjoining grammars. Potential applica-
tions are discussed, such as parameter es-
timation across formalisms.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966732394367">
In parsing theory, strings and trees traditionally
have had a very different status. Whereas strings
in general receive the central focus, the trees in-
volved in derivations of strings are often seen as
auxiliary concepts at best. Theorems tend to be
about the power of grammatical formalisms to
produce strings (weak generative power) rather
than trees (strong generative power).
This can be explained by looking at typical
applications of parsing. In compiler construc-
tion for example, one distinguishes between parse
trees and (abstract) syntax trees, the former being
shaped according to a grammar that is massaged
to make it satisfy relatively artificial constraints,
e.g. that of LALR(1), which is required by many
compiler generators (Aho et al., 2007). The form
of syntax trees is often chosen to simplify phases
of semantic processing that follow parsing. As
the machinery used in such processing is generally
powerful, this offers much flexibility in the choice
of the exact shape and labelling of syntax trees, as
intermediate form between parsing and semantic
analysis.
In the study of natural languages, parse trees
have played a more important role. Whereas lin-
guistic utterances are directly observable and trees
deriving them are not, there are nevertheless tradi-
tions within linguistics that would see one struc-
tural analysis of a sentence as strongly preferred
over another. Furthermore, within computational
linguistics there are empirical arguments to claim
certain parses are correct and others are incorrect.
For example, a question answering systems may
verifiably give the wrong answer if the question
is parsed incorrectly. See (Jurafsky and Martin,
2000) for general discussion on the role of parsing
in NLP.
Despite the relative importance of strong gen-
erative power in computational linguistics, there
is still much freedom in how exactly parse trees
are shaped and how vertices are labelled, due to
the power of semantic analysis that typically fol-
lows parsing. This has affected much of the the-
oretical investigations into the power of linguistic
formalisms, and where strong equivalence is con-
sidered at all, it is often ”modulo relabelling” or
allowing minor structural changes.
With the advent of syntax-based machine trans-
lation, trees have however gained much impor-
tance, and are even considered as the main ob-
jects of study. This is because many MT mod-
ules have trees both as input and output, which
means the computational strength of such mod-
ules can be measured only in terms of the tree lan-
guages they accept and the transductions between
tree languages that they implement. See for exam-
ple (Knight, 2007).
In contrast, trees have always been the central
issue in an important and well-established subfield
of formal language theory that studies tree lan-
guages, tree automata and tree transducers (Gc-
seg and Steinby, 1997). The string languages gen-
erated by the relevant formalisms in this context
are mostly taken to be of secondary importance, if
they are considered at all.
This paper focuses on tree languages, but in-
volves a technique that was devised for string lan-
guages, and shows how the technique carries over
to tree languages. The original technique can be
seen as the most fundamental idea in the field of
context-free parsing, as it captures the essence of
</bodyText>
<page confidence="0.99151">
13
</page>
<note confidence="0.8773665">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 13–24,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999914076923077">
finding hierarchical structure in a linear sequence.
The generalization also finds structure in a lin-
ear sequence, but now the sequence corresponds
to paths in trees each leading down from a vertex
to a leaf. This means that the proposed type of
parsing is orthogonal to the conventional parsing
of strings.
The insights this offers have the potential to cre-
ate new avenues of research into the relation be-
tween formalisms that were until now considered
only in isolation. We seek credence to this claim
by investigating how probability distributions can
be carried over from tree adjoining grammars to
regular tree grammars, and vice versa.
The implication that the class of tree languages
of tree adjoining grammars (TAGs) is closed under
intersection with regular tree languages is not sur-
prising, as the linear context-free tree languages
(LCFTLs) are closed under intersection with reg-
ular tree languages (Kepser and M¨onnich, 2006).
The tree languages of TAGs form a subclass of the
LCFTLs, and the main construction in the proof
of the closure result for the latter can be suitably
restricted to the former.
The structure of this paper is as follows. The
main grammatical formalisms considered in this
paper are summarized in Section 2 and Sec-
tion 3 discusses a number of analyses of these for-
malisms that will be used in later sections. Sec-
tion 4 starts by explaining how parsing of a string
can be seen as the construction of a grammar that
generates the intersection of two languages, and
then moves on to a type of parsing involving in-
tersection of tree languages in place of string lan-
guages.
In order to illustrate the implications of the the-
ory, we consider how it can be used to solve a prac-
tical problem, in Section 5. A number of possible
extensions are outlined in Section 6.
</bodyText>
<sectionHeader confidence="0.997992" genericHeader="introduction">
2 Formalisms
</sectionHeader>
<bodyText confidence="0.999928378378378">
In this section, we recall the formalisms of
weighted regular tree grammars and weighted tree
adjoining grammars. We use similar notation and
terminology for both, in order to prepare for Sec-
tion 4, where we investigate the combination of
these formalisms through intersection. As a conse-
quence of the required unified notation, we deviate
to some degree from standard definitions, without
affecting generative power however.
For common definitions of weighted regular
tree grammars, the reader is referred to (Graehl
and Knight, 2004). Weighted tree adjoining gram-
mars are a straightforward generalization of prob-
abilistic (or stochastic) tree adjoining grammars,
as introduced by (Resnik, 1992) and (Schabes,
1992).
For both regular tree grammars (RTGs) and tree
adjoining grammars (TAGs), we will write a la-
beled and ordered tree as A(α). where A is the la-
bel of the root node, and α is a sequence of expres-
sions of the same form that each represent an im-
mediate subtree. In our presentation, labels do not
have explicit ranks, that is, the number of children
of a node is not determined by its label. This al-
lows an interesting generalization, to be discussed
in Section 6.2.
Where we are interested in the string language
generated by a tree-generating grammar, we may
distinguish between two kinds of labels, the ter-
minal labels, which may occur only at leaves, and
the nonterminal labels, which may occur at any
node. It is customary to write terminal leaves as
a instead of a(). The yield of a tree is the string
of occurrences of terminal labels in it, from left to
right. Note that also nonterminal labels may occur
at the leaves, but they will not be included in the
yield; cf. epsilon rules in context-free grammars.
</bodyText>
<subsectionHeader confidence="0.989176">
2.1 Weighted regular tree grammars
</subsectionHeader>
<bodyText confidence="0.9999384">
A weighted regular tree grammar (WRTG) is a 4-
tuple G = (5, L, R, s�-), where 5 and L are two
finite sets of states and labels, respectively, sl- E 5
is the initial state, and R is a finite set of rules.
Each rule has the form:
</bodyText>
<equation confidence="0.399195">
so —* A(si ··· sm) (w),
</equation>
<bodyText confidence="0.9999124">
where so, si, ... , sm are states (0 &lt; m), A is a
label and w is a weight.
Rewriting starts with a string containing only
the initial state s�-. This string is repeatedly rewrit-
ten by replacing the left-hand side state of a rule by
the right-hand side of the same rule, until no state
remains. It may be convenient to assume a canoni-
cal order of rewriting, for example in terms of left-
most derivations (Hopcroft and Ullman, 1979).
Although alternative semirings can be consid-
ered, here we always assume that the weights
of rules are non-negative real numbers, and the
weight of a derivation of a tree is the product of
the weights of the rule occurrences. If several
(left-most) derivations result in the same tree, then
</bodyText>
<page confidence="0.998272">
14
</page>
<bodyText confidence="0.971218967741936">
the weight of that tree is given by the sum of the
weights of those derivations. Where we are inter-
ested in the string language, the weights of trees
with the same yield are added to obtain the weight
of that yield.
A (weighted) context-free grammar can be seen
as a special case of a (weighted) regular tree gram-
mar, where the set of states equals the set of labels,
and rules have the form:
A → A(B1 ··· Bm).
Also the class of (weighted) tree substitution
grammars (Sima’an, 1997) can be seen as a spe-
cial case of (weighted) regular tree grammars, by
letting the set of labels overlap with the set of
states, and imposing two constraints on the allow-
able rules. The first constraint is that for each la-
bel that is also a state, all defining rules are of the
form:
A → A(s1 ··· sm).
The second constraint is that for each state that is
not a label, there is exactly one rule with that state
in the left-hand side. This means that exactly one
subtree (or elementary tree) can be built top-down
out of such states, down to a level where we again
encounter states that are also labels. If desired, we
can exclude infinite elementary trees by imposing
an additional constraint on allowed sets of rules
(no cycles composed of states that are not labels);
alternatively, we can demand that the grammar
does not contain any useless rules, which automat-
ically excludes such infinite elementary trees.
</bodyText>
<subsectionHeader confidence="0.99644">
2.2 Weighted linear indexed grammars
</subsectionHeader>
<bodyText confidence="0.999638454545455">
Although we are mainly interested in the tree lan-
guages of tree adjoining grammars, we will use
an equivalent representation in terms of linear in-
dexed grammars, in order to obtain a uniform no-
tation with regard to regular tree grammars.
Thus, a weighted linear indexed grammar
(WLIG) is a 5-tuple G = (5, I, L, R, s-), where
5, I and L are three finite sets of states, indices
and labels, respectively, sl- ∈ 5 is the initial state,
and R is a finite set of rules. Each rule has one of
the following four forms:
</bodyText>
<equation confidence="0.9832175">
1. s0[◦◦] → A( s1[ ] · · ·
sj-1[ ] sj[◦◦] sj+1[ ] · · ·
</equation>
<bodyText confidence="0.851091666666667">
sm[] ) hwi,
where s0, s1, ... , sm are states (1 ≤ j ≤ m),
A is a label and w is a weight;
</bodyText>
<listItem confidence="0.998569">
2. s[ ] → A() hwi;
3. s[◦◦] → s&apos;[t◦◦] hwi, where t is an index;
4. s[t◦◦] → s&apos;[◦◦] hwi.
</listItem>
<bodyText confidence="0.999978628571429">
The expression ◦◦ may be thought of as a vari-
able denoting a string of indices on a stack, and
this variable is to be consistently substituted in
the left-hand and the right-hand sides of rules
upon application during rewriting. In other words,
stacks are copied from the left-hand side of a rule
to at most one member in the right-hand side,
which we will call the head of that rule. The ex-
pression [ ] stands for the empty stack and [t◦◦] de-
notes a stack with top element t. Thereby, rules of
the third type implement a stack push and rules of
the fourth type implement a pop. Rewriting starts
from s�-[ ]. The four subsets of R containing rules
of the respective four forms above will be referred
to as R1, R2, R3 and R4.
In terms of tree adjoining grammars, which as-
sume a finite number of elementary trees, the in-
tuition behind the four types of rules is as fol-
lows. Rules of the first type correspond to con-
tinued construction of the same elementary tree.
Rules of the third type correspond to the initiation
of a newly adjoined auxiliary tree and rules of the
fourth type correspond to its completion at a foot
node, returning to an embedding elementary tree
that is encoded in the index that is popped. Rules
of the second type correspond to construction of
leaves, as in the case of regular tree grammars.
See further (Vijay-Shanker and Weir, 1994) for the
equivalence of linear indexed grammars and tree
adjoining grammars.
Note that regular tree grammars can be seen as
special cases of linear indexed grammars, by ex-
cluding rules of the third and fourth types, which
means that stacks of indices always remain empty
(Joshi and Schabes, 1997).
</bodyText>
<subsectionHeader confidence="0.99898">
2.3 Probabilistic grammars
</subsectionHeader>
<bodyText confidence="0.9995508">
A weighted regular tree grammar, or weighted lin-
ear indexed grammar, respectively, is called prob-
abilistic if the weights are probabilities, that is,
values between 0 and 1. A probabilistic regular
tree grammar (PRTG) is proper if for each state
s, the probabilities of all rules that have left-hand
side s sum to one.
Properness for a probabilistic linear indexed
grammar (PLIG) is more difficult to define, due
to the possible overlap of applicability between
</bodyText>
<page confidence="0.987305">
15
</page>
<bodyText confidence="0.999969533333333">
the four types of rules, listed in the section above.
However, if we encode a given TAG as a LIG in a
reasonable way, then a state s may occur both in
left-hand sides of rules from R1 and in left-hand
sides of rules from R3, but all other such overlap
between the four types is precluded.
Intuitively, a state may represent an internal
node of an elementary tree, in which case rules
from both R1 and R3 may apply, or it may rep-
resent a non-foot leaf node, in which case a rule
from R2 may apply, or it may be a foot node, in
which case a rule from R4 may apply.
With this assumption that the only overlap in ap-
plicability is between R1 and R3, properness can
be defined as follows.
</bodyText>
<listItem confidence="0.976151545454545">
• For each state s, either there are no rules in
R1 or R3 with s in the left-hand side, or the
sum of probabilities of all such rules equals
one.
• For each state s, either there are no rules in
R2 with s in the left-hand side, or the sum of
probabilities of all such rules equals one.
• For each state s and index t, either there
are no rules in R4 with left-hand side s[too],
or the sum of probabilities of all such rules
equals one.
</listItem>
<bodyText confidence="0.9999736">
We say a weighted regular tree grammar, or
weighted linear indexed grammar, respectively, is
consistent if the sum of weights of all (left-most)
derivations is one. This is equivalent to saying that
the sum of weights of all trees is one, and to saying
that the sum of weights of all strings is one.
For each consistent WRTG (WLIG, respec-
tively), there is an equivalent proper and consistent
PRTG (PLIG, respectively). The proof lies in nor-
malization. For WRTGs this is a trivial extension
of normalization of weighted context-free gram-
mars, as described for example by (Nederhof and
Satta, 2003). For WLIGs (and weighted TAGs),
the problem of normalization also becomes very
similar once we consider that the set of derivation
trees of tree adjoining grammars can be described
with context-free grammars, and that this carries
over to weighted derivation trees. See also (Sarkar,
1998).
WLIGs seemingly incur an extra complication,
if a state may occur in combination with an index
on top of the associated stack such that no rules are
applicable. However, for LIGs that encode TAGs,
the problem does not arise as, informally, one may
always resume construction of the embedding el-
ementary tree below the foot node of an adjoined
auxiliary tree.
We say a LIG is in TAG-normal form if (a) at
least one rule is applicable for each combination
of state s and index t such that s[too] is deriv-
able from s�-[ ], and (b) the only overlap in ap-
plicability of the four types of rules is between
R1 and R3. Statements in what follows involv-
ing WLIGs (or PLIGs) in TAG-normal form also
hold for weighted (or probabilistic) TAGs.
</bodyText>
<subsectionHeader confidence="0.529879">
3 Analysis of grammars
</subsectionHeader>
<bodyText confidence="0.99872365">
We call a grammar rule useless if it cannot be part
of any derivation of a tree (or of a string, in the
case of grammars with an emphasis on string lan-
guages). We say a grammar is reduced if it does
not contain useless rules.
Whereas most grammars written by hand or in-
duced by a corpus or treebank are reduced, there
are practical operations that turn reduced gram-
mars into grammars with useless rules; we will
see an example in the next section, where gram-
mars are constructed that generate the intersection
of two given languages. In order to determine
whether the intersection is non-empty, it suffices to
identify useless rules in the intersection grammar.
If and only if all rules are useless, the generated
language is empty.
In the case of context-free grammars (see for ex-
ample (Sippu and Soisalon-Soininen, 1988)), the
analysis to identify useless rules can be split into
two phases:
</bodyText>
<listItem confidence="0.799596714285714">
1. a bottom-up phase to identify the grammar
symbols that generate substrings, which may
include the start symbol if the generated lan-
guage is non-empty; and
2. a top-down phase to identify the grammar
symbols that are reachable from the start
symbol.
</listItem>
<bodyText confidence="0.999965125">
The intersection of the generating symbols and the
reachable symbols gives the set of useful symbols.
One can then identify useless rules as those that
contain one or more symbols that are not useful.
The procedure for linear indexed grammars is
similarly split into two phases, of which the first
is given in Figure 1 in the form of a deduction
system. The inference rules simultaneously derive
</bodyText>
<page confidence="0.850202">
16
</page>
<equation confidence="0.999059545454545">
(s, s) ns ∈ S (a)
s ns [ ] → A() (b)
s1 ··· sj−1 (sj, s) sj+1 ··· smn s0[◦◦] → A(s1[ ] ··· sj[◦◦] ··· sm[ ]) (c)
(s0, s)
s1 ··· sm ns0[◦◦] → A(s1[ ] ··· sj[◦◦] ··· sm[ ]) (d)
s0
(s1, s2)
(s3, s4) s0[◦◦] → s1[ι◦◦] (e)
(
s3 s0[◦◦] → s1[ι◦◦] (f)
s0 s2[ι◦◦] → s3[◦◦]
</equation>
<figureCaption confidence="0.838457">
Figure 1: Simultaneous analysis of two kinds of subderivations in a LIG. Items (s, s0) represent existence
</figureCaption>
<bodyText confidence="0.9113178">
of one or more subderivations s[ ] →∗ α(s0[ ]), where α is a tree with a gap in the form of an unresolved
state s0 associated with an empty stack. Furthermore, s and s0 are connected through propagation of a
stack of indices, or in other words, the occurrence of s0 is the head of a rule, of which the left-hand side
state is the head of another rule, etc., up to s. In the inference rules, items s represent existence of one or
more subderivations s[ ] →∗ α, where α is a complete tree (without any unresolved states).
</bodyText>
<equation confidence="0.998447">
((s0, s4) s2[ι◦◦] → s3[◦◦]
(s1, s2)
</equation>
<bodyText confidence="0.999857863636364">
two types of item. The generated language is non-
empty if the item s` can be derived.
We will explain inference rule (f), which is the
most involved of the six rules. The two items
in the antecedent indicate the existence of deriva-
tions s1[] →∗ α(s2[]) and s3[] →∗ β. Note
that s1[ ] →∗ α(s2[ ]) implies s1[ι] →∗ α(s2[ι]),
because an additional element in the bottom of
a stack would not block an existing derivation.
Hence s0[] → s1[ι] →∗ α(s2[ι]) → α(s3[]) →∗
α(β), which justifies the item s0 in the consequent
of the rule.
After determining which items can be derived
through the deduction system, it is straightforward
to identify those rules that are useful, by applying
the inference rules in reverse, from consequent to
antecedents, starting with s`.
The running time of the analysis is determined
by how often each of the inference rules can be
applied, which is bounded by the number of ways
each can be instantiated with states and rules from
the grammar. The six inference rules together give
</bodyText>
<equation confidence="0.942606333333333">
us O(|S |+ |R2 |+ |R1|· |S |+ |R1 |+ |R3|·|R4|·
|S |+ |R3 |· |R4|) = O(|S |+ |R1 |· |S |+ |R2|
+ |R3 |· |R4 |· |S|) = |G|3, where we assume a
</equation>
<bodyText confidence="0.999573">
reasonable measure for the size |G |of a LIG G, for
example, the total number of occurrences of states,
labels and indices in the rules.
It is not difficult to see that there is exactly one
deduction of s` in the deduction system for each
complete derivation in the grammar. We leave the
full proof to the interested reader, but provide the
hint that items (s, s0) can only play a role in a
complete deduction provided s0 is rewritten by a
rule that pops an index from the stack. Because
of this, derivations in the grammar of the form
s[] →∗ α(s0[]) or of the form s[] →∗ α can be
divided in a unique way into subderivations repre-
sentable by our items.
The above deduction system is conceptually
very close to a system of equations that expresses
the sum of weights of all derivations in the gram-
mar, or in(s`), in terms of similar values of the
form in(s), which is the sum of weights of all
subderivations s[ ] →∗ α, and in(s, s0), which is
the sum of weights of all subderivations s[ ] →∗
α(s0[ ]). The equations are given in Figure 2.
Although the expressions look unwieldy, they
</bodyText>
<page confidence="0.905593">
17
</page>
<equation confidence="0.995582384615385">
in(s0) = E w +
s0[] → A() hwi
E w · in(s1) · ... · in(sm) +
s0[◦◦] → A(s1[ ] ··· sj[◦◦] ··· sm[ ]) hwi
w · v · in(s1, s2) · in(s3)
s0[◦◦] → s1[ι◦◦] hwi w · in(s1) · ... · in(sj,s) · ... · in(sm) +
s2[ι◦◦] → s3[◦◦] hvi
in(s0, s) = δ(s0 = s) +
E
s0[◦◦] → A(s1[ ] ··· sj[◦◦] ··· sm[ ]) hwi
w · v · in(s1, s2) · in(s3, s)
s0[◦◦] → s1[ι◦◦] hwi
s2[ι◦◦] → s3[◦◦] hvi
</equation>
<figureCaption confidence="0.662107333333333">
Figure 2: The sum of weights of all derivations in a WLIG, or in(s�_), is defined by the smallest non-
negative solution to a system of equations. The function δ with a boolean argument evaluates to 1 if the
condition is true and to 0 otherwise.
</figureCaption>
<bodyText confidence="0.99983884">
express exactly the ‘inside’ value of the weighted
context-free grammar that we can extract out of
the deduction system from Figure 1, by instanti-
ating the inference rules in all possible ways, and
then taking the consequent as the left-hand side of
a rule, and the antecedent as the right-hand side.
The weight is the product of weights of rules that
appear in the side conditions. It is possible to ef-
fectively solve the system of equations, as shown
by (Wojtczak and Etessami, 2007).
In the same vein we can compute ‘outside’
values for weighted linear indexed grammars, as
straightforward analogues of the outside values of
weighted and probabilistic context-free grammars.
The outside value is the sum of weights of partial
derivations that may lie ‘outside’ a subderivation
s[ ] →* α in the case of out(s), or a subderivation
s[ ] →* α(s&apos;[ ]) in the case of out(s, s&apos;). The equa-
tions in Figure 3 again follow trivially from the
view of Figure 1 as weighted context-free gram-
mar and the usual definition of outside values.
The functions in and out are particularly useful
for PLIGs in TAG-normal form, as they allow the
expected number of occurrences of state s to be
expressed as:
</bodyText>
<equation confidence="0.997086">
E(s) = in(s) · out(s)
</equation>
<bodyText confidence="0.99176625">
Similarly, the expected number of subderivations
of the form s[ ] →* α(s&apos;[]) is:
E(s, s&apos;) = in(s, s&apos;) · out(s, s&apos;)
We will return to this issue in Section 5.
</bodyText>
<sectionHeader confidence="0.997408" genericHeader="method">
4 Weighted intersection
</sectionHeader>
<bodyText confidence="0.999967181818182">
Before we discuss intersection on the level of
trees, we first show how a well-established type of
intersection on the level of strings, with weighted
context-free grammars and weighted finite au-
tomata (WFAs), can be trivially extended to re-
place CFGs with RTGs or LIGs. The intersec-
tion paradigm is originally due to (Bar-Hillel et
al., 1964). Extension to tree adjoining grammars
and linear indexed grammars was proposed before
by (Lang, 1994) and (Vijay-Shanker and Weir,
1993b).
</bodyText>
<subsectionHeader confidence="0.996533">
4.1 Intersection of string languages
</subsectionHeader>
<bodyText confidence="0.960748">
Let us assume a WLIG G with terminal and nonter-
minal labels. Furthermore, we assume a weighted
finite automaton A, with an input alphabet equal
to the set of terminal labels of G. The transitions
of A are of the form:
q 7→ q&apos; hwi,
a
where q and q&apos; are states, a is a terminal symbol,
and w is a weight. To simplify the presentation,
</bodyText>
<page confidence="0.950627">
18
</page>
<equation confidence="0.9984121875">
out(s0) = δ(s0 = s`) +
E 11 w · out(s0, s) · in(sj, s) · in(sp) +
s0[◦◦] → A(s1[ ] ··· sj[◦◦] ··· sm[ ]) hwi
k ∈ {1, ... , sj−1, sj+1, ... , sm} s.t. s0 = sk
E 11
w · out(s0) ·
p ∈/ {j, k}
in(sp) +
s0[◦◦] → A(s1[ ] ··· sj[◦◦] ··· sm[ ]) hwi p =6k
k ∈ {1, ... , sm} s.t. s0 = sk
E w · v · out(s0) · in(s1, s2)
s0[◦◦] → s1[t◦◦] hwi
s2[t◦◦] → s0[◦◦] hvi
out(s0, s) = E 11 w · out(s0, s) · in(sp) +
s0[◦◦] → A(s1[ ] ··· sj[◦◦] ··· sm[ ]) hwi p =6j
s0 = sj
</equation>
<figure confidence="0.944026666666667">
E w · v · out(s0, s4) · in(s3, s4) +
s0[◦◦] → s0[t◦◦] hwi w · v · out(s0, s) · in(s1, s2) +
s[t◦◦] → s3[◦◦] hvi w · v · out(s0) · in(s3)
E
s0[◦◦] → s1[t◦◦] hwi
s2[t◦◦] → s0[◦◦] hvi
E
s0[◦◦] → s0[t◦◦] hwi
s[t◦◦] → s3[◦◦] hvi
</figure>
<figureCaption confidence="0.999992">
Figure 3: The outside values in a WLIG.
</figureCaption>
<bodyText confidence="0.9802015">
we ignore epsilon transitions, and assume there is
a unique initial state q` and a unique final state qa.
We can construct a new WLIG G0 whose gen-
erated language is the intersection of the language
generated by G and the language accepted by A.
The rules of G0 are:
</bodyText>
<equation confidence="0.9982955">
1. (q0, s0, qm)[◦◦] →
A( (q0, s1, q1)[] ·· ·
(qj−2, sj−1, qj−1)[]
(qj−1, sj, qj)[◦◦]
(qj, sj+1, qj+1)[ ] ·· ·
(qm−1,sm,qm)[] ) hwi,
for each rule s0[◦◦] → A(s1[ ] · · · sj−1[ ]
sj[◦◦] sj+1[] · · · sm[]) hwi from G and se-
</equation>
<bodyText confidence="0.438154">
quence q0, ... , qm of states from A;
</bodyText>
<listItem confidence="0.925350666666667">
2. (q, s, q)[ ] → A() hwi, for each rule s[ ] →
A() hwi from G and state q from A;
3. (q, s, q0)[ ] → a hw · vi, for each rule s[ ] →
</listItem>
<figure confidence="0.552816666666667">
a hwi from G and transition q a
7→ q0 hvi from
A;
</figure>
<listItem confidence="0.9787565">
4. (q, s, q0)[◦◦] → (q, s0, q0)[t◦◦] hwi, for each
rule s[◦◦] → s0[t◦◦] hwi from G and states
q, q0 from A;
5. (q, s, q0)[t◦◦] → (q, s0, q0)[◦◦] hwi, for each
rule s[t◦◦] → s0[◦◦] hwi from G and states
q, q0 from A.
</listItem>
<bodyText confidence="0.994887363636364">
The new states (q, s, q0) give (left-most) deriva-
tions in G0 that each simultaneously represent one
(left-most) derivation in G of a certain substring,
starting from state s, and one sequence of transi-
tions taking the automaton A from state q to state
q0 while scanning the same substring. The initial
state of G0 is naturally (q`, s`, qa), which derives
strings in the intersection of the original two lan-
guages.
Further note that each derivation in G0 has a
weight that is the product of the weight of the cor-
</bodyText>
<page confidence="0.99829">
19
</page>
<bodyText confidence="0.999580709677419">
responding derivation in G and the weight of the
corresponding sequence of transitions in A. This
allows a range of useful applications. For exam-
ple, if A is deterministic (the minimum require-
ment is in fact absence of ambiguity) and if it as-
signs the weight one to all transitions, then G0 gen-
erates a set of trees that is exactly the subset of
trees generated by G whose yields are accepted by
A. Furthermore, the weights of those derivations
are preserved. If G is a consistent PLIG in TAG-
normal form, and if A accepts the language of all
strings containing a fixed substring x, then the sum
of probabilities of all derivations in G0 gives the
substring probability of x. The effective computa-
tion of this probability was addressed in Section 3.
An even more restricted, but perhaps more fa-
miliar case is if A is a linear structure that accepts
a single input string y of length n. Then G0 gen-
erates exactly the set of trees generated by G that
have y as yield. In other words, the string y is
thereby parsed.
If G is binary, i.e. all rules have at most two
states in the right-hand side, then G0 has a size
that is cubic in n. This may seem surprising, in
the light of the awareness that practical parsing al-
gorithms for tree adjoining grammars have a time
complexity of no less than O(n6). However, in or-
der to solve the recognition problem, an analysis
is needed to determine whether G0 allows at least
one derivation.
The analysis from Figure 1 requires O(|S0 |+
</bodyText>
<equation confidence="0.994683">
|R01 |· |S0 |+ |R02 |+ |R0 3 |· |R04 |· |S0|) steps, where
|S0 |= O(n2) is the number of states of G0, and
|R01 |= O(n3), |R02 |= |R03 |= |R04 |= O(n2) are
</equation>
<bodyText confidence="0.997840307692308">
the numbers of rules of G0, divided into the four
main types. This leads to an overall time com-
plexity of O(n6), as expected.
The observation that recognition can be harder
than parsing was made before by (Lang, 1994).
The central new insight this provided was that the
notion of ‘parsing’ is ill-defined in the literature.
One may choose a form in which to capture all
parses of an input allowed by a grammar, but dif-
ferent such forms may incur different costs of ex-
tracting individual parse trees.
In Section 6.2 we will consider the complexity
of parsing and recognition if G is not binary.
</bodyText>
<subsectionHeader confidence="0.990597">
4.2 Intersection of tree languages
</subsectionHeader>
<bodyText confidence="0.999918857142857">
We now shift our attention from strings to trees,
and consider the intersection of the tree language
generated by a weighted linear indexed grammar
G1 and the tree language generated by a weighted
regular tree grammar G2. This intersection is gen-
erated by another weighted linear indexed gram-
mar G, which has the following rules:
</bodyText>
<equation confidence="0.982801875">
1. (s0,q0)[◦◦] → A( (s1,q1)[] ···
(sj−1, qj−1)[]
(sj, qj)[◦◦]
(sj+1, qj+1)[ ] · · ·
(sm,qm)[] ) hw · vi,
for each rule s0[◦◦] → A(s1[ ] · · · sj−1[ ]
sj[◦◦] sj+1[ ] · · · sm[ ]) hwi from G1 and each
rule q0 → A(q1 ··· qm) hvi from G2;
</equation>
<listItem confidence="0.985681111111111">
2. (s, q)[ ] → A() hw · vi, for each rule s[ ] →
A() hwi from G1 and each rule q → A() hvi
from G2;
3. (s, q)[◦◦] → (s0, q)[ι◦◦] hwi, for each rule
s[◦◦] → s0[ι◦◦] hwi from G1 and state q from
G2;
4. (s, q)[ι◦◦] → (s0, q)[◦◦] hwi, for each rule
s[ι◦◦] → s0[◦◦] hwi from G1 and state q from
G2.
</listItem>
<bodyText confidence="0.999424769230769">
Much as in the previous section, each (left-
most) derivation in G corresponds to one (left-
most) derivation in G1 and one in G2. Further-
more, these three derivations derive the same la-
belled tree, and a derivation in G has a weight that
is the product of the weights of the corresponding
derivations in G1 and G2.
It can be instructive to look at special cases.
Suppose that G2 is an unambiguous regular tree
grammar of size O(n) generating a single tree t
with n vertices, assigning weight one to all its
rules. Then the above construction can be seen
as parsing of that tree t. The sum of weights of
derivations in G then gives the weight of the tree
in G1. See Section 3 once more for a general way
to compute this weight, as the inside value of the
initial state of G, which is naturally (s`, q`).
In order to do recognition of t, or in other words,
to determine whether G allows at least one deriva-
tion, the analysis from Figure 1 can be used, which
has time complexity O(|S |+ |R1 |· |S |+ |R2|
+ |R3 |· |R4 |· |S|), where |S |= O(n) is the
number of states of G, and the numbers of rules
are |R1 |= O(n), |R2 |= |R3 |= |R4 |= O(n).
Note that |R1 |= O(n) because we have assumed
that G2 allows only one derivation of one tree t,
</bodyText>
<page confidence="0.986438">
20
</page>
<bodyText confidence="0.850672176470588">
hence q0 uniquely determines q1, ... , qm. Over-
all, we obtain O(n3) steps, which concurs with a
known result about the complexity of TAG parsing
of trees, as opposed to strings (Poller and Becker,
1998).
Another special case is if WLIG G1 simplifies
to a WRTG (i.e. the stacks of indices remain al-
ways empty), which means we compute the inter-
section of two weighted regular tree grammars G1
and G2. For recognition, or in other words to de-
cide non-emptiness of the intersection, we can still
use Figure 1, although now only inference rules
(b) and (d) are applicable (with a small refinement
to the algorithm we can block spurious application
of (a) where no rules exist that pop indices.) The
complexity is determined by (d), which requires
O(|G1|·|G2|) steps.
</bodyText>
<sectionHeader confidence="0.942589" genericHeader="method">
5 Parameter estimation
</sectionHeader>
<bodyText confidence="0.99980644117647">
PLIGs allow finer description of probability distri-
butions than PRTG, both over string languages and
over tree languages. However, the (string) pars-
ing complexity of regular tree grammars is O(n3)
and that of LIGs is O(n6). It may therefore be
preferable for reasons of performance to do pars-
ing with a PRTG even when a PTAGs or PLIG is
available with accurately trained probabilities. Al-
ternatively, one may do both, with a PRTG used
in a first phase to heuristically reduce the search
space.
This section outlines how a suitable PRTG G2
can be extracted out of a PLIG G1, assuming the
underlying RTG G02 without weights is already
given. The tree language generated by G02 may be
an approximation of that generated by G1. The ob-
jective is to make G2 as close as possible to G1 in
terms of probability distributions over trees. We
assume that G02 is unambiguous, that is, for each
tree it generates, there is at most one derivation.
The procedure is a variant of the one described
by (Nederhof, 2005). The idea is that derivations
in G1 are mapped to those in G02, via the trees in the
intersection of the two tree languages. The proba-
bility distribution of states and rules in G2 is esti-
mated based on the expected frequencies of states
and rules from G02 in the intersection.
Concretely, we turn the RTG G02 into a PRTG
G002 that is obtained simply be assigning weight
one to all rules. We then compute the intersec-
tion grammar G as in Section 4.2. Subsequently,
the inside and outside values are computed for G,
as explained in Section 3. The expected number of
occurrences of a rule in G of the form:
</bodyText>
<equation confidence="0.9902058">
(s0, q0)[◦◦] → A( (s1, q1)[ ] ···
(sj−1, qj−1)[]
(sj, qj)[◦◦]
(sj+1, qj+1)[ ] · · ·
(sm,qm)[] ) hw · vi,
</equation>
<bodyText confidence="0.999333142857143">
is given by multiplying the outside and inside
probabilities and the rule probability, as usual.
We get two terms however that we need to sum.
The intuition is that we must count both rule oc-
currences used for building initial TAG trees and
those used for building auxiliary TAG trees. This
gives:
</bodyText>
<equation confidence="0.987567666666667">
11 w · v · out((s0, q0)) · in((sk, qk)) +
k
�w · v ·
s,q out((s0, q0), (s, q)) ·
in((sj, qj), (s, q)) · 11 in((sk, qk))
k6�j
</equation>
<bodyText confidence="0.971912545454546">
By summing these expected numbers for different
rules s0[◦◦] → A(s1[ ] ··· sj−1[ ] sj[◦◦] sj+1[ ]
· · · sm[ ]), we obtain the expected number of oc-
currences of q0 → A(q1 · · · qm), Let us denote
this sum by E(q0 → A(q1 ··· qm)). By summing
these for fixed q0, we obtain the expected number
of occurrences of q0, which we denote by E(q0).
The probability of q0 → A(q1 · · · qm) in G2 is then
set to be the ratio of E(q0 → A(q1 · · · qm)) and
E(q0).
By this procedure, the Kullback-Leibler dis-
tance between G1 and G2 is minimized. Although
the present paper deals with very different for-
malisms, the proof of correctness is identical to
that in (Nederhof, 2005). The reason is that in both
cases the mathematical analysis must focus on the
objects in the intersection (strings or trees) which
may correspond to multiple derivations in the orig-
inal model (here G1) but to a single derivation in
the unambiguous model to be trained (here G2),
and each derivation is composed of rules, whose
probabilities are to be multiplied.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="evaluation">
6 Extensions
</sectionHeader>
<subsectionHeader confidence="0.938711">
6.1 Transduction
</subsectionHeader>
<bodyText confidence="0.999962666666667">
For various formalisms describing (string or tree)
languages, there are straightforward generaliza-
tions that describe a relation between two or more
</bodyText>
<page confidence="0.99682">
21
</page>
<bodyText confidence="0.99986725">
languages, which is known as a transduction. The
idea is that the underlying control mechanism,
such as the states in regular tree grammars or lin-
ear indexed grammars, is now coupled to two or
more surface forms that are synchronously pro-
duced. For example, a rule in a weighted syn-
chronous regular tree grammar (WSRTG) has the
form:
</bodyText>
<equation confidence="0.859636">
s0 —* A(s1 ··· sm), B(sπ(1) ··· sπ(m)) (w),
</equation>
<bodyText confidence="0.993874">
where π is a permutation of 1, ... , m. We can gen-
eralize this to having a third label C and a second
permutation π&apos;, in order to describe simultaneous
relations between three tree languages, etc. In this
section we will restrict ourselves to binary rela-
tions however, and call the first surface form the
input and the second surface form the output. For
synchronous tree adjoining grammars, see for ex-
ample (Shieber, 1994).
If we apply intersection on the input or on the
output of a synchronous grammar formalism, then
this is best seen as composition. This is well-
known in the case of finite-state transducers and
some forms of context-free transduction (Berstel,
1979), and application to a wider range of for-
malisms is gaining interest in the area of machine
translation (Knight, 2007).
With the intersection from Section 4.2 trivially
extended to composition, we can now implement
composition of the form:
τ1 o ... o τk,
where the different τj are transducers, of which
k − 1 are (W)SRTGs and at most one is a
(weighted) synchronous LIG ((W)SLIG). The re-
sult of the composition is another (W)SLIG. It
should be noted that a (W)RTS (or (W)LIG) can
be seen as a (W)SRTG (or (W)SLIG, respectively)
that represents the identity relation on its tree lan-
guage.
</bodyText>
<subsectionHeader confidence="0.995847">
6.2 Binarization
</subsectionHeader>
<bodyText confidence="0.999932617647059">
In the discussion of complexity in Section 4.1, we
assumed that rules are binary, that is, that they
have at most two states in each right-hand side.
However, whereas any context-free grammar can
be transformed into a binary form (e.g. Chomsky
normal form), the grammars as we have defined
them cannot be. We will show that this is to a large
extent a consequence of our definitions, which
were motivated by presentational ease, rather than
by generality.
The main problem is formed by rules of the
form s0 —* A(s1 · · · sm), where m &gt; 2. Such
long rules cannot be broken up into shorter rules
of the same form, as this would require an addi-
tional labelled vertex, changing the tree language.
An apparent solution lies in allowing branching
rules without any label, for example s1 —* s2 s3.
Regrettably this could create substantial computa-
tional problems for intersection of the described
tree languages. As labels provide the mechanism
through which to intersect tree languages, rules
of the above form are somewhat similar to unit
rules or epsilon rules in context-free grammars, in
that they are not bound to observable elements.
Branching rules furthermore have the potential
to generate context-free languages, and therefore
they are more pernicious to intersection, consider-
ing that emptiness of intersection of context-free
languages is undecidable.
It therefore seems better to restrict branching
rules s1 —* s2 s3 to finite-state power, for exam-
ple by making these rules exclusively left-linear
or right-linear. A more elegant but equivalent way
of looking at this may be to have rules of the form:
</bodyText>
<equation confidence="0.890712">
s0 —* A(R),
</equation>
<bodyText confidence="0.999901666666667">
where R is a regular language over states. In the
case of linear indexed grammars, we would have
rules of the form:
</bodyText>
<equation confidence="0.717577">
s[oo] —* A(L s&apos;[oo] R)
</equation>
<bodyText confidence="0.999991894736842">
where L and R are regular languages over expres-
sions of the form s[ ]. Appropriate weighted fi-
nite automata can be used to assign weights to se-
quences of such expressions in L and R. With
these extended types of rules, our construction
from Section 4.2 still works. The key observation
here is that regular languages are closed under in-
tersection.
One of the implications of the above extended
definitions is that labels appear not only with-
out fixed ranks, as we have assumed from the
start in Section 2, but even without a bound on
the rank. Concretely, a vertex may appear with
any number of children in a tree. Whereas this
may be unconventional in certain areas of formal
language theory, it is a well-accepted practice in
the parsing of natural language to make the num-
ber of constituents of syntactic categories flexi-
ble and conceptually unbounded; see for example
</bodyText>
<page confidence="0.986673">
22
</page>
<bodyText confidence="0.9997774">
(Collins, 1997). Also the literature on unranked
tree automata is very relevant; see for example
(Schwentick, 2007). Binarization for LIGs was
considered before by (Vijay-Shanker and Weir,
1993a).
</bodyText>
<subsectionHeader confidence="0.997682">
6.3 Beyond TAGs
</subsectionHeader>
<bodyText confidence="0.999982789473684">
In the light of results by (Kepser and M¨onnich,
2006) it is relatively straightforward to consider
larger classes of linear context-free tree grammars
in place of tree-adjoining grammars, in order to
generalize the construction in Section 4.2.
The generalization described in what follows
seems less straightforward. Context-free lan-
guages can be characterized in terms of parse trees
in which path sets (sets of strings of labels on
paths from the root to a leaf) are regular. In the
case of tree adjoining languages, the path sets are
context-free. There is a hierarchy of classes of lan-
guages in which the third step is to consider path
sets that are tree adjoining languages (Weir, 1992).
In this paper, we have considered the parsing-as-
intersection paradigm for the first two members of
the hierarchy. It may be possible that the paradigm
is also applicable to the third and following mem-
bers. This avenue is yet to be pursued.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999920428571429">
This paper has extended the parsing-as-
intersection paradigm from string languages
to tree languages. Probabilities, or weights in
general, were incorporated in this framework in a
natural way. We have discussed one particular ap-
plication involving a special case of the extended
paradigm.
</bodyText>
<sectionHeader confidence="0.996547" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999990714285714">
Helpful comments by anonymous reviewers are
gratefully acknowledged. The basic result from
Section 4.1 as it pertains to RTGs as subclass of
LIGs was discussed with Heiko Vogler, who pro-
posed two alternative proofs. Sylvain Schmitz
pointed out to me the relevance of literature on lin-
ear context-free tree languages.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99794720754717">
A.V. Aho, M.S. Lam, R. Sethi, and J.D. Ullman.
2007. Compilers: Principles, Techniques, &amp; Tools.
Addison-Wesley.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On
formal properties of simple phrase structure gram-
mars. In Y. Bar-Hillel, editor, Language and Infor-
mation: Selected Essays on their Theory and Appli-
cation, chapter 9, pages 116–150. Addison-Wesley,
Reading, Massachusetts.
J. Berstel. 1979. Transductions and Context-Free Lan-
guages. B.G. Teubner, Stuttgart.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In 35th Annual Meeting of
the Association for Computational Linguistics, Pro-
ceedings of the Conference, pages 16–23, Madrid,
Spain, July.
J. Graehl and K. Knight. 2004. Training tree transduc-
ers. In HLT-NAACL 2004, Proceedings of the Main
Conference, Boston, Massachusetts, USA, May.
F. Gcseg and M. Steinby. 1997. Tree languages. In
G. Rozenberg and A. Salomaa, editors, Handbook
of Formal Languages, Vol. 3, chapter 1, pages 1–68.
Springer, Berlin.
J.E. Hopcroft and J.D. Ullman. 1979. Introduction
to Automata Theory, Languages, and Computation.
Addison-Wesley.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, edi-
tors, Handbook of Formal Languages. Vol 3: Beyond
Words, chapter 2, pages 69–123. Springer-Verlag,
Berlin/Heidelberg/New York.
D. Jurafsky and J.H. Martin. 2000. Speech and Lan-
guage Processing. Prentice-Hall.
S. Kepser and U. M¨onnich. 2006. Closure properties
of linear context-free tree languages with an appli-
cation to optimality theory. Theoretical Computer
Science, 354:82–97.
K. Knight. 2007. Capturing practical natural language
transformations. Machine Translation, 21:121–133.
B. Lang. 1994. Recognition can be harder than pars-
ing. Computational Intelligence, 10(4):486–494.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop
on Parsing Technologies, pages 137–148, LORIA,
Nancy, France, April.
M.-J. Nederhof. 2005. A general technique to train
language models on language models. Computa-
tional Linguistics, 31(2):173–185.
P. Poller and T. Becker. 1998. Two-step TAG pars-
ing revisited. In Fourth International Workshop on
Tree Adjoining Grammars and Related Frameworks,
pages 143–146. Institute for Research in Cognitive
Science, University of Pennsylvania, August.
</reference>
<page confidence="0.97275">
23
</page>
<reference confidence="0.99982018">
P. Resnik. 1992. Probabilistic tree-adjoining grammar
as a framework for statistical natural language pro-
cessing. In Proc. of the fifteenth International Con-
ference on Computational Linguistics, pages 418–
424. Nantes, August.
A. Sarkar. 1998. Conditions on consistency of prob-
abilistic tree adjoining grammars. In 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, volume 2, pages 1164–1170,
Montreal, Quebec, Canada, August.
Y. Schabes. 1992. Stochastic lexicalized tree-
adjoining grammars. In Proc. of the fifteenth Inter-
national Conference on Computational Linguistics,
pages 426–432. Nantes, August.
Thomas Schwentick. 2007. Automata for XML–a
survey. Journal of Computer and System Sciences,
73:289–315.
S.M. Shieber. 1994. Restricting the weak-generative
capacity of synchronous tree-adjoining grammars.
Computational Intelligence, 10(4):371–385.
K. Sima’an. 1997. Efficient disambiguation by means
of stochastic tree substitution grammars. In D. Jones
and H. Somers, editors, New Methods in Language
Processing. UCL Press, UK.
S. Sippu and E. Soisalon-Soininen. 1988. Parsing The-
ory, Vol. I: Languages and Parsing, volume 15 of
EATCS Monographs on Theoretical Computer Sci-
ence. Springer-Verlag.
K. Vijay-Shanker and D.J. Weir. 1993a. Parsing some
constrained grammar formalisms. Computational
Linguistics, 19(4):591–636.
K. Vijay-Shanker and D.J. Weir. 1993b. The use of
shared forests in tree adjoining grammar parsing. In
Sixth Conference of the European Chapter of the As-
sociation for Computational Linguistics, Proceed-
ings of the Conference, pages 384–393, Utrecht, The
Netherlands, April.
K. Vijay-Shanker and D.J. Weir. 1994. The equiva-
lence of four extensions of context-free grammars.
Mathematical Systems Theory, 27:511–546.
D.J. Weir. 1992. A geometric hierarchy beyond
context-free languages. Theoretical Computer Sci-
ence, 104:235–261.
D. Wojtczak and K. Etessami. 2007. PReMo: an an-
alyzer for Probabilistic Recursive Models. In Tools
and Algorithms for the Construction and Analysis
of Systems, 13th International Conference, volume
4424 of Lecture Notes in Computer Science, pages
66–71, Braga, Portugal. Springer-Verlag.
</reference>
<page confidence="0.999179">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947270">
<title confidence="0.991226">Weighted parsing of trees</title>
<author confidence="0.997903">Mark-Jan Nederhof</author>
<affiliation confidence="0.999755">School of Computer Science, University of St</affiliation>
<address confidence="0.987267">North Haugh, St Andrews, KY16 9SX, Scotland</address>
<abstract confidence="0.99619325">We show how parsing of trees can be formalized in terms of the intersection of two tree languages. The focus is on weighted regular tree grammars and weighted tree adjoining grammars. Potential applications are discussed, such as parameter estimation across formalisms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>M S Lam</author>
<author>R Sethi</author>
<author>J D Ullman</author>
</authors>
<date>2007</date>
<booktitle>Compilers: Principles, Techniques, &amp; Tools.</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="1193" citStr="Aho et al., 2007" startWordPosition="182" endWordPosition="185">e trees involved in derivations of strings are often seen as auxiliary concepts at best. Theorems tend to be about the power of grammatical formalisms to produce strings (weak generative power) rather than trees (strong generative power). This can be explained by looking at typical applications of parsing. In compiler construction for example, one distinguishes between parse trees and (abstract) syntax trees, the former being shaped according to a grammar that is massaged to make it satisfy relatively artificial constraints, e.g. that of LALR(1), which is required by many compiler generators (Aho et al., 2007). The form of syntax trees is often chosen to simplify phases of semantic processing that follow parsing. As the machinery used in such processing is generally powerful, this offers much flexibility in the choice of the exact shape and labelling of syntax trees, as intermediate form between parsing and semantic analysis. In the study of natural languages, parse trees have played a more important role. Whereas linguistic utterances are directly observable and trees deriving them are not, there are nevertheless traditions within linguistics that would see one structural analysis of a sentence as</context>
</contexts>
<marker>Aho, Lam, Sethi, Ullman, 2007</marker>
<rawString>A.V. Aho, M.S. Lam, R. Sethi, and J.D. Ullman. 2007. Compilers: Principles, Techniques, &amp; Tools. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars.</title>
<date>1964</date>
<booktitle>Language and Information: Selected Essays on their Theory and Application, chapter 9,</booktitle>
<pages>116--150</pages>
<editor>In Y. Bar-Hillel, editor,</editor>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="22591" citStr="Bar-Hillel et al., 1964" startWordPosition="4025" endWordPosition="4028">they allow the expected number of occurrences of state s to be expressed as: E(s) = in(s) · out(s) Similarly, the expected number of subderivations of the form s[ ] →* α(s&apos;[]) is: E(s, s&apos;) = in(s, s&apos;) · out(s, s&apos;) We will return to this issue in Section 5. 4 Weighted intersection Before we discuss intersection on the level of trees, we first show how a well-established type of intersection on the level of strings, with weighted context-free grammars and weighted finite automata (WFAs), can be trivially extended to replace CFGs with RTGs or LIGs. The intersection paradigm is originally due to (Bar-Hillel et al., 1964). Extension to tree adjoining grammars and linear indexed grammars was proposed before by (Lang, 1994) and (Vijay-Shanker and Weir, 1993b). 4.1 Intersection of string languages Let us assume a WLIG G with terminal and nonterminal labels. Furthermore, we assume a weighted finite automaton A, with an input alphabet equal to the set of terminal labels of G. The transitions of A are of the form: q 7→ q&apos; hwi, a where q and q&apos; are states, a is a terminal symbol, and w is a weight. To simplify the presentation, 18 out(s0) = δ(s0 = s`) + E 11 w · out(s0, s) · in(sj, s) · in(sp) + s0[◦◦] → A(s1[ ] ··· </context>
</contexts>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In Y. Bar-Hillel, editor, Language and Information: Selected Essays on their Theory and Application, chapter 9, pages 116–150. Addison-Wesley, Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berstel</author>
</authors>
<title>Transductions and Context-Free Languages.</title>
<date>1979</date>
<publisher>B.G. Teubner,</publisher>
<location>Stuttgart.</location>
<contexts>
<context position="34782" citStr="Berstel, 1979" startWordPosition="6377" endWordPosition="6378">this to having a third label C and a second permutation π&apos;, in order to describe simultaneous relations between three tree languages, etc. In this section we will restrict ourselves to binary relations however, and call the first surface form the input and the second surface form the output. For synchronous tree adjoining grammars, see for example (Shieber, 1994). If we apply intersection on the input or on the output of a synchronous grammar formalism, then this is best seen as composition. This is wellknown in the case of finite-state transducers and some forms of context-free transduction (Berstel, 1979), and application to a wider range of formalisms is gaining interest in the area of machine translation (Knight, 2007). With the intersection from Section 4.2 trivially extended to composition, we can now implement composition of the form: τ1 o ... o τk, where the different τj are transducers, of which k − 1 are (W)SRTGs and at most one is a (weighted) synchronous LIG ((W)SLIG). The result of the composition is another (W)SLIG. It should be noted that a (W)RTS (or (W)LIG) can be seen as a (W)SRTG (or (W)SLIG, respectively) that represents the identity relation on its tree language. 6.2 Binariz</context>
</contexts>
<marker>Berstel, 1979</marker>
<rawString>J. Berstel. 1979. Transductions and Context-Free Languages. B.G. Teubner, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In 35th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>16--23</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="38038" citStr="Collins, 1997" startWordPosition="6935" endWordPosition="6936">servation here is that regular languages are closed under intersection. One of the implications of the above extended definitions is that labels appear not only without fixed ranks, as we have assumed from the start in Section 2, but even without a bound on the rank. Concretely, a vertex may appear with any number of children in a tree. Whereas this may be unconventional in certain areas of formal language theory, it is a well-accepted practice in the parsing of natural language to make the number of constituents of syntactic categories flexible and conceptually unbounded; see for example 22 (Collins, 1997). Also the literature on unranked tree automata is very relevant; see for example (Schwentick, 2007). Binarization for LIGs was considered before by (Vijay-Shanker and Weir, 1993a). 6.3 Beyond TAGs In the light of results by (Kepser and M¨onnich, 2006) it is relatively straightforward to consider larger classes of linear context-free tree grammars in place of tree-adjoining grammars, in order to generalize the construction in Section 4.2. The generalization described in what follows seems less straightforward. Context-free languages can be characterized in terms of parse trees in which path se</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In 35th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 16–23, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Graehl</author>
<author>K Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004, Proceedings of the Main Conference,</booktitle>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="6254" citStr="Graehl and Knight, 2004" startWordPosition="1008" endWordPosition="1011">m, in Section 5. A number of possible extensions are outlined in Section 6. 2 Formalisms In this section, we recall the formalisms of weighted regular tree grammars and weighted tree adjoining grammars. We use similar notation and terminology for both, in order to prepare for Section 4, where we investigate the combination of these formalisms through intersection. As a consequence of the required unified notation, we deviate to some degree from standard definitions, without affecting generative power however. For common definitions of weighted regular tree grammars, the reader is referred to (Graehl and Knight, 2004). Weighted tree adjoining grammars are a straightforward generalization of probabilistic (or stochastic) tree adjoining grammars, as introduced by (Resnik, 1992) and (Schabes, 1992). For both regular tree grammars (RTGs) and tree adjoining grammars (TAGs), we will write a labeled and ordered tree as A(α). where A is the label of the root node, and α is a sequence of expressions of the same form that each represent an immediate subtree. In our presentation, labels do not have explicit ranks, that is, the number of children of a node is not determined by its label. This allows an interesting gen</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>J. Graehl and K. Knight. 2004. Training tree transducers. In HLT-NAACL 2004, Proceedings of the Main Conference, Boston, Massachusetts, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Gcseg</author>
<author>M Steinby</author>
</authors>
<title>Tree languages.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>1--68</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="3296" citStr="Gcseg and Steinby, 1997" startWordPosition="518" endWordPosition="522">ased machine translation, trees have however gained much importance, and are even considered as the main objects of study. This is because many MT modules have trees both as input and output, which means the computational strength of such modules can be measured only in terms of the tree languages they accept and the transductions between tree languages that they implement. See for example (Knight, 2007). In contrast, trees have always been the central issue in an important and well-established subfield of formal language theory that studies tree languages, tree automata and tree transducers (Gcseg and Steinby, 1997). The string languages generated by the relevant formalisms in this context are mostly taken to be of secondary importance, if they are considered at all. This paper focuses on tree languages, but involves a technique that was devised for string languages, and shows how the technique carries over to tree languages. The original technique can be seen as the most fundamental idea in the field of context-free parsing, as it captures the essence of 13 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 13–24, Paris, October 2009. c�2009 Association for Computatio</context>
</contexts>
<marker>Gcseg, Steinby, 1997</marker>
<rawString>F. Gcseg and M. Steinby. 1997. Tree languages. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, Vol. 3, chapter 1, pages 1–68. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hopcroft</author>
<author>J D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="8165" citStr="Hopcroft and Ullman, 1979" startWordPosition="1359" endWordPosition="1362"> (5, L, R, s�-), where 5 and L are two finite sets of states and labels, respectively, sl- E 5 is the initial state, and R is a finite set of rules. Each rule has the form: so —* A(si ··· sm) (w), where so, si, ... , sm are states (0 &lt; m), A is a label and w is a weight. Rewriting starts with a string containing only the initial state s�-. This string is repeatedly rewritten by replacing the left-hand side state of a rule by the right-hand side of the same rule, until no state remains. It may be convenient to assume a canonical order of rewriting, for example in terms of leftmost derivations (Hopcroft and Ullman, 1979). Although alternative semirings can be considered, here we always assume that the weights of rules are non-negative real numbers, and the weight of a derivation of a tree is the product of the weights of the rule occurrences. If several (left-most) derivations result in the same tree, then 14 the weight of that tree is given by the sum of the weights of those derivations. Where we are interested in the string language, the weights of trees with the same yield are added to obtain the weight of that yield. A (weighted) context-free grammar can be seen as a special case of a (weighted) regular t</context>
</contexts>
<marker>Hopcroft, Ullman, 1979</marker>
<rawString>J.E. Hopcroft and J.D. Ullman. 1979. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages. Vol 3: Beyond Words, chapter 2,</booktitle>
<pages>69--123</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin/Heidelberg/New York.</location>
<contexts>
<context position="12292" citStr="Joshi and Schabes, 1997" startWordPosition="2125" endWordPosition="2128">oined auxiliary tree and rules of the fourth type correspond to its completion at a foot node, returning to an embedding elementary tree that is encoded in the index that is popped. Rules of the second type correspond to construction of leaves, as in the case of regular tree grammars. See further (Vijay-Shanker and Weir, 1994) for the equivalence of linear indexed grammars and tree adjoining grammars. Note that regular tree grammars can be seen as special cases of linear indexed grammars, by excluding rules of the third and fourth types, which means that stacks of indices always remain empty (Joshi and Schabes, 1997). 2.3 Probabilistic grammars A weighted regular tree grammar, or weighted linear indexed grammar, respectively, is called probabilistic if the weights are probabilities, that is, values between 0 and 1. A probabilistic regular tree grammar (PRTG) is proper if for each state s, the probabilities of all rules that have left-hand side s sum to one. Properness for a probabilistic linear indexed grammar (PLIG) is more difficult to define, due to the possible overlap of applicability between 15 the four types of rules, listed in the section above. However, if we encode a given TAG as a LIG in a reas</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A.K. Joshi and Y. Schabes. 1997. Tree-adjoining grammars. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages. Vol 3: Beyond Words, chapter 2, pages 69–123. Springer-Verlag, Berlin/Heidelberg/New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2000</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="2114" citStr="Jurafsky and Martin, 2000" startWordPosition="323" endWordPosition="326">sing and semantic analysis. In the study of natural languages, parse trees have played a more important role. Whereas linguistic utterances are directly observable and trees deriving them are not, there are nevertheless traditions within linguistics that would see one structural analysis of a sentence as strongly preferred over another. Furthermore, within computational linguistics there are empirical arguments to claim certain parses are correct and others are incorrect. For example, a question answering systems may verifiably give the wrong answer if the question is parsed incorrectly. See (Jurafsky and Martin, 2000) for general discussion on the role of parsing in NLP. Despite the relative importance of strong generative power in computational linguistics, there is still much freedom in how exactly parse trees are shaped and how vertices are labelled, due to the power of semantic analysis that typically follows parsing. This has affected much of the theoretical investigations into the power of linguistic formalisms, and where strong equivalence is considered at all, it is often ”modulo relabelling” or allowing minor structural changes. With the advent of syntax-based machine translation, trees have howev</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>D. Jurafsky and J.H. Martin. 2000. Speech and Language Processing. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kepser</author>
<author>U M¨onnich</author>
</authors>
<title>Closure properties of linear context-free tree languages with an application to optimality theory.</title>
<date>2006</date>
<journal>Theoretical Computer Science,</journal>
<pages>354--82</pages>
<marker>Kepser, M¨onnich, 2006</marker>
<rawString>S. Kepser and U. M¨onnich. 2006. Closure properties of linear context-free tree languages with an application to optimality theory. Theoretical Computer Science, 354:82–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Capturing practical natural language transformations.</title>
<date>2007</date>
<booktitle>Machine Translation,</booktitle>
<pages>21--121</pages>
<contexts>
<context position="3079" citStr="Knight, 2007" startWordPosition="487" endWordPosition="488"> investigations into the power of linguistic formalisms, and where strong equivalence is considered at all, it is often ”modulo relabelling” or allowing minor structural changes. With the advent of syntax-based machine translation, trees have however gained much importance, and are even considered as the main objects of study. This is because many MT modules have trees both as input and output, which means the computational strength of such modules can be measured only in terms of the tree languages they accept and the transductions between tree languages that they implement. See for example (Knight, 2007). In contrast, trees have always been the central issue in an important and well-established subfield of formal language theory that studies tree languages, tree automata and tree transducers (Gcseg and Steinby, 1997). The string languages generated by the relevant formalisms in this context are mostly taken to be of secondary importance, if they are considered at all. This paper focuses on tree languages, but involves a technique that was devised for string languages, and shows how the technique carries over to tree languages. The original technique can be seen as the most fundamental idea in</context>
<context position="34900" citStr="Knight, 2007" startWordPosition="6397" endWordPosition="6398">ree languages, etc. In this section we will restrict ourselves to binary relations however, and call the first surface form the input and the second surface form the output. For synchronous tree adjoining grammars, see for example (Shieber, 1994). If we apply intersection on the input or on the output of a synchronous grammar formalism, then this is best seen as composition. This is wellknown in the case of finite-state transducers and some forms of context-free transduction (Berstel, 1979), and application to a wider range of formalisms is gaining interest in the area of machine translation (Knight, 2007). With the intersection from Section 4.2 trivially extended to composition, we can now implement composition of the form: τ1 o ... o τk, where the different τj are transducers, of which k − 1 are (W)SRTGs and at most one is a (weighted) synchronous LIG ((W)SLIG). The result of the composition is another (W)SLIG. It should be noted that a (W)RTS (or (W)LIG) can be seen as a (W)SRTG (or (W)SLIG, respectively) that represents the identity relation on its tree language. 6.2 Binarization In the discussion of complexity in Section 4.1, we assumed that rules are binary, that is, that they have at mos</context>
</contexts>
<marker>Knight, 2007</marker>
<rawString>K. Knight. 2007. Capturing practical natural language transformations. Machine Translation, 21:121–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Recognition can be harder than parsing.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="22693" citStr="Lang, 1994" startWordPosition="4042" endWordPosition="4043">xpected number of subderivations of the form s[ ] →* α(s&apos;[]) is: E(s, s&apos;) = in(s, s&apos;) · out(s, s&apos;) We will return to this issue in Section 5. 4 Weighted intersection Before we discuss intersection on the level of trees, we first show how a well-established type of intersection on the level of strings, with weighted context-free grammars and weighted finite automata (WFAs), can be trivially extended to replace CFGs with RTGs or LIGs. The intersection paradigm is originally due to (Bar-Hillel et al., 1964). Extension to tree adjoining grammars and linear indexed grammars was proposed before by (Lang, 1994) and (Vijay-Shanker and Weir, 1993b). 4.1 Intersection of string languages Let us assume a WLIG G with terminal and nonterminal labels. Furthermore, we assume a weighted finite automaton A, with an input alphabet equal to the set of terminal labels of G. The transitions of A are of the form: q 7→ q&apos; hwi, a where q and q&apos; are states, a is a terminal symbol, and w is a weight. To simplify the presentation, 18 out(s0) = δ(s0 = s`) + E 11 w · out(s0, s) · in(sj, s) · in(sp) + s0[◦◦] → A(s1[ ] ··· sj[◦◦] ··· sm[ ]) hwi k ∈ {1, ... , sj−1, sj+1, ... , sm} s.t. s0 = sk E 11 w · out(s0) · p ∈/ {j, k} </context>
<context position="27101" citStr="Lang, 1994" startWordPosition="4948" endWordPosition="4949">joining grammars have a time complexity of no less than O(n6). However, in order to solve the recognition problem, an analysis is needed to determine whether G0 allows at least one derivation. The analysis from Figure 1 requires O(|S0 |+ |R01 |· |S0 |+ |R02 |+ |R0 3 |· |R04 |· |S0|) steps, where |S0 |= O(n2) is the number of states of G0, and |R01 |= O(n3), |R02 |= |R03 |= |R04 |= O(n2) are the numbers of rules of G0, divided into the four main types. This leads to an overall time complexity of O(n6), as expected. The observation that recognition can be harder than parsing was made before by (Lang, 1994). The central new insight this provided was that the notion of ‘parsing’ is ill-defined in the literature. One may choose a form in which to capture all parses of an input allowed by a grammar, but different such forms may incur different costs of extracting individual parse trees. In Section 6.2 we will consider the complexity of parsing and recognition if G is not binary. 4.2 Intersection of tree languages We now shift our attention from strings to trees, and consider the intersection of the tree language generated by a weighted linear indexed grammar G1 and the tree language generated by a </context>
</contexts>
<marker>Lang, 1994</marker>
<rawString>B. Lang. 1994. Recognition can be harder than parsing. Computational Intelligence, 10(4):486–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Probabilistic parsing as intersection.</title>
<date>2003</date>
<booktitle>In 8th International Workshop on Parsing Technologies,</booktitle>
<pages>137--148</pages>
<location>LORIA, Nancy, France,</location>
<contexts>
<context position="14501" citStr="Nederhof and Satta, 2003" startWordPosition="2529" endWordPosition="2532">f all such rules equals one. We say a weighted regular tree grammar, or weighted linear indexed grammar, respectively, is consistent if the sum of weights of all (left-most) derivations is one. This is equivalent to saying that the sum of weights of all trees is one, and to saying that the sum of weights of all strings is one. For each consistent WRTG (WLIG, respectively), there is an equivalent proper and consistent PRTG (PLIG, respectively). The proof lies in normalization. For WRTGs this is a trivial extension of normalization of weighted context-free grammars, as described for example by (Nederhof and Satta, 2003). For WLIGs (and weighted TAGs), the problem of normalization also becomes very similar once we consider that the set of derivation trees of tree adjoining grammars can be described with context-free grammars, and that this carries over to weighted derivation trees. See also (Sarkar, 1998). WLIGs seemingly incur an extra complication, if a state may occur in combination with an index on top of the associated stack such that no rules are applicable. However, for LIGs that encode TAGs, the problem does not arise as, informally, one may always resume construction of the embedding elementary tree </context>
</contexts>
<marker>Nederhof, Satta, 2003</marker>
<rawString>M.-J. Nederhof and G. Satta. 2003. Probabilistic parsing as intersection. In 8th International Workshop on Parsing Technologies, pages 137–148, LORIA, Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>A general technique to train language models on language models.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="31394" citStr="Nederhof, 2005" startWordPosition="5756" endWordPosition="5757">Alternatively, one may do both, with a PRTG used in a first phase to heuristically reduce the search space. This section outlines how a suitable PRTG G2 can be extracted out of a PLIG G1, assuming the underlying RTG G02 without weights is already given. The tree language generated by G02 may be an approximation of that generated by G1. The objective is to make G2 as close as possible to G1 in terms of probability distributions over trees. We assume that G02 is unambiguous, that is, for each tree it generates, there is at most one derivation. The procedure is a variant of the one described by (Nederhof, 2005). The idea is that derivations in G1 are mapped to those in G02, via the trees in the intersection of the two tree languages. The probability distribution of states and rules in G2 is estimated based on the expected frequencies of states and rules from G02 in the intersection. Concretely, we turn the RTG G02 into a PRTG G002 that is obtained simply be assigning weight one to all rules. We then compute the intersection grammar G as in Section 4.2. Subsequently, the inside and outside values are computed for G, as explained in Section 3. The expected number of occurrences of a rule in G of the f</context>
<context position="33186" citStr="Nederhof, 2005" startWordPosition="6107" endWordPosition="6108">ent rules s0[◦◦] → A(s1[ ] ··· sj−1[ ] sj[◦◦] sj+1[ ] · · · sm[ ]), we obtain the expected number of occurrences of q0 → A(q1 · · · qm), Let us denote this sum by E(q0 → A(q1 ··· qm)). By summing these for fixed q0, we obtain the expected number of occurrences of q0, which we denote by E(q0). The probability of q0 → A(q1 · · · qm) in G2 is then set to be the ratio of E(q0 → A(q1 · · · qm)) and E(q0). By this procedure, the Kullback-Leibler distance between G1 and G2 is minimized. Although the present paper deals with very different formalisms, the proof of correctness is identical to that in (Nederhof, 2005). The reason is that in both cases the mathematical analysis must focus on the objects in the intersection (strings or trees) which may correspond to multiple derivations in the original model (here G1) but to a single derivation in the unambiguous model to be trained (here G2), and each derivation is composed of rules, whose probabilities are to be multiplied. 6 Extensions 6.1 Transduction For various formalisms describing (string or tree) languages, there are straightforward generalizations that describe a relation between two or more 21 languages, which is known as a transduction. The idea </context>
</contexts>
<marker>Nederhof, 2005</marker>
<rawString>M.-J. Nederhof. 2005. A general technique to train language models on language models. Computational Linguistics, 31(2):173–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Poller</author>
<author>T Becker</author>
</authors>
<title>Two-step TAG parsing revisited.</title>
<date>1998</date>
<booktitle>In Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks,</booktitle>
<pages>143--146</pages>
<institution>Cognitive Science, University of Pennsylvania,</institution>
<note>Institute for Research in</note>
<contexts>
<context position="29810" citStr="Poller and Becker, 1998" startWordPosition="5479" endWordPosition="5482"> recognition of t, or in other words, to determine whether G allows at least one derivation, the analysis from Figure 1 can be used, which has time complexity O(|S |+ |R1 |· |S |+ |R2| + |R3 |· |R4 |· |S|), where |S |= O(n) is the number of states of G, and the numbers of rules are |R1 |= O(n), |R2 |= |R3 |= |R4 |= O(n). Note that |R1 |= O(n) because we have assumed that G2 allows only one derivation of one tree t, 20 hence q0 uniquely determines q1, ... , qm. Overall, we obtain O(n3) steps, which concurs with a known result about the complexity of TAG parsing of trees, as opposed to strings (Poller and Becker, 1998). Another special case is if WLIG G1 simplifies to a WRTG (i.e. the stacks of indices remain always empty), which means we compute the intersection of two weighted regular tree grammars G1 and G2. For recognition, or in other words to decide non-emptiness of the intersection, we can still use Figure 1, although now only inference rules (b) and (d) are applicable (with a small refinement to the algorithm we can block spurious application of (a) where no rules exist that pop indices.) The complexity is determined by (d), which requires O(|G1|·|G2|) steps. 5 Parameter estimation PLIGs allow finer</context>
</contexts>
<marker>Poller, Becker, 1998</marker>
<rawString>P. Poller and T. Becker. 1998. Two-step TAG parsing revisited. In Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks, pages 143–146. Institute for Research in Cognitive Science, University of Pennsylvania, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Probabilistic tree-adjoining grammar as a framework for statistical natural language processing.</title>
<date>1992</date>
<booktitle>In Proc. of the fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>418--424</pages>
<location>Nantes,</location>
<contexts>
<context position="6415" citStr="Resnik, 1992" startWordPosition="1032" endWordPosition="1033">ghted tree adjoining grammars. We use similar notation and terminology for both, in order to prepare for Section 4, where we investigate the combination of these formalisms through intersection. As a consequence of the required unified notation, we deviate to some degree from standard definitions, without affecting generative power however. For common definitions of weighted regular tree grammars, the reader is referred to (Graehl and Knight, 2004). Weighted tree adjoining grammars are a straightforward generalization of probabilistic (or stochastic) tree adjoining grammars, as introduced by (Resnik, 1992) and (Schabes, 1992). For both regular tree grammars (RTGs) and tree adjoining grammars (TAGs), we will write a labeled and ordered tree as A(α). where A is the label of the root node, and α is a sequence of expressions of the same form that each represent an immediate subtree. In our presentation, labels do not have explicit ranks, that is, the number of children of a node is not determined by its label. This allows an interesting generalization, to be discussed in Section 6.2. Where we are interested in the string language generated by a tree-generating grammar, we may distinguish between tw</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>P. Resnik. 1992. Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. In Proc. of the fifteenth International Conference on Computational Linguistics, pages 418– 424. Nantes, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
</authors>
<title>Conditions on consistency of probabilistic tree adjoining grammars.</title>
<date>1998</date>
<booktitle>In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>1164--1170</pages>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="14791" citStr="Sarkar, 1998" startWordPosition="2576" endWordPosition="2577">hts of all strings is one. For each consistent WRTG (WLIG, respectively), there is an equivalent proper and consistent PRTG (PLIG, respectively). The proof lies in normalization. For WRTGs this is a trivial extension of normalization of weighted context-free grammars, as described for example by (Nederhof and Satta, 2003). For WLIGs (and weighted TAGs), the problem of normalization also becomes very similar once we consider that the set of derivation trees of tree adjoining grammars can be described with context-free grammars, and that this carries over to weighted derivation trees. See also (Sarkar, 1998). WLIGs seemingly incur an extra complication, if a state may occur in combination with an index on top of the associated stack such that no rules are applicable. However, for LIGs that encode TAGs, the problem does not arise as, informally, one may always resume construction of the embedding elementary tree below the foot node of an adjoined auxiliary tree. We say a LIG is in TAG-normal form if (a) at least one rule is applicable for each combination of state s and index t such that s[too] is derivable from s�-[ ], and (b) the only overlap in applicability of the four types of rules is betwee</context>
</contexts>
<marker>Sarkar, 1998</marker>
<rawString>A. Sarkar. 1998. Conditions on consistency of probabilistic tree adjoining grammars. In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, volume 2, pages 1164–1170, Montreal, Quebec, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Stochastic lexicalized treeadjoining grammars.</title>
<date>1992</date>
<booktitle>In Proc. of the fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>426--432</pages>
<location>Nantes,</location>
<contexts>
<context position="6435" citStr="Schabes, 1992" startWordPosition="1035" endWordPosition="1036">g grammars. We use similar notation and terminology for both, in order to prepare for Section 4, where we investigate the combination of these formalisms through intersection. As a consequence of the required unified notation, we deviate to some degree from standard definitions, without affecting generative power however. For common definitions of weighted regular tree grammars, the reader is referred to (Graehl and Knight, 2004). Weighted tree adjoining grammars are a straightforward generalization of probabilistic (or stochastic) tree adjoining grammars, as introduced by (Resnik, 1992) and (Schabes, 1992). For both regular tree grammars (RTGs) and tree adjoining grammars (TAGs), we will write a labeled and ordered tree as A(α). where A is the label of the root node, and α is a sequence of expressions of the same form that each represent an immediate subtree. In our presentation, labels do not have explicit ranks, that is, the number of children of a node is not determined by its label. This allows an interesting generalization, to be discussed in Section 6.2. Where we are interested in the string language generated by a tree-generating grammar, we may distinguish between two kinds of labels, t</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Y. Schabes. 1992. Stochastic lexicalized treeadjoining grammars. In Proc. of the fifteenth International Conference on Computational Linguistics, pages 426–432. Nantes, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Schwentick</author>
</authors>
<title>Automata for XML–a survey.</title>
<date>2007</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>73--289</pages>
<contexts>
<context position="38138" citStr="Schwentick, 2007" startWordPosition="6950" endWordPosition="6951">f the above extended definitions is that labels appear not only without fixed ranks, as we have assumed from the start in Section 2, but even without a bound on the rank. Concretely, a vertex may appear with any number of children in a tree. Whereas this may be unconventional in certain areas of formal language theory, it is a well-accepted practice in the parsing of natural language to make the number of constituents of syntactic categories flexible and conceptually unbounded; see for example 22 (Collins, 1997). Also the literature on unranked tree automata is very relevant; see for example (Schwentick, 2007). Binarization for LIGs was considered before by (Vijay-Shanker and Weir, 1993a). 6.3 Beyond TAGs In the light of results by (Kepser and M¨onnich, 2006) it is relatively straightforward to consider larger classes of linear context-free tree grammars in place of tree-adjoining grammars, in order to generalize the construction in Section 4.2. The generalization described in what follows seems less straightforward. Context-free languages can be characterized in terms of parse trees in which path sets (sets of strings of labels on paths from the root to a leaf) are regular. In the case of tree adj</context>
</contexts>
<marker>Schwentick, 2007</marker>
<rawString>Thomas Schwentick. 2007. Automata for XML–a survey. Journal of Computer and System Sciences, 73:289–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>Restricting the weak-generative capacity of synchronous tree-adjoining grammars.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="34533" citStr="Shieber, 1994" startWordPosition="6336" endWordPosition="6337">wo or more surface forms that are synchronously produced. For example, a rule in a weighted synchronous regular tree grammar (WSRTG) has the form: s0 —* A(s1 ··· sm), B(sπ(1) ··· sπ(m)) (w), where π is a permutation of 1, ... , m. We can generalize this to having a third label C and a second permutation π&apos;, in order to describe simultaneous relations between three tree languages, etc. In this section we will restrict ourselves to binary relations however, and call the first surface form the input and the second surface form the output. For synchronous tree adjoining grammars, see for example (Shieber, 1994). If we apply intersection on the input or on the output of a synchronous grammar formalism, then this is best seen as composition. This is wellknown in the case of finite-state transducers and some forms of context-free transduction (Berstel, 1979), and application to a wider range of formalisms is gaining interest in the area of machine translation (Knight, 2007). With the intersection from Section 4.2 trivially extended to composition, we can now implement composition of the form: τ1 o ... o τk, where the different τj are transducers, of which k − 1 are (W)SRTGs and at most one is a (weight</context>
</contexts>
<marker>Shieber, 1994</marker>
<rawString>S.M. Shieber. 1994. Restricting the weak-generative capacity of synchronous tree-adjoining grammars. Computational Intelligence, 10(4):371–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima’an</author>
</authors>
<title>Efficient disambiguation by means of stochastic tree substitution grammars.</title>
<date>1997</date>
<booktitle>New Methods in Language Processing.</booktitle>
<editor>In D. Jones and H. Somers, editors,</editor>
<publisher>UCL Press, UK.</publisher>
<marker>Sima’an, 1997</marker>
<rawString>K. Sima’an. 1997. Efficient disambiguation by means of stochastic tree substitution grammars. In D. Jones and H. Somers, editors, New Methods in Language Processing. UCL Press, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sippu</author>
<author>E Soisalon-Soininen</author>
</authors>
<title>Parsing Theory, Vol. I: Languages and Parsing,</title>
<date>1988</date>
<journal>of EATCS Monographs on Theoretical Computer Science.</journal>
<volume>15</volume>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="16370" citStr="Sippu and Soisalon-Soininen, 1988" startWordPosition="2855" endWordPosition="2858">if it does not contain useless rules. Whereas most grammars written by hand or induced by a corpus or treebank are reduced, there are practical operations that turn reduced grammars into grammars with useless rules; we will see an example in the next section, where grammars are constructed that generate the intersection of two given languages. In order to determine whether the intersection is non-empty, it suffices to identify useless rules in the intersection grammar. If and only if all rules are useless, the generated language is empty. In the case of context-free grammars (see for example (Sippu and Soisalon-Soininen, 1988)), the analysis to identify useless rules can be split into two phases: 1. a bottom-up phase to identify the grammar symbols that generate substrings, which may include the start symbol if the generated language is non-empty; and 2. a top-down phase to identify the grammar symbols that are reachable from the start symbol. The intersection of the generating symbols and the reachable symbols gives the set of useful symbols. One can then identify useless rules as those that contain one or more symbols that are not useful. The procedure for linear indexed grammars is similarly split into two phase</context>
</contexts>
<marker>Sippu, Soisalon-Soininen, 1988</marker>
<rawString>S. Sippu and E. Soisalon-Soininen. 1988. Parsing Theory, Vol. I: Languages and Parsing, volume 15 of EATCS Monographs on Theoretical Computer Science. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>Parsing some constrained grammar formalisms.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="22727" citStr="Vijay-Shanker and Weir, 1993" startWordPosition="4045" endWordPosition="4048"> subderivations of the form s[ ] →* α(s&apos;[]) is: E(s, s&apos;) = in(s, s&apos;) · out(s, s&apos;) We will return to this issue in Section 5. 4 Weighted intersection Before we discuss intersection on the level of trees, we first show how a well-established type of intersection on the level of strings, with weighted context-free grammars and weighted finite automata (WFAs), can be trivially extended to replace CFGs with RTGs or LIGs. The intersection paradigm is originally due to (Bar-Hillel et al., 1964). Extension to tree adjoining grammars and linear indexed grammars was proposed before by (Lang, 1994) and (Vijay-Shanker and Weir, 1993b). 4.1 Intersection of string languages Let us assume a WLIG G with terminal and nonterminal labels. Furthermore, we assume a weighted finite automaton A, with an input alphabet equal to the set of terminal labels of G. The transitions of A are of the form: q 7→ q&apos; hwi, a where q and q&apos; are states, a is a terminal symbol, and w is a weight. To simplify the presentation, 18 out(s0) = δ(s0 = s`) + E 11 w · out(s0, s) · in(sj, s) · in(sp) + s0[◦◦] → A(s1[ ] ··· sj[◦◦] ··· sm[ ]) hwi k ∈ {1, ... , sj−1, sj+1, ... , sm} s.t. s0 = sk E 11 w · out(s0) · p ∈/ {j, k} in(sp) + s0[◦◦] → A(s1[ ] ··· sj[◦</context>
<context position="38216" citStr="Vijay-Shanker and Weir, 1993" startWordPosition="6959" endWordPosition="6962">hout fixed ranks, as we have assumed from the start in Section 2, but even without a bound on the rank. Concretely, a vertex may appear with any number of children in a tree. Whereas this may be unconventional in certain areas of formal language theory, it is a well-accepted practice in the parsing of natural language to make the number of constituents of syntactic categories flexible and conceptually unbounded; see for example 22 (Collins, 1997). Also the literature on unranked tree automata is very relevant; see for example (Schwentick, 2007). Binarization for LIGs was considered before by (Vijay-Shanker and Weir, 1993a). 6.3 Beyond TAGs In the light of results by (Kepser and M¨onnich, 2006) it is relatively straightforward to consider larger classes of linear context-free tree grammars in place of tree-adjoining grammars, in order to generalize the construction in Section 4.2. The generalization described in what follows seems less straightforward. Context-free languages can be characterized in terms of parse trees in which path sets (sets of strings of labels on paths from the root to a leaf) are regular. In the case of tree adjoining languages, the path sets are context-free. There is a hierarchy of clas</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1993</marker>
<rawString>K. Vijay-Shanker and D.J. Weir. 1993a. Parsing some constrained grammar formalisms. Computational Linguistics, 19(4):591–636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>The use of shared forests in tree adjoining grammar parsing.</title>
<date>1993</date>
<booktitle>In Sixth Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>384--393</pages>
<location>Utrecht, The Netherlands,</location>
<contexts>
<context position="22727" citStr="Vijay-Shanker and Weir, 1993" startWordPosition="4045" endWordPosition="4048"> subderivations of the form s[ ] →* α(s&apos;[]) is: E(s, s&apos;) = in(s, s&apos;) · out(s, s&apos;) We will return to this issue in Section 5. 4 Weighted intersection Before we discuss intersection on the level of trees, we first show how a well-established type of intersection on the level of strings, with weighted context-free grammars and weighted finite automata (WFAs), can be trivially extended to replace CFGs with RTGs or LIGs. The intersection paradigm is originally due to (Bar-Hillel et al., 1964). Extension to tree adjoining grammars and linear indexed grammars was proposed before by (Lang, 1994) and (Vijay-Shanker and Weir, 1993b). 4.1 Intersection of string languages Let us assume a WLIG G with terminal and nonterminal labels. Furthermore, we assume a weighted finite automaton A, with an input alphabet equal to the set of terminal labels of G. The transitions of A are of the form: q 7→ q&apos; hwi, a where q and q&apos; are states, a is a terminal symbol, and w is a weight. To simplify the presentation, 18 out(s0) = δ(s0 = s`) + E 11 w · out(s0, s) · in(sj, s) · in(sp) + s0[◦◦] → A(s1[ ] ··· sj[◦◦] ··· sm[ ]) hwi k ∈ {1, ... , sj−1, sj+1, ... , sm} s.t. s0 = sk E 11 w · out(s0) · p ∈/ {j, k} in(sp) + s0[◦◦] → A(s1[ ] ··· sj[◦</context>
<context position="38216" citStr="Vijay-Shanker and Weir, 1993" startWordPosition="6959" endWordPosition="6962">hout fixed ranks, as we have assumed from the start in Section 2, but even without a bound on the rank. Concretely, a vertex may appear with any number of children in a tree. Whereas this may be unconventional in certain areas of formal language theory, it is a well-accepted practice in the parsing of natural language to make the number of constituents of syntactic categories flexible and conceptually unbounded; see for example 22 (Collins, 1997). Also the literature on unranked tree automata is very relevant; see for example (Schwentick, 2007). Binarization for LIGs was considered before by (Vijay-Shanker and Weir, 1993a). 6.3 Beyond TAGs In the light of results by (Kepser and M¨onnich, 2006) it is relatively straightforward to consider larger classes of linear context-free tree grammars in place of tree-adjoining grammars, in order to generalize the construction in Section 4.2. The generalization described in what follows seems less straightforward. Context-free languages can be characterized in terms of parse trees in which path sets (sets of strings of labels on paths from the root to a leaf) are regular. In the case of tree adjoining languages, the path sets are context-free. There is a hierarchy of clas</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1993</marker>
<rawString>K. Vijay-Shanker and D.J. Weir. 1993b. The use of shared forests in tree adjoining grammar parsing. In Sixth Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, pages 384–393, Utrecht, The Netherlands, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>The equivalence of four extensions of context-free grammars.</title>
<date>1994</date>
<booktitle>Mathematical Systems Theory,</booktitle>
<pages>27--511</pages>
<contexts>
<context position="11996" citStr="Vijay-Shanker and Weir, 1994" startWordPosition="2076" endWordPosition="2079">terms of tree adjoining grammars, which assume a finite number of elementary trees, the intuition behind the four types of rules is as follows. Rules of the first type correspond to continued construction of the same elementary tree. Rules of the third type correspond to the initiation of a newly adjoined auxiliary tree and rules of the fourth type correspond to its completion at a foot node, returning to an embedding elementary tree that is encoded in the index that is popped. Rules of the second type correspond to construction of leaves, as in the case of regular tree grammars. See further (Vijay-Shanker and Weir, 1994) for the equivalence of linear indexed grammars and tree adjoining grammars. Note that regular tree grammars can be seen as special cases of linear indexed grammars, by excluding rules of the third and fourth types, which means that stacks of indices always remain empty (Joshi and Schabes, 1997). 2.3 Probabilistic grammars A weighted regular tree grammar, or weighted linear indexed grammar, respectively, is called probabilistic if the weights are probabilities, that is, values between 0 and 1. A probabilistic regular tree grammar (PRTG) is proper if for each state s, the probabilities of all r</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1994</marker>
<rawString>K. Vijay-Shanker and D.J. Weir. 1994. The equivalence of four extensions of context-free grammars. Mathematical Systems Theory, 27:511–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Weir</author>
</authors>
<title>A geometric hierarchy beyond context-free languages.</title>
<date>1992</date>
<journal>Theoretical Computer Science,</journal>
<pages>104--235</pages>
<contexts>
<context position="38928" citStr="Weir, 1992" startWordPosition="7078" endWordPosition="7079">orward to consider larger classes of linear context-free tree grammars in place of tree-adjoining grammars, in order to generalize the construction in Section 4.2. The generalization described in what follows seems less straightforward. Context-free languages can be characterized in terms of parse trees in which path sets (sets of strings of labels on paths from the root to a leaf) are regular. In the case of tree adjoining languages, the path sets are context-free. There is a hierarchy of classes of languages in which the third step is to consider path sets that are tree adjoining languages (Weir, 1992). In this paper, we have considered the parsing-asintersection paradigm for the first two members of the hierarchy. It may be possible that the paradigm is also applicable to the third and following members. This avenue is yet to be pursued. 7 Conclusions This paper has extended the parsing-asintersection paradigm from string languages to tree languages. Probabilities, or weights in general, were incorporated in this framework in a natural way. We have discussed one particular application involving a special case of the extended paradigm. Acknowledgements Helpful comments by anonymous reviewer</context>
</contexts>
<marker>Weir, 1992</marker>
<rawString>D.J. Weir. 1992. A geometric hierarchy beyond context-free languages. Theoretical Computer Science, 104:235–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wojtczak</author>
<author>K Etessami</author>
</authors>
<title>PReMo: an analyzer for Probabilistic Recursive Models.</title>
<date>2007</date>
<booktitle>In Tools and Algorithms for the Construction and Analysis of Systems, 13th International Conference,</booktitle>
<volume>4424</volume>
<pages>66--71</pages>
<publisher>Springer-Verlag.</publisher>
<location>Braga,</location>
<contexts>
<context position="21342" citStr="Wojtczak and Etessami, 2007" startWordPosition="3807" endWordPosition="3810">negative solution to a system of equations. The function δ with a boolean argument evaluates to 1 if the condition is true and to 0 otherwise. express exactly the ‘inside’ value of the weighted context-free grammar that we can extract out of the deduction system from Figure 1, by instantiating the inference rules in all possible ways, and then taking the consequent as the left-hand side of a rule, and the antecedent as the right-hand side. The weight is the product of weights of rules that appear in the side conditions. It is possible to effectively solve the system of equations, as shown by (Wojtczak and Etessami, 2007). In the same vein we can compute ‘outside’ values for weighted linear indexed grammars, as straightforward analogues of the outside values of weighted and probabilistic context-free grammars. The outside value is the sum of weights of partial derivations that may lie ‘outside’ a subderivation s[ ] →* α in the case of out(s), or a subderivation s[ ] →* α(s&apos;[ ]) in the case of out(s, s&apos;). The equations in Figure 3 again follow trivially from the view of Figure 1 as weighted context-free grammar and the usual definition of outside values. The functions in and out are particularly useful for PLIG</context>
</contexts>
<marker>Wojtczak, Etessami, 2007</marker>
<rawString>D. Wojtczak and K. Etessami. 2007. PReMo: an analyzer for Probabilistic Recursive Models. In Tools and Algorithms for the Construction and Analysis of Systems, 13th International Conference, volume 4424 of Lecture Notes in Computer Science, pages 66–71, Braga, Portugal. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>