<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008317">
<title confidence="0.988688">
Interactive Feature Space Construction using Semantic Information
</title>
<author confidence="0.998475">
Dan Roth and Kevin Small
</author>
<affiliation confidence="0.9987575">
Department of Computer Science
University of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.814271">
Urbana, IL 61801
</address>
<email confidence="0.999101">
{danr,ksmall}@illinois.edu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999198428571429">
Specifying an appropriate feature space is an
important aspect of achieving good perfor-
mance when designing systems based upon
learned classifiers. Effectively incorporat-
ing information regarding semantically related
words into the feature space is known to pro-
duce robust, accurate classifiers and is one ap-
parent motivation for efforts to automatically
generate such resources. However, naive in-
corporation of this semantic information may
result in poor performance due to increased
ambiguity. To overcome this limitation, we
introduce the interactive feature space con-
struction protocol, where the learner identi-
fies inadequate regions of the feature space
and in coordination with a domain expert adds
descriptiveness through existing semantic re-
sources. We demonstrate effectiveness on an
entity and relation extraction system includ-
ing both performance improvements and ro-
bustness to reductions in annotated data.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973">
An important natural language processing (NLP)
task is the design of learning systems which per-
form well over a wide range of domains with limited
training data. While the NLP community has a long
tradition of incorporating linguistic information into
statistical systems, machine learning approaches to
these problems often emphasize learning sophisti-
cated models over simple, mostly lexical, features.
This trend is not surprising as a primary motivation
for machine learning solutions is to reduce the man-
ual effort required to achieve state of the art perfor-
</bodyText>
<page confidence="0.871043">
66
</page>
<bodyText confidence="0.999026724137931">
mance. However, one notable advantage of discrimi-
native classifiers is the capacity to encode arbitrarily
complex features, which partially accounts for their
popularity. While this flexibility is powerful, it often
overwhelms the system designer causing them to re-
sort to simple features. This work presents a method
to partially automate feature engineering through an
interactive learning protocol.
While it is widely accepted that classifier perfor-
mance is predicated on feature engineering, design-
ing good features requires significant effort. One un-
derutilized resource for descriptive features are ex-
isting semantically related word lists (SRWLs), gen-
erated both manually (Fellbaum, 1998) and automat-
ically (Pantel and Lin, 2002). Consider the follow-
ing named entity recognition (NER) example:
His father was rushed to [Westlake
Hospital]ORG, an arm of [Resurrection
Health Care]ORG, in west suburban
[Chicagoland]LOC.
For such tasks, it is helpful to know that west is
a member of the SRWL [Compass Direction] and
other such designations. If extracting features using
this information, we would require observing only
a subset of the SRWL in the data to learn the cor-
responding parameter. This statement suggests that
one method for learning robust classifiers is to in-
corporate semantic information through features ex-
tracted from the more descriptive representation:
</bodyText>
<footnote confidence="0.98665825">
His father was rushed to Westlake [Health
Care Institution], an [Subsidiary] of Resur-
rection Health Care, [Locative Preposition]
[Compass Direction] suburban Chicagoland.
</footnote>
<note confidence="0.994252">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 66–74,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999640344827586">
Deriving discriminative features from this rep-
resentation often results in more informative fea-
tures and a correspondingly simpler classification
task. Although effective approaches along this vein
have been shown to induce more accurate classi-
fiers (Boggess et al., 1991; Miller et al., 2004; Li and
Roth, 2005), naive approaches may instead result in
higher sample complexity due to increased ambi-
guity introduced through these semantic resources.
Features based upon SRWLs must therefore balance
the tradeoff between descriptiveness and noise.
This paper introduces the interactive feature
space construction (IFSC) protocol, which facil-
itates coordination between a domain expert and
learning algorithm to interactively define the feature
space during training. This paper describes the par-
ticular instance of the IFSC protocol where seman-
tic information is introduced through abstraction of
lexical terms in the feature space with their SRWL
labels. Specifically, there are two notable contri-
butions of this work: (1) an interactive method for
the expert to directly encode semantic knowledge
into the feature space with minimal effort and (2) a
querying function which uses both the current state
of the learner and properties of the available SRWLs
to select informative instances for presentation to
the expert. We demonstrate the effectiveness of this
protocol on an entity and relation extraction task in
terms of performance and labeled data requirements.
</bodyText>
<sectionHeader confidence="0.982822" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.99933175">
Following standard notation, let x ∈ X represent
members of an input domain and y ∈ Y represent
members of an output domain where a learning al-
gorithm uses a training sample S = {(xi, yi)}mi=1
to induce a prediction function h : X → Y. We
are specifically interested in discriminative classi-
fiers which use a feature vector generating procedure
Φ(x) → x, taking an input domain member x and
generating a feature vector x. We further assume the
output assignment of h is based upon a scoring func-
tion f : Φ(X) × Y → R such that the prediction is
stated as yˆ = h(x) = argmaxy,EY f(x, y&apos;).
The feature vector generating procedure is com-
posed of a vector of feature generation functions
(FGFs), Φ(x) = hΦ1(x), Φ2(x), ... , Φn(x)i, where
each feature generation function, Φi(x) → {0, 1},
takes the input x and returns the appropriate fea-
ture vector value. Consider the text “in west sub-
urban Chicagoland” where we wish to predict the
entity classification for Chicagoland. In this case,
example active FGFs include Φtext=Chicagoland,
ΦisCapitalized, and Φtext(−2)=west while FGFs such
as Φtext=and would remain inactive. Since we are
constructing sparse feature vectors, we use the infi-
nite attribute model (Blum, 1992).
Semantically related word list (SRWL) feature
abstraction begins with a set of variable sized
word lists {W} such that each member lexical
element (i.e. word, phrase) has at least one
sense that is semantically related to the concept
represented by W (e.g. Wcompass direction =
north, east, ... , southwest). For the purpose of
feature extraction, whenever the sense of a lexical el-
ement associated with a particular W appears in the
corpus, it is replaced by the name of the correspond-
ing SRWL. This is equivalent to defining a FGF for
the specified W which is a disjunction of the func-
tionally related FGFs over the member lexical ele-
ments (e.g. ΦtextEWcompass direction = Φtext=north ∨
Φtext=east ∨ . . . ∨ Φtext=southwest).
</bodyText>
<sectionHeader confidence="0.995905" genericHeader="method">
3 Interactive Feature Space Construction
</sectionHeader>
<bodyText confidence="0.999977952380953">
The machine learning community has become in-
creasingly interested in protocols which allow inter-
action with a domain expert during training, such as
the active learning protocol (Cohn et al., 1994). In
active learning, the learning algorithm reduces the
labeling effort by using a querying function to in-
crementally select unlabeled examples from a data
source for annotation during learning. By care-
fully selecting examples for annotation, active learn-
ing maximizes the quality of inductive information
while minimizing label acquisition cost.
While active learning has been shown to reduce
sample complexity, we contend that it significantly
underutilizes the domain expert – particularly for
complex annotation tasks. More precisely, when a
domain expert receives an instance, world knowl-
edge is used to reason about the instance and sup-
ply an annotation. Once annotated and provided for
training, the learner must recover this world knowl-
edge and incorporate it into its model from a small
number of instances, exclusively through induction.
</bodyText>
<page confidence="0.998986">
67
</page>
<bodyText confidence="0.999982885714286">
Learning algorithms generally assume that the
feature space and model are specified before learn-
ing begins and remain static throughout learning,
where training data is exclusively used for parameter
estimation. Conversely, the interactive feature space
construction (IFSC) protocol relaxes this static fea-
ture space assumption by using information about
the current state of the learner, properties of knowl-
edge resources (e.g. SRWLs, gazetteers, unlabeled
data, etc.), and access to the domain expert during
training to interactively improve the feature space.
Whereas active learning focuses on the labeling ef-
fort, IFSC reduces sample complexity and improves
performance by modifying the underlying represen-
tation to simplify the overall learning task.
The IFSC protocol for SRWL abstraction is pre-
sented in Algorithm 1. Given a labeled data set S,
an initial feature vector generating procedure 4&gt;0, a
querying function Q : S × h → Sselect, and an
existing set of semantically related word lists, {W}
(line 1), an initial hypothesis is learned (line 3). The
querying function scores the labeled examples and
selects an instance for interaction (line 6). The ex-
pert selects lexical elements from this instance for
which feature abstractions may be performed (line
8). If the expert doesn’t deem any elements vi-
able for interaction, the algorithm returns to line 5.
Once lexical elements are selected for interaction,
the SRWL WE associated with each selected element
is retrieved (line 11) and refined by the expert (line
12). Using the validated SRWL definition W∗E , the
lexical FGFs are replaced with the SRWL FGF (line
14). This new feature vector generating procedure
4&gt;t+1 is used to train a new classifier (line 18) and
the algorithm is repeated until the annotator halts.
</bodyText>
<subsectionHeader confidence="0.99998">
3.1 Method of Expert Interaction
</subsectionHeader>
<bodyText confidence="0.959208">
The method of interaction for active learning is
very natural; data annotation is required regardless.
To increase the bandwidth between the expert and
learner, a more sophisticated interaction must be al-
lowed while ensuring that the expert task of remains
reasonable. We require the interaction be restricted
to mouse clicks. When using this protocol to in-
corporate semantic information, the primary tasks
of the expert are (1) selecting lexical elements for
SRWL feature abstraction and (2) validating mem-
bership of the SRWL for the specified application.
Algorithm 1 Interactive Feature Space Construction
</bodyText>
<listItem confidence="0.997933666666667">
1: Input: Labeled training data S, feature vector
generating procedure 4&gt;0, querying function Q,
set of known SRWLs {W }, domain expert A∗
2: t ← 0
3: ht ← A(4&gt;t, S); learn initial hypothesis
4: Sselected ← ∅
5: while annotator is willing do
6: Sselect ← Q(S\Sselected, ht); Q proposes
(labeled) instance for interaction
7: Sselected ← Sselected ∪ Sselect; mark selected
examples to prevent reselection
8: Eselect ← A∗(Sselect); the expert selects lex-
ical elements for semantic abstraction
9: 4&gt;t+1 ← 4&gt;t; initialize new FGF vector with
existing FGFs
10: for each E ∈ Eselect do
11: Retrieve word list WE
12: W∗E ← A∗(WE); the expert refines the ex-
isting semantic class WE for this task
13: for each O ∼ E do
14: 4&gt;t+1 ← (4&gt;t+1\O) ∪ OW∗� ; re-
place features with SRWL features (e.g.
Otext=E → Otext∈W∗� )
15: end for
16: end for
17: t ← t + 1
18: ht ← A(4&gt;t, S); learn new hypothesis
19: end while
20: Output: Learned hypothesis hT, final feature
space 4&gt;T, refined semantic classes {W∗}
</listItem>
<subsectionHeader confidence="0.935686">
3.1.1 Lexical Feature Selection (Line 8)
</subsectionHeader>
<bodyText confidence="0.999961538461538">
Once an instance is selected by the querying func-
tion (line 6), the the domain expert selects lexical el-
ements (i.e. words, phrases) believed appropriate for
SRWL feature abstraction. This step is summarized
by Figure 1 for the example introduced in Section 1.
For this NER example, features extracted include
the words and bigrams which form the named en-
tity and those within a surrounding two word win-
dow. All lexical elements which have membership
to at least one SRWL and are used for feature ex-
traction are marked with a box and may be selected
by the user for interaction. In this particular case,
the system has made a mistake in classification of
</bodyText>
<page confidence="0.998562">
68
</page>
<figureCaption confidence="0.999325">
Figure 1: Lexical Feature Selection – All lexical ele-
ments with SRWL membership used to derive features
are boxed. Elements used for the incorrect prediction for
Chicagoland are double-boxed. The expert may select
any boxed element for SRWL validation.
</figureCaption>
<bodyText confidence="0.999917">
Chicagoland and the lexical elements used to derive
features for this prediction are emphasized with a
double-box for expository purposes. The expert se-
lects lexical elements which they believe will result
in good feature abstractions; the querying function
must present examples believed to have high impact.
</bodyText>
<subsectionHeader confidence="0.79056">
3.1.2 Word List Validation (Lines 11 &amp;12)
</subsectionHeader>
<bodyText confidence="0.99540265">
Once the domain expert has selected a lexical el-
ement for SRWL feature abstraction, they are pre-
sented with the SRWL W to validate membership
for the target application as shown in Figure 2. In
this particular case, the expert has chosen to perform
two interactions, namely for the lexical elements
west and suburban. Once they have chosen which
words and phrases will be included in this particular
feature abstraction, W is updated and the associated
features are replaced with their SRWL counterpart.
For example, Φtext=west, Φtext=north, etc. would all
be replaced with Φtext∈WA,a0s later in lines 13 &amp; 14.
A1806: southeast, northeast, south
southeast, northeast, south, north, south-
inland,
west, west, east, northwest, outside
A1558: suburban, nearby, downtown
downtown,
suburban, nearby, urban,
metropolitan, neighboring, near, coastal
</bodyText>
<figureCaption confidence="0.94485575">
Figure 2: Word List Validation – Completing two domain
expert interactions. Upon selecting either double-boxed
element in Figure 1, the expert validates the respective
SRWL for feature extraction.
</figureCaption>
<bodyText confidence="0.999773923076923">
Accurate sense disambiguation is helpful for ef-
fective SRWL feature abstraction to manage situa-
tions where lexical elements belong to multiple lists.
In this work, we first disambiguate by predicted part
of speech (POS) tags. In cases of multiple SRWL
senses for a POS, the given SRWLs (Pantel and Lin,
2002) rank list elements according their semantic
representativeness which we use to return the high-
est ranked sense for a particular lexical element.
Also, as SRWL resources emphasize recall over pre-
cision, we reduce expert effort by using the Google
n-gram counts (Brandts and Franz, 2006) to auto-
matically prune SRWLs.
</bodyText>
<subsectionHeader confidence="0.999814">
3.2 Querying Function (Line 6)
</subsectionHeader>
<bodyText confidence="0.9998814">
A primary contribution of this work is designing an
appropriate querying function. In doing so, we look
to maximize the impact of interactions while min-
imizing the total number. Therefore, we look to
select instances for which (1) the current hypoth-
esis indicates the feature space is insufficient and
(2) the resulting SRWL feature abstraction will help
improve performance. To account for these two
somewhat orthogonal goals, we design two query-
ing functions and aggregate their results.
</bodyText>
<subsectionHeader confidence="0.524282">
3.2.1 Hypothesis-Driven Querying
</subsectionHeader>
<bodyText confidence="0.999991444444444">
To find areas of the feature space which are be-
lieved to require more descriptiveness, we look to
emphasize those instances which will result in the
largest updates to the hypothesis. To accomplish
this, we adopt an idea from the active learning
community and score instances according to their
margin relative to the current learned hypothesis,
p(ft, xi, yi) (Tong and Koller, 2001). This results
in the hypothesis-driven querying function
</bodyText>
<equation confidence="0.9682885">
Qmargin = argsort p(ft, xi, yi)
i=1,...,m
</equation>
<bodyText confidence="0.999366090909091">
where the argsort operator is used to sort the input
elements in ascending order (for multiple instance
selection). Unlike active learning, where selection
is from an unlabeled data source, the quantity of la-
beled data is fixed and labeled data is selected during
each round. Therefore, we use the true margin and
not the expected margin. This means that we will
first select instances which have large mistakes, fol-
lowed by those instances with small mistakes, and
finally instances that make correct predictions in the
order of their confidence.
</bodyText>
<table confidence="0.97205075">
His father was rushed to [Westlake
Hospital ]ORG, an arm of [Resurrection
Health Care ]ORG, in west suburban
[Chicagoland]ORG.
</table>
<page confidence="0.979045">
69
</page>
<subsubsectionHeader confidence="0.627369">
3.2.2 SRWL-Driven Querying
</subsubsectionHeader>
<bodyText confidence="0.999244789473684">
An equally important goal of the querying func-
tion is to present examples which will result in
SRWL feature abstractions of broad usability. Intu-
itively, there are two criteria distinguishing desirable
SRWLs for this purpose. First of all, large lists are
desirable as there are many lists of cities, countries,
corporations, etc. which are extremely informative.
Secondly, preference should be given to lists where
the distribution of lexical elements within a particu-
lar word list, c E W, is more uniform. For example,
consider W = {devour, feed on, eat, consume}.
While all of these terms belong to the same SRWL,
learning features based on eat is sufficient to cover
most examples. To derive a SRWL-driven querying
function based on these principles, we use the word
list entropy, H(W) = − &amp;∈W p(c) logp(c). The
querying score for a sentence is determined by its
highest entropy lexical element used for feature ex-
traction, resulting in the querying function
</bodyText>
<equation confidence="0.977675">
Qentropy = argsortI argmin −H(M)
i=1,...,m L E∼ Dxi J
</equation>
<bodyText confidence="0.999969041666667">
This querying function is supported by the under-
lying assumption of SRWL abstraction is that there
exists a true feature space Φ∗(x) which is built upon
SRWLs and lexical elements but is being approxi-
mated by Φ(x), which doesn’t use semantic infor-
mation. In this context, a lexical feature provides
one bit of information to the prediction function
while a SRWL feature provides information content
proportional to its SRWL entropy H(W).
To study one aspect of this phenomena empiri-
cally, we examine the rate at which words are first
encountered in our training corpus from Section 4,
as shown by Figure 3. The first observation is
the usefulness of SRWL feature abstraction in gen-
eral as we see that when including an entire SRWL
from (Pantel and Lin, 2002) whenever the first ele-
ment of the list is encountered, we cover the unigram
vocabulary much more rapidly. The second observa-
tion is that when sentences are presented in the or-
der of the average SRWL entropy of their words, this
coverage rate is further accelerated. Figure 3 helps
explain the recall focused aspect of SRWL abstrac-
tion while we rely on hypothesis-driven querying to
target interactions for the specific task at hand.
</bodyText>
<figure confidence="0.981369">
0 200 400 600 800 1000
sentences
</figure>
<figureCaption confidence="0.992641833333333">
Figure 3: The Impact of SRWL Abstraction and SRWL-
driven Querying – The first occurrence of words occur at
a much lower rate than the first occurrence of words when
abstracted through SRWLs, particularly when sentences
are introduced as ranked by average SRWL entropy cal-
culated using (Brandts and Franz, 2006).
</figureCaption>
<subsectionHeader confidence="0.9792">
3.2.3 Aggregating Querying Functions
</subsectionHeader>
<bodyText confidence="0.999885428571429">
To combine these two measures, we use the Borda
count method of rank aggregation (Young, 1974) to
find a consensus between the two querying func-
tions without requiring calibration amongst the ac-
tual ranking scores. Defining the rank position of
an instance by r(x), the Borda count based querying
function is stated by
</bodyText>
<equation confidence="0.9734625">
QBorda = argsort [rmargin(xi) + rentropy(xi)]
i=1,...,m
</equation>
<bodyText confidence="0.999509666666667">
QBorda selects instances which consider both wide
applicability through rentropy and which focus on
the specific task through rmargin.
</bodyText>
<sectionHeader confidence="0.998156" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999960833333333">
To demonstrate the IFSC protocol on a practical ap-
plication, we examine a three-stage pipeline model
for entity and relation extraction, where the task is
decomposed into sequential stages of segmentation,
entity classification, and relation classification (Roth
and Small, 2008). Extending the standard classifi-
cation task, a pipeline model decomposes the over-
all classification into a sequence of D stages such
that each stage d = 1, ... , D has access to the in-
put instance along with the classifications from all
previous stages, ˆy(d). Each stage of the pipeline
model uses a feature vector generating procedure
</bodyText>
<figure confidence="0.9972815">
0.8
0.6
0.4
0.2
0
1
SRWL - Entropy
SRWL - Sequence
Lexical - Sequence
unigram type coverage (%)
</figure>
<page confidence="0.985879">
70
</page>
<bodyText confidence="0.998901333333333">
b(d)(x, ˆy(0), ... , ˆy(d−1)) → x(d) to learn a hypoth-
esis h(d). Once each stage of the pipelined classifier
is learned, predictions are made sequentially, where
sification uses the same features as entity classifica-
tion along with the entity labels, the length of the
entities, and the number of tokens between them.
</bodyText>
<equation confidence="0.97289625">
� l)d=1
D
yˆ = h(x) = argmax f (d) (x(d), y )
y,∈Y(d)
</equation>
<bodyText confidence="0.99709">
Each pipeline stage requires a classifier which
makes multiple interdependent predictions based on
input from multiple sentence elements x E X1 x
</bodyText>
<equation confidence="0.7905215">
· · · x Xnx using a structured output space, y(d) E
Y(d) 1x · · · x Y(d)
</equation>
<bodyText confidence="0.993667918918919">
ny . More specifically, segmenta-
tion makes a prediction for each sentence word over
Y E {begin, inside, outside} and constraints are
enforced between predictions to ensure that an in-
side label can only follow a begin label. Entity clas-
sification begins with the results of the segmenta-
tion classifier and classifies each segment into Y E
{person, location, organization}. Finally, rela-
tion classification labels each predicted entity pair
with Y E {located in, work for, org based in,
live in, kill} x {left, right} + no relation.
The data used for empirical evaluation was taken
from (Roth and Yih, 2004) and consists of 1436 sen-
tences, which is split into a 1149 (80%) sentence
training set and a 287 (20%) sentence testing set
such that all have at least one active relation. SR-
WLs are provided by (Pantel and Lin, 2002) and
experiments were conducted using a custom graphi-
cal user interface (GUI) designed specifically for the
IFSC protocol. The learning algorithm used for each
stage of the classification task is a regularized vari-
ant of the structured Perceptron (Collins, 2002). Re-
sources used to perform experiments are available at
http://L2R.cs.uiuc.edu/∼cogcomp/.
We extract features in a method similar to (Roth
and Small, 2008), except that we do not include
gazetteer features in b(d) 0as we will include this
type of external information interactively. Secondly,
we use SRWL features as introduced. The segmen-
tation features include the word/SRWL itself along
with the word/SRWL of three words before and two
words after, bigrams of the word/SRWL surround-
ing the word, capitalization of the word, and capi-
talization of its neighbor on each side. Entity clas-
sification uses the segment size, the word/SRWL
members within the segment, and a window of two
word/SRWL elements on each side. Relation clas-
</bodyText>
<subsectionHeader confidence="0.981704">
4.1 Interactive Querying Function
</subsectionHeader>
<bodyText confidence="0.999969916666667">
When using the interactive feature space construc-
tion protocol for this task, we require a querying
function which captures the hypothesis-driven as-
pect of instance selection. We observed that basing
Qmargin on the relation stage performs best, which
is not surprising given that this stage makes the most
mistakes, benefits the most from semantic informa-
tion, and also has many features which are similar to
features from previous stages. Therefore, we adapt
the querying function described by (Roth and Small,
2008) for the relation classification stage and define
our margin for the purposes of instance selection as
</bodyText>
<equation confidence="0.986990333333333">
[fy+(x, i) − f˙y+(x, i)]
ρrelation = min
i=1,...,ny
</equation>
<bodyText confidence="0.955684">
where y˙ = argmaxy,∈Y\y fy,(x), the highest scor-
ing class which is not the true label, and Y+ =
Y\no relation.
</bodyText>
<subsectionHeader confidence="0.981414">
4.2 Interactive Protocol on Entire Data Set
</subsectionHeader>
<bodyText confidence="0.999896086956522">
The first experiments we conduct uses all available
training data (i.e. |S |= 1149) to examine the im-
provement achieved with a fixed number of IFSC
interactions. A single interaction is defined by the
expert selecting a lexical element from a sentence
presented by the querying function and validating
the associated word list. Therefore, it is possible that
a single sentence may result in multiple interactions.
The results for this experimental setup are sum-
marized in Table 1. For each protocol configura-
tion, we report F1 measure for all three stages of
the pipeline. As our simplest baseline, we first train
using the default feature set without any semantic
features (Lexical Features). The second baseline
is to replace all instances of any lexical element
with its SRWL representation as provided by (Pan-
tel and Lin, 2002) (Semantic Features). The next
two baselines attempt to automatically increase pre-
cision by defining each semantic class using only the
top fraction of the elements in each SRWL (Pruned
Semantic (top {1/2,1/4})). This pruning procedure
often results in smaller SRWLs with a more precise
specification of the semantic concept.
</bodyText>
<page confidence="0.995911">
71
</page>
<table confidence="0.999212666666667">
Lexical Semantic Pruned Pruned 50 interactions
Features Features Semantic Semantic Interactive Interactive
(top 1/2) (top 1/4) (select only) (select &amp; validate)
Segmentation 90.23 90.14 90.77 89.71 92.24 93.43
Entity Class. 82.17 83.28 83.93 83.04 85.81 88.76
Relation Class. 54.67 55.20 56.34 56.21 59.14 62.08
</table>
<tableCaption confidence="0.980327333333333">
Table 1: Relative performance of the stated experiments conducted over the entire available dataset. The interactive
feature construction protocol outperforms all non-interactive baselines, particularly for later stages of the pipeline
while requiring only 50 interactions.
</tableCaption>
<bodyText confidence="0.999994833333333">
Finally, we consider the interactive feature space
construction protocol at two different stages. We
first consider the case where 50 interactions are per-
formed such that the algorithm assumes W∗ = W,
that is, the expert selects features for abstraction,
but doesn’t perform validation (Interactive (select
only)). The second experiment performs the entire
protocol, including validation (Interactive (select &amp;
validate)) for 50 interactions. On the relation ex-
traction task, we observe a 13.6% relative improve-
ment over the lexical model and a 10.2% relative im-
provement over the best SRWL baseline F1 score.
</bodyText>
<subsectionHeader confidence="0.998092">
4.3 Examination of the Querying Function
</subsectionHeader>
<bodyText confidence="0.9999915">
As stated in section 3.2, an appropriate querying
function presents sentences which will result in the
expert selecting features from that example and for
which the resulting interactions will result in a large
performance increase. The former is difficult to
model, as it is dependent on properties of the sen-
tence (such as length), will differ from user to user,
and anecdotally is negligibly different for the three
querying functions for earlier interactions. How-
ever, we are able to measure the performance im-
provement of interactions associated with different
querying functions. For our second experiment, we
evaluate the relative performance of the three query-
ing functions defined after every ten interactions in
terms of the F1 measure for relation extraction. The
results of this experiment are shown in figure 4,
where we first see that the Qrandom generally leads
to the least useful interactions. Secondly, while
Qentropy performs well early, Qmargin works bet-
ter as more interactions are performed. Finally, we
also observe that QBorda exceeds the performance
envelope of the two constituent querying functions.
</bodyText>
<figure confidence="0.890362">
0 10 20 30 40 50
interactions
</figure>
<figureCaption confidence="0.7158512">
Figure 4: Relative performance of interactions generated
through the respective querying functions. We see that
Qentropy performs well for a small number of interac-
tions, Qmargin performs well as more interactions are
performed and QBorda outperforms both consistently.
</figureCaption>
<subsectionHeader confidence="0.998758">
4.4 Robustness to Reduced Annotation
</subsectionHeader>
<bodyText confidence="0.999365352941177">
The third set of experiments consider the relative
performance of the configurations from the first set
of experiments as the amount of available training
data is reduced. To study this scenario, we per-
form the same set of experiments with 50 interac-
tions while varying the size of the training set (e.g.
|S |= {250,500,600,675,750, 1000}), summariz-
ing the results in Figure 5. One observation is that
the interactive feature space construction protocol
outperforms all other configurations at all annota-
tion levels. A second important observation is made
when comparing these results to those presented in
(Roth and Small, 2008), where this data is labeled
using active learning. In (Roth and Small, 2008),
once 65% of the labeled data is observed, a perfor-
mance level is achieved comparable to training on
the entire labeled dataset. In this work, an interpo-
</bodyText>
<figure confidence="0.997093066666667">
relation extraction (F1)
0.63
0.62
0.61
0.59
0.58
0.57
0.56
0.55
0.54
0.6
QBorda
Qentropy
Qmargin
Qrandom
</figure>
<page confidence="0.989018">
72
</page>
<bodyText confidence="0.9989788">
lation of the performance at 600 and 675 labeled in-
stances implies that we achieve a performance level
comparable to training on all of the data of the base-
line learner while about 55% of the labeled data is
observed at random. Furthermore, as more labeled
data is introduced, the performance continues to im-
prove with only 50 interactions. This supports the
hypothesis that a good representation is often more
important than additional training data, even when
the data is carefully selected.
</bodyText>
<figure confidence="0.829018">
300 400 500 600 700 800 900 1000
labeled data
</figure>
<figureCaption confidence="0.97285275">
Figure 5: Relative performance of several baseline al-
gorithm configurations and the interactive feature space
construction protocol with variable labeled dataset sizes.
The interactive protocol outperforms other baseline meth-
ods in all cases. Furthermore, the interactive protocol (In-
teractive) outperforms the baseline lexical system (Base-
line) trained on all 1149 sentences even when trained
with a significantly smaller subset of labeled data.
</figureCaption>
<sectionHeader confidence="0.999917" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999989857142857">
There has been significant recent work on designing
learning algorithms which attempt to reduce annota-
tion requirements through a more sophisticated an-
notation method. These methods allow the annota-
tor to directly specify information about the feature
space in addition to providing labels, which is then
incorporated into the learning algorithm (Huang and
Mitchell, 2006; Raghavan and Allan, 2007; Zaidan
et al., 2007; Druck et al., 2008; Zaidan and Eisner,
2008). Additionally, there has been recent work us-
ing explanation-based learning techniques to encode
a more expressive feature space (Lim et al., 2007).
Amongst these works, the only interactive learning
protocol is (Raghavan and Allan, 2007) where in-
stances are presented to an expert and features are
labeled which are then emphasized by the learning
algorithm. Thus, in this case, although additional
information is provided the feature space itself re-
mains static. To the best of our knowledge, this is
the first work that interactively modifies the feature
space by abstracting the FGFs.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999942827586207">
This work introduces the interactive feature space
construction protocol, where the learning algorithm
selects examples for which the feature space is be-
lieved to be deficient and uses existing semantic
resources in coordination with a domain expert to
abstract lexical features with their SRWL names.
While the power of SRWL abstraction in terms of
sample complexity is evident, incorporating this in-
formation is fraught with pitfalls regarding the in-
troduction of additional ambiguity. This interactive
protocol finds examples for which the domain ex-
pert will recognize promising semantic abstractions
and for which those semantic abstraction will signif-
icantly improve the performance of the learner. We
demonstrate the effectiveness of this protocol on a
named entity and relation extraction system.
As a relatively new direction, there are many
possibilities for future work. The most immedi-
ate task is effectively quantifying interaction costs
with a user study, including the impact of includ-
ing users with varying levels of expertise. Recent
work on modeling the costs of the active learn-
ing protocol (Settles et al., 2009; Haertel et al.,
2009) provides some insight on modeling costs as-
sociated with interactive learning protocols. A sec-
ond potentially interesting direction would be to
incorporate other semantic resources such as lexi-
cal patterns (Hearst, 1992) or Wikipedia-generated
gazetteers (Toral and Mu˜noz, 2006).
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999192">
The authors would like to thank Ming-Wei Chang,
Margaret Fleck, Julia Hockenmaier, Alex Klemen-
tiev, Ivan Titov, and the anonymous reviewers for
their valuable suggestions. This work is supported
by DARPA funding under the Bootstrap Learning
Program and by MIAS, a DHS-IDS Center for Mul-
timodal Information Access and Synthesis at UIUC.
</bodyText>
<figure confidence="0.996226833333333">
relation extraction (F1)
0.55
0.45
0.35
0.6
0.5
0.4
Interactive (select &amp; verify)
Pruned Semantic (top 1/2)
Semantic Features
Lexical Features
Baseline (Lexical Features)
</figure>
<page confidence="0.995138">
73
</page>
<sectionHeader confidence="0.995494" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999606">
Avrim Blum. 1992. Learning boolean functions in an
infinite attribute space. Machine Learning, 9(4):373–
386.
Lois Boggess, Rajeev Agarwal, and Ron Davis. 1991.
Disambiguation of prepositional phrases in automat-
ically labelled technical text. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 155–159.
Thorsten Brandts and Alex Franz. 2006. Web 1T 5-gram
Version 1.
David Cohn, Les Atlas, and Richard Ladner. 1994. Im-
proving generalization with active learning. Machine
Learning, 15(2):201–222.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of the Conference
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 1–8.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ization expectation criteria. In Proc. of International
Conference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 595–602.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Robbie Haertel, Kevin D. Seppi, Eric K. Ringger, and
James L. Carroll. 2009. Return on investment for
active learning. In NIPS Workshop on Cost Sensitive
Learning.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. the In-
ternational Conference on Computational Linguistics
(COLING), pages 539–545.
Yifen Huang and Tom M. Mitchell. 2006. Text clustering
with extended user feedback. In Proc. of International
Conference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 413–420.
Xin Li and Dan Roth. 2005. Learning question clas-
sifiers: The role of semantic information. Journal of
Natural Language Engineering, 11(4).
Siau Hong Lim, Li-Lun Wang, and Gerald DeJong. 2007.
Explanation-based feature construction. In Proc. of
the International Joint Conference on Artificial Intelli-
gence (IJCAI), pages 931–936.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proc. of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 337–342.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proc. of the International Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 613–619.
Hema Raghavan and James Allan. 2007. An interactive
algorithm for asking and incorporating feature feed-
back into support vector machines. In Proc. of Inter-
national Conference on Research and Development in
Information Retrieval (SIGIR), pages 79–86.
Dan Roth and Kevin Small. 2008. Active learning for
pipeline models. In Proceedings of the National Con-
ference on Artificial Intelligence (AAAI), pages 683–
688.
Dan Roth and Wen-Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proc. of the Annual Conference on
Computational Natural Language Learning (CoNLL),
pages 1–8.
Burr Settles, Mark Craven, and Lewis Friedland. 2009.
Active learning with real annotation costs. In NIPS
Workshop on Cost Sensitive Learning.
Simon Tong and Daphne Koller. 2001. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Research,
2:45–66.
Antonio Toral and Rafael Mu˜noz. 2006. A proposal
to automatically build and maintain gazetteers using
wikipedia. In Proc. of the Annual Meeting of the
European Association of Computational Linguistics
(EACL), pages 56–61.
H. Peyton Young. 1974. An axiomatization of borda’s
rule. Journal of Economic Theory, 9(1):43–52.
Omar F. Zaidan and Jason Eisner. 2008. Modeling anno-
tators: A generative approach to learning from annota-
tor rationales. In Proc. of the Conference on Empirical
Methods for Natural Language Processing (EMNLP),
pages 31–40.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using “annotator rationales” to improve machine
learning for text categorization. In Proc. of the Annual
Meeting of the North American Association of Compu-
tational Linguistics (NAACL), pages 260–267.
</reference>
<page confidence="0.999114">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874409">
<title confidence="0.999876">Interactive Feature Space Construction using Semantic Information</title>
<author confidence="0.992824">Roth</author>
<affiliation confidence="0.9986205">Department of Computer University of Illinois at</affiliation>
<address confidence="0.921325">Urbana, IL</address>
<abstract confidence="0.997957409090909">Specifying an appropriate feature space is an important aspect of achieving good performance when designing systems based upon learned classifiers. Effectively incorporating information regarding semantically related words into the feature space is known to produce robust, accurate classifiers and is one apparent motivation for efforts to automatically generate such resources. However, naive incorporation of this semantic information may result in poor performance due to increased ambiguity. To overcome this limitation, we the feature space conwhere the learner identifies inadequate regions of the feature space and in coordination with a domain expert adds descriptiveness through existing semantic resources. We demonstrate effectiveness on an entity and relation extraction system including both performance improvements and robustness to reductions in annotated data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
</authors>
<title>Learning boolean functions in an infinite attribute space.</title>
<date>1992</date>
<booktitle>Machine Learning,</booktitle>
<volume>9</volume>
<issue>4</issue>
<pages>386</pages>
<contexts>
<context position="6150" citStr="Blum, 1992" startWordPosition="923" endWordPosition="924">or generating procedure is composed of a vector of feature generation functions (FGFs), Φ(x) = hΦ1(x), Φ2(x), ... , Φn(x)i, where each feature generation function, Φi(x) → {0, 1}, takes the input x and returns the appropriate feature vector value. Consider the text “in west suburban Chicagoland” where we wish to predict the entity classification for Chicagoland. In this case, example active FGFs include Φtext=Chicagoland, ΦisCapitalized, and Φtext(−2)=west while FGFs such as Φtext=and would remain inactive. Since we are constructing sparse feature vectors, we use the infinite attribute model (Blum, 1992). Semantically related word list (SRWL) feature abstraction begins with a set of variable sized word lists {W} such that each member lexical element (i.e. word, phrase) has at least one sense that is semantically related to the concept represented by W (e.g. Wcompass direction = north, east, ... , southwest). For the purpose of feature extraction, whenever the sense of a lexical element associated with a particular W appears in the corpus, it is replaced by the name of the corresponding SRWL. This is equivalent to defining a FGF for the specified W which is a disjunction of the functionally re</context>
</contexts>
<marker>Blum, 1992</marker>
<rawString>Avrim Blum. 1992. Learning boolean functions in an infinite attribute space. Machine Learning, 9(4):373– 386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lois Boggess</author>
<author>Rajeev Agarwal</author>
<author>Ron Davis</author>
</authors>
<title>Disambiguation of prepositional phrases in automatically labelled technical text.</title>
<date>1991</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>155--159</pages>
<contexts>
<context position="3723" citStr="Boggess et al., 1991" startWordPosition="531" endWordPosition="534">her was rushed to Westlake [Health Care Institution], an [Subsidiary] of Resurrection Health Care, [Locative Preposition] [Compass Direction] suburban Chicagoland. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 66–74, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Deriving discriminative features from this representation often results in more informative features and a correspondingly simpler classification task. Although effective approaches along this vein have been shown to induce more accurate classifiers (Boggess et al., 1991; Miller et al., 2004; Li and Roth, 2005), naive approaches may instead result in higher sample complexity due to increased ambiguity introduced through these semantic resources. Features based upon SRWLs must therefore balance the tradeoff between descriptiveness and noise. This paper introduces the interactive feature space construction (IFSC) protocol, which facilitates coordination between a domain expert and learning algorithm to interactively define the feature space during training. This paper describes the particular instance of the IFSC protocol where semantic information is introduce</context>
</contexts>
<marker>Boggess, Agarwal, Davis, 1991</marker>
<rawString>Lois Boggess, Rajeev Agarwal, and Ron Davis. 1991. Disambiguation of prepositional phrases in automatically labelled technical text. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 155–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brandts</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Version 1.</booktitle>
<contexts>
<context position="14320" citStr="Brandts and Franz, 2006" startWordPosition="2233" endWordPosition="2236"> SRWL for feature extraction. Accurate sense disambiguation is helpful for effective SRWL feature abstraction to manage situations where lexical elements belong to multiple lists. In this work, we first disambiguate by predicted part of speech (POS) tags. In cases of multiple SRWL senses for a POS, the given SRWLs (Pantel and Lin, 2002) rank list elements according their semantic representativeness which we use to return the highest ranked sense for a particular lexical element. Also, as SRWL resources emphasize recall over precision, we reduce expert effort by using the Google n-gram counts (Brandts and Franz, 2006) to automatically prune SRWLs. 3.2 Querying Function (Line 6) A primary contribution of this work is designing an appropriate querying function. In doing so, we look to maximize the impact of interactions while minimizing the total number. Therefore, we look to select instances for which (1) the current hypothesis indicates the feature space is insufficient and (2) the resulting SRWL feature abstraction will help improve performance. To account for these two somewhat orthogonal goals, we design two querying functions and aggregate their results. 3.2.1 Hypothesis-Driven Querying To find areas o</context>
<context position="18644" citStr="Brandts and Franz, 2006" startWordPosition="2940" endWordPosition="2943"> are presented in the order of the average SRWL entropy of their words, this coverage rate is further accelerated. Figure 3 helps explain the recall focused aspect of SRWL abstraction while we rely on hypothesis-driven querying to target interactions for the specific task at hand. 0 200 400 600 800 1000 sentences Figure 3: The Impact of SRWL Abstraction and SRWLdriven Querying – The first occurrence of words occur at a much lower rate than the first occurrence of words when abstracted through SRWLs, particularly when sentences are introduced as ranked by average SRWL entropy calculated using (Brandts and Franz, 2006). 3.2.3 Aggregating Querying Functions To combine these two measures, we use the Borda count method of rank aggregation (Young, 1974) to find a consensus between the two querying functions without requiring calibration amongst the actual ranking scores. Defining the rank position of an instance by r(x), the Borda count based querying function is stated by QBorda = argsort [rmargin(xi) + rentropy(xi)] i=1,...,m QBorda selects instances which consider both wide applicability through rentropy and which focus on the specific task through rmargin. 4 Experimental Evaluation To demonstrate the IFSC p</context>
</contexts>
<marker>Brandts, Franz, 2006</marker>
<rawString>Thorsten Brandts and Alex Franz. 2006. Web 1T 5-gram Version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Les Atlas</author>
<author>Richard Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="7118" citStr="Cohn et al., 1994" startWordPosition="1081" endWordPosition="1084">whenever the sense of a lexical element associated with a particular W appears in the corpus, it is replaced by the name of the corresponding SRWL. This is equivalent to defining a FGF for the specified W which is a disjunction of the functionally related FGFs over the member lexical elements (e.g. ΦtextEWcompass direction = Φtext=north ∨ Φtext=east ∨ . . . ∨ Φtext=southwest). 3 Interactive Feature Space Construction The machine learning community has become increasingly interested in protocols which allow interaction with a domain expert during training, such as the active learning protocol (Cohn et al., 1994). In active learning, the learning algorithm reduces the labeling effort by using a querying function to incrementally select unlabeled examples from a data source for annotation during learning. By carefully selecting examples for annotation, active learning maximizes the quality of inductive information while minimizing label acquisition cost. While active learning has been shown to reduce sample complexity, we contend that it significantly underutilizes the domain expert – particularly for complex annotation tasks. More precisely, when a domain expert receives an instance, world knowledge i</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>David Cohn, Les Atlas, and Richard Ladner. 1994. Improving generalization with active learning. Machine Learning, 15(2):201–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="21611" citStr="Collins, 2002" startWordPosition="3436" endWordPosition="3437">org based in, live in, kill} x {left, right} + no relation. The data used for empirical evaluation was taken from (Roth and Yih, 2004) and consists of 1436 sentences, which is split into a 1149 (80%) sentence training set and a 287 (20%) sentence testing set such that all have at least one active relation. SRWLs are provided by (Pantel and Lin, 2002) and experiments were conducted using a custom graphical user interface (GUI) designed specifically for the IFSC protocol. The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron (Collins, 2002). Resources used to perform experiments are available at http://L2R.cs.uiuc.edu/∼cogcomp/. We extract features in a method similar to (Roth and Small, 2008), except that we do not include gazetteer features in b(d) 0as we will include this type of external information interactively. Secondly, we use SRWL features as introduced. The segmentation features include the word/SRWL itself along with the word/SRWL of three words before and two words after, bigrams of the word/SRWL surrounding the word, capitalization of the word, and capitalization of its neighbor on each side. Entity classification u</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Gideon Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning from labeled features using generalization expectation criteria.</title>
<date>2008</date>
<booktitle>In Proc. of International Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>595--602</pages>
<contexts>
<context position="29472" citStr="Druck et al., 2008" startWordPosition="4658" endWordPosition="4661">Interactive) outperforms the baseline lexical system (Baseline) trained on all 1149 sentences even when trained with a significantly smaller subset of labeled data. 5 Related Work There has been significant recent work on designing learning algorithms which attempt to reduce annotation requirements through a more sophisticated annotation method. These methods allow the annotator to directly specify information about the feature space in addition to providing labels, which is then incorporated into the learning algorithm (Huang and Mitchell, 2006; Raghavan and Allan, 2007; Zaidan et al., 2007; Druck et al., 2008; Zaidan and Eisner, 2008). Additionally, there has been recent work using explanation-based learning techniques to encode a more expressive feature space (Lim et al., 2007). Amongst these works, the only interactive learning protocol is (Raghavan and Allan, 2007) where instances are presented to an expert and features are labeled which are then emphasized by the learning algorithm. Thus, in this case, although additional information is provided the feature space itself remains static. To the best of our knowledge, this is the first work that interactively modifies the feature space by abstrac</context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>Gregory Druck, Gideon Mann, and Andrew McCallum. 2008. Learning from labeled features using generalization expectation criteria. In Proc. of International Conference on Research and Development in Information Retrieval (SIGIR), pages 595–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2414" citStr="Fellbaum, 1998" startWordPosition="345" endWordPosition="346">ity to encode arbitrarily complex features, which partially accounts for their popularity. While this flexibility is powerful, it often overwhelms the system designer causing them to resort to simple features. This work presents a method to partially automate feature engineering through an interactive learning protocol. While it is widely accepted that classifier performance is predicated on feature engineering, designing good features requires significant effort. One underutilized resource for descriptive features are existing semantically related word lists (SRWLs), generated both manually (Fellbaum, 1998) and automatically (Pantel and Lin, 2002). Consider the following named entity recognition (NER) example: His father was rushed to [Westlake Hospital]ORG, an arm of [Resurrection Health Care]ORG, in west suburban [Chicagoland]LOC. For such tasks, it is helpful to know that west is a member of the SRWL [Compass Direction] and other such designations. If extracting features using this information, we would require observing only a subset of the SRWL in the data to learn the corresponding parameter. This statement suggests that one method for learning robust classifiers is to incorporate semantic</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbie Haertel</author>
<author>Kevin D Seppi</author>
<author>Eric K Ringger</author>
<author>James L Carroll</author>
</authors>
<title>Return on investment for active learning.</title>
<date>2009</date>
<booktitle>In NIPS Workshop on Cost Sensitive Learning.</booktitle>
<contexts>
<context position="31270" citStr="Haertel et al., 2009" startWordPosition="4936" endWordPosition="4939"> which the domain expert will recognize promising semantic abstractions and for which those semantic abstraction will significantly improve the performance of the learner. We demonstrate the effectiveness of this protocol on a named entity and relation extraction system. As a relatively new direction, there are many possibilities for future work. The most immediate task is effectively quantifying interaction costs with a user study, including the impact of including users with varying levels of expertise. Recent work on modeling the costs of the active learning protocol (Settles et al., 2009; Haertel et al., 2009) provides some insight on modeling costs associated with interactive learning protocols. A second potentially interesting direction would be to incorporate other semantic resources such as lexical patterns (Hearst, 1992) or Wikipedia-generated gazetteers (Toral and Mu˜noz, 2006). Acknowledgments The authors would like to thank Ming-Wei Chang, Margaret Fleck, Julia Hockenmaier, Alex Klementiev, Ivan Titov, and the anonymous reviewers for their valuable suggestions. This work is supported by DARPA funding under the Bootstrap Learning Program and by MIAS, a DHS-IDS Center for Multimodal Informati</context>
</contexts>
<marker>Haertel, Seppi, Ringger, Carroll, 2009</marker>
<rawString>Robbie Haertel, Kevin D. Seppi, Eric K. Ringger, and James L. Carroll. 2009. Return on investment for active learning. In NIPS Workshop on Cost Sensitive Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>539--545</pages>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. the International Conference on Computational Linguistics (COLING), pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifen Huang</author>
<author>Tom M Mitchell</author>
</authors>
<title>Text clustering with extended user feedback.</title>
<date>2006</date>
<booktitle>In Proc. of International Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>413--420</pages>
<contexts>
<context position="29405" citStr="Huang and Mitchell, 2006" startWordPosition="4646" endWordPosition="4649">er baseline methods in all cases. Furthermore, the interactive protocol (Interactive) outperforms the baseline lexical system (Baseline) trained on all 1149 sentences even when trained with a significantly smaller subset of labeled data. 5 Related Work There has been significant recent work on designing learning algorithms which attempt to reduce annotation requirements through a more sophisticated annotation method. These methods allow the annotator to directly specify information about the feature space in addition to providing labels, which is then incorporated into the learning algorithm (Huang and Mitchell, 2006; Raghavan and Allan, 2007; Zaidan et al., 2007; Druck et al., 2008; Zaidan and Eisner, 2008). Additionally, there has been recent work using explanation-based learning techniques to encode a more expressive feature space (Lim et al., 2007). Amongst these works, the only interactive learning protocol is (Raghavan and Allan, 2007) where instances are presented to an expert and features are labeled which are then emphasized by the learning algorithm. Thus, in this case, although additional information is provided the feature space itself remains static. To the best of our knowledge, this is the </context>
</contexts>
<marker>Huang, Mitchell, 2006</marker>
<rawString>Yifen Huang and Tom M. Mitchell. 2006. Text clustering with extended user feedback. In Proc. of International Conference on Research and Development in Information Retrieval (SIGIR), pages 413–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers: The role of semantic information.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>11</volume>
<issue>4</issue>
<contexts>
<context position="3764" citStr="Li and Roth, 2005" startWordPosition="539" endWordPosition="542">titution], an [Subsidiary] of Resurrection Health Care, [Locative Preposition] [Compass Direction] suburban Chicagoland. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 66–74, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Deriving discriminative features from this representation often results in more informative features and a correspondingly simpler classification task. Although effective approaches along this vein have been shown to induce more accurate classifiers (Boggess et al., 1991; Miller et al., 2004; Li and Roth, 2005), naive approaches may instead result in higher sample complexity due to increased ambiguity introduced through these semantic resources. Features based upon SRWLs must therefore balance the tradeoff between descriptiveness and noise. This paper introduces the interactive feature space construction (IFSC) protocol, which facilitates coordination between a domain expert and learning algorithm to interactively define the feature space during training. This paper describes the particular instance of the IFSC protocol where semantic information is introduced through abstraction of lexical terms in</context>
</contexts>
<marker>Li, Roth, 2005</marker>
<rawString>Xin Li and Dan Roth. 2005. Learning question classifiers: The role of semantic information. Journal of Natural Language Engineering, 11(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siau Hong Lim</author>
<author>Li-Lun Wang</author>
<author>Gerald DeJong</author>
</authors>
<title>Explanation-based feature construction.</title>
<date>2007</date>
<booktitle>In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>931--936</pages>
<contexts>
<context position="29645" citStr="Lim et al., 2007" startWordPosition="4684" endWordPosition="4687"> Work There has been significant recent work on designing learning algorithms which attempt to reduce annotation requirements through a more sophisticated annotation method. These methods allow the annotator to directly specify information about the feature space in addition to providing labels, which is then incorporated into the learning algorithm (Huang and Mitchell, 2006; Raghavan and Allan, 2007; Zaidan et al., 2007; Druck et al., 2008; Zaidan and Eisner, 2008). Additionally, there has been recent work using explanation-based learning techniques to encode a more expressive feature space (Lim et al., 2007). Amongst these works, the only interactive learning protocol is (Raghavan and Allan, 2007) where instances are presented to an expert and features are labeled which are then emphasized by the learning algorithm. Thus, in this case, although additional information is provided the feature space itself remains static. To the best of our knowledge, this is the first work that interactively modifies the feature space by abstracting the FGFs. 6 Conclusions and Future Work This work introduces the interactive feature space construction protocol, where the learning algorithm selects examples for whic</context>
</contexts>
<marker>Lim, Wang, DeJong, 2007</marker>
<rawString>Siau Hong Lim, Li-Lun Wang, and Gerald DeJong. 2007. Explanation-based feature construction. In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI), pages 931–936.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL),</booktitle>
<pages>337--342</pages>
<contexts>
<context position="3744" citStr="Miller et al., 2004" startWordPosition="535" endWordPosition="538">lake [Health Care Institution], an [Subsidiary] of Resurrection Health Care, [Locative Preposition] [Compass Direction] suburban Chicagoland. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 66–74, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Deriving discriminative features from this representation often results in more informative features and a correspondingly simpler classification task. Although effective approaches along this vein have been shown to induce more accurate classifiers (Boggess et al., 1991; Miller et al., 2004; Li and Roth, 2005), naive approaches may instead result in higher sample complexity due to increased ambiguity introduced through these semantic resources. Features based upon SRWLs must therefore balance the tradeoff between descriptiveness and noise. This paper introduces the interactive feature space construction (IFSC) protocol, which facilitates coordination between a domain expert and learning algorithm to interactively define the feature space during training. This paper describes the particular instance of the IFSC protocol where semantic information is introduced through abstraction</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL), pages 337–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proc. of the International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>613--619</pages>
<contexts>
<context position="2455" citStr="Pantel and Lin, 2002" startWordPosition="350" endWordPosition="353">eatures, which partially accounts for their popularity. While this flexibility is powerful, it often overwhelms the system designer causing them to resort to simple features. This work presents a method to partially automate feature engineering through an interactive learning protocol. While it is widely accepted that classifier performance is predicated on feature engineering, designing good features requires significant effort. One underutilized resource for descriptive features are existing semantically related word lists (SRWLs), generated both manually (Fellbaum, 1998) and automatically (Pantel and Lin, 2002). Consider the following named entity recognition (NER) example: His father was rushed to [Westlake Hospital]ORG, an arm of [Resurrection Health Care]ORG, in west suburban [Chicagoland]LOC. For such tasks, it is helpful to know that west is a member of the SRWL [Compass Direction] and other such designations. If extracting features using this information, we would require observing only a subset of the SRWL in the data to learn the corresponding parameter. This statement suggests that one method for learning robust classifiers is to incorporate semantic information through features extracted f</context>
<context position="14034" citStr="Pantel and Lin, 2002" startWordPosition="2188" endWordPosition="2191">tside A1558: suburban, nearby, downtown downtown, suburban, nearby, urban, metropolitan, neighboring, near, coastal Figure 2: Word List Validation – Completing two domain expert interactions. Upon selecting either double-boxed element in Figure 1, the expert validates the respective SRWL for feature extraction. Accurate sense disambiguation is helpful for effective SRWL feature abstraction to manage situations where lexical elements belong to multiple lists. In this work, we first disambiguate by predicted part of speech (POS) tags. In cases of multiple SRWL senses for a POS, the given SRWLs (Pantel and Lin, 2002) rank list elements according their semantic representativeness which we use to return the highest ranked sense for a particular lexical element. Also, as SRWL resources emphasize recall over precision, we reduce expert effort by using the Google n-gram counts (Brandts and Franz, 2006) to automatically prune SRWLs. 3.2 Querying Function (Line 6) A primary contribution of this work is designing an appropriate querying function. In doing so, we look to maximize the impact of interactions while minimizing the total number. Therefore, we look to select instances for which (1) the current hypothesi</context>
<context position="17868" citStr="Pantel and Lin, 2002" startWordPosition="2809" endWordPosition="2812">h is built upon SRWLs and lexical elements but is being approximated by Φ(x), which doesn’t use semantic information. In this context, a lexical feature provides one bit of information to the prediction function while a SRWL feature provides information content proportional to its SRWL entropy H(W). To study one aspect of this phenomena empirically, we examine the rate at which words are first encountered in our training corpus from Section 4, as shown by Figure 3. The first observation is the usefulness of SRWL feature abstraction in general as we see that when including an entire SRWL from (Pantel and Lin, 2002) whenever the first element of the list is encountered, we cover the unigram vocabulary much more rapidly. The second observation is that when sentences are presented in the order of the average SRWL entropy of their words, this coverage rate is further accelerated. Figure 3 helps explain the recall focused aspect of SRWL abstraction while we rely on hypothesis-driven querying to target interactions for the specific task at hand. 0 200 400 600 800 1000 sentences Figure 3: The Impact of SRWL Abstraction and SRWLdriven Querying – The first occurrence of words occur at a much lower rate than the </context>
<context position="21349" citStr="Pantel and Lin, 2002" startWordPosition="3394" endWordPosition="3397">follow a begin label. Entity classification begins with the results of the segmentation classifier and classifies each segment into Y E {person, location, organization}. Finally, relation classification labels each predicted entity pair with Y E {located in, work for, org based in, live in, kill} x {left, right} + no relation. The data used for empirical evaluation was taken from (Roth and Yih, 2004) and consists of 1436 sentences, which is split into a 1149 (80%) sentence training set and a 287 (20%) sentence testing set such that all have at least one active relation. SRWLs are provided by (Pantel and Lin, 2002) and experiments were conducted using a custom graphical user interface (GUI) designed specifically for the IFSC protocol. The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron (Collins, 2002). Resources used to perform experiments are available at http://L2R.cs.uiuc.edu/∼cogcomp/. We extract features in a method similar to (Roth and Small, 2008), except that we do not include gazetteer features in b(d) 0as we will include this type of external information interactively. Secondly, we use SRWL features as introduced. The segm</context>
<context position="24033" citStr="Pantel and Lin, 2002" startWordPosition="3822" endWordPosition="3826">lecting a lexical element from a sentence presented by the querying function and validating the associated word list. Therefore, it is possible that a single sentence may result in multiple interactions. The results for this experimental setup are summarized in Table 1. For each protocol configuration, we report F1 measure for all three stages of the pipeline. As our simplest baseline, we first train using the default feature set without any semantic features (Lexical Features). The second baseline is to replace all instances of any lexical element with its SRWL representation as provided by (Pantel and Lin, 2002) (Semantic Features). The next two baselines attempt to automatically increase precision by defining each semantic class using only the top fraction of the elements in each SRWL (Pruned Semantic (top {1/2,1/4})). This pruning procedure often results in smaller SRWLs with a more precise specification of the semantic concept. 71 Lexical Semantic Pruned Pruned 50 interactions Features Features Semantic Semantic Interactive Interactive (top 1/2) (top 1/4) (select only) (select &amp; validate) Segmentation 90.23 90.14 90.77 89.71 92.24 93.43 Entity Class. 82.17 83.28 83.93 83.04 85.81 88.76 Relation Cl</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proc. of the International Conference on Knowledge Discovery and Data Mining (KDD), pages 613–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hema Raghavan</author>
<author>James Allan</author>
</authors>
<title>An interactive algorithm for asking and incorporating feature feedback into support vector machines.</title>
<date>2007</date>
<booktitle>In Proc. of International Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>79--86</pages>
<contexts>
<context position="29431" citStr="Raghavan and Allan, 2007" startWordPosition="4650" endWordPosition="4653"> cases. Furthermore, the interactive protocol (Interactive) outperforms the baseline lexical system (Baseline) trained on all 1149 sentences even when trained with a significantly smaller subset of labeled data. 5 Related Work There has been significant recent work on designing learning algorithms which attempt to reduce annotation requirements through a more sophisticated annotation method. These methods allow the annotator to directly specify information about the feature space in addition to providing labels, which is then incorporated into the learning algorithm (Huang and Mitchell, 2006; Raghavan and Allan, 2007; Zaidan et al., 2007; Druck et al., 2008; Zaidan and Eisner, 2008). Additionally, there has been recent work using explanation-based learning techniques to encode a more expressive feature space (Lim et al., 2007). Amongst these works, the only interactive learning protocol is (Raghavan and Allan, 2007) where instances are presented to an expert and features are labeled which are then emphasized by the learning algorithm. Thus, in this case, although additional information is provided the feature space itself remains static. To the best of our knowledge, this is the first work that interactiv</context>
</contexts>
<marker>Raghavan, Allan, 2007</marker>
<rawString>Hema Raghavan and James Allan. 2007. An interactive algorithm for asking and incorporating feature feedback into support vector machines. In Proc. of International Conference on Research and Development in Information Retrieval (SIGIR), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Kevin Small</author>
</authors>
<title>Active learning for pipeline models.</title>
<date>2008</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>683--688</pages>
<contexts>
<context position="19498" citStr="Roth and Small, 2008" startWordPosition="3066" endWordPosition="3069">tual ranking scores. Defining the rank position of an instance by r(x), the Borda count based querying function is stated by QBorda = argsort [rmargin(xi) + rentropy(xi)] i=1,...,m QBorda selects instances which consider both wide applicability through rentropy and which focus on the specific task through rmargin. 4 Experimental Evaluation To demonstrate the IFSC protocol on a practical application, we examine a three-stage pipeline model for entity and relation extraction, where the task is decomposed into sequential stages of segmentation, entity classification, and relation classification (Roth and Small, 2008). Extending the standard classification task, a pipeline model decomposes the overall classification into a sequence of D stages such that each stage d = 1, ... , D has access to the input instance along with the classifications from all previous stages, ˆy(d). Each stage of the pipeline model uses a feature vector generating procedure 0.8 0.6 0.4 0.2 0 1 SRWL - Entropy SRWL - Sequence Lexical - Sequence unigram type coverage (%) 70 b(d)(x, ˆy(0), ... , ˆy(d−1)) → x(d) to learn a hypothesis h(d). Once each stage of the pipelined classifier is learned, predictions are made sequentially, where s</context>
<context position="21767" citStr="Roth and Small, 2008" startWordPosition="3456" endWordPosition="3459">f 1436 sentences, which is split into a 1149 (80%) sentence training set and a 287 (20%) sentence testing set such that all have at least one active relation. SRWLs are provided by (Pantel and Lin, 2002) and experiments were conducted using a custom graphical user interface (GUI) designed specifically for the IFSC protocol. The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron (Collins, 2002). Resources used to perform experiments are available at http://L2R.cs.uiuc.edu/∼cogcomp/. We extract features in a method similar to (Roth and Small, 2008), except that we do not include gazetteer features in b(d) 0as we will include this type of external information interactively. Secondly, we use SRWL features as introduced. The segmentation features include the word/SRWL itself along with the word/SRWL of three words before and two words after, bigrams of the word/SRWL surrounding the word, capitalization of the word, and capitalization of its neighbor on each side. Entity classification uses the segment size, the word/SRWL members within the segment, and a window of two word/SRWL elements on each side. Relation clas4.1 Interactive Querying F</context>
<context position="27692" citStr="Roth and Small, 2008" startWordPosition="4376" endWordPosition="4379">d set of experiments consider the relative performance of the configurations from the first set of experiments as the amount of available training data is reduced. To study this scenario, we perform the same set of experiments with 50 interactions while varying the size of the training set (e.g. |S |= {250,500,600,675,750, 1000}), summarizing the results in Figure 5. One observation is that the interactive feature space construction protocol outperforms all other configurations at all annotation levels. A second important observation is made when comparing these results to those presented in (Roth and Small, 2008), where this data is labeled using active learning. In (Roth and Small, 2008), once 65% of the labeled data is observed, a performance level is achieved comparable to training on the entire labeled dataset. In this work, an interporelation extraction (F1) 0.63 0.62 0.61 0.59 0.58 0.57 0.56 0.55 0.54 0.6 QBorda Qentropy Qmargin Qrandom 72 lation of the performance at 600 and 675 labeled instances implies that we achieve a performance level comparable to training on all of the data of the baseline learner while about 55% of the labeled data is observed at random. Furthermore, as more labeled dat</context>
</contexts>
<marker>Roth, Small, 2008</marker>
<rawString>Dan Roth and Kevin Small. 2008. Active learning for pipeline models. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 683– 688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-Tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="21131" citStr="Roth and Yih, 2004" startWordPosition="3353" endWordPosition="3356"> · · · x Y(d) ny . More specifically, segmentation makes a prediction for each sentence word over Y E {begin, inside, outside} and constraints are enforced between predictions to ensure that an inside label can only follow a begin label. Entity classification begins with the results of the segmentation classifier and classifies each segment into Y E {person, location, organization}. Finally, relation classification labels each predicted entity pair with Y E {located in, work for, org based in, live in, kill} x {left, right} + no relation. The data used for empirical evaluation was taken from (Roth and Yih, 2004) and consists of 1436 sentences, which is split into a 1149 (80%) sentence training set and a 287 (20%) sentence testing set such that all have at least one active relation. SRWLs are provided by (Pantel and Lin, 2002) and experiments were conducted using a custom graphical user interface (GUI) designed specifically for the IFSC protocol. The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron (Collins, 2002). Resources used to perform experiments are available at http://L2R.cs.uiuc.edu/∼cogcomp/. We extract features in a meth</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-Tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
<author>Lewis Friedland</author>
</authors>
<title>Active learning with real annotation costs.</title>
<date>2009</date>
<booktitle>In NIPS Workshop on Cost Sensitive Learning.</booktitle>
<contexts>
<context position="31247" citStr="Settles et al., 2009" startWordPosition="4932" endWordPosition="4935">col finds examples for which the domain expert will recognize promising semantic abstractions and for which those semantic abstraction will significantly improve the performance of the learner. We demonstrate the effectiveness of this protocol on a named entity and relation extraction system. As a relatively new direction, there are many possibilities for future work. The most immediate task is effectively quantifying interaction costs with a user study, including the impact of including users with varying levels of expertise. Recent work on modeling the costs of the active learning protocol (Settles et al., 2009; Haertel et al., 2009) provides some insight on modeling costs associated with interactive learning protocols. A second potentially interesting direction would be to incorporate other semantic resources such as lexical patterns (Hearst, 1992) or Wikipedia-generated gazetteers (Toral and Mu˜noz, 2006). Acknowledgments The authors would like to thank Ming-Wei Chang, Margaret Fleck, Julia Hockenmaier, Alex Klementiev, Ivan Titov, and the anonymous reviewers for their valuable suggestions. This work is supported by DARPA funding under the Bootstrap Learning Program and by MIAS, a DHS-IDS Center f</context>
</contexts>
<marker>Settles, Craven, Friedland, 2009</marker>
<rawString>Burr Settles, Mark Craven, and Lewis Friedland. 2009. Active learning with real annotation costs. In NIPS Workshop on Cost Sensitive Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--45</pages>
<contexts>
<context position="15288" citStr="Tong and Koller, 2001" startWordPosition="2386" endWordPosition="2389"> insufficient and (2) the resulting SRWL feature abstraction will help improve performance. To account for these two somewhat orthogonal goals, we design two querying functions and aggregate their results. 3.2.1 Hypothesis-Driven Querying To find areas of the feature space which are believed to require more descriptiveness, we look to emphasize those instances which will result in the largest updates to the hypothesis. To accomplish this, we adopt an idea from the active learning community and score instances according to their margin relative to the current learned hypothesis, p(ft, xi, yi) (Tong and Koller, 2001). This results in the hypothesis-driven querying function Qmargin = argsort p(ft, xi, yi) i=1,...,m where the argsort operator is used to sort the input elements in ascending order (for multiple instance selection). Unlike active learning, where selection is from an unlabeled data source, the quantity of labeled data is fixed and labeled data is selected during each round. Therefore, we use the true margin and not the expected margin. This means that we will first select instances which have large mistakes, followed by those instances with small mistakes, and finally instances that make correc</context>
</contexts>
<marker>Tong, Koller, 2001</marker>
<rawString>Simon Tong and Daphne Koller. 2001. Support vector machine active learning with applications to text classification. Journal of Machine Learning Research, 2:45–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Toral</author>
<author>Rafael Mu˜noz</author>
</authors>
<title>A proposal to automatically build and maintain gazetteers using wikipedia.</title>
<date>2006</date>
<booktitle>In Proc. of the Annual Meeting of the European Association of Computational Linguistics (EACL),</booktitle>
<pages>56--61</pages>
<marker>Toral, Mu˜noz, 2006</marker>
<rawString>Antonio Toral and Rafael Mu˜noz. 2006. A proposal to automatically build and maintain gazetteers using wikipedia. In Proc. of the Annual Meeting of the European Association of Computational Linguistics (EACL), pages 56–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Peyton Young</author>
</authors>
<title>An axiomatization of borda’s rule.</title>
<date>1974</date>
<journal>Journal of Economic Theory,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="18777" citStr="Young, 1974" startWordPosition="2962" endWordPosition="2963">all focused aspect of SRWL abstraction while we rely on hypothesis-driven querying to target interactions for the specific task at hand. 0 200 400 600 800 1000 sentences Figure 3: The Impact of SRWL Abstraction and SRWLdriven Querying – The first occurrence of words occur at a much lower rate than the first occurrence of words when abstracted through SRWLs, particularly when sentences are introduced as ranked by average SRWL entropy calculated using (Brandts and Franz, 2006). 3.2.3 Aggregating Querying Functions To combine these two measures, we use the Borda count method of rank aggregation (Young, 1974) to find a consensus between the two querying functions without requiring calibration amongst the actual ranking scores. Defining the rank position of an instance by r(x), the Borda count based querying function is stated by QBorda = argsort [rmargin(xi) + rentropy(xi)] i=1,...,m QBorda selects instances which consider both wide applicability through rentropy and which focus on the specific task through rmargin. 4 Experimental Evaluation To demonstrate the IFSC protocol on a practical application, we examine a three-stage pipeline model for entity and relation extraction, where the task is dec</context>
</contexts>
<marker>Young, 1974</marker>
<rawString>H. Peyton Young. 1974. An axiomatization of borda’s rule. Journal of Economic Theory, 9(1):43–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Jason Eisner</author>
</authors>
<title>Modeling annotators: A generative approach to learning from annotator rationales.</title>
<date>2008</date>
<booktitle>In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>31--40</pages>
<contexts>
<context position="29498" citStr="Zaidan and Eisner, 2008" startWordPosition="4662" endWordPosition="4665">orms the baseline lexical system (Baseline) trained on all 1149 sentences even when trained with a significantly smaller subset of labeled data. 5 Related Work There has been significant recent work on designing learning algorithms which attempt to reduce annotation requirements through a more sophisticated annotation method. These methods allow the annotator to directly specify information about the feature space in addition to providing labels, which is then incorporated into the learning algorithm (Huang and Mitchell, 2006; Raghavan and Allan, 2007; Zaidan et al., 2007; Druck et al., 2008; Zaidan and Eisner, 2008). Additionally, there has been recent work using explanation-based learning techniques to encode a more expressive feature space (Lim et al., 2007). Amongst these works, the only interactive learning protocol is (Raghavan and Allan, 2007) where instances are presented to an expert and features are labeled which are then emphasized by the learning algorithm. Thus, in this case, although additional information is provided the feature space itself remains static. To the best of our knowledge, this is the first work that interactively modifies the feature space by abstracting the FGFs. 6 Conclusio</context>
</contexts>
<marker>Zaidan, Eisner, 2008</marker>
<rawString>Omar F. Zaidan and Jason Eisner. 2008. Modeling annotators: A generative approach to learning from annotator rationales. In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Zaidan</author>
<author>Jason Eisner</author>
<author>Christine Piatko</author>
</authors>
<title>Using “annotator rationales” to improve machine learning for text categorization.</title>
<date>2007</date>
<booktitle>In Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL),</booktitle>
<pages>260--267</pages>
<contexts>
<context position="29452" citStr="Zaidan et al., 2007" startWordPosition="4654" endWordPosition="4657">nteractive protocol (Interactive) outperforms the baseline lexical system (Baseline) trained on all 1149 sentences even when trained with a significantly smaller subset of labeled data. 5 Related Work There has been significant recent work on designing learning algorithms which attempt to reduce annotation requirements through a more sophisticated annotation method. These methods allow the annotator to directly specify information about the feature space in addition to providing labels, which is then incorporated into the learning algorithm (Huang and Mitchell, 2006; Raghavan and Allan, 2007; Zaidan et al., 2007; Druck et al., 2008; Zaidan and Eisner, 2008). Additionally, there has been recent work using explanation-based learning techniques to encode a more expressive feature space (Lim et al., 2007). Amongst these works, the only interactive learning protocol is (Raghavan and Allan, 2007) where instances are presented to an expert and features are labeled which are then emphasized by the learning algorithm. Thus, in this case, although additional information is provided the feature space itself remains static. To the best of our knowledge, this is the first work that interactively modifies the feat</context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2007</marker>
<rawString>Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve machine learning for text categorization. In Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL), pages 260–267.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>