<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001269">
<title confidence="0.996824">
TransDoop: A Map-Reduce based Crowdsourced Translation for
Complex Domains
</title>
<author confidence="0.8074645">
Anoop Kunchukuttan*, Rajen Chatterjee*, Shourya Roy†, Abhijit Mishra*,
Pushpak Bhattacharyya*
</author>
<affiliation confidence="0.951356">
* Department of Computer Science and Engineering, IIT Bombay,
</affiliation>
<email confidence="0.900992">
{anoopk,abhijitmishra,pb}@cse.iitb.ac.in, rajen.k.chatterjee@gmail.com
</email>
<affiliation confidence="0.322498">
† Xerox India Research Centre,
</affiliation>
<email confidence="0.988847">
Shourya.Roy@xerox.com
</email>
<sectionHeader confidence="0.99355" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922269230769">
Large amount of parallel corpora is re-
quired for building Statistical Machine
Translation (SMT) systems. We describe
the TransDoop system for gathering trans-
lations to create parallel corpora from on-
line crowd workforce who have familiar-
ity with multiple languages but are not
expert translators. Our system uses a
Map-Reduce-like approach to translation
crowdsourcing where sentence translation
is decomposed into the following smaller
tasks: (a) translation of constituent phrases
of the sentence; (b) validation of qual-
ity of the phrase translations; and (c)
composition of complete sentence trans-
lations from phrase translations. Trans-
Doop incorporates quality control mech-
anisms and easy-to-use worker user in-
terfaces designed to address issues with
translation crowdsourcing. We have eval-
uated the crowd’s output using the ME-
TEOR metric. For a complex domain like
judicial proceedings, the higher scores ob-
tained by the map-reduce based approach
compared to complete sentence translation
establishes the efficacy of our work.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963652173913">
Crowdsourcing is no longer a new term in the do-
main of Computational Linguistics and Machine
Translation research (Callison-Burch and Dredze,
2010; Snow et al., 2008; Callison-Burch, 2009).
Crowdsourcing - basically where task outsourcing
is delegated to a largely unknown Internet audi-
ence - is emerging as a new paradigm of human
in the loop approaches for developing sophisti-
cated techniques for understanding and generat-
ing natural language content. Amazon Mechanical
Turk(AMT) and CrowdFlower 1 are representative
general purpose crowdsourcing platforms where
as Lingotek and Gengo2 are companies targeted
at localization and translation of content typically
leveraging freelancers.
Our interest is towards developing a crowd-
sourcing based system to enable general, non-
expert crowd-workers generate natural language
content equivalent in quality to that of expert lin-
guists. Realization of the potential of attaining
great scalability and cost-benefit of crowdsourcing
for natural language tasks is limited by the abil-
ity of novice multi-lingual workers generate high
quality translations. We have specific interest in
Indian languages due to the large linguistic diver-
sity as well as the scarcity of linguistic resources in
these languages when compared to European lan-
guages. Crowdsourcing is a promising approach
as many Indian languages are spoken by hundreds
of Millions of people (approximately, Hindi-Urdu
by 500M, Bangla by 200M, Punjabi by over 100M
3) coupled with the fact that representation of In-
dian workers in online crowdsourcing platforms is
very high (close to 40% in Amazon Mechanical
Turk (AMT)).
However, this is a non-trivial task owing to lack
of expertise of novice crowd workers in transla-
tion of content. It is well understood that famil-
iarity with multiple languages might not be good
enough for people to generate high quality transla-
tions. This is compounded by lack of sincerity and
in certain cases, dishonest intention of earning re-
wards disproportionate to the effort and time spent
for online tasks. Common techniques for quality
control like gold data based validation and worker
reputation are not effective for a subjective task
</bodyText>
<footnote confidence="0.993893">
1http://www.mturk.com,http://www.
crowdflower.com
2http://www.lingotek.com,http:///www.
gengo.com
3http://en.wikipedia.org/wiki/List_of_
languages_by_total_number_of_speakers
</footnote>
<page confidence="0.949019">
175
</page>
<bodyText confidence="0.96442575">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 175–180,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
like translation which does not have any task spe-
cific measurements. Having expert linguists man-
ually validate crowd generated content defies the
purpose of deploying crowdsourcing on a large
scale.
In this work, we propose a technique, based
on the Divide-and-Conquer principle. The tech-
nique can be considered similar to a Map-Reduce
task run on crowd processors, where the transla-
tion task is split into simpler tasks distributed to
the crowd (the map stage) and the results are later
combined in a reduce stage to generate complete
translations. The attempt is to make translation
tasks easy and intuitive for novice crowd-workers
by providing translations aids to help them gen-
erate high quality of translations. Our contribu-
tion in this work is a end-to-end, crowdsourcing-
platform-independent, translation crowdsourcing
system that completely automates the translation
crowdsourcing task by (i) managing the transla-
tion pipeline through software components and the
crowd; (ii) performing quality control on work-
ers’ output; and (iii) interfacing with crowdsourc-
ing service providers. The multi-stage, Map-
reduce approach simplifies the translation task for
crowd workers, while novel design of user inter-
face makes the task convenient for the worker and
discourages spamming. The system thus offers the
potential to generate high quality parallel corpora
on a large scale.
We discuss related work in Section 2 and the
multi-staged approach which is central to our sys-
tem in Section 3. Section 4 describes the sys-
tem architecture and workflow, while Section 5
presents important aspects of the user interfaces
in the system. We present our preliminary exper-
iments and observations in Section 6. Section 7
concludes the paper, pointing to future directions.
</bodyText>
<sectionHeader confidence="0.999488" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999986392156863">
Lately, crowdsourcing has been explored as a
source for generating data for NLP tasks (Snow
et al., 2008; Callison-Burch and Dredze, 2010).
Specifically, it has been explored as a channel for
collecting different resources for SMT - evalua-
tions of MT output (Callison-Burch, 2009), word
alignments in parallel sentences (Gao et al., 2010)
and post-edited versions of MT output (Aikawa et
al., 2012). Ambati and Vogel (2010), Kunchukut-
tan et al. (2012) have shown the feasibility of
crowdsourcing for collecting parallel corpora and
pointed out that quality assurance is a major issue
for successful translation crowdsourcing.
The most popular methods for quality control
of crowdsourced tasks are based on sampling and
redundancy. For translation crowdsourcing, Am-
bati et al. (2010) use inter-translator agreement for
selection of a good translation from multiple, re-
dundant worker translations. Zaidan and Callison-
Burch (2011) score translations using a feature
based model comprising sentence level, worker
level and crowd ranking based features. However,
automatic evaluation of translation quality is diffi-
cult, such automatic methods being either inaccu-
rate or expensive. Post et al. (2012) have collected
Indic language corpora data utilizing the crowd for
collecting translations as well as validations. The
quality of the validations is ensured using gold-
standard sentence translations. Our approach to
quality control is similar to Post et al. (2012), but
we work at the level of phrases.
While most crowdsourcing activities for data
gathering has been concerned with collecting sim-
ple annotations like relevance judgments, there has
been work to explore the use of crowdsourcing
for more complex tasks, of which translation is
a good example. Little et al. (2010) propose that
many complex tasks can be modeled either as iter-
ative workflows (where workers iteratively build
on each other’s works) or as parallel workflows
(where workers solve the tasks in parallel, with the
best result voted upon later). Kittur et al. (2011)
suggest a map-and-reduce approach to solve com-
plex problems, where a problem is decomposed
into smaller problems, which are solved in the map
stage and the results are combined in the reduce
stage. Our method can be seen as an instance
of the map-reduce approach applied to translation
crowdsourcing, with two map stages (phrase trans-
lation and translation validation) and one reduce
stage (sentence combination).
</bodyText>
<sectionHeader confidence="0.903289" genericHeader="method">
3 Multi-Stage Crowdsourcing Pipeline
</sectionHeader>
<bodyText confidence="0.999870888888889">
Our system is based on a multi-stage pipeline,
whose central idea is to simplify the translation
task into smaller tasks. The high level block di-
agram of the system is shown in Figure 1. Source
language documents are sentencified using stan-
dard NLP tokenizers and sentence splitters. Ex-
tracted sentences are then split into phrases us-
ing a standard chunker and rule-based merging
of small chunks. This step creates small phrases
</bodyText>
<page confidence="0.997565">
176
</page>
<figureCaption confidence="0.999777">
Figure 1: Multistage crowdsourced translation
</figureCaption>
<bodyText confidence="0.999701526315789">
from complex sentences which can be easily and
independently translated. This leads to a crowd-
sourcing pipeline, with three stages of tasks for the
crowd: Phrase Translation (PT), Phrase Transla-
tion Validation (PV), Sentence Composition (SC).
A group of crowd workers translate source lan-
guage phrases, the translations are validated by a
different group of workers and finally a third group
of workers put the phrase translation together to
create target language sentences. The validation
is done by workers by providing ratings on a k-
point scale. This kind of divide and conquer ap-
proach helps to tackle the complexity of crowd-
sourcing translations since: (1) the tasks are sim-
pler for workers; (2) uniformity of smaller tasks
brings about efficiency as in any industrial assem-
bly line; (3) pricing can be controlled for each
stage depending on the complexity; and (4) quality
control can be performed better for smaller tasks.
</bodyText>
<sectionHeader confidence="0.990955" genericHeader="method">
4 System Architecture
</sectionHeader>
<bodyText confidence="0.999904385964913">
Figure 2 shows the architecture of TransDoop,
which implements the 3-stage pipeline. The major
design considerations were: (i) translation crowd-
sourcing pipeline should be independent of spe-
cific crowdsourcing platforms; (ii) support multi-
ple crowdsourcing platforms; (iii) customize job
parameters like pricing, quality control method
and task design; and (iv) support multiple lan-
guages and domains.
The core component in the system is the
Crowdsourcing Engine. The engine manages the
execution of the crowdsourcing pipeline, lifecycle
of jobs and quality control of submitted tasks. The
Engine exposes its capabilities through the Re-
quester API, which can be used by clients for
setting up, customizing and monitoring transla-
tion crowdsourcing jobs and controlling their exe-
cution. These capabilities are made available to
requesters via the Requester Portal. In order
to make the crowdsourcing engine independent
of any specific crowdsourcing platform, platform
specific Connectors are developed. The Crowd-
sourcing system makes the tasks to be crowd-
sourced available through the Connector API.
The connectors are responsible for polling the en-
gine for tasks to be crowdsourced, pushing the
tasks to crowdsourcing platforms, hosting worker
interfaces for the tasks and pushing the results
back to the engine after they have been completed
by workers on the crowdsourcing platform. Cur-
rently the system supports the AMT crowdsourc-
ing platform.
Figure 3 depicts the lifecycle of a translation
crowdsourcing job. The requester initiates a trans-
lation job for a document (a set of sentences). The
Crowdsourcing Engine schedules the job for exe-
cution. It first splits each sentence into phrases.
For the job, PT tasks are created and made avail-
able through the Connector API. The connector
for the specified platform periodically polls the
Crowdsourcing Engine via the Connector API.
Once the connector has new PT tasks for crowd-
sourcing, it interacts with the crowdsourcing plat-
form to request crowdsourcing services. The con-
nector monitors the progress of the tasks and on
completion provides the results and execution sta-
tus to the Crowdsourcing Engine. Once all the PT
tasks for the job are completed, the crowdsourcing
Engine initiates the PV task to obtain validations
for the translations. The Quality Control system
kicks in when all the PV tasks for the job have
been completed.
The quality control (QC) relies on a combina-
tion of sampling and redundancy. Each PV task
has a few gold-standard phrase translation pairs,
which is used to ensure that the validators are hon-
estly doing their tasks. The judgments from the
</bodyText>
<page confidence="0.996414">
177
</page>
<figureCaption confidence="0.999998">
Figure 2: Architecture of TransDoop
Figure 3: Lifecycle of a Translation Job
</figureCaption>
<bodyText confidence="0.999924125">
good validators are used to determine the quality
of the phrase translation, based on majority voting,
average rating, etc. using multiple judgments col-
lected for each phrase translation. If any phrase
validations or translations are incorrect, then the
corresponding phrases/translations are again sent
to the PT/PV stage as the case may be. This will
continue until all phrase translations in the job are
correctly translated or a pre-configured number of
iterations are done.
Once phrase translations are obtained for all
phrases in a sentence, the Crowdsourcing Engine
creates SC tasks, where the workers are asked
to compose a single correct, coherent translation
from the phrase translation obtained in the previ-
ous stages.
</bodyText>
<sectionHeader confidence="0.999162" genericHeader="method">
5 User Interfaces
</sectionHeader>
<subsectionHeader confidence="0.999648">
5.1 Worker User Interfaces
</subsectionHeader>
<bodyText confidence="0.883516">
This section describes the worker user interfaces
for each stage in the pipeline. These are man-
aged by the Connector and have been designed to
make the task convenient for the worker and pre-
vent spam submissions. In the rest of the section,
we describe the salient features of the PT and SC
UI’s. PV UI is similar to k-scale voting tasks com-
monly found in crowdsourcing platforms.
</bodyText>
<listItem confidence="0.985071866666667">
• Translation UI: Figure 4a shows the trans-
lation UI for the PT stage. The user in-
terface discourages spamming by: (a) dis-
playing source text as images; and (b) alert-
ing workers if they don’t provide a transla-
tion or spend very little time on a task. The
UI also provides transliteration support for
non-Latin scripts (especially helpful for Indic
scripts). A Vocabulary Support, which shows
translation suggestions for word sequences
appearing in the source phrase, is also avail-
able. Suggested translations can be copied to
the input area with ease and speed.
• Sentence Translation Composition UI: The
sentence translation composition UI (shown
</listItem>
<bodyText confidence="0.964943285714286">
in Figure 4b) facilitates composition of sen-
tence translations from phrase translations.
First, the worker can drag and rearrange the
translated phrases into the right order, fol-
lowed by reordering of individual words.
This is important because many Indian lan-
guages have different constituent order ( S-O-
V) with respect to English (S-V-O). Finally,
the synthesized language sentence can be
post-edited to correct spelling, case marking,
inflectional errors, etc. The system also cap-
tures the reordering performed by the worker,
an important byproduct, which can be used
for training reordering models for SMT.
</bodyText>
<subsectionHeader confidence="0.996309">
5.2 Requester UI
</subsectionHeader>
<bodyText confidence="0.9996750625">
The system provides a Requester Portal through
which the requester can create, control and mon-
itor jobs and retrieve results. The portal allows
the requester to customize the job during creation
by configuring various parameters: (a) domain
and language pair (b) entire sentence vs multi-
stage translation (c) price for task at each stage
(d) task design (number of tasks in a task group,
etc.) (e) translation redundancy (f) validation qual-
ity parameters. Translation redundancy refers to
the number of translations requested for a source
phrase. Validation redundancy refers to the num-
ber of validations collected for each phrase trans-
lation pair and the redundancy based acceptance
criteria for phrase translations (majority, consen-
sus, threshold, etc.)
</bodyText>
<page confidence="0.987563">
178
</page>
<figure confidence="0.961121">
(a) Phrase Translation UI (b) Sentence Composition UI
</figure>
<figureCaption confidence="0.999573">
Figure 4: Worker User Interfaces
</figureCaption>
<sectionHeader confidence="0.987124" genericHeader="evaluation">
6 Experiments and Observations
</sectionHeader>
<bodyText confidence="0.991625490909091">
Using TransDoop, we conducted a set of small-
scale, preliminary translation experiments. We ob-
tained translations for English-Hindi and English-
Marathi language pairs for the Judicial and
Tourism domains. For each experiment, 15 sen-
tences were given as input to the pipeline. For
evaluation, we chose METEOR, a well-known
translation evaluation metric (Banerjee and Lavie,
2005). We compared the results obtained from the
crowdsourcing system with a expert human trans-
lation and the output of Google Translate. We also
compared two expert translations using METEOR
to establish a skyline for the translation accuracy.
Table 1 summarizes the results of our experiments.
The translations with Quality Control and mul-
tistage pipeline are better than Google translations
and translations obtained from the crowd without
any quality control, as evaluated by METEOR.
Multi-stage translation yields better than complete
sentence translation. Moreover, the translation
quality is comparable to that of expert human
translation. This behavior is observed across the
two language pairs and domains. This can be seen
in some examples of crowdsourced translations
obtained through the system which are shown in
Table 2.
Incorrect splitting of sentences can cause diffi-
culties in translation for the worker. For instance,
discontinuous phrases will not be available to the
worker as a single translation unit. In the English
interrogative sentence, the noun phrase splits the
verb phrase, therefore the auxiliary and main verb
could be in different translation units. e.g.
Why did you buy the book?
In addition, the phrase structures of the source
and target languages may not map, making trans-
lation difficult. For instance, the vaala modifier in
Hindi translates to a clause in English. It does not
contain any tense information, therefore the tense
of the English clause cannot be determined by the
worker. e.g.
Lucknow vaalaa ladkaa
could translate to any one of:
the boy who lives/lived/is living in Lucknow
We rely on the worker in sentence composition
stage to correct mistakes due to these inadequacies
and compose a good translation. In addition, the
worker in the PT stage could be provided with the
sentence context for translation. However, there
is a tradeoff between the cognitive load of context
processing versus uncertainty in translation. More
elaborately, to what extent can the cognitive load
be reduced before uncertainty of translation sets
in? Similarly, how much of context can be shown
before the cognitive load becomes pressing?
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999935571428572">
In this system demonstration, we present Trans-
Doop as a translation crowdsourcing system which
has the potential to harness the strength of the
crowd to collect high quality human translations
on a large scale. It simplifies the tedious trans-
lation tasks by decomposing them into several
“easy-to-solve” subtasks while ensuring quality.
Our evaluation on small scale data shows that
the multistage approach performs better than com-
plete sentence translation. We would like to exten-
sively use this platform for large scale experiments
on more language pairs and complex domains like
Health, Parliamentary Proceedings, Technical and
Scientific literature etc. to establish the utility of
</bodyText>
<page confidence="0.996517">
179
</page>
<table confidence="0.9996962">
Language Pair Domain Google No QC Translation with QC Reference
Translate single stage multi stage Human
en-mr Tourism 0.227∗ 0.30 0.368 0.372 0.48
en-hi Tourism 0.292 0.363 0.387 0.422 0.51
en-hi Judicial 0.252 0.30 0.388 0.436 0.49
</table>
<tableCaption confidence="0.99995">
Table 1: Experimental Results: Comparison of METEOR scores for different techniques, language pairs and domains
</tableCaption>
<figure confidence="0.9803005">
∗
Translated by an internal Moses-based SMT system
(a) English-Hindi Judicial Translation
(b) English-Hindi Tourism Translation
</figure>
<tableCaption confidence="0.85519625">
Table 2: Examples of translation from Google and three
staged pipeline for source sentence (2nd, 3rd and 1st rows
of each table respectively). Domains and languages are indi-
cated above.
</tableCaption>
<bodyText confidence="0.980338">
the method for collection of parallel corpora on a
large scale.
</bodyText>
<sectionHeader confidence="0.997535" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995331328358209">
Takako Aikawa, Kentaro Yamamoto, and Hitoshi Isa-
hara. 2012. The impact of crowdsourcing post-
editing with the collaborative translation frame-
work. In Advances in Natural Language Processing.
Springer Berlin Heidelberg.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation sys-
tems? In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon’s Mechanical Turk.
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. Language Resources and Evalua-
tion LREC.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with amazon’s me-
chanical turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using amazon’s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing.
Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A
semi-supervised word alignment algorithm with par-
tial manual alignments. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR.
Aniket Kittur, Boris Smus, Susheel Khamkar, and
Robert E Kraut. 2011. Crowdforge: Crowdsourc-
ing complex work. In Proceedings of the 24th an-
nual ACM symposium on User interface software
and technology.
Anoop Kunchukuttan, Shourya Roy, Pratik Patel,
Kushal Ladha, Somya Gupta, Mitesh Khapra, and
Pushpak Bhattacharyya. 2012. Experiences in re-
source generation for machine translation through
crowdsourcing. Language Resources and Evalua-
tion LREC.
Greg Little, Lydia B Chilton, Max Goldman, and
Robert C Miller. 2010. Exploring iterative and par-
allel human computation processes. In Proceedings
of the ACM SIGKDD workshop on human computa-
tion.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Omar Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk.
</reference>
<bodyText confidence="0.5284844">
tdAn�sAr e ao ’ArA lgAyA gyA d\X jAy) nhF\ h{ aOr us� rŒ kr EdyA h{
Accordingly A O by imposed penalty justified not is and that cancel did
Accordingly the penalty imposed by AO is not justified and the same is cancelled.
isk� an� sAr e aO ’ArA lgAy� gy� d\X uEct nhF\ h{ aOr ek hF rŒ kr EdyA h{
Accordingly A O by imposed penalty justified not is and one also cancel did
TMˆAl� ao\kF BFX fAm m�\ d{Enk þAT�nA k� smy hEr’Ar ko apnF cp~V m�\ l�tF h{
devotees of crowd evening in daily prayer of time haridwar its engulf in take
A crowd of devotees engulf Haridwar during the time of daily prayer in the evening
fAm m�\ d{Enk þAT�nA k� smy k� dOrAn BÄo\ ko apnF cp�V m~\ l� hEr’Ar kF BFX
evening in daily prayer of time during devotees its engulf in take Haridwar of crowd
</bodyText>
<page confidence="0.984208">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951836">
<title confidence="0.998971">A Map-Reduce based Crowdsourced Translation Complex Domains</title>
<author confidence="0.996579">Rajen Shourya Abhijit</author>
<affiliation confidence="0.995895">of Computer Science and Engineering, IIT Bombay,</affiliation>
<email confidence="0.999684">rajen.k.chatterjee@gmail.com</email>
<affiliation confidence="0.987591">India Research</affiliation>
<email confidence="0.980043">Shourya.Roy@xerox.com</email>
<abstract confidence="0.999566444444444">Large amount of parallel corpora is required for building Statistical Machine Translation (SMT) systems. We describe for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators. Our system uses a Map-Reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks: (a) translation of constituent phrases of the sentence; (b) validation of quality of the phrase translations; and (c) composition of complete sentence transfrom phrase translations. Transquality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowd’s output using the ME- TEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Takako Aikawa</author>
<author>Kentaro Yamamoto</author>
<author>Hitoshi Isahara</author>
</authors>
<title>The impact of crowdsourcing postediting with the collaborative translation framework.</title>
<date>2012</date>
<booktitle>In Advances in Natural Language Processing.</booktitle>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="6122" citStr="Aikawa et al., 2012" startWordPosition="888" endWordPosition="891">Section 5 presents important aspects of the user interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and pointed out that quality assurance is a major issue for successful translation crowdsourcing. The most popular methods for quality control of crowdsourced tasks are based on sampling and redundancy. For translation crowdsourcing, Ambati et al. (2010) use inter-translator agreement for selection of a good translation from multiple, redundant worker translations. Zaidan and CallisonBurch (2011) score translations using a feature based model comprising sentence lev</context>
</contexts>
<marker>Aikawa, Yamamoto, Isahara, 2012</marker>
<rawString>Takako Aikawa, Kentaro Yamamoto, and Hitoshi Isahara. 2012. The impact of crowdsourcing postediting with the collaborative translation framework. In Advances in Natural Language Processing. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Stephan Vogel</author>
</authors>
<title>Can crowds build parallel corpora for machine translation systems?</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="6147" citStr="Ambati and Vogel (2010)" startWordPosition="892" endWordPosition="895">ortant aspects of the user interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and pointed out that quality assurance is a major issue for successful translation crowdsourcing. The most popular methods for quality control of crowdsourced tasks are based on sampling and redundancy. For translation crowdsourcing, Ambati et al. (2010) use inter-translator agreement for selection of a good translation from multiple, redundant worker translations. Zaidan and CallisonBurch (2011) score translations using a feature based model comprising sentence level, worker level and crow</context>
</contexts>
<marker>Ambati, Vogel, 2010</marker>
<rawString>Vamshi Ambati and Stephan Vogel. 2010. Can crowds build parallel corpora for machine translation systems? In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Stephan Vogel</author>
<author>Jaime Carbonell</author>
</authors>
<title>Active learning and crowd-sourcing for machine translation. Language Resources and Evaluation LREC.</title>
<date>2010</date>
<contexts>
<context position="6506" citStr="Ambati et al. (2010)" startWordPosition="944" endWordPosition="948">een explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and pointed out that quality assurance is a major issue for successful translation crowdsourcing. The most popular methods for quality control of crowdsourced tasks are based on sampling and redundancy. For translation crowdsourcing, Ambati et al. (2010) use inter-translator agreement for selection of a good translation from multiple, redundant worker translations. Zaidan and CallisonBurch (2011) score translations using a feature based model comprising sentence level, worker level and crowd ranking based features. However, automatic evaluation of translation quality is difficult, such automatic methods being either inaccurate or expensive. Post et al. (2012) have collected Indic language corpora data utilizing the crowd for collecting translations as well as validations. The quality of the validations is ensured using goldstandard sentence t</context>
</contexts>
<marker>Ambati, Vogel, Carbonell, 2010</marker>
<rawString>Vamshi Ambati, Stephan Vogel, and Jaime Carbonell. 2010. Active learning and crowd-sourcing for machine translation. Language Resources and Evaluation LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="16036" citStr="Banerjee and Lavie, 2005" startWordPosition="2439" endWordPosition="2442">e translation pair and the redundancy based acceptance criteria for phrase translations (majority, consensus, threshold, etc.) 178 (a) Phrase Translation UI (b) Sentence Composition UI Figure 4: Worker User Interfaces 6 Experiments and Observations Using TransDoop, we conducted a set of smallscale, preliminary translation experiments. We obtained translations for English-Hindi and EnglishMarathi language pairs for the Judicial and Tourism domains. For each experiment, 15 sentences were given as input to the pipeline. For evaluation, we chose METEOR, a well-known translation evaluation metric (Banerjee and Lavie, 2005). We compared the results obtained from the crowdsourcing system with a expert human translation and the output of Google Translate. We also compared two expert translations using METEOR to establish a skyline for the translation accuracy. Table 1 summarizes the results of our experiments. The translations with Quality Control and multistage pipeline are better than Google translations and translations obtained from the crowd without any quality control, as evaluated by METEOR. Multi-stage translation yields better than complete sentence translation. Moreover, the translation quality is compar</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with amazon’s mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="1557" citStr="Callison-Burch and Dredze, 2010" startWordPosition="209" endWordPosition="212">composition of complete sentence translations from phrase translations. TransDoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowd’s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work. 1 Introduction Crowdsourcing is no longer a new term in the domain of Computational Linguistics and Machine Translation research (Callison-Burch and Dredze, 2010; Snow et al., 2008; Callison-Burch, 2009). Crowdsourcing - basically where task outsourcing is delegated to a largely unknown Internet audience - is emerging as a new paradigm of human in the loop approaches for developing sophisticated techniques for understanding and generating natural language content. Amazon Mechanical Turk(AMT) and CrowdFlower 1 are representative general purpose crowdsourcing platforms where as Lingotek and Gengo2 are companies targeted at localization and translation of content typically leveraging freelancers. Our interest is towards developing a crowdsourcing based s</context>
<context position="5862" citStr="Callison-Burch and Dredze, 2010" startWordPosition="847" endWordPosition="850">he system thus offers the potential to generate high quality parallel corpora on a large scale. We discuss related work in Section 2 and the multi-staged approach which is central to our system in Section 3. Section 4 describes the system architecture and workflow, while Section 5 presents important aspects of the user interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and pointed out that quality assurance is a major issue for successful translation crowdsourcing. The most popular methods for quality control of crowdsourced tasks are based on sampling and redundancy. For tra</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with amazon’s mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: evaluating translation quality using amazon’s mechanical turk.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1599" citStr="Callison-Burch, 2009" startWordPosition="217" endWordPosition="218">hrase translations. TransDoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowd’s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work. 1 Introduction Crowdsourcing is no longer a new term in the domain of Computational Linguistics and Machine Translation research (Callison-Burch and Dredze, 2010; Snow et al., 2008; Callison-Burch, 2009). Crowdsourcing - basically where task outsourcing is delegated to a largely unknown Internet audience - is emerging as a new paradigm of human in the loop approaches for developing sophisticated techniques for understanding and generating natural language content. Amazon Mechanical Turk(AMT) and CrowdFlower 1 are representative general purpose crowdsourcing platforms where as Lingotek and Gengo2 are companies targeted at localization and translation of content typically leveraging freelancers. Our interest is towards developing a crowdsourcing based system to enable general, nonexpert crowd-w</context>
<context position="6004" citStr="Callison-Burch, 2009" startWordPosition="871" endWordPosition="872"> approach which is central to our system in Section 3. Section 4 describes the system architecture and workflow, while Section 5 presents important aspects of the user interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and pointed out that quality assurance is a major issue for successful translation crowdsourcing. The most popular methods for quality control of crowdsourced tasks are based on sampling and redundancy. For translation crowdsourcing, Ambati et al. (2010) use inter-translator agreement for selection of a good translation from multiple, redundant worke</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: evaluating translation quality using amazon’s mechanical turk. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Nguyen Bach</author>
<author>Stephan Vogel</author>
</authors>
<title>A semi-supervised word alignment algorithm with partial manual alignments.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR.</booktitle>
<contexts>
<context position="6062" citStr="Gao et al., 2010" startWordPosition="878" endWordPosition="881"> 4 describes the system architecture and workflow, while Section 5 presents important aspects of the user interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and pointed out that quality assurance is a major issue for successful translation crowdsourcing. The most popular methods for quality control of crowdsourced tasks are based on sampling and redundancy. For translation crowdsourcing, Ambati et al. (2010) use inter-translator agreement for selection of a good translation from multiple, redundant worker translations. Zaidan and CallisonBurch (2011) score tran</context>
</contexts>
<marker>Gao, Bach, Vogel, 2010</marker>
<rawString>Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A semi-supervised word alignment algorithm with partial manual alignments. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniket Kittur</author>
<author>Boris Smus</author>
<author>Susheel Khamkar</author>
<author>Robert E Kraut</author>
</authors>
<title>Crowdforge: Crowdsourcing complex work.</title>
<date>2011</date>
<booktitle>In Proceedings of the 24th annual ACM symposium on User interface software and technology.</booktitle>
<contexts>
<context position="7760" citStr="Kittur et al. (2011)" startWordPosition="1138" endWordPosition="1141">ty control is similar to Post et al. (2012), but we work at the level of phrases. While most crowdsourcing activities for data gathering has been concerned with collecting simple annotations like relevance judgments, there has been work to explore the use of crowdsourcing for more complex tasks, of which translation is a good example. Little et al. (2010) propose that many complex tasks can be modeled either as iterative workflows (where workers iteratively build on each other’s works) or as parallel workflows (where workers solve the tasks in parallel, with the best result voted upon later). Kittur et al. (2011) suggest a map-and-reduce approach to solve complex problems, where a problem is decomposed into smaller problems, which are solved in the map stage and the results are combined in the reduce stage. Our method can be seen as an instance of the map-reduce approach applied to translation crowdsourcing, with two map stages (phrase translation and translation validation) and one reduce stage (sentence combination). 3 Multi-Stage Crowdsourcing Pipeline Our system is based on a multi-stage pipeline, whose central idea is to simplify the translation task into smaller tasks. The high level block diagr</context>
</contexts>
<marker>Kittur, Smus, Khamkar, Kraut, 2011</marker>
<rawString>Aniket Kittur, Boris Smus, Susheel Khamkar, and Robert E Kraut. 2011. Crowdforge: Crowdsourcing complex work. In Proceedings of the 24th annual ACM symposium on User interface software and technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Kunchukuttan</author>
<author>Shourya Roy</author>
<author>Pratik Patel</author>
<author>Kushal Ladha</author>
<author>Somya Gupta</author>
<author>Mitesh Khapra</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Experiences in resource generation for machine translation through crowdsourcing. Language Resources and Evaluation LREC.</title>
<date>2012</date>
<contexts>
<context position="6175" citStr="Kunchukuttan et al. (2012)" startWordPosition="896" endWordPosition="900">r interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and pointed out that quality assurance is a major issue for successful translation crowdsourcing. The most popular methods for quality control of crowdsourced tasks are based on sampling and redundancy. For translation crowdsourcing, Ambati et al. (2010) use inter-translator agreement for selection of a good translation from multiple, redundant worker translations. Zaidan and CallisonBurch (2011) score translations using a feature based model comprising sentence level, worker level and crowd ranking based features. Ho</context>
</contexts>
<marker>Kunchukuttan, Roy, Patel, Ladha, Gupta, Khapra, Bhattacharyya, 2012</marker>
<rawString>Anoop Kunchukuttan, Shourya Roy, Pratik Patel, Kushal Ladha, Somya Gupta, Mitesh Khapra, and Pushpak Bhattacharyya. 2012. Experiences in resource generation for machine translation through crowdsourcing. Language Resources and Evaluation LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Little</author>
<author>Lydia B Chilton</author>
<author>Max Goldman</author>
<author>Robert C Miller</author>
</authors>
<title>Exploring iterative and parallel human computation processes.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM SIGKDD workshop on human computation.</booktitle>
<contexts>
<context position="7497" citStr="Little et al. (2010)" startWordPosition="1095" endWordPosition="1098">naccurate or expensive. Post et al. (2012) have collected Indic language corpora data utilizing the crowd for collecting translations as well as validations. The quality of the validations is ensured using goldstandard sentence translations. Our approach to quality control is similar to Post et al. (2012), but we work at the level of phrases. While most crowdsourcing activities for data gathering has been concerned with collecting simple annotations like relevance judgments, there has been work to explore the use of crowdsourcing for more complex tasks, of which translation is a good example. Little et al. (2010) propose that many complex tasks can be modeled either as iterative workflows (where workers iteratively build on each other’s works) or as parallel workflows (where workers solve the tasks in parallel, with the best result voted upon later). Kittur et al. (2011) suggest a map-and-reduce approach to solve complex problems, where a problem is decomposed into smaller problems, which are solved in the map stage and the results are combined in the reduce stage. Our method can be seen as an instance of the map-reduce approach applied to translation crowdsourcing, with two map stages (phrase transla</context>
</contexts>
<marker>Little, Chilton, Goldman, Miller, 2010</marker>
<rawString>Greg Little, Lydia B Chilton, Max Goldman, and Robert C Miller. 2010. Exploring iterative and parallel human computation processes. In Proceedings of the ACM SIGKDD workshop on human computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Constructing parallel corpora for six indian languages via crowdsourcing.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="6919" citStr="Post et al. (2012)" startWordPosition="1004" endWordPosition="1007"> issue for successful translation crowdsourcing. The most popular methods for quality control of crowdsourced tasks are based on sampling and redundancy. For translation crowdsourcing, Ambati et al. (2010) use inter-translator agreement for selection of a good translation from multiple, redundant worker translations. Zaidan and CallisonBurch (2011) score translations using a feature based model comprising sentence level, worker level and crowd ranking based features. However, automatic evaluation of translation quality is difficult, such automatic methods being either inaccurate or expensive. Post et al. (2012) have collected Indic language corpora data utilizing the crowd for collecting translations as well as validations. The quality of the validations is ensured using goldstandard sentence translations. Our approach to quality control is similar to Post et al. (2012), but we work at the level of phrases. While most crowdsourcing activities for data gathering has been concerned with collecting simple annotations like relevance judgments, there has been work to explore the use of crowdsourcing for more complex tasks, of which translation is a good example. Little et al. (2010) propose that many com</context>
</contexts>
<marker>Post, Callison-Burch, Osborne, 2012</marker>
<rawString>Matt Post, Chris Callison-Burch, and Miles Osborne. 2012. Constructing parallel corpora for six indian languages via crowdsourcing. In Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Crowdsourcing translation: Professional quality from nonprofessionals.</title>
<date>2011</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar Zaidan and Chris Callison-Burch. 2011. Crowdsourcing translation: Professional quality from nonprofessionals. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>