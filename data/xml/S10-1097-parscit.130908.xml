<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004954">
<title confidence="0.99823">
Twitter Based System: Using Twitter for Disambiguating
Sentiment Ambiguous Adjectives
</title>
<author confidence="0.99414">
Alexander Pak, Patrick Paroubek
</author>
<affiliation confidence="0.941108">
Universit´e de Paris-Sud,
</affiliation>
<address confidence="0.7693415">
Laboratoire LIMSI-CNRS, Bˆatiment 508,
F-91405 Orsay Cedex, France
</address>
<email confidence="0.99783">
alexpak@limsi.fr, pap@limsi.fr
</email>
<sectionHeader confidence="0.993846" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995865">
In this paper, we describe our system
which participated in the SemEval 2010
task of disambiguating sentiment ambigu-
ous adjectives for Chinese. Our system
uses text messages from Twitter, a popu-
lar microblogging platform, for building a
dataset of emotional texts. Using the built
dataset, the system classifies the meaning
of adjectives into positive or negative sen-
timent polarity according to the given con-
text. Our approach is fully automatic. It
does not require any additional hand-built
language resources and it is language in-
dependent.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999923136363636">
The dataset of the SemEval task (Wu and Jin,
2010) consists of short texts in Chinese contain-
ing target adjectives whose sentiments need to be
disambiguated in the given contexts. Those adjec-
tives are: Q big, 小 small, 多 many, 少 few, 高
high, 低 low, 厚 thick, 薄 thin, 深 deep, shallow,
重 heavy, light, 巨Q huge, 重Q grave.
Disambiguating sentiment ambiguous adjec-
tives is a challenging task for NLP. Previous stud-
ies were mostly focused on word sense disam-
biguation rather than sentiment disambiguation.
Although both problems look similar, the latter is
more challenging in our opinion because impreg-
nated with more subjectivity. In order to solve the
task, one has to deal not only with the semantics
of the context, but also with the psychological as-
pects of human perception of emotions from the
written text.
In our approach, we use Twitter1 microblogging
platform to retrieve emotional messages and form
two sets of texts: messages with positive emotions
and those with negative ones (Pak and Paroubek,
</bodyText>
<footnote confidence="0.975556">
1http://twitter.com
</footnote>
<bodyText confidence="0.977406833333333">
2010). We use emoticons2 as indicators of an emo-
tion (Read, 2005) to automatically classify texts
into positive or negative sets. The reason we use
Twitter is because it allows us to collect the data
with minimal supervision efforts. It provides an
API3 which makes the data retrieval process much
more easier then Web based search or other re-
sources.
After the dataset of emotional texts has been
obtained, we build a classifier based on n-grams
Naive Bayes approach. We tested two approaches
to build a sentiment classifier:
</bodyText>
<listItem confidence="0.9273">
1. In the first one, we collected Chinese texts
from Twitter and used them to train a classi-
fier to annotate the test dataset.
2. In the second one, we used machine trans-
lator to translate the dataset from Chinese to
English and annotated it using collected En-
glish texts from Twitter as the training data.
</listItem>
<bodyText confidence="0.999741714285714">
We have made the second approach because we
were able to collect much more of English texts
from Twitter than Chinese ones and we wanted
to test the impact of machine translation on the
performance of our classifier. We have exper-
imented with Google Translate and Yahoo Ba-
belfish4. Google Translate yielded better results.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999826142857143">
In (Yang et al., 2007), the authors use web-blogs
to construct a corpora for sentiment analysis and
use emotion icons assigned to blog posts as indica-
tors of users’ mood. The authors applied SVM and
CRF learners to classify sentiments at the sentence
level and then investigated several strategies to de-
termine the overall sentiment of the document. As
</bodyText>
<footnote confidence="0.999921">
2An emoticon is a textual representation of an author’s
emotion often used in Internet blogs and textual chats
3http://dev.twitter.com/doc/get/search
4http://babelfish.yahoo.com/
</footnote>
<page confidence="0.977114">
436
</page>
<bodyText confidence="0.974226333333333">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 436–439,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
the result, the winning strategy is defined by con-
sidering the sentiment of the last sentence of the
document as the sentiment at the document level.
J. Read in (Read, 2005) used emoticons such as
“:-)” and “:-(” to form a training set for the sen-
timent classification. For this purpose, the author
collected texts containing emoticons from Usenet
newsgroups. The dataset was divided into “pos-
itive” (texts with happy emoticons) and “nega-
tive” (texts with sad or angry emoticons) samples.
Emoticons-trained classifiers: SVM and Naive
Bayes, were able to obtain up to 70% accuracy on
the test set.
In (Go et al., 2009), authors used Twitter to
collect training data and then to perform a senti-
ment search. The approach is similar to the one
in (Read, 2005). The authors construct corpora
by using emoticons to obtain “positive” and “neg-
ative” samples, and then use various classifiers.
The best result was obtained by the Naive Bayes
classifier with a mutual information measure for
feature selection. The authors were able to obtain
up to 84% of accuracy on their test set. However,
the method showed a bad performance with three
classes (“negative”, “positive” and “neutral”).
In our system, we use a similar idea as in (Go
et al., 2009), however, we improve it by using a
combination of unigrams, bigrams and trigrams (
(Go et al., 2009) used only unigrams). We also
handle negations by attaching a negation particle
to adjacent words when forming ngrams.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="method">
3 Our method
</sectionHeader>
<subsectionHeader confidence="0.999876">
3.1 Corpus collection
</subsectionHeader>
<bodyText confidence="0.9998424">
Using Twitter API we collected a corpus of text
posts and formed a dataset of two classes: positive
sentiments and negative sentiments. We queried
Twitter for two types of emoticons considering
eastern and western types of emoticons5:
</bodyText>
<listItem confidence="0.999928">
• Happy emoticons: :-), :), ˆ ˆ, ˆoˆ, etc.
• Sad emoticons::-(, :(, T T, ; ;, etc.
</listItem>
<bodyText confidence="0.9998316">
We were able to obtain 10,000 Twitter posts in
Chinese, and 300,000 posts in English evenly split
between negative and positive classes.
The collected texts were processed as follows to
obtain a set of n-grams:
</bodyText>
<listItem confidence="0.8397720625">
1. Filtering – we remove URL links (e.g.
http://example.com), Twitter user names (e.g.
5http://en.wikipedia.org/wiki/Emoticon#Asian style
@alex – with symbol @ indicating a
user name), Twitter special words (such as
“RT”6), and emoticons.
2. Tokenization – we segment text by split-
ting it by spaces and punctuation marks, and
form a bag of words. For English, we kept
short forms as a single word: “don’t”, “I’ll”,
“she’d”.
3. Stopwords removal – in English, texts we re-
moved articles (“a”, “an”, “the”) from the bag
of words.
4. N-grams construction – we make a set of n-
grams out of consecutive words.
</listItem>
<bodyText confidence="0.9947858">
A negation particle is attached to a word which
precedes it and follows it. For example, a sen-
tence “I do not like fish” will form three bigrams:
“I do+not”, “do+not like”, “not+like fish”. Such
a procedure improves the accuracy of the classi-
fication since the negation plays a special role in
opinion and sentiment expression (Wilson et al.,
2005). In English, we used negative particles ’no’
and ’not’. In Chinese, we used the following par-
ticles:
</bodyText>
<listItem confidence="0.99935675">
1. 不 – is not + noun
2. 未 – does not + verb, will not + verb
3. 莫 (別) – do not (imperative)
4. 無 (沒有) – does not have
</listItem>
<subsectionHeader confidence="0.703204">
3.2 Classifier
</subsectionHeader>
<bodyText confidence="0.988688333333333">
We build a sentiment classifier using the multi-
nomial Naive Bayes classifier which is based on
Bayes’ theorem.
</bodyText>
<equation confidence="0.991539666666667">
P(s) P(M|s)
P(s|M) = (1)
P(M)
</equation>
<bodyText confidence="0.999635666666667">
where s is a sentiment, M is a text. We assume
that a target adjective has the same sentiment po-
larity as the whole text, because in general the
lengths of the given texts are small.
Since we have sets of equal number of positive
and negative messages, we simplify the equation:
</bodyText>
<equation confidence="0.99431">
P (M|s)
P(s|M) = (2)
P (M)
</equation>
<footnote confidence="0.924413">
6An abbreviation for retweet, which means citation or re-
posting of a message
</footnote>
<page confidence="0.959616">
437
</page>
<equation confidence="0.996761">
P(s|M) ∼ P(M|s) (3)
</equation>
<bodyText confidence="0.999673368421053">
We train Bayes classifiers which use a presence
of an n-grams as a binary feature. We have ex-
perimented with unigrams, bigrams, and trigrams.
Pang et al. (Pang et al., 2002) reported that uni-
grams outperform bigrams when doing sentiment
classification of movie reviews, but Dave et al.
(Dave et al., 2003) have obtained contrary re-
sults: bigrams and trigrams worked better for the
product-review polarity classification. We tried to
determine the best settings for our microblogging
data. On the one hand high-order n-grams, such
as trigrams, should capture patterns of sentiments
expressions better. On the other hand, unigrams
should provide a good coverage of the data. There-
fore we combine three classifiers that are based
on different n-gram orders (unigrams, bigrams and
trigrams). We make an assumption of conditional
independence of n-gram for the calculation sim-
plicity:
</bodyText>
<equation confidence="0.712336">
P(s|M) ∼ P(G1|s) · P(G2|s) · P(G3|s) (4)
where G1 is a set of unigrams representing the
message, G2 is a set of bigrams, and G3 is a set of
</equation>
<figureCaption confidence="0.746197">
trigrams. We assume that n-grams are condition-
ally independent:
Figure 1: Micro accuracy when using Google
Translate and Yahoo Babelfish
</figureCaption>
<figure confidence="0.993322464285714">
micro accuracy
0.65
0.63
0.61
0.59
0.57
0.55
0.53
0.51
0.49
0.47
0.45
0 5 10 15 20 25 30 35 40 45
google yahoo
window size
macro accuracy
0.65
0.55
0.45
0.6
0.5
0.4
0 5 10 15 20 25 30 35 40 45
google yahoo
window size
Y P(g|s) (5) Figure 2: Macro accuracy when using Google
P(Gn|s) _ Translate and Yahoo Babelfish
gEGn
</figure>
<equation confidence="0.826651">
Where Gn is a set of n-grams of order n.
P(s|M) ∼ Y P(g|s)· Y P(g|s)· Y P(g|s)
gEG1 gEG2 gEG3
</equation>
<bodyText confidence="0.8161025">
Finally, we calculate a log-likelihood of each sen-
timent:
</bodyText>
<equation confidence="0.996376">
L(s|M) _ X log(P(g|s)) + X log(P(g|s))
gEG1 gEG2
+ X log(P(g|s))
gEG3
</equation>
<bodyText confidence="0.99987675">
In order to improve the accuracy, we changed
the size of the context window, i.e. the number of
words before and after the target adjective used for
classification.
</bodyText>
<sectionHeader confidence="0.991319" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999732260869565">
In our experiments, we used two datasets: a trial
dataset containing 100 sentences in Chinese and
a test dataset with 2917 sentences. Both datasets
were provided by the task organizers. Micro and
macro accuracy were chosen as the evaluation
metrics.
First, we compared the performance of our
method when using Google Translate and Yahoo
Babelfish for translating the trial dataset. The re-
sults for micro and macro accuracy are shown in
Graphs 1 and 2 respectively. The x-axis repre-
sents a context window-size, equal to a number of
words on both sides of the target adjective. The y-
axis shows accuracy values. From the graphs we
see that Google Translate provides better results,
therefore it was chosen when annotating the test
dataset.
Next, we studied the impact of the context win-
dow size on micro and macro accuracy. The
impact of the size of the context window on
the accuracy of the classifier trained on Chinese
texts is depicted in Graph 3 and for the classifier
trained on English texts with translated test dataset
</bodyText>
<page confidence="0.998426">
438
</page>
<figureCaption confidence="0.969278">
Figure 3: Micro and macro accuracy for the first
approach (training on Chinese texts)
Figure 4: Micro and macro accuracy for the sec-
ond approach (training on English texts which
have been machine translated)
</figureCaption>
<bodyText confidence="0.99454">
in Graph 4.
The second approach achieves better results.
We were able to obtain 64% of macro and 61% of
micro accuracy when using the second approach
but only 63% of macro and 61% of micro accu-
racy when using the first approach.
Another observation from the graphs is that
Chinese requires a smaller size of a context win-
dow to obtain the best performance. For the first
approach, a window size of 8 words gave the best
macro accuracy. For the second approach, we ob-
tained the highest accuracy with a window size of
22 words.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999955733333333">
In this paper, we have described our system for
disambiguating sentiments of adjectives in Chi-
nese texts. Our Naive Bayes approach uses infor-
mation automatically extracted from Twitter mi-
croblogs using emoticons. The techniques used in
our approach can be applied to any other language.
Our system is fully automate and does not utilize
any hand-built lexicon. We were able to achieve
up to 64% of macro and 61% of micro accuracy at
the SemEval 2010 task
For the future work, we would like to collect
more Chinese texts from Twitter or similar mi-
croblogging platforms. We think that increasing
the training dataset will improve much the accu-
racy of the sentiment disambiguation.
</bodyText>
<sectionHeader confidence="0.998531" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999527641025641">
Kushal Dave, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: opinion
extraction and semantic classification of product re-
views. In WWW ’03: Proceedings of the 12th in-
ternational conference on World Wide Web, pages
519–528, New York, NY, USA. ACM.
Alec Go, Lei Huang, and Richa Bhayani. 2009. Twit-
ter sentiment analysis. Final Projects from CS224N
for Spring 2008/2009 at The Stanford Natural Lan-
guage Processing Group.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings ofLREC 2010.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 79–86.
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL Stu-
dent Research Workshop, pages 43–48.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ’05: Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 347–354, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Yunfang Wu and Peng Jin. 2010. Semeval-2010
task 18: Disambiguating sentiment ambiguous ad-
jectives. In SemEval2010: Proceedings ofInterna-
tional Workshop of Semantic Evaluations.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-
Hsi Chen. 2007. Emotion classification using
web blog corpora. In WI ’07: Proceedings of
the IEEE/WIC/ACM International Conference on
Web Intelligence, pages 275–278, Washington, DC,
USA. IEEE Computer Society.
</reference>
<figure confidence="0.9993866">
accuracy
0.65
0.63
0.61
0.59
0.57
0.55
0.53
0.51
0.49
0.47
0.45
0 5 10 15 20 25 30 35 40 45
Micro Macro
window size
accuracy
0.65
0.63
0.61
0.59
0.57
0.55
0.53
0.51
0.49
0.47
0.45
0 5 10 15 20 25 30 35 40 45
Micro Macro
window size
</figure>
<page confidence="0.991213">
439
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448410">
<title confidence="0.9989775">Twitter Based System: Using Twitter for Disambiguating Sentiment Ambiguous Adjectives</title>
<author confidence="0.999803">Alexander Pak</author>
<author confidence="0.999803">Patrick Paroubek</author>
<affiliation confidence="0.998962">Universit´e de Paris-Sud,</affiliation>
<address confidence="0.8378545">Laboratoire LIMSI-CNRS, Bˆatiment 508, F-91405 Orsay Cedex, France</address>
<email confidence="0.997983">alexpak@limsi.fr,pap@limsi.fr</email>
<abstract confidence="0.977691266666667">In this paper, we describe our system which participated in the SemEval 2010 task of disambiguating sentiment ambiguous adjectives for Chinese. Our system uses text messages from Twitter, a popular microblogging platform, for building a dataset of emotional texts. Using the built dataset, the system classifies the meaning of adjectives into positive or negative sentiment polarity according to the given context. Our approach is fully automatic. It does not require any additional hand-built language resources and it is language independent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In WWW ’03: Proceedings of the 12th international conference on World Wide Web,</booktitle>
<pages>519--528</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7781" citStr="Dave et al., 2003" startWordPosition="1290" endWordPosition="1293">polarity as the whole text, because in general the lengths of the given texts are small. Since we have sets of equal number of positive and negative messages, we simplify the equation: P (M|s) P(s|M) = (2) P (M) 6An abbreviation for retweet, which means citation or reposting of a message 437 P(s|M) ∼ P(M|s) (3) We train Bayes classifiers which use a presence of an n-grams as a binary feature. We have experimented with unigrams, bigrams, and trigrams. Pang et al. (Pang et al., 2002) reported that unigrams outperform bigrams when doing sentiment classification of movie reviews, but Dave et al. (Dave et al., 2003) have obtained contrary results: bigrams and trigrams worked better for the product-review polarity classification. We tried to determine the best settings for our microblogging data. On the one hand high-order n-grams, such as trigrams, should capture patterns of sentiments expressions better. On the other hand, unigrams should provide a good coverage of the data. Therefore we combine three classifiers that are based on different n-gram orders (unigrams, bigrams and trigrams). We make an assumption of conditional independence of n-gram for the calculation simplicity: P(s|M) ∼ P(G1|s) · P(G2|s</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In WWW ’03: Proceedings of the 12th international conference on World Wide Web, pages 519–528, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Lei Huang</author>
<author>Richa Bhayani</author>
</authors>
<title>Twitter sentiment analysis. Final Projects from CS224N for Spring 2008/2009 at The Stanford Natural Language Processing Group.</title>
<date>2009</date>
<contexts>
<context position="4337" citStr="Go et al., 2009" startWordPosition="689" endWordPosition="692">he result, the winning strategy is defined by considering the sentiment of the last sentence of the document as the sentiment at the document level. J. Read in (Read, 2005) used emoticons such as “:-)” and “:-(” to form a training set for the sentiment classification. For this purpose, the author collected texts containing emoticons from Usenet newsgroups. The dataset was divided into “positive” (texts with happy emoticons) and “negative” (texts with sad or angry emoticons) samples. Emoticons-trained classifiers: SVM and Naive Bayes, were able to obtain up to 70% accuracy on the test set. In (Go et al., 2009), authors used Twitter to collect training data and then to perform a sentiment search. The approach is similar to the one in (Read, 2005). The authors construct corpora by using emoticons to obtain “positive” and “negative” samples, and then use various classifiers. The best result was obtained by the Naive Bayes classifier with a mutual information measure for feature selection. The authors were able to obtain up to 84% of accuracy on their test set. However, the method showed a bad performance with three classes (“negative”, “positive” and “neutral”). In our system, we use a similar idea as</context>
</contexts>
<marker>Go, Huang, Bhayani, 2009</marker>
<rawString>Alec Go, Lei Huang, and Richa Bhayani. 2009. Twitter sentiment analysis. Final Projects from CS224N for Spring 2008/2009 at The Stanford Natural Language Processing Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings ofLREC</booktitle>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings ofLREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>79--86</pages>
<contexts>
<context position="7649" citStr="Pang et al., 2002" startWordPosition="1269" endWordPosition="1272">heorem. P(s) P(M|s) P(s|M) = (1) P(M) where s is a sentiment, M is a text. We assume that a target adjective has the same sentiment polarity as the whole text, because in general the lengths of the given texts are small. Since we have sets of equal number of positive and negative messages, we simplify the equation: P (M|s) P(s|M) = (2) P (M) 6An abbreviation for retweet, which means citation or reposting of a message 437 P(s|M) ∼ P(M|s) (3) We train Bayes classifiers which use a presence of an n-grams as a binary feature. We have experimented with unigrams, bigrams, and trigrams. Pang et al. (Pang et al., 2002) reported that unigrams outperform bigrams when doing sentiment classification of movie reviews, but Dave et al. (Dave et al., 2003) have obtained contrary results: bigrams and trigrams worked better for the product-review polarity classification. We tried to determine the best settings for our microblogging data. On the one hand high-order n-grams, such as trigrams, should capture patterns of sentiments expressions better. On the other hand, unigrams should provide a good coverage of the data. Therefore we combine three classifiers that are based on different n-gram orders (unigrams, bigrams </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon Read</author>
</authors>
<title>Using emoticons to reduce dependency in machine learning techniques for sentiment classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="1899" citStr="Read, 2005" startWordPosition="295" endWordPosition="296">ment disambiguation. Although both problems look similar, the latter is more challenging in our opinion because impregnated with more subjectivity. In order to solve the task, one has to deal not only with the semantics of the context, but also with the psychological aspects of human perception of emotions from the written text. In our approach, we use Twitter1 microblogging platform to retrieve emotional messages and form two sets of texts: messages with positive emotions and those with negative ones (Pak and Paroubek, 1http://twitter.com 2010). We use emoticons2 as indicators of an emotion (Read, 2005) to automatically classify texts into positive or negative sets. The reason we use Twitter is because it allows us to collect the data with minimal supervision efforts. It provides an API3 which makes the data retrieval process much more easier then Web based search or other resources. After the dataset of emotional texts has been obtained, we build a classifier based on n-grams Naive Bayes approach. We tested two approaches to build a sentiment classifier: 1. In the first one, we collected Chinese texts from Twitter and used them to train a classifier to annotate the test dataset. 2. In the s</context>
<context position="3893" citStr="Read, 2005" startWordPosition="618" endWordPosition="619">ed several strategies to determine the overall sentiment of the document. As 2An emoticon is a textual representation of an author’s emotion often used in Internet blogs and textual chats 3http://dev.twitter.com/doc/get/search 4http://babelfish.yahoo.com/ 436 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 436–439, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics the result, the winning strategy is defined by considering the sentiment of the last sentence of the document as the sentiment at the document level. J. Read in (Read, 2005) used emoticons such as “:-)” and “:-(” to form a training set for the sentiment classification. For this purpose, the author collected texts containing emoticons from Usenet newsgroups. The dataset was divided into “positive” (texts with happy emoticons) and “negative” (texts with sad or angry emoticons) samples. Emoticons-trained classifiers: SVM and Naive Bayes, were able to obtain up to 70% accuracy on the test set. In (Go et al., 2009), authors used Twitter to collect training data and then to perform a sentiment search. The approach is similar to the one in (Read, 2005). The authors cons</context>
</contexts>
<marker>Read, 2005</marker>
<rawString>Jonathon Read. 2005. Using emoticons to reduce dependency in machine learning techniques for sentiment classification. In Proceedings of the ACL Student Research Workshop, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6693" citStr="Wilson et al., 2005" startWordPosition="1086" endWordPosition="1089">a bag of words. For English, we kept short forms as a single word: “don’t”, “I’ll”, “she’d”. 3. Stopwords removal – in English, texts we removed articles (“a”, “an”, “the”) from the bag of words. 4. N-grams construction – we make a set of ngrams out of consecutive words. A negation particle is attached to a word which precedes it and follows it. For example, a sentence “I do not like fish” will form three bigrams: “I do+not”, “do+not like”, “not+like fish”. Such a procedure improves the accuracy of the classification since the negation plays a special role in opinion and sentiment expression (Wilson et al., 2005). In English, we used negative particles ’no’ and ’not’. In Chinese, we used the following particles: 1. 不 – is not + noun 2. 未 – does not + verb, will not + verb 3. 莫 (別) – do not (imperative) 4. 無 (沒有) – does not have 3.2 Classifier We build a sentiment classifier using the multinomial Naive Bayes classifier which is based on Bayes’ theorem. P(s) P(M|s) P(s|M) = (1) P(M) where s is a sentiment, M is a text. We assume that a target adjective has the same sentiment polarity as the whole text, because in general the lengths of the given texts are small. Since we have sets of equal number of pos</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunfang Wu</author>
<author>Peng Jin</author>
</authors>
<title>Semeval-2010 task 18: Disambiguating sentiment ambiguous adjectives.</title>
<date>2010</date>
<booktitle>In SemEval2010: Proceedings ofInternational Workshop of Semantic Evaluations.</booktitle>
<contexts>
<context position="861" citStr="Wu and Jin, 2010" startWordPosition="121" endWordPosition="124">stract In this paper, we describe our system which participated in the SemEval 2010 task of disambiguating sentiment ambiguous adjectives for Chinese. Our system uses text messages from Twitter, a popular microblogging platform, for building a dataset of emotional texts. Using the built dataset, the system classifies the meaning of adjectives into positive or negative sentiment polarity according to the given context. Our approach is fully automatic. It does not require any additional hand-built language resources and it is language independent. 1 Introduction The dataset of the SemEval task (Wu and Jin, 2010) consists of short texts in Chinese containing target adjectives whose sentiments need to be disambiguated in the given contexts. Those adjectives are: Q big, 小 small, 多 many, 少 few, 高 high, 低 low, 厚 thick, 薄 thin, 深 deep, shallow, 重 heavy, light, 巨Q huge, 重Q grave. Disambiguating sentiment ambiguous adjectives is a challenging task for NLP. Previous studies were mostly focused on word sense disambiguation rather than sentiment disambiguation. Although both problems look similar, the latter is more challenging in our opinion because impregnated with more subjectivity. In order to solve the tas</context>
</contexts>
<marker>Wu, Jin, 2010</marker>
<rawString>Yunfang Wu and Peng Jin. 2010. Semeval-2010 task 18: Disambiguating sentiment ambiguous adjectives. In SemEval2010: Proceedings ofInternational Workshop of Semantic Evaluations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changhua Yang</author>
<author>Kevin Hsin-Yih Lin</author>
<author>HsinHsi Chen</author>
</authors>
<title>Emotion classification using web blog corpora.</title>
<date>2007</date>
<booktitle>In WI ’07: Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence,</booktitle>
<pages>275--278</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="3028" citStr="Yang et al., 2007" startWordPosition="487" endWordPosition="490">rom Twitter and used them to train a classifier to annotate the test dataset. 2. In the second one, we used machine translator to translate the dataset from Chinese to English and annotated it using collected English texts from Twitter as the training data. We have made the second approach because we were able to collect much more of English texts from Twitter than Chinese ones and we wanted to test the impact of machine translation on the performance of our classifier. We have experimented with Google Translate and Yahoo Babelfish4. Google Translate yielded better results. 2 Related work In (Yang et al., 2007), the authors use web-blogs to construct a corpora for sentiment analysis and use emotion icons assigned to blog posts as indicators of users’ mood. The authors applied SVM and CRF learners to classify sentiments at the sentence level and then investigated several strategies to determine the overall sentiment of the document. As 2An emoticon is a textual representation of an author’s emotion often used in Internet blogs and textual chats 3http://dev.twitter.com/doc/get/search 4http://babelfish.yahoo.com/ 436 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages </context>
</contexts>
<marker>Yang, Lin, Chen, 2007</marker>
<rawString>Changhua Yang, Kevin Hsin-Yih Lin, and HsinHsi Chen. 2007. Emotion classification using web blog corpora. In WI ’07: Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence, pages 275–278, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>