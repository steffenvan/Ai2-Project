<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000191">
<title confidence="0.969472">
RTM-DCU: Referential Translation Machines for Semantic Similarity
</title>
<author confidence="0.992335">
Ergun Bic¸ici
</author>
<affiliation confidence="0.991726333333333">
Centre for Global Intelligent Content
School of Computing
Dublin City University, Dublin, Ireland.
</affiliation>
<email confidence="0.994865">
ebicici@computing.dcu.ie
</email>
<author confidence="0.997407">
Andy Way
</author>
<affiliation confidence="0.99231">
Centre for Global Intelligent Content
School of Computing,
Dublin City University, Dublin, Ireland.
</affiliation>
<email confidence="0.994801">
away@computing.dcu.ie
</email>
<sectionHeader confidence="0.993802" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989740740741">
We use referential translation machines
(RTMs) for predicting the semantic simi-
larity of text. RTMs are a computational
model for identifying the translation acts
between any two data sets with respect
to interpretants selected in the same do-
main, which are effective when making
monolingual and bilingual similarity judg-
ments. RTMs judge the quality or the se-
mantic similarity of text by using retrieved
relevant training data as interpretants for
reaching shared semantics. We derive fea-
tures measuring the closeness of the test
sentences to the training data via inter-
pretants, the difficulty of translating them,
and the presence of the acts of transla-
tion, which may ubiquitously be observed
in communication. RTMs provide a lan-
guage independent approach to all simi-
larity tasks and achieve top performance
when predicting monolingual cross-level
semantic similarity (Task 3) and good re-
sults in semantic relatedness and entail-
ment (Task 1) and multilingual semantic
textual similarity (STS) (Task 10). RTMs
remove the need to access any task or do-
main specific information or resource.
</bodyText>
<sectionHeader confidence="0.929509" genericHeader="method">
1 Semantic Similarity Judgments
</sectionHeader>
<bodyText confidence="0.99965768">
We introduce a fully automated judge for seman-
tic similarity that performs well in three seman-
tic similarity tasks at SemEval-2014, Semantic
Evaluation Exercises - International Workshop on
Semantic Evaluation (Nakov and Zesch, 2014).
RTMs provide a language independent solution for
the semantic textual similarity (STS) task (Task
10) (Agirre et al., 2014), achieve top perfor-
mance when predicting monolingual cross-level
semantic similarity (Task 3) (Jurgens et al., 2014),
and achieve good results in the semantic related-
ness and entailment task (Task 1) (Marelli et al.,
2014a).
Referential translation machine (Section 2) is
a computational model for identifying the acts of
translation for translating between any given two
data sets with respect to a reference corpus se-
lected in the same domain. An RTM model is
based on the selection of interpretants, training
data close to both the training set and the test set,
which allow shared semantics by providing con-
text for similarity judgments. In semiotics, an in-
terpretant I interprets the signs used to refer to the
real objects (Bic¸ici, 2008). Each RTM model is
a data translation and translation prediction model
between the instances in the training set and the
test set and translation acts are indicators of the
data transformation and translation. RTMs present
an accurate and language independent solution for
making semantic similarity judgments.
We describe the tasks we participated below.
Section 2 describes the RTM model and the fea-
tures used. Section 3 presents the training and test
results we obtain on the three tasks we competed
and the last section concludes.
Task 1 Evaluation of Compositional Distribu-
tional Semantic Models on Full Sentences
through Semantic Relatedness and Entail-
ment (SRE) (Marelli et al., 2014a):
Given two sentences, produce a related-
ness score indicating the extent to which
the sentences express a related meaning: a
number in the range [1, 5].
We model the problem as a translation perfor-
mance prediction task where one possible inter-
pretation is obtained by translating S1 (the source
to translate, S) to S2 (the target translation, T).
Since linguistic processing can reveal deeper sim-
ilarity relationships, we also look at the translation
task at different granularities of information: plain
</bodyText>
<page confidence="0.975997">
487
</page>
<note confidence="0.980985">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 487–496,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.972632913043478">
text (R for regular) and after lemmatization (L).
We lowercase all text.
Task 3 Cross-Level Semantic Similarity
(CLSS) (Jurgens et al., 2014):
Given two text from different levels, pro-
duce a semantic similarity rating: a num-
ber in the range [0, 4].
CLSS task targets semantic similarity compar-
isons between text having different levels of gran-
ularity and we address the following level cross-
ings: paragraph to sentence, sentence to phrase,
and phrase to word. We model the problem as
a translation performance prediction task among
text from different levels.
Task 10 Multilingual Semantic Textual Similarity
(MSTS) (Agirre et al., 2014)
Given two sentences 51 and 52 in the same
language, quantify the degree of similar-
ity: a number in the range [0, 5].
MSTS task addresses the problem in English
and Spanish (score range is [0, 4]). We model the
problem as a translation performance prediction
task between 51 and 52.
</bodyText>
<sectionHeader confidence="0.9934145" genericHeader="method">
2 Referential Translation Machine
(RTM)
</sectionHeader>
<bodyText confidence="0.9967555">
Referential translation machines provide a compu-
tational model for quality and semantic similarity
judgments in monolingual and bilingual settings
using retrieval of relevant training data (Bic¸ici,
2011; Bic¸ici and Yuret, 2014) as interpretants for
reaching shared semantics (Bic¸ici, 2008). RTMs
are a language independent approach and achieve
top performance when predicting the quality of
translations (Bic¸ici, 2013; Bic¸ici and Way, 2014)
and when predicting monolingual cross-level se-
mantic similarity (Jurgens et al., 2014), and good
performance when evaluating the semantic relat-
edness of sentences and their entailment (Marelli
et al., 2014a), as an automated student answer
grader (Bic¸ici and van Genabith, 2013b), and
when judging the semantic similarity of sen-
tences (Bic¸ici and van Genabith, 2013a; Agirre et
al., 2014). We improve the RTM models by:
</bodyText>
<listItem confidence="0.876572217391304">
• using a parameterized, fast implementation
of FDA, FDA5, and our Parallel FDA5 in-
stance selection model (Bic¸ici et al., 2014),
• better modeling of the language in which
Algorithm 1: Referential Translation Machine
Input: Training set train, test set test,
corpus C, and learning model M.
Data: Features of train and test, Ftrain
and Ftest.
Output: Predictions of similarity scores on
the test ˆq.
1 FDA5(train, test, C) → Z
2 MTPP(Z,train) → Ftrain
3 MTPP(Z, test) → Ftest
4 learn(M,Ftrain) → M
5 predict(M,Ftest) → qˆ
similarity judgments are made with improved
optimization and selection of the LM data,
• using a general domain corpus to select inter-
pretants from,
• increased feature set for also modeling the
structural properties of sentences,
• extended learning models.
</listItem>
<bodyText confidence="0.997801814814815">
We use the Parallel FDA5 (Feature Decay Algo-
rithms) instance selection model for selecting the
interpretants (Bic¸ici et al., 2014; Bic¸ici and Yuret,
2014) this year, which allows efficient parameteri-
zation, optimization, and implementation of FDA,
and build an MTPP model (Section 2.1). We view
that acts of translation are ubiquitously used dur-
ing communication:
Every act of communication is an act of
translation (Bliss, 2012).
Translation need not be between different lan-
guages and paraphrasing or communication also
contain acts of translation. When creating sen-
tences, we use our background knowledge and
translate information content according to the cur-
rent context.
The inputs to the RTM algorithm Algorithm 1
are a training set train, a test set test, some
corpus C, preferably in the same domain as the
training and test sets, and a learning model. Step 1
selects the interpretants, Z, relevant to both the
training and test data. Steps 2 and 3 use Z to map
train and test to a new space where similari-
ties between translation acts can be derived more
easily. Step 4 trains a learning model M over the
training features, Ftrain, and Step 5 obtains the
predictions. Figure 1 depicts the RTM.
</bodyText>
<page confidence="0.998655">
488
</page>
<figureCaption confidence="0.999903">
Figure 1: RTM depiction.
</figureCaption>
<bodyText confidence="0.999972461538462">
Our encouraging results in the semantic simi-
larity tasks increase our understanding of the acts
of translation we ubiquitously use when commu-
nicating and how they can be used to predict the
semantic similarity of text. RTM and MTPP mod-
els are not data or language specific and their mod-
eling power and good performance are applicable
in different domains and tasks. RTM expands the
applicability of MTPP by making it feasible when
making monolingual quality and similarity judg-
ments and it enhances the computational scalabil-
ity by building models over smaller and more rel-
evant set of interpretants.
</bodyText>
<subsectionHeader confidence="0.9830005">
2.1 The Machine Translation Performance
Predictor (MTPP)
</subsectionHeader>
<bodyText confidence="0.999966833333334">
MTPP (Bic¸ici et al., 2013) is a state-of-the-art
and top performing machine translation perfor-
mance predictor, which uses machine learning
models over features measuring how well the test
set matches the training set to predict the quality
of a translation without using a reference trans-
lation. MTPP measures the coverage of individ-
ual test sentence features found in the training set
and derives indicators of the closeness of test sen-
tences to the available training data, the difficulty
of translating the sentence, and the presence of
acts of translation for data transformation.
</bodyText>
<subsectionHeader confidence="0.997257">
2.2 MTPP Features for Translation Acts
</subsectionHeader>
<bodyText confidence="0.99920252173913">
MTPP feature functions use statistics involving
the training set and the test sentences to deter-
mine their closeness. Since they are language
independent, MTPP allows quality estimation to
be performed extrinsically. MTPP uses n-gram
features defined over text or common cover link
(CCL) (Seginer, 2007) structures as the basic units
of information over which similarity calculations
are made. Unsupervised parsing with CCL ex-
tracts links from base words to head words, rep-
resenting the grammatical information instantiated
in the training and test data.
We extend the MTPP model we used last
year (Bic¸ici, 2013) in its learning module and the
features included. Categories for the features (S
for source, T for target) used are listed below
where the number of features are given in brackets
for S and T, {#S, #T}, and the detailed descriptions
for some of the features are presented in (Bic¸ici et
al., 2013). The number of features for each task
differs since we perform an initial feature selection
step on the tree structural features (Section 2.3).
The number of features are in the range 337−437.
</bodyText>
<listItem confidence="0.86426275">
• Coverage 156, 541: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
• Perplexity 145, 451: Measures the fluency of
</listItem>
<bodyText confidence="0.931277333333333">
the sentences according to language models
(LM). We use both forward ({30}) and back-
ward (1151) LM features for S and T.
</bodyText>
<listItem confidence="0.8736767">
• TreeF 10, 10-1101: 10 base features and up
to 100 selected features of T among parse tree
structures (Section 2.3).
• Retrieval Closeness 116, 121: Measures the
degree to which sentences close to the test set
are found in the selected training set, Z, using
FDA (Bic¸ici and Yuret, 2011a) and BLEU,
F1 (Bic¸ici, 2011), dice, and tf-idf cosine sim-
ilarity metrics.
• IBM2 Alignment Features 10, 221: Calcu-
</listItem>
<bodyText confidence="0.99310925">
lates the sum of the entropy of the dis-
tribution of alignment probabilities for S
(E,∈S −p log p for p = p(t|s) where s and
t are tokens) and T, their average for S and
T, the number of entries with p &gt; 0.2 and
p &gt; 0.01, the entropy of the word align-
ment between S and T and its average, and
word alignment log probability and its value
in terms of bits per word. We also com-
pute word alignment percentage as in (Ca-
margo de Souza et al., 2013) and potential
BLEU, F1, WER, PER scores for S and T.
</bodyText>
<listItem confidence="0.9663945">
• IBM1 Translation Probability 14, 121: Cal-
culates the translation probability of test
sentences using the selected training set,
Z (Brown et al., 1993).
• Feature Vector Similarity 18, 81: Calculates
similarities between vector representations.
</listItem>
<page confidence="0.9992">
489
</page>
<tableCaption confidence="0.999561">
Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP).
</tableCaption>
<figure confidence="0.989113333333333">
2
1
1
1
1
3 1
3 4
5 1
7 15
CCL
numB depthB avg depthB R/L avg R/L
24.0 9.0 0.375 2.1429 3.401
1 8
2 10
1 1
1 2
1
1
1
1
1 13
</figure>
<listItem confidence="0.997840357142857">
• Entropy {2, 8}: Calculates the distributional
similarity of test sentences to the training set
over top N retrieved sentences (Bic¸ici et al.,
2013).
• Length {6, 3}: Calculates the number of
words and characters for S and T and their
average token lengths and their ratios.
• Diversity {3, 3}: Measures the diver-
sity of co-occurring features in the training
set (Bic¸ici et al., 2013).
• Synthetic Translation Performance {3, 3}:
Calculates translation scores achievable ac-
cording to the n-gram coverage.
• Character n-grams {5}: Calculates cosine
between character n-grams (for n=2,3,4,5,6)
obtained for S and T (B¨ar et al., 2012).
• Minimum Bayes Retrieval Risk {0, 4}: Cal-
culates the translation probability for the
translation having the minimum Bayes risk
among the retrieved training instances.
• Sentence Translation Performance {0, 3}:
Calculates translation scores obtained ac-
cording to q(T, R) using BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), or
F1 (Bic¸ici and Yuret, 2011b) for q.
• LIX {1, 1}: Calculates the LIX readability
score (Wikipedia, 2013; Bj¨ornsson, 1968) for
S and T. 1
</listItem>
<subsectionHeader confidence="0.999296">
2.3 Bracketing Tree Structural Features
</subsectionHeader>
<bodyText confidence="0.9994364">
We use the parse tree outputs obtained by CCL
to derive features based on the bracketing struc-
ture. We derive 5 statistics based on the geometric
properties of the parse trees: number of brackets
used (numB), depth (depthB), average depth (avg
</bodyText>
<equation confidence="0.71896">
1LIX= B + C 100, where A is the number of words, C is
</equation>
<bodyText confidence="0.998539863636363">
words longer than 6 characters, B is words that start or end
with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012).
depthB), number of brackets on the right branches
over the number of brackets on the left (R/L) 2, av-
erage right to left branching over all internal tree
nodes (avg R/L). The ratio of the number of right
to left branches shows the degree to which the sen-
tence is right branching or not. Additionally, we
capture the different types of branching present
in a given parse tree identified by the number of
nodes in each of its children.
Table 1 depicts the parsing output obtained by
CCL for the following sentence from WSJ23 3:
Many fund managers argue that now ’s the time
to buy.
We use Tregex (Levy and Andrew, 2006) for vi-
sualizing the output parse trees presented on the
left. The bracketing structure statistics and fea-
tures are given on the right hand side. The root
node of each tree structural feature represents the
number of times that feature is present in the pars-
ing output of a document.
</bodyText>
<sectionHeader confidence="0.996645" genericHeader="method">
3 SemEval-14 Results
</sectionHeader>
<bodyText confidence="0.9997116">
We develop individual RTM models for each task
and subtask that we participate at SemEval-2014
with the RTM-DCU team name. The interpre-
tants are selected from the LM corpora distributed
by the translation task of WMT14 (Bojar et al.,
2014) and the LM corpora provided by LDC for
English (Parker et al., 2011) and Spanish ( ˆAngelo
Mendonc¸a, 2011) 4. We use the Stanford POS tag-
ger (Toutanova et al., 2003) to obtain the lemma-
tized corpora for the SRE task. For each RTM
</bodyText>
<footnote confidence="0.9941658">
2For nodes with uneven number of children, the nodes in
the odd child contribute to the right branches.
3Wall Street Journal (WSJ) corpus section 23, distributed
with Penn Treebank version 3 (Marcus et al., 1993).
4English Gigaword 5th, Spanish Gigaword 3rd edition.
</footnote>
<page confidence="0.995607">
490
</page>
<bodyText confidence="0.99973725">
model, we extract the features both on the train-
ing set and the test set. The number of instances
we select for the interpretants in each task is given
in Table 2.
</bodyText>
<table confidence="0.999407">
Task Setting Train LM
Task 1, SRE English 770 10770
Task 3, CLSS Par2S 302 2802
Task 3, CLSS S2Phrase 202 2702
Task 3, CLSS Phrase2W 102 2602
Task 10, MSTS English 504 8002
Task 10, MSTS English OnWN 504 8004
Task 10, MSTS Spanish 502 8002
</table>
<tableCaption confidence="0.965205">
Table 2: Number of sentences in Z (in thousands)
selected for each task.
</tableCaption>
<bodyText confidence="0.999877785714286">
We use ridge regression (RR), support vector
regression (SVR) with RBF (radial basis func-
tions) kernel (Smola and Sch¨olkopf, 2004), and
extremely randomized trees (TREE) (Geurts et al.,
2006) as the learning models. TREE is an en-
semble learning method over randomized decision
trees. These models learn a regression function
using the features to estimate a numerical target
value. We also use these learning models after
a feature subset selection with recursive feature
elimination (RFE) (Guyon et al., 2002) or a di-
mensionality reduction and mapping step using
partial least squares (PLS) (Specia et al., 2009),
both of which are described in (Bic¸ici et al., 2013).
We optimize the learning parameters, the num-
ber of features to select, the number of dimen-
sions used for PLS, and the parameters for paral-
lel FDA5. More detailed descriptions of the opti-
mization processes are given in (Bic¸ici et al., 2013;
Bic¸ici et al., 2014). We optimize the learning pa-
rameters by selecting e close to the standard devi-
ation of the noise in the training set (Bic¸ici, 2013)
since the optimal value for e is shown to have
linear dependence to the noise level for different
noise models (Smola et al., 1998). At testing time,
the predictions are bounded to obtain scores in the
corresponding ranges. We obtain the confidence
scores using support vector classification (SVC).
</bodyText>
<subsectionHeader confidence="0.993623">
3.1 Task 1: Semantic Relatedness and
Entailment
</subsectionHeader>
<bodyText confidence="0.999842368421053">
MSTS contains sentence pairs from the SICK
(Sentences Involving Compositional Knowledge)
data set (Marelli et al., 2014b), which contain sen-
tence pairs that contain rich lexical, syntactic and
semantic phenomena. Official evaluation metric
in SRE is the Pearson’s correlation score, which
is used to select the top systems on the training
set. SRE task allows the submission of 5 entries.
We present the performance of the top 5 individ-
ual RTM models on the training set in Table 3.
ACC is entailment accuracy, rp is Pearson’s corre-
lation, rs is Spearman’s correlation, MSE is mean
squared error, MAE is mean absolute error, and
RAE is relative absolute error. L uses the lem-
matized corpora and R uses the true-cased corpora
corresponding to regular. R+L correspond to the
perspective using the features from both R and L,
which doubles the number of features. We com-
pute the entailment by SVC.
</bodyText>
<table confidence="0.999849166666667">
Data Model ACC rp rs MSE MAE RAE
L SVR 67.52 .7372 .6918 .6946 .5511 .6856
L PLS-SVR 67.04 .7539 .6927 .6763 .5369 .668
R+L PLS-SVR 66.76 .75 .6879 .6815 .539 .6705
R+L SVR 66.66 .7295 .6814 .7027 .5591 .6956
L PLS-RR 66.56 .7247 .6765 .7054 .5687 .7075
</table>
<tableCaption confidence="0.9470085">
Table 3: SRE training results of the top 5 RTM
systems selected.
</tableCaption>
<bodyText confidence="0.9994532">
SRE challenge results on the test set are given
in Table 4. The setting R using PLS-SVR learning
becomes the 8th out of 17 submissions when pre-
dicting the semantic relatedness and 17th out of 18
submissions when predicting the entailment.
</bodyText>
<table confidence="0.999853833333333">
Data Model ACC rp rs RMSE MAE RAE
R PLS-SVR 67.20 .7639 .6877 .655 .5246 .6645
R+L PLS-SVR 67.65 .7688 .6918 .6492 .5194 .658
L SVR 67.65 .7559 .6887 .664 .531 .6726
R+L SVR 67.44 .7625 .6899 .6555 .5251 .6651
R PLS-SVR 66.61 .7570 .6683 .6637 .5324 .6744
</table>
<tableCaption confidence="0.99876">
Table 4: RTM-DCU test results on the SRE task.
</tableCaption>
<table confidence="0.9998419">
Model rp RMSE MAE RAE
Par2S TREE 0.8013 0.8345 0.6277 0.5083
Par2S PLS-TREE 0.7737 0.8824 0.673 0.5449
Par2S SVR 0.7718 0.8863 0.6791 0.5499
S2Phrase TREE 0.6756 0.9887 0.7746 0.6665
S2Phrase PLS-TREE 0.6119 1.0616 0.8582 0.7384
S2Phrase SVR 0.6059 1.0662 0.8668 0.7458
Phrase2W TREE 0.201 1.3275 1.1353 0.9706
Phrase2W RR 0.1255 1.3463 1.1594 0.9912
Phrase2W SVR 0.0847 1.3548 1.1663 0.9972
</table>
<tableCaption confidence="0.69488875">
Table 5: CLSS training results of the top 3 RTM
systems for each subtask. Levels correspond to
paragraph to sentence (Par2S), sentence to phrase
(S2Phrase), and phrase to word (Phrase2W).
</tableCaption>
<page confidence="0.997816">
491
</page>
<subsectionHeader confidence="0.991015">
3.2 Task 3: Cross-Level Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.9952014">
CLSS contains sentence pairs from different gen-
res including text from newswire, travel, reviews,
metaphoric text, community question answering
sites, idiomatic text, descriptions, lexicographic
text, and search. Official evaluation metric in
CLSS is the sum of the Pearson’s correlation
scores for different levels 5. CLSS task allows the
submission of 3 entries per subtask. We present
the performance of the top 3 individual RTM mod-
els on the training set in Table 5. RMSE is the root
mean squared error. As the compared text size de-
crease, the performance decrease since it can be-
come harder and more ambiguous to find the simi-
larity using less context. RTM-DCU results on the
CLSS challenge test set are provided in Table 6.
</bodyText>
<table confidence="0.9990164">
Model rp RMSE MAE RAE
Par2S TREE .8445 .7417 .5622 .4579
Par2S PLS-TREE .7847 .853 .6456 .5258
Par2S SVR .7858 .8428 .6539 .5325
S2Phrase TREE .75 .8827 .7053 .6255
S2Phrase PLS-TREE .6979 .9491 .7781 .69
S2Phrase SVR .6631 .9835 .7992 .7088
Phrase2W TREE .3053 1.3351 1.14 .9488
Phrase2W RR .2207 1.3644 1.1574 .9633
Phrase2W SVR .1712 1.3792 1.1792 .9815
</table>
<tableCaption confidence="0.979261666666667">
Table 6: RTM-DCU test results on CLSS for the
top 3 RTM systems for each subtask.
Table 7 lists the results along with their ranks
</tableCaption>
<bodyText confidence="0.983296222222222">
for rp and rs, Spearman’s correlation, out of
CHECK submissions. The baseline in Table 7
is normalized longest common substring (LCS)
scaled in the range [0, 4]. Top individual rank row
lists the ranks in each subtask. We present the re-
sults for both our official and late (about 1 day)
submissions including word to sense (W2S) re-
sults 6. RTM-DCU is able to obtain the top result
in Par2S in the CLSS task.
</bodyText>
<subsectionHeader confidence="0.983458">
3.3 Task 10: Multilingual Semantic Textual
Similarity
</subsectionHeader>
<bodyText confidence="0.999960285714286">
MSTS contains sentence pairs from different do-
mains: sense definitions from semantic lexical re-
sources such as OnWN (from OntoNotes (Prad-
han et al., 2007) and WordNet (Miller, 1995)) and
FNWN (from FrameNet (Baker et al., 1998) and
WordNet), news headlines, image descriptions,
news title tweet comments, deft forum and news,
</bodyText>
<footnote confidence="0.993549">
5Giving advantage to participants submitting to all levels.
6W2S results for the late submission is obtained from the
LCS baseline to calculate the ranks.
</footnote>
<table confidence="0.999966833333334">
rp Par2S S2Phrase Phrase2W W2S Rank
LCS 0.527 0.562 0.165 0.109 25
0.780 0.677 0.208 14
Official 0.747 0.588 0.164 19
0.786 0.666 0.171 18
0.845 0.750 0.305 0.109 6
Late 0.785 0.698 0.221 0.109 13
0.786 0.663 0.171 0.109 17
Top Rank 1 5 3
rs Par2S S2Phrase Phrase2W W2S Rank
LCS 0.527 0.562 0.165 0.13 23
0.780 0.677 0.208 17
Official 0.747 0.588 0.164 22
0.786 0.666 0.171 18
0.829 0.734 0.295 0.13 8
Late 0.778 0.687 0.219 0.13 15
0.778 0.667 0.166 0.13 16
Top Rank 1 5 5
</table>
<tableCaption confidence="0.99993">
Table 7: RTM-DCU test results on CLSS.
</tableCaption>
<bodyText confidence="0.989156923076923">
paraphrases. Official evaluation metric in MSTS
is the Pearson’s correlation score.
MSTS task provides 7622 training instances
and 3750 test instances. For the OnWN domain,
1316 training instances are available and therefore,
we build a separate RTM model for this domain.
Separate modeling of the OnWN dataset results
with higher confidence scores on the test instances
than we would obtain using the overall model to
predict. MSTS task allows the submission of 3 en-
tries per subtask. We present the performance of
the top 3 individual RTM models on the training
set in Table 8.
</bodyText>
<table confidence="0.9721451">
Lang Model rp RMSE MAE RAE
TREE 0.6931 1.0627 0.8058 0.6649
PLS-TREE 0.6875 1.0753 0.8038 0.6632
PLS-SVR 0.6884 1.0698 0.8157 0.6730
TREE 0.8094 0.9295 0.694 0.5245
PLS-TREE 0.7953 0.9604 0.7203 0.5444
PLS-SVR 0.7888 0.9779 0.7234 0.5468
TREE 0.6513 0.7341 0.5904 0.7508
PLS-TREE 0.4157 0.9007 0.7108 0.9039
PLS-SVR 0.4239 1.1427 0.8293 1.0545
</table>
<tableCaption confidence="0.7957625">
Table 8: MSTS training results on the English, En-
glish OnWN, and Spanish tasks.
</tableCaption>
<bodyText confidence="0.9993176">
RTM results on the MSTS challenge test set are
provided in Table 9 along with the RTM results in
STS 2013 (Bic¸ici and van Genabith, 2013a). Ta-
ble 10 and Table 11 lists the official results on En-
glish and Spanish tasks with rankings calculated
according to weighted rp, which weights accord-
ing to the number of instances in each domain.
RTM-DCU is able to become 10th in the OnWN
domain and 19th overall out of 38 submissions in
MSTS English and 18th out of 22 submissions in
</bodyText>
<figure confidence="0.975631">
English
OnWN
Spanish
</figure>
<page confidence="0.78258">
492
</page>
<table confidence="0.978491675">
English
Spanish
STS 2013 English
Model rp RMSE MAE RAE
TREE .4341 1.4306 1.1609 1.0908
deft-forum PLS-TREE .3965 1.4115 1.1472 1.078
PLS-SVR .3078 1.6277 1.3482 1.2669
TREE .6974 1.1469 .9032 .8716
deft-news PLS-TREE .6811 1.1229 .8769 .8462
PLS-SVR .5562 1.2803 .9835 .9491
TREE .6199 1.1495 .9254 .7845
headlines PLS-TREE .6125 1.1552 .9314 .7896
PLS-SVR .6301 1.1041 .8807 .7467
TREE .6995 1.2034 .9499 .7395
images PLS-TREE .6656 1.2298 .9692 .7545
PLS-SVR .6474 1.4406 1.1057 .8607
TREE .8058 1.3122 1.0028 .5585
OnWN PLS-TREE .7992 1.2997 .9815 .5467
PLS-SVR .8004 1.2913 .9449 .5263
TREE .6882 .9869 .831 .8093
tweet-news PLS-TREE .6691 1.0101 .8433 .8213
PLS-SVR .5531 1.0633 .8653 .8427
TREE .7 1.5185 1.351 1.4141
News PLS-TREE .6253 1.6523 1.4464 1.514
PLS-SVR .6411 1.554 1.3196 1.3813
TREE .4216 1.5433 1.298 1.3579
Wikipedia PLS-TREE .3689 1.6655 1.4015 1.4662
PLS-SVR .4242 1.5998 1.3141 1.3748
L+S SVR .6552 1.5649 1.2763 1.0231
headlines L+P+S SVR .651 1.4845 1.1984 .9607
L+P+S SVR TL .6385 1.4878 1.2008 .9626
L+S SVR .6943 1.7065 1.3545 .8255
OnWN L+P+S SVR .6971 1.6737 1.333 .8124
L+P+S SVR TL .6755 1.7124 1.3598 .8287
L+S SVR .3005 .8833 .6886 1.6132
SMT L+P+S SVR .2861 .8810 .6821 1.598
L+P+S SVR TL .3098 .8635 .6547 1.5339
L+S SVR .2016 1.2957 1.0604 1.2633
FNWN L+P+S SVR .118 1.4369 1.1866 1.4136
L+P+S SVR TL .1823 1.3245 1.0962 1.3059
</table>
<tableCaption confidence="0.8983555">
Table 9: RTM-DCU test results on MSTS for the
top 3 RTM systems for each subtask as well as
RTM results in STS 2013 (Bic¸ici and van Gen-
abith, 2013a).
</tableCaption>
<bodyText confidence="0.9268676">
MSTS Spanish. The performance difference be-
tween MSTS English and MSTS Spanish may be
due to the fewer training data available for the
MSTS Spanish task, which may be decreasing the
performance of our supervised learning approach.
</bodyText>
<subsectionHeader confidence="0.991872">
3.4 RTMs Across Tasks and Years
</subsectionHeader>
<bodyText confidence="0.9992049">
We compare the difficulty of tasks according to the
RAE levels achieved. RAE measures the error rel-
ative to the error when predicting the actual mean.
A high RAE is an indicator that the task is hard.
In Table 12, we list the RAE obtained for differ-
ent tasks and subtasks, also listing RTM results in
STS 2013 (Bic¸ici and van Genabith, 2013a) and
RTM results (Bic¸ici and Way, 2014) on the quality
estimation task (QET) (Bojar et al., 2014) where
post-editing effort (PEE), human-targeted transla-
</bodyText>
<table confidence="0.9979415">
Model Wikipedia News Weighted rp Rank
TREE 0.4216 0.7000 0.5878 18
PLS-TREE 0.3689 0.6253 0.5219 20
PLS-SVR 0.4242 0.6411 0.5537 19
</table>
<tableCaption confidence="0.971193">
Table 11: RTM-DCU test results on MSTS Span-
ish task. Rankings are calculated according to the
weighted Pearson’s correlation.
</tableCaption>
<bodyText confidence="0.947533916666667">
tion edit rate (HTER), or post-editing time (PET)
of translations are predicted.
The best results are obtained for the CLSS
Par2S subtask, which may be due to the larger
contextual information that paragraphs can pro-
vide for the RTM models. For the SRE task, we
can only reduce the error with respect to knowing
and predicting the mean by about 35%. Prediction
of bilingual similarity as in quality estimation of
translation can be expected to be harder and RTMs
achieve state-of-the-art performance in this task as
well (Bic¸ici and Way, 2014).
</bodyText>
<sectionHeader confidence="0.99594" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999806222222222">
Referential translation machines provide a clean
and intuitive computational model for automati-
cally measuring semantic similarity by measur-
ing the acts of translation involved and achieve to
be the top on some semantic similarity tasks at
SemEval-2014. RTMs make quality and seman-
tic similarity judgments possible based on the re-
trieval of relevant training data as interpretants for
reaching shared semantics.
</bodyText>
<sectionHeader confidence="0.984418" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.976007142857143">
This work is supported in part by SFI
(07/CE/I1142) as part of the CNGL Centre
for Global Intelligent Content (www.cngl.org)
at Dublin City University, in part by SFI
(13/TIDA/I2740) for the project “Monolin-
gual and Bilingual Text Quality Judgments
with Translation Performance Prediction”
(www.computing.dcu.ie/˜ebicici/
Projects/TIDA RTM.html), and in part
by the European Commission through the QT-
LaunchPad FP7 project (No: 296347). We
also thank the SFI/HEA Irish Centre for High-
End Computing (ICHEC) for the provision of
computational facilities and support.
</bodyText>
<page confidence="0.997934">
493
</page>
<table confidence="0.999835222222222">
Model deft-forum deft-news headlines images OnWN tweet-news Weighted rP Rank
TREE .4341 .6974 .6199 .6995 .8058 .6882 .6706 20
PLS-TREE .3965 .6811 .6125 .6656 .7992 .6691 .6513 23
PLS-SVR .3078 .5562 .6301 .6475 .8004 .5531 .6076 27
Top Rank 17 16 25 26 16 13
TREE .4181 .6846 .6216 .6981 .8331 .6870 .6729 19
PLS-TREE .3831 .6739 .6094 .6629 .8260 .6691 .6534 23
PLS-SVR .2731 .5526 .6330 .6441 .8246 .5683 .6110 26
Top Rank 18 18 23 27 10 14
</table>
<tableCaption confidence="0.998671">
Table 10: RTM-DCU test results with ranks on MSTS English task.
</tableCaption>
<table confidence="0.999927454545455">
Task Subtask Domain Model RAE
SRE English SICK R PLS-SVR .6645
R+L PLS-SVR .6580
L SVR .6726
R+L SVR .6651
R PLS-SVR .6744
CLSS Par2S Mixed TREE .4579
S2Phrase TREE .6255
Phrase2W TREE .9488
MSTS English deft-forum PLS-TREE 1.078
deft-news PLS-TREE .8462
headlines PLS-SVR .7467
images TREE .7395
OnWN PLS-SVR .5263
tweet-news TREE .8093
Spanish News PLS-SVR 1.3813
Wikipedia TREE 1.3579
STS 2013 English headlines L+P+S SVR .9607
OnWN L+P+S SVR .8124
SMT L+P+S SVR TL 1.5339
FNWN L+S SVR 1.2633
QET PEE Spanish-English Europarl FS-RR .9000
Spanish-English Europarl PLS-RR .9409
English-German Europarl PLS-TREE .8883
English-German Europarl TREE .8602
English-Spanish Europarl TREE 1.0983
English-Spanish Europarl PLS-TREE 1.0794
German-English Europarl RR .8204
German-English Euruparl PLS-RR .8437
QET HTER English-Spanish Europarl SVR .8532
English-Spanish Europarl TREE .8931
QET PET English-Spanish Europarl SVR .7223
English-Spanish Europarl RR .7536
</table>
<tableCaption confidence="0.819218666666667">
Table 12: Best RTM-DCU RAE test results for different tasks and subtasks as well as STS 2013 re-
sults (Bic¸ici and van Genabith, 2013a) and results from quality estimation task of translation (Bojar et
al., 2014; Bic¸ici and Way, 2014).
</tableCaption>
<bodyText confidence="0.634564">
With Conf.
</bodyText>
<sectionHeader confidence="0.997287" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9493035">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilin-
gual semantic textual similarity. In Proc. of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proc. of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics - Volume 1, ACL
’98, pages 86–90, Stroudsburg, PA, USA.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In *SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics – Volume 1: Proc. of the main conference and
the shared task, and Volume 2: Proc. of the Sixth In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2012), pages 435–440, Montr´eal, Canada, 7-
8 June. Association for Computational Linguistics.
Ergun Bic¸ici and Josef van Genabith. 2013a. CNGL-
</reference>
<page confidence="0.995864">
494
</page>
<reference confidence="0.99900471559633">
CORE: Referential translation machines for measur-
ing semantic similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, Georgia, USA, 13-14 June. Asso-
ciation for Computational Linguistics.
Ergun Bic¸ici and Josef van Genabith. 2013b. CNGL:
Grading student answers by acts of translation. In
*SEM 2013: The Second Joint Conference on Lex-
ical and Computational Semantics and Proc. of the
Seventh International Workshop on Semantic Eval-
uation (SemEval 2013), Atlanta, Georgia, USA, 14-
15 June. Association for Computational Linguistics.
Ergun Bic¸ici and Andy Way. 2014. Referential trans-
lation machines for predicting translation quality. In
Proc. of the Ninth Workshop on Statistical Machine
Translation, Baltimore, USA, June. Association for
Computational Linguistics.
Ergun Bic¸ici and Deniz Yuret. 2011a. Instance se-
lection for machine translation using feature decay
algorithms. In Proc. of the Sixth Workshop on Sta-
tistical Machine Translation, pages 272–283, Ed-
inburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ergun Bic¸ici and Deniz Yuret. 2011b. RegMT sys-
tem for machine translation, system combination,
and evaluation. In Proc. of the Sixth Workshop on
Statistical Machine Translation, pages 323–329, Ed-
inburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ergun Bic¸ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Ergun Bic¸ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation, 27:171–192, December.
Ergun Bic¸ici, Qun Liu, and Andy Way. 2014. Paral-
lel FDA5 for fast deployment of accurate statistical
machine translation systems. In Proc. of the Ninth
Workshop on Statistical Machine Translation, Bal-
timore, USA, June. Association for Computational
Linguistics.
Ergun Bic¸ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc¸ University. Supervi-
sor: Deniz Yuret.
Ergun Bic¸ici. 2013. Referential translation machines
for quality estimation. In Proc. of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Ergun Bic¸ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multia-
gent and Grid Systems.
Carl Hugo Bj¨ornsson. 1968. L¨asbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Ond&amp;quot;rej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Matou&amp;quot;s Mach´a&amp;quot;cek,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, and Lucia Specia.
2014. Findings of the 2014 workshop on statisti-
cal machine translation. In Proc. of the Ninth Work-
shop on Statistical Machine Translation, Balrimore,
USA, June. Association for Computational Linguis-
tics.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263–311, June.
Jos´e Guilherme Camargo de Souza, Christian Buck,
Marco Turchi, and Matteo Negri. 2013. FBK-
UEdin participation to the WMT13 quality estima-
tion shared task. In Proc. of the Eighth Workshop
on Statistical Machine Translation, pages 352–358,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proc. of the second interna-
tional conference on Human Language Technology
Research, pages 138–145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3–42.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Ma-
chine Learning, 46(1-3):389–422.
Kenth Hagstr¨om. 2012. Swedish readabil-
ity calculator. https://github.com/keha76/Swedish-
Readability-Calculator.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-level semantic similarity. In Proc. of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Roger Levy and Galen Andrew. 2006. Tregex and
Tsurgeon: tools for querying and manipulating tree
data structures. In Proc. of the fifth international
conference on Language Resources and Evaluation.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Comput.
Linguist., 19(2):313–330, June.
</reference>
<page confidence="0.987966">
495
</page>
<reference confidence="0.99990853030303">
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014a. SemEval-2014 Task 1: Evaluation
of compositional distributional semantic models on
full sentences through semantic relatedness and tex-
tual entailment. In Proc. of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland, August.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation of
compositional distributional semantic models. In
Proc. of LREC 2014, Reykjavik, Iceland.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39–41, November.
Preslav Nakov and Torsten Zesch, editors. 2014.
Proc. of SemEval-2014 Semantic Evaluation Exer-
cises - International Workshop on Semantic Evalua-
tion. Dublin, Ireland, 23-24 August.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword fifth edi-
tion, Linguistic Data Consortium.
Sameer S. Pradhan, Eduard H. Hovy, Mitchell P.
Marcus, Martha Palmer, Lance A. Ramshaw, and
Ralph M. Weischedel. 2007. Ontonotes: a unified
relational semantic representation. Int. J. Semantic
Computing, 1(4):405–419.
Yoav Seginer. 2007. Learning Syntactic Structure.
Ph.D. thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199–222, August.
A. J. Smola, N. Murata, B. Sch¨olkopf, and K.-R.
M¨uller. 1998. Asymptotically optimal choice of ε-
loss for support vector machines. In L. Niklasson,
M. Boden, and T. Ziemke, editors, Proc. of the Inter-
national Conference on Artificial Neural Networks,
Perspectives in Neural Computing, pages 105–110,
Berlin. Springer.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proc. of the 13th Annual Conference of
the European Association for Machine Translation
(EAMT), pages 28–35, Barcelona, May. EAMT.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proc. of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 173–180, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Wikipedia. 2013. LIX. http://en.wikipedia.org/
wiki/LIX.
David Graff Denise DiPersio ˆAngelo Mendonc¸a,
Daniel Jaquette. 2011. Spanish Gigaword third edi-
tion, Linguistic Data Consortium.
</reference>
<page confidence="0.999127">
496
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.704708">RTM-DCU: Referential Translation Machines for Semantic Similarity</title>
<affiliation confidence="0.945129">Centre for Global Intelligent School of Dublin City University, Dublin,</affiliation>
<email confidence="0.970835">ebicici@computing.dcu.ie</email>
<author confidence="0.983197">Andy</author>
<affiliation confidence="0.995385666666667">Centre for Global Intelligent School of Dublin City University, Dublin,</affiliation>
<email confidence="0.98351">away@computing.dcu.ie</email>
<abstract confidence="0.926600282407407">We use referential translation machines (RTMs) for predicting the semantic similarity of text. RTMs are a computational model for identifying the translation acts between any two data sets with respect to interpretants selected in the same domain, which are effective when making monolingual and bilingual similarity judgments. RTMs judge the quality or the semantic similarity of text by using retrieved relevant training data as interpretants for reaching shared semantics. We derive features measuring the closeness of the test sentences to the training data via interpretants, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. RTMs provide a language independent approach to all similarity tasks and achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) and good results in semantic relatedness and entailment (Task 1) and multilingual semantic textual similarity (STS) (Task 10). RTMs remove the need to access any task or domain specific information or resource. 1 Semantic Similarity Judgments We introduce a fully automated judge for semantic similarity that performs well in three semantic similarity tasks at SemEval-2014, Semantic Evaluation Exercises - International Workshop on Semantic Evaluation (Nakov and Zesch, 2014). RTMs provide a language independent solution for the semantic textual similarity (STS) task (Task 10) (Agirre et al., 2014), achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) (Jurgens et al., 2014), and achieve good results in the semantic relatedness and entailment task (Task 1) (Marelli et al., 2014a). Referential translation machine (Section 2) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an inthe signs used to refer to the objects 2008). Each RTM model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. RTMs present an accurate and language independent solution for making semantic similarity judgments. We describe the tasks we participated below. Section 2 describes the RTM model and the features used. Section 3 presents the training and test results we obtain on the three tasks we competed and the last section concludes. Task 1 Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Entailment (SRE) (Marelli et al., 2014a): Given two sentences, produce a relatedness score indicating the extent to which the sentences express a related meaning: a in the range We model the problem as a translation performance prediction task where one possible interis obtained by translating source translate, S) to target translation, T). Since linguistic processing can reveal deeper similarity relationships, we also look at the translation task at different granularities of information: plain 487 of the 8th International Workshop on Semantic Evaluation (SemEval pages Dublin, Ireland, August 23-24, 2014. text (R for regular) and after lemmatization (L). We lowercase all text. Task 3 Cross-Level Semantic Similarity (CLSS) (Jurgens et al., 2014): Given two text from different levels, produce a semantic similarity rating: a numin the range CLSS task targets semantic similarity comparisons between text having different levels of granularity and we address the following level crossings: paragraph to sentence, sentence to phrase, and phrase to word. We model the problem as a translation performance prediction task among text from different levels. Task 10 Multilingual Semantic Textual Similarity (MSTS) (Agirre et al., 2014) two sentences the same language, quantify the degree of similara number in the range MSTS task addresses the problem in English Spanish (score range is We model the problem as a translation performance prediction between 2 Referential Translation Machine (RTM) Referential translation machines provide a computational model for quality and semantic similarity judgments in monolingual and bilingual settings retrieval of relevant training data and Yuret, 2014) as interpretants for shared semantics 2008). RTMs are a language independent approach and achieve top performance when predicting the quality of 2013; and Way, 2014) and when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014), and good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014a), as an automated student answer and van Genabith, 2013b), and when judging the semantic similarity of senand van Genabith, 2013a; Agirre et al., 2014). We improve the RTM models by: • using a parameterized, fast implementation of FDA, FDA5, and our Parallel FDA5 inselection model et al., 2014), • better modeling of the language in which 1: Translation Machine Training set test set and learning model M. Features of Predictions of similarity scores on test Z M similarity judgments are made with improved optimization and selection of the LM data, • using a general domain corpus to select interpretants from, • increased feature set for also modeling the structural properties of sentences, • extended learning models. We use the Parallel FDA5 (Feature Decay Algorithms) instance selection model for selecting the et al., 2014; and Yuret, 2014) this year, which allows efficient parameterization, optimization, and implementation of FDA, and build an MTPP model (Section 2.1). We view that acts of translation are ubiquitously used during communication: Every act of communication is an act 2012). Translation need not be between different languages and paraphrasing or communication also contain acts of translation. When creating sentences, we use our background knowledge and translate information content according to the current context. The inputs to the RTM algorithm Algorithm 1 a training set a test set some preferably in the same domain as the training and test sets, and a learning model. Step 1 the interpretants, relevant to both the and test data. Steps 2 and 3 use map a new space where similarities between translation acts can be derived more easily. Step 4 trains a learning model M over the features, and Step 5 obtains the predictions. Figure 1 depicts the RTM. 488 Figure 1: RTM depiction. Our encouraging results in the semantic similarity tasks increase our understanding of the acts of translation we ubiquitously use when communicating and how they can be used to predict the semantic similarity of text. RTM and MTPP models are not data or language specific and their modeling power and good performance are applicable in different domains and tasks. RTM expands the applicability of MTPP by making it feasible when making monolingual quality and similarity judgments and it enhances the computational scalability by building models over smaller and more relevant set of interpretants. 2.1 The Machine Translation Performance Predictor (MTPP) et al., 2013) is a state-of-the-art and top performing machine translation performance predictor, which uses machine learning models over features measuring how well the test set matches the training set to predict the quality of a translation without using a reference translation. MTPP measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. 2.2 MTPP Features for Translation Acts MTPP feature functions use statistics involving the training set and the test sentences to determine their closeness. Since they are language independent, MTPP allows quality estimation to be performed extrinsically. MTPP uses n-gram features defined over text or common cover link (CCL) (Seginer, 2007) structures as the basic units of information over which similarity calculations made. Unsupervised parsing with CCL extracts links from base words to head words, representing the grammatical information instantiated in the training and test data. We extend the MTPP model we used last 2013) in its learning module and the features included. Categories for the features (S for source, T for target) used are listed below where the number of features are given in brackets S and T, and the detailed descriptions some of the features are presented in et al., 2013). The number of features for each task differs since we perform an initial feature selection step on the tree structural features (Section 2.3). number of features are in the range Coverage the degree to the test features are found in the trainset for both S and T Perplexity the fluency of the sentences according to language models We use both forward and back- LM features for S and T. TreeF base features and up to 100 selected features of T among parse tree structures (Section 2.3). Retrieval Closeness the degree to which sentences close to the test set found in the selected training set, using and Yuret, 2011a) and BLEU, 2011), and tf-idf cosine similarity metrics. IBM2 Alignment Features Calculates the sum of the entropy of the distribution of alignment probabilities for S for p s and t are tokens) and T, their average for S and the number of entries with p entropy of the word alignment between S and T and its average, and alignment and its value terms of bits per word. We also compute word alignment percentage as in (Camargo de Souza et al., 2013) and potential WER, PER scores for S and T. IBM1 Translation Probability Calculates the translation probability of test sentences using the selected training set, et al., 1993). Feature Vector Similarity similarities between vector representations. 489 Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP). 2 1 1 1 1 3 1 3 4 5 1 7 15 CCL depthB avg R/Lavg R/L 24.0 9.0 0.375 2.1429 3.401 1 8 2 10 1 1 1 2 1 1 1 1 1 13 Entropy the distributional similarity of test sentences to the training set top N retrieved sentences et al., 2013). Length the number of words and characters for S and T and their average token lengths and their ratios. Diversity the diversity of co-occurring features in the training et al., 2013). Synthetic Translation Performance Calculates translation scores achievable according to the n-gram coverage. Character cosine between character n-grams (for n=2,3,4,5,6) obtained for S and T (B¨ar et al., 2012). Minimum Bayes Retrieval Risk Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. Sentence Translation Performance Calculates translation scores obtained acto BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or and Yuret, 2011b) for q. LIX the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for and T. 2.3 Bracketing Tree Structural Features We use the parse tree outputs obtained by CCL to derive features based on the bracketing struc- We derive based on the geometric properties of the parse trees: number of brackets used (numB), depth (depthB), average depth (avg where A is the number of words, C is words longer than 6 characters, B is words that start or end with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012). depthB), number of brackets on the right branches the number of brackets on the left (R/L) average right to left branching over all internal tree nodes (avg R/L). The ratio of the number of right to left branches shows the degree to which the sentence is right branching or not. Additionally, we capture the different types of branching present in a given parse tree identified by the number of nodes in each of its children. Table 1 depicts the parsing output obtained by for the following sentence from WSJ23 Many fund managers argue that now ’s the time to buy. We use Tregex (Levy and Andrew, 2006) for visualizing the output parse trees presented on the left. The bracketing structure statistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. 3 SemEval-14 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2014 with the RTM-DCU team name. The interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and the LM corpora provided by LDC for (Parker et al., 2011) and Spanish ( 2011) We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the lemmatized corpora for the SRE task. For each RTM nodes with uneven number of children, the nodes in the odd child contribute to the right branches. Street Journal (WSJ) corpus section 23, distributed with Penn Treebank version 3 (Marcus et al., 1993). Gigaword 5th, Spanish Gigaword 3rd edition. 490 model, we extract the features both on the training set and the test set. The number of instances we select for the interpretants in each task is given in Table 2. Task Setting Train LM Task 1, SRE English 770 10770 Task 3, CLSS Par2S 302 2802 Task 3, CLSS S2Phrase 202 2702 Task 3, CLSS Phrase2W 102 2602 Task 10, MSTS English 504 8002 Task 10, MSTS English OnWN 504 8004 Task 10, MSTS Spanish 502 8002 2: Number of sentences in thousands) selected for each task. We use ridge regression (RR), support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch¨olkopf, 2004), and extremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. TREE is an ensemble learning method over randomized decision trees. These models learn a regression function using the features to estimate a numerical target value. We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), of which are described in et al., 2013). We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More detailed descriptions of the optiprocesses are given in et al., 2013; et al., 2014). We optimize the learning paby selecting to the standard deviof the noise in the training set 2013) the optimal value for shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). At testing time, the predictions are bounded to obtain scores in the corresponding ranges. We obtain the confidence scores using support vector classification (SVC). 3.1 Task 1: Semantic Relatedness and Entailment MSTS contains sentence pairs from the SICK (Sentences Involving Compositional Knowledge) data set (Marelli et al., 2014b), which contain sentence pairs that contain rich lexical, syntactic and semantic phenomena. Official evaluation metric in SRE is the Pearson’s correlation score, which is used to select the top systems on the training set. SRE task allows the submission of 5 entries. We present the performance of the top 5 individual RTM models on the training set in Table 3. is entailment accuracy, Pearson’s corre- Spearman’s correlation, MSE is mean squared error, MAE is mean absolute error, and RAE is relative absolute error. L uses the lemmatized corpora and R uses the true-cased corpora corresponding to regular. R+L correspond to the perspective using the features from both R and L, which doubles the number of features. We compute the entailment by SVC. Data Model ACC MSE MAE RAE L SVR 67.52 .7372 .6918 .6946 .5511 .6856 L PLS-SVR 67.04 .7539 .6927 .6763 .5369 .668 R+L PLS-SVR 66.76 .75 .6879 .6815 .539 .6705 R+L SVR 66.66 .7295 .6814 .7027 .5591 .6956 L PLS-RR 66.56 .7247 .6765 .7054 .5687 .7075 Table 3: SRE training results of the top 5 RTM systems selected. SRE challenge results on the test set are given in Table 4. The setting R using PLS-SVR learning becomes the 8th out of 17 submissions when predicting the semantic relatedness and 17th out of 18 submissions when predicting the entailment.</abstract>
<note confidence="0.945722272727273">Data Model ACC RMSE MAE RAE R PLS-SVR 67.20 .7639 .6877 .655 .5246 .6645 R+L PLS-SVR 67.65 .7688 .6918 .6492 .5194 .658 L SVR 67.65 .7559 .6887 .664 .531 .6726 R+L SVR 67.44 .7625 .6899 .6555 .5251 .6651 R PLS-SVR 66.61 .7570 .6683 .6637 .5324 .6744 Table 4: RTM-DCU test results on the SRE task. Model RMSE MAE RAE Par2S TREE 0.8013 0.8345 0.6277 0.5083 Par2S PLS-TREE 0.7737 0.8824 0.673 0.5449 Par2S SVR 0.7718 0.8863 0.6791 0.5499 S2Phrase TREE 0.6756 0.9887 0.7746 0.6665 S2Phrase PLS-TREE 0.6119 1.0616 0.8582 0.7384 S2Phrase SVR 0.6059 1.0662 0.8668 0.7458 Phrase2W TREE 0.201 1.3275 1.1353 0.9706 Phrase2W RR 0.1255 1.3463 1.1594 0.9912 Phrase2W SVR 0.0847 1.3548 1.1663 0.9972 Table 5: CLSS training results of the top 3 RTM systems for each subtask. Levels correspond to paragraph to sentence (Par2S), sentence to phrase (S2Phrase), and phrase to word (Phrase2W). 491</note>
<abstract confidence="0.954582533333333">3.2 Task 3: Cross-Level Semantic Similarity CLSS contains sentence pairs from different genres including text from newswire, travel, reviews, metaphoric text, community question answering sites, idiomatic text, descriptions, lexicographic text, and search. Official evaluation metric in CLSS is the sum of the Pearson’s correlation for different levels CLSS task allows the submission of 3 entries per subtask. We present the performance of the top 3 individual RTM models on the training set in Table 5. RMSE is the root mean squared error. As the compared text size decrease, the performance decrease since it can become harder and more ambiguous to find the similarity using less context. RTM-DCU results on the</abstract>
<note confidence="0.790926166666667">CLSS challenge test set are provided in Table 6. Model RMSE MAE RAE Par2S TREE .8445 .7417 .5622 .4579 Par2S PLS-TREE .7847 .853 .6456 .5258 Par2S SVR .7858 .8428 .6539 .5325 S2Phrase TREE .75 .8827 .7053 .6255 S2Phrase PLS-TREE .6979 .9491 .7781 .69 S2Phrase SVR .6631 .9835 .7992 .7088 Phrase2W TREE .3053 1.3351 1.14 .9488 Phrase2W RR .2207 1.3644 1.1574 .9633 Phrase2W SVR .1712 1.3792 1.1792 .9815 Table 6: RTM-DCU test results on CLSS for the</note>
<abstract confidence="0.7382096">top 3 RTM systems for each subtask. Table 7 lists the results along with their ranks Spearman’s correlation, out of CHECK submissions. The baseline in Table 7 is normalized longest common substring (LCS) in the range Top individual rank row lists the ranks in each subtask. We present the results for both our official and late (about 1 day) submissions including word to sense (W2S) re- RTM-DCU is able to obtain the top result in Par2S in the CLSS task. 3.3 Task 10: Multilingual Semantic Textual Similarity MSTS contains sentence pairs from different domains: sense definitions from semantic lexical resources such as OnWN (from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995)) and FNWN (from FrameNet (Baker et al., 1998) and WordNet), news headlines, image descriptions, news title tweet comments, deft forum and news, advantage to participants submitting to all levels. results for the late submission is obtained from the LCS baseline to calculate the ranks. Par2S S2Phrase Phrase2W W2S Rank LCS 0.527 0.562 0.165 0.109 25 0.780 0.677 0.208 14 Official 0.747 0.588 0.164 19 0.786 0.666 0.171 18 0.845 0.750 0.305 0.109 6 Late 0.785 0.698 0.221 0.109 13 0.786 0.663 0.171 0.109 17 Top Rank 1 5 3 Par2S S2Phrase Phrase2W W2S Rank LCS 0.527 0.562 0.165 0.13 23 0.780 0.677 0.208 17 Official 0.747 0.588 0.164 22 0.786 0.666 0.171 18 0.829 0.734 0.295 0.13 8 Late 0.778 0.687 0.219 0.13 15 0.778 0.667 0.166 0.13 16 Top Rank 1 5 5 Table 7: RTM-DCU test results on CLSS. paraphrases. Official evaluation metric in MSTS is the Pearson’s correlation score. MSTS task provides 7622 training instances and 3750 test instances. For the OnWN domain, 1316 training instances are available and therefore, we build a separate RTM model for this domain. Separate modeling of the OnWN dataset results with higher confidence scores on the test instances than we would obtain using the overall model to predict. MSTS task allows the submission of 3 entries per subtask. We present the performance of the top 3 individual RTM models on the training set in Table 8.</abstract>
<author confidence="0.688575">Model MAE RAE</author>
<phone confidence="0.817159222222222">TREE 0.6931 1.0627 0.8058 0.6649 PLS-TREE 0.6875 1.0753 0.8038 0.6632 PLS-SVR 0.6884 1.0698 0.8157 0.6730 TREE 0.8094 0.9295 0.694 0.5245 PLS-TREE 0.7953 0.9604 0.7203 0.5444 PLS-SVR 0.7888 0.9779 0.7234 0.5468 TREE 0.6513 0.7341 0.5904 0.7508 PLS-TREE 0.4157 0.9007 0.7108 0.9039 PLS-SVR 0.4239 1.1427 0.8293 1.0545</phone>
<abstract confidence="0.689120714285714">Table 8: MSTS training results on the English, English OnWN, and Spanish tasks. RTM results on the MSTS challenge test set are provided in Table 9 along with the RTM results in 2013 and van Genabith, 2013a). Table 10 and Table 11 lists the official results on English and Spanish tasks with rankings calculated to weighted which weights according to the number of instances in each domain. RTM-DCU is able to become 10th in the OnWN domain and 19th overall out of 38 submissions in MSTS English and 18th out of 22 submissions in English OnWN Spanish 492 English Spanish STS 2013 English Model RMSE MAE RAE TREE .4341 1.4306 1.1609 1.0908 deft-forum PLS-TREE .3965 1.4115 1.1472 1.078</abstract>
<phone confidence="0.6746742">PLS-SVR .3078 1.6277 1.3482 1.2669 TREE .6974 1.1469 .9032 .8716 deft-news PLS-TREE .6811 1.1229 .8769 .8462 PLS-SVR .5562 1.2803 .9835 .9491 TREE .6199 1.1495 .9254 .7845</phone>
<note confidence="0.7134706">headlines PLS-TREE .6125 1.1552 .9314 .7896 PLS-SVR .6301 1.1041 .8807 .7467 TREE .6995 1.2034 .9499 .7395 images PLS-TREE .6656 1.2298 .9692 .7545 PLS-SVR .6474 1.4406 1.1057 .8607 TREE .8058 1.3122 1.0028 .5585 OnWN PLS-TREE .7992 1.2997 .9815 .5467 PLS-SVR .8004 1.2913 .9449 .5263 TREE .6882 .9869 .831 .8093 tweet-news PLS-TREE .6691 1.0101 .8433 .8213 PLS-SVR .5531 1.0633 .8653 .8427 TREE .7 1.5185 1.351 1.4141 News PLS-TREE .6253 1.6523 1.4464 1.514 PLS-SVR .6411 1.554 1.3196 1.3813 TREE .4216 1.5433 1.298 1.3579 Wikipedia PLS-TREE .3689 1.6655 1.4015 1.4662 PLS-SVR .4242 1.5998 1.3141 1.3748 L+S SVR .6552 1.5649 1.2763 1.0231 headlines L+P+S SVR .651 1.4845 1.1984 .9607 L+P+S SVR TL .6385 1.4878 1.2008 .9626 L+S SVR .6943 1.7065 1.3545 .8255 OnWN L+P+S SVR .6971 1.6737 1.333 .8124 L+P+S SVR TL .6755 1.7124 1.3598 .8287 L+S SVR .3005 .8833 .6886 1.6132 SMT L+P+S SVR .2861 .8810 .6821 1.598 L+P+S SVR TL .3098 .8635 .6547 1.5339 L+S SVR .2016 1.2957 1.0604 1.2633 FNWN L+P+S SVR .118 1.4369 1.1866 1.4136 L+P+S SVR TL .1823 1.3245 1.0962 1.3059 Table 9: RTM-DCU test results on MSTS for the</note>
<abstract confidence="0.981102604166667">top 3 RTM systems for each subtask as well as results in STS 2013 and van Genabith, 2013a). MSTS Spanish. The performance difference between MSTS English and MSTS Spanish may be due to the fewer training data available for the MSTS Spanish task, which may be decreasing the performance of our supervised learning approach. 3.4 RTMs Across Tasks and Years We compare the difficulty of tasks according to the RAE levels achieved. RAE measures the error relative to the error when predicting the actual mean. A high RAE is an indicator that the task is hard. In Table 12, we list the RAE obtained for different tasks and subtasks, also listing RTM results in 2013 and van Genabith, 2013a) and results and Way, 2014) on the quality estimation task (QET) (Bojar et al., 2014) where effort (PEE), human-targeted transla- Model Wikipedia News Rank TREE 0.4216 0.7000 0.5878 18 PLS-TREE 0.3689 0.6253 0.5219 20 PLS-SVR 0.4242 0.6411 0.5537 19 Table 11: RTM-DCU test results on MSTS Spanish task. Rankings are calculated according to the weighted Pearson’s correlation. tion edit rate (HTER), or post-editing time (PET) of translations are predicted. The best results are obtained for the CLSS Par2S subtask, which may be due to the larger contextual information that paragraphs can provide for the RTM models. For the SRE task, we can only reduce the error with respect to knowing predicting the mean by about Prediction of bilingual similarity as in quality estimation of translation can be expected to be harder and RTMs achieve state-of-the-art performance in this task as and Way, 2014). 4 Conclusion Referential translation machines provide a clean and intuitive computational model for automatically measuring semantic similarity by measuring the acts of translation involved and achieve to be the top on some semantic similarity tasks at SemEval-2014. RTMs make quality and semantic similarity judgments possible based on the retrieval of relevant training data as interpretants for reaching shared semantics.</abstract>
<note confidence="0.842307333333333">Acknowledgments This work is supported in part by SFI (07/CE/I1142) as part of the CNGL Centre</note>
<title confidence="0.737934">Global Intelligent Content</title>
<abstract confidence="0.585192761904762">at Dublin City University, in part by SFI (13/TIDA/I2740) for the project “Monolingual and Bilingual Text Quality Judgments with Translation Performance Prediction” and in part by the European Commission through the QT- LaunchPad FP7 project (No: 296347). We also thank the SFI/HEA Irish Centre for High- End Computing (ICHEC) for the provision of computational facilities and support. 493 Model deft-forum deft-news headlines images OnWN tweet-news Rank TREE .4341 .6974 .6199 .6995 .8058 .6882 .6706 20 PLS-TREE .3965 .6811 .6125 .6656 .7992 .6691 .6513 23 PLS-SVR .3078 .5562 .6301 .6475 .8004 .5531 .6076 27 Top Rank 17 16 25 26 16 13 TREE .4181 .6846 .6216 .6981 .8331 .6870 .6729 19 PLS-TREE .3831 .6739 .6094 .6629 .8260 .6691 .6534 23 PLS-SVR .2731 .5526 .6330 .6441 .8246 .5683 .6110 26 Top Rank 18 18 23 27 10 14 Table 10: RTM-DCU test results with ranks on MSTS English task.</abstract>
<author confidence="0.511299">Task Subtask Domain Model RAE</author>
<pubnum confidence="0.296971">SRE English SICK R PLS-SVR R+L PLS-SVR L SVR .6645 .6580 .6726 .6651 .6744</pubnum>
<title confidence="0.825313">R+L SVR</title>
<author confidence="0.485211">R PLS-SVR</author>
<address confidence="0.8013468">CLSS Par2S S2Phrase Phrase2W Mixed TREE TREE TREE .4579 .6255 .9488 MSTS English deft-forum deft-news headlines images OnWN tweet-news PLS-TREE PLS-TREE PLS-SVR TREE PLS-SVR TREE 1.078 .8462 .7467 .7395 .5263 .8093 Spanish News Wikipedia PLS-SVR TREE 1.3813</address>
<date confidence="0.587078">1.3579</date>
<note confidence="0.782750055555556">STS 2013 English headlines OnWN SMT FNWN L+P+S SVR L+P+S SVR L+P+S SVR TL L+S SVR .9607 .8124 1.5339 1.2633 QET PEE Spanish-English Spanish-English English-German English-German English-Spanish English-Spanish German-English German-English Europarl Europarl Europarl Europarl Europarl Europarl Europarl Euruparl FS-RR PLS-RR PLS-TREE TREE TREE PLS-TREE RR PLS-RR .9000 .9409 .8883 .8602 1.0983 1.0794 .8204 .8437 QET HTER English-Spanish English-Spanish Europarl Europarl SVR TREE .8532 .8931 QET PET English-Spanish English-Spanish Europarl Europarl SVR RR .7223 .7536 Table 12: Best RTM-DCU RAE test results for different tasks and subtasks as well as STS 2013 reand van Genabith, 2013a) and results from quality estimation task of translation (Bojar et 2014; and Way, 2014). With Conf. References Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 Task 10: Multilinsemantic textual similarity. In of the 8th International Workshop on Semantic Evaluation Dublin, Ireland, August. Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The berkeley framenet project. In of the 36th Annual Meeting of the Association for Compu- Linguistics and 17th International Conferon Computational Linguistics - Volume ACL ’98, pages 86–90, Stroudsburg, PA, USA. Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content measures. In 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proc. of the main conference and the shared task, and Volume 2: Proc. of the Sixth International Workshop on Semantic Evaluation (Sepages 435–440, Montr´eal, Canada, 7- 8 June. Association for Computational Linguistics. and Josef van Genabith. 2013a. CNGL- 494 CORE: Referential translation machines for measursemantic similarity. In 2013: The Second Joint Conference on Lexical and Computational Se- Atlanta, Georgia, USA, 13-14 June. Association for Computational Linguistics. and Josef van Genabith. 2013b. CNGL: Grading student answers by acts of translation. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics and Proc. of the Seventh International Workshop on Semantic Eval- (SemEval Atlanta, Georgia, USA, 14- 15 June. Association for Computational Linguistics. and Andy Way. 2014. Referential translation machines for predicting translation quality. In Proc. of the Ninth Workshop on Statistical Machine Baltimore, USA, June. Association for Computational Linguistics. and Deniz Yuret. 2011a. Instance se-</note>
<abstract confidence="0.98002775">lection for machine translation using feature decay In of the Sixth Workshop on Sta- Machine pages 272–283, Edinburgh, Scotland, July. Association for Computational Linguistics. and Deniz Yuret. 2011b. RegMT system for machine translation, system combination, evaluation. In of the Sixth Workshop on Machine pages 323–329, Edinburgh, Scotland, July. Association for Computational Linguistics. and Deniz Yuret. 2014. Optimizing instance selection for statistical machine translation feature decay algorithms. Transactions On Audio, Speech, and Language Processing Declan Groves, and Josef van Genabith. 2013. Predicting sentence translation quality using and language independent features. Ma- 27:171–192, December. Qun Liu, and Andy Way. 2014. Parallel FDA5 for fast deployment of accurate statistical translation systems. In of the Ninth on Statistical Machine Baltimore, USA, June. Association for Computational Linguistics. 2011. Regression Model of Machine Ph.D. thesis, University. Supervisor: Deniz Yuret. 2013. Referential translation machines quality estimation. In of the Eighth Workon Statistical Machine Sofia, Bulgaria, August. Association for Computational Linguistics. 2008. Consensus ontologies in socially multiagent systems. of Multiaand Grid Hugo Bj¨ornsson. 1968. Liber. Bliss. 2012. Comedy is translation, February. chris bliss comedy is translation.html.</abstract>
<author confidence="0.8087305">Ondrej Bojar</author>
<author confidence="0.8087305">Christian Buck</author>
<author confidence="0.8087305">Christian Federmann</author>
<author confidence="0.8087305">Barry Haddow</author>
<author confidence="0.8087305">Philipp Koehn</author>
<author confidence="0.8087305">Matous Mach´acek</author>
<address confidence="0.633615">Christof Monz, Pavel Pecina, Matt Post, Herve</address>
<abstract confidence="0.315020909090909">Saint-Amand, Radu Soricut, and Lucia Specia. 2014. Findings of the 2014 workshop on statistimachine translation. In of the Ninth Workon Statistical Machine Balrimore, USA, June. Association for Computational Linguistics. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The mathematics of statistical machine translation: estimation. 19(2):263–311, June.</abstract>
<author confidence="0.6708055">FBK-</author>
<note confidence="0.640849793103448">UEdin participation to the WMT13 quality estimashared task. In of the Eighth Workshop Statistical Machine pages 352–358, Sofia, Bulgaria, August. Association for Computational Linguistics. George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram costatistics. In of the second international conference on Human Language Technology pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Learn- 63(1):3–42. Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. 2002. Gene selection for cancer using support vector machines. Ma- 46(1-3):389–422. Hagstr¨om. 2012. Swedish readability calculator. https://github.com/keha76/Swedish- Readability-Calculator. David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. SemEval-2014 Task 3: semantic similarity. In of the 8th International Workshop on Semantic Evaluation Dublin, Ireland, August. Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon: tools for querying and manipulating tree structures. In of the fifth international</note>
<title confidence="0.662175">on Language Resources and</title>
<author confidence="0.603186">Building a large anno-</author>
<affiliation confidence="0.179003">corpus of english: the penn treebank.</affiliation>
<address confidence="0.361999">19(2):313–330, June. 495</address>
<author confidence="0.9405905">Marco Marelli</author>
<author confidence="0.9405905">Stefano Menini</author>
<author confidence="0.9405905">Marco Baroni</author>
<author confidence="0.9405905">Luisa Bentivogli</author>
<author confidence="0.9405905">Raffaella Bernardi</author>
<author confidence="0.9405905">Roberto Zam-</author>
<abstract confidence="0.856989">parelli. 2014a. SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and texentailment. In of the 8th International</abstract>
<affiliation confidence="0.629317">on Semantic Evaluation</affiliation>
<address confidence="0.82344">Dublin, Ireland, August.</address>
<author confidence="0.824578">Marco Marelli</author>
<author confidence="0.824578">Stefano Menini</author>
<author confidence="0.824578">Marco Baroni</author>
<author confidence="0.824578">Luisa Bentivogli</author>
<author confidence="0.824578">Raffaella Bernardi</author>
<author confidence="0.824578">Roberto Zam-</author>
<abstract confidence="0.6668445">parelli. 2014b. A sick cure for the evaluation of compositional distributional semantic models. In</abstract>
<note confidence="0.759024727272727">of LREC Reykjavik, Iceland. George A. Miller. 1995. WordNet: A lexical for English. of the 38(11):39–41, November. Preslav Nakov and Torsten Zesch, editors. 2014. Proc. of SemEval-2014 Semantic Evaluation Exercises - International Workshop on Semantic Evalua- Dublin, Ireland, 23-24 August. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for autoevaluation of machine translation. In of 40th Annual Meeting of the Association for Compages 311–318, Philadelphia, Pennsylvania, USA, July. Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword fifth edition, Linguistic Data Consortium. Sameer S. Pradhan, Eduard H. Hovy, Mitchell P. Marcus, Martha Palmer, Lance A. Ramshaw, and Ralph M. Weischedel. 2007. Ontonotes: a unified semantic representation. J. Semantic 1(4):405–419. Seginer. 2007. Syntactic Ph.D. thesis, Universiteit van Amsterdam. Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tuon support vector regression. and 14(3):199–222, August. A. J. Smola, N. Murata, B. Sch¨olkopf, and K.-R. 1998. Asymptotically optimal choice of loss for support vector machines. In L. Niklasson, Boden, and T. Ziemke, editors, of the Inter- Conference on Artificial Neural Perspectives in Neural Computing, pages 105–110, Berlin. Springer. Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation In of the 13th Annual Conference of the European Association for Machine Translation pages 28–35, Barcelona, May. EAMT. Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Vol- NAACL ’03, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics. Wikipedia. 2013. LIX. http://en.wikipedia.org/ wiki/LIX. Graff Denise DiPersio Daniel Jaquette. 2011. Spanish Gigaword third edition, Linguistic Data Consortium. 496</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 10: Multilingual semantic textual similarity. In Proc. of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="1826" citStr="Agirre et al., 2014" startWordPosition="264" endWordPosition="267">imilarity (Task 3) and good results in semantic relatedness and entailment (Task 1) and multilingual semantic textual similarity (STS) (Task 10). RTMs remove the need to access any task or domain specific information or resource. 1 Semantic Similarity Judgments We introduce a fully automated judge for semantic similarity that performs well in three semantic similarity tasks at SemEval-2014, Semantic Evaluation Exercises - International Workshop on Semantic Evaluation (Nakov and Zesch, 2014). RTMs provide a language independent solution for the semantic textual similarity (STS) task (Task 10) (Agirre et al., 2014), achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) (Jurgens et al., 2014), and achieve good results in the semantic relatedness and entailment task (Task 1) (Marelli et al., 2014a). Referential translation machine (Section 2) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by p</context>
<context position="4547" citStr="Agirre et al., 2014" startWordPosition="692" endWordPosition="695">ar) and after lemmatization (L). We lowercase all text. Task 3 Cross-Level Semantic Similarity (CLSS) (Jurgens et al., 2014): Given two text from different levels, produce a semantic similarity rating: a number in the range [0, 4]. CLSS task targets semantic similarity comparisons between text having different levels of granularity and we address the following level crossings: paragraph to sentence, sentence to phrase, and phrase to word. We model the problem as a translation performance prediction task among text from different levels. Task 10 Multilingual Semantic Textual Similarity (MSTS) (Agirre et al., 2014) Given two sentences 51 and 52 in the same language, quantify the degree of similarity: a number in the range [0, 5]. MSTS task addresses the problem in English and Spanish (score range is [0, 4]). We model the problem as a translation performance prediction task between 51 and 52. 2 Referential Translation Machine (RTM) Referential translation machines provide a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bic¸ici, 2011; Bic¸ici and Yuret, 2014) as interpretants for reaching shared semantics </context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 Task 10: Multilingual semantic textual similarity. In Proc. of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL ’98,</booktitle>
<pages>86--90</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21228" citStr="Baker et al., 1998" startWordPosition="3512" endWordPosition="3515">aseline in Table 7 is normalized longest common substring (LCS) scaled in the range [0, 4]. Top individual rank row lists the ranks in each subtask. We present the results for both our official and late (about 1 day) submissions including word to sense (W2S) results 6. RTM-DCU is able to obtain the top result in Par2S in the CLSS task. 3.3 Task 10: Multilingual Semantic Textual Similarity MSTS contains sentence pairs from different domains: sense definitions from semantic lexical resources such as OnWN (from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995)) and FNWN (from FrameNet (Baker et al., 1998) and WordNet), news headlines, image descriptions, news title tweet comments, deft forum and news, 5Giving advantage to participants submitting to all levels. 6W2S results for the late submission is obtained from the LCS baseline to calculate the ranks. rp Par2S S2Phrase Phrase2W W2S Rank LCS 0.527 0.562 0.165 0.109 25 0.780 0.677 0.208 14 Official 0.747 0.588 0.164 19 0.786 0.666 0.171 18 0.845 0.750 0.305 0.109 6 Late 0.785 0.698 0.221 0.109 13 0.786 0.663 0.171 0.109 17 Top Rank 1 5 3 rs Par2S S2Phrase Phrase2W W2S Rank LCS 0.527 0.562 0.165 0.13 23 0.780 0.677 0.208 17 Official 0.747 0.588</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL ’98, pages 86–90, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proc. of the main conference and the shared task, and Volume 2: Proc. of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>435--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proc. of the main conference and the shared task, and Volume 2: Proc. of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 435–440, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Josef van Genabith</author>
</authors>
<title>CNGLCORE: Referential translation machines for measuring semantic similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<marker>Bic¸ici, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici and Josef van Genabith. 2013a. CNGLCORE: Referential translation machines for measuring semantic similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics, Atlanta, Georgia, USA, 13-14 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Josef van Genabith</author>
</authors>
<title>CNGL: Grading student answers by acts of translation.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics and Proc. of the Seventh International Workshop on Semantic Evaluation (SemEval 2013),</booktitle>
<pages>14--15</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<marker>Bic¸ici, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici and Josef van Genabith. 2013b. CNGL: Grading student answers by acts of translation. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics and Proc. of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), Atlanta, Georgia, USA, 14-15 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Andy Way</author>
</authors>
<title>Referential translation machines for predicting translation quality.</title>
<date>2014</date>
<booktitle>In Proc. of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA,</location>
<marker>Bic¸ici, Way, 2014</marker>
<rawString>Ergun Bic¸ici and Andy Way. 2014. Referential translation machines for predicting translation quality. In Proc. of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>Instance selection for machine translation using feature decay algorithms.</title>
<date>2011</date>
<booktitle>In Proc. of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>272--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Bic¸ici, Yuret, 2011</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2011a. Instance selection for machine translation using feature decay algorithms. In Proc. of the Sixth Workshop on Statistical Machine Translation, pages 272–283, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>RegMT system for machine translation, system combination, and evaluation.</title>
<date>2011</date>
<booktitle>In Proc. of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>323--329</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Bic¸ici, Yuret, 2011</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2011b. RegMT system for machine translation, system combination, and evaluation. In Proc. of the Sixth Workshop on Statistical Machine Translation, pages 323–329, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>Optimizing instance selection for statistical machine translation with feature decay algorithms.</title>
<date>2014</date>
<journal>IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP).</journal>
<marker>Bic¸ici, Yuret, 2014</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2014. Optimizing instance selection for statistical machine translation with feature decay algorithms. IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Declan Groves</author>
<author>Josef van Genabith</author>
</authors>
<title>Predicting sentence translation quality using extrinsic and language independent features.</title>
<date>2013</date>
<journal>Machine Translation,</journal>
<pages>27--171</pages>
<marker>Bic¸ici, Groves, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici, Declan Groves, and Josef van Genabith. 2013. Predicting sentence translation quality using extrinsic and language independent features. Machine Translation, 27:171–192, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Qun Liu</author>
<author>Andy Way</author>
</authors>
<title>Parallel FDA5 for fast deployment of accurate statistical machine translation systems.</title>
<date>2014</date>
<booktitle>In Proc. of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA,</location>
<marker>Bic¸ici, Liu, Way, 2014</marker>
<rawString>Ergun Bic¸ici, Qun Liu, and Andy Way. 2014. Parallel FDA5 for fast deployment of accurate statistical machine translation systems. In Proc. of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>The Regression Model of Machine Translation.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Koc¸ University. Supervisor: Deniz Yuret.</institution>
<marker>Bic¸ici, 2011</marker>
<rawString>Ergun Bic¸ici. 2011. The Regression Model of Machine Translation. Ph.D. thesis, Koc¸ University. Supervisor: Deniz Yuret.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>Referential translation machines for quality estimation.</title>
<date>2013</date>
<booktitle>In Proc. of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria,</location>
<marker>Bic¸ici, 2013</marker>
<rawString>Ergun Bic¸ici. 2013. Referential translation machines for quality estimation. In Proc. of the Eighth Workshop on Statistical Machine Translation, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>Consensus ontologies in socially interacting multiagent systems.</title>
<date>2008</date>
<journal>Journal of Multiagent and Grid Systems.</journal>
<marker>Bic¸ici, 2008</marker>
<rawString>Ergun Bic¸ici. 2008. Consensus ontologies in socially interacting multiagent systems. Journal of Multiagent and Grid Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Hugo Bj¨ornsson</author>
</authors>
<date>1968</date>
<note>L¨asbarhet. Liber.</note>
<marker>Bj¨ornsson, 1968</marker>
<rawString>Carl Hugo Bj¨ornsson. 1968. L¨asbarhet. Liber.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Bliss</author>
</authors>
<title>Comedy is translation, February. http://www.ted.com/talks/ chris bliss comedy is translation.html.</title>
<date>2012</date>
<contexts>
<context position="6951" citStr="Bliss, 2012" startWordPosition="1070" endWordPosition="1071">LM data, • using a general domain corpus to select interpretants from, • increased feature set for also modeling the structural properties of sentences, • extended learning models. We use the Parallel FDA5 (Feature Decay Algorithms) instance selection model for selecting the interpretants (Bic¸ici et al., 2014; Bic¸ici and Yuret, 2014) this year, which allows efficient parameterization, optimization, and implementation of FDA, and build an MTPP model (Section 2.1). We view that acts of translation are ubiquitously used during communication: Every act of communication is an act of translation (Bliss, 2012). Translation need not be between different languages and paraphrasing or communication also contain acts of translation. When creating sentences, we use our background knowledge and translate information content according to the current context. The inputs to the RTM algorithm Algorithm 1 are a training set train, a test set test, some corpus C, preferably in the same domain as the training and test sets, and a learning model. Step 1 selects the interpretants, Z, relevant to both the training and test data. Steps 2 and 3 use Z to map train and test to a new space where similarities between tr</context>
</contexts>
<marker>Bliss, 2012</marker>
<rawString>Chris Bliss. 2012. Comedy is translation, February. http://www.ted.com/talks/ chris bliss comedy is translation.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Matous Mach´acek</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
<author>Herve Saint-Amand</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2014 workshop on statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Balrimore, USA,</location>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Mach´acek, Monz, Pecina, Post, Saint-Amand, Soricut, Specia, 2014</marker>
<rawString>Ond&amp;quot;rej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Matou&amp;quot;s Mach´a&amp;quot;cek, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, and Lucia Specia. 2014. Findings of the 2014 workshop on statistical machine translation. In Proc. of the Ninth Workshop on Statistical Machine Translation, Balrimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="11494" citStr="Brown et al., 1993" startWordPosition="1844" endWordPosition="1847">f the distribution of alignment probabilities for S (E,∈S −p log p for p = p(t|s) where s and t are tokens) and T, their average for S and T, the number of entries with p &gt; 0.2 and p &gt; 0.01, the entropy of the word alignment between S and T and its average, and word alignment log probability and its value in terms of bits per word. We also compute word alignment percentage as in (Camargo de Souza et al., 2013) and potential BLEU, F1, WER, PER scores for S and T. • IBM1 Translation Probability 14, 121: Calculates the translation probability of test sentences using the selected training set, Z (Brown et al., 1993). • Feature Vector Similarity 18, 81: Calculates similarities between vector representations. 489 Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP). 2 1 1 1 1 3 1 3 4 5 1 7 15 CCL numB depthB avg depthB R/L avg R/L 24.0 9.0 0.375 2.1429 3.401 1 8 2 10 1 1 1 2 1 1 1 1 1 13 • Entropy {2, 8}: Calculates the distributional similarity of test sentences to the training set over top N retrieved sentences (Bic¸ici et al., 2013). • Length {6, 3}: Calculates the number of words and characters for S and T and their average token lengths and their ratios. • Dive</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Guilherme Camargo de Souza</author>
<author>Christian Buck</author>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>FBKUEdin participation to the WMT13 quality estimation shared task.</title>
<date>2013</date>
<booktitle>In Proc. of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>352--358</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>de Souza, Buck, Turchi, Negri, 2013</marker>
<rawString>Jos´e Guilherme Camargo de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013. FBKUEdin participation to the WMT13 quality estimation shared task. In Proc. of the Eighth Workshop on Statistical Machine Translation, pages 352–358, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. of the second international conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="12778" citStr="Doddington, 2002" startWordPosition="2064" endWordPosition="2065">e training set (Bic¸ici et al., 2013). • Synthetic Translation Performance {3, 3}: Calculates translation scores achievable according to the n-gram coverage. • Character n-grams {5}: Calculates cosine between character n-grams (for n=2,3,4,5,6) obtained for S and T (B¨ar et al., 2012). • Minimum Bayes Retrieval Risk {0, 4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {0, 3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • LIX {1, 1}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for S and T. 1 2.3 Bracketing Tree Structural Features We use the parse tree outputs obtained by CCL to derive features based on the bracketing structure. We derive 5 statistics based on the geometric properties of the parse trees: number of brackets used (numB), depth (depthB), average depth (avg 1LIX= B + C 100, where A is the number of words, C is words longer than 6 characters, B is words that start or end with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 201</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proc. of the second international conference on Human Language Technology Research, pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Geurts</author>
<author>Damien Ernst</author>
<author>Louis Wehenkel</author>
</authors>
<title>Extremely randomized trees.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>63--1</pages>
<contexts>
<context position="15715" citStr="Geurts et al., 2006" startWordPosition="2585" endWordPosition="2588">ng set and the test set. The number of instances we select for the interpretants in each task is given in Table 2. Task Setting Train LM Task 1, SRE English 770 10770 Task 3, CLSS Par2S 302 2802 Task 3, CLSS S2Phrase 202 2702 Task 3, CLSS Phrase2W 102 2602 Task 10, MSTS English 504 8002 Task 10, MSTS English OnWN 504 8004 Task 10, MSTS Spanish 502 8002 Table 2: Number of sentences in Z (in thousands) selected for each task. We use ridge regression (RR), support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch¨olkopf, 2004), and extremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. TREE is an ensemble learning method over randomized decision trees. These models learn a regression function using the features to estimate a numerical target value. We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), both of which are described in (Bic¸ici et al., 2013). We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the par</context>
</contexts>
<marker>Geurts, Ernst, Wehenkel, 2006</marker>
<rawString>Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Machine Learning, 63(1):3–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Jason Weston</author>
<author>Stephen Barnhill</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Gene selection for cancer classification using support vector machines.</title>
<date>2002</date>
<booktitle>Machine Learning,</booktitle>
<pages>46--1</pages>
<contexts>
<context position="16034" citStr="Guyon et al., 2002" startWordPosition="2635" endWordPosition="2638">004 Task 10, MSTS Spanish 502 8002 Table 2: Number of sentences in Z (in thousands) selected for each task. We use ridge regression (RR), support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch¨olkopf, 2004), and extremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. TREE is an ensemble learning method over randomized decision trees. These models learn a regression function using the features to estimate a numerical target value. We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), both of which are described in (Bic¸ici et al., 2013). We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More detailed descriptions of the optimization processes are given in (Bic¸ici et al., 2013; Bic¸ici et al., 2014). We optimize the learning parameters by selecting e close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for e is shown to ha</context>
</contexts>
<marker>Guyon, Weston, Barnhill, Vapnik, 2002</marker>
<rawString>Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. 2002. Gene selection for cancer classification using support vector machines. Machine Learning, 46(1-3):389–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenth Hagstr¨om</author>
</authors>
<date>2012</date>
<note>Swedish readability calculator. https://github.com/keha76/SwedishReadability-Calculator.</note>
<marker>Hagstr¨om, 2012</marker>
<rawString>Kenth Hagstr¨om. 2012. Swedish readability calculator. https://github.com/keha76/SwedishReadability-Calculator.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2014 Task 3: Cross-level semantic similarity.</title>
<date>2014</date>
<booktitle>In Proc. of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="1943" citStr="Jurgens et al., 2014" startWordPosition="280" endWordPosition="283">l similarity (STS) (Task 10). RTMs remove the need to access any task or domain specific information or resource. 1 Semantic Similarity Judgments We introduce a fully automated judge for semantic similarity that performs well in three semantic similarity tasks at SemEval-2014, Semantic Evaluation Exercises - International Workshop on Semantic Evaluation (Nakov and Zesch, 2014). RTMs provide a language independent solution for the semantic textual similarity (STS) task (Task 10) (Agirre et al., 2014), achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) (Jurgens et al., 2014), and achieve good results in the semantic relatedness and entailment task (Task 1) (Marelli et al., 2014a). Referential translation machine (Section 2) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the </context>
<context position="4051" citStr="Jurgens et al., 2014" startWordPosition="613" endWordPosition="616">problem as a translation performance prediction task where one possible interpretation is obtained by translating S1 (the source to translate, S) to S2 (the target translation, T). Since linguistic processing can reveal deeper similarity relationships, we also look at the translation task at different granularities of information: plain 487 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 487–496, Dublin, Ireland, August 23-24, 2014. text (R for regular) and after lemmatization (L). We lowercase all text. Task 3 Cross-Level Semantic Similarity (CLSS) (Jurgens et al., 2014): Given two text from different levels, produce a semantic similarity rating: a number in the range [0, 4]. CLSS task targets semantic similarity comparisons between text having different levels of granularity and we address the following level crossings: paragraph to sentence, sentence to phrase, and phrase to word. We model the problem as a translation performance prediction task among text from different levels. Task 10 Multilingual Semantic Textual Similarity (MSTS) (Agirre et al., 2014) Given two sentences 51 and 52 in the same language, quantify the degree of similarity: a number in the </context>
<context position="5402" citStr="Jurgens et al., 2014" startWordPosition="821" endWordPosition="824">n performance prediction task between 51 and 52. 2 Referential Translation Machine (RTM) Referential translation machines provide a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bic¸ici, 2011; Bic¸ici and Yuret, 2014) as interpretants for reaching shared semantics (Bic¸ici, 2008). RTMs are a language independent approach and achieve top performance when predicting the quality of translations (Bic¸ici, 2013; Bic¸ici and Way, 2014) and when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014), and good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014a), as an automated student answer grader (Bic¸ici and van Genabith, 2013b), and when judging the semantic similarity of sentences (Bic¸ici and van Genabith, 2013a; Agirre et al., 2014). We improve the RTM models by: • using a parameterized, fast implementation of FDA, FDA5, and our Parallel FDA5 instance selection model (Bic¸ici et al., 2014), • better modeling of the language in which Algorithm 1: Referential Translation Machine Input: Training set train, test set test, corp</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. SemEval-2014 Task 3: Cross-level semantic similarity. In Proc. of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Galen Andrew</author>
</authors>
<title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In Proc. of the fifth international conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="13999" citStr="Levy and Andrew, 2006" startWordPosition="2284" endWordPosition="2287">12). depthB), number of brackets on the right branches over the number of brackets on the left (R/L) 2, average right to left branching over all internal tree nodes (avg R/L). The ratio of the number of right to left branches shows the degree to which the sentence is right branching or not. Additionally, we capture the different types of branching present in a given parse tree identified by the number of nodes in each of its children. Table 1 depicts the parsing output obtained by CCL for the following sentence from WSJ23 3: Many fund managers argue that now ’s the time to buy. We use Tregex (Levy and Andrew, 2006) for visualizing the output parse trees presented on the left. The bracketing structure statistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. 3 SemEval-14 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2014 with the RTM-DCU team name. The interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and the LM corpora provided by LDC for English (Parker </context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon: tools for querying and manipulating tree data structures. In Proc. of the fifth international conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="14987" citStr="Marcus et al., 1993" startWordPosition="2456" endWordPosition="2459"> we participate at SemEval-2014 with the RTM-DCU team name. The interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and the LM corpora provided by LDC for English (Parker et al., 2011) and Spanish ( ˆAngelo Mendonc¸a, 2011) 4. We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the lemmatized corpora for the SRE task. For each RTM 2For nodes with uneven number of children, the nodes in the odd child contribute to the right branches. 3Wall Street Journal (WSJ) corpus section 23, distributed with Penn Treebank version 3 (Marcus et al., 1993). 4English Gigaword 5th, Spanish Gigaword 3rd edition. 490 model, we extract the features both on the training set and the test set. The number of instances we select for the interpretants in each task is given in Table 2. Task Setting Train LM Task 1, SRE English 770 10770 Task 3, CLSS Par2S 302 2802 Task 3, CLSS S2Phrase 202 2702 Task 3, CLSS Phrase2W 102 2602 Task 10, MSTS English 504 8002 Task 10, MSTS English OnWN 504 8004 Task 10, MSTS Spanish 502 8002 Table 2: Number of sentences in Z (in thousands) selected for each task. We use ridge regression (RR), support vector regression (SVR) wi</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Comput. Linguist., 19(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proc. of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2048" citStr="Marelli et al., 2014" startWordPosition="298" endWordPosition="301">source. 1 Semantic Similarity Judgments We introduce a fully automated judge for semantic similarity that performs well in three semantic similarity tasks at SemEval-2014, Semantic Evaluation Exercises - International Workshop on Semantic Evaluation (Nakov and Zesch, 2014). RTMs provide a language independent solution for the semantic textual similarity (STS) task (Task 10) (Agirre et al., 2014), achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) (Jurgens et al., 2014), and achieve good results in the semantic relatedness and entailment task (Task 1) (Marelli et al., 2014a). Referential translation machine (Section 2) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the real objects (Bic¸ici, 2008). Each RTM model is a data translation and translation prediction model betwe</context>
<context position="5521" citStr="Marelli et al., 2014" startWordPosition="839" endWordPosition="842">s provide a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bic¸ici, 2011; Bic¸ici and Yuret, 2014) as interpretants for reaching shared semantics (Bic¸ici, 2008). RTMs are a language independent approach and achieve top performance when predicting the quality of translations (Bic¸ici, 2013; Bic¸ici and Way, 2014) and when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014), and good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014a), as an automated student answer grader (Bic¸ici and van Genabith, 2013b), and when judging the semantic similarity of sentences (Bic¸ici and van Genabith, 2013a; Agirre et al., 2014). We improve the RTM models by: • using a parameterized, fast implementation of FDA, FDA5, and our Parallel FDA5 instance selection model (Bic¸ici et al., 2014), • better modeling of the language in which Algorithm 1: Referential Translation Machine Input: Training set train, test set test, corpus C, and learning model M. Data: Features of train and test, Ftrain and Ftest. Output: Predictions of similarity score</context>
<context position="17056" citStr="Marelli et al., 2014" startWordPosition="2804" endWordPosition="2807">ic¸ici et al., 2014). We optimize the learning parameters by selecting e close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for e is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). At testing time, the predictions are bounded to obtain scores in the corresponding ranges. We obtain the confidence scores using support vector classification (SVC). 3.1 Task 1: Semantic Relatedness and Entailment MSTS contains sentence pairs from the SICK (Sentences Involving Compositional Knowledge) data set (Marelli et al., 2014b), which contain sentence pairs that contain rich lexical, syntactic and semantic phenomena. Official evaluation metric in SRE is the Pearson’s correlation score, which is used to select the top systems on the training set. SRE task allows the submission of 5 entries. We present the performance of the top 5 individual RTM models on the training set in Table 3. ACC is entailment accuracy, rp is Pearson’s correlation, rs is Spearman’s correlation, MSE is mean squared error, MAE is mean absolute error, and RAE is relative absolute error. L uses the lemmatized corpora and R uses the true-cased co</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014a. SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proc. of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>A sick cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proc. of LREC</booktitle>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="2048" citStr="Marelli et al., 2014" startWordPosition="298" endWordPosition="301">source. 1 Semantic Similarity Judgments We introduce a fully automated judge for semantic similarity that performs well in three semantic similarity tasks at SemEval-2014, Semantic Evaluation Exercises - International Workshop on Semantic Evaluation (Nakov and Zesch, 2014). RTMs provide a language independent solution for the semantic textual similarity (STS) task (Task 10) (Agirre et al., 2014), achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) (Jurgens et al., 2014), and achieve good results in the semantic relatedness and entailment task (Task 1) (Marelli et al., 2014a). Referential translation machine (Section 2) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the real objects (Bic¸ici, 2008). Each RTM model is a data translation and translation prediction model betwe</context>
<context position="5521" citStr="Marelli et al., 2014" startWordPosition="839" endWordPosition="842">s provide a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bic¸ici, 2011; Bic¸ici and Yuret, 2014) as interpretants for reaching shared semantics (Bic¸ici, 2008). RTMs are a language independent approach and achieve top performance when predicting the quality of translations (Bic¸ici, 2013; Bic¸ici and Way, 2014) and when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014), and good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014a), as an automated student answer grader (Bic¸ici and van Genabith, 2013b), and when judging the semantic similarity of sentences (Bic¸ici and van Genabith, 2013a; Agirre et al., 2014). We improve the RTM models by: • using a parameterized, fast implementation of FDA, FDA5, and our Parallel FDA5 instance selection model (Bic¸ici et al., 2014), • better modeling of the language in which Algorithm 1: Referential Translation Machine Input: Training set train, test set test, corpus C, and learning model M. Data: Features of train and test, Ftrain and Ftest. Output: Predictions of similarity score</context>
<context position="17056" citStr="Marelli et al., 2014" startWordPosition="2804" endWordPosition="2807">ic¸ici et al., 2014). We optimize the learning parameters by selecting e close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for e is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). At testing time, the predictions are bounded to obtain scores in the corresponding ranges. We obtain the confidence scores using support vector classification (SVC). 3.1 Task 1: Semantic Relatedness and Entailment MSTS contains sentence pairs from the SICK (Sentences Involving Compositional Knowledge) data set (Marelli et al., 2014b), which contain sentence pairs that contain rich lexical, syntactic and semantic phenomena. Official evaluation metric in SRE is the Pearson’s correlation score, which is used to select the top systems on the training set. SRE task allows the submission of 5 entries. We present the performance of the top 5 individual RTM models on the training set in Table 3. ACC is entailment accuracy, rp is Pearson’s correlation, rs is Spearman’s correlation, MSE is mean squared error, MAE is mean absolute error, and RAE is relative absolute error. L uses the lemmatized corpora and R uses the true-cased co</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014b. A sick cure for the evaluation of compositional distributional semantic models. In Proc. of LREC 2014, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="21182" citStr="Miller, 1995" startWordPosition="3506" endWordPosition="3507">elation, out of CHECK submissions. The baseline in Table 7 is normalized longest common substring (LCS) scaled in the range [0, 4]. Top individual rank row lists the ranks in each subtask. We present the results for both our official and late (about 1 day) submissions including word to sense (W2S) results 6. RTM-DCU is able to obtain the top result in Par2S in the CLSS task. 3.3 Task 10: Multilingual Semantic Textual Similarity MSTS contains sentence pairs from different domains: sense definitions from semantic lexical resources such as OnWN (from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995)) and FNWN (from FrameNet (Baker et al., 1998) and WordNet), news headlines, image descriptions, news title tweet comments, deft forum and news, 5Giving advantage to participants submitting to all levels. 6W2S results for the late submission is obtained from the LCS baseline to calculate the ranks. rp Par2S S2Phrase Phrase2W W2S Rank LCS 0.527 0.562 0.165 0.109 25 0.780 0.677 0.208 14 Official 0.747 0.588 0.164 19 0.786 0.666 0.171 18 0.845 0.750 0.305 0.109 6 Late 0.785 0.698 0.221 0.109 13 0.786 0.663 0.171 0.109 17 Top Rank 1 5 3 rs Par2S S2Phrase Phrase2W W2S Rank LCS 0.527 0.562 0.165 0.1</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<title>Preslav Nakov and Torsten Zesch, editors.</title>
<date>2014</date>
<booktitle>Proc. of SemEval-2014 Semantic Evaluation Exercises - International Workshop on Semantic Evaluation.</booktitle>
<pages>23--24</pages>
<location>Dublin, Ireland,</location>
<marker>2014</marker>
<rawString>Preslav Nakov and Torsten Zesch, editors. 2014. Proc. of SemEval-2014 Semantic Evaluation Exercises - International Workshop on Semantic Evaluation. Dublin, Ireland, 23-24 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="12753" citStr="Papineni et al., 2002" startWordPosition="2059" endWordPosition="2062">of co-occurring features in the training set (Bic¸ici et al., 2013). • Synthetic Translation Performance {3, 3}: Calculates translation scores achievable according to the n-gram coverage. • Character n-grams {5}: Calculates cosine between character n-grams (for n=2,3,4,5,6) obtained for S and T (B¨ar et al., 2012). • Minimum Bayes Retrieval Risk {0, 4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {0, 3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • LIX {1, 1}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for S and T. 1 2.3 Bracketing Tree Structural Features We use the parse tree outputs obtained by CCL to derive features based on the bracketing structure. We derive 5 statistics based on the geometric properties of the parse trees: number of brackets used (numB), depth (depthB), average depth (avg 1LIX= B + C 100, where A is the number of words, C is words longer than 6 characters, B is words that start or end with any of “.”, “:”, “!”, “?” s</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword fifth edition, Linguistic Data Consortium.</title>
<date>2011</date>
<contexts>
<context position="14612" citStr="Parker et al., 2011" startWordPosition="2390" endWordPosition="2393">, 2006) for visualizing the output parse trees presented on the left. The bracketing structure statistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. 3 SemEval-14 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2014 with the RTM-DCU team name. The interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and the LM corpora provided by LDC for English (Parker et al., 2011) and Spanish ( ˆAngelo Mendonc¸a, 2011) 4. We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the lemmatized corpora for the SRE task. For each RTM 2For nodes with uneven number of children, the nodes in the odd child contribute to the right branches. 3Wall Street Journal (WSJ) corpus section 23, distributed with Penn Treebank version 3 (Marcus et al., 1993). 4English Gigaword 5th, Spanish Gigaword 3rd edition. 490 model, we extract the features both on the training set and the test set. The number of instances we select for the interpretants in each task is given in Table 2. Ta</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword fifth edition, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Eduard H Hovy</author>
<author>Mitchell P Marcus</author>
<author>Martha Palmer</author>
<author>Lance A Ramshaw</author>
<author>Ralph M Weischedel</author>
</authors>
<title>Ontonotes: a unified relational semantic representation.</title>
<date>2007</date>
<journal>Int. J. Semantic Computing,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="21155" citStr="Pradhan et al., 2007" startWordPosition="3499" endWordPosition="3503">anks for rp and rs, Spearman’s correlation, out of CHECK submissions. The baseline in Table 7 is normalized longest common substring (LCS) scaled in the range [0, 4]. Top individual rank row lists the ranks in each subtask. We present the results for both our official and late (about 1 day) submissions including word to sense (W2S) results 6. RTM-DCU is able to obtain the top result in Par2S in the CLSS task. 3.3 Task 10: Multilingual Semantic Textual Similarity MSTS contains sentence pairs from different domains: sense definitions from semantic lexical resources such as OnWN (from OntoNotes (Pradhan et al., 2007) and WordNet (Miller, 1995)) and FNWN (from FrameNet (Baker et al., 1998) and WordNet), news headlines, image descriptions, news title tweet comments, deft forum and news, 5Giving advantage to participants submitting to all levels. 6W2S results for the late submission is obtained from the LCS baseline to calculate the ranks. rp Par2S S2Phrase Phrase2W W2S Rank LCS 0.527 0.562 0.165 0.109 25 0.780 0.677 0.208 14 Official 0.747 0.588 0.164 19 0.786 0.666 0.171 18 0.845 0.750 0.305 0.109 6 Late 0.785 0.698 0.221 0.109 13 0.786 0.663 0.171 0.109 17 Top Rank 1 5 3 rs Par2S S2Phrase Phrase2W W2S Ran</context>
</contexts>
<marker>Pradhan, Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2007</marker>
<rawString>Sameer S. Pradhan, Eduard H. Hovy, Mitchell P. Marcus, Martha Palmer, Lance A. Ramshaw, and Ralph M. Weischedel. 2007. Ontonotes: a unified relational semantic representation. Int. J. Semantic Computing, 1(4):405–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Learning Syntactic Structure.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Universiteit van Amsterdam.</institution>
<contexts>
<context position="9340" citStr="Seginer, 2007" startWordPosition="1458" endWordPosition="1459"> coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. 2.2 MTPP Features for Translation Acts MTPP feature functions use statistics involving the training set and the test sentences to determine their closeness. Since they are language independent, MTPP allows quality estimation to be performed extrinsically. MTPP uses n-gram features defined over text or common cover link (CCL) (Seginer, 2007) structures as the basic units of information over which similarity calculations are made. Unsupervised parsing with CCL extracts links from base words to head words, representing the grammatical information instantiated in the training and test data. We extend the MTPP model we used last year (Bic¸ici, 2013) in its learning module and the features included. Categories for the features (S for source, T for target) used are listed below where the number of features are given in brackets for S and T, {#S, #T}, and the detailed descriptions for some of the features are presented in (Bic¸ici et al</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Learning Syntactic Structure. Ph.D. thesis, Universiteit van Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>2004</date>
<journal>Statistics and Computing,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>Smola, Sch¨olkopf, 2004</marker>
<rawString>Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial on support vector regression. Statistics and Computing, 14(3):199–222, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Smola</author>
<author>N Murata</author>
<author>B Sch¨olkopf</author>
<author>K-R M¨uller</author>
</authors>
<title>Asymptotically optimal choice of ε-loss for support vector machines. In</title>
<date>1998</date>
<booktitle>Proc. of the International Conference on Artificial Neural Networks, Perspectives in Neural Computing,</booktitle>
<pages>105--110</pages>
<editor>L. Niklasson, M. Boden, and T. Ziemke, editors,</editor>
<publisher>Springer.</publisher>
<location>Berlin.</location>
<marker>Smola, Murata, Sch¨olkopf, M¨uller, 1998</marker>
<rawString>A. J. Smola, N. Murata, B. Sch¨olkopf, and K.-R. M¨uller. 1998. Asymptotically optimal choice of ε-loss for support vector machines. In L. Niklasson, M. Boden, and T. Ziemke, editors, Proc. of the International Conference on Artificial Neural Networks, Perspectives in Neural Computing, pages 105–110, Berlin. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Marco Turchi</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In Proc. of the 13th Annual Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>28--35</pages>
<publisher>EAMT.</publisher>
<location>Barcelona,</location>
<contexts>
<context position="16137" citStr="Specia et al., 2009" startWordPosition="2652" endWordPosition="2655">task. We use ridge regression (RR), support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch¨olkopf, 2004), and extremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. TREE is an ensemble learning method over randomized decision trees. These models learn a regression function using the features to estimate a numerical target value. We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), both of which are described in (Bic¸ici et al., 2013). We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More detailed descriptions of the optimization processes are given in (Bic¸ici et al., 2013; Bic¸ici et al., 2014). We optimize the learning parameters by selecting e close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for e is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). At testing tim</context>
</contexts>
<marker>Specia, Cancedda, Dymetman, Turchi, Cristianini, 2009</marker>
<rawString>Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. In Proc. of the 13th Annual Conference of the European Association for Machine Translation (EAMT), pages 28–35, Barcelona, May. EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<contexts>
<context position="14710" citStr="Toutanova et al., 2003" startWordPosition="2408" endWordPosition="2411">statistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. 3 SemEval-14 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2014 with the RTM-DCU team name. The interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and the LM corpora provided by LDC for English (Parker et al., 2011) and Spanish ( ˆAngelo Mendonc¸a, 2011) 4. We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the lemmatized corpora for the SRE task. For each RTM 2For nodes with uneven number of children, the nodes in the odd child contribute to the right branches. 3Wall Street Journal (WSJ) corpus section 23, distributed with Penn Treebank version 3 (Marcus et al., 1993). 4English Gigaword 5th, Spanish Gigaword 3rd edition. 490 model, we extract the features both on the training set and the test set. The number of instances we select for the interpretants in each task is given in Table 2. Task Setting Train LM Task 1, SRE English 770 10770 Task 3, CLSS Par2S 302 2802 Task 3, CLSS S2Phras</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proc. of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker></marker>
<rawString>In Proc. of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<date>2013</date>
<note>LIX. http://en.wikipedia.org/ wiki/LIX.</note>
<contexts>
<context position="12887" citStr="Wikipedia, 2013" startWordPosition="2083" endWordPosition="2084">s achievable according to the n-gram coverage. • Character n-grams {5}: Calculates cosine between character n-grams (for n=2,3,4,5,6) obtained for S and T (B¨ar et al., 2012). • Minimum Bayes Retrieval Risk {0, 4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {0, 3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • LIX {1, 1}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for S and T. 1 2.3 Bracketing Tree Structural Features We use the parse tree outputs obtained by CCL to derive features based on the bracketing structure. We derive 5 statistics based on the geometric properties of the parse trees: number of brackets used (numB), depth (depthB), average depth (avg 1LIX= B + C 100, where A is the number of words, C is words longer than 6 characters, B is words that start or end with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012). depthB), number of brackets on the right branches over the number of brackets on the left (R/L) 2, averag</context>
</contexts>
<marker>Wikipedia, 2013</marker>
<rawString>Wikipedia. 2013. LIX. http://en.wikipedia.org/ wiki/LIX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>Denise DiPersio ˆAngelo Mendonc¸a, Daniel Jaquette.</title>
<date>2011</date>
<marker>Graff, 2011</marker>
<rawString>David Graff Denise DiPersio ˆAngelo Mendonc¸a, Daniel Jaquette. 2011. Spanish Gigaword third edition, Linguistic Data Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>