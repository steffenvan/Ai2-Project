<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.666914">
Summarizing Blog Entries versus News Texts
</title>
<author confidence="0.633187">
Shamima Mithun and Leila Kosseim
</author>
<affiliation confidence="0.917889333333333">
Concordia University
Department of Computer Science and Software Engineering
Montreal, Quebec, Canada
</affiliation>
<email confidence="0.982293">
{s mithun, kosseim}@encs.concordia.ca
</email>
<sectionHeader confidence="0.986414" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999819466666667">
As more and more people are expressing their
opinions on the web in the form of weblogs (or
blogs), research on the blogosphere is gaining
popularity. As the outcome of this research,
different natural language tools such as query-
based opinion summarizers have been devel-
oped to mine and organize opinions on a par-
ticular event or entity in blog entries. How-
ever, the variety of blog posts and the infor-
mal style and structure of blog entries pose
many difficulties for these natural language
tools. In this paper, we identify and cate-
gorize errors which typically occur in opinion
summarization from blog entries and compare
blog entry summaries with traditional news
text summaries based on these error types
to quantify the differences between these two
genres of texts for the purpose of summariza-
tion. For evaluation, we used summaries from
participating systems of the TAC 2008 opin-
ion summarization track and updated summa-
rization track. Our results show that some er-
rors are much more frequent to blog entries
(e.g. topic irrelevant information) compared
to news texts; while other error types, such
as content overlap, seem to be comparable.
These findings can be used to prioritize these
error types and give clear indications as to
where we should put effort to improve blog
summarization.
</bodyText>
<sectionHeader confidence="0.990678" genericHeader="keywords">
Keywords
</sectionHeader>
<keyword confidence="0.9689495">
Opinion summarization, blog summarization, news text sum-
marization.
</keyword>
<sectionHeader confidence="0.999169" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988295081967">
Everyday, people express their opinions on a variety
of topics ranging from politics, movies, music to newly
launched products on the web in weblogs (or blogs),
wikis, online-forums, review sites, and social network-
ing web sites. As more and more people are expressing
their opinions on the web, the Internet is becoming a
popular and dynamic source of opinions. Natural lan-
guage tools for automatically mining and organizing
these opinions on various events will be very useful for
individuals, organizations, and governments.
Various natural language tools to process and
utilize event-related information from texts have
already been developed. Event-based question an-
swering systems [21] and event-based summarization
systems [12] are only a few examples. However, most
of the event-based systems have been developed to
process events from traditional news texts. Blog
entries are different in style and structure compared
to news texts. As a result, successful natural language
approaches that deal with news texts might not be as
successful for processing blog entries; thus adaptation
of existing successful NLP approaches for news texts
to process blog entries is an interesting and challeng-
ing task. The first step towards this adaptation is
to identify the differences between these two textual
genres in order to develop approaches to handle this
new genre of texts (blogs) with greater accuracy.
In this study, we compare automatically generated
summaries of blog entries with summaries of news
texts with the goal of improving opinion summariza-
tion from blog entries. In particular, we compared
summaries for these two genres of texts on the basis of
various errors which typically occur in summarization.
In this paper, we first investigate what kind of er-
rors typically occur in query-based opinionated sum-
mary for blog entries. The errors that we have iden-
tified are categorized and then used to compare blog
summaries with news texts summaries. For evalua-
tion, we used summaries from participating systems
at the TAC 2008 [1] opinion summarization track and
updated summarization track. Summaries of the TAC
2008 opinion summarization track and updated sum-
marization track were generated from blogs entries and
traditional news texts, respectively. The systems par-
ticipating in the TAC opinion summarization track
and in the updated summarization track are quite dif-
ferent in several aspects, as they are targeted to re-
solve two different tasks. The systems participating
in the updated summarization track were mainly re-
quired to find the answers to given queries and detect
redundant information while the systems participating
in the opinion summarization track were required to
perform opinion mining and polarity classification in
addition. Moreover, the systems participating in the
opinion summarization track were provided optional
snippets (described in section 3.1) and were restricted
with a maximum summary length which were much
higher compared to the updated summarization track.
Despite these differences, these two datasets were used
in our work because they are the most comparable
datasets for our task.
</bodyText>
<page confidence="0.835951">
1
</page>
<bodyText confidence="0.847191">
Events in Emerging Text Types (eETTs) - Borovets, Bulgaria, pages 1–8
</bodyText>
<sectionHeader confidence="0.324025" genericHeader="method">
2 Characteristics of Blogs
</sectionHeader>
<bodyText confidence="0.99971132">
Blogs (or weblogs) are online diaries that appear in
chronological order. Blogs reflect personal thinking
and feelings on all kinds of topics including day to
day activities of bloggers; hence an essential feature
of blogs is their subjectivity. Some blogs focus on a
specific topic while others cover several topics; some
describe personal daily lives of bloggers while others
describe common artifacts or news. Many different
sub-genres of blogs exist. The two most common are
personal journals and notebooks [5]. Personal jour-
nals discuss internal experiences and personal lives of
bloggers and tend to be short [5]. They are usually
informal in nature and written in casual and infor-
mal language. They may contain much and some-
times only unrelated information such as ads, photos,
and other non-textual elements. They also contain
spelling and grammatical errors, and punctuation and
capitalization are often missing. On the other hand,
notebooks contain comments on internal and external
events. Similarly to newspaper articles, they are usu-
ally long and written in a more formal style [5]. Most
NLP work on blogs has tended to study personal jour-
nals as opposed to notebooks. For example, the Blog-
06 corpus [15], used at TREC and at TAC, contains
mostly personal journals.
</bodyText>
<sectionHeader confidence="0.981808" genericHeader="method">
3 Blog Summarization
</sectionHeader>
<bodyText confidence="0.9996422">
Opinion summarization, and in particular blog sum-
marization, is a fairly recent field. Some systems (e.g.
[9, 10]) have been developed for opinion summarization
to generate a summary from a document. In 2008, the
Text Analysis Conference (TAC) introduced a query-
based opinion summarization track. They provided
questions, a blog corpus and optional snippets which
are found by QA systems. These query-based summa-
rization systems are designed to retrieve specific an-
swers on an event or entity instead of an overview of
the whole document.
Opinion summarization uses opinionated docu-
ments such as blogs, reviews, newspaper editorials or
letters to the editor to answer opinionated questions.
On the other hand, summarization of traditional news
texts uses fact-based information such as formal and
non-opinionated texts. As we are interested in opinion
summarization from blog entries, we will use the two
terms opinion summarization and blog summarization
interchangeably.
</bodyText>
<subsectionHeader confidence="0.999252">
3.1 Current Approaches
</subsectionHeader>
<bodyText confidence="0.9998403125">
A query-based opinion summarizer recapitulates what
people think or feel on a particular topic (or an event
or entity) by answering a specific query. For exam-
ple, one such opinionated query could be What has
been Russia’s reaction to U.S. bombing of Kosovo?.
A query-based opinion summarizer can answer opin-
ion questions posed in natural language; thus it helps
users to get specific answers to questions they are in-
terested in, instead of retrieving an entire document.
At the TAC 2008 opinion summarization track, a
set of target topics on various events or entities were
given on which participating systems were evaluated.
For each topic, a set of questions and a set of relevant
blog entries (mostly personal journals) were provided.
For example, for the topic “UN Commission on Hu-
man Rights”, two questions were asked:
</bodyText>
<listItem confidence="0.993135">
1. “What reasons are given as examples of their in-
effectiveness?”
2. “What steps are being suggested to correct this
problem?”
</listItem>
<bodyText confidence="0.921996444444444">
and a set of IDs of related blog entries were pro-
vided. Systems needed to extract answers to questions
from these specified sets of blog entries. Additionally,
some sample answer snippets were provided for ev-
ery topic that summarization systems may use. These
snippets were extracted by the participating QA sys-
tems at the TAC 2008 QA track. Here are two sam-
ple snippets for the topic UN Commission on Human
Rights:
</bodyText>
<listItem confidence="0.999188833333333">
1. “Issues regular resolutions condemning Israel
while overlooking real offenders.”
2. “To ensure this new body would be no facsimile
of its predecessor, the legislation prohibits mem-
bership to countries that violate human rights or
are subject to specific human rights resolutions.”
</listItem>
<bodyText confidence="0.99985">
Two types of summarization approaches were used
by TAC participants, namely: snippet-driven ap-
proaches and snippet-free approaches. Snippet-driven
approaches use snippet information to extract sen-
tences which contain these snippets from the input
blog entries. They then generate a summary by in-
corporating these sentences. Snippet-free approaches
do not use snippets. They mainly utilize query in-
formation and sentiment degree for sentence scoring.
Participating systems first filter blog entries to iden-
tify the relevant content and remove irrelevant infor-
mation such as ads, photos, music, videos, and other
non-textual elements. The focus and polarity of the
question are identified; then sentences are ranked ac-
cording to their relevance with the query. The polarity
of the sentences is also calculated and matched with
the polarity of the query. To find the relevance with
the query, overlap with the query terms is calculated
using different techniques such as the cosine similarity,
language models etc. Opinion dictionaries and differ-
ent machine learning techniques are used to identify
the polarity of the question and sentences. Finally, the
summaries are generated using the ranked sentences.
</bodyText>
<subsectionHeader confidence="0.99549">
3.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999915636363636">
Evaluation of blog summaries use the same criteria as
for traditional news text summarization. The quality
of a summary is assessed mostly on its content and
linguistic quality [14]. Content evaluation of a query-
based summary is performed based on the relevance
assessment (with the topic and query) and inclusion
of important contents from the input documents.
Currently, the automatic evaluation tool
ROUGE [11] is the most popular evaluation ap-
proach for content evaluation. ROUGE automatically
compares system generated summaries with a set of
</bodyText>
<page confidence="0.962558">
2
</page>
<bodyText confidence="0.9732110625">
model summaries (human generated) by computing
n-gram word overlaps between them. Conferences and
workshops such as TAC and DUC (Document Under-
standing Conference) [2] use ROUGE. The pyramid
method [18] is also used for content evaluation. In
the pyramid method, multiple human generated
summaries are analyzed manually to generated a
gold standard. In this process, summary analysis
is done semantically such that information with the
same meaning (expressed using different wording) is
marked as summary content unit (SCU). A weight is
assigned for each SCU based on the number of human
summarizers that express it in their summaries. In
this method, the pyramid score for a system generated
summary is calculated as follows [17]:
score = (the sum of weights of SCUs expressed in
a generated summary) / (the sum of weights
of an ideally informative summary with the
same number of SCUs)
The linguistic quality of a summary is evaluated
manually based on how it structures and presents the
contents. Grammaticality, non-redundancy, referen-
tial clarity, focus, structure and coherence are the com-
monly used factors considered to evaluate the linguis-
tic quality. Mainly, subjective evaluation is done to
assess the linguistic quality of an automatically gen-
erated summary. In this process, human assessors di-
rectly assign scores on a scale based on agreement or
disagreement with predefined set of questions such as
“Are they ungrammatical?”, “Do they contain redun-
dant information?”. The assessments are done with-
out reference to any model summaries.
</bodyText>
<subsectionHeader confidence="0.998197">
3.3 News Text Summarization versus
Blog Summarization
</subsectionHeader>
<bodyText confidence="0.999693875">
As most work has been done on news text summa-
rization, it is not surprising that the performance of
such systems are generally higher than blog summa-
rizers. For example, as shown in Table 1, at the TAC-
2008 conference, the average scores for news text sum-
maries (updated summarization track) are higher than
for blog summaries (opinion summarization track) us-
ing all 3 evaluation criteria.
</bodyText>
<tableCaption confidence="0.995589">
Table 1: Average TAC-2008 Summarization Results
</tableCaption>
<table confidence="0.9758434">
- Blogs vs. News Texts
Genre Pyramid Linguistic Resp.
Score Score Score
Blogs 0.21 2.13 1.61
News 0.27 2.33 2.32
</table>
<bodyText confidence="0.990741361111111">
Table 1 shows summary evaluation using the
pyramid score, linguistic quality and responsiveness
(Resp.). The last two criteria were evaluated by
human assessors on a scale of 1 to 5 (1, being the
worst). In this evaluation, the responsiveness of a
summary was judged to measure the overall quality
or usefulness of the summary, considering both the
information content and readability.
This difference in performance between blogs and
news texts can be attributed to the differences in the
two textual genres. Indeed, one of the essential char-
acteristics of blogs as opposed to news texts, is their
subjectivity (or opinion). Unlike traditional news text
summarization, sentiment (subjectivity) plays a key
role for blog summarization. For blog summarization,
sentiment degree is often used to rank sentences. In
the case of query-based blog summarization, the sen-
timent polarity of the question needs to be matched
with that of summary sentences.
In addition, as opposed to traditional news texts,
blogs are usually written in casual language. For blogs,
it is usually very difficult to identify which portions of
blog entries are relevant to the topic. News texts are
more uniform in style and structure. Blogs may con-
tain many unrelated information such as ads, photos,
music, videos. For blogs, it is often difficult to find
sentence boundaries. In most cases punctuation and
capitalization are unreliable. As a result, for blog sum-
marization, systems need to put additional efforts to
pre-process the text compared to news text summa-
rization. Furthermore, because blogs do not exhibit
a stereotypical structure, some features such as posi-
tion of sentence, or similarity with the first sentence,
which are shown to be useful for traditional news text
summarization are not as useful for blog summariza-
tion [6].
</bodyText>
<sectionHeader confidence="0.998542" genericHeader="method">
4 Error Analysis
</sectionHeader>
<bodyText confidence="0.999201925925926">
To identify the different challenges posed by blog sum-
marization as opposed to traditional news texts sum-
marization, we have studied 50 summaries from partic-
ipating systems at the TAC 2008 opinion summariza-
tion track and compared these to 50 summaries from
the TAC 2008 updated summarization tracks. The av-
erage summary length of the opinion summarization
track was 1224 words, while that of the updated sum-
marization track was 179 words. The average input
documents length of the opinion summarization track
was 1888 words, while that of the updated summa-
rization track was 505 words. Summaries were ran-
domly selected for the evaluation. However, we en-
sured that we selected summaries from all participat-
ing systems on all topics. The task of the updated
summarization track was chosen for comparison be-
cause it is similar in nature to the blog summariza-
tion task in the sense that its goal is also to generate
query focused (but non-opinionated) summaries (using
news articles). Even though there are several differ-
ences between the summarization approaches in TAC
opinion summarization track and updated summariza-
tion track, these two datasets are the most comparable
datasets for our task.
In this study, we have analyzed the most common
types of errors in our 100-summary corpus and have
categorized them in 3 main categories:
</bodyText>
<listItem confidence="0.979853">
1. Summary-Level Error (SuLE)
2. Sentence-Level Error (SeLE)
</listItem>
<page confidence="0.918931">
3
</page>
<sectionHeader confidence="0.511537" genericHeader="method">
3. Intra-Sentence-Level Error (ISLE)
</sectionHeader>
<bodyText confidence="0.977579">
These are shown in Figure 1 and discussed in the
following sub-sections.
</bodyText>
<figureCaption confidence="0.989895">
Fig. 1: Types of Errors in Blog vs. News Summaries
</figureCaption>
<subsectionHeader confidence="0.97338">
4.1 Summary-Level Errors
</subsectionHeader>
<bodyText confidence="0.991034">
We define a Summary-Level Error (SuLE) as the tex-
tual contents which reduce the understandability and
readability of the overall summary. There are two
types of SuLE:
</bodyText>
<listItem confidence="0.9943765">
1. Discourse Incoherency (DI)
2. Content Overlap (CO)
</listItem>
<sectionHeader confidence="0.776952" genericHeader="method">
Discourse Incoherency (DI)
</sectionHeader>
<bodyText confidence="0.99995">
A summary will exhibit a Discourse Incoherency (DI)
if the reader cannot identify the communicative in-
tentions of the writer from the propositions or if the
propositions do not seem to be interrelated [8]. In the
sample summaries that we have studied, Discourse In-
coherency occurred both at the sentence level and at
the proposition level.
Consider the following summary (ID:T1004.20 1)
where a DI occurs at the sentence level:
</bodyText>
<construct confidence="0.9638601">
Topic: Starbucks coffee shops
Question: Why do people like Starbucks better than
Dunkin Donuts?
Summary: I am firmly in the Dunkin’ Donuts camp.
It’s a smooth, soothing cuppa, with no disastrous
gastric side effects, very comforting indeed. I have a
special relationship with the lovely people who work in
the Dunkin’ Donuts in the Harvard Square T Station in
Cambridge. I was away yesterday and did not know.
[...]
</construct>
<bodyText confidence="0.999838">
In this summary, the underlined sentence is not
coherent with the rest of the text because it is not
interrelated with the rest of the text.
Consider the following summary (ID:T1001.3)
where a DI occurs at the proposition level:
</bodyText>
<figure confidence="0.399344666666667">
Topic: Carmax
Question: What motivated positive opinions of Car-
max from car buyers?
</figure>
<footnote confidence="0.9055544">
Summary: At Carmax, the price is the price
and when you want a car you go get one.
Tyler Sink Says: December 7th, 2005 at 9:22 am,
1 All summaries numbered ID:Txxxx.xx are taken from the
TAC 2008 opinion summarization track.
</footnote>
<construct confidence="0.900168">
with Carmax you will generally always pay more than
from going to a good used car dealer. [...]
</construct>
<bodyText confidence="0.986554333333333">
In the underlined sentence, the textual fragments
Tyler Sink .. and with Carmax .. do not seem to be
related to one another.
</bodyText>
<sectionHeader confidence="0.34926" genericHeader="method">
Content Overlap (CO)
</sectionHeader>
<bodyText confidence="0.9998395">
If semantically similar information exists in different
units of a text, then we define it as Content Overlap
(CO). Content overlap can range from a simple
duplication of text fragments to a more complex
textual entailment problem. For example, consider
the summary below (ID:T1019.35):
</bodyText>
<note confidence="0.396103">
Topic: China one-child per family law
</note>
<construct confidence="0.719676857142857">
Question: What complaints are made about China’s
one-child per family law?
Summary: [...] If you have money to pay the fines,
you can have 2 or 4 children. [...] $6400 - a typical
fine for having more than one child- in China is about
2-3 years salary. [...] Imagine losing your job, being
fined 2-3 years salary for having a second child. [...]
</construct>
<bodyText confidence="0.999230333333333">
In this summary, the underlined sentences carry
similar contents. So it may seem redundant to include
all these sentences in the final summary.
</bodyText>
<tableCaption confidence="0.814942">
Table 2: Summary-Level Errors - Blogs vs. News
Texts
</tableCaption>
<table confidence="0.9984102">
Error Type Blogs News Δ
DI Discourse 30.44% 10.66% 19.78%
Incoherency
CO Content 19.14% 14.66% 4.48%
Overlap
</table>
<bodyText confidence="0.9643718">
Table 2 compares Summary-Level errors in our 50
blog summaries corpus and our 50 news texts sum-
maries corpus. Table 2 shows that opinionated blog
summarization and non-opinionated news texts sum-
marization both exhibit an important number of Dis-
course Incoherency and Content Overlap errors. How-
ever, blog summarization have around 20% more Dis-
course Incoherency and about 4.5% more Content
Overlap errors, than those of news article summariza-
tion. We suspect that the reason behind this is that
blogs are generally informal in nature. As a result,
in blogs, propositions are often incoherent and con-
tain redundant information. On the other hand, the
formal nature of news articles reduces these errors for
news texts summarization.
</bodyText>
<subsectionHeader confidence="0.959273">
4.2 Sentence-Level Errors
</subsectionHeader>
<bodyText confidence="0.979055">
If a summary sentence is irrelevant to the central topic
of the input documents or to user query, then the sum-
mary contains a Sentence-Level Error (SeLE). Two
types of SeLE were identified:
</bodyText>
<footnote confidence="0.42589">
1. Topic Irrelevancy (TI)
</footnote>
<page confidence="0.943571">
4
</page>
<figure confidence="0.205846">
2. Question Irrelevancy (QI).
Topic Irrelevancy (TI)
</figure>
<bodyText confidence="0.999443">
As mentioned in Sections 3.1 and 4, in both the TAC
2008 opinion summarization track (blogs) and the up-
dated summarization track (news texts), participating
systems needed to generate a summary answering a set
of questions on a specific target (topic). However, in
both tasks, many systems generated a summary con-
taining sentences that are not related to the specified
topic. Here is an example of a TI (ID:T1004.33):
</bodyText>
<figure confidence="0.819979222222222">
Topic: Starbucks coffee shops
Question: Why do people like Starbucks better than
Dunkin Donuts?
Summary: Well ... I really only have two. [...]
I didn’t get a chance to go ice-skating at Frog Pond
like I wanted but I did get a chance to go to the IMAX
theatre again where I saw a movie about the Tour de
France it wasn’t that good. [...]
Question Irrelevancy (QI)
</figure>
<bodyText confidence="0.944829470588235">
Many of the system generated summary sentences are
not relevant to the question even though they are re-
lated to the topic. An example of a QI is shown below
(ID:T1004.3):
Topic: Starbucks coffee shops
Question: Why do people like Starbucks better than
Dunkin’ Donuts?
Summary: Posted by: Ian Palmer — November
22, 2005 at 05:44 PM Strangely enough, I read a
few months back of a coffee taste test where Dunkin’
Donuts coffee tested better than Starbucks. [...]
Not having a Dunkin’ Donuts in Sinless City I am
obviously missing out... but Starbucks are doing a
Christmas Open House today where you can turn up
for a free coffee. [...]
The underlined sentence is relevant to the topic but
not to the question.
</bodyText>
<tableCaption confidence="0.962167">
Table 3: Sentence-Level Errors - Blogs vs. News
Texts
</tableCaption>
<table confidence="0.9893128">
Error Type Blog News Δ
TI Topic 41.67% 5.86% 35.81%
Irrelevancy
QI Question 47.87% 16.67% 31.20%
Irrelevancy
</table>
<bodyText confidence="0.985760717948718">
Table 3 compares Sentence-Level errors for blog
summaries and news text summaries. Note that in
the table, Topic Irrelevancy is calculated based on
the entire corpus. However, Question Irrelevancy is
calculated based only on the sentences which are re-
lated to the topic. Table 3 shows that a large number
of sentences from blog summaries have Topic Irrele-
vancy and Question Irrelevancy errors. In contrast,
in news text summarization, Topic Irrelevancy error
occurs only occasionally and Question Irrelevancy er-
ror is also not very frequent. Blogs summarization has
around 30% more of these two errors than that of news
text summarization. We suspect that the main rea-
son behind such a difference is brought about by the
summary evaluation scheme. Indeed, many systems
use the optimal summary length (7000 characters per
question) allowed in TAC which results in many out of
context sentences to be used as filler. As a result, the
average summary length of the opinion summarization
track is much longer than that of the updated summa-
rization track (1224 words versus 179 words). Another
important reason for these errors is the informal style
and structure of blog entries. Indeed, sentences in blog
entries do not have a predictable rhetorical structure
(e.g. in formal writing, the first and the last sentences
of a paragraph usually contain important information)
which can be used to rank sentence during summariza-
tion. As a result, it is much more difficult to rank blog
sentences compared to news text sentences. Opinion
(sentiment) information is typically used to rank blog
sentences for summarization. We also believe that be-
cause opinion identification can be quite imprecise, it
can possibly add more noise to the blog sentence rank-
ing process. Moreover, unlike pre-focused news arti-
cles, blogs are quite unfocused. In blogs, bloggers ex-
press various opinions about the topic which are not
relevant to the question. Together all these issues may
lead to a high number of topic and question unrelated
sentences in blog summarization.
</bodyText>
<subsectionHeader confidence="0.948468">
4.3 Intra-Sentence-Level Errors
</subsectionHeader>
<bodyText confidence="0.99962225">
Intra-Sentence-Level (ISLE) errors occur within a sen-
tence and involve irrelevant or missing information,
grammatical errors, or lexical errors (e.g. spelling er-
rors). Intra-Sentence-Level Errors include:
</bodyText>
<listItem confidence="0.999860666666667">
1. Irrelevant Information (II)
2. Missing Information (MI)
3. Syntactic and Lexical Incorrectness (SLI)
</listItem>
<bodyText confidence="0.9494185">
Each of these categories are described below with
examples.
</bodyText>
<sectionHeader confidence="0.706515" genericHeader="method">
Irrelevant Information (II)
</sectionHeader>
<bodyText confidence="0.856424428571429">
Under Irrelevant Information (II) errors, a significant
portion of a sentence is irrelevant to the summary topic
or question. For example, consider the summary below
(ID:T1003.9):
Topic: Jiffy Lube
Question: What reasons are given for liking the ser-
vices provided by Jiffy Lube?
Summary: They know it’s fine cause Jiffy Lube
sent them a little card in the mail and they have
about a month before they need an oil change. [...]
Well, they suppose it is a little bit of a PITA to figure
out what to do with the spent oil, but after some
digging, they found out that every Jiffy Lube will take
used oil for free! [...]
</bodyText>
<page confidence="0.98839">
5
</page>
<bodyText confidence="0.843771739130435">
The underlined snippet above is irrelevant to the
question even though it holds a coherent discourse re-
lation with the last proposition.
Missing Information (MI)
If a sentence does not contain all the necessary infor-
mation to make it comprehensible for the reader and
the required information to understand the sentence
is also not available in the context then this error is
defined as a Missing Information (MI) error.
Here is an example of MI. In the following summary
(ID:T1021.17):
Topic: Sheep and Wool Festival
Question: Why do people like to go to Sheep and
Wool festivals?
Summary: [...] i hope to go again this year and
possibly meet some other knit bloggers this time
around since i missed tons of people last year.
I love going because of the tons of wonderful people,
yarn, Sheep, rabbits, alpacas, llamas, cheese,
sheepdogs, fun stuff to buy, etc. , etc. [...]
The underlined sentence contains incomplete infor-
mation, which cannot be resolved from the context ei-
ther making it incomprehensible.
</bodyText>
<subsectionHeader confidence="0.801053">
Syntactic and Lexical Incorrectness (SLI)
</subsectionHeader>
<bodyText confidence="0.999732285714286">
Syntactical level errors such as grammatical incorrect-
ness and incompleteness of a sentence or lexical level
errors such as spelling errors, short forms, stylistic
twists of informal writing ... in a sentence is defined
as Syntactic and Lexical Incorrectness (SLI) error.
For example, consider the following summary
(ID:T1009.32):
</bodyText>
<subsectionHeader confidence="0.841073">
Topic: Architecture of Frank Gehry
</subsectionHeader>
<bodyText confidence="0.9488712">
Question: What compliments are made concerning
his structures?
Summary: Central to Millennium Park in Chicago
is the Frank Gehry-designed Jay Pritzker Pavilion,
described as the most sophisticated outdoor con-
cert venue of its kind in the United States. [...]
Designing a right-angles-be-damned concert hall for
Springfield, hometown of Bart et al.. [...]
In this summary, the underlined sentence is an
example of a SLI.
</bodyText>
<tableCaption confidence="0.933849">
Table 4: Intra-Sentence-Level Errors - Blogs vs.
News Texts
</tableCaption>
<table confidence="0.99577325">
Error Type Blog News Δ
II Irrelevant 30.91% 15.66% 15.25%
Information
MI Missing 9.33% 2.33% 7.00%
Information
SLI Syntactic 18.79% 4.00% 14.79%
and Lexical
Incorrectness
</table>
<tableCaption confidence="0.730788">
Table 4 compares Intra-Sentence-Level errors for
</tableCaption>
<bodyText confidence="0.988062">
blog summaries and news text summaries. From Ta-
ble 4, we can see that Irrelevant Information, Missing
Information, and Syntactic and Lexical Incorrectness
errors appear about 15%, 7%, and 15% more respec-
tively in blog summarization. Here again, we believe
that the informal nature of blogs explains these differ-
ence.
</bodyText>
<sectionHeader confidence="0.996067" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999953166666667">
Compared to a manual linguistic evaluation of a sum-
mary, our work tries to identify and quantify the differ-
ences in error types between two textual genres: blogs
and news.
Our error types incorporate both what the auto-
matic and manual summary evaluation try to measure.
Indeed, Sentence-Level Errors (Topic Irrelevancy and
Question Irrelevancy) evaluate the content and rele-
vance of the summaries similarly to what ROUGE tries
to evaluate; whereas the remaining errors (Summary-
Level Errors and Intra-Sentence Errors) evaluate more
the linguistic quality of a summary.
It is not surprising to see that Topic Irrelevancy,
Question Irrelevancy, Discourse Incoherency, Irrele-
vant Information and Syntactic and Lexical Incorrect-
ness are much more frequent in blogs than in news
texts (from 36% to 19% more frequent). Content Over-
lap and Missing Information, on the other hand, seem
to be only slightly more frequent (5% and 7%) in blogs
summaries than in news texts summaries. These re-
sults give a clear idea of how difficult it is to pro-
cess blog entries for summarization compared to news
texts and where efforts should be made to improve
such summaries.
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="related work">
6 Related Work
</sectionHeader>
<subsectionHeader confidence="0.993609">
6.1 NLP on blogs
</subsectionHeader>
<bodyText confidence="0.999989086956522">
Recently, the availability of opinions on current events
on weblogs opened up new directions in natural lan-
guage research. Even though natural language pro-
cessing on blogs is a fairly new trend, its popularity
is growing rapidly. Many conferences and workshops
(e.g. [1, 3, 4, 15]) are taking place to address differ-
ent aspects of the analysis of blog entries. Current
NLP work on blog entries include: subjectivity and
sentiment analysis; question answering; and opinion
summarization.
Subjectivity and sentiment analysis include classi-
fying sentiments of reviews [19] and analyzing blog-
ger mood and sentiment on various events [16]. Sen-
timent classification of reviews on different events is
often done on movie or product reviews. Rating indi-
cators of reviews are used to identify the polarity of
the blogs namely positive, negative or neutral. To an-
alyze blogger mood and sentiment, systems make use
of information regarding bloggers’ mood varying over
time. To record bloggers’ varying mood, the polarity
information of the blog post is often used. Some works
(e.g. [16]) are done to measure how bloggers’ varying
mood affects different events. In addition, the TREC
</bodyText>
<page confidence="0.996172">
6
</page>
<bodyText confidence="0.9995272">
blog track [15] provides an opportunity to build new
techniques of sentiment tagging on blog posts. The
task is to identify and rank blog posts on a given topic
from a corpus of blog entries.
Question answering (QA) on blog entries is a
relatively new field. Most notable QA work on blog
entries was conducted at TREC 2007 [15] and TAC
2008 [1]. To answer queries on an event or entity,
TREC provided a blog corpus in addition to the
AQUAINT newspaper corpus [15].
</bodyText>
<subsectionHeader confidence="0.999827">
6.2 Analysis of blogs versus news
</subsectionHeader>
<bodyText confidence="0.999995842105264">
To the best of our knowledge there have been only a
few work carried out to compare the difference between
blog entries and news texts; however, none seems to
have analyzed it at the linguistic level for a specific
NLP application.
Ku et al. [10] developed a language independent
opinion summarization approach. For summarization,
they retrieved all sentences which are relevant to the
main topic of the document set and determined the
opinion polarity and degree of these relevant sentences.
They also found that the identification of correlated
events on a time interval is also important for opinion
summarization. They tested their approach for blog
entries and news texts for English and Chinese lan-
guages. From their evaluation, they found that blog
entries contain more topic irrelevant information com-
pared to news texts. Their results confirm our own re-
sults. Ku et al. also found that news texts use a larger
vocabulary compared to blog entries which makes the
filtering of non-relevant sentences task harder for news
texts. On the other hand, this larger vocabulary helps
to decide sentiment polarities. Due to the limited vo-
cabulary the judgment of sentiment polarity of blog
entries was difficult.
Somasundaran et al. [20] developed an opinion
question answering approach for blogs and news texts.
They exploited attitude information namely senti-
ment and argument types to answer opinion questions.
They received comparable result with both text types.
Lloyd et al. [13] developed the Lydia system to ana-
lyze blog entries. They analyzed temporal relationship
between blogs and news texts. In particular, they an-
alyzed how often bloggers report a story before news-
papers and how often bloggers react to news that has
already been reported. To study this leads/lag rela-
tionship, they analyzed frequency time series of 197
most popular entries in news texts and blog corpora
over six week period. Lydia first recognized name en-
tities to extract information from both corpora. Then
the system resolved noun phrase coreference because
a single entity is often mentioned using multiple vari-
ations on their name. Then it performed a temporal
analysis to identify which entities are referred more
frequently over a certain period of time. In their anal-
ysis, they found that 30 entities exhibited no lead/lag
relationship, 73 had news leading the blogs, and 94
had blogs leading the news.
Godbole et al. [7] developed a large-scale senti-
ment analysis system on top of the Lydia text anal-
ysis system [13] for news texts and blog entities. They
determined the public sentiment on various entities
and identified how this sentiment varies with time.
They found that the same entities (person) except cer-
tain controversial political figures received compara-
ble opinions (favorable or adverse) in blogs and news
texts. Controversial political figures received different
opinions in blogs compared to news texts because of
the political biases among bloggers, and perhaps the
mainstream press.
Though both the work Lloyd et al. and Godbole
et al. handle news text and blog entries, their appli-
cation domains (temporal relationship and sentiment
analysis) are different from ours. Somasundaran et
al. tested their question answering approach for news
texts and blogs. They compared their approach for
both genres of text mainly on the basis of subjectivity
information. On the other hand, we compared sum-
maries of both text types on the basis of errors which
mainly occurred from informal style and structure of
blog entries. Our work is most similar to Ku et al.’s
work. However, we identified a larger number of er-
rors of summarization and compared blog summaries
with traditional news texts summaries on the basis of
these errors. As a result, our work will better enable
us to pinpoint the difference between these two genres
of texts for summarization task.
</bodyText>
<sectionHeader confidence="0.998413" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99999164">
As the performance of blog summarization is gener-
ally much lower than for news text summarization,
we set out to compare automatically generated sum-
maries for blogs entries with news texts based on
the most common errors which occurred in summa-
rization. The goal of our comparison was to assess
whether these summary related errors affect tradi-
tional news texts based non-opinionated summaries
differently than opinionated blog summaries.
We first analyzed and categorized errors that oc-
cur in opinion summarization on blogs using the sum-
maries from participating systems at the TAC 2008
opinion summarization track. Then we compared
these results with those of the TAC 2008 updated sum-
marization track. Our results show that all types of
summary related errors occur more often in blog sum-
marization than news texts summarization. However,
topic and question irrelevancy pose a much greater
problem for blog summarization than for traditional
news texts; while content overlap and missing infor-
mation seem to be only slightly more frequent in blog
than traditional news texts. These findings can be
used to prioritize these error types and give clear in-
dications as to where we should put effort to improve
blog summarization.
</bodyText>
<sectionHeader confidence="0.998924" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99656575">
The authors would like to thank the anonymous refer-
ees for their valuable comments on an earlier version
of the paper.
This work was financially supported by NSERC.
</bodyText>
<page confidence="0.999224">
7
</page>
<sectionHeader confidence="0.988925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999904278481013">
[1] Text Analysis Conference (TAC): http://www.nist.gov/tac.
(Last accessed 2009-05-20).
[2] Document Understanding Conferences (DUC):
http://duc.nist.gov. (Last accessed 2009-05-20).
[3] Third International AAAI Conference on Weblogs and Social
Media, San Jose, California, May 2009.
[4] Third Annual Workshop on the Weblogging Ecosystem: Ag-
gregation, Analysis, and Dynamics. In Workshop Of WWW-
2006, Edinburgh, May 2006.
[5] A. Andreevskaia, S. Bergler, and M. Urseanu. All Blogs are
Not Made Equal: Exploring Genre Differences in Sentiment
Tagging of Blogs. In Proceedings of the International Confer-
ence on Weblogs and Social Media (ICWSM-2007), Boulder,
Colorado, March 2007.
[6] A. Bossard and M. Genereux. Description of the LIPN Sys-
tems at TAC 2008: Summarizing Information and Opinions.
In Notebook Papers and Results, Text Analysis Conference
(TAC-2008), Gaithersburg, Maryland, USA, November 2008.
[7] N. Godbole, M. Srinivasaiah, and S. Skiena. Large-Scale
Sentiment Analysis for News and Blogs. In Proceedings of
the International Conference on Weblogs and Social Me-
dia (ICWSM’2007), pages 219–222, Boulder, Colorado, USA,
March 2007.
[8] E. H. Hovy. Automated Discourse Generation using Discourse
Structure Relations. Artificial Intelligence, 63(1-2):341–385,
1993.
[9] M. Hu and B. Liu. Mining and Summarizing Customer Re-
views. In SIGKDD 2004, pages 168–177, 2004.
[10] L. W. Ku, Y. T. Liang, and H. H. Chen. Opinion Extraction,
Summarization and Tracking in News and Blog Corpora. In
Proceedings of the AAAI-2006 Spring Symposium on Compu-
tational Approaches to Analyzing Weblogs, California, USA,
March 2006.
[11] C. Y. Lin. ROUGE: A Package for Automatic Evaluation of
Summaries. In Text Summarization Branches Out: Proceed-
ings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain,
July 2004.
[12] M. Liu, W. Li, M. Wu, and H. Hu. Event-Based Extractive
Summarization using Event Semantic Relevance from External
Linguistic Resource. In Proceedings of the Sixth International
Conference on Advanced Language Processing and Web In-
formation Technology, ALPIT 2007, pages 117–122, Henan,
China, 2007.
[13] L. Lloyd, P. Kaulgud, and S. Skiena. Newspapers vs. Blogs:
Who Gets the Scoop? In Proceedings of the AAAI Spring
Symposium on Computational Approaches to Analyzing We-
blogs, 2006, California, USA, March 2006.
[14] A. Louis and A. Nenkova. Automatic Summary Evaluation
without Human Models. In Notebook Papers and Results,
Text Analysis Conference (TAC-2008), Gaithersburg, Mary-
land (USA), November 2008.
[15] C. Macdonald, I. Ounis, and I. Soboroff. Overview of the TREC
2007 Blog Track. In Proceedings of the Sixteenth Text RE-
trieval Conference (TREC 2007), Gaithersburg, Maryland,
USA, November 2007.
[16] G. Mishne and N. Glance. Predicting Movie Sales from Blog-
ger Sentiment. In Proceedings of the AAAI 2006 Spring Sym-
posium on Computational Approaches to Analysing Weblogs
(AAAI-CAAW 2006), 2006.
[17] A. Nenkova. Summarization Evaluation for Text and Speech:
Issues and Approaches. In Proceedings of Interspeech 2006,
Pittsburg, USA, 2006.
[18] A. Nenkova and R. Passonneau. Evaluating Content Selection
in Summarization: The Pyramid Method. In Proceedings of
the HLT/NAACL, 2004.
[19] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Techniques. In
Proceedings of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP), 2002.
[20] S. Somasundaran, T. Wilson, J. Wiebe, and V. Stoyanov. QA
with Attitude: Exploiting Opinion Type Analysis for Improv-
ing Question Answering in On-line Discussions and the News.
In Proceedings of the International Conference on Weblogs
and Social Media, Boulder, Colorado, USA, March 2007.
[21] H. Yang, T. S. Chua, S. Wang, and C. K. Koh. Structured use
of External Knowledge for Event-based Open Domain Question
Answering. In Proceedings of the 26th Annual International
ACM SIGIR Conference on Research and Development in
Information Retrieval, pages 33–40, Toronto, Canada, 2003.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.310170">
<title confidence="0.999277">Summarizing Blog Entries versus News Texts</title>
<author confidence="0.771726">Shamima Mithun</author>
<author confidence="0.771726">Leila</author>
<affiliation confidence="0.7535065">Concordia Department of Computer Science and Software</affiliation>
<address confidence="0.93471">Montreal, Quebec,</address>
<email confidence="0.994754">mithun,</email>
<abstract confidence="0.999925483870968">As more and more people are expressing their opinions on the web in the form of weblogs (or blogs), research on the blogosphere is gaining popularity. As the outcome of this research, different natural language tools such as querybased opinion summarizers have been developed to mine and organize opinions on a particular event or entity in blog entries. However, the variety of blog posts and the informal style and structure of blog entries pose many difficulties for these natural language tools. In this paper, we identify and categorize errors which typically occur in opinion summarization from blog entries and compare blog entry summaries with traditional news text summaries based on these error types to quantify the differences between these two genres of texts for the purpose of summarization. For evaluation, we used summaries from participating systems of the TAC 2008 opinion summarization track and updated summarization track. Our results show that some errors are much more frequent to blog entries (e.g. topic irrelevant information) compared to news texts; while other error types, such as content overlap, seem to be comparable. These findings can be used to prioritize these error types and give clear indications as to where we should put effort to improve blog summarization.</abstract>
<keyword confidence="0.973197">Keywords Opinion summarization, blog summarization, news text sum-</keyword>
<intro confidence="0.692974">marization.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Text Analysis Conference (TAC): http://www.nist.gov/tac. (Last accessed</booktitle>
<pages>2009--05</pages>
<contexts>
<context position="3660" citStr="[1]" startWordPosition="572" endWordPosition="572">atically generated summaries of blog entries with summaries of news texts with the goal of improving opinion summarization from blog entries. In particular, we compared summaries for these two genres of texts on the basis of various errors which typically occur in summarization. In this paper, we first investigate what kind of errors typically occur in query-based opinionated summary for blog entries. The errors that we have identified are categorized and then used to compare blog summaries with news texts summaries. For evaluation, we used summaries from participating systems at the TAC 2008 [1] opinion summarization track and updated summarization track. Summaries of the TAC 2008 opinion summarization track and updated summarization track were generated from blogs entries and traditional news texts, respectively. The systems participating in the TAC opinion summarization track and in the updated summarization track are quite different in several aspects, as they are targeted to resolve two different tasks. The systems participating in the updated summarization track were mainly required to find the answers to given queries and detect redundant information while the systems participa</context>
<context position="28678" citStr="[1, 3, 4, 15]" startWordPosition="4577" endWordPosition="4580">, on the other hand, seem to be only slightly more frequent (5% and 7%) in blogs summaries than in news texts summaries. These results give a clear idea of how difficult it is to process blog entries for summarization compared to news texts and where efforts should be made to improve such summaries. 6 Related Work 6.1 NLP on blogs Recently, the availability of opinions on current events on weblogs opened up new directions in natural language research. Even though natural language processing on blogs is a fairly new trend, its popularity is growing rapidly. Many conferences and workshops (e.g. [1, 3, 4, 15]) are taking place to address different aspects of the analysis of blog entries. Current NLP work on blog entries include: subjectivity and sentiment analysis; question answering; and opinion summarization. Subjectivity and sentiment analysis include classifying sentiments of reviews [19] and analyzing blogger mood and sentiment on various events [16]. Sentiment classification of reviews on different events is often done on movie or product reviews. Rating indicators of reviews are used to identify the polarity of the blogs namely positive, negative or neutral. To analyze blogger mood and sent</context>
<context position="29916" citStr="[1]" startWordPosition="4783" endWordPosition="4783">ion regarding bloggers’ mood varying over time. To record bloggers’ varying mood, the polarity information of the blog post is often used. Some works (e.g. [16]) are done to measure how bloggers’ varying mood affects different events. In addition, the TREC 6 blog track [15] provides an opportunity to build new techniques of sentiment tagging on blog posts. The task is to identify and rank blog posts on a given topic from a corpus of blog entries. Question answering (QA) on blog entries is a relatively new field. Most notable QA work on blog entries was conducted at TREC 2007 [15] and TAC 2008 [1]. To answer queries on an event or entity, TREC provided a blog corpus in addition to the AQUAINT newspaper corpus [15]. 6.2 Analysis of blogs versus news To the best of our knowledge there have been only a few work carried out to compare the difference between blog entries and news texts; however, none seems to have analyzed it at the linguistic level for a specific NLP application. Ku et al. [10] developed a language independent opinion summarization approach. For summarization, they retrieved all sentences which are relevant to the main topic of the document set and determined the opinion p</context>
</contexts>
<marker>[1]</marker>
<rawString>Text Analysis Conference (TAC): http://www.nist.gov/tac. (Last accessed 2009-05-20).</rawString>
</citation>
<citation valid="false">
<title>Document Understanding Conferences (DUC): http://duc.nist.gov.</title>
<journal>Last</journal>
<volume>accessed</volume>
<pages>2009--05</pages>
<contexts>
<context position="10741" citStr="[2]" startWordPosition="1669" endWordPosition="1669">mmary is assessed mostly on its content and linguistic quality [14]. Content evaluation of a querybased summary is performed based on the relevance assessment (with the topic and query) and inclusion of important contents from the input documents. Currently, the automatic evaluation tool ROUGE [11] is the most popular evaluation approach for content evaluation. ROUGE automatically compares system generated summaries with a set of 2 model summaries (human generated) by computing n-gram word overlaps between them. Conferences and workshops such as TAC and DUC (Document Understanding Conference) [2] use ROUGE. The pyramid method [18] is also used for content evaluation. In the pyramid method, multiple human generated summaries are analyzed manually to generated a gold standard. In this process, summary analysis is done semantically such that information with the same meaning (expressed using different wording) is marked as summary content unit (SCU). A weight is assigned for each SCU based on the number of human summarizers that express it in their summaries. In this method, the pyramid score for a system generated summary is calculated as follows [17]: score = (the sum of weights of SCU</context>
</contexts>
<marker>[2]</marker>
<rawString>Document Understanding Conferences (DUC): http://duc.nist.gov. (Last accessed 2009-05-20).</rawString>
</citation>
<citation valid="true">
<date>2009</date>
<booktitle>Third International AAAI Conference on Weblogs and Social Media,</booktitle>
<location>San Jose, California,</location>
<contexts>
<context position="28678" citStr="[1, 3, 4, 15]" startWordPosition="4577" endWordPosition="4580">, on the other hand, seem to be only slightly more frequent (5% and 7%) in blogs summaries than in news texts summaries. These results give a clear idea of how difficult it is to process blog entries for summarization compared to news texts and where efforts should be made to improve such summaries. 6 Related Work 6.1 NLP on blogs Recently, the availability of opinions on current events on weblogs opened up new directions in natural language research. Even though natural language processing on blogs is a fairly new trend, its popularity is growing rapidly. Many conferences and workshops (e.g. [1, 3, 4, 15]) are taking place to address different aspects of the analysis of blog entries. Current NLP work on blog entries include: subjectivity and sentiment analysis; question answering; and opinion summarization. Subjectivity and sentiment analysis include classifying sentiments of reviews [19] and analyzing blogger mood and sentiment on various events [16]. Sentiment classification of reviews on different events is often done on movie or product reviews. Rating indicators of reviews are used to identify the polarity of the blogs namely positive, negative or neutral. To analyze blogger mood and sent</context>
</contexts>
<marker>[3]</marker>
<rawString>Third International AAAI Conference on Weblogs and Social Media, San Jose, California, May 2009.</rawString>
</citation>
<citation valid="true">
<title>Third Annual Workshop on the Weblogging Ecosystem: Aggregation, Analysis, and Dynamics.</title>
<date>2006</date>
<booktitle>In Workshop Of WWW2006,</booktitle>
<location>Edinburgh,</location>
<contexts>
<context position="28678" citStr="[1, 3, 4, 15]" startWordPosition="4577" endWordPosition="4580">, on the other hand, seem to be only slightly more frequent (5% and 7%) in blogs summaries than in news texts summaries. These results give a clear idea of how difficult it is to process blog entries for summarization compared to news texts and where efforts should be made to improve such summaries. 6 Related Work 6.1 NLP on blogs Recently, the availability of opinions on current events on weblogs opened up new directions in natural language research. Even though natural language processing on blogs is a fairly new trend, its popularity is growing rapidly. Many conferences and workshops (e.g. [1, 3, 4, 15]) are taking place to address different aspects of the analysis of blog entries. Current NLP work on blog entries include: subjectivity and sentiment analysis; question answering; and opinion summarization. Subjectivity and sentiment analysis include classifying sentiments of reviews [19] and analyzing blogger mood and sentiment on various events [16]. Sentiment classification of reviews on different events is often done on movie or product reviews. Rating indicators of reviews are used to identify the polarity of the blogs namely positive, negative or neutral. To analyze blogger mood and sent</context>
</contexts>
<marker>[4]</marker>
<rawString>Third Annual Workshop on the Weblogging Ecosystem: Aggregation, Analysis, and Dynamics. In Workshop Of WWW2006, Edinburgh, May 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Andreevskaia</author>
<author>S Bergler</author>
<author>M Urseanu</author>
</authors>
<title>All Blogs are Not Made Equal: Exploring Genre Differences in Sentiment Tagging of Blogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media (ICWSM-2007),</booktitle>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5370" citStr="[5]" startWordPosition="830" endWordPosition="830">n Emerging Text Types (eETTs) - Borovets, Bulgaria, pages 1–8 2 Characteristics of Blogs Blogs (or weblogs) are online diaries that appear in chronological order. Blogs reflect personal thinking and feelings on all kinds of topics including day to day activities of bloggers; hence an essential feature of blogs is their subjectivity. Some blogs focus on a specific topic while others cover several topics; some describe personal daily lives of bloggers while others describe common artifacts or news. Many different sub-genres of blogs exist. The two most common are personal journals and notebooks [5]. Personal journals discuss internal experiences and personal lives of bloggers and tend to be short [5]. They are usually informal in nature and written in casual and informal language. They may contain much and sometimes only unrelated information such as ads, photos, and other non-textual elements. They also contain spelling and grammatical errors, and punctuation and capitalization are often missing. On the other hand, notebooks contain comments on internal and external events. Similarly to newspaper articles, they are usually long and written in a more formal style [5]. Most NLP work on b</context>
</contexts>
<marker>[5]</marker>
<rawString>A. Andreevskaia, S. Bergler, and M. Urseanu. All Blogs are Not Made Equal: Exploring Genre Differences in Sentiment Tagging of Blogs. In Proceedings of the International Conference on Weblogs and Social Media (ICWSM-2007), Boulder, Colorado, March 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bossard</author>
<author>M Genereux</author>
</authors>
<title>Description of the LIPN Systems at TAC 2008: Summarizing Information and Opinions.</title>
<date>2008</date>
<booktitle>In Notebook Papers and Results, Text Analysis Conference (TAC-2008),</booktitle>
<location>Gaithersburg, Maryland, USA,</location>
<contexts>
<context position="14542" citStr="[6]" startWordPosition="2275" endWordPosition="2275">gs may contain many unrelated information such as ads, photos, music, videos. For blogs, it is often difficult to find sentence boundaries. In most cases punctuation and capitalization are unreliable. As a result, for blog summarization, systems need to put additional efforts to pre-process the text compared to news text summarization. Furthermore, because blogs do not exhibit a stereotypical structure, some features such as position of sentence, or similarity with the first sentence, which are shown to be useful for traditional news text summarization are not as useful for blog summarization [6]. 4 Error Analysis To identify the different challenges posed by blog summarization as opposed to traditional news texts summarization, we have studied 50 summaries from participating systems at the TAC 2008 opinion summarization track and compared these to 50 summaries from the TAC 2008 updated summarization tracks. The average summary length of the opinion summarization track was 1224 words, while that of the updated summarization track was 179 words. The average input documents length of the opinion summarization track was 1888 words, while that of the updated summarization track was 505 wo</context>
</contexts>
<marker>[6]</marker>
<rawString>A. Bossard and M. Genereux. Description of the LIPN Systems at TAC 2008: Summarizing Information and Opinions. In Notebook Papers and Results, Text Analysis Conference (TAC-2008), Gaithersburg, Maryland, USA, November 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Godbole</author>
<author>M Srinivasaiah</author>
<author>S Skiena</author>
</authors>
<title>Large-Scale Sentiment Analysis for News and Blogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media (ICWSM’2007),</booktitle>
<pages>219--222</pages>
<location>Boulder, Colorado, USA,</location>
<contexts>
<context position="32482" citStr="[7]" startWordPosition="5200" endWordPosition="5200">uency time series of 197 most popular entries in news texts and blog corpora over six week period. Lydia first recognized name entities to extract information from both corpora. Then the system resolved noun phrase coreference because a single entity is often mentioned using multiple variations on their name. Then it performed a temporal analysis to identify which entities are referred more frequently over a certain period of time. In their analysis, they found that 30 entities exhibited no lead/lag relationship, 73 had news leading the blogs, and 94 had blogs leading the news. Godbole et al. [7] developed a large-scale sentiment analysis system on top of the Lydia text analysis system [13] for news texts and blog entities. They determined the public sentiment on various entities and identified how this sentiment varies with time. They found that the same entities (person) except certain controversial political figures received comparable opinions (favorable or adverse) in blogs and news texts. Controversial political figures received different opinions in blogs compared to news texts because of the political biases among bloggers, and perhaps the mainstream press. Though both the wor</context>
</contexts>
<marker>[7]</marker>
<rawString>N. Godbole, M. Srinivasaiah, and S. Skiena. Large-Scale Sentiment Analysis for News and Blogs. In Proceedings of the International Conference on Weblogs and Social Media (ICWSM’2007), pages 219–222, Boulder, Colorado, USA, March 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
</authors>
<title>Automated Discourse Generation using Discourse Structure Relations.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--1</pages>
<contexts>
<context position="16594" citStr="[8]" startWordPosition="2606" endWordPosition="2606">) These are shown in Figure 1 and discussed in the following sub-sections. Fig. 1: Types of Errors in Blog vs. News Summaries 4.1 Summary-Level Errors We define a Summary-Level Error (SuLE) as the textual contents which reduce the understandability and readability of the overall summary. There are two types of SuLE: 1. Discourse Incoherency (DI) 2. Content Overlap (CO) Discourse Incoherency (DI) A summary will exhibit a Discourse Incoherency (DI) if the reader cannot identify the communicative intentions of the writer from the propositions or if the propositions do not seem to be interrelated [8]. In the sample summaries that we have studied, Discourse Incoherency occurred both at the sentence level and at the proposition level. Consider the following summary (ID:T1004.20 1) where a DI occurs at the sentence level: Topic: Starbucks coffee shops Question: Why do people like Starbucks better than Dunkin Donuts? Summary: I am firmly in the Dunkin’ Donuts camp. It’s a smooth, soothing cuppa, with no disastrous gastric side effects, very comforting indeed. I have a special relationship with the lovely people who work in the Dunkin’ Donuts in the Harvard Square T Station in Cambridge. I was</context>
</contexts>
<marker>[8]</marker>
<rawString>E. H. Hovy. Automated Discourse Generation using Discourse Structure Relations. Artificial Intelligence, 63(1-2):341–385, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and Summarizing Customer Reviews.</title>
<date>2004</date>
<booktitle>In SIGKDD 2004,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="6269" citStr="[9, 10]" startWordPosition="975" endWordPosition="976"> elements. They also contain spelling and grammatical errors, and punctuation and capitalization are often missing. On the other hand, notebooks contain comments on internal and external events. Similarly to newspaper articles, they are usually long and written in a more formal style [5]. Most NLP work on blogs has tended to study personal journals as opposed to notebooks. For example, the Blog06 corpus [15], used at TREC and at TAC, contains mostly personal journals. 3 Blog Summarization Opinion summarization, and in particular blog summarization, is a fairly recent field. Some systems (e.g. [9, 10]) have been developed for opinion summarization to generate a summary from a document. In 2008, the Text Analysis Conference (TAC) introduced a querybased opinion summarization track. They provided questions, a blog corpus and optional snippets which are found by QA systems. These query-based summarization systems are designed to retrieve specific answers on an event or entity instead of an overview of the whole document. Opinion summarization uses opinionated documents such as blogs, reviews, newspaper editorials or letters to the editor to answer opinionated questions. On the other hand, sum</context>
</contexts>
<marker>[9]</marker>
<rawString>M. Hu and B. Liu. Mining and Summarizing Customer Reviews. In SIGKDD 2004, pages 168–177, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L W Ku</author>
<author>Y T Liang</author>
<author>H H Chen</author>
</authors>
<title>Opinion Extraction, Summarization and Tracking in News and Blog Corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI-2006 Spring Symposium on Computational Approaches to Analyzing Weblogs,</booktitle>
<location>California, USA,</location>
<contexts>
<context position="6269" citStr="[9, 10]" startWordPosition="975" endWordPosition="976"> elements. They also contain spelling and grammatical errors, and punctuation and capitalization are often missing. On the other hand, notebooks contain comments on internal and external events. Similarly to newspaper articles, they are usually long and written in a more formal style [5]. Most NLP work on blogs has tended to study personal journals as opposed to notebooks. For example, the Blog06 corpus [15], used at TREC and at TAC, contains mostly personal journals. 3 Blog Summarization Opinion summarization, and in particular blog summarization, is a fairly recent field. Some systems (e.g. [9, 10]) have been developed for opinion summarization to generate a summary from a document. In 2008, the Text Analysis Conference (TAC) introduced a querybased opinion summarization track. They provided questions, a blog corpus and optional snippets which are found by QA systems. These query-based summarization systems are designed to retrieve specific answers on an event or entity instead of an overview of the whole document. Opinion summarization uses opinionated documents such as blogs, reviews, newspaper editorials or letters to the editor to answer opinionated questions. On the other hand, sum</context>
<context position="30317" citStr="[10]" startWordPosition="4855" endWordPosition="4855">ts on a given topic from a corpus of blog entries. Question answering (QA) on blog entries is a relatively new field. Most notable QA work on blog entries was conducted at TREC 2007 [15] and TAC 2008 [1]. To answer queries on an event or entity, TREC provided a blog corpus in addition to the AQUAINT newspaper corpus [15]. 6.2 Analysis of blogs versus news To the best of our knowledge there have been only a few work carried out to compare the difference between blog entries and news texts; however, none seems to have analyzed it at the linguistic level for a specific NLP application. Ku et al. [10] developed a language independent opinion summarization approach. For summarization, they retrieved all sentences which are relevant to the main topic of the document set and determined the opinion polarity and degree of these relevant sentences. They also found that the identification of correlated events on a time interval is also important for opinion summarization. They tested their approach for blog entries and news texts for English and Chinese languages. From their evaluation, they found that blog entries contain more topic irrelevant information compared to news texts. Their results co</context>
</contexts>
<marker>[10]</marker>
<rawString>L. W. Ku, Y. T. Liang, and H. H. Chen. Opinion Extraction, Summarization and Tracking in News and Blog Corpora. In Proceedings of the AAAI-2006 Spring Symposium on Computational Approaches to Analyzing Weblogs, California, USA, March 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ROUGE</author>
</authors>
<title>A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="10437" citStr="[11]" startWordPosition="1624" endWordPosition="1624">ies and different machine learning techniques are used to identify the polarity of the question and sentences. Finally, the summaries are generated using the ranked sentences. 3.2 Evaluation Evaluation of blog summaries use the same criteria as for traditional news text summarization. The quality of a summary is assessed mostly on its content and linguistic quality [14]. Content evaluation of a querybased summary is performed based on the relevance assessment (with the topic and query) and inclusion of important contents from the input documents. Currently, the automatic evaluation tool ROUGE [11] is the most popular evaluation approach for content evaluation. ROUGE automatically compares system generated summaries with a set of 2 model summaries (human generated) by computing n-gram word overlaps between them. Conferences and workshops such as TAC and DUC (Document Understanding Conference) [2] use ROUGE. The pyramid method [18] is also used for content evaluation. In the pyramid method, multiple human generated summaries are analyzed manually to generated a gold standard. In this process, summary analysis is done semantically such that information with the same meaning (expressed usi</context>
</contexts>
<marker>[11]</marker>
<rawString>C. Y. Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liu</author>
<author>W Li</author>
<author>M Wu</author>
<author>H Hu</author>
</authors>
<title>Event-Based Extractive Summarization using Event Semantic Relevance from External Linguistic Resource.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixth International Conference on Advanced Language Processing and Web Information Technology, ALPIT</booktitle>
<pages>117--122</pages>
<location>Henan, China,</location>
<contexts>
<context position="2352" citStr="[12]" startWordPosition="361" endWordPosition="361">the web in weblogs (or blogs), wikis, online-forums, review sites, and social networking web sites. As more and more people are expressing their opinions on the web, the Internet is becoming a popular and dynamic source of opinions. Natural language tools for automatically mining and organizing these opinions on various events will be very useful for individuals, organizations, and governments. Various natural language tools to process and utilize event-related information from texts have already been developed. Event-based question answering systems [21] and event-based summarization systems [12] are only a few examples. However, most of the event-based systems have been developed to process events from traditional news texts. Blog entries are different in style and structure compared to news texts. As a result, successful natural language approaches that deal with news texts might not be as successful for processing blog entries; thus adaptation of existing successful NLP approaches for news texts to process blog entries is an interesting and challenging task. The first step towards this adaptation is to identify the differences between these two textual genres in order to develop ap</context>
</contexts>
<marker>[12]</marker>
<rawString>M. Liu, W. Li, M. Wu, and H. Hu. Event-Based Extractive Summarization using Event Semantic Relevance from External Linguistic Resource. In Proceedings of the Sixth International Conference on Advanced Language Processing and Web Information Technology, ALPIT 2007, pages 117–122, Henan, China, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lloyd</author>
<author>P Kaulgud</author>
<author>S Skiena</author>
</authors>
<title>Newspapers vs. Blogs: Who Gets the Scoop?</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Computational Approaches to Analyzing Weblogs,</booktitle>
<location>California, USA,</location>
<contexts>
<context position="31554" citStr="[13]" startWordPosition="5047" endWordPosition="5047">also found that news texts use a larger vocabulary compared to blog entries which makes the filtering of non-relevant sentences task harder for news texts. On the other hand, this larger vocabulary helps to decide sentiment polarities. Due to the limited vocabulary the judgment of sentiment polarity of blog entries was difficult. Somasundaran et al. [20] developed an opinion question answering approach for blogs and news texts. They exploited attitude information namely sentiment and argument types to answer opinion questions. They received comparable result with both text types. Lloyd et al. [13] developed the Lydia system to analyze blog entries. They analyzed temporal relationship between blogs and news texts. In particular, they analyzed how often bloggers report a story before newspapers and how often bloggers react to news that has already been reported. To study this leads/lag relationship, they analyzed frequency time series of 197 most popular entries in news texts and blog corpora over six week period. Lydia first recognized name entities to extract information from both corpora. Then the system resolved noun phrase coreference because a single entity is often mentioned using</context>
</contexts>
<marker>[13]</marker>
<rawString>L. Lloyd, P. Kaulgud, and S. Skiena. Newspapers vs. Blogs: Who Gets the Scoop? In Proceedings of the AAAI Spring Symposium on Computational Approaches to Analyzing Weblogs, 2006, California, USA, March 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Automatic Summary Evaluation without Human Models.</title>
<date>2008</date>
<booktitle>In Notebook Papers and Results, Text Analysis Conference (TAC-2008),</booktitle>
<location>Gaithersburg, Maryland (USA),</location>
<contexts>
<context position="10205" citStr="[14]" startWordPosition="1589" endWordPosition="1589">calculated and matched with the polarity of the query. To find the relevance with the query, overlap with the query terms is calculated using different techniques such as the cosine similarity, language models etc. Opinion dictionaries and different machine learning techniques are used to identify the polarity of the question and sentences. Finally, the summaries are generated using the ranked sentences. 3.2 Evaluation Evaluation of blog summaries use the same criteria as for traditional news text summarization. The quality of a summary is assessed mostly on its content and linguistic quality [14]. Content evaluation of a querybased summary is performed based on the relevance assessment (with the topic and query) and inclusion of important contents from the input documents. Currently, the automatic evaluation tool ROUGE [11] is the most popular evaluation approach for content evaluation. ROUGE automatically compares system generated summaries with a set of 2 model summaries (human generated) by computing n-gram word overlaps between them. Conferences and workshops such as TAC and DUC (Document Understanding Conference) [2] use ROUGE. The pyramid method [18] is also used for content eva</context>
</contexts>
<marker>[14]</marker>
<rawString>A. Louis and A. Nenkova. Automatic Summary Evaluation without Human Models. In Notebook Papers and Results, Text Analysis Conference (TAC-2008), Gaithersburg, Maryland (USA), November 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macdonald</author>
<author>I Ounis</author>
<author>I Soboroff</author>
</authors>
<title>Overview of the TREC</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixteenth Text REtrieval Conference (TREC 2007),</booktitle>
<location>Gaithersburg, Maryland, USA,</location>
<contexts>
<context position="6073" citStr="[15]" startWordPosition="945" endWordPosition="945">t [5]. They are usually informal in nature and written in casual and informal language. They may contain much and sometimes only unrelated information such as ads, photos, and other non-textual elements. They also contain spelling and grammatical errors, and punctuation and capitalization are often missing. On the other hand, notebooks contain comments on internal and external events. Similarly to newspaper articles, they are usually long and written in a more formal style [5]. Most NLP work on blogs has tended to study personal journals as opposed to notebooks. For example, the Blog06 corpus [15], used at TREC and at TAC, contains mostly personal journals. 3 Blog Summarization Opinion summarization, and in particular blog summarization, is a fairly recent field. Some systems (e.g. [9, 10]) have been developed for opinion summarization to generate a summary from a document. In 2008, the Text Analysis Conference (TAC) introduced a querybased opinion summarization track. They provided questions, a blog corpus and optional snippets which are found by QA systems. These query-based summarization systems are designed to retrieve specific answers on an event or entity instead of an overview o</context>
<context position="28678" citStr="[1, 3, 4, 15]" startWordPosition="4577" endWordPosition="4580">, on the other hand, seem to be only slightly more frequent (5% and 7%) in blogs summaries than in news texts summaries. These results give a clear idea of how difficult it is to process blog entries for summarization compared to news texts and where efforts should be made to improve such summaries. 6 Related Work 6.1 NLP on blogs Recently, the availability of opinions on current events on weblogs opened up new directions in natural language research. Even though natural language processing on blogs is a fairly new trend, its popularity is growing rapidly. Many conferences and workshops (e.g. [1, 3, 4, 15]) are taking place to address different aspects of the analysis of blog entries. Current NLP work on blog entries include: subjectivity and sentiment analysis; question answering; and opinion summarization. Subjectivity and sentiment analysis include classifying sentiments of reviews [19] and analyzing blogger mood and sentiment on various events [16]. Sentiment classification of reviews on different events is often done on movie or product reviews. Rating indicators of reviews are used to identify the polarity of the blogs namely positive, negative or neutral. To analyze blogger mood and sent</context>
<context position="29899" citStr="[15]" startWordPosition="4779" endWordPosition="4779">ke use of information regarding bloggers’ mood varying over time. To record bloggers’ varying mood, the polarity information of the blog post is often used. Some works (e.g. [16]) are done to measure how bloggers’ varying mood affects different events. In addition, the TREC 6 blog track [15] provides an opportunity to build new techniques of sentiment tagging on blog posts. The task is to identify and rank blog posts on a given topic from a corpus of blog entries. Question answering (QA) on blog entries is a relatively new field. Most notable QA work on blog entries was conducted at TREC 2007 [15] and TAC 2008 [1]. To answer queries on an event or entity, TREC provided a blog corpus in addition to the AQUAINT newspaper corpus [15]. 6.2 Analysis of blogs versus news To the best of our knowledge there have been only a few work carried out to compare the difference between blog entries and news texts; however, none seems to have analyzed it at the linguistic level for a specific NLP application. Ku et al. [10] developed a language independent opinion summarization approach. For summarization, they retrieved all sentences which are relevant to the main topic of the document set and determi</context>
</contexts>
<marker>[15]</marker>
<rawString>C. Macdonald, I. Ounis, and I. Soboroff. Overview of the TREC 2007 Blog Track. In Proceedings of the Sixteenth Text REtrieval Conference (TREC 2007), Gaithersburg, Maryland, USA, November 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mishne</author>
<author>N Glance</author>
</authors>
<title>Predicting Movie Sales from Blogger Sentiment.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI 2006 Spring Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW</booktitle>
<contexts>
<context position="29031" citStr="[16]" startWordPosition="4632" endWordPosition="4632"> opinions on current events on weblogs opened up new directions in natural language research. Even though natural language processing on blogs is a fairly new trend, its popularity is growing rapidly. Many conferences and workshops (e.g. [1, 3, 4, 15]) are taking place to address different aspects of the analysis of blog entries. Current NLP work on blog entries include: subjectivity and sentiment analysis; question answering; and opinion summarization. Subjectivity and sentiment analysis include classifying sentiments of reviews [19] and analyzing blogger mood and sentiment on various events [16]. Sentiment classification of reviews on different events is often done on movie or product reviews. Rating indicators of reviews are used to identify the polarity of the blogs namely positive, negative or neutral. To analyze blogger mood and sentiment, systems make use of information regarding bloggers’ mood varying over time. To record bloggers’ varying mood, the polarity information of the blog post is often used. Some works (e.g. [16]) are done to measure how bloggers’ varying mood affects different events. In addition, the TREC 6 blog track [15] provides an opportunity to build new techni</context>
</contexts>
<marker>[16]</marker>
<rawString>G. Mishne and N. Glance. Predicting Movie Sales from Blogger Sentiment. In Proceedings of the AAAI 2006 Spring Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW 2006), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
</authors>
<title>Summarization Evaluation for Text and Speech: Issues and Approaches.</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<location>Pittsburg, USA,</location>
<contexts>
<context position="11305" citStr="[17]" startWordPosition="1759" endWordPosition="1759"> (Document Understanding Conference) [2] use ROUGE. The pyramid method [18] is also used for content evaluation. In the pyramid method, multiple human generated summaries are analyzed manually to generated a gold standard. In this process, summary analysis is done semantically such that information with the same meaning (expressed using different wording) is marked as summary content unit (SCU). A weight is assigned for each SCU based on the number of human summarizers that express it in their summaries. In this method, the pyramid score for a system generated summary is calculated as follows [17]: score = (the sum of weights of SCUs expressed in a generated summary) / (the sum of weights of an ideally informative summary with the same number of SCUs) The linguistic quality of a summary is evaluated manually based on how it structures and presents the contents. Grammaticality, non-redundancy, referential clarity, focus, structure and coherence are the commonly used factors considered to evaluate the linguistic quality. Mainly, subjective evaluation is done to assess the linguistic quality of an automatically generated summary. In this process, human assessors directly assign scores on </context>
</contexts>
<marker>[17]</marker>
<rawString>A. Nenkova. Summarization Evaluation for Text and Speech: Issues and Approaches. In Proceedings of Interspeech 2006, Pittsburg, USA, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating Content Selection in Summarization: The Pyramid Method.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT/NAACL,</booktitle>
<contexts>
<context position="10776" citStr="[18]" startWordPosition="1675" endWordPosition="1675">ntent and linguistic quality [14]. Content evaluation of a querybased summary is performed based on the relevance assessment (with the topic and query) and inclusion of important contents from the input documents. Currently, the automatic evaluation tool ROUGE [11] is the most popular evaluation approach for content evaluation. ROUGE automatically compares system generated summaries with a set of 2 model summaries (human generated) by computing n-gram word overlaps between them. Conferences and workshops such as TAC and DUC (Document Understanding Conference) [2] use ROUGE. The pyramid method [18] is also used for content evaluation. In the pyramid method, multiple human generated summaries are analyzed manually to generated a gold standard. In this process, summary analysis is done semantically such that information with the same meaning (expressed using different wording) is marked as summary content unit (SCU). A weight is assigned for each SCU based on the number of human summarizers that express it in their summaries. In this method, the pyramid score for a system generated summary is calculated as follows [17]: score = (the sum of weights of SCUs expressed in a generated summary)</context>
</contexts>
<marker>[18]</marker>
<rawString>A. Nenkova and R. Passonneau. Evaluating Content Selection in Summarization: The Pyramid Method. In Proceedings of the HLT/NAACL, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="28967" citStr="[19]" startWordPosition="4621" endWordPosition="4621">s. 6 Related Work 6.1 NLP on blogs Recently, the availability of opinions on current events on weblogs opened up new directions in natural language research. Even though natural language processing on blogs is a fairly new trend, its popularity is growing rapidly. Many conferences and workshops (e.g. [1, 3, 4, 15]) are taking place to address different aspects of the analysis of blog entries. Current NLP work on blog entries include: subjectivity and sentiment analysis; question answering; and opinion summarization. Subjectivity and sentiment analysis include classifying sentiments of reviews [19] and analyzing blogger mood and sentiment on various events [16]. Sentiment classification of reviews on different events is often done on movie or product reviews. Rating indicators of reviews are used to identify the polarity of the blogs namely positive, negative or neutral. To analyze blogger mood and sentiment, systems make use of information regarding bloggers’ mood varying over time. To record bloggers’ varying mood, the polarity information of the blog post is often used. Some works (e.g. [16]) are done to measure how bloggers’ varying mood affects different events. In addition, the TR</context>
</contexts>
<marker>[19]</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sentiment Classification using Machine Learning Techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>V Stoyanov</author>
</authors>
<title>QA with Attitude: Exploiting Opinion Type Analysis for Improving Question Answering in On-line Discussions and the News.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social</booktitle>
<location>Media, Boulder, Colorado, USA,</location>
<contexts>
<context position="31306" citStr="[20]" startWordPosition="5010" endWordPosition="5010">heir approach for blog entries and news texts for English and Chinese languages. From their evaluation, they found that blog entries contain more topic irrelevant information compared to news texts. Their results confirm our own results. Ku et al. also found that news texts use a larger vocabulary compared to blog entries which makes the filtering of non-relevant sentences task harder for news texts. On the other hand, this larger vocabulary helps to decide sentiment polarities. Due to the limited vocabulary the judgment of sentiment polarity of blog entries was difficult. Somasundaran et al. [20] developed an opinion question answering approach for blogs and news texts. They exploited attitude information namely sentiment and argument types to answer opinion questions. They received comparable result with both text types. Lloyd et al. [13] developed the Lydia system to analyze blog entries. They analyzed temporal relationship between blogs and news texts. In particular, they analyzed how often bloggers report a story before newspapers and how often bloggers react to news that has already been reported. To study this leads/lag relationship, they analyzed frequency time series of 197 mo</context>
</contexts>
<marker>[20]</marker>
<rawString>S. Somasundaran, T. Wilson, J. Wiebe, and V. Stoyanov. QA with Attitude: Exploiting Opinion Type Analysis for Improving Question Answering in On-line Discussions and the News. In Proceedings of the International Conference on Weblogs and Social Media, Boulder, Colorado, USA, March 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yang</author>
<author>T S Chua</author>
<author>S Wang</author>
<author>C K Koh</author>
</authors>
<title>Structured use of External Knowledge for Event-based Open Domain Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>33--40</pages>
<location>Toronto, Canada,</location>
<contexts>
<context position="2309" citStr="[21]" startWordPosition="356" endWordPosition="356">ovies, music to newly launched products on the web in weblogs (or blogs), wikis, online-forums, review sites, and social networking web sites. As more and more people are expressing their opinions on the web, the Internet is becoming a popular and dynamic source of opinions. Natural language tools for automatically mining and organizing these opinions on various events will be very useful for individuals, organizations, and governments. Various natural language tools to process and utilize event-related information from texts have already been developed. Event-based question answering systems [21] and event-based summarization systems [12] are only a few examples. However, most of the event-based systems have been developed to process events from traditional news texts. Blog entries are different in style and structure compared to news texts. As a result, successful natural language approaches that deal with news texts might not be as successful for processing blog entries; thus adaptation of existing successful NLP approaches for news texts to process blog entries is an interesting and challenging task. The first step towards this adaptation is to identify the differences between thes</context>
</contexts>
<marker>[21]</marker>
<rawString>H. Yang, T. S. Chua, S. Wang, and C. K. Koh. Structured use of External Knowledge for Event-based Open Domain Question Answering. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33–40, Toronto, Canada, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>