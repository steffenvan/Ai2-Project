<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000265">
<title confidence="0.669288">
EliXa: A modular and flexible ABSA platform
</title>
<note confidence="0.66273">
Rodrigo Agerri
IXA NLP Group
University of the Basque Country (UPV/EHU)
</note>
<address confidence="0.436588">
Donostia-San Sebasti´an
</address>
<email confidence="0.913029">
rodrigo.agerri@ehu.eus
</email>
<author confidence="0.532115">
I˜naki San Vicente, Xabier Saralegi
</author>
<affiliation confidence="0.41413">
Elhuyar Foundation
</affiliation>
<address confidence="0.753645">
Osinalde industrialdea 3
Usurbil, 20170, Spain
</address>
<email confidence="0.996724">
li.sanvicente,x.saralegil@elhuyar.com
</email>
<sectionHeader confidence="0.995616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999478352941176">
This paper presents a supervised Aspect Based
Sentiment Analysis (ABSA) system. Our aim
is to develop a modular platform which allows
to easily conduct experiments by replacing the
modules or adding new features. We obtain
the best result in the Opinion Target Extrac-
tion (OTE) task (slot 2) using an off-the-shelf
sequence labeler. The target polarity classi-
fication (slot 3) is addressed by means of a
multiclass SVM algorithm which includes lex-
ical based features such as the polarity values
obtained from domain and open polarity lex-
icons. The system obtains accuracies of 0.70
and 0.73 for the restaurant and laptop domain
respectively, and performs second best in the
out-of-domain hotel, achieving an accuracy of
0.80.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9945755">
Nowadays Sentiment Analysis is proving very use-
ful for tasks such as decision making and market
analysis. The ever increasing interest is also shown
in the number of related shared tasks organized:
TASS (Villena-Rom´an et al., 2012; Villena-Rom´an
et al., 2014), SemEval (Nakov et al., 2013; Pon-
tiki et al., 2014; Rosenthal et al., 2014), or the
SemSA Challenge at ESWC20141. Research has
also been evolving towards specific opinion ele-
ments such as entities or properties of a certain opin-
ion target, which is also known as ABSA. The Se-
meval 2015 ABSA shared task aims at covering the
</bodyText>
<footnote confidence="0.865156">
1http://challenges.2014.eswc-
</footnote>
<page confidence="0.501946">
conferences.org/index.php/SemSA
</page>
<bodyText confidence="0.99991205">
most common problems in an ABSA task: detect-
ing the specific topics an opinion refers to (slot1);
extracting the opinion targets (slot2), combining the
topic and target identification (slot1&amp;2) and, finally,
computing the polarity of the identified word/targets
(slot3). Participants were allowed to send one con-
strained (no external resources allowed) and one un-
constrained run for each subtask. We participated in
the slot2 and slot3 subtasks.
Our main is to develop an ABSA system to be
used in the future for further experimentation. Thus,
rather than focusing on tuning the different modules
our main goal is to develop a platform to facilitate
future experimentation. The EliXa system consists
of three independent supervised modules based on
the IXA pipes tools (Agerri et al., 2014) and Weka
(Hall et al., 2009). Next section describes the ex-
ternal resources used in the unconstrained systems.
Sections 3 and 4 describe the systems developed for
each subtask and briefly discuss the obtained results.
</bodyText>
<sectionHeader confidence="0.982933" genericHeader="method">
2 External Resources
</sectionHeader>
<bodyText confidence="0.984002">
Several polarity Lexicons and various corpora were
used for the unconstrained versions of our systems.
To facilitate reproducibility of results, every re-
source listed here is publicly available.
</bodyText>
<subsectionHeader confidence="0.970662">
2.1 Corpora
</subsectionHeader>
<bodyText confidence="0.9983635">
For the restaurant domain we used the Yelp Dataset
Challenge dataset2. Following (Kiritchenko et al.,
2014), we manually filtered out categories not corre-
sponding to food related businesses (173 out of 720
</bodyText>
<footnote confidence="0.997283">
2http://www.yelp.com/dataset challenge
</footnote>
<page confidence="0.865447">
748
</page>
<bodyText confidence="0.909512">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 748–752,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
were finally selected). A total of 997,721 reviews
(117.1M tokens) comprise what we henceforth call
the Yelp food corpus (CY elp).
For the laptop domain we leveraged a corpus
composed of Amazon reviews of electronic devices
(Jo and Oh, 2011). Although only 17,53% of the re-
views belong to laptop products, early experiments
showed the advantage of using the full corpus for
both slot 2 and slot 3 subtasks. The Amazon elec-
tronics corpus (CAmazon) consists of 24,259 reviews
(4.4M tokens). Finally, the English Wikipedia was
also used to induce word clusters using word2vec
(Mikolov et al., 2013).
</bodyText>
<subsectionHeader confidence="0.998982">
2.2 Polarity Lexicons
</subsectionHeader>
<bodyText confidence="0.995684857142857">
We generated two types of polarity lexicons to rep-
resent polarity in the slot3 subtasks: general purpose
and domain specific polarity lexicons.
A general purpose polarity lexicon Lgen was built
by combining four well known polarity lexicons:
SentiWordnet SWN (Baccianella et al., 2010), Gen-
eral Inquirer GI (Stone et al., 1966), Opinion Finder
OF (Wilson et al., 2005) and Liu’s sentiment lexi-
con Liu (Hu and Liu, 2004). When a lemma oc-
curs in several lexicons, its polarity is solved ac-
cording to the following priority order: Liu &gt; OF
&gt; GI &gt; SWN. The order was set based on the
results of (San Vicente et al., 2014). All polarity
weights were normalized to a [−1, 1] interval. Po-
larity categories were mapped to weights for GI
(neg+—*−0.8; neg—*-0.6; neg_—*-0.2; pos_—*0.2;
pos—*0.6; pos+—*0.8), Liu and OF (neg—*-0.7;
pos—*0.7 for both). In addition, a restricted lexicon
Lgenres including only the strongest polarity words
was derived from Lgen by applying a threshold of
f0.6.
</bodyText>
<table confidence="0.999487">
Domain Polarity Lexicon Total
General Lgen 42,218
General Lgenres 12,398
Electronic LAmazon 4,511
devices
Food LYelp 4,691
</table>
<tableCaption confidence="0.999897">
Table 1: Statistics of the polarity lexicons.
</tableCaption>
<bodyText confidence="0.999902">
Domain specific polarity lexicons LYelp and
LAmazon were automatically extracted from CYelp
and CAmazon reviews corpora. Reviews are rated
in a [1..5] interval, being 1 the most negative and
5 the most positive. Using the Log-likelihood ratio
(LLR) (Dunning, 1993) we obtained the ranking of
the words which occur more with negative and pos-
itive reviews respectively. We considered reviews
with 1 and 2 rating as negative and those with 4 and 5
ratings as positive. LLR scores were normalized to a
[−1, 1] interval and included in LYelp and LAmazon
lexicons as polarity weights.
</bodyText>
<sectionHeader confidence="0.9712405" genericHeader="method">
3 Slot2 Subtask: Opinion Target
Extraction
</sectionHeader>
<bodyText confidence="0.999855913043478">
The Opinion Target Extraction task (OTE) is ad-
dressed as a sequence labeling problem. We use the
ixa-pipe-nerc Named Entity Recognition system3
(Agerri et al., 2014) off-the-shelf to train our OTE
models; the system learns supervised models via
the Perceptron algorithm as described by (Collins,
2002). ixa-pipe-nerc uses the Apache OpenNLP
project implementation of the Perceptron algorithm4
customized with its own features. Specifically, ixa-
pipe-nerc implements basic non-linguistic local fea-
tures and on top of those a combination of word class
representation features partially inspired by (Turian
et al., 2010). The word representation features use
large amounts of unlabeled data. The result is a
quite simple but competitive system which obtains
the best constrained and unconstrained results and
the first and third best overall results.
The local features implemented are: current to-
ken and token shape (digits, lowercase, punctuation,
etc.) in a 2 range window, previous prediction, be-
ginning of sentence, 4 characters in prefix and suffix,
bigrams and trigrams (token and shape). On top of
them we induce three types of word representations:
</bodyText>
<listItem confidence="0.989378555555556">
• Brown (Brown et al., 1992) clusters, taking the
4th, 8th, 12th and 20th node in the path. We in-
duced 1000 clusters on the Yelp reviews dataset
described in section 2.1 using the tool imple-
mented by Liang5.
• Clark (Clark, 2003) clusters, using the standard
configuration to induce 200 clusters on the Yelp
reviews dataset and 100 clusters on the food
portion of the Yelp reviews dataset.
</listItem>
<footnote confidence="0.99974">
3https://github.com/ixa-ehu/ixa-pipe-nerc
4http://opennlp.apache.org/
5https://github.com/percyliang/brown-cluster
</footnote>
<page confidence="0.99155">
749
</page>
<listItem confidence="0.804055">
• Word2vec (Mikolov et al., 2013) clusters,
</listItem>
<bodyText confidence="0.919226588235294">
based on K-means applied over the extracted
word vectors using the skip-gram algorithm6;
400 clusters were induced using the Wikipedia.
The implementation of the clustering features
looks for the cluster class of the incoming token in
one or more of the clustering lexicons induced fol-
lowing the three methods listed above. If found,
then we add the class as a feature. The Brown
clusters only apply to the token related features,
which are duplicated. We chose the best combina-
tion of features using 5-fold cross validation, ob-
taining 73.03 F1 score with local features (e.g. con-
strained mode) and 77.12 adding the word clustering
features, namely, in unconstrained mode. These two
configurations were used to process the test set in
this task. Table 2 lists the official results for the first
4 systems in the task.
</bodyText>
<table confidence="0.999659166666667">
System (type) Precision Recall F1 score
Baseline 55.42 43.4 48.68
EliXa (u) 68.93 71.22 70.05
NLANGP (u) 70.53 64.02 67.12
EliXa (c) 67.23 66.61 66.91
IHS-RD-Belarus (c) 67.58 59.23 63.13
</table>
<tableCaption confidence="0.9923615">
Table 2: Results obtained on the slot2 evaluation on
restaurant data.
</tableCaption>
<bodyText confidence="0.999973714285714">
The results show that leveraging unlabeled text is
helpful in the OTE task, obtaining an increase of 7
points in recall. It is also worth mentioning that our
constrained system (using non-linguistic local fea-
tures) performs very closely to the second best over-
all system by the NLANGP team (unconstrained).
Finally, we would like to point out to the overall
low results in this task (for example, compared to
the 2014 edition), due to the very small and diffi-
cult training set (e.g., containing many short samples
such as “Tasty Dog!”) which made it extremely hard
to learn good models for this task. The OTE mod-
els will be made freely available in the ixa-pipe-nerc
website in time for SemEval 2015.
</bodyText>
<sectionHeader confidence="0.997156" genericHeader="method">
4 Slot3 Subtask: Sentiment Polarity
</sectionHeader>
<bodyText confidence="0.988423">
The EliXa system implements a single multiclass
SVM classifier. We use the SMO implementation
</bodyText>
<footnote confidence="0.879186">
6https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.998795666666667">
provided by the Weka library (Hall et al., 2009). All
the classifiers built over the training data were eval-
uated via 10-fold cross validation. The complexity
parameter was optimized as (C = 1.0). Many con-
figurations were tested in this experiments, but in the
following we only will describe the final setting.
</bodyText>
<subsectionHeader confidence="0.937174">
4.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999943444444444">
The very first features we introduced in our classi-
fier were token ngrams. Initial experiments showed
that lemma ngrams (lgrams) performed better than
raw form ngrams. One feature per lgram is added
to the vector representation, and lemma frequency
is stored. With respect to the ngram size used, we
tested up to 4-gram features and improvement was
achieved in laptop domain but only when not com-
bined with other features.
</bodyText>
<subsectionHeader confidence="0.895254">
4.2 PoS
</subsectionHeader>
<bodyText confidence="0.999807333333333">
PoS tag and lemma information, obtained using the
IXA pipes tools (Agerri et al., 2014), were also in-
cluded as features. One feature per PoS tag was
added again storing the number of occurrences of a
tag in the sentence. These features slightly improve
over the baseline only in the restaurant domain.
</bodyText>
<subsectionHeader confidence="0.991972">
4.3 Window
</subsectionHeader>
<bodyText confidence="0.999979285714286">
Given that a sentence may contain multiple opin-
ions, we define a window span around a given opin-
ion target (5 words before and 5 words after). When
the target of an opinion is null the whole sentence is
taken as span. Only the restaurant and hotel domains
contained gold target annotations so we did not use
this feature in the laptop domain.
</bodyText>
<subsectionHeader confidence="0.998478">
4.4 Polarity Lexicons
</subsectionHeader>
<bodyText confidence="0.9999981">
The positive and negative scores we extracted as fea-
tures from both general purpose and domain specific
lexicons. Both scores are calculated as the sum of
every positive/negative score in the corresponding
lexicon divided by the number of words in the sen-
tence. Features obtained from the general lexicons
provide a slight improvement. Lgenres is better for
restaurant domain, while Lgen is better for laptops.
Domain specific lexicons LAmazon and LYelp also
help as shown by tables 3 and 4.
</bodyText>
<page confidence="0.990246">
750
</page>
<subsectionHeader confidence="0.972834">
4.5 Word Clusters
</subsectionHeader>
<bodyText confidence="0.9998885">
Word2vec clustering features combine best with the
rest as shown by table 3. These features only were
useful for the restaurant domain, perhaps due to the
small size of the laptops domain data.
</bodyText>
<subsectionHeader confidence="0.988786">
4.6 Feature combinations
</subsectionHeader>
<bodyText confidence="0.999733222222222">
Every feature, when used in isolation, only
marginally improves the baseline. Some of them,
such as the E&amp;A features (using the gold informa-
tion from the slot1 subtask) for the laptop domain,
only help when combined with others. Best perfor-
mance is achieved when several features are com-
bined. As shown by tables 4 and 5, improvement
over the baseline ranges between 2,8% and 1,9% in
the laptop and restaurant domains respectively.
</bodyText>
<table confidence="0.998446133333333">
Classifier Acc Rest
Baseline (organizers) 78.8
Baseline 80.11
1lgram
2lgram 79.3
1lgram + E&amp;A 79.8
1lgram(w5) 80.41
1lgram + PoS 80.59 (c)
Lexicons 80.6
1lgram + Lgen
1lgram + Lgenres 81
1lgram + LY elp 80.9
Combinations 82.34 (u)
1lgram(w5)+w2v(CY elp)+Lgenres+
LY elp + PoS
</table>
<tableCaption confidence="0.9765035">
Table 3: Slot3 ablation experiments for restaurants. (c)
and (u) refer to constrained and unconstrained tracks.
</tableCaption>
<subsectionHeader confidence="0.692849">
4.7 Results
</subsectionHeader>
<bodyText confidence="0.999945666666667">
Table 5 shows the result achieved by our sentiment
polarity classifier. Although for both restaurant and
laptops domains we obtain results over the baseline
both performance are modest.
In contrast, for the out of domain track, which was
evaluated on hotel reviews our system obtains the
third highest score. Because of the similarity of the
domains, we straightforwardly applied our restau-
rant domain models. The good results of the con-
strained system could mean that the feature combi-
nation used may be robust across domains. With re-
spect to the unconstrained system, we suspect that
</bodyText>
<table confidence="0.999503058823529">
Classifier Acc Lapt
Baseline (organizers) 78.3
Baseline 79.33
1lgram
2lgram 79.7
1lgram + clusters(w2v) 79.23
1lgram + E&amp;A 79.23
1lgram + PoS 78.88
Lexicons 79.2
1lgram + Lgen
1lgram + Lgenres 79
1lgram + LAmazon 79.7
Combinations 79.99 (c)
1lgram + PoS + E&amp;A
2lgram + PoS + E&amp;A 78.27
1lgram+ Lgenres +LAmazon + PoS + 80.85 (u)
E&amp;A
</table>
<tableCaption confidence="0.998172">
Table 4: Slot3 ablation experiments for laptops; (c) and
(u) refer to constrained and unconstrained tracks.
</tableCaption>
<bodyText confidence="0.9938065">
such a good performance is achieved due to the fact
that word cluster information was very adequate for
the hotel domain, because Cyelp contains a 10.55%
of hotel reviews.
</bodyText>
<table confidence="0.999667166666667">
System Rest. Lapt. Hotel
Baseline 63.55 69.97 71.68 (majority)
Sentiue 78.70 (1) 79.35 (1) 71.68 (4)
lsislif 75.50 (3) 77.87 (3) 85.84 (1)
EliXa (u) 70.06(10) 72.92 (7) 79.65 (3)
EliXa (c) 67.34 (14) 71.55 (9) 74.93 (5)
</table>
<tableCaption confidence="0.9959665">
Table 5: Results obtained on the slot3 evaluation on
restaurant data; ranking in brackets.
</tableCaption>
<sectionHeader confidence="0.993889" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999774777777778">
We have presented a modular and supervised ABSA
platform developed to facilitate future experimenta-
tion in the field. We submitted runs corresponding
to the slot2 and slot3 subtasks, obtaining competi-
tive results. In particular, we obtained the best re-
sults in slot2 (OTE) and for slot3 we obtain 3rd best
result in the out-of-domain track, which is nice for a
supervised system. Finally, a system for topic detec-
tion (slot1) is currently under development.
</bodyText>
<page confidence="0.99716">
751
</page>
<sectionHeader confidence="0.998942" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999204">
This work has been supported by the following
projects: ADi project (Etortek grant No. IE-14-382),
NewsReader (FP7-ICT 2011-8-316404), SKaTer
(TIN2012-38584-C06-02) and Tacardi (TIN2012-
38523-C02-01).
</bodyText>
<sectionHeader confidence="0.998493" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994057020618557">
Rodrigo Agerri, Josu Bermudez, and German Rigau.
2014. Ixa pipeline: Efficient and ready to use multi-
lingual nlp tools. In Proceedings of the 9th Language
Resources and Evaluation Conference (LREC2014),
pages 26–31, Reykjavik, Iceland, May.
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Senti-
WordNet 3.0: An enhanced lexical resource for senti-
ment analysis and opinion mining. In Seventh confer-
ence on International Language Resources and Eval-
uation (LREC-2010), Malta., volume 25.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J Della Pietra, and Jenifer C Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional linguistics, 18(4):467–479.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth conference on Eu-
ropean chapter of the Association for Computational
Linguistics-Volume 1, pages 59–66.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing-Volume 10, pages 1–8.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computacional Linguis-
tics, 19(1):61–74, March.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18, november.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168–177.
Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment
unification model for online review analysis. In Pro-
ceedings of the fourth ACM international conference
on Web search and data mining, WSDM ’11, pages
815–824, New York, NY, USA. ACM.
Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. NRC-canada-2014: Detect-
ing aspects and sentiment in customer reviews. In
Proceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval 2014), pages 437–442,
Dublin, Ireland, August.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 312–320, Atlanta, Georgia, USA, June.
Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Har-
ris Papageorgiou, Ion Androutsopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspect
based sentiment analysis. In Proceedings of the In-
ternational Workshop on Semantic Evaluation (Se-
mEval).
Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin
Stoyanov. 2014. Semeval-2014 task 9: Sentiment
analysis in twitter. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation, SemEval,
volume 14.
I˜naki San Vicente, Rodrigo Agerri, and German Rigau.
2014. Simple, robust and (almost) unsupervised gen-
eration of polarity lexicons for multiple languages. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL2014, pages 88–97, Gothenburg, Sweden.
P. Stone, D. Dunphy, M. Smith, and D. Ogilvie. 1966.
The General Inquirer: A Computer Approach to Con-
tent Analysis. Cambridge (MA): MIT Press.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 384–394, Uppsala,
Sweden, July.
Julio Villena-Rom´an, Sara Lana-Serrano, Eugenio
Martinez-C´amara, and Jos´e Carlos Gonz´alez-
Crist´obal. 2012. Tass-workshop on sentiment
analysis at sepln. Procesamiento del Lenguaje
Natural, 50:37–44.
Julio Villena-Rom´an, Janine Garcia-Morera, Sara Lana-
Serrano, and Jos´e Carlos Gonz´alez-Crist´obal. 2014.
Tass 2013 - a second step in reputation analysis in
spanish. Procesamiento del Lenguaje Natural, 52(0).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, page 347–354.
</reference>
<page confidence="0.997704">
752
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.086834">
<title confidence="0.998481">EliXa: A modular and flexible ABSA platform</title>
<author confidence="0.998618">Rodrigo</author>
<affiliation confidence="0.978293">IXA NLP University of the Basque Country</affiliation>
<author confidence="0.738961">Donostia-San Sebasti´an</author>
<email confidence="0.941928">rodrigo.agerri@ehu.eus</email>
<note confidence="0.2976495">San Vicente, Xabier Elhuyar Osinalde industrialdea Usurbil, 20170, Spain</note>
<email confidence="0.999218">li.sanvicente,x.saralegil@elhuyar.com</email>
<abstract confidence="0.992240666666667">This paper presents a supervised Aspect Based Sentiment Analysis (ABSA) system. Our aim is to develop a modular platform which allows to easily conduct experiments by replacing the modules or adding new features. We obtain the best result in the Opinion Target Extraction (OTE) task (slot 2) using an off-the-shelf sequence labeler. The target polarity classification (slot 3) is addressed by means of a multiclass SVM algorithm which includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rodrigo Agerri</author>
<author>Josu Bermudez</author>
<author>German Rigau</author>
</authors>
<title>Ixa pipeline: Efficient and ready to use multilingual nlp tools.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th Language Resources and Evaluation Conference (LREC2014),</booktitle>
<pages>26--31</pages>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="2489" citStr="Agerri et al., 2014" startWordPosition="375" endWordPosition="378">dentification (slot1&amp;2) and, finally, computing the polarity of the identified word/targets (slot3). Participants were allowed to send one constrained (no external resources allowed) and one unconstrained run for each subtask. We participated in the slot2 and slot3 subtasks. Our main is to develop an ABSA system to be used in the future for further experimentation. Thus, rather than focusing on tuning the different modules our main goal is to develop a platform to facilitate future experimentation. The EliXa system consists of three independent supervised modules based on the IXA pipes tools (Agerri et al., 2014) and Weka (Hall et al., 2009). Next section describes the external resources used in the unconstrained systems. Sections 3 and 4 describe the systems developed for each subtask and briefly discuss the obtained results. 2 External Resources Several polarity Lexicons and various corpora were used for the unconstrained versions of our systems. To facilitate reproducibility of results, every resource listed here is publicly available. 2.1 Corpora For the restaurant domain we used the Yelp Dataset Challenge dataset2. Following (Kiritchenko et al., 2014), we manually filtered out categories not corr</context>
<context position="5925" citStr="Agerri et al., 2014" startWordPosition="915" endWordPosition="918">1 the most negative and 5 the most positive. Using the Log-likelihood ratio (LLR) (Dunning, 1993) we obtained the ranking of the words which occur more with negative and positive reviews respectively. We considered reviews with 1 and 2 rating as negative and those with 4 and 5 ratings as positive. LLR scores were normalized to a [−1, 1] interval and included in LYelp and LAmazon lexicons as polarity weights. 3 Slot2 Subtask: Opinion Target Extraction The Opinion Target Extraction task (OTE) is addressed as a sequence labeling problem. We use the ixa-pipe-nerc Named Entity Recognition system3 (Agerri et al., 2014) off-the-shelf to train our OTE models; the system learns supervised models via the Perceptron algorithm as described by (Collins, 2002). ixa-pipe-nerc uses the Apache OpenNLP project implementation of the Perceptron algorithm4 customized with its own features. Specifically, ixapipe-nerc implements basic non-linguistic local features and on top of those a combination of word class representation features partially inspired by (Turian et al., 2010). The word representation features use large amounts of unlabeled data. The result is a quite simple but competitive system which obtains the best co</context>
<context position="10254" citStr="Agerri et al., 2014" startWordPosition="1602" endWordPosition="1605">ested in this experiments, but in the following we only will describe the final setting. 4.1 Baseline The very first features we introduced in our classifier were token ngrams. Initial experiments showed that lemma ngrams (lgrams) performed better than raw form ngrams. One feature per lgram is added to the vector representation, and lemma frequency is stored. With respect to the ngram size used, we tested up to 4-gram features and improvement was achieved in laptop domain but only when not combined with other features. 4.2 PoS PoS tag and lemma information, obtained using the IXA pipes tools (Agerri et al., 2014), were also included as features. One feature per PoS tag was added again storing the number of occurrences of a tag in the sentence. These features slightly improve over the baseline only in the restaurant domain. 4.3 Window Given that a sentence may contain multiple opinions, we define a window span around a given opinion target (5 words before and 5 words after). When the target of an opinion is null the whole sentence is taken as span. Only the restaurant and hotel domains contained gold target annotations so we did not use this feature in the laptop domain. 4.4 Polarity Lexicons The posit</context>
</contexts>
<marker>Agerri, Bermudez, Rigau, 2014</marker>
<rawString>Rodrigo Agerri, Josu Bermudez, and German Rigau. 2014. Ixa pipeline: Efficient and ready to use multilingual nlp tools. In Proceedings of the 9th Language Resources and Evaluation Conference (LREC2014), pages 26–31, Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Baccianella</author>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Seventh conference on International Language Resources and Evaluation (LREC-2010),</booktitle>
<volume>25</volume>
<contexts>
<context position="4272" citStr="Baccianella et al., 2010" startWordPosition="645" endWordPosition="648">belong to laptop products, early experiments showed the advantage of using the full corpus for both slot 2 and slot 3 subtasks. The Amazon electronics corpus (CAmazon) consists of 24,259 reviews (4.4M tokens). Finally, the English Wikipedia was also used to induce word clusters using word2vec (Mikolov et al., 2013). 2.2 Polarity Lexicons We generated two types of polarity lexicons to represent polarity in the slot3 subtasks: general purpose and domain specific polarity lexicons. A general purpose polarity lexicon Lgen was built by combining four well known polarity lexicons: SentiWordnet SWN (Baccianella et al., 2010), General Inquirer GI (Stone et al., 1966), Opinion Finder OF (Wilson et al., 2005) and Liu’s sentiment lexicon Liu (Hu and Liu, 2004). When a lemma occurs in several lexicons, its polarity is solved according to the following priority order: Liu &gt; OF &gt; GI &gt; SWN. The order was set based on the results of (San Vicente et al., 2014). All polarity weights were normalized to a [−1, 1] interval. Polarity categories were mapped to weights for GI (neg+—*−0.8; neg—*-0.6; neg_—*-0.2; pos_—*0.2; pos—*0.6; pos+—*0.8), Liu and OF (neg—*-0.7; pos—*0.7 for both). In addition, a restricted lexicon Lgenres in</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>S. Baccianella, A. Esuli, and F. Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Seventh conference on International Language Resources and Evaluation (LREC-2010), Malta., volume 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="6942" citStr="Brown et al., 1992" startWordPosition="1069" endWordPosition="1072"> features partially inspired by (Turian et al., 2010). The word representation features use large amounts of unlabeled data. The result is a quite simple but competitive system which obtains the best constrained and unconstrained results and the first and third best overall results. The local features implemented are: current token and token shape (digits, lowercase, punctuation, etc.) in a 2 range window, previous prediction, beginning of sentence, 4 characters in prefix and suffix, bigrams and trigrams (token and shape). On top of them we induce three types of word representations: • Brown (Brown et al., 1992) clusters, taking the 4th, 8th, 12th and 20th node in the path. We induced 1000 clusters on the Yelp reviews dataset described in section 2.1 using the tool implemented by Liang5. • Clark (Clark, 2003) clusters, using the standard configuration to induce 200 clusters on the Yelp reviews dataset and 100 clusters on the food portion of the Yelp reviews dataset. 3https://github.com/ixa-ehu/ixa-pipe-nerc 4http://opennlp.apache.org/ 5https://github.com/percyliang/brown-cluster 749 • Word2vec (Mikolov et al., 2013) clusters, based on K-means applied over the extracted word vectors using the skip-gra</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Classbased n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="7143" citStr="Clark, 2003" startWordPosition="1108" endWordPosition="1109">ned and unconstrained results and the first and third best overall results. The local features implemented are: current token and token shape (digits, lowercase, punctuation, etc.) in a 2 range window, previous prediction, beginning of sentence, 4 characters in prefix and suffix, bigrams and trigrams (token and shape). On top of them we induce three types of word representations: • Brown (Brown et al., 1992) clusters, taking the 4th, 8th, 12th and 20th node in the path. We induced 1000 clusters on the Yelp reviews dataset described in section 2.1 using the tool implemented by Liang5. • Clark (Clark, 2003) clusters, using the standard configuration to induce 200 clusters on the Yelp reviews dataset and 100 clusters on the food portion of the Yelp reviews dataset. 3https://github.com/ixa-ehu/ixa-pipe-nerc 4http://opennlp.apache.org/ 5https://github.com/percyliang/brown-cluster 749 • Word2vec (Mikolov et al., 2013) clusters, based on K-means applied over the extracted word vectors using the skip-gram algorithm6; 400 clusters were induced using the Wikipedia. The implementation of the clustering features looks for the cluster class of the incoming token in one or more of the clustering lexicons in</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6061" citStr="Collins, 2002" startWordPosition="937" endWordPosition="938">ccur more with negative and positive reviews respectively. We considered reviews with 1 and 2 rating as negative and those with 4 and 5 ratings as positive. LLR scores were normalized to a [−1, 1] interval and included in LYelp and LAmazon lexicons as polarity weights. 3 Slot2 Subtask: Opinion Target Extraction The Opinion Target Extraction task (OTE) is addressed as a sequence labeling problem. We use the ixa-pipe-nerc Named Entity Recognition system3 (Agerri et al., 2014) off-the-shelf to train our OTE models; the system learns supervised models via the Perceptron algorithm as described by (Collins, 2002). ixa-pipe-nerc uses the Apache OpenNLP project implementation of the Perceptron algorithm4 customized with its own features. Specifically, ixapipe-nerc implements basic non-linguistic local features and on top of those a combination of word class representation features partially inspired by (Turian et al., 2010). The word representation features use large amounts of unlabeled data. The result is a quite simple but competitive system which obtains the best constrained and unconstrained results and the first and third best overall results. The local features implemented are: current token and </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computacional Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="5402" citStr="Dunning, 1993" startWordPosition="830" endWordPosition="831"> and OF (neg—*-0.7; pos—*0.7 for both). In addition, a restricted lexicon Lgenres including only the strongest polarity words was derived from Lgen by applying a threshold of f0.6. Domain Polarity Lexicon Total General Lgen 42,218 General Lgenres 12,398 Electronic LAmazon 4,511 devices Food LYelp 4,691 Table 1: Statistics of the polarity lexicons. Domain specific polarity lexicons LYelp and LAmazon were automatically extracted from CYelp and CAmazon reviews corpora. Reviews are rated in a [1..5] interval, being 1 the most negative and 5 the most positive. Using the Log-likelihood ratio (LLR) (Dunning, 1993) we obtained the ranking of the words which occur more with negative and positive reviews respectively. We considered reviews with 1 and 2 rating as negative and those with 4 and 5 ratings as positive. LLR scores were normalized to a [−1, 1] interval and included in LYelp and LAmazon lexicons as polarity weights. 3 Slot2 Subtask: Opinion Target Extraction The Opinion Target Extraction task (OTE) is addressed as a sequence labeling problem. We use the ixa-pipe-nerc Named Entity Recognition system3 (Agerri et al., 2014) off-the-shelf to train our OTE models; the system learns supervised models v</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computacional Linguistics, 19(1):61–74, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="2518" citStr="Hall et al., 2009" startWordPosition="381" endWordPosition="384">nally, computing the polarity of the identified word/targets (slot3). Participants were allowed to send one constrained (no external resources allowed) and one unconstrained run for each subtask. We participated in the slot2 and slot3 subtasks. Our main is to develop an ABSA system to be used in the future for further experimentation. Thus, rather than focusing on tuning the different modules our main goal is to develop a platform to facilitate future experimentation. The EliXa system consists of three independent supervised modules based on the IXA pipes tools (Agerri et al., 2014) and Weka (Hall et al., 2009). Next section describes the external resources used in the unconstrained systems. Sections 3 and 4 describe the systems developed for each subtask and briefly discuss the obtained results. 2 External Resources Several polarity Lexicons and various corpora were used for the unconstrained versions of our systems. To facilitate reproducibility of results, every resource listed here is publicly available. 2.1 Corpora For the restaurant domain we used the Yelp Dataset Challenge dataset2. Following (Kiritchenko et al., 2014), we manually filtered out categories not corresponding to food related bus</context>
<context position="9459" citStr="Hall et al., 2009" startWordPosition="1470" endWordPosition="1473">rained). Finally, we would like to point out to the overall low results in this task (for example, compared to the 2014 edition), due to the very small and difficult training set (e.g., containing many short samples such as “Tasty Dog!”) which made it extremely hard to learn good models for this task. The OTE models will be made freely available in the ixa-pipe-nerc website in time for SemEval 2015. 4 Slot3 Subtask: Sentiment Polarity The EliXa system implements a single multiclass SVM classifier. We use the SMO implementation 6https://code.google.com/p/word2vec/ provided by the Weka library (Hall et al., 2009). All the classifiers built over the training data were evaluated via 10-fold cross validation. The complexity parameter was optimized as (C = 1.0). Many configurations were tested in this experiments, but in the following we only will describe the final setting. 4.1 Baseline The very first features we introduced in our classifier were token ngrams. Initial experiments showed that lemma ngrams (lgrams) performed better than raw form ngrams. One feature per lgram is added to the vector representation, and lemma frequency is stored. With respect to the ngram size used, we tested up to 4-gram fea</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explor. Newsl., 11(1):10–18, november.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="4406" citStr="Hu and Liu, 2004" startWordPosition="670" endWordPosition="673">ctronics corpus (CAmazon) consists of 24,259 reviews (4.4M tokens). Finally, the English Wikipedia was also used to induce word clusters using word2vec (Mikolov et al., 2013). 2.2 Polarity Lexicons We generated two types of polarity lexicons to represent polarity in the slot3 subtasks: general purpose and domain specific polarity lexicons. A general purpose polarity lexicon Lgen was built by combining four well known polarity lexicons: SentiWordnet SWN (Baccianella et al., 2010), General Inquirer GI (Stone et al., 1966), Opinion Finder OF (Wilson et al., 2005) and Liu’s sentiment lexicon Liu (Hu and Liu, 2004). When a lemma occurs in several lexicons, its polarity is solved according to the following priority order: Liu &gt; OF &gt; GI &gt; SWN. The order was set based on the results of (San Vicente et al., 2014). All polarity weights were normalized to a [−1, 1] interval. Polarity categories were mapped to weights for GI (neg+—*−0.8; neg—*-0.6; neg_—*-0.2; pos_—*0.2; pos—*0.6; pos+—*0.8), Liu and OF (neg—*-0.7; pos—*0.7 for both). In addition, a restricted lexicon Lgenres including only the strongest polarity words was derived from Lgen by applying a threshold of f0.6. Domain Polarity Lexicon Total General</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM international conference on Web search and data mining, WSDM ’11,</booktitle>
<pages>815--824</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3609" citStr="Jo and Oh, 2011" startWordPosition="541" endWordPosition="544">nge dataset2. Following (Kiritchenko et al., 2014), we manually filtered out categories not corresponding to food related businesses (173 out of 720 2http://www.yelp.com/dataset challenge 748 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 748–752, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics were finally selected). A total of 997,721 reviews (117.1M tokens) comprise what we henceforth call the Yelp food corpus (CY elp). For the laptop domain we leveraged a corpus composed of Amazon reviews of electronic devices (Jo and Oh, 2011). Although only 17,53% of the reviews belong to laptop products, early experiments showed the advantage of using the full corpus for both slot 2 and slot 3 subtasks. The Amazon electronics corpus (CAmazon) consists of 24,259 reviews (4.4M tokens). Finally, the English Wikipedia was also used to induce word clusters using word2vec (Mikolov et al., 2013). 2.2 Polarity Lexicons We generated two types of polarity lexicons to represent polarity in the slot3 subtasks: general purpose and domain specific polarity lexicons. A general purpose polarity lexicon Lgen was built by combining four well known</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of the fourth ACM international conference on Web search and data mining, WSDM ’11, pages 815–824, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Colin Cherry</author>
<author>Saif Mohammad</author>
</authors>
<title>NRC-canada-2014: Detecting aspects and sentiment in customer reviews.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>437--442</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="3043" citStr="Kiritchenko et al., 2014" startWordPosition="459" endWordPosition="462">t supervised modules based on the IXA pipes tools (Agerri et al., 2014) and Weka (Hall et al., 2009). Next section describes the external resources used in the unconstrained systems. Sections 3 and 4 describe the systems developed for each subtask and briefly discuss the obtained results. 2 External Resources Several polarity Lexicons and various corpora were used for the unconstrained versions of our systems. To facilitate reproducibility of results, every resource listed here is publicly available. 2.1 Corpora For the restaurant domain we used the Yelp Dataset Challenge dataset2. Following (Kiritchenko et al., 2014), we manually filtered out categories not corresponding to food related businesses (173 out of 720 2http://www.yelp.com/dataset challenge 748 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 748–752, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics were finally selected). A total of 997,721 reviews (117.1M tokens) comprise what we henceforth call the Yelp food corpus (CY elp). For the laptop domain we leveraged a corpus composed of Amazon reviews of electronic devices (Jo and Oh, 2011). Although only 17,53% of the revi</context>
</contexts>
<marker>Kiritchenko, Zhu, Cherry, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif Mohammad. 2014. NRC-canada-2014: Detecting aspects and sentiment in customer reviews. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437–442, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="3963" citStr="Mikolov et al., 2013" startWordPosition="599" endWordPosition="602">Computational Linguistics were finally selected). A total of 997,721 reviews (117.1M tokens) comprise what we henceforth call the Yelp food corpus (CY elp). For the laptop domain we leveraged a corpus composed of Amazon reviews of electronic devices (Jo and Oh, 2011). Although only 17,53% of the reviews belong to laptop products, early experiments showed the advantage of using the full corpus for both slot 2 and slot 3 subtasks. The Amazon electronics corpus (CAmazon) consists of 24,259 reviews (4.4M tokens). Finally, the English Wikipedia was also used to induce word clusters using word2vec (Mikolov et al., 2013). 2.2 Polarity Lexicons We generated two types of polarity lexicons to represent polarity in the slot3 subtasks: general purpose and domain specific polarity lexicons. A general purpose polarity lexicon Lgen was built by combining four well known polarity lexicons: SentiWordnet SWN (Baccianella et al., 2010), General Inquirer GI (Stone et al., 1966), Opinion Finder OF (Wilson et al., 2005) and Liu’s sentiment lexicon Liu (Hu and Liu, 2004). When a lemma occurs in several lexicons, its polarity is solved according to the following priority order: Liu &gt; OF &gt; GI &gt; SWN. The order was set based on </context>
<context position="7456" citStr="Mikolov et al., 2013" startWordPosition="1142" endWordPosition="1145">oken and shape). On top of them we induce three types of word representations: • Brown (Brown et al., 1992) clusters, taking the 4th, 8th, 12th and 20th node in the path. We induced 1000 clusters on the Yelp reviews dataset described in section 2.1 using the tool implemented by Liang5. • Clark (Clark, 2003) clusters, using the standard configuration to induce 200 clusters on the Yelp reviews dataset and 100 clusters on the food portion of the Yelp reviews dataset. 3https://github.com/ixa-ehu/ixa-pipe-nerc 4http://opennlp.apache.org/ 5https://github.com/percyliang/brown-cluster 749 • Word2vec (Mikolov et al., 2013) clusters, based on K-means applied over the extracted word vectors using the skip-gram algorithm6; 400 clusters were induced using the Wikipedia. The implementation of the clustering features looks for the cluster class of the incoming token in one or more of the clustering lexicons induced following the three methods listed above. If found, then we add the class as a feature. The Brown clusters only apply to the token related features, which are duplicated. We chose the best combination of features using 5-fold cross validation, obtaining 73.03 F1 score with local features (e.g. constrained </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="1343" citStr="Nakov et al., 2013" startWordPosition="196" endWordPosition="199">s SVM algorithm which includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80. 1 Introduction Nowadays Sentiment Analysis is proving very useful for tasks such as decision making and market analysis. The ever increasing interest is also shown in the number of related shared tasks organized: TASS (Villena-Rom´an et al., 2012; Villena-Rom´an et al., 2014), SemEval (Nakov et al., 2013; Pontiki et al., 2014; Rosenthal et al., 2014), or the SemSA Challenge at ESWC20141. Research has also been evolving towards specific opinion elements such as entities or properties of a certain opinion target, which is also known as ABSA. The Semeval 2015 ABSA shared task aims at covering the 1http://challenges.2014.eswcconferences.org/index.php/SemSA most common problems in an ABSA task: detecting the specific topics an opinion refers to (slot1); extracting the opinion targets (slot2), combining the topic and target identification (slot1&amp;2) and, finally, computing the polarity of the identi</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312–320, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval).</booktitle>
<contexts>
<context position="1365" citStr="Pontiki et al., 2014" startWordPosition="200" endWordPosition="204">h includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80. 1 Introduction Nowadays Sentiment Analysis is proving very useful for tasks such as decision making and market analysis. The ever increasing interest is also shown in the number of related shared tasks organized: TASS (Villena-Rom´an et al., 2012; Villena-Rom´an et al., 2014), SemEval (Nakov et al., 2013; Pontiki et al., 2014; Rosenthal et al., 2014), or the SemSA Challenge at ESWC20141. Research has also been evolving towards specific opinion elements such as entities or properties of a certain opinion target, which is also known as ABSA. The Semeval 2015 ABSA shared task aims at covering the 1http://challenges.2014.eswcconferences.org/index.php/SemSA most common problems in an ABSA task: detecting the specific topics an opinion refers to (slot1); extracting the opinion targets (slot2), combining the topic and target identification (slot1&amp;2) and, finally, computing the polarity of the identified word/targets (slo</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the International Workshop on Semantic Evaluation (SemEval).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2014 task 9: Sentiment analysis in twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval,</booktitle>
<volume>14</volume>
<contexts>
<context position="1390" citStr="Rosenthal et al., 2014" startWordPosition="205" endWordPosition="208">ed features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80. 1 Introduction Nowadays Sentiment Analysis is proving very useful for tasks such as decision making and market analysis. The ever increasing interest is also shown in the number of related shared tasks organized: TASS (Villena-Rom´an et al., 2012; Villena-Rom´an et al., 2014), SemEval (Nakov et al., 2013; Pontiki et al., 2014; Rosenthal et al., 2014), or the SemSA Challenge at ESWC20141. Research has also been evolving towards specific opinion elements such as entities or properties of a certain opinion target, which is also known as ABSA. The Semeval 2015 ABSA shared task aims at covering the 1http://challenges.2014.eswcconferences.org/index.php/SemSA most common problems in an ABSA task: detecting the specific topics an opinion refers to (slot1); extracting the opinion targets (slot2), combining the topic and target identification (slot1&amp;2) and, finally, computing the polarity of the identified word/targets (slot3). Participants were al</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. Semeval-2014 task 9: Sentiment analysis in twitter. In Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval, volume 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I˜naki San Vicente</author>
<author>Rodrigo Agerri</author>
<author>German Rigau</author>
</authors>
<title>Simple, robust and (almost) unsupervised generation of polarity lexicons for multiple languages.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL2014,</booktitle>
<pages>88--97</pages>
<location>Gothenburg,</location>
<contexts>
<context position="4604" citStr="Vicente et al., 2014" startWordPosition="710" endWordPosition="713">ons We generated two types of polarity lexicons to represent polarity in the slot3 subtasks: general purpose and domain specific polarity lexicons. A general purpose polarity lexicon Lgen was built by combining four well known polarity lexicons: SentiWordnet SWN (Baccianella et al., 2010), General Inquirer GI (Stone et al., 1966), Opinion Finder OF (Wilson et al., 2005) and Liu’s sentiment lexicon Liu (Hu and Liu, 2004). When a lemma occurs in several lexicons, its polarity is solved according to the following priority order: Liu &gt; OF &gt; GI &gt; SWN. The order was set based on the results of (San Vicente et al., 2014). All polarity weights were normalized to a [−1, 1] interval. Polarity categories were mapped to weights for GI (neg+—*−0.8; neg—*-0.6; neg_—*-0.2; pos_—*0.2; pos—*0.6; pos+—*0.8), Liu and OF (neg—*-0.7; pos—*0.7 for both). In addition, a restricted lexicon Lgenres including only the strongest polarity words was derived from Lgen by applying a threshold of f0.6. Domain Polarity Lexicon Total General Lgen 42,218 General Lgenres 12,398 Electronic LAmazon 4,511 devices Food LYelp 4,691 Table 1: Statistics of the polarity lexicons. Domain specific polarity lexicons LYelp and LAmazon were automatic</context>
</contexts>
<marker>Vicente, Agerri, Rigau, 2014</marker>
<rawString>I˜naki San Vicente, Rodrigo Agerri, and German Rigau. 2014. Simple, robust and (almost) unsupervised generation of polarity lexicons for multiple languages. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL2014, pages 88–97, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Stone</author>
<author>D Dunphy</author>
<author>M Smith</author>
<author>D Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<location>Cambridge (MA):</location>
<contexts>
<context position="4314" citStr="Stone et al., 1966" startWordPosition="653" endWordPosition="656">wed the advantage of using the full corpus for both slot 2 and slot 3 subtasks. The Amazon electronics corpus (CAmazon) consists of 24,259 reviews (4.4M tokens). Finally, the English Wikipedia was also used to induce word clusters using word2vec (Mikolov et al., 2013). 2.2 Polarity Lexicons We generated two types of polarity lexicons to represent polarity in the slot3 subtasks: general purpose and domain specific polarity lexicons. A general purpose polarity lexicon Lgen was built by combining four well known polarity lexicons: SentiWordnet SWN (Baccianella et al., 2010), General Inquirer GI (Stone et al., 1966), Opinion Finder OF (Wilson et al., 2005) and Liu’s sentiment lexicon Liu (Hu and Liu, 2004). When a lemma occurs in several lexicons, its polarity is solved according to the following priority order: Liu &gt; OF &gt; GI &gt; SWN. The order was set based on the results of (San Vicente et al., 2014). All polarity weights were normalized to a [−1, 1] interval. Polarity categories were mapped to weights for GI (neg+—*−0.8; neg—*-0.6; neg_—*-0.2; pos_—*0.2; pos—*0.6; pos+—*0.8), Liu and OF (neg—*-0.7; pos—*0.7 for both). In addition, a restricted lexicon Lgenres including only the strongest polarity words </context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>P. Stone, D. Dunphy, M. Smith, and D. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. Cambridge (MA): MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6376" citStr="Turian et al., 2010" startWordPosition="979" endWordPosition="982">ion The Opinion Target Extraction task (OTE) is addressed as a sequence labeling problem. We use the ixa-pipe-nerc Named Entity Recognition system3 (Agerri et al., 2014) off-the-shelf to train our OTE models; the system learns supervised models via the Perceptron algorithm as described by (Collins, 2002). ixa-pipe-nerc uses the Apache OpenNLP project implementation of the Perceptron algorithm4 customized with its own features. Specifically, ixapipe-nerc implements basic non-linguistic local features and on top of those a combination of word class representation features partially inspired by (Turian et al., 2010). The word representation features use large amounts of unlabeled data. The result is a quite simple but competitive system which obtains the best constrained and unconstrained results and the first and third best overall results. The local features implemented are: current token and token shape (digits, lowercase, punctuation, etc.) in a 2 range window, previous prediction, beginning of sentence, 4 characters in prefix and suffix, bigrams and trigrams (token and shape). On top of them we induce three types of word representations: • Brown (Brown et al., 1992) clusters, taking the 4th, 8th, 12</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Villena-Rom´an</author>
<author>Sara Lana-Serrano</author>
<author>Eugenio Martinez-C´amara</author>
<author>Jos´e Carlos Gonz´alezCrist´obal</author>
</authors>
<title>Tass-workshop on sentiment analysis at sepln.</title>
<date>2012</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>50--37</pages>
<marker>Villena-Rom´an, Lana-Serrano, Martinez-C´amara, Gonz´alezCrist´obal, 2012</marker>
<rawString>Julio Villena-Rom´an, Sara Lana-Serrano, Eugenio Martinez-C´amara, and Jos´e Carlos Gonz´alezCrist´obal. 2012. Tass-workshop on sentiment analysis at sepln. Procesamiento del Lenguaje Natural, 50:37–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Villena-Rom´an</author>
<author>Janine Garcia-Morera</author>
<author>Sara LanaSerrano</author>
<author>Jos´e Carlos Gonz´alez-Crist´obal</author>
</authors>
<title>Tass</title>
<date>2014</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<volume>52</volume>
<issue>0</issue>
<marker>Villena-Rom´an, Garcia-Morera, LanaSerrano, Gonz´alez-Crist´obal, 2014</marker>
<rawString>Julio Villena-Rom´an, Janine Garcia-Morera, Sara LanaSerrano, and Jos´e Carlos Gonz´alez-Crist´obal. 2014. Tass 2013 - a second step in reputation analysis in spanish. Procesamiento del Lenguaje Natural, 52(0).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="4355" citStr="Wilson et al., 2005" startWordPosition="660" endWordPosition="663">us for both slot 2 and slot 3 subtasks. The Amazon electronics corpus (CAmazon) consists of 24,259 reviews (4.4M tokens). Finally, the English Wikipedia was also used to induce word clusters using word2vec (Mikolov et al., 2013). 2.2 Polarity Lexicons We generated two types of polarity lexicons to represent polarity in the slot3 subtasks: general purpose and domain specific polarity lexicons. A general purpose polarity lexicon Lgen was built by combining four well known polarity lexicons: SentiWordnet SWN (Baccianella et al., 2010), General Inquirer GI (Stone et al., 1966), Opinion Finder OF (Wilson et al., 2005) and Liu’s sentiment lexicon Liu (Hu and Liu, 2004). When a lemma occurs in several lexicons, its polarity is solved according to the following priority order: Liu &gt; OF &gt; GI &gt; SWN. The order was set based on the results of (San Vicente et al., 2014). All polarity weights were normalized to a [−1, 1] interval. Polarity categories were mapped to weights for GI (neg+—*−0.8; neg—*-0.6; neg_—*-0.2; pos_—*0.2; pos—*0.6; pos+—*0.8), Liu and OF (neg—*-0.7; pos—*0.7 for both). In addition, a restricted lexicon Lgenres including only the strongest polarity words was derived from Lgen by applying a thres</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, page 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>