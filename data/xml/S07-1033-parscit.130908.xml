<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.038412">
<title confidence="0.970729">
GYDER: maxent metonymy resolution
</title>
<author confidence="0.9978">
Rich´ard Farkas
</author>
<affiliation confidence="0.998068">
University of Szeged
Department of Informatics
</affiliation>
<address confidence="0.880795">
H-6720 Szeged, ´Arp´ad t´er 2.
</address>
<email confidence="0.989821">
rfarkas@inf.u-szeged.hu
</email>
<author confidence="0.97552">
Gy¨orgy Szarvas
</author>
<affiliation confidence="0.998087">
University of Szeged
Department of Informatics
</affiliation>
<address confidence="0.909833">
H-6720 Szeged, ´Arp´ad t´er 2.
</address>
<email confidence="0.997638">
szarvas@inf.u-szeged.hu
</email>
<sectionHeader confidence="0.995615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999163">
Though the GYDER system has achieved
the highest accuracy scores for the
metonymy resolution shared task at
SemEval-2007 in all six subtasks, we don’t
consider the results (72.80% accuracy for
org, 84.36% for loc) particularly impres-
sive, and argue that metonymy resolution
needs more features.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919954545455">
In linguistics metonymy means using one term, or
one specific sense of a term, to refer to another,
related term or sense. For example, in ‘the pen
is mightier than the sword’ pen refers to writing,
the force of ideas, while sword refers to military
force. Named Entity Recognition (NER) is of
key importance in numerous natural language pro-
cessing applications ranging from information ex-
traction to machine translation. Metonymic usage
of named entities is frequent in natural language.
On the basic NER categories person, place,
organisation state-of-the-art systems generally
perform in the mid to the high nineties. These sys-
tems typically do not distinguish between literal or
metonymic usage of entity names, even though this
would be helpful for most applications. Resolving
metonymic usage of proper names would therefore
directly benefit NER and indirectly all NLP tasks
(such as anaphor resolution) that require NER.
Markert and Nissim (2002) outlined a corpus-
based approach to proper name metonymy as a se-
mantic classification problem that forms the basis
</bodyText>
<note confidence="0.62950675">
Eszter Simon
Budapest U. of Technology
Dept. of Cognitive Science
H-1111 Budapest, Stoczek u 2.
</note>
<email confidence="0.812322">
esimon@cogsci.bme.hu
</email>
<author confidence="0.781647">
D´aniel Varga
</author>
<affiliation confidence="0.7402735">
Budapest U. of Technology
MOKK Media Research
</affiliation>
<address confidence="0.68417">
H-1111 Budapest, Stoczek u 2.
</address>
<email confidence="0.739856">
daniel@mokk.bme.hu
</email>
<bodyText confidence="0.999929352941177">
of the 2007 SemEval metonymy resolution task.
Instances like ‘He was shocked by Vietnam’ or
‘Schengen boosted tourism’ were assigned to broad
categories like place-for-event, sometimes
ignoring narrower distinctions, such as the fact that
it wasn’t the signing of the treaty at Schengen but
rather its actual implementation (which didn’t take
place at Schengen) that boosted tourism. But the
corpus makes clear that even with these (sometimes
coarse) class distinctions, several metonymy types
seem to appear extremely rarely in actual texts.
The shared task focused on two broad named en-
tity classes as metonymic sources, location and
org, each having several target classes. For more
details on the data sets, see the task description pa-
per Markert and Nissim (2007).
Several categories (e.g. place-for-event,
organisation-for-index) did not contain a
sufficient number of examples for machine learn-
ing, and we decided early on to accept the fact that
these categories will not be learned and to concen-
trate on those classes where learning seemed feasi-
ble. The shared task itself consisted of 3 subtasks
of different granularity for both organisation and lo-
cation names. The fine-grained evaluation aimed
at distinguishing between all categories, while the
medium-grained evaluation grouped different types
of metonymic usage together and addressed literal /
mixed / metonymic usage. The coarse-grained sub-
task was in fact a literal / nonliteral two-class classi-
fication task.
Though GYDER has obtained the highest accu-
racy for the metonymy shared task at SemEval-2007
in all six subtasks, we don’t consider the results
</bodyText>
<page confidence="0.976176">
161
</page>
<bodyText confidence="0.956975571428571">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 161–164,
Prague, June 2007. c�2007 Association for Computational Linguistics
(72.80% accuracy for org, 84.36% for loc) par-
ticularly impressive. In Section 3 we describe the
feature engineering lessons learned from working on
the task. In Section 5 we offer some speculative re-
marks on what it would take to improve the results.
</bodyText>
<sectionHeader confidence="0.904799" genericHeader="introduction">
2 Learning
</sectionHeader>
<bodyText confidence="0.99993325">
GYDER (the acronym was formed from the initials
of the author’ first names) is a maximum entropy
learner. It uses Zhang Le’s 1 maximum entropy
toolkit, setting the Gaussian prior to 1. We used ran-
dom 5-fold cross-validation to determine the useful-
ness of a particular feature. Due to the small num-
ber of instances and features, the learning algorithm
always converged before 30 iterations, so the cross-
validation process took only seconds.
We also tested the classic C4.5 decision tree learn-
ing algorithm Quinlan (1993), but our early exper-
iments showed that the maximum entropy learner
was consistently superior to the decision tree clas-
sifier for this task, yielding about 2-5% higher accu-
racy scores on average on both tasks (on the training
set, using cross-validation).
</bodyText>
<sectionHeader confidence="0.984934" genericHeader="method">
3 Feature Engineering
</sectionHeader>
<bodyText confidence="0.999997333333333">
We tested several features describing orthographic,
syntactic, or semantic characteristics of the Possibly
Metonymic Words (PMWs). Here we follow Nissim
and Markert (2005), who reported three classes of
features to be the most relevant for metonymy res-
olution: the grammatical annotations provided for
the corpus examples by the task organizers, the de-
terminer, and the grammatical number of the PMW.
We also report on some features that didn’t work.
</bodyText>
<subsectionHeader confidence="0.999699">
3.1 Grammatical annotations
</subsectionHeader>
<bodyText confidence="0.999485888888889">
We used the grammatical annotations provided for
each PMW in several ways. First, we used as a
feature the type of the grammatical relation and the
word form of the related word. (If there was more
than one related word, each became a feature.) To
overcome data sparseness, it is useful to general-
ize from individual headwords Markert and Nissim
(2003). We used three different methods to achieve
this:
</bodyText>
<footnote confidence="0.882243">
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
</footnote>
<bodyText confidence="0.99995355">
First, we used Levin’s (1993) verb classification
index to generalize the headwords of the most rele-
vant grammatical relations (subject and object). The
added feature was simply the class assigned to the
verb by Levin.
We also used WordNet (Fellbaum 1998) to gen-
eralize headwords. First we gathered the hypernym
path from WordNet for each headword’s sense#1 in
the train corpus. Based on these paths we collected
synsets whose tree frequently indicated metonymic
sense. We indicated with a feature if the headword
in question was in one of such collected subtrees.
Third, we have manually built a very small verb
classification ‘Trigger’ table for specific cases. E.g.
announce, say, declare all trigger the same feature.
This table is the only resource in our final system
that was manually built by us, so we note that on the
test corpus, disabling this ‘Trigger’ feature does not
alter org accuracy, and decreases loc accuracy by
0.44%.
</bodyText>
<subsectionHeader confidence="0.995122">
3.2 Determiners
</subsectionHeader>
<bodyText confidence="0.999967583333333">
Following Nissim and Markert (2005), we distin-
guished between definite, indefinite, demonstrative,
possessive, wh and other determiners. We also
marked if the PMW was sentence-initial, and thus
necessarily determinerless. This feature was useful
for the resolution of organisation PMWs so we used
it only for the org tasks. It was not straightforward,
however, to assign determiners to the PMWs without
proper syntactic analysis. After some experiments,
we linked the nearest determiner and the PMW to-
gether if we found only adjectives (or nothing) be-
tween them.
</bodyText>
<subsectionHeader confidence="0.982892">
3.3 Number
</subsectionHeader>
<bodyText confidence="0.9999744">
This feature was particularly useful to separate
metonymies of the org-for-product class. We
assumed that only PMWs ending with letter s might
be in plural form, and for them we compared the web
search result numbers obtained by the Google API.
We ran two queries for each PMWs, one for the full
name, and one for the name without its last charac-
ter. If we observed a significant increase in the num-
ber of hits returned by Google for the shorter phrase,
we set this feature for plural.
</bodyText>
<page confidence="0.988836">
162
</page>
<subsectionHeader confidence="0.864172">
3.4 PMW word form
</subsectionHeader>
<bodyText confidence="0.99998375">
We included the surface form of the PMW as a fea-
ture, but only for the org domain. Cross-validation
on the training corpus showed that the use of this
feature causes an 1.5% accuracy improvement for
organisations, and a slight degradation for locations.
The improvement perfectly generalized to the test
corpora. Some company names are indeed more
likely to be used in a metonymic way, so we be-
lieve that this feature does more than just exploit-
ing some specificity of the shared task corpora. We
note that the ranking of our system would have been
unaffected even if we didn’t use this feature.
</bodyText>
<subsectionHeader confidence="0.924006">
3.5 Unsuccessful features
</subsectionHeader>
<bodyText confidence="0.998544863636364">
Here we discuss those features where cross-
validation didn’t show improvements (and thus were
not included in the submitted system).
Trigger words were automatically collected lists of
word forms and phrases that more frequently
appeared near metonymic PMWs.
Expert triggers were similar trigger words or
phrases, but suggested by a linguist expert to
be potentially indicative for metonymic usage.
We experimented with sample-level, sentence-
level and vicinity trigger phrases.
Named entity labels given by a state-of-the-art
named entity recognizer (Szarvas et al. 2006).
POS tags around PMWs.
Ortographical features such as capitalisation and
and other surface characteristics for the PMW
and nearby words.
Individual tokens of the potentially metonymic
phrase.
Main category of Levin’s hierarchical classification.
Inflectional category of the verb nearest to the PMW
in the sentence.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.998306571428571">
Table 1. shows the accuracy scores of our submitted
system on fine classification granularity. As a base-
line, we also evalute the system without the Word-
Net, Levin, Trigger and PMW word form features.
This baseline system is quite similar to the one de-
scribed by Nissim and Markert (2005). We also pub-
lish the majority baseline scores.
</bodyText>
<table confidence="0.9199022">
run majority baseline submitted
org train 5-fold 63.30 77.51 80.92
org test 61.76 70.55 72.80
loc train 5-fold 79.68 85.58 88.36
loc test 79.41 83.59 84.36
</table>
<tableCaption confidence="0.99968">
Table 1: Accuracy of the submitted system
</tableCaption>
<bodyText confidence="0.999861846153846">
We could not exploit the hierarchical structure of
the fine-grained tag set, and ended up treating it as
totally unstructured even for the mixed class, unlike
Nissim and Markert, who apply complicated heuris-
tics to exploit the special semantics of this class.
For the coarse and medium subtasks of the loc
domain, we simply coarsened the fine-grained re-
sults. For the coarse and medium subtasks of
the org domain, we coarsened the train corpus to
medium coarseness before training. This idea was
based on observations on training data, but was
proven to be unjustified: it slightly decreased the
system’s accuracy on the medium subtask.
</bodyText>
<table confidence="0.972509">
coarse medium fine
location 85.24 84.80 84.36
organisation 76.72 73.28 72.80
</table>
<tableCaption confidence="0.9032515">
Table 2: Accuracy of the GYDER system for each
domain / granularity
</tableCaption>
<bodyText confidence="0.999624388888889">
In general, the coarser grained evaluation did not
show a significantly higher accuracy (see Table 2.),
proving that the main difficulty is to distinguish be-
tween literal and metonymic usage, rather than sepa-
rating metonymy classes from each other (since dif-
ferent classes represent significantly different usage
/ context). Because of this, data sparseness remained
a problem for coarse-grained classification as well.
Per-class results of the submitted system for
both domains are shown on Table 3. Note
that our system never predicted loc values from
the four small classes place-for-event and
product, object-for-name and other as
these had only 26 instances altogether. Since
we never had significant results for the mixed
category, in effect the loc task ended up a bi-
nary classification task between literal and
place-for-people.
</bodyText>
<page confidence="0.996202">
163
</page>
<figure confidence="0.999047888888889">
# prec rec f
721 86.83 95.98 91.17
141 68.22 51.77 58.87
20 25.00 5.00 8.33
11 - 0.0 -
10 - 0.0 -
4 - 0.0 -
1 - 0.0 -
# prec rec f
520 75.76 90.77 82.59
161 65.99 60.25 62.99
67 82.76 35.82 50.00
60 43.59 28.33 34.34
16 100.0 12.50 22.22
8 - 0.0 -
6 50.00 16.67 25.00
3 - 0.0 -
1 - 0.0 -
loc class
literal
place-for-people
mixed
othermet
place-for-event
object-for-name
place-for-product
org class
literal
org-for-members
org-for-product
mixed
org-for-facility
othermet
object-for-name
org-for-index
org-for-event
</figure>
<tableCaption confidence="0.971286">
Table 3: Per-class accuracies for both domains
</tableCaption>
<bodyText confidence="0.999910923076923">
as “location for sports team”, “capital city for gov-
ernment”, and so on, which are not yet always dis-
tinguished by the depth of the annotation.
It would be a true challenge to create a data set
of non-cliche metonymy cases, or a corpus large
enough to represent rare metonymy types and chal-
lenging non-cliche metonymies better.
We feel that at least regarding the corpus used for
the shared task, the potential of the grammatical an-
notation for PMWs was more or less well exploited.
Future systems should exploit more semantic knowl-
edge, or the power of a larger data set, or preferably
both.
</bodyText>
<sectionHeader confidence="0.978684" genericHeader="method">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999860333333333">
We wish to thank Andr´as Kornai for help and
encouragement, and the anonymous reviewers for
valuable comments.
</bodyText>
<sectionHeader confidence="0.998186" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999171448275862">
Christiane Fellbaum ed. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. A Preliminary Investigation. The University of
Chicago Press.
Katja Markert and Malvina Nissim. 2002. Metonymy
resolution as a classification task. Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002). Philadelphia, USA.
Katja Markert and Malvina Nissim. 2003. Syntactic Fea-
tures and Word Similarity for Supervised Metonymy
Resolution. Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL2003). Sapporo, Japan.
Malvina Nissim and Katja Markert. 2005. Learning
to buy a Renault and talk to BMW: A supervised
approach to conventional metonymy. International
Workshop on Computational Semantics (IWCS2005).
Tilburg, Netherlands.
Katja Markert and Malvina Nissim. 2007. SemEval-
2007 Task 08: Metonymy Resolution at SemEval-
2007. In Proceedings of SemEval-2007.
Ross Quinlan. 1993. C4.5: Programs for machine learn-
ing. Morgan Kaufmann.
Gy¨orgy Szarvas, Rich´ard Farkas and Andr´as Kocsor.
2006. Multilingual Named Entity Recognition Sys-
tem Using Boosting and C4.5 Decision Tree Learning
Algorithms. Proceedings of Discovery Science 2006,
DS2006, LNAI 4265 pp. 267-278. Springer-Verlag.
</reference>
<bodyText confidence="0.999721142857143">
While in the org set the system also ig-
nores the smallest categories othermet,
org-for-index and event (a total of 11
instances), the six major categories literal,
org-for-members, org-for-product,
org-for-facility, object-for-name,
mixed all receive meaningful hypotheses.
</bodyText>
<sectionHeader confidence="0.66797" genericHeader="method">
5 Conclusions, Further Directions
</sectionHeader>
<bodyText confidence="0.9998880625">
The features we eventually selected performed well
enough to actually achieve the best scores in all six
subtasks of the shared task, and we think they are
useful in general. But it is worth emphasizing that
many of these features are based on the grammatical
annotation provided by the task organizers, and as
such, would require a better dependency parser than
we currently have at our disposal to create a fully
automatic system.
That said, there is clearly a great deal of merit to
provide this level of annotation, and we would like
to speculate what would happen if even more de-
tailed annotation, not just grammatical, but also se-
mantical, were provided manually. We hypothesize
that the metonymy task would break down into the
task of identifying several journalistic cliches such
</bodyText>
<page confidence="0.997733">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573344">
<title confidence="0.999093">GYDER: maxent metonymy resolution</title>
<author confidence="0.999986">Rich´ard Farkas</author>
<affiliation confidence="0.998571">University of Szeged Department of Informatics</affiliation>
<address confidence="0.894216">H-6720 Szeged, ´Arp´ad t´er 2.</address>
<email confidence="0.993797">rfarkas@inf.u-szeged.hu</email>
<affiliation confidence="0.913237333333333">Gy¨orgy Szarvas University of Szeged Department of Informatics</affiliation>
<address confidence="0.871762">H-6720 Szeged, ´Arp´ad t´er 2.</address>
<email confidence="0.986229">szarvas@inf.u-szeged.hu</email>
<abstract confidence="0.999091333333333">Though the GYDER system has achieved the highest accuracy scores for the metonymy resolution shared task at SemEval-2007 in all six subtasks, we don’t consider the results (72.80% accuracy for 84.36% for particularly impressive, and argue that metonymy resolution needs more features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christiane Fellbaum ed</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<marker>ed, 1998</marker>
<rawString>Christiane Fellbaum ed. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations. A Preliminary Investigation.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations. A Preliminary Investigation. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Metonymy resolution as a classification task.</title>
<date>2002</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2002).</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="1542" citStr="Markert and Nissim (2002)" startWordPosition="225" endWordPosition="228"> language processing applications ranging from information extraction to machine translation. Metonymic usage of named entities is frequent in natural language. On the basic NER categories person, place, organisation state-of-the-art systems generally perform in the mid to the high nineties. These systems typically do not distinguish between literal or metonymic usage of entity names, even though this would be helpful for most applications. Resolving metonymic usage of proper names would therefore directly benefit NER and indirectly all NLP tasks (such as anaphor resolution) that require NER. Markert and Nissim (2002) outlined a corpusbased approach to proper name metonymy as a semantic classification problem that forms the basis Eszter Simon Budapest U. of Technology Dept. of Cognitive Science H-1111 Budapest, Stoczek u 2. esimon@cogsci.bme.hu D´aniel Varga Budapest U. of Technology MOKK Media Research H-1111 Budapest, Stoczek u 2. daniel@mokk.bme.hu of the 2007 SemEval metonymy resolution task. Instances like ‘He was shocked by Vietnam’ or ‘Schengen boosted tourism’ were assigned to broad categories like place-for-event, sometimes ignoring narrower distinctions, such as the fact that it wasn’t the signin</context>
</contexts>
<marker>Markert, Nissim, 2002</marker>
<rawString>Katja Markert and Malvina Nissim. 2002. Metonymy resolution as a classification task. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2002). Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Syntactic Features and Word Similarity for Supervised Metonymy Resolution.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL2003).</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="5568" citStr="Markert and Nissim (2003)" startWordPosition="854" endWordPosition="857">tures to be the most relevant for metonymy resolution: the grammatical annotations provided for the corpus examples by the task organizers, the determiner, and the grammatical number of the PMW. We also report on some features that didn’t work. 3.1 Grammatical annotations We used the grammatical annotations provided for each PMW in several ways. First, we used as a feature the type of the grammatical relation and the word form of the related word. (If there was more than one related word, each became a feature.) To overcome data sparseness, it is useful to generalize from individual headwords Markert and Nissim (2003). We used three different methods to achieve this: 1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html First, we used Levin’s (1993) verb classification index to generalize the headwords of the most relevant grammatical relations (subject and object). The added feature was simply the class assigned to the verb by Levin. We also used WordNet (Fellbaum 1998) to generalize headwords. First we gathered the hypernym path from WordNet for each headword’s sense#1 in the train corpus. Based on these paths we collected synsets whose tree frequently indicated metonymic sense. We indicated with a</context>
</contexts>
<marker>Markert, Nissim, 2003</marker>
<rawString>Katja Markert and Malvina Nissim. 2003. Syntactic Features and Word Similarity for Supervised Metonymy Resolution. Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL2003). Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malvina Nissim</author>
<author>Katja Markert</author>
</authors>
<title>Learning to buy a Renault and talk to BMW: A supervised approach to conventional metonymy.</title>
<date>2005</date>
<booktitle>International Workshop on Computational Semantics (IWCS2005).</booktitle>
<location>Tilburg, Netherlands.</location>
<contexts>
<context position="4908" citStr="Nissim and Markert (2005)" startWordPosition="744" endWordPosition="747">gorithm always converged before 30 iterations, so the crossvalidation process took only seconds. We also tested the classic C4.5 decision tree learning algorithm Quinlan (1993), but our early experiments showed that the maximum entropy learner was consistently superior to the decision tree classifier for this task, yielding about 2-5% higher accuracy scores on average on both tasks (on the training set, using cross-validation). 3 Feature Engineering We tested several features describing orthographic, syntactic, or semantic characteristics of the Possibly Metonymic Words (PMWs). Here we follow Nissim and Markert (2005), who reported three classes of features to be the most relevant for metonymy resolution: the grammatical annotations provided for the corpus examples by the task organizers, the determiner, and the grammatical number of the PMW. We also report on some features that didn’t work. 3.1 Grammatical annotations We used the grammatical annotations provided for each PMW in several ways. First, we used as a feature the type of the grammatical relation and the word form of the related word. (If there was more than one related word, each became a feature.) To overcome data sparseness, it is useful to ge</context>
<context position="6670" citStr="Nissim and Markert (2005)" startWordPosition="1025" endWordPosition="1028">rain corpus. Based on these paths we collected synsets whose tree frequently indicated metonymic sense. We indicated with a feature if the headword in question was in one of such collected subtrees. Third, we have manually built a very small verb classification ‘Trigger’ table for specific cases. E.g. announce, say, declare all trigger the same feature. This table is the only resource in our final system that was manually built by us, so we note that on the test corpus, disabling this ‘Trigger’ feature does not alter org accuracy, and decreases loc accuracy by 0.44%. 3.2 Determiners Following Nissim and Markert (2005), we distinguished between definite, indefinite, demonstrative, possessive, wh and other determiners. We also marked if the PMW was sentence-initial, and thus necessarily determinerless. This feature was useful for the resolution of organisation PMWs so we used it only for the org tasks. It was not straightforward, however, to assign determiners to the PMWs without proper syntactic analysis. After some experiments, we linked the nearest determiner and the PMW together if we found only adjectives (or nothing) between them. 3.3 Number This feature was particularly useful to separate metonymies o</context>
<context position="9524" citStr="Nissim and Markert (2005)" startWordPosition="1488" endWordPosition="1491"> al. 2006). POS tags around PMWs. Ortographical features such as capitalisation and and other surface characteristics for the PMW and nearby words. Individual tokens of the potentially metonymic phrase. Main category of Levin’s hierarchical classification. Inflectional category of the verb nearest to the PMW in the sentence. 4 Results Table 1. shows the accuracy scores of our submitted system on fine classification granularity. As a baseline, we also evalute the system without the WordNet, Levin, Trigger and PMW word form features. This baseline system is quite similar to the one described by Nissim and Markert (2005). We also publish the majority baseline scores. run majority baseline submitted org train 5-fold 63.30 77.51 80.92 org test 61.76 70.55 72.80 loc train 5-fold 79.68 85.58 88.36 loc test 79.41 83.59 84.36 Table 1: Accuracy of the submitted system We could not exploit the hierarchical structure of the fine-grained tag set, and ended up treating it as totally unstructured even for the mixed class, unlike Nissim and Markert, who apply complicated heuristics to exploit the special semantics of this class. For the coarse and medium subtasks of the loc domain, we simply coarsened the fine-grained res</context>
</contexts>
<marker>Nissim, Markert, 2005</marker>
<rawString>Malvina Nissim and Katja Markert. 2005. Learning to buy a Renault and talk to BMW: A supervised approach to conventional metonymy. International Workshop on Computational Semantics (IWCS2005). Tilburg, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<date>2007</date>
<booktitle>SemEval2007 Task 08: Metonymy Resolution at SemEval2007. In Proceedings of SemEval-2007.</booktitle>
<contexts>
<context position="2651" citStr="Markert and Nissim (2007)" startWordPosition="395" endWordPosition="398">ategories like place-for-event, sometimes ignoring narrower distinctions, such as the fact that it wasn’t the signing of the treaty at Schengen but rather its actual implementation (which didn’t take place at Schengen) that boosted tourism. But the corpus makes clear that even with these (sometimes coarse) class distinctions, several metonymy types seem to appear extremely rarely in actual texts. The shared task focused on two broad named entity classes as metonymic sources, location and org, each having several target classes. For more details on the data sets, see the task description paper Markert and Nissim (2007). Several categories (e.g. place-for-event, organisation-for-index) did not contain a sufficient number of examples for machine learning, and we decided early on to accept the fact that these categories will not be learned and to concentrate on those classes where learning seemed feasible. The shared task itself consisted of 3 subtasks of different granularity for both organisation and location names. The fine-grained evaluation aimed at distinguishing between all categories, while the medium-grained evaluation grouped different types of metonymic usage together and addressed literal / mixed /</context>
</contexts>
<marker>Markert, Nissim, 2007</marker>
<rawString>Katja Markert and Malvina Nissim. 2007. SemEval2007 Task 08: Metonymy Resolution at SemEval2007. In Proceedings of SemEval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Quinlan</author>
</authors>
<title>C4.5: Programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="4459" citStr="Quinlan (1993)" startWordPosition="679" endWordPosition="680">n 5 we offer some speculative remarks on what it would take to improve the results. 2 Learning GYDER (the acronym was formed from the initials of the author’ first names) is a maximum entropy learner. It uses Zhang Le’s 1 maximum entropy toolkit, setting the Gaussian prior to 1. We used random 5-fold cross-validation to determine the usefulness of a particular feature. Due to the small number of instances and features, the learning algorithm always converged before 30 iterations, so the crossvalidation process took only seconds. We also tested the classic C4.5 decision tree learning algorithm Quinlan (1993), but our early experiments showed that the maximum entropy learner was consistently superior to the decision tree classifier for this task, yielding about 2-5% higher accuracy scores on average on both tasks (on the training set, using cross-validation). 3 Feature Engineering We tested several features describing orthographic, syntactic, or semantic characteristics of the Possibly Metonymic Words (PMWs). Here we follow Nissim and Markert (2005), who reported three classes of features to be the most relevant for metonymy resolution: the grammatical annotations provided for the corpus examples </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Ross Quinlan. 1993. C4.5: Programs for machine learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
<author>Rich´ard Farkas</author>
<author>Andr´as Kocsor</author>
</authors>
<title>Multilingual Named Entity Recognition System Using Boosting and C4.5 Decision Tree Learning Algorithms.</title>
<date>2006</date>
<booktitle>Proceedings of Discovery Science 2006, DS2006, LNAI</booktitle>
<pages>4265--267</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="8909" citStr="Szarvas et al. 2006" startWordPosition="1391" endWordPosition="1394">idn’t use this feature. 3.5 Unsuccessful features Here we discuss those features where crossvalidation didn’t show improvements (and thus were not included in the submitted system). Trigger words were automatically collected lists of word forms and phrases that more frequently appeared near metonymic PMWs. Expert triggers were similar trigger words or phrases, but suggested by a linguist expert to be potentially indicative for metonymic usage. We experimented with sample-level, sentencelevel and vicinity trigger phrases. Named entity labels given by a state-of-the-art named entity recognizer (Szarvas et al. 2006). POS tags around PMWs. Ortographical features such as capitalisation and and other surface characteristics for the PMW and nearby words. Individual tokens of the potentially metonymic phrase. Main category of Levin’s hierarchical classification. Inflectional category of the verb nearest to the PMW in the sentence. 4 Results Table 1. shows the accuracy scores of our submitted system on fine classification granularity. As a baseline, we also evalute the system without the WordNet, Levin, Trigger and PMW word form features. This baseline system is quite similar to the one described by Nissim and</context>
</contexts>
<marker>Szarvas, Farkas, Kocsor, 2006</marker>
<rawString>Gy¨orgy Szarvas, Rich´ard Farkas and Andr´as Kocsor. 2006. Multilingual Named Entity Recognition System Using Boosting and C4.5 Decision Tree Learning Algorithms. Proceedings of Discovery Science 2006, DS2006, LNAI 4265 pp. 267-278. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>