<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002082">
<title confidence="0.979825">
VCA: An Experiment With A Multiparty Virtual Chat Agent
</title>
<author confidence="0.999613">
Samira Shaikh1, Tomek Strzalkowski1, 2, Sarah Taylor3, Nick Webb1
</author>
<affiliation confidence="0.984779666666667">
1ILS Institute, University at Albany, State University of New York
2Institute of Computer Science, Polish Academy of Sciences
3Advancded Technology Office, Lockheed Martin IS&amp;GS
</affiliation>
<email confidence="0.998197">
E-mail: ss578726@albany.edu, tomek@albany.edu
</email>
<sectionHeader confidence="0.993909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999616">
The purpose of this research was to advance
the understanding of the behavior of small
groups in online chat rooms. The research was
conducted using Internet chat data collected
through planned exercises with recruited par-
ticipants. Analysis of the collected data led to
construction of preliminary models of social
behavior in online discourse. Some of these
models, e.g., how to effectively change the
topic of conversation, were subsequently im-
plemented into an automated Virtual Chat
Agent (VCA) prototype. VCA has been dem-
onstrated to perform effectively and convinc-
ingly in Internet conversation in multiparty
chat environments.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999994723076923">
Internet chat rooms provide a ready means of
communication for people of most age groups
these days. More often than not, these virtual
chat rooms have multiple participants conversing
on a wide variety of topics, using a highly infor-
mal and free-form text dialect. An increasing use
of virtual chat rooms by a variety of demograph-
ics such as small children and impressionable
youth leads to the risk of exploitation by deceit-
ful individuals or organizations. Such risks might
be reduced by presence of virtual chat agents that
could keep conversations from progressing into
certain topics by changing the topic of conversa-
tion.
Our aim was to study the behavior of small
groups of online chat participants and derive
models of social phenomena that occur fre-
quently in a virtual chat environment. We used
the MPC chat corpus (Shaikh et al., 2010), which
is 20 hours of multi-party chat data collected
through a series of carefully designed online chat
sessions. Chat data collected from public chat
rooms, while easily available, presents signifi-
cant concerns regarding its adaptability for our
research use. Publicly available chat data is com-
pletely anonymous, has a high level of noise and
lack of focus, in addition to engendering user
privacy issues for its use in modeling tasks. The
MPC corpus was used in (1) understanding how
certain social behaviors are reflected in language
and (2) building an automated chat agent that
could effectively achieve certain (initially lim-
ited) social objectives in the chat-room. A brief
description of the MPC corpus and its relevant
characteristics is given in Section 3 of this paper.
One specific phenomenon of social behavior
we wanted to model was an effective change of
conversation topic, when a participant or a group
of participants deliberately (if perhaps only tem-
porarily) shift the discussion to a different, pos-
sibly related topic. Both success and failure of
these actions was of interest because the outcome
depended upon the choice of utterance, the per-
sons to whom it was addressed, their reaction,
and the time when it was produced. Our analysis
of the corpus for such phenomena led to the use
of an annotation scheme that allows us to anno-
tate for topic and focus change in conversation.
We describe the annotation scheme used in Sec-
tion 4.
We constructed an autonomous virtual chat
agent (VCA) that could achieve initially limited
social goals in a chat room with human partici-
pants. We used a novel approach of exploiting
the topic of conversation underway to search the
web and find related topics that could be inserted
in the conversation to change its flow. We tested
the first prototype with the capability to opportu-
nistically change to topic of conversation using a
combination of linguistic, dialogic, and topic
reference devices, which we observed effectively
deployed by the most influential chat participants
in the MPC corpus. The VCA design, architec-
ture and mode of operation are described in de-
tail in Section 5 of this paper.
</bodyText>
<sectionHeader confidence="0.999381" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9066875">
Automated dialogue agents such as the early
ELIZA (Weizenbaum, 1966) and PARRY
</bodyText>
<page confidence="0.997606">
43
</page>
<note confidence="0.630838">
Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 43–48,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999788461538462">
(Colby, 1974) could conduct a one-on-one “con-
versation” with a human using rules and pattern-
matching algorithms. More recently, the addition
of heuristic pattern matching in A.L.I.C.E
(Wallace, 2008) led to development of chat bots
using AIML1 and its variations, such as Project
CyN2. Most of the work on conversational agents
was limited to one-on-one situations, where a
single agent converses with a human user,
whether to perform a transaction (such as book-
ing a flight or banking transactions) (Hardy et al.,
2006) or for companionship (e.g., browsing of
family photographs) (Wilks, 2010). Many of
these systems were inspired by the challenge of
the Turing Test or its more limited variants such
as Loebner Prize.
Research in the field of developing a multi-user
chat-room agent has been limited. This is some-
what surprising because a multi-user setting
makes the agent’s task of maintaining conversa-
tion far less onerous than in one-on-one situa-
tions. In a chat-room, with many users engaged
in conversations, it is much easier for an agent to
pass as just another user. Indeed, a skillfully de-
signed agent may be able to influence an ongoing
conversation.
</bodyText>
<sectionHeader confidence="0.995939" genericHeader="method">
3 MPC Chat Corpus
</sectionHeader>
<bodyText confidence="0.999963076923077">
The MPC chat corpus is a collection of 20 hours
of chat sessions with multiple participants (on
average 4), conversing for about 90 minutes in a
secure online chat room. The topics of conversa-
tion vary from free-flowing chat in the initial
collection phase to allow participants to build
comfortable a rapport with each other, to specific
task-oriented dialogues in the latter phase; such
as choosing the right candidate for a job inter-
view from a list of given resumes. This corpus is
suitable for our research purposes since the chat
sessions were designed around enabling the so-
cial phenomena we were interested in modeling.
</bodyText>
<sectionHeader confidence="0.918444" genericHeader="method">
4 Annotation Scheme
</sectionHeader>
<bodyText confidence="0.997617625">
We wished to annotate the data we collected to
derive models from language use for social phe-
nomena. These represent complex pragmatic
concepts that are difficult to annotate directly, let
alone detect automatically. Our approach was to
build a multi-level annotation scheme.
In this paper we briefly outline our annotation
scheme that consists of three layers: communica-
</bodyText>
<footnote confidence="0.993778">
1 http://www.alicebot.org/aiml.html
2 http://www.daxtron.com/123start.htm?Cyn
</footnote>
<bodyText confidence="0.999945">
tive links, dialogue acts, and topic/focus changes.
A more detailed description of the annotation
scheme will be presented in a future publication.
</bodyText>
<subsectionHeader confidence="0.995131">
4.1 Communicative Links
</subsectionHeader>
<bodyText confidence="0.9999702">
Annotators are asked to mark each utterance in
one of three categories – utterance is addressed
to a participant or a set of participants, it is in
response to a specific prior utterance by another
participant or it is a continuation of the partici-
pant’s own prior utterance. By an utterance, we
mean the set of words in a single turn by a par-
ticipant. In multi-party chat, participants do not
generally add addressing information in their
utterances and it is often ambiguous to whom
they are speaking. Communicative link annota-
tion allows us to accurately map who is speaking
to whom in the conversation, which is required
for tracking social phenomena across partici-
pants.
</bodyText>
<subsectionHeader confidence="0.979262">
4.2 Dialogue Acts
</subsectionHeader>
<bodyText confidence="0.999987285714286">
At this annotation level, we developed a hierar-
chy of 20 dialogue acts, based loosely on
DAMSL (Allen &amp; Core, 1997) and SWBD-
DAMSL (Jurafsky et al., 1997), but greatly re-
duced and more tuned to dialogue pragmatics.
For example, the utterance “It is cold here today”
may function as a Response-Answer when given
in response to a question about the weather, and
would act as an Assertion-Opinion if it is evalu-
ated alone. The dialogue acts, thus augmented,
become an important feature in modeling partici-
pant behavior for our research purpose. A de-
tailed description of the tags is beyond the scope
of this paper.
</bodyText>
<subsectionHeader confidence="0.995775">
4.3 Topic and Focus boundaries
</subsectionHeader>
<bodyText confidence="0.999949058823529">
The flow of discussion in chat shifts quite rapidly
from one topic to another. Furthermore, within
each topic (e.g., music bands) the focus of conver-
sation (e.g., dc for cutie) moves just as rapidly. We
distinguish between topic and focus to accom-
modate both broader thematic shifts and more
narrow aspect changes of the topic being dis-
cussed. For example, participants might discuss
the topic of healthcare reform, by focusing on
President Obama, and then switch the focus to some
particulars of the reform, such as the “public op-
tion”. Similarly, topics may shift while the focus
remains the same (e.g., moving on to Obama’s
economic policies), although such changes are
less common. Annotators typically marked the
first mention of a substantive noun phrase as a
topic or focus introduction.
</bodyText>
<page confidence="0.995059">
44
</page>
<bodyText confidence="0.999654166666667">
The effect of topic change is apparent when a
subsequent utterance by another participant is
about the same topic. This is a successful attempt
at changing the topic. Shown in Figure 1 is an
example of topic shift annotated in our data col-
lection.
</bodyText>
<figureCaption confidence="0.354507">
AA 1: did anyone watch the morning talk
shows today (MTP, for example)?
KA 2: nope!
AA 3: I missed them Ð I was hoping
someone else had.
AA 4: My kids tell me the band you’re
going to hear (dc for cutie) is great.
(TOPIC: music bands, FOCUS: dc for cutie)
KA 5: oh cool! Their lyrics are nice, I
think.
</figureCaption>
<table confidence="0.813227714285714">
(TOPIC: music bands, FOCUS: dc for cutie)
KA 6. what kind of music do you guys
listen to?
(TOPIC: music, FOCUS: none)
KN 7: I don’t really have a favorite
genreÉ.you on youtube right now?
(TOPIC: music, FOCUS: youtube)
</table>
<figureCaption confidence="0.997675">
Figure 1. A topic change in dialogue, with three
participants (AA, KA and KN)
</figureCaption>
<bodyText confidence="0.998126255813954">
We found this model of topic change fairly con-
sistently exhibited, where the participants would
ask an open question, in order to get other par-
ticipants to respond to them, thereby changing
the course of conversation. We collected all ut-
terances marked topic shifts and focus shifts and
created a set of templates from them. These
templates served as a model for the VCA to util-
ize when creating a response.
Another model of behavior that we found as a
consequence of topic change is topic sustain.
This is an instance where the utterance is marked
to be on the same topic as the one currently being
discussed, for example, utterance 5 in Figure 1.
These may be in the form of offering support or
agreement with a previous utterance or asking a
question about a new in-topic aspect.
We gave our annotators a fair amount of lev-
erage on how to label the topics and how to rec-
ognize the focus. Our primary interest was in an
accurate detection of topic/focus boundaries and
shifts. Of the 14 sessions we selected from the
MPC corpus, we selected 10 for annotation, with
at least 3 annotators for each session. In Table 1
some of the overall statistics computed from this
set are shown. We computed inter-annotator
agreement on all three levels of our annotation,
i.e. Communication Links, Dialogue Acts and
Topic/Focus Shifts. Topic and Focus shifts had
the highest inter-annotator agreement scores on
different measures such as Krippendorf’s Alpha
(Krippendorff, 1980) and Fliess’ Kappa (Fliess,
1971). In Figure 2, we show inter-annotator
agreement measures on Topic/Focus shift anno-
tation for four of the annotated sessions. Krip-
pendorff’s Alpha and Fleiss’ Kappa measures
show inter-annotator agreement on topic shift
alone, and Conflated Krippendorff’s Alpha
measures show the agreement when topic and
focus are conflated as one category. With such
high degree of agreement, we can reliably derive
models of topic shift behavior from our anno-
tated data.
</bodyText>
<table confidence="0.996474625">
Total Number of Sessions Annotated 10
Number of annotators per file 3
Total Utterances Annotated 4640
Average number of utterances per ses- ~520
sion
Total topics identified per session 174
Total topic shifts identified per ses- 344
sion
</table>
<tableCaption confidence="0.9470515">
Table 1. Selected statistics from annotated
data set
</tableCaption>
<figureCaption confidence="0.841893">
Figure 2. Inter-annotator agreement measures for
Topic/Focus shifts
</figureCaption>
<sectionHeader confidence="0.993661" genericHeader="method">
5 VCA Design
</sectionHeader>
<bodyText confidence="0.999873461538462">
A virtual chat agent is an automated program
with the ability to respond to utterances in chat.
Our VCA is distinctive in its ability to participate
in multi-party chat and manage to steer the flow
of conversation to a new topic. We exploit the
dialogue mechanism underlying HITIQA (Small
et al. 2009) to drive the dialogue in VCA.
The topic as defined by the information con-
tained in the participant’s utterance is used to
mine outside data sources (e.g., a corpus, the
web) in order to locate and learn additional in-
formation about that topic. The objective is to
identify some of the salient concepts that appear
</bodyText>
<figure confidence="0.997946583333333">
0.8
0.6
0.4
0.2
0
1
Krippendorff
&apos;s Alpha
Conflated
Krippendorff
&apos;s Alpha
Fleiss &apos; Kappa
</figure>
<page confidence="0.997767">
45
</page>
<bodyText confidence="0.999974923076923">
associated with the topic, but are not directly
mentioned in the utterance. Such associations
may be postulated because additional concepts
are repeatedly found near the concepts men-
tioned in the utterance.
An illustrative example found in our annotated
corpus is the utterance, “Lars Ulrich might have a
thing or two to say about technology.” Here, the topic
of conversation prior to this utterance was “tech-
nology” and it was changed to “music” after this
utterance. Here, “Lars Ulrich” is the bridge that
connects the two concepts “technology” and “mu-
sic” together.
</bodyText>
<subsectionHeader confidence="0.988422">
5.1 VCA Architecture
</subsectionHeader>
<bodyText confidence="0.999586">
The VCA is composed of the following modules
that interact as shown in Figure 3.
</bodyText>
<subsubsectionHeader confidence="0.868077">
5.1.1 Chat Analyzer
</subsubsectionHeader>
<bodyText confidence="0.9994245">
Every utterance in chat is first analyzed by the
Chat Analyzer component. This process removes
stop words, emoticons and punctuation, as well
as any participant nicknames from the utterance.
We postulate that the remaining content bearing
words in the utterance represent the topic of that
utterance. We call this analyzed utterance our
chat “query” which is sent in parallel to the
Document Retrieval and NL Processing compo-
nent.
</bodyText>
<subsubsectionHeader confidence="0.727824">
5.1.2 Document Retrieval
</subsubsectionHeader>
<bodyText confidence="0.9999506">
The document retrieval process retrieves docu-
ments from either the web or a test document
collection, creating a stable document set for ex-
perimental purposes. Currently, the document
corpus contains about 1Gb of text data.
</bodyText>
<subsectionHeader confidence="0.698226">
5.1.3 Clustering
</subsectionHeader>
<bodyText confidence="0.999993666666666">
We cluster the paragraphs in documents retrieved
using clustering method in Hardy et al. (Hardy et
al., 2009) This process groups the paragraphs
containing salient entities into sets of closely as-
sociated concepts. From each cluster, we choose
the most representative paragraph, usually called
the “seed” paragraph for further NL processing.
Each seed paragraph and the chat query undergo
the same further NL processing sequence.
</bodyText>
<subsubsectionHeader confidence="0.783067">
5.1.4 Natural Language Processing
</subsubsectionHeader>
<bodyText confidence="0.999833777777778">
We process each chat query by performing
stemming, part-of-speech tagging and named-
entity recognition on it. Each seed paragraph is
also run through same three natural language
processing tasks. We are using Stanford POS
tagger for our part-of-speech tagging. For named
entity recognition, we have the ability to choose
between BBN’s IdentiFinder and AeroTextTM
(Taylor, 2004).
</bodyText>
<subsectionHeader confidence="0.721607">
5.1.5 Framing
</subsectionHeader>
<bodyText confidence="0.999738666666667">
We build frames from the entities and attributes
found in both the chat query and the paragraphs..
This work extends the concept of framing devel-
oped for HITIQA (Small et al, 2009) and COL-
LANE (Strzalkowski, 2009). Framing provides
an informative handle on text, which can be ex-
</bodyText>
<figureCaption confidence="0.989508">
Figure 3. VCA Architecture
</figureCaption>
<bodyText confidence="0.999939375">
corpus. We use Google AJAX api for our web
retrieval process and InQuery (Callan et al.,
1992) retrieval engine for our offline mode of
operation to retrieve documents from the test
corpus. The test document corpus was collected
by mining the web for all utterances in our data
ploited to compare the underlying textual repre-
sentations, as we explain in the next section.
</bodyText>
<subsectionHeader confidence="0.456681">
5.1.6 Scoring and Frame Matching
</subsectionHeader>
<bodyText confidence="0.995197">
Using the information in the frames built in the
previous step; we compare the chat query frame
</bodyText>
<page confidence="0.998459">
46
</page>
<bodyText confidence="0.999889538461538">
built from the chat query, to the frames created
from the paragraphs, called paragraph frames.
We assign a score for each paragraph frame
based on how many attributes and their corre-
sponding values match; in the current version of
VCA a very basic approach to counting how
many attribute-value pairs match is taken. Of all
the paragraph frames we select the highest scor-
ing frames and select the attribute-value pairs
that are not part of the chat query frame. For ex-
ample, as shown in Figure 4a below, the chat
utterance “Aruba might be nice!” created the fol-
lowing chat query frame.
</bodyText>
<figure confidence="0.943770066666667">
[POS]
NNP, Aruba
JJ, nice
[ENT] PLACE
a. Example chat query frame
Aruba Entity List:
VALUE = NASCAR and TYPE = ORGANIZATION
and SCORE = 0
VALUE = Dallas and TYPE = PLACE and SCORE =
1
VALUE = Mateo and TYPE = PERSON and SCORE
= 0
VCA: How about Dallas?
b. Frame Matching, Scoring and Template
Selection
</figure>
<figureCaption confidence="0.988719">
Figure 4. From frames to VCA responses
</figureCaption>
<bodyText confidence="0.999964083333333">
Correspondingly, we select all PLACE type en-
tities from the highest-ranking paragraph frames.
These are shown in Figure 4b as Aruba Entity
list. The entities “NASCAR”, “Women Seeking Men”
and “Mateo” are not of entity type – PLACE, we
assign them a score of 0. The score is the fre-
quency of occurrence of that entity in the para-
graph; in this example it is found to be 1. Assign-
ing scores by frequency of occurrence ensures
that the most commonly occurring concept
around the one that is being discussed in the chat
query utterance will be used to respond with.
</bodyText>
<subsubsectionHeader confidence="0.614911">
5.1.7 Template Selection
</subsubsectionHeader>
<bodyText confidence="0.9999765625">
Once we have chosen the entity to respond with,
we select a template from the set of templates for
that entity. These are templates that are created
based on the models created from topic change
utterances annotated in our data set. For a select
group of entities, which are quite frequently en-
countered in our data collection such as PLACE,
PERSON, ORGANIZATION etc., we have a set of
templates specific to that entity type. We also
have several generic templates that may be used
if the entity type does not match the ones that we
have selected. For example, a PLACE specific
template is “Have you ever been to __?” and a PER-
SON specific template is “You heard about __?”. Not
all templates are formulated as questions. An-
other example of a generic template is “__rules!”.
</bodyText>
<sectionHeader confidence="0.966248" genericHeader="method">
6 Example of VCA Interaction
</sectionHeader>
<bodyText confidence="0.9672114">
Figure 5 represents an example of the VCA in
action in a simulated environment; the VCA is
the participant “renee”. We can see how the con-
versation changes from “gun laws” to “hunting”
after renee’s utterance at 11:48 AM.
</bodyText>
<figureCaption confidence="0.923728">
Figure 5. Topic change example
</figureCaption>
<sectionHeader confidence="0.997226" genericHeader="conclusions">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999953363636364">
We ran two tests of this initial VCA prototype
in a public chat-room. VCA was inserted into a
public chat-room with multiple participants on
two separate occasions. The general topic of dis-
cussion during both instances was “anime”. We
have developed an evaluation protocol in order
to test the effectiveness of the VCA prototype in
a realistic setting. The initial metric of VCA ef-
fectiveness is the rate of involvement measured
in the number of utterances generated by the
VCA during the test period. These utterances are
subsequently judged for appropriateness using
the metric developed for the Companions Project
(Webb, 2010). The actual appropriateness anno-
tation scheme can be quite involved, but for this
simple test we reduced the coding to only binary
assessment, so that the VCA utterances were an-
notated as either appropriate or inappropriate,
given the content of the utterance and the flow of
dialogue thus far. Using this coarse grain evalua-
tion on a live chat segment we noted that the
VCA made 9 appropriate utterances and 7 inap-
</bodyText>
<page confidence="0.997698">
47
</page>
<bodyText confidence="0.999963555555556">
propriate utterances, which gives the appropri-
ateness score of 56%. While some of VCA utter-
ances seem inappropriate (i.e., not related to the
conversation topic), we noted also that other
posters generally tolerated these inappropriate
utterances that occurred early in the dialogue.
Moreover, these early inappropriate utterances
did generate appropriate responses from the hu-
man users. This “positive” dynamic changed
gradually as the dialogue progressed, when the
participants began to ignore VCA’s utterances.
While this coarse grained evaluation is useful,
our plan is to conduct evaluation experiments by
recruiting subjects for chat sessions and inserting
the VCA in the discussion. We will measure the
impact of the VCA in the chat session by having
participants fill out post-session questionnaires,
which can elicit their responses regarding (a) if
they detect presence of a VCA at any time during
the dialogue; (b) who was the VCA; (c) who
changed the topic of conversation most often;
and so on. Another metric of interest is the level
of engagement of the VCA, which can be meas-
ured by the number of direct responses to an ut-
terance by the VCA. We are developing the
evaluation process, and report on the results in a
separate publication.
</bodyText>
<sectionHeader confidence="0.998176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999935207792208">
Allen, J. M. Core. (1997). Draft of DAMSL: Dialog
Act Markup in Several Layers.
http://www.cs.rochester.edu/research/cisd/resource
s/damsl/
Callan, J. P., W. B. Croft, and S. M. Harding. 1992.
The INQUERY Retrieval System, in Proceedings of
the 3rd Inter- national Conference on Database and
Expert Systems.
Colby, K.M, Hilf, F.D, and S. Weber. 1972. Turing-
like indistinguishability tests for the validation of a
computer simulation of paranoid processes. In: Ar-
tificial Intelligence , Vol. 3, p. 199-221.
Fleiss, Joseph L. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 74(5):378{382.
Hardy, Hilda, Nobuyuki Shimizu, Tomek Strzalk-
owski, Ting Liu, Bowden Wise and Xinyang
Zhang. 2002. Cross-document summarization by
concept classification. In Proceedings of ACM
SIGIR &apos;02 Conference, pages 121-128, Tampere,
Finland.
Hardy, H., A Biermann, R. Bryce Inouye, A.
McKenzie, T. Strzalkowski, C. Ursu, N. Webb and
M. Wu. 2006. The AMITIES System: Data-Driven
Techniques for Automated Dialogue. In
Speech Communication 48 (3-4), pages 354-
373. Elsevier.
Jurafsky, Dan, Elizabeth Shriberg, and Debra Biasca.
(1997). Switchboard SWBD-DAMSL Shallow-
Discourse-Function Annotation Coders Manual.
http://stripe.colorado.edu/~jurafsky/manual.august
1.html
Krippendorff, Klaus. 1980. Content Analysis, an In-
troduction to its Methodology. Sage Publications,
Thousand Oaks, CA.
Samira S., Tomek Strzalkowski, Sarah Taylor and
Jonathan Smith (2009) Comparing an Integrated
QA system performance - A Preliminary Model.
Proceedings of PACLING Conference, Sapporo,
Japan.
Shaikh, S., Strzalkowski, T., Broadwell, A., Stromer-
Galley, J., Taylor, Sarah and Webb, N. 2010. Pro-
ceedings of LREC Conference, Malta.
Sharon Small and Tomek Strzalkowski. 2009.
HITIQA: High-Quality Intelligence through Inter-
active Question Answering. Journal of Natural
Language Engineering, Vol. 15 (1), pp. 31—54.
Cambridge.
Tomek Strzalkowski, Sarah Taylor, Samira Shaikh,
Ben-Ami Lipetz, Hilda Hardy, Nick Webb, Tony
Cresswell, Min Wu, Yu Zhan, Ting Liu, and Song
Chen. 2009. COLLANE: An experiment in com-
puter-mediated tacit collaboration. In Aspects of
Natural Language Processing (M. Marciniak and
A. Mykowiecka, editors). Springer.
Taylor, Sarah M. 2004. &amp;quot;Information Extraction
Tools: Deciphering Human Language.&amp;quot; IT Profes-
sional. Vol. 06, no. 6, pages: 28-34. Novem-
ber/December, 2004. Online.
http://ieeexplore.ieee.org/iel5/6294/30282/0139087
0.pdf?tp=&amp;arnumber=1390870&amp;isnumber=30282
Wallace, R. 2008. The Anatomy of A.L.I.C.E. In
Parsing the Turing Test. (Robert Epstein,
Gary Roberts and Grace Beber, editors). Springer.
Webb, N., D. Benyon, P. Hansen and O. Mival. 2010.
Evaluating Human-Machine Conversation for Ap-
propriateness. In Proceedings of the 7th Interna-
tional Conference on Language Resources and
Evaluation (LREC2010), Valletta, Malta.
Weizenbaum, Joseph. January 1966. &amp;quot;ELIZA — A
Computer Program For the Study of Natural Lan-
guage Communication Between Man And Ma-
chine&amp;quot;, Communications of the ACM 9 (1): 36–45.
Wilks, Y. 2010. Artificial Companions. In: Y.Wilks
(ed.) Close Engagement with Companions: scien-
tific, economic, psychological and philosophical
perspectives. John Benjamins: Amsterdam.
</reference>
<page confidence="0.999354">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.301959">
<title confidence="0.998587">VCA: An Experiment With A Multiparty Virtual Chat Agent</title>
<author confidence="0.999049">Tomek Sarah Nick</author>
<affiliation confidence="0.835727">Institute, University at Albany, State University of New of Computer Science, Polish Academy of</affiliation>
<address confidence="0.374502">Technology Office, Lockheed Martin</address>
<email confidence="0.998655">E-mail:ss578726@albany.edu,tomek@albany.edu</email>
<abstract confidence="0.9971495">The purpose of this research was to advance the understanding of the behavior of small groups in online chat rooms. The research was conducted using Internet chat data collected through planned exercises with recruited participants. Analysis of the collected data led to construction of preliminary models of social behavior in online discourse. Some of these models, e.g., how to effectively change the topic of conversation, were subsequently implemented into an automated Virtual Chat Agent (VCA) prototype. VCA has been demonstrated to perform effectively and convincingly in Internet conversation in multiparty chat environments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J M Core Allen</author>
</authors>
<title>Draft of DAMSL: Dialog Act Markup in Several Layers.</title>
<date>1997</date>
<note>http://www.cs.rochester.edu/research/cisd/resource s/damsl/</note>
<marker>Allen, 1997</marker>
<rawString>Allen, J. M. Core. (1997). Draft of DAMSL: Dialog Act Markup in Several Layers. http://www.cs.rochester.edu/research/cisd/resource s/damsl/</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Callan</author>
<author>W B Croft</author>
<author>S M Harding</author>
</authors>
<title>The INQUERY Retrieval System,</title>
<date>1992</date>
<booktitle>in Proceedings of the 3rd Inter- national Conference on Database and Expert Systems.</booktitle>
<contexts>
<context position="15393" citStr="Callan et al., 1992" startWordPosition="2505" endWordPosition="2508">guage processing tasks. We are using Stanford POS tagger for our part-of-speech tagging. For named entity recognition, we have the ability to choose between BBN’s IdentiFinder and AeroTextTM (Taylor, 2004). 5.1.5 Framing We build frames from the entities and attributes found in both the chat query and the paragraphs.. This work extends the concept of framing developed for HITIQA (Small et al, 2009) and COLLANE (Strzalkowski, 2009). Framing provides an informative handle on text, which can be exFigure 3. VCA Architecture corpus. We use Google AJAX api for our web retrieval process and InQuery (Callan et al., 1992) retrieval engine for our offline mode of operation to retrieve documents from the test corpus. The test document corpus was collected by mining the web for all utterances in our data ploited to compare the underlying textual representations, as we explain in the next section. 5.1.6 Scoring and Frame Matching Using the information in the frames built in the previous step; we compare the chat query frame 46 built from the chat query, to the frames created from the paragraphs, called paragraph frames. We assign a score for each paragraph frame based on how many attributes and their corresponding</context>
</contexts>
<marker>Callan, Croft, Harding, 1992</marker>
<rawString>Callan, J. P., W. B. Croft, and S. M. Harding. 1992. The INQUERY Retrieval System, in Proceedings of the 3rd Inter- national Conference on Database and Expert Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M Colby</author>
<author>F D Hilf</author>
<author>S Weber</author>
</authors>
<title>Turinglike indistinguishability tests for the validation of a computer simulation of paranoid processes.</title>
<date>1972</date>
<journal>In: Artificial Intelligence ,</journal>
<volume>3</volume>
<pages>199--221</pages>
<marker>Colby, Hilf, Weber, 1972</marker>
<rawString>Colby, K.M, Hilf, F.D, and S. Weber. 1972. Turinglike indistinguishability tests for the validation of a computer simulation of paranoid processes. In: Artificial Intelligence , Vol. 3, p. 199-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>74</volume>
<issue>5</issue>
<marker>Fleiss, 1971</marker>
<rawString>Fleiss, Joseph L. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 74(5):378{382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hilda Hardy</author>
<author>Nobuyuki Shimizu</author>
<author>Tomek Strzalkowski</author>
<author>Ting Liu</author>
<author>Bowden Wise</author>
<author>Xinyang Zhang</author>
</authors>
<title>Cross-document summarization by concept classification.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGIR &apos;02 Conference,</booktitle>
<pages>121--128</pages>
<location>Tampere, Finland.</location>
<marker>Hardy, Shimizu, Strzalkowski, Liu, Wise, Zhang, 2002</marker>
<rawString>Hardy, Hilda, Nobuyuki Shimizu, Tomek Strzalkowski, Ting Liu, Bowden Wise and Xinyang Zhang. 2002. Cross-document summarization by concept classification. In Proceedings of ACM SIGIR &apos;02 Conference, pages 121-128, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hardy</author>
<author>A Biermann</author>
<author>R Bryce Inouye</author>
<author>A McKenzie</author>
<author>T Strzalkowski</author>
<author>C Ursu</author>
<author>N Webb</author>
<author>M Wu</author>
</authors>
<title>The AMITIES System: Data-Driven Techniques for Automated Dialogue.</title>
<date>2006</date>
<journal>In Speech Communication</journal>
<volume>48</volume>
<pages>3--4</pages>
<publisher>Elsevier.</publisher>
<contexts>
<context position="4801" citStr="Hardy et al., 2006" startWordPosition="760" endWordPosition="763">2010, pages 43–48, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics (Colby, 1974) could conduct a one-on-one “conversation” with a human using rules and patternmatching algorithms. More recently, the addition of heuristic pattern matching in A.L.I.C.E (Wallace, 2008) led to development of chat bots using AIML1 and its variations, such as Project CyN2. Most of the work on conversational agents was limited to one-on-one situations, where a single agent converses with a human user, whether to perform a transaction (such as booking a flight or banking transactions) (Hardy et al., 2006) or for companionship (e.g., browsing of family photographs) (Wilks, 2010). Many of these systems were inspired by the challenge of the Turing Test or its more limited variants such as Loebner Prize. Research in the field of developing a multi-user chat-room agent has been limited. This is somewhat surprising because a multi-user setting makes the agent’s task of maintaining conversation far less onerous than in one-on-one situations. In a chat-room, with many users engaged in conversations, it is much easier for an agent to pass as just another user. Indeed, a skillfully designed agent may be</context>
</contexts>
<marker>Hardy, Biermann, Inouye, McKenzie, Strzalkowski, Ursu, Webb, Wu, 2006</marker>
<rawString>Hardy, H., A Biermann, R. Bryce Inouye, A. McKenzie, T. Strzalkowski, C. Ursu, N. Webb and M. Wu. 2006. The AMITIES System: Data-Driven Techniques for Automated Dialogue. In Speech Communication 48 (3-4), pages 354-373. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Debra Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL ShallowDiscourse-Function Annotation Coders Manual.</title>
<date>1997</date>
<note>http://stripe.colorado.edu/~jurafsky/manual.august 1.html</note>
<contexts>
<context position="7579" citStr="Jurafsky et al., 1997" startWordPosition="1210" endWordPosition="1213">inuation of the participant’s own prior utterance. By an utterance, we mean the set of words in a single turn by a participant. In multi-party chat, participants do not generally add addressing information in their utterances and it is often ambiguous to whom they are speaking. Communicative link annotation allows us to accurately map who is speaking to whom in the conversation, which is required for tracking social phenomena across participants. 4.2 Dialogue Acts At this annotation level, we developed a hierarchy of 20 dialogue acts, based loosely on DAMSL (Allen &amp; Core, 1997) and SWBDDAMSL (Jurafsky et al., 1997), but greatly reduced and more tuned to dialogue pragmatics. For example, the utterance “It is cold here today” may function as a Response-Answer when given in response to a question about the weather, and would act as an Assertion-Opinion if it is evaluated alone. The dialogue acts, thus augmented, become an important feature in modeling participant behavior for our research purpose. A detailed description of the tags is beyond the scope of this paper. 4.3 Topic and Focus boundaries The flow of discussion in chat shifts quite rapidly from one topic to another. Furthermore, within each topic (</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Jurafsky, Dan, Elizabeth Shriberg, and Debra Biasca. (1997). Switchboard SWBD-DAMSL ShallowDiscourse-Function Annotation Coders Manual. http://stripe.colorado.edu/~jurafsky/manual.august 1.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis, an Introduction to its Methodology. Sage Publications,</title>
<date>1980</date>
<location>Thousand Oaks, CA.</location>
<contexts>
<context position="11180" citStr="Krippendorff, 1980" startWordPosition="1835" endWordPosition="1836"> the topics and how to recognize the focus. Our primary interest was in an accurate detection of topic/focus boundaries and shifts. Of the 14 sessions we selected from the MPC corpus, we selected 10 for annotation, with at least 3 annotators for each session. In Table 1 some of the overall statistics computed from this set are shown. We computed inter-annotator agreement on all three levels of our annotation, i.e. Communication Links, Dialogue Acts and Topic/Focus Shifts. Topic and Focus shifts had the highest inter-annotator agreement scores on different measures such as Krippendorf’s Alpha (Krippendorff, 1980) and Fliess’ Kappa (Fliess, 1971). In Figure 2, we show inter-annotator agreement measures on Topic/Focus shift annotation for four of the annotated sessions. Krippendorff’s Alpha and Fleiss’ Kappa measures show inter-annotator agreement on topic shift alone, and Conflated Krippendorff’s Alpha measures show the agreement when topic and focus are conflated as one category. With such high degree of agreement, we can reliably derive models of topic shift behavior from our annotated data. Total Number of Sessions Annotated 10 Number of annotators per file 3 Total Utterances Annotated 4640 Average </context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Krippendorff, Klaus. 1980. Content Analysis, an Introduction to its Methodology. Sage Publications, Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Samira</author>
<author>Tomek Strzalkowski</author>
<author>Sarah Taylor</author>
<author>Jonathan Smith</author>
</authors>
<title>Comparing an Integrated QA system performance - A Preliminary Model.</title>
<date>2009</date>
<booktitle>Proceedings of PACLING Conference,</booktitle>
<location>Sapporo, Japan.</location>
<marker>Samira, Strzalkowski, Taylor, Smith, 2009</marker>
<rawString>Samira S., Tomek Strzalkowski, Sarah Taylor and Jonathan Smith (2009) Comparing an Integrated QA system performance - A Preliminary Model. Proceedings of PACLING Conference, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shaikh</author>
<author>T Strzalkowski</author>
<author>A Broadwell</author>
<author>J StromerGalley</author>
<author>Sarah Taylor</author>
<author>N Webb</author>
</authors>
<date>2010</date>
<booktitle>Proceedings of LREC Conference,</booktitle>
<contexts>
<context position="1852" citStr="Shaikh et al., 2010" startWordPosition="282" endWordPosition="285">y informal and free-form text dialect. An increasing use of virtual chat rooms by a variety of demographics such as small children and impressionable youth leads to the risk of exploitation by deceitful individuals or organizations. Such risks might be reduced by presence of virtual chat agents that could keep conversations from progressing into certain topics by changing the topic of conversation. Our aim was to study the behavior of small groups of online chat participants and derive models of social phenomena that occur frequently in a virtual chat environment. We used the MPC chat corpus (Shaikh et al., 2010), which is 20 hours of multi-party chat data collected through a series of carefully designed online chat sessions. Chat data collected from public chat rooms, while easily available, presents significant concerns regarding its adaptability for our research use. Publicly available chat data is completely anonymous, has a high level of noise and lack of focus, in addition to engendering user privacy issues for its use in modeling tasks. The MPC corpus was used in (1) understanding how certain social behaviors are reflected in language and (2) building an automated chat agent that could effectiv</context>
</contexts>
<marker>Shaikh, Strzalkowski, Broadwell, StromerGalley, Taylor, Webb, 2010</marker>
<rawString>Shaikh, S., Strzalkowski, T., Broadwell, A., StromerGalley, J., Taylor, Sarah and Webb, N. 2010. Proceedings of LREC Conference, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Small</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>HITIQA: High-Quality Intelligence through Interactive Question Answering.</title>
<date>2009</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<pages>31--54</pages>
<location>Cambridge.</location>
<marker>Small, Strzalkowski, 2009</marker>
<rawString>Sharon Small and Tomek Strzalkowski. 2009. HITIQA: High-Quality Intelligence through Interactive Question Answering. Journal of Natural Language Engineering, Vol. 15 (1), pp. 31—54. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen</author>
</authors>
<title>COLLANE: An experiment in computer-mediated tacit collaboration.</title>
<date>2009</date>
<booktitle>In Aspects of Natural Language Processing</booktitle>
<editor>Tomek Strzalkowski, Sarah Taylor, Samira Shaikh, Ben-Ami Lipetz, Hilda Hardy, Nick Webb, Tony Cresswell, Min Wu, Yu Zhan, Ting Liu, and Song</editor>
<publisher>Springer.</publisher>
<marker>Chen, 2009</marker>
<rawString>Tomek Strzalkowski, Sarah Taylor, Samira Shaikh, Ben-Ami Lipetz, Hilda Hardy, Nick Webb, Tony Cresswell, Min Wu, Yu Zhan, Ting Liu, and Song Chen. 2009. COLLANE: An experiment in computer-mediated tacit collaboration. In Aspects of Natural Language Processing (M. Marciniak and A. Mykowiecka, editors). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah M Taylor</author>
</authors>
<title>Information Extraction Tools: Deciphering Human Language.&amp;quot;</title>
<date>2004</date>
<journal>IT Professional.</journal>
<volume>06</volume>
<pages>28--34</pages>
<note>Online. http://ieeexplore.ieee.org/iel5/6294/30282/0139087 0.pdf?tp=&amp;arnumber=1390870&amp;isnumber=30282</note>
<contexts>
<context position="14978" citStr="Taylor, 2004" startWordPosition="2436" endWordPosition="2437">uster, we choose the most representative paragraph, usually called the “seed” paragraph for further NL processing. Each seed paragraph and the chat query undergo the same further NL processing sequence. 5.1.4 Natural Language Processing We process each chat query by performing stemming, part-of-speech tagging and namedentity recognition on it. Each seed paragraph is also run through same three natural language processing tasks. We are using Stanford POS tagger for our part-of-speech tagging. For named entity recognition, we have the ability to choose between BBN’s IdentiFinder and AeroTextTM (Taylor, 2004). 5.1.5 Framing We build frames from the entities and attributes found in both the chat query and the paragraphs.. This work extends the concept of framing developed for HITIQA (Small et al, 2009) and COLLANE (Strzalkowski, 2009). Framing provides an informative handle on text, which can be exFigure 3. VCA Architecture corpus. We use Google AJAX api for our web retrieval process and InQuery (Callan et al., 1992) retrieval engine for our offline mode of operation to retrieve documents from the test corpus. The test document corpus was collected by mining the web for all utterances in our data p</context>
</contexts>
<marker>Taylor, 2004</marker>
<rawString>Taylor, Sarah M. 2004. &amp;quot;Information Extraction Tools: Deciphering Human Language.&amp;quot; IT Professional. Vol. 06, no. 6, pages: 28-34. November/December, 2004. Online. http://ieeexplore.ieee.org/iel5/6294/30282/0139087 0.pdf?tp=&amp;arnumber=1390870&amp;isnumber=30282</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wallace</author>
</authors>
<title>The Anatomy of A.L.I.C.E.</title>
<date>2008</date>
<booktitle>In Parsing the Turing</booktitle>
<editor>Test. (Robert Epstein, Gary Roberts and Grace Beber, editors).</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="4480" citStr="Wallace, 2008" startWordPosition="708" endWordPosition="709">ential chat participants in the MPC corpus. The VCA design, architecture and mode of operation are described in detail in Section 5 of this paper. 2 Related Work Automated dialogue agents such as the early ELIZA (Weizenbaum, 1966) and PARRY 43 Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 43–48, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics (Colby, 1974) could conduct a one-on-one “conversation” with a human using rules and patternmatching algorithms. More recently, the addition of heuristic pattern matching in A.L.I.C.E (Wallace, 2008) led to development of chat bots using AIML1 and its variations, such as Project CyN2. Most of the work on conversational agents was limited to one-on-one situations, where a single agent converses with a human user, whether to perform a transaction (such as booking a flight or banking transactions) (Hardy et al., 2006) or for companionship (e.g., browsing of family photographs) (Wilks, 2010). Many of these systems were inspired by the challenge of the Turing Test or its more limited variants such as Loebner Prize. Research in the field of developing a multi-user chat-room agent has been limit</context>
</contexts>
<marker>Wallace, 2008</marker>
<rawString>Wallace, R. 2008. The Anatomy of A.L.I.C.E. In Parsing the Turing Test. (Robert Epstein, Gary Roberts and Grace Beber, editors). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Webb</author>
<author>D Benyon</author>
<author>P Hansen</author>
<author>O Mival</author>
</authors>
<title>Evaluating Human-Machine Conversation for Appropriateness.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC2010),</booktitle>
<location>Valletta,</location>
<marker>Webb, Benyon, Hansen, Mival, 2010</marker>
<rawString>Webb, N., D. Benyon, P. Hansen and O. Mival. 2010. Evaluating Human-Machine Conversation for Appropriateness. In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC2010), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Weizenbaum</author>
</authors>
<title>ELIZA — A Computer Program For the Study of Natural Language Communication Between Man And Machine&amp;quot;,</title>
<date>1966</date>
<journal>Communications of the ACM</journal>
<volume>9</volume>
<issue>1</issue>
<pages>36--45</pages>
<contexts>
<context position="4096" citStr="Weizenbaum, 1966" startWordPosition="653" endWordPosition="654">of exploiting the topic of conversation underway to search the web and find related topics that could be inserted in the conversation to change its flow. We tested the first prototype with the capability to opportunistically change to topic of conversation using a combination of linguistic, dialogic, and topic reference devices, which we observed effectively deployed by the most influential chat participants in the MPC corpus. The VCA design, architecture and mode of operation are described in detail in Section 5 of this paper. 2 Related Work Automated dialogue agents such as the early ELIZA (Weizenbaum, 1966) and PARRY 43 Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 43–48, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics (Colby, 1974) could conduct a one-on-one “conversation” with a human using rules and patternmatching algorithms. More recently, the addition of heuristic pattern matching in A.L.I.C.E (Wallace, 2008) led to development of chat bots using AIML1 and its variations, such as Project CyN2. Most of the work on conversational agents was limited to one-on-one situations, where a single agent converses with a human user,</context>
</contexts>
<marker>Weizenbaum, 1966</marker>
<rawString>Weizenbaum, Joseph. January 1966. &amp;quot;ELIZA — A Computer Program For the Study of Natural Language Communication Between Man And Machine&amp;quot;, Communications of the ACM 9 (1): 36–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>Artificial Companions. In:</title>
<date>2010</date>
<booktitle>Close Engagement with Companions: scientific, economic, psychological and philosophical perspectives. John Benjamins:</booktitle>
<editor>Y.Wilks (ed.)</editor>
<location>Amsterdam.</location>
<contexts>
<context position="4875" citStr="Wilks, 2010" startWordPosition="772" endWordPosition="773">ional Linguistics (Colby, 1974) could conduct a one-on-one “conversation” with a human using rules and patternmatching algorithms. More recently, the addition of heuristic pattern matching in A.L.I.C.E (Wallace, 2008) led to development of chat bots using AIML1 and its variations, such as Project CyN2. Most of the work on conversational agents was limited to one-on-one situations, where a single agent converses with a human user, whether to perform a transaction (such as booking a flight or banking transactions) (Hardy et al., 2006) or for companionship (e.g., browsing of family photographs) (Wilks, 2010). Many of these systems were inspired by the challenge of the Turing Test or its more limited variants such as Loebner Prize. Research in the field of developing a multi-user chat-room agent has been limited. This is somewhat surprising because a multi-user setting makes the agent’s task of maintaining conversation far less onerous than in one-on-one situations. In a chat-room, with many users engaged in conversations, it is much easier for an agent to pass as just another user. Indeed, a skillfully designed agent may be able to influence an ongoing conversation. 3 MPC Chat Corpus The MPC chat</context>
</contexts>
<marker>Wilks, 2010</marker>
<rawString>Wilks, Y. 2010. Artificial Companions. In: Y.Wilks (ed.) Close Engagement with Companions: scientific, economic, psychological and philosophical perspectives. John Benjamins: Amsterdam.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>