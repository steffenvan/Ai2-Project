<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.200737">
<sectionHeader confidence="0.730431" genericHeader="abstract">
INVITED KEYNOTE PRESENTATION
</sectionHeader>
<title confidence="0.970378">
Distributional Semantics and the Lexicon
</title>
<author confidence="0.978909">
Eduard Hovy
</author>
<affiliation confidence="0.979871">
Information Sciences Institute
University of Southern California
</affiliation>
<email confidence="0.977082">
hovy@isi.edu
</email>
<bodyText confidence="0.993127857142857">
The lexicons used in computational linguistics
systems contain morphological, syntactic, and
occasionally also some semantic information
(such as definitions, pointers to an ontology,
verb frame filler preferences, etc.). But the
human cognitive lexicon contains a great deal
more, crucially, expectations about how a
word tends to combine with others: not just
general information-extraction-like patterns,
but specific instantial expectations. Such in-
formation is very useful when it comes to lis-
tening in bad aural conditions and reading
texts in which background information is
taken for granted; without such specific ex-
pectation, one would be hard-pressed (and
computers are completely unable) to form co-
herent and richly connected multi-sentence
interpretations.
Over the past few years, NLP work has in-
creasingly treated topic signature word distri-
butions (also called ‘context vectors’, ‘topic
models’, etc.) as a de facto replacement for
semantics. Whether the task is wordsense dis-
ambiguation, certain forms of textual entail-
ment, information extraction, paraphrase
learning, and so on, it turns out to be very use-
ful to consider a word(sense) as being defined
by the distribution of word(senses) that regu-
larly accompany it (in the classic words of
Firth, “you shall know a word by the company
it keeps”). And this is true not only for indi-
vidual wordsenses, but also for larger units such
as topics: the product of LDA and similar topic
characterization engines is similar.
In this talk I argue for a new kind of seman-
tics, which is being called Distributional Se-
mantics. It combines traditional symbolic logic-
based semantics with (computation-based)
statistical word distribution information. The
core resource is a single lexico-semantic lexicon
that can be used for a variety of tasks, provided
that it is reformulated accordingly. I show how
to define such a semantics, how to build the
appropriate lexicon, how to format it, and how
to use it for various tasks. The talk pulls
together a wide range of related topics,
including Pantel-style resources like DIRT,
inferences / expectations such as those used in
Schank-style expectation-based parsing and
expectation-driven NLU, PropBank-style word
valence lexical items, and the treatment of
negation and modalities. I conclude by arguing
that the human cognitive lexicon has to have the
same kinds of properties as the Distributional
Semantics lexicon, given the ways people do
things with words.
</bodyText>
<page confidence="0.435591">
1
</page>
<reference confidence="0.874669">
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), page 1,
Beijing, August 2010
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.503815">
<title confidence="0.99875">Distributional Semantics and the Lexicon</title>
<author confidence="0.999285">Eduard Hovy</author>
<affiliation confidence="0.998675">Information Sciences University of Southern California</affiliation>
<email confidence="0.993729">hovy@isi.edu</email>
<abstract confidence="0.993730689655172">The lexicons used in computational linguistics systems contain morphological, syntactic, and occasionally also some semantic information (such as definitions, pointers to an ontology, verb frame filler preferences, etc.). But the human cognitive lexicon contains a great deal more, crucially, expectations about how a word tends to combine with others: not just general information-extraction-like patterns, but specific instantial expectations. Such information is very useful when it comes to listening in bad aural conditions and reading texts in which background information is taken for granted; without such specific expectation, one would be hard-pressed (and computers are completely unable) to form coherent and richly connected multi-sentence interpretations. Over the past few years, NLP work has intreated signature word districalled ‘context vectors’, ‘topic models’, etc.) as a de facto replacement for semantics. Whether the task is wordsense disambiguation, certain forms of textual entailment, information extraction, paraphrase learning, and so on, it turns out to be very useful to consider a word(sense) as being defined by the distribution of word(senses) that regularly accompany it (in the classic words of Firth, “you shall know a word by the company it keeps”). And this is true not only for individual wordsenses, but also for larger units such the product of LDA and similar topic characterization engines is similar. In this talk I argue for a new kind of semantics, which is being called Distributional Semantics. It combines traditional symbolic logicbased semantics with (computation-based) statistical word distribution information. The core resource is a single lexico-semantic lexicon that can be used for a variety of tasks, provided that it is reformulated accordingly. I show how to define such a semantics, how to build the appropriate lexicon, how to format it, and how to use it for various tasks. The talk pulls together a wide range of related topics, including Pantel-style resources like DIRT, inferences / expectations such as those used in Schank-style expectation-based parsing and expectation-driven NLU, PropBank-style word valence lexical items, and the treatment of negation and modalities. I conclude by arguing that the human cognitive lexicon has to have the same kinds of properties as the Distributional lexicon, given the ways people with 1 of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex page</abstract>
<address confidence="0.661049">Beijing, August 2010</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010),</booktitle>
<pages>1</pages>
<marker></marker>
<rawString>Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), page 1,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beijing</author>
</authors>
<date>2010</date>
<marker>Beijing, 2010</marker>
<rawString>Beijing, August 2010</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>