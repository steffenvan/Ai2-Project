<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000486">
<title confidence="0.978355">
Who caught a cold? — Identifying the subject of a symptom
</title>
<author confidence="0.881762">
Shin Kanouchi†, Mamoru Komachi†, Naoaki Okazaki‡,
Eiji Aramaki§, and Hiroshi Ishikawa†
</author>
<affiliation confidence="0.692381666666667">
†Tokyo Metropolitan University, {kanouchi-shin at ed., komachi at, ishikawh at}tmu.ac.jp
‡Tohoku University, okazaki at ecei.tohoku.ac.jp
§Kyoto University, eiji.aramaki at gmail.com
</affiliation>
<sectionHeader confidence="0.972643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980807692308">
The development and proliferation of so-
cial media services has led to the emer-
gence of new approaches for surveying the
population and addressing social issues.
One popular application of social media
data is health surveillance, e.g., predicting
the outbreak of an epidemic by recogniz-
ing diseases and symptoms from text mes-
sages posted on social media platforms. In
this paper, we propose a novel task that
is crucial and generic from the viewpoint
of health surveillance: estimating a sub-
ject (carrier) of a disease or symptom men-
tioned in a Japanese tweet. By designing
an annotation guideline for labeling the
subject of a disease/symptom in a tweet,
we perform annotations on an existing cor-
pus for public surveillance. In addition,
we present a supervised approach for pre-
dicting the subject of a disease/symptom.
The results of our experiments demon-
strate the impact of subject identification
on the effective detection of an episode of
a disease/symptom. Moreover, the results
suggest that our task is independent of the
type of disease/symptom.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999299442307693">
Social media services, including Twitter and Face-
book, provide opportunities for individuals to
share their experiences, thoughts, and opinions.
The wide use of social media services has led
to the emergence of new approaches for survey-
ing the population and addressing social issues.
One popular application of social media data is
flu surveillance, i.e., predicting the outbreak of in-
fluenza epidemics by detecting mentions of flu in-
fections on social media platforms (Culotta, 2010;
Lampos and Cristianini, 2010; Aramaki et al.,
2011; Paul and Dredze, 2011; Signorini et al.,
2011; Collier, 2012; Dredze et al., 2013; Gesualdo
et al., 2013; Stoové and Pedrana, 2014).
Previous studies mainly relied on shallow tex-
tual clues in Twitter posts in order to predict the
number of flu infections, e.g., the number of oc-
currences of specific keywords (such as “flu” or
“influenza”) on Twitter. However, such a simple
approach can lead to incorrect predictions. Bro-
niatowski et al. (2013) argued that media atten-
tion increases chatter, i.e., the number of tweets
that mention the flu without the poster being ac-
tually infected. Examples include, “I don’t wish
the flu on anyone” and “A Harry Potter actor hos-
pitalised after severe flu-like syndromes.” Lazer
et al. (2014) reported large errors in Google Flu
Trends (Carneiro and Mylonakis, 2009) on the ba-
sis of a comparison with the proportion of doctor
visits for influenza-like illnesses.
Lamb et al. (2013) aimed to improve the ac-
curacy of detecting mentions of flu infections.
Their method trains a binary classifier to distin-
guish tweets reporting flu infections from those
expressing concern or awareness about the flu,
e.g., “Starting to get worried about swine flu.” Ac-
cordingly, they reported encouraging results (e.g.,
better correlations with CDC trends), but their ap-
proach requires supervision data and a lexicon
(word class features) specially designed for the flu.
Moreover, even though this method is a reason-
able choice for improving the accuracy, it is not
readily applicable to other types of diseases (e.g.,
dengue fever) and symptoms (e.g., runny nose),
which are also important for public health (Velardi
et al., 2014).
In this paper, we propose a more generalized
task setting for public surveillance. In other
words, our objective is to estimate the subject
(carrier) of a disease or symptom mentioned in a
Japanese tweet. More specifically, we are inter-
ested in determining who has a disease/symptom
</bodyText>
<page confidence="0.90917">
1660
</page>
<note confidence="0.975303">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1660–1670,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.994654727272727">
(if any) in order to examine whether the poster suf-
fers from the disease or symptom. For example,
given the sentence “I caught a cold,” we would
predict that the first person (“I,” i.e., the poster)
is the subject (carrier) of the cold. On the other
hand, we can ignore the sentence, “The TV pre-
senter caught a cold” only if we predict that the
subject of the cold is the third person, who is at a
different location from the poster.
Although the task setting is simple and intuitive,
we identify several key challenges in this study.
</bodyText>
<listItem confidence="0.657892925925926">
1. Novel task setting. The task of identifying
the subject of a disease/symptom is similar
to predicate-argument structure (PAS) anal-
ysis for nominal predicates (Meyers et al.,
2004; Sasano et al., 2004; Komachi et al.,
2007; Gerber and Chai, 2010). However,
these studies do not treat diseases (e.g., “in-
fluenza”) and symptoms (e.g., “headache”) as
nominal predicates. To the best of our knowl-
edge, this task has not been explored in natu-
ral language processing (NLP) thus far.
2. Identifying whether the subject has a dis-
ease/symptom. Besides the work on PAS
analysis for nominal predicates, the most rel-
evant work is PAS analysis for verb predi-
cates. However, our task is not as simple as
predicting the subject of the verb governing
a disease/symptom-related noun. For exam-
ple, the subject of the verb “beat” is the first
person “I” in the sentence “I beat the flu,” but
this does not imply that the poster has the flu.
At the same time, we can use a variety of
expressions for indicating an infection, e.g.,
“I’m still sick!! This flu is just incredible...,”
“I can feel the flu bug in me,” and “I tested
positive for the flu.”
3. Omitted subjects. We often come across
</listItem>
<bodyText confidence="0.693651363636364">
tweets with omitted subjects, e.g., “Down
with the flu feel” and “Thanks the flu for
striking in hard this week” even in English
tweets. Because the first person is omitted
frequently, it is important to predict omitted
subjects from the viewpoint of the applica-
tion (public surveillance).
In this paper, we present an approach for iden-
tifying the subjects of various types of diseases
and symptoms. The contributions of this paper are
three-fold.
</bodyText>
<listItem confidence="0.923276391304348">
1. In order to explore a novel and general task
setting, we design an annotation guideline for
labeling a subject of a disease/symptom in a
tweet, and we deliver annotations in an exist-
ing corpus for public surveillance. Further,
we propose a method for predicting the sub-
ject of a disease/symptom by using the anno-
tated corpus.
2. The experimental results show that the task
of identifying subjects is independent of the
type of diseases/symptom. We verify the
possibility of transferring supervision data to
different targets of diseases and symptoms.
In other words, we verify that it is possi-
ble to utilize the supervision data for a par-
ticular disease/symptom to improve the ac-
curacy of predicting subjects of another dis-
ease/symptom.
3. In addition, the experimental results demon-
strate the impact of identifying subjects on
improving the accuracy of the downstream
application (identification of an episode of a
disease/symptom).
</listItem>
<bodyText confidence="0.999866615384615">
The remainder of this paper is organized as fol-
lows. Section 2 describes the corpus used in this
study as well as our annotation work for identify-
ing subjects of diseases and symptoms. Section
3.1 presents our method for predicting subjects on
the basis of the annotated corpus. Sections 3.2
and 3.3 report the performance of the proposed
method. Section 3.4 describes the contributions
of this study toward identifying episodes of dis-
eases and symptoms. Section 4 reviews some re-
lated studies. Finally, Section 5 summarizes our
findings and concludes the paper with a brief dis-
cussion on the scope for future work.
</bodyText>
<sectionHeader confidence="0.996483" genericHeader="introduction">
2 Corpus
</sectionHeader>
<subsectionHeader confidence="0.999088">
2.1 Target corpus
</subsectionHeader>
<bodyText confidence="0.999962111111111">
We used a Japanese corpus for public surveil-
lance of diseases and symptoms (Aramaki et al.,
2011). The corpus targets seven types of dis-
eases and symptoms: cold, cough, headache, chill,
runny nose, fever, and sore throat. Tweets con-
taining keywords for each disease/symptom were
collected using the Twitter Search API: for exam-
ple, tweets about sore throat were collected using
the query “(sore OR pain) AND throat”. Further,
</bodyText>
<page confidence="0.996399">
1661
</page>
<figureCaption confidence="0.999931">
Figure 1: Examples of annotations of subject labels.
</figureCaption>
<bodyText confidence="0.5756682">
Subject label Definition Example
FIRSTPERSON The subject of the disease/symptom is the poster I wish I have fever or some-
of the tweet. thing so that I don’t have to
go to school.
NEARBYPERSON The subject of the disease/symptom is a person my sister continues to have
whom the poster can directly see or hear. a high fever...
FARAWAYPERSON The subject of the disease/symptom is a person @***** does sour stuff
who is at a different location from the poster. give you a headache?
NONHUMAN The subject of the disease/symptom is not a per- My room is so chill. But I
son. Alternatively, the sentence does not describe like it.
a disease/symptom but a phenomenon or event re-
lated to the disease/symptom.
NONE The subject of the disease/symptom does not ex- I hate buyin cold medicine
ist. Alternatively, the sentence does not mention cuz I never know which
an occurrence of a disease/symptom. one to buy
</bodyText>
<tableCaption confidence="0.973853">
Table 1: Definitions of subject labels and example tweets.
</tableCaption>
<bodyText confidence="0.999602461538461">
the corpus consists of 1,000 tweets for each dis-
ease/symptom besides cold, and 5,000 tweets for
cold. The corpus was collected through whole
years 2007-2008. This period was not in the
A/H1N1 flu pandemic season.
An instance in this corpus consists of a tweet
text (in Japanese) and a binary label (episode la-
bel, hereafter) indicating whether someone near
the poster has the target disease/symptom1. A pos-
itive episode indicates an occurrence of the dis-
ease/symptom. In this study, we disregarded in-
stances of sore throat in the experiments because
most such instances were positive episodes2.
</bodyText>
<footnote confidence="0.998610666666667">
1This label is positive if someone mentioned in the tweet
is in the same prefecture as the poster. This is because the cor-
pus was designed to survey the spread of a disease/symptom
in every prefecture.
2In Japanese tweets, sore throat or throat pain mostly de-
scribes the health condition of the poster.
</footnote>
<subsectionHeader confidence="0.999888">
2.2 Annotating subjects
</subsectionHeader>
<bodyText confidence="0.9994913125">
In this study, we annotated the subjects of diseases
and symptoms in the corpus described in Section
2.1. Specifically, we annotated the subjects in 500
tweets for each disease/symptom (except for sore
throat). Thus, our corpus includes a total of 3,000
tweets in which the subjects of diseases and symp-
toms are annotated.
Figure 1 shows examples of annotations in
this study. Episode labels, tweet texts, and dis-
ease/symptom keywords were annotated by Ara-
maki et al. (2011) in the corpus.
We annotated the subject labels of the dis-
eases/symptoms in each tweet and identified those
who had the target disease/symptom. The sub-
ject labels indicate those who have the correspond-
ing disease/symptom; they are described in detail
</bodyText>
<page confidence="0.971184">
1662
</page>
<table confidence="0.999620333333333">
Label FIRSTPERSON NEARBYPERSON FARAWAYPERSON NONHUMAN NONE Total
#tweets 2,153 129 201 40 401 2,924
# explicit subjects 70 (3.3%) 112 (86.8%) 175 (87.1%) 38 (95.0%) 0 (0.0%) 395
# positive episodes 1,833 99 2 0 16 1,950
# negative episodes 320 30 199 40 385 974
Positive ratio 85.1% 76.7% 1.0% 0.0% 4.0% 66.7%
</table>
<tableCaption confidence="0.7396445">
Table 2: Associations between subject labels and positive/negative episodes of diseases and symptoms.
herein.
</tableCaption>
<bodyText confidence="0.99853734939759">
In addition to the subject labels, we annotated
the text span that indicates a subject. However, the
subjects of diseases/symptoms are often omitted in
tweet texts. Example 3 in Figure 1 shows a case
in which the subject is omitted. The information
as to whether the subject is omitted is useful for
analyzing the difficulty in predicting the subject
of a disease/symptom.
Table 1 lists the definitions of the subject la-
bels with tweeted examples. Because it is impor-
tant to distinguish the primary information (infor-
mation that is observed and experienced by the
poster) from the secondary information (informa-
tion that is broadcasted by the media) for the ap-
plication of public surveillance, we introduced five
labels: FIRSTPERSON, NEARBYPERSON, FAR-
AWAYPERSON, NONHUMAN, and NONE.
FIRSTPERSON is assigned when the subject of
the disease/symptom is the poster of the tweet.
When annotating this label, we ignore the modal-
ity or factuality of the event of acquiring the dis-
ease/symptom. For example, the example tweet
corresponding to FIRSTPERSON in Table 1 does
not state that the poster has a fever but only that
the poster has a desire to have a fever. Although
such tweets may be inappropriate for identifying
a disease/symptom, this study focuses on identify-
ing the possessive relation between a subject and
a disease/symptom. The concept underlying this
decision is to divide the task of public surveillance
into several sub-tasks that are sufficiently general-
ized for use in other NLP applications. Therefore,
the task of analyzing the modality lies beyond of
scope of this study (Kitagawa et al., ). We apply
the same criterion to the labels NEARBYPERSON,
FARAWAYPERSON, and NONHUMAN.
NEARBYPERSON is assigned when the subject
of the disease/symptom is a person whom the
poster can directly see or hear. In the original cor-
pus (Aramaki et al., 2011), a tweet is labeled as
positive if the person having a disease/symptom is
in the same prefecture as the poster. However, it is
extremely difficult for annotators to judge from a
tweet whether the person mentioned in the tweet
is in the same prefecture as the poster. Never-
theless, we would like to determine from a tweet
whether the poster can directly see or hear a pa-
tient. For these reasons, we introduced the label
NEARBYPERSON in this study.
FARAWAYPERSON applies to all cases in which
the subject is a human, but not classified as FIRST-
PERSON or NEARBYPERSON. This category fre-
quently includes tweeted replies, as in the case of
the example corresponding to FARAWAYPERSON
in Table 1. We assign FARAWAYPERSON to such
sentences because we are unsure whether the sub-
ject of the symptom is a person whom the poster
can physically see or hear.
NONHUMAN applies to cases in which the sub-
ject is not a human but an object or a concept. For
example, a sentence with the phrase “My room is
so chill” is annotated with this label.
NONE indicates that the sentence does not men-
tion a target disease or symptom even though it
includes a keyword for the disease/symptom.
In order to investigate the inter-annotator agree-
ment, we sampled 100 tweets of cold at random,
and examined the Cohen’s r. statistic by two an-
notators. The r. statistic is 0.83, indicating a high
level agreement (Carletta, 1996).
Table 2 reports the distribution of subject la-
bels in the corpus annotated in this study. When
the subject of a disease/symptom is FIRSTPER-
SON, only 3.3% of the tweets have explicit tex-
tual clues for the first person3. In other words,
when the subject of a disease/symptom is FIRST-
PERSON, we rarely find textual clues in tweets. In
contrast, there is a greater likelihood of finding ex-
plicit clues for NEARBYPERSON, FARAWAYPER-
SON, and NONHUMAN subjects.
Table 2 also lists the probability of positive
episodes given a subject label, i.e., the posi-
tive ratio. The likelihood of a positive episode
</bodyText>
<footnote confidence="0.855356">
3This ratio may appear to be extremely low, but it is very
common to omit first person pronouns in Japanese sentences.
</footnote>
<page confidence="0.943383">
1663
</page>
<bodyText confidence="0.9996345">
is extremely high when the subject label of a
disease/symptom is FIRSTPERSON (85.1%) or
NEARBYPERSON (76.7%). In contrast, FAR-
AWAYPERSON, NONHUMAN, and NONE sub-
jects represent negative episodes (less than 5.0%).
These facts suggest that identifying subject labels
can improve the accuracy of predicting patient la-
bels for diseases and symptoms.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="background">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.99998">
3.1 Subject classifier
</subsectionHeader>
<bodyText confidence="0.998549147058823">
We built a classifier to predict a subject label for
a disease/symptom mentioned in a sentence by us-
ing the corpus described in the previous section.
In our experiment, we merged training instances
having the label NONHUMAN with those having
the label NONE because the number of NONHU-
MAN instances was small and we did not need to
distinguish the label NONHUMAN from the label
NONE in the final episode detection task. Thus,
the classifier was trained to choose a subject la-
bel from among FIRSTPERSON, NEARBYPER-
SON, FARAWAYPERSON, and NONE. We dis-
carded instances in which multiple diseases or
symptoms are mentioned in a tweet as well as
those in which multiple subjects are associated
with a disease/symptom in a tweet. In addition,
we removed text spans corresponding to retweets,
replies, and URLs; the existence of these spans
was retained for firing features. We trained an L2-
regularized logistic regression model using Clas-
sias 1.14. The following features were used.
Bag-of-Words (BoW). Nine words included be-
fore and after a disease/symptom keyword. We
split a Japanese sentence into a sequence of words
using a Japanese morphological analyzer, MeCab
(ver.0.98) with IPADic (ver.2.7.0)5.
Disease/symptom word (Keyword). The sur-
face form of the disease/symptom keyword (e.g.
“cold” and “headache”).
2,3-gram. Character-based bigrams and tri-
grams before and after the disease/symptom key-
word within a window of six letters.
URL. A boolean feature indicating whether the
tweet includes a URL.
</bodyText>
<footnote confidence="0.999548333333333">
4http://www.chokkan.org/software/
classias/
5http://taku910.github.io/mecab/
</footnote>
<table confidence="0.9999805">
Feature Micro F1 Macro F1
BoW (baseline) 77.2 42.2
BoW + Keyword 81.9 53.6
BoW + 2,3-gram 79.1 46.1
BoW + URL 77.3 42.7
BoW + RT &amp; reply 80.0 47.1
BoW + NearWord 77.6 46.8
BoW + FarWord 77.3 42.7
BoW + Title word 77.1 42.7
BoW + Tweet length 77.4 43.3
BoW + Is-head 77.6 43.5
All features 84.0 61.8
</table>
<tableCaption confidence="0.99987">
Table 3: Performance of the subject classifier.
</tableCaption>
<bodyText confidence="0.9981068">
RT &amp; reply. Boolean features indicating
whether the tweet is a reply or a retweet.
Word list for NEARBYPERSON (NearWord).
A boolean feature indicating whether the tweet
contains a word that is included in the lexicon for
NEARBYPERSON. We manually collected words
that may refer to a person who is near the poster,
e.g., “girlfriend,” “sister,” and “staff.” The Near-
Word list includes 97 words.
Word list for FARAWAYPERSON (FarWord).
A boolean feature indicating whether the tweet
contains a word that is included in the lexicon for
FARAWAYPERSON. Similarly to the NearWord
list, we manually collected 50 words (e.g., “in-
fant”) for compiling this list.
Title word. A boolean feature indicating
whether the tweet contains a title word accom-
panied by a proper noun. The list of title words
includes expressions such as “さ/v” and “C/v”
(roughly corresponding to “Ms” and “Mr”) that
describe the title of a person.
Tweet length. Three types of boolean features
that fire when the tweet has less than 11 words, 11
to 30 words, and more than 30 words, respectively.
Is-head. A boolean feature indicating whether
the word following a disease/symptom keyword
is a noun. In Japanese, when the word follow-
ing a disease/symptom keyword is a noun, the dis-
ease/symptom keyword is unlikely to be the head
of the noun phrase.
</bodyText>
<page confidence="0.925544">
1664
</page>
<table confidence="0.999775833333333">
Correct/predicted label FIRSTPERSON NEARBY. FARAWAY. NONE Total
FIRSTPERSON 2,084 (−15) 6 25 (+21) 38 (−7) 2,153
NEARBYPERSON 80 (−20) 41 (+29) 4 (−5) 4 (−4) 129
FARAWAYPERSON 88 (−49) 8 89 (+46) 16 (+1) 201
NONE 174 (−158) 2 (+1) 10 (+4) 255 (+153) 441
Total predictions 2,426 (−237) 57 (+33) 128 (+66) 313 (+137) 2,924
</table>
<tableCaption confidence="0.999775">
Table 4: Confusion matrix between predicted and correct subject labels.
</tableCaption>
<subsectionHeader confidence="0.998743">
3.2 Evaluation of the subject classifier
</subsectionHeader>
<bodyText confidence="0.999980026666667">
Table 3 reports the performance of the subject
classifier measured via five-fold cross validation.
We used 3,000 tweets corresponding to six types
of diseases and symptoms for this experiment. The
Bag-of-Words (BoW) feature achieved micro and
macro F1 scores of 77.2 and 42.2, respectively.
When all the features were used, the performance
was boosted, i.e., micro and macro F1 scores of
84.0 and 61.8 were achieved. Features such as dis-
ease/symptom keywords, retweet &amp; reply, and the
lexicon for NEARBYPERSON were particularly ef-
fective in improving the performance.
The surface form of the disease/symptom key-
word was found to be the most effective feature
in this task, the reasons for which are discussed in
Section 3.3.
A retweet or reply tweet provides evidence
that the poster has interacted with another person.
Such meta-linguistic features may facilitate se-
mantic and discourse analysis in web texts. How-
ever, this feature is mainly limited to tweets.
The lexicon for NEARBYPERSON provided an
improvement of 4.6 points in terms of the macro
F1 score. This is because (i) around 90% of
the subjects for NEARBYPERSON were explicitly
stated in the tweets and (ii) the vocabulary of peo-
ple near the poster was limited.
Table 4 shows the confusion matrix between the
correct labels and the predicted labels. The diag-
onal elements (in bold face) represent the number
of correct predictions. The figures in parentheses
denote the number of instances for which the base-
line feature set made incorrect predictions, but the
full feature set made correct predictions. For ex-
ample, the classifier predicted NEARBYPERSON
subjects 48 times; 34 out of 48 predictions were
correct. The full feature set increased the number
of correct predictions by 22.
From the diagonal elements (in bold face), we
can confirm that the number of correct predictions
increased significantly from the baseline case, ex-
cept for FIRSTPERSON. One of the reasons for
the improved accuracy of NONE prediction is the
imbalanced label ratio of each disease/symptom.
NONE accounts for 14% of the entire corpus, but
only 5% of the runny nose corpus. On the other
hand, NONE accounts for more than 30% of the
chill corpus. The disease/symptom keyword fea-
ture adjusts the ratio of the subject labels for
each disease/symptom, and the accuracy of sub-
ject identification is improved.
As compared to the baseline case, the number of
FIRSTPERSON cases that were predicted as FAR-
AWAYPERSON increased. Such errors may be at-
tributed to the reply feature. According to our
annotation scheme, FARAWAYPERSON contains
many reply tweets. Because the reply &amp; retweet
features make the second-largest contribution in
our experiment, the subject classifier tends to out-
put FARAWAYPERSON if the tweet is a reply.
Table 5 summarizes the subject classification re-
sults comparing the case in which the subject of
a disease/symptom exists in the tweet with that
in which the subject does not exist. The pre-
diction of FIRSTPERSON is not affected by the
presence of the subject because FIRSTPERSON
subjects are often omitted (especially in Japanese
tweets). The prediction of NEARBYPERSON and
FARAWAYPERSON is difficult if the subject is not
stated explicitly. In contrast, it is easy to correctly
predict NONE even though the subject is not ex-
pressed explicitly. This is because it is not easy to
capture a variety of human-related subjects using
Bag-of-Words, N-gram, or other simple features
used in this experiment.
</bodyText>
<subsectionHeader confidence="0.999169">
3.3 Dependency on diseases/symptoms
</subsectionHeader>
<bodyText confidence="0.999947833333333">
The experiments described in Section 3.2 use
training instances for all types of diseases and
symptoms. However, each disease/symptom may
have a set of special expressions for describing
the state of an episode. For example, even though
“catch a cold” is a common expression, we cannot
</bodyText>
<page confidence="0.975704">
1665
</page>
<table confidence="0.99953825">
Subject FIRSTPERSON NEARBYPERSON FARAWAYPERSON NONE
# Explicit 66/69 (95.7 %) 40/112 (35.7%) 79/174 (45.4 %) 1/26 (3.8%)
# Omitted 2,018/2,084 (96.8 %) 1/17 (5.9%) 10/27 (37.0 %) 254/415 (61.2%)
#Total 2,084/2,153 (96.8 %) 41/129 (31.8%) 89/201 (44.3 %) 255/441 (57.8%)
</table>
<tableCaption confidence="0.998088">
Table 5: Subject classification results comparing explicit subjects with omitted subjects.
</tableCaption>
<figureCaption confidence="0.8503865">
Figure 2: F1 scores for predicting subjects of cold
with different types and sizes of training data.
</figureCaption>
<bodyText confidence="0.998175347826087">
say “catch a fever” by combining the verb “catch”
and the disease “fever.” The corpus developed in
Section 2.2 can be considered as the supervision
data for weighting linguistic patterns that connect
diseases/symptoms with their subjects. This view-
point raises another question: how strongly does
the subject classifier depend on specific diseases
and symptoms?
In order to answer this question, we compare the
performance of recognizing subjects of cold when
using the training instances for all types of dis-
eases and symptoms with that when using only the
training instances for the target disease/symptom.
Figure 2 shows the macro F1 scores with all train-
ing instances (dotted line) and with only cold
training instances (solid line)6.
In this case, training with cold instances is nat-
urally more efficient than training with other types
of diseases/symptoms. When trained with 400 in-
stances only for cold, the classifier achieved an
F1 score of 45.2. Moreover, we confirmed that
adding training instances for other types of dis-
eases/symptoms improved the F1 score: the max-
</bodyText>
<footnote confidence="0.997346833333333">
6For the solid line, we used 500 instances of “cold” as a
test set, and we plotted the learning curve by increasing the
number of training instances for other diseases/symptoms.
For the dotted line, we fixed 100 instances for a test set, and
we plotted the learning curve by increasing the number of
training instances (100, 200, 300, and 400).
</footnote>
<figureCaption confidence="0.999638">
Figure 3: Overall structure of the system.
</figureCaption>
<bodyText confidence="0.999954333333333">
imum F1 score was 54.6 with 2,900 instances.
These results indicate the possibility of building
a subject classifier that is independent of specific
diseases/symptoms but applicable to a variety of
diseases/symptoms. We observed a similar ten-
dency for other types of diseases/symptoms.
</bodyText>
<subsectionHeader confidence="0.991192">
3.4 Contributions to the episode classifier
</subsectionHeader>
<bodyText confidence="0.998816052631579">
The ultimate objective of this study is to detect
outbreaks of epidemics by recognizing diseases
and symptoms. In order to demonstrate the contri-
butions of this study, we built an episode classifier
that judges whether the poster or a person close to
the poster suffers from a target disease/symptom.
Figure 3 shows the overall structure of the system.
Given a tweet, the system predicts the subject la-
bel for a disease/symptom, and integrates the pre-
dicted subject label as a feature for the episode
classifier. In addition to the features used in Ara-
maki et al. (2011), we included binary features,
each of which corresponds to a subject label pre-
dicted by the proposed method. We trained an L2-
regularized logistic regression model using Clas-
sias 1.1.
Table 6 summarizes the performance of the
episode classifier with different settings: without
subject labels (baseline), with predicted subject la-
</bodyText>
<page confidence="0.976186">
1666
</page>
<table confidence="0.99803475">
Setting Cold Cough Headache Chill Runny nose Fever Macro F1
Baseline (BL) 84.4 88.5 90.8 75.9 89.2 78.1 84.5
BL + predicted subjects 85.0 88.3 90.7 81.4 89.4 80.2 85.8
BL + gold-standard subjects 87.7 92.6 93.5 88.5 91.4 88.6 90.4
</table>
<tableCaption confidence="0.999291">
Table 6: Performance of the episode classifier.
</tableCaption>
<bodyText confidence="0.999985214285714">
bels , and with gold-standard subject labels. We
measured the F1 scores via five-fold cross vali-
dation7. Further, we confirmed the contribution
of subject label prediction, which achieved an im-
provement of 1.3 points over the baseline method
(85.8 vs. 84.5). When using the gold-standard
subject labels, the episode classifier achieved an
improvement of 5.9 points. These results highlight
the importance of recognizing a subject who has a
disease/symptom using the episode classifier.
Considering the F1 score for each dis-
ease/symptom, we observed the largest improve-
ment for chill. This is because the Japanese word
for “chill” has another meaning a cold air mass.
When the word “chill” stands for a cold air mass
in a tweet, the subject for “chill” is NONE. There-
fore, the episode classifier can disambiguate the
meaning of “chill’ on the basis of the subject la-
bels. Similarly, the subject labels improved the
performance for “fever”.
In contrast, the subject labels did not improve
the performance for headache and runny nose con-
siderably. This is because the subjects for these
symptoms are mostly FIRSTPERSON, as we sel-
dom mention the symptoms of another person in
such cases. In other words, the episode classi-
fier can predict a positive label for these symptoms
without knowing the subjects of these symptoms.
</bodyText>
<sectionHeader confidence="0.999882" genericHeader="related work">
4 Related Work
</sectionHeader>
<subsectionHeader confidence="0.993431">
4.1 Twitter and NLP
</subsectionHeader>
<bodyText confidence="0.999703333333333">
NLP researchers have addressed two major direc-
tions for Twitter: adapting existing NLP technolo-
gies to noisy texts and extracting useful knowl-
edge from Twitter. The former includes improving
the accuracy of part-of-speech tagging (Gimpel et
al., 2011) and named entity recognition (Plank et
al., 2014), as well as normalizing ill-formed words
into canonical forms (Han and Baldwin, 2011;
Chrupała, 2014). Even though we did not incor-
</bodyText>
<footnote confidence="0.985908333333333">
7For the “predicted” setting, first, we predicted the subject
labels in a similar manner to five-fold cross validation, and we
used the predicted labels as features for the episode classifier.
</footnote>
<bodyText confidence="0.999075055555556">
porate the findings of these studies, they could be
beneficial to our work in the future.
The latter has led to the development of sev-
eral interesting applications besides health surveil-
lance. These include prediction of future rev-
enue (Asur and Huberman, 2010) and stock mar-
ket trends (Si et al., 2013), mining of public opin-
ion (O’Connor et al., 2010), event extraction and
summarization (Sakaki et al., 2010; Thelwall et
al., 2011; Marchetti-Bowick and Chambers, 2012;
Shen et al., 2013; Li et al., 2014a), user profil-
ing (Bergsma et al., 2013; Han et al., 2013; Li
et al., 2014b; Zhou et al., 2014), disaster man-
agement (Varga et al., 2013), and extraction of
common-sense knowledge (Williams and Katz,
2012). Our work can directly contribute to these
applications, e.g., sentiment analysis, user profil-
ing, event extraction, and disaster management.
</bodyText>
<subsectionHeader confidence="0.999864">
4.2 Semantic analysis for nouns
</subsectionHeader>
<bodyText confidence="0.999975375">
Our work can be considered as a semantic anal-
ysis that identifies an argument (subject) for a
disease/symptom-related noun. NomBank (Mey-
ers et al., 2004) provides annotations of noun ar-
guments in a similar manner to PropBank (Palmer
et al., 2005), which provides annotations of verbs.
In NomBank, nominal predicates and their argu-
ments are identified: for example, ARG0 (typi-
cally, subject or agent) is “customer” and ARG1
(typically, objects, patients, themes) is “issue” for
the nominal predicate “complaints” in the sen-
tence “There have been no customer complaints
about that issue.” Gerber and Chai (2010) im-
proved the coverage of NomBank by handling im-
plicit arguments. Some studies have addressed the
task of identifying implicit and omitted arguments
for nominal predicates in Japanese (Komachi et
al., 2007; Sasano et al., 2008).
Our work shares a similar goal with the above-
mentioned studies, i.e., identifying an implicit
ARG0 for a disease and symptom. However, these
studies do not regard a disease/symptom as a nom-
inal predicate because they consider verb nom-
inalizations as nominal predicates. In addition,
</bodyText>
<page confidence="0.986079">
1667
</page>
<bodyText confidence="0.999878625">
they use a corpus that consists of newswire text,
the writing style and word usage of which differ
considerably from those of tweets. For these rea-
sons, we proposed a novel task setting for identi-
fying subjects of diseases and symptoms, and we
built an annotated corpus for developing the sub-
ject classifier and analyzing the challenges of this
task.
</bodyText>
<sectionHeader confidence="0.993514" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999918055555556">
In this paper, we presented a novel approach to
the identification of subjects of various types of
diseases and symptoms. First, we constructed
an annotated corpus based on an existing cor-
pus for public surveillance. Then, we trained
a classifier for predicting the subject of a dis-
ease/symptom. The results of our experiments
showed that the task of identifying the subjects
is independent of the type of disease/symptom.
In addition, the results demonstrated the contribu-
tions of our work toward identifying an episode of
a disease/symptom from a tweet.
In the future, we plan to consider a greater vari-
ety of diseases and symptoms in order to develop
applications for public health, e.g., monitoring the
mental condition of individuals. Thus, we can not
only improve the accuracy of subject identification
but also enhance the generality of this task.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999065">
This study was partly supported by Japan Sci-
ence and Technology Agency (JST). We are grate-
ful to the anonymous referees for their construc-
tive reviews. We are also grateful to Takayuki
Sato and Yasunobu Asakura for their annotation
efforts. This study was inspired by Project Next
NLP8, a workshop for error analysis on various
NLP tasks. We appreciate Takenobu Tokunaga,
Satoshi Sekine, and Kentaro Inui for their helpful
comments.
</bodyText>
<sectionHeader confidence="0.998693" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9609855">
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza
epidemics using twitter. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1568–1576.
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proceedings
8https://sites.google.com/site/
projectnextnlp/english-page
of the 2010 IEEE/WIC/ACM International Confer-
ence on Web Intelligence and IntelligentAgent Tech-
nology - Volume 01, WI-IAT ’10, pages 492–499,
Washington, DC, USA. IEEE Computer Society.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on Twitter. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1010–1019.
David Broniatowski, Michael J. Paul, and Mark
Dredze. 2013. National and local influenza surveil-
lance through Twitter: An analysis of the 2012-2013
influenza epidemic. PLoS ONE, 8(12):e83672.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
linguistics, 22(2):249–254.
Herman Anthony Carneiro and Eleftherios Mylonakis.
2009. Google trends: a web-based tool for real-time
surveillance of disease outbreaks. Clinical Infec-
tious Diseases, 49(10):1557–1564.
Grzegorz Chrupała. 2014. Normalizing tweets with
edit scripts and recurrent neural embeddings. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 680–686.
Nigel Collier. 2012. Uncovering text mining: a survey
of current work on web-based epidemic intelligence.
Global Public Health: An International Journal for
Research, Policy and Practice, 7(7):731–749.
Aron Culotta. 2010. Towards detecting influenza
epidemics by analyzing Twitter messages. In Pro-
ceedings of the Workshop on Social Media Analytics
(SOMA), pages 115–122.
Mark Dredze, Michael J. Paul, Shane Bergsma, and
Hieu Tran. 2013. Carmen: A Twitter geolocation
system with applications to public health. In Pro-
ceedings of the AAAI Workshop on Expanding the
Boundaries of Health Informatics Using AI (HIAI),
pages 20–24.
Matthew Gerber and Joyce Y. Chai. 2010. Beyond
NomBank: A study of implicit arguments for nom-
inal predicates. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1583–1592.
Francesco Gesualdo, Giovanni Stilo, Eleonora Agri-
cola, Michaela V. Gonfiantini, Elisabetta Pan-
dolfi, Paola Velardi, and Alberto E. Tozzi.
2013. Influenza-like illness surveillance on Twit-
ter through automated learning of naïve language.
PLoS One, 8(12):e82489.
</reference>
<page confidence="0.929313">
1668
</page>
<reference confidence="0.99857592920354">
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 42–47.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 368–378.
Bo Han, Paul Cook, and Timothy Baldwin. 2013. A
stacking-based approach to Twitter user geolocation
prediction. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics: System Demonstrations, pages 7–12.
Yoshiaki Kitagawa, Mamoru Komachi, Eiji Aramaki,
Naoaki Okazaki, and Hiroshi Ishikawa. Disease
event detection based on deep modality analysis.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP) 2015 Student
Research Workshop.
Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji
Matsumoto. 2007. Learning based argument struc-
ture analysis of event-nouns in Japanese. In Pro-
ceedings of the Conference of the Pacific Association
for Computational Linguistics (PACLING), pages
120–128.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
Twitter. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 789–795.
Vasileios Lampos and Nello Cristianini. 2010. Track-
ing the flu pandemic by monitoring the social web.
In 2nd IAPR Workshop on Cognitive Information
Processing (CIP 2010), pages 411–416.
David Lazer, Ryan Kennedy, Gary King, and Alessan-
dro Vespignani. 2014. The parable of Google
flu: Traps in big data analysis. Science,
343(6176):1203–1205.
Jiwei Li, Alan Ritter, Claire Cardie, and Eduard Hovy.
2014a. Major life event extraction from Twitter
based on congratulations/condolences speech acts.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1997–2007.
Jiwei Li, Alan Ritter, and Eduard Hovy. 2014b.
Weakly supervised user profile extraction from Twit-
ter. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 165–174.
Micol Marchetti-Bowick and Nathanael Chambers.
2012. Learning for microblogs with distant supervi-
sion: Political forecasting with Twitter. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 603–612.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank
Project: An interim report. In Proceedings of the
NAACL/HLT Workshop on Frontiers in Corpus An-
notation, pages 24–31.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, , and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
Fourth International AAAI Conference on Weblogs
and Social Media (ICWSM), pages 122–129.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Michael J. Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing Twitter for public health. In
Proceedings of the Fifth International AAAI Confer-
ence on Weblogs and Social Media (ICWSM), pages
265–272.
Barbara Plank, Dirk Hovy, Ryan McDonald, and An-
ders Søgaard. 2014. Adapting taggers to Twit-
ter with not-so-distant supervision. In Proceedings
of COLING 2014, the 25th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 1783–1792.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th international conference on World Wide
Web (WWW), pages 851–860.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proceedings of the 20th international
conference on Computational Linguistics, pages
1201–1207.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for Japanese zero anaphora resolution. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics-Volume 1, pages 769–776.
Chao Shen, Fei Liu, Fuliang Weng, and Tao Li. 2013.
A participant-based approach for event summariza-
tion using Twitter streams. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1152–1162.
</reference>
<page confidence="0.881994">
1669
</page>
<reference confidence="0.999718738095238">
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,
Huayi Li, and Xiaotie Deng. 2013. Exploiting topic
based Twitter sentiment for stock prediction. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 24–29.
Alessio Signorini, Alberto Maria Segre, and Philip M.
Polgreen. 2011. The use of Twitter to track levels of
disease activity and public concern in the U.S. dur-
ing the influenza A H1N1 pandemic. PLoS ONE,
6(5):e19467.
Mark A. Stoové and Alisa E. Pedrana. 2014. Making
the most of a brave new world: Opportunities and
considerations for using Twitter as a public health
monitoring tool. Preventive Medicine, 63:109–111.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2011. Sentiment in Twitter events. Journal
of the American Society for Information Science and
Technology, 62(2):406–418.
István Varga, Motoki Sano, Kentaro Torisawa, Chikara
Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-
Hoon Oh, and Stijn De Saeger. 2013. Aid is
out there: Looking for help from tweets during a
large scale disaster. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1619–
1629.
Paola Velardi, Giovanni Stilo, Alberto E. Tozzi, and
Francesco Gesualdo. 2014. Twitter mining for fine-
grained syndromic surveillance. Artificial Intelli-
gence in Medicine, 61(3):153–163.
Jennifer Williams and Graham Katz. 2012. Extracting
and modeling durations for habits and events from
Twitter. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 223–227.
Deyu Zhou, Liangyu Chen, and Yulan He. 2014. A
simple bayesian modelling approach to event extrac-
tion from Twitter. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 700–
705.
</reference>
<page confidence="0.989299">
1670
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.245223">
<title confidence="0.987662">Who caught a cold? — Identifying the subject of a symptom</title>
<author confidence="0.966059">Mamoru Naoaki</author>
<affiliation confidence="0.65508575">and Hiroshi Metropolitan University, {kanouchi-shin at ed., komachi at, ishikawh University, okazaki at University, eiji.aramaki at gmail.com</affiliation>
<abstract confidence="0.999454888888889">The development and proliferation of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is health surveillance, e.g., predicting the outbreak of an epidemic by recognizing diseases and symptoms from text messages posted on social media platforms. In this paper, we propose a novel task that is crucial and generic from the viewpoint of health surveillance: estimating a subject (carrier) of a disease or symptom mentioned in a Japanese tweet. By designing an annotation guideline for labeling the subject of a disease/symptom in a tweet, we perform annotations on an existing corpus for public surveillance. In addition, we present a supervised approach for predicting the subject of a disease/symptom. The results of our experiments demonstrate the impact of subject identification on the effective detection of an episode of a disease/symptom. Moreover, the results suggest that our task is independent of the type of disease/symptom.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eiji Aramaki</author>
<author>Sachiko Maskawa</author>
<author>Mizuki Morita</author>
</authors>
<title>Twitter catches the flu: Detecting influenza epidemics using twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1568--1576</pages>
<contexts>
<context position="1943" citStr="Aramaki et al., 2011" startWordPosition="293" endWordPosition="296">suggest that our task is independent of the type of disease/symptom. 1 Introduction Social media services, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples incl</context>
<context position="7981" citStr="Aramaki et al., 2011" startWordPosition="1291" endWordPosition="1294">nnotation work for identifying subjects of diseases and symptoms. Section 3.1 presents our method for predicting subjects on the basis of the annotated corpus. Sections 3.2 and 3.3 report the performance of the proposed method. Section 3.4 describes the contributions of this study toward identifying episodes of diseases and symptoms. Section 4 reviews some related studies. Finally, Section 5 summarizes our findings and concludes the paper with a brief discussion on the scope for future work. 2 Corpus 2.1 Target corpus We used a Japanese corpus for public surveillance of diseases and symptoms (Aramaki et al., 2011). The corpus targets seven types of diseases and symptoms: cold, cough, headache, chill, runny nose, fever, and sore throat. Tweets containing keywords for each disease/symptom were collected using the Twitter Search API: for example, tweets about sore throat were collected using the query “(sore OR pain) AND throat”. Further, 1661 Figure 1: Examples of annotations of subject labels. Subject label Definition Example FIRSTPERSON The subject of the disease/symptom is the poster I wish I have fever or someof the tweet. thing so that I don’t have to go to school. NEARBYPERSON The subject of the di</context>
<context position="10722" citStr="Aramaki et al. (2011)" startWordPosition="1746" endWordPosition="1750"> in every prefecture. 2In Japanese tweets, sore throat or throat pain mostly describes the health condition of the poster. 2.2 Annotating subjects In this study, we annotated the subjects of diseases and symptoms in the corpus described in Section 2.1. Specifically, we annotated the subjects in 500 tweets for each disease/symptom (except for sore throat). Thus, our corpus includes a total of 3,000 tweets in which the subjects of diseases and symptoms are annotated. Figure 1 shows examples of annotations in this study. Episode labels, tweet texts, and disease/symptom keywords were annotated by Aramaki et al. (2011) in the corpus. We annotated the subject labels of the diseases/symptoms in each tweet and identified those who had the target disease/symptom. The subject labels indicate those who have the corresponding disease/symptom; they are described in detail 1662 Label FIRSTPERSON NEARBYPERSON FARAWAYPERSON NONHUMAN NONE Total #tweets 2,153 129 201 40 401 2,924 # explicit subjects 70 (3.3%) 112 (86.8%) 175 (87.1%) 38 (95.0%) 0 (0.0%) 395 # positive episodes 1,833 99 2 0 16 1,950 # negative episodes 320 30 199 40 385 974 Positive ratio 85.1% 76.7% 1.0% 0.0% 4.0% 66.7% Table 2: Associations between subj</context>
<context position="13255" citStr="Aramaki et al., 2011" startWordPosition="2158" endWordPosition="2161">s study focuses on identifying the possessive relation between a subject and a disease/symptom. The concept underlying this decision is to divide the task of public surveillance into several sub-tasks that are sufficiently generalized for use in other NLP applications. Therefore, the task of analyzing the modality lies beyond of scope of this study (Kitagawa et al., ). We apply the same criterion to the labels NEARBYPERSON, FARAWAYPERSON, and NONHUMAN. NEARBYPERSON is assigned when the subject of the disease/symptom is a person whom the poster can directly see or hear. In the original corpus (Aramaki et al., 2011), a tweet is labeled as positive if the person having a disease/symptom is in the same prefecture as the poster. However, it is extremely difficult for annotators to judge from a tweet whether the person mentioned in the tweet is in the same prefecture as the poster. Nevertheless, we would like to determine from a tweet whether the poster can directly see or hear a patient. For these reasons, we introduced the label NEARBYPERSON in this study. FARAWAYPERSON applies to all cases in which the subject is a human, but not classified as FIRSTPERSON or NEARBYPERSON. This category frequently includes</context>
<context position="26043" citStr="Aramaki et al. (2011)" startWordPosition="4242" endWordPosition="4246">ptoms. 3.4 Contributions to the episode classifier The ultimate objective of this study is to detect outbreaks of epidemics by recognizing diseases and symptoms. In order to demonstrate the contributions of this study, we built an episode classifier that judges whether the poster or a person close to the poster suffers from a target disease/symptom. Figure 3 shows the overall structure of the system. Given a tweet, the system predicts the subject label for a disease/symptom, and integrates the predicted subject label as a feature for the episode classifier. In addition to the features used in Aramaki et al. (2011), we included binary features, each of which corresponds to a subject label predicted by the proposed method. We trained an L2- regularized logistic regression model using Classias 1.1. Table 6 summarizes the performance of the episode classifier with different settings: without subject labels (baseline), with predicted subject la1666 Setting Cold Cough Headache Chill Runny nose Fever Macro F1 Baseline (BL) 84.4 88.5 90.8 75.9 89.2 78.1 84.5 BL + predicted subjects 85.0 88.3 90.7 81.4 89.4 80.2 85.8 BL + gold-standard subjects 87.7 92.6 93.5 88.5 91.4 88.6 90.4 Table 6: Performance of the epis</context>
</contexts>
<marker>Aramaki, Maskawa, Morita, 2011</marker>
<rawString>Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita. 2011. Twitter catches the flu: Detecting influenza epidemics using twitter. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1568–1576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sitaram Asur</author>
<author>Bernardo A Huberman</author>
</authors>
<title>Predicting the future with social media.</title>
<date>2010</date>
<booktitle>In Proceedings 8https://sites.google.com/site/</booktitle>
<contexts>
<context position="28900" citStr="Asur and Huberman, 2010" startWordPosition="4703" endWordPosition="4706">ntity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Se</context>
</contexts>
<marker>Asur, Huberman, 2010</marker>
<rawString>Sitaram Asur and Bernardo A. Huberman. 2010. Predicting the future with social media. In Proceedings 8https://sites.google.com/site/</rawString>
</citation>
<citation valid="false">
<booktitle>projectnextnlp/english-page of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and IntelligentAgent Technology - Volume 01, WI-IAT ’10,</booktitle>
<pages>492--499</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<marker></marker>
<rawString>projectnextnlp/english-page of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and IntelligentAgent Technology - Volume 01, WI-IAT ’10, pages 492–499, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Mark Dredze</author>
<author>Benjamin Van Durme</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Broadly improving user classification via communication-based name and location clustering on Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of</booktitle>
<pages>1010--1019</pages>
<marker>Bergsma, Dredze, Van Durme, Wilson, Yarowsky, 2013</marker>
<rawString>Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, and David Yarowsky. 2013. Broadly improving user classification via communication-based name and location clustering on Twitter. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1010–1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Broniatowski</author>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>National and local influenza surveillance through Twitter: An analysis of the 2012-2013 influenza epidemic.</title>
<date>2013</date>
<journal>PLoS ONE,</journal>
<volume>8</volume>
<issue>12</issue>
<contexts>
<context position="2390" citStr="Broniatowski et al. (2013)" startWordPosition="366" endWordPosition="370">predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples include, “I don’t wish the flu on anyone” and “A Harry Potter actor hospitalised after severe flu-like syndromes.” Lazer et al. (2014) reported large errors in Google Flu Trends (Carneiro and Mylonakis, 2009) on the basis of a comparison with the proportion of doctor visits for influenza-like illnesses. Lamb et al. (2013) aimed to improve the accuracy of detecting mentions of flu infections. Their method trains a binary classifier to distinguish t</context>
</contexts>
<marker>Broniatowski, Paul, Dredze, 2013</marker>
<rawString>David Broniatowski, Michael J. Paul, and Mark Dredze. 2013. National and local influenza surveillance through Twitter: An analysis of the 2012-2013 influenza epidemic. PLoS ONE, 8(12):e83672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="14649" citStr="Carletta, 1996" startWordPosition="2404" endWordPosition="2405">he symptom is a person whom the poster can physically see or hear. NONHUMAN applies to cases in which the subject is not a human but an object or a concept. For example, a sentence with the phrase “My room is so chill” is annotated with this label. NONE indicates that the sentence does not mention a target disease or symptom even though it includes a keyword for the disease/symptom. In order to investigate the inter-annotator agreement, we sampled 100 tweets of cold at random, and examined the Cohen’s r. statistic by two annotators. The r. statistic is 0.83, indicating a high level agreement (Carletta, 1996). Table 2 reports the distribution of subject labels in the corpus annotated in this study. When the subject of a disease/symptom is FIRSTPERSON, only 3.3% of the tweets have explicit textual clues for the first person3. In other words, when the subject of a disease/symptom is FIRSTPERSON, we rarely find textual clues in tweets. In contrast, there is a greater likelihood of finding explicit clues for NEARBYPERSON, FARAWAYPERSON, and NONHUMAN subjects. Table 2 also lists the probability of positive episodes given a subject label, i.e., the positive ratio. The likelihood of a positive episode 3T</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herman Anthony</author>
</authors>
<title>Carneiro and Eleftherios Mylonakis.</title>
<date>2009</date>
<journal>Clinical Infectious Diseases,</journal>
<volume>49</volume>
<issue>10</issue>
<marker>Anthony, 2009</marker>
<rawString>Herman Anthony Carneiro and Eleftherios Mylonakis. 2009. Google trends: a web-based tool for real-time surveillance of disease outbreaks. Clinical Infectious Diseases, 49(10):1557–1564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
</authors>
<title>Normalizing tweets with edit scripts and recurrent neural embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>680--686</pages>
<contexts>
<context position="28416" citStr="Chrupała, 2014" startWordPosition="4626" endWordPosition="4627">symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and s</context>
</contexts>
<marker>Chrupała, 2014</marker>
<rawString>Grzegorz Chrupała. 2014. Normalizing tweets with edit scripts and recurrent neural embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 680–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Collier</author>
</authors>
<title>Uncovering text mining: a survey of current work on web-based epidemic intelligence. Global Public Health:</title>
<date>2012</date>
<booktitle>An International Journal for Research, Policy and Practice,</booktitle>
<volume>7</volume>
<issue>7</issue>
<contexts>
<context position="2005" citStr="Collier, 2012" startWordPosition="305" endWordPosition="306">1 Introduction Social media services, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples include, “I don’t wish the flu on anyone” and “A Harry Potter acto</context>
</contexts>
<marker>Collier, 2012</marker>
<rawString>Nigel Collier. 2012. Uncovering text mining: a survey of current work on web-based epidemic intelligence. Global Public Health: An International Journal for Research, Policy and Practice, 7(7):731–749.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
</authors>
<title>Towards detecting influenza epidemics by analyzing Twitter messages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Social Media Analytics (SOMA),</booktitle>
<pages>115--122</pages>
<contexts>
<context position="1891" citStr="Culotta, 2010" startWordPosition="287" endWordPosition="288"> of a disease/symptom. Moreover, the results suggest that our task is independent of the type of disease/symptom. 1 Introduction Social media services, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu witho</context>
</contexts>
<marker>Culotta, 2010</marker>
<rawString>Aron Culotta. 2010. Towards detecting influenza epidemics by analyzing Twitter messages. In Proceedings of the Workshop on Social Media Analytics (SOMA), pages 115–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Michael J Paul</author>
<author>Shane Bergsma</author>
<author>Hieu Tran</author>
</authors>
<title>Carmen: A Twitter geolocation system with applications to public health.</title>
<date>2013</date>
<booktitle>In Proceedings of the AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI (HIAI),</booktitle>
<pages>20--24</pages>
<contexts>
<context position="2026" citStr="Dredze et al., 2013" startWordPosition="307" endWordPosition="310">Social media services, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples include, “I don’t wish the flu on anyone” and “A Harry Potter actor hospitalised after </context>
</contexts>
<marker>Dredze, Paul, Bergsma, Tran, 2013</marker>
<rawString>Mark Dredze, Michael J. Paul, Shane Bergsma, and Hieu Tran. 2013. Carmen: A Twitter geolocation system with applications to public health. In Proceedings of the AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI (HIAI), pages 20–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Y Chai</author>
</authors>
<title>Beyond NomBank: A study of implicit arguments for nominal predicates.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1583--1592</pages>
<contexts>
<context position="4929" citStr="Gerber and Chai, 2010" startWordPosition="779" endWordPosition="782"> first person (“I,” i.e., the poster) is the subject (carrier) of the cold. On the other hand, we can ignore the sentence, “The TV presenter caught a cold” only if we predict that the subject of the cold is the third person, who is at a different location from the poster. Although the task setting is simple and intuitive, we identify several key challenges in this study. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example, the subject of the verb “beat” is the first person “I” in the sent</context>
<context position="30135" citStr="Gerber and Chai (2010)" startWordPosition="4900" endWordPosition="4903">is for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG0 (typically, subject or agent) is “customer” and ARG1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted arguments for nominal predicates in Japanese (Komachi et al., 2007; Sasano et al., 2008). Our work shares a similar goal with the abovementioned studies, i.e., identifying an implicit ARG0 for a disease and symptom. However, these studies do not regard a disease/symptom as a nominal predicate because they consider verb nominalizations as nominal predicates. In addition, 1667 they use a corpus that consists of newswire text, the writing style and word usage </context>
</contexts>
<marker>Gerber, Chai, 2010</marker>
<rawString>Matthew Gerber and Joyce Y. Chai. 2010. Beyond NomBank: A study of implicit arguments for nominal predicates. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583–1592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Gesualdo</author>
<author>Giovanni Stilo</author>
<author>Eleonora Agricola</author>
<author>Michaela V Gonfiantini</author>
<author>Elisabetta Pandolfi</author>
<author>Paola Velardi</author>
<author>Alberto E Tozzi</author>
</authors>
<title>Influenza-like illness surveillance on Twitter through automated learning of naïve language.</title>
<date>2013</date>
<journal>PLoS One,</journal>
<volume>8</volume>
<issue>12</issue>
<contexts>
<context position="2049" citStr="Gesualdo et al., 2013" startWordPosition="311" endWordPosition="314">, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples include, “I don’t wish the flu on anyone” and “A Harry Potter actor hospitalised after severe flu-like syndrom</context>
</contexts>
<marker>Gesualdo, Stilo, Agricola, Gonfiantini, Pandolfi, Velardi, Tozzi, 2013</marker>
<rawString>Francesco Gesualdo, Giovanni Stilo, Eleonora Agricola, Michaela V. Gonfiantini, Elisabetta Pandolfi, Paola Velardi, and Alberto E. Tozzi. 2013. Influenza-like illness surveillance on Twitter through automated learning of naïve language. PLoS One, 8(12):e82489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>368--378</pages>
<contexts>
<context position="28399" citStr="Han and Baldwin, 2011" startWordPosition="4622" endWordPosition="4625"> we seldom mention the symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 368–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>A stacking-based approach to Twitter user geolocation prediction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="29203" citStr="Han et al., 2013" startWordPosition="4756" endWordPosition="4759">cted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of</context>
</contexts>
<marker>Han, Cook, Baldwin, 2013</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2013. A stacking-based approach to Twitter user geolocation prediction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 7–12.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yoshiaki Kitagawa</author>
<author>Mamoru Komachi</author>
<author>Eiji Aramaki</author>
<author>Naoaki Okazaki</author>
<author>Hiroshi Ishikawa</author>
</authors>
<title>Disease event detection based on deep modality analysis.</title>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP) 2015 Student Research Workshop.</booktitle>
<marker>Kitagawa, Komachi, Aramaki, Okazaki, Ishikawa, </marker>
<rawString>Yoshiaki Kitagawa, Mamoru Komachi, Eiji Aramaki, Naoaki Okazaki, and Hiroshi Ishikawa. Disease event detection based on deep modality analysis. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP) 2015 Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Learning based argument structure analysis of event-nouns in Japanese.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the Pacific Association for Computational Linguistics (PACLING),</booktitle>
<pages>120--128</pages>
<contexts>
<context position="4905" citStr="Komachi et al., 2007" startWordPosition="775" endWordPosition="778">would predict that the first person (“I,” i.e., the poster) is the subject (carrier) of the cold. On the other hand, we can ignore the sentence, “The TV presenter caught a cold” only if we predict that the subject of the cold is the third person, who is at a different location from the poster. Although the task setting is simple and intuitive, we identify several key challenges in this study. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example, the subject of the verb “beat” is the firs</context>
<context position="30340" citStr="Komachi et al., 2007" startWordPosition="4932" endWordPosition="4935">n a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG0 (typically, subject or agent) is “customer” and ARG1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted arguments for nominal predicates in Japanese (Komachi et al., 2007; Sasano et al., 2008). Our work shares a similar goal with the abovementioned studies, i.e., identifying an implicit ARG0 for a disease and symptom. However, these studies do not regard a disease/symptom as a nominal predicate because they consider verb nominalizations as nominal predicates. In addition, 1667 they use a corpus that consists of newswire text, the writing style and word usage of which differ considerably from those of tweets. For these reasons, we proposed a novel task setting for identifying subjects of diseases and symptoms, and we built an annotated corpus for developing the</context>
</contexts>
<marker>Komachi, Iida, Inui, Matsumoto, 2007</marker>
<rawString>Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007. Learning based argument structure analysis of event-nouns in Japanese. In Proceedings of the Conference of the Pacific Association for Computational Linguistics (PACLING), pages 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lamb</author>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>Separating fact from fear: Tracking flu infections on Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>789--795</pages>
<contexts>
<context position="2862" citStr="Lamb et al. (2013)" startWordPosition="445" endWordPosition="448"> keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples include, “I don’t wish the flu on anyone” and “A Harry Potter actor hospitalised after severe flu-like syndromes.” Lazer et al. (2014) reported large errors in Google Flu Trends (Carneiro and Mylonakis, 2009) on the basis of a comparison with the proportion of doctor visits for influenza-like illnesses. Lamb et al. (2013) aimed to improve the accuracy of detecting mentions of flu infections. Their method trains a binary classifier to distinguish tweets reporting flu infections from those expressing concern or awareness about the flu, e.g., “Starting to get worried about swine flu.” Accordingly, they reported encouraging results (e.g., better correlations with CDC trends), but their approach requires supervision data and a lexicon (word class features) specially designed for the flu. Moreover, even though this method is a reasonable choice for improving the accuracy, it is not readily applicable to other types </context>
</contexts>
<marker>Lamb, Paul, Dredze, 2013</marker>
<rawString>Alex Lamb, Michael J. Paul, and Mark Dredze. 2013. Separating fact from fear: Tracking flu infections on Twitter. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 789–795.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Nello Cristianini</author>
</authors>
<title>Tracking the flu pandemic by monitoring the social web.</title>
<date>2010</date>
<booktitle>In 2nd IAPR Workshop on Cognitive Information Processing (CIP 2010),</booktitle>
<pages>411--416</pages>
<contexts>
<context position="1921" citStr="Lampos and Cristianini, 2010" startWordPosition="289" endWordPosition="292">ymptom. Moreover, the results suggest that our task is independent of the type of disease/symptom. 1 Introduction Social media services, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually i</context>
</contexts>
<marker>Lampos, Cristianini, 2010</marker>
<rawString>Vasileios Lampos and Nello Cristianini. 2010. Tracking the flu pandemic by monitoring the social web. In 2nd IAPR Workshop on Cognitive Information Processing (CIP 2010), pages 411–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lazer</author>
<author>Ryan Kennedy</author>
<author>Gary King</author>
<author>Alessandro Vespignani</author>
</authors>
<title>The parable of Google flu: Traps in big data analysis.</title>
<date>2014</date>
<journal>Science,</journal>
<volume>343</volume>
<issue>6176</issue>
<contexts>
<context position="2673" citStr="Lazer et al. (2014)" startWordPosition="414" endWordPosition="417">ové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples include, “I don’t wish the flu on anyone” and “A Harry Potter actor hospitalised after severe flu-like syndromes.” Lazer et al. (2014) reported large errors in Google Flu Trends (Carneiro and Mylonakis, 2009) on the basis of a comparison with the proportion of doctor visits for influenza-like illnesses. Lamb et al. (2013) aimed to improve the accuracy of detecting mentions of flu infections. Their method trains a binary classifier to distinguish tweets reporting flu infections from those expressing concern or awareness about the flu, e.g., “Starting to get worried about swine flu.” Accordingly, they reported encouraging results (e.g., better correlations with CDC trends), but their approach requires supervision data and a le</context>
</contexts>
<marker>Lazer, Kennedy, King, Vespignani, 2014</marker>
<rawString>David Lazer, Ryan Kennedy, Gary King, and Alessandro Vespignani. 2014. The parable of Google flu: Traps in big data analysis. Science, 343(6176):1203–1205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Alan Ritter</author>
<author>Claire Cardie</author>
<author>Eduard Hovy</author>
</authors>
<title>Major life event extraction from Twitter based on congratulations/condolences speech acts.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1997--2007</pages>
<contexts>
<context position="29145" citStr="Li et al., 2014" startWordPosition="4745" endWordPosition="4748">nner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to Pro</context>
</contexts>
<marker>Li, Ritter, Cardie, Hovy, 2014</marker>
<rawString>Jiwei Li, Alan Ritter, Claire Cardie, and Eduard Hovy. 2014a. Major life event extraction from Twitter based on congratulations/condolences speech acts. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1997–2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Alan Ritter</author>
<author>Eduard Hovy</author>
</authors>
<title>Weakly supervised user profile extraction from Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>165--174</pages>
<contexts>
<context position="29145" citStr="Li et al., 2014" startWordPosition="4745" endWordPosition="4748">nner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to Pro</context>
</contexts>
<marker>Li, Ritter, Hovy, 2014</marker>
<rawString>Jiwei Li, Alan Ritter, and Eduard Hovy. 2014b. Weakly supervised user profile extraction from Twitter. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 165–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micol Marchetti-Bowick</author>
<author>Nathanael Chambers</author>
</authors>
<title>Learning for microblogs with distant supervision: Political forecasting with Twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>603--612</pages>
<contexts>
<context position="29109" citStr="Marchetti-Bowick and Chambers, 2012" startWordPosition="4737" endWordPosition="4740">, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun </context>
</contexts>
<marker>Marchetti-Bowick, Chambers, 2012</marker>
<rawString>Micol Marchetti-Bowick and Nathanael Chambers. 2012. Learning for microblogs with distant supervision: Political forecasting with Twitter. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 603–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The NomBank Project: An interim report.</title>
<date>2004</date>
<booktitle>In Proceedings of the NAACL/HLT Workshop on Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<contexts>
<context position="4862" citStr="Meyers et al., 2004" startWordPosition="767" endWordPosition="770"> given the sentence “I caught a cold,” we would predict that the first person (“I,” i.e., the poster) is the subject (carrier) of the cold. On the other hand, we can ignore the sentence, “The TV presenter caught a cold” only if we predict that the subject of the cold is the third person, who is at a different location from the poster. Although the task setting is simple and intuitive, we identify several key challenges in this study. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example,</context>
<context position="29679" citStr="Meyers et al., 2004" startWordPosition="4828" endWordPosition="4832"> et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG0 (typically, subject or agent) is “customer” and ARG1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted argum</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The NomBank Project: An interim report. In Proceedings of the NAACL/HLT Workshop on Frontiers in Corpus Annotation, pages 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media (ICWSM),</booktitle>
<pages>122--129</pages>
<marker>O’Connor, Balasubramanyan, Routledge, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, , and Noah A. Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media (ICWSM), pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="29772" citStr="Palmer et al., 2005" startWordPosition="4845" endWordPosition="4848">r profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG0 (typically, subject or agent) is “customer” and ARG1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted arguments for nominal predicates in Japanese (Komachi et al., 2007; Sasano et al., 2008). Our work</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>You are what you tweet: Analyzing Twitter for public health.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media (ICWSM),</booktitle>
<pages>265--272</pages>
<contexts>
<context position="1966" citStr="Paul and Dredze, 2011" startWordPosition="297" endWordPosition="300">is independent of the type of disease/symptom. 1 Introduction Social media services, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples include, “I don’t wish the </context>
</contexts>
<marker>Paul, Dredze, 2011</marker>
<rawString>Michael J. Paul and Mark Dredze. 2011. You are what you tweet: Analyzing Twitter for public health. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media (ICWSM), pages 265–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Dirk Hovy</author>
<author>Ryan McDonald</author>
<author>Anders Søgaard</author>
</authors>
<title>Adapting taggers to Twitter with not-so-distant supervision.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1783--1792</pages>
<contexts>
<context position="28314" citStr="Plank et al., 2014" startWordPosition="4609" endWordPosition="4612">derably. This is because the subjects for these symptoms are mostly FIRSTPERSON, as we seldom mention the symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock mar</context>
</contexts>
<marker>Plank, Hovy, McDonald, Søgaard, 2014</marker>
<rawString>Barbara Plank, Dirk Hovy, Ryan McDonald, and Anders Søgaard. 2014. Adapting taggers to Twitter with not-so-distant supervision. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1783–1792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquake shakes Twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World Wide Web (WWW),</booktitle>
<pages>851--860</pages>
<contexts>
<context position="29049" citStr="Sakaki et al., 2010" startWordPosition="4729" endWordPosition="4732">we did not incor7For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun.</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes Twitter users: real-time event detection by social sensors. In Proceedings of the 19th international conference on World Wide Web (WWW), pages 851–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Automatic construction of nominal case frames and its application to indirect anaphora resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1201--1207</pages>
<contexts>
<context position="4883" citStr="Sasano et al., 2004" startWordPosition="771" endWordPosition="774">I caught a cold,” we would predict that the first person (“I,” i.e., the poster) is the subject (carrier) of the cold. On the other hand, we can ignore the sentence, “The TV presenter caught a cold” only if we predict that the subject of the cold is the third person, who is at a different location from the poster. Although the task setting is simple and intuitive, we identify several key challenges in this study. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example, the subject of the v</context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2004</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2004. Automatic construction of nominal case frames and its application to indirect anaphora resolution. In Proceedings of the 20th international conference on Computational Linguistics, pages 1201–1207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for Japanese zero anaphora resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>769--776</pages>
<contexts>
<context position="30362" citStr="Sasano et al., 2008" startWordPosition="4936" endWordPosition="4939">PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG0 (typically, subject or agent) is “customer” and ARG1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted arguments for nominal predicates in Japanese (Komachi et al., 2007; Sasano et al., 2008). Our work shares a similar goal with the abovementioned studies, i.e., identifying an implicit ARG0 for a disease and symptom. However, these studies do not regard a disease/symptom as a nominal predicate because they consider verb nominalizations as nominal predicates. In addition, 1667 they use a corpus that consists of newswire text, the writing style and word usage of which differ considerably from those of tweets. For these reasons, we proposed a novel task setting for identifying subjects of diseases and symptoms, and we built an annotated corpus for developing the subject classifier an</context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2008</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2008. A fully-lexicalized probabilistic model for Japanese zero anaphora resolution. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 769–776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Tao Li</author>
</authors>
<title>A participant-based approach for event summarization using Twitter streams.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1152--1162</pages>
<contexts>
<context position="29128" citStr="Shen et al., 2013" startWordPosition="4741" endWordPosition="4744">els in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a simi</context>
</contexts>
<marker>Shen, Liu, Weng, Li, 2013</marker>
<rawString>Chao Shen, Fei Liu, Fuliang Weng, and Tao Li. 2013. A participant-based approach for event summarization using Twitter streams. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152–1162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Si</author>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
<author>Qing Li</author>
<author>Huayi Li</author>
<author>Xiaotie Deng</author>
</authors>
<title>Exploiting topic based Twitter sentiment for stock prediction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>24--29</pages>
<contexts>
<context position="28942" citStr="Si et al., 2013" startWordPosition="4712" endWordPosition="4715"> normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be </context>
</contexts>
<marker>Si, Mukherjee, Liu, Li, Li, Deng, 2013</marker>
<rawString>Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie Deng. 2013. Exploiting topic based Twitter sentiment for stock prediction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 24–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessio Signorini</author>
<author>Alberto Maria Segre</author>
<author>Philip M Polgreen</author>
</authors>
<title>The use of Twitter to track levels of disease activity and public concern in the U.S. during the influenza A H1N1 pandemic.</title>
<date>2011</date>
<journal>PLoS ONE,</journal>
<volume>6</volume>
<issue>5</issue>
<contexts>
<context position="1990" citStr="Signorini et al., 2011" startWordPosition="301" endWordPosition="304">ype of disease/symptom. 1 Introduction Social media services, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples include, “I don’t wish the flu on anyone” and “A Ha</context>
</contexts>
<marker>Signorini, Segre, Polgreen, 2011</marker>
<rawString>Alessio Signorini, Alberto Maria Segre, and Philip M. Polgreen. 2011. The use of Twitter to track levels of disease activity and public concern in the U.S. during the influenza A H1N1 pandemic. PLoS ONE, 6(5):e19467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Stoové</author>
<author>Alisa E Pedrana</author>
</authors>
<title>Making the most of a brave new world: Opportunities and considerations for using Twitter as a public health monitoring tool. Preventive Medicine,</title>
<date>2014</date>
<pages>63--109</pages>
<contexts>
<context position="2076" citStr="Stoové and Pedrana, 2014" startWordPosition="315" endWordPosition="318"> Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples include, “I don’t wish the flu on anyone” and “A Harry Potter actor hospitalised after severe flu-like syndromes.” Lazer et al. (2014) re</context>
</contexts>
<marker>Stoové, Pedrana, 2014</marker>
<rawString>Mark A. Stoové and Alisa E. Pedrana. 2014. Making the most of a brave new world: Opportunities and considerations for using Twitter as a public health monitoring tool. Preventive Medicine, 63:109–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
</authors>
<title>Sentiment in Twitter events.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>2</issue>
<contexts>
<context position="29072" citStr="Thelwall et al., 2011" startWordPosition="4733" endWordPosition="4736">the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al.</context>
</contexts>
<marker>Thelwall, Buckley, Paltoglou, 2011</marker>
<rawString>Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. 2011. Sentiment in Twitter events. Journal of the American Society for Information Science and Technology, 62(2):406–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>István Varga</author>
<author>Motoki Sano</author>
</authors>
<title>Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1619--1629</pages>
<marker>Varga, Sano, 2013</marker>
<rawString>István Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao Kawai, JongHoon Oh, and Stijn De Saeger. 2013. Aid is out there: Looking for help from tweets during a large scale disaster. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1619– 1629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Velardi</author>
<author>Giovanni Stilo</author>
<author>Alberto E Tozzi</author>
<author>Francesco Gesualdo</author>
</authors>
<title>Twitter mining for finegrained syndromic surveillance.</title>
<date>2014</date>
<journal>Artificial Intelligence in Medicine,</journal>
<volume>61</volume>
<issue>3</issue>
<contexts>
<context position="3593" citStr="Velardi et al., 2014" startWordPosition="560" endWordPosition="563">r to distinguish tweets reporting flu infections from those expressing concern or awareness about the flu, e.g., “Starting to get worried about swine flu.” Accordingly, they reported encouraging results (e.g., better correlations with CDC trends), but their approach requires supervision data and a lexicon (word class features) specially designed for the flu. Moreover, even though this method is a reasonable choice for improving the accuracy, it is not readily applicable to other types of diseases (e.g., dengue fever) and symptoms (e.g., runny nose), which are also important for public health (Velardi et al., 2014). In this paper, we propose a more generalized task setting for public surveillance. In other words, our objective is to estimate the subject (carrier) of a disease or symptom mentioned in a Japanese tweet. More specifically, we are interested in determining who has a disease/symptom 1660 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1660–1670, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics (if any) in order to examine whether the poster</context>
</contexts>
<marker>Velardi, Stilo, Tozzi, Gesualdo, 2014</marker>
<rawString>Paola Velardi, Giovanni Stilo, Alberto E. Tozzi, and Francesco Gesualdo. 2014. Twitter mining for finegrained syndromic surveillance. Artificial Intelligence in Medicine, 61(3):153–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Williams</author>
<author>Graham Katz</author>
</authors>
<title>Extracting and modeling durations for habits and events from Twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>223--227</pages>
<contexts>
<context position="29351" citStr="Williams and Katz, 2012" startWordPosition="4780" endWordPosition="4783">. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG0 (typically, subject or agent) is “customer” and ARG1 (t</context>
</contexts>
<marker>Williams, Katz, 2012</marker>
<rawString>Jennifer Williams and Graham Katz. 2012. Extracting and modeling durations for habits and events from Twitter. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 223–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyu Zhou</author>
<author>Liangyu Chen</author>
<author>Yulan He</author>
</authors>
<title>A simple bayesian modelling approach to event extraction from Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>700--705</pages>
<contexts>
<context position="29241" citStr="Zhou et al., 2014" startWordPosition="4764" endWordPosition="4767">ode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates</context>
</contexts>
<marker>Zhou, Chen, He, 2014</marker>
<rawString>Deyu Zhou, Liangyu Chen, and Yulan He. 2014. A simple bayesian modelling approach to event extraction from Twitter. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 700– 705.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>