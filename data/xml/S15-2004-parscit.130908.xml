<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.047573">
<title confidence="0.9963555">
HLTC-HKUST: A Neural Network Paraphrase Classifier using Translation
Metrics, Semantic Roles and Lexical Similarity Features
</title>
<author confidence="0.999834">
Dario Bertero, Pascale Fung
</author>
<affiliation confidence="0.997313666666667">
Human Language Technology Center
Department of Electronic and Computer Engineering
The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong
</affiliation>
<email confidence="0.993992">
dbertero@ust.hk, pascale@ece.ust.hk
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997268">
This paper describes the system developed by
our team (HLTC-HKUST) for task 1 of Se-
mEval 2015 workshop about paraphrase clas-
sification and semantic similarity in Twitter.
We trained a neural network classifier over a
range of features that includes translation met-
rics, lexical and syntactic similarity score and
semantic features based on semantic roles. The
neural network was trained taking into consid-
eration in the objective function the six dif-
ferent similarity levels provided in the corpus,
in order to give as output a more fine-grained
estimation of the similarity level of the two
sentences, as required by subtask 2. With an
F-score of 0.651 in the binary paraphrase clas-
sification subtask 1, and a Pearson coefficient
of 0.697 for the sentence similarity subtask 2,
we achieved respectively the 6th place and the
3rd place, above the average of what obtained
by the other contestants.
</bodyText>
<sectionHeader confidence="0.999123" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999839413043478">
Paraphrase identification is the problem to determine
whether two sentences have the same meaning, and is
the objective of the task 1 of SemEval 2015 workshop
(Xu et al., 2015).
Conventionally this task has been mainly evaluated
on the Microsoft Research Paraphrase corpus (Dolan
and Brockett, 2005), which consists of pairs of sen-
tences taken out from news headlines and articles.
News domain sentences are usually grammatically
correct and of average to long length. The current
state-of-the-art method to our knowledge on this cor-
pus (Ji and Eisenstein, 2013) trains an SVM over
latent semantic vectors, lexical and syntactic simi-
larity features. Although their main objective was
to show the effectiveness of a method based on la-
tent semantic analysis, it is also evident that other
features pertinent to different aspects of sentence sim-
ilarity are able to boost the results. Previously Socher
et al. (2011) used a recursive autoencoder to simi-
larly obtain a vector representation of each sentence,
again combining other lexical similarity features to
improve the results. Other methods, such as Mad-
nani et al. (2012) or Wan et al. (2006) used instead a
more traditional supervised classification approach
over different sets of features and different classifiers,
most of which improved previous results.
Task 1 of SemEval 2015 workshop required to
evaluate paraphrases on a new corpus, consisting of
sentences taken from Twitter posts (Xu et al., 2014).
Twitter sentences notoriously differ from those taken
from news articles: the 140 characters limit makes
the sentences short, with few words, lots of different
abbreviations; they also include many misspelled and
invented words, and often lack a correct grammatical
structure. Another important difference is the six-
level classification labels provided, compared to the
binary labels of MSRP corpus, which allows a fine-
grained evaluation of the similarity level between the
sentences.
The task was divided into two subtasks. Subtask
1 was the classical binary paraphrase classification
task, where given a pair of sentences the system had
to identify if it is a paraphrase or not. Subtask 2
instead required the system to provide a score in the
range [0, 1] that measures the actual similarity level
of the two sentences.
</bodyText>
<page confidence="0.982626">
23
</page>
<note confidence="0.610183">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 23–28,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.948343" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999946363636364">
We chose a supervised machine learning strategy
based on a multi-view set of features. Our first goal
was to select the features in order to get a complete
estimation of lexical, syntactic and semantic similar-
ity between any given pair of sentences. In particular
we were interested in what roles semantic features
can play in this task. The second goal was to make
use of a classifier which can take full advantage of the
six level labeling provided in order to have good per-
formance in both subtasks, identified in an artificial
neural network.
</bodyText>
<subsectionHeader confidence="0.994017">
2.1 Lexical and Syntactic Similarity Features
</subsectionHeader>
<bodyText confidence="0.999993785714286">
The first set of lexical features includes three binary
indexes obtained from the analysis of the numerical
tokens: the first of them is 1 if they are the same in
both sentences or there are not any, the second is 1
only if they are the same, and the third is 1 if the
tokens representing numbers of one sentences are the
subset of the other (Socher et al., 2011). Two other
features include the percentage of overlapping tokens,
and the difference in sentence length. Another feature
considers the word order: starting from one sentence
we align the tokens that matches with the other sen-
tence, and for each aligned pair we take the average
of the differences of the absolute positions of the two
elements, normalized by the length of the first sen-
tence, and we do the same switching the order of the
two sentences. Another group of features involves
WordNet word synonym sets (Miller, 1995). We take
from them, separately for nouns and verbs, the av-
erage of the path similarity scores obtained, among
all word alignments, from the one which gives the
maximum score. When the two words in the pair
to be scored have multiple synonym sets we select
the two sets that again are giving the highest score.
Finally, in order to include an estimation of the level
of similarity in the syntax parse tree of the sentences,
we use the parse tree edit distance from the Zhang-
Shasha algorithm (Zhang and Shasha, 1989; Wan
et al., 2006).
</bodyText>
<subsectionHeader confidence="0.999428">
2.2 Semantic Similarity Features
</subsectionHeader>
<bodyText confidence="0.999982666666666">
The way we evaluate the semantic similarity of each
pair of sentences is through the analysis of the se-
mantic roles. The first feature we choose in this
sense is the semantic role based MEANT machine
translation score (Lo et al., 2012), effective to pro-
vide, as shown by various experiments, a translation
evaluation closer to human judges. This metric first
annotates each sentence with semantic roles (Pradhan
et al., 2004), then aligns them and computes a similar-
ity score only within the aligned frames (Fung et al.,
2007) using the Jaccard coefficient (Tumuluru et al.,
2012). Another set of features is obtained by looking
at the semantic roles themselves and their alignment
without looking at the content: these include the per-
centage of semantic roles of one sentence that are
also present in the other, the percentage of correct
pairs of semantic roles after the alignment operated
for MEANT, and a binary feature equal to 1 in case
the semantic parser fails to give any output for at
least one of the sentences. In this last case all the
other features based on semantic roles are 0 except
the MEANT score which is set to the value of the
Jaccard coefficient between the whole sentences (Lo
and Wu, 2013).
</bodyText>
<subsectionHeader confidence="0.997475">
2.3 Translation Metrics
</subsectionHeader>
<bodyText confidence="0.9999598">
Previous work (Finch et al., 2005; Madnani et al.,
2012) have shown that machine translation evalua-
tion metrics are useful for the paraphrase recognition
task, due to their ability to capture useful similarity
information to correctly classify the sentence pairs.
The various translation metrics all take into
account different aspects of sentence similarities.
BLEU (Papineni et al., 2002) and the subsequent
evaluation metrics such as NIST (Goutte, 2006) and
SEPIA (Habash and Elkholy, 2008) look at n-gram
overlaps between the source and the target sentences.
While the most basic BLEU takes into consideration
only n-gram overlap, the other metrics also consider
synonyms, stemming, simple paraphrase patterns and
the syntactic structure of the n-grams. Yet another
set of metrics are based instead on different princi-
ples: TER (Snover et al., 2006) and TERp (Snover
et al., 2009) count the number of edits needed to
transform a sentence into the other, MAXSIM (Chan
and Ng, 2008) evaluates lexical similarity perform-
ing a word-by-word matching and finding out how
much the aligned words are similar in each mean-
ing, BADGER (Parker, 2008) the distance between
the compression of each sentence obtained from the
Burrows-Wheeler transform algorithm (Burrows and
</bodyText>
<page confidence="0.992304">
24
</page>
<bodyText confidence="0.998491846153846">
Wheeler, 1994), and MEANT which, as discussed in
the previous section, scores the similarity of aligned
semantic frames.
For each pair of sentences the scores are calculated
first taking one of the sentences as the reference and
the other as the sample and then vice-versa. Both
scores are included as distinct features except in the
case of BADGER, as it computes a distance between
two objects without taking into account the direction.
In case of BLEU and NIST we use the scores from
unigrams up to 4-grams for BLEU (Madnani et al.,
2012) and up to the maximum order which gives at
least one result different than zero for NIST.
</bodyText>
<subsectionHeader confidence="0.943859">
2.4 Classifier
</subsectionHeader>
<bodyText confidence="0.99938670967742">
To classify the sentence pairs we design a feedfor-
ward neural network. One of the main properties
of the neural network is its ability to learn complex
functions of the input values (Hornik et al., 1989). It
follows that in our task, given the combination of fea-
tures, the network would learn how to combine them
effectively and take advantage of their mutual interac-
tion. The neural network can also be trained using an
objective function that takes into consideration a la-
bel not just binary but which can take multiple values
in a given range. Therefore it has a good ability to
determine as output a precise estimation of the sim-
ilarity level of the sentence pair, particularly useful
in subtask 2. During our experiments the results we
obtained in the binary classification task over the de-
velopment set with the neural network were always at
least slightly higher than those obtained with an SVM
we used as a comparison system, further justifying
our neural network choice.
We choose a two layer standard configuration (hid-
den and output layer), where we fix the size of the
hidden layer large enough at three times the size of
the input layer; the hyperbolic tangent (tanh) and the
sigmoid are used respectively as the non-linear acti-
vation functions of the hidden layer and the output
layer. Due to this choice the output assumes values
in the interval [0, 1], which is also exactly the output
range required in subtask 2. The network weights,
with the exception of the ones associated to the bias
terms set at zero, are initialized (Glorot and Bengio,
2010) with uniform values in the range:
</bodyText>
<equation confidence="0.947641666666667">
� � 6 �1 � 6 �1 �
wt=0 ∈ −α , α (1)
nin + nout nin + nout
</equation>
<bodyText confidence="0.998231">
Where α = 1 in case the activation function is the
hyperbolic tangent, and α = 4 with the sigmoid. We
train the model using standard backpropagation algo-
rithm, taking the cross-entropy as the cost objective
function:
</bodyText>
<equation confidence="0.999669">
E = −l log(y) − (1 − l) log(1 − y) + R (2)
</equation>
<bodyText confidence="0.998457333333333">
where y is the network output, l the objective value
(both in the range [0,1]), and R is an L2 regulariza-
tion term.
</bodyText>
<sectionHeader confidence="0.999533" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.988">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9995263">
We made use of the corpus provided for the contest
(Xu et al., 2014), made of a training set of 13063
sentence pairs, a development set of 4727 pairs, and
a test set of 972 pairs released a few days before the
deadline without the labels. Each pair of sentences
was labeled by five users via Amazon Mechanical
Turk, hence providing a six-level classification label
(from (5, 0) when all the five user classify the pair as
a paraphrase, to (0, 5) when none of them identifies
the pair to be a paraphrase).
</bodyText>
<subsectionHeader confidence="0.993769">
3.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999775590909091">
The neural network was setup with a hidden layer
dimension of three times the input. The development
set was used to tune the L2 regularization coefficient,
set at γ = 0.01, as well as the learning rate and the
other hyperparameters, and to have a measure of im-
provement against the official thresholding baseline
provided for the task (Das and Smith, 2009). To
implement the neural network we used THEANO
Python toolkit (Bergstra et al., 2010).
We train the network with all the sentences pro-
vided in the training set. The objective label of the
cross-entropy objective function was set to 1.0 for
pairs labeled (5, 0) and (4, 1), 0.75 for pairs labeled
(3, 2), 0.5 for pairs labeled (2, 3) and 0.0 for pairs la-
beled (0, 5). This choice allowed a more fine training
for task 2, where a continuous similarity value must
be estimated, without altering too much the behavior
in the binary estimation task 1.
The training procedure was repeated several times,
each time with a different random initialization of the
weights and with a different random pair order. In
order to avoid overfitting, in each run the training was
</bodyText>
<page confidence="0.996262">
25
</page>
<table confidence="0.999965">
Description Precision Subtask 1 F-score Precision Subtask 2 Pearson
Recall Recall F-score
Subtask 1 best (ASOBEK) 0.680 0.669 0.674 0.732 0.531 0.616 0.475
Subtask 2 best (MITRE) 0.569 0.806 0.667 0.750 0.686 0.716 0.619
Our method, run 2 0.574 0.754 0.652 0.738 0.611 0.669 0.545
Our method, run 1 0.594 0.720 0.651 0.697 0.657 0.676 0.563
Baseline (Das and Smith, 2009) 0.679 0.520 0.589 0.674 0.543 0.601 0.511
Contest average result 0.600 0.626 0.581 0.645 0.626 0.631 0.483
</table>
<tableCaption confidence="0.999929">
Table 1: Result comparison between our method and the winners of subtask 1 and subtask 2.
</tableCaption>
<bodyText confidence="0.9994362">
stopped when the best results on the development set
were obtained. The final results were taken from the
run that yielded the best accuracy, and in case of tie
the best F1 score, on the development set for subtask
1.
Run 2 instead was an attempt to include latent
semantic vectors obtained through the procedure de-
scribed in Ji and Eisenstein (2013) and added to the
network from an extra layer whose output was con-
catenated to the features input vector.
</bodyText>
<subsectionHeader confidence="0.870732">
3.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999983282608696">
F-measure and Pearson coefficient were the official
evaluation metrics used to rank respectively subtask
1 and subtask 2. In subtask 1 – binary evaluation of
the sentence pairs – we achieved an F-score of 0.651
and ranked 6th over 18 methods, the best method
(ASOBEK) achieved an F-score of 0.674. In subtask
2, which was aimed at finding a similarity score in
the range [0, 1], with a Pearson coefficient of 0.563
we reached the 3rd place among 13 methods (the
other five provided only a binary output), with the
winner (MITRE) obtaining a Pearson score of 0.619.
A summary and comparison of our results with the
winners of the two subtasks, with the average results
and with the supervised official baseline (n-gram
overlapping features with logistic regression from
Das and Smith (2009)) is shown in table 1. For both
tasks our results are above the average both in term
of ranking and average results.
Semantic features were useful to identify para-
phrases, as they improved the accuracy and F-score
on the development set by 0.6%. But often the shal-
low semantic parser failed to give an output for many
sentences, limiting their potential contribution. This
is due to two main reasons. The first one is the imper-
fect accuracy of the semantic parser itself, also ob-
served in previous experiments where we employed
it, which fails to analyze sentences containing certain
patterns and predicates. The second reason, more
specific to Twitter domain, is that some sentences
lack a valid predicate or a proper grammatical struc-
ture. This prevents the semantic parser from giving
an accurate output.
The inclusion on latent semantic features in run
2 proved to be ineffective, as it improved subtask
1 F-score by less than 0.001, and gave a worse per-
formance in subtask 2. During the evaluation phase
other experiments were tried as using the latent se-
mantic vectors of Guo and Diab (2012), or using
the vectors as described in Ji and Eisenstein (2013)
instead of the extra layer, and other modifications,
all without obtaining any perceptible improvement
when the system was tested on the development set.
The non-perfect implementation and usage of these
features, together with the fact they might not be suit-
able to be applied to Twitter domain, may explain
this lack of improvement.
</bodyText>
<sectionHeader confidence="0.997904" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999973">
We have used a neural network classifier, with a com-
bination of multiple views of lexical, syntactic and
semantic information, as the system which partici-
pated in SemEval 2015 task 1, whose goal was to
classify paraphrases in Twitter. The inaccurate se-
mantic parsing is the main reason which prevented
us from obtain higher results. A possible future di-
rections that can improve the quality of the semantic
roles annotations, apart from improving the semantic
parser, is to apply an effective lexical normalization
method (such as Han and Baldwin (2011)), and even-
tually find ways to reconstruct the predicate in case
it is missing.
</bodyText>
<sectionHeader confidence="0.989719" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.941299">
This work is partially funded by the Hong Kong PhD
Fellowship Scheme and by grant number 1314159-
0PAFT20F003 of the Ping An Research Institute.
</bodyText>
<page confidence="0.996051">
26
</page>
<sectionHeader confidence="0.96171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990741112903226">
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. Theano: a CPU and GPU
Math Expression Compiler. In Proceedings of
the Python for Scientific Computing Conference
(SciPy), June 2010.
Michael Burrows and David J. Wheeler. A Block-
sorting Lossless Data Compression Algorithm.
Technical report, 1994.
Yee Seng Chan and Hwee Tou Ng. Maxsim: A Max-
imum Similarity Metric for Machine Translation
Evaluation. In ACL, pages 55–62, 2008.
Dipanjan Das and Noah A. Smith. Paraphrase
Identification as Probabilistic Quasi-Synchronous
Recognition. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1,
pages 468–476, 2009.
William B. Dolan and Chris Brockett. Automatically
Constructing a Corpus of Sentential Paraphrases.
In Proc. of IWP, 2005.
Andrew Finch, Young-Sook Hwang, and Eiichiro
Sumita. Using Machine Translation Evaluation
Techniques to Determine Sentence-level Semantic
Equivalence. In Proceedings of the Third Inter-
national Workshop on Paraphrasing (IWP2005),
pages 17–24, 2005.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and
Dekai Wu. Learning Bilingual Semantic Frames:
Shallow Semantic Parsing vs. Semantic Role Pro-
jection. In 11th Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI 2007), pages 75–84, 2007.
Xavier Glorot and Yoshua Bengio. Understanding
the difficulty of training deep feedforward neural
networks. In International Conference on Artificial
Intelligence and Statistics, pages 249–256, 2010.
Cyril Goutte. Automatic Evaluation of Machine
Translation Quality. Presentation at the European
Community, Xerox Research Centre Europe, on
January, 2006.
Weiwei Guo and Mona Diab. Modeling Sentences
in the Latent Space. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1, pages
864–872, 2012.
Nizar Habash and Ahmed Elkholy. Sepia: Sur-
face Span Extension to Syntactic Dependency
Precision-based mt Evaluation. In Proceedings
of the NIST metrics for machine translation work-
shop at the association for machine translation
in the Americas conference, AMTA-2008. Waikiki,
HI, 2008.
Bo Han and Timothy Baldwin. Lexical Normalisa-
tion of Short Text Messages: Makn Sens a #Twit-
ter. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT
’11, pages 368–378, 2011.
Kurt Hornik, Maxwell Stinchcombe, and Halbert
</reference>
<bodyText confidence="0.7581043">
White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359–
366, 1989.
Yangfeng Ji and Jacob Eisenstein. Discriminative Im-
provements to Distributional Sentence Similarity.
In EMNLP, pages 891–896, 2013.
Chi-kiu Lo and Dekai Wu. Meant at wmt 2013: A
tunable, accurate yet inexpensive semantic frame
based mt evaluation metric. In 8th Workshop on
Statistical Machine Translation, page 422, 2013.
</bodyText>
<reference confidence="0.9979344375">
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully Automatic Semantic mt Evaluation. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, pages 243–252, 2012.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
Re-examining Machine Translation Metrics for
Paraphrase Identification. In Proceedings of the
2012 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, pages 182–190,
2012.
George A. Miller. Wordnet: A Lexical Database
for English. Communications of the ACM, 38(11):
39–41, 1995.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. Bleu: a Method for Automatic
</reference>
<page confidence="0.966327">
27
</page>
<reference confidence="0.993026137254902">
Evaluation of Machine Translation. In Proceed-
ings of the 40th annual meeting on association for
computational linguistics, pages 311–318, 2002.
Steven Parker. Badger: A New Machine Translation
Metric. Metrics for Machine Translation Chal-
lenge, 2008.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. Shallow
Semantic Parsing using Support Vector Machines.
In HLT-NAACL, pages 233–240, 2004.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A Study of
Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of association for machine
translation in the Americas, pages 223–231, 2006.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. Ter-Plus: paraphrase, seman-
tic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3):117–127,
2009.
Richard Socher, Eric H. Huang, Jeffrey Pennin,
Christopher D. Manning, and Andrew Y. Ng. Dy-
namic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Advances
in Neural Information Processing Systems, pages
801–809, 2011.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu.
Accuracy and robustness in measuring the lexical
similarity of semantic role fillers for automatic
semantic mt evaluation. In Proceedings of the 26th
Pacific Asia Conference on Language, Information,
and Computation, 2012.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile
Paris. Using Dependency-Based Features to Take
the Para-farce out of Paraphrase. In Proceedings of
the Australasian Language Technology Workshop,
2006.
Wei Xu, Alan Ritter, Chris Callison-Burch,
William B. Dolan, and Yangfeng Ji. Extract-
ing Lexically Divergent Paraphrases from Twitter.
Transactions Of The Association For Computa-
tional Linguistics, 2:435–448, 2014.
Wei Xu, Chris Callison-Burch, and William B. Dolan.
SemEval-2015 Task 1: Paraphrase and semantic
similarity in Twitter (PIT). In Proceedings of the
9th International Workshop on Semantic Evalua-
tion (SemEval), 2015.
Kaizhong Zhang and Dennis Shasha. Simple fast
algorithms for the editing distance between trees
and related problems. SIAM journal on computing,
18(6):1245–1262, 1989.
</reference>
<page confidence="0.99902">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.260466">
<title confidence="0.999791">HLTC-HKUST: A Neural Network Paraphrase Classifier using Translation Metrics, Semantic Roles and Lexical Similarity Features</title>
<author confidence="0.7675255">Dario Bertero</author>
<author confidence="0.7675255">Pascale Human Language Technology</author>
<affiliation confidence="0.980002">Department of Electronic and Computer</affiliation>
<author confidence="0.515551">The Hong Kong University of Science</author>
<author confidence="0.515551">Clear Water Bay Technology</author>
<author confidence="0.515551">Hong</author>
<email confidence="0.983936">dbertero@ust.hk,pascale@ece.ust.hk</email>
<abstract confidence="0.998527333333333">This paper describes the system developed by our team (HLTC-HKUST) for task 1 of SemEval 2015 workshop about paraphrase classification and semantic similarity in Twitter. We trained a neural network classifier over a range of features that includes translation metrics, lexical and syntactic similarity score and semantic features based on semantic roles. The neural network was trained taking into consideration in the objective function the six different similarity levels provided in the corpus, in order to give as output a more fine-grained estimation of the similarity level of the two sentences, as required by subtask 2. With an of the binary paraphrase classification subtask 1, and a Pearson coefficient the sentence similarity subtask 2, we achieved respectively the 6th place and the 3rd place, above the average of what obtained by the other contestants.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU Math Expression Compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy),</booktitle>
<contexts>
<context position="12001" citStr="Bergstra et al., 2010" startWordPosition="2003" endWordPosition="2006">fication label (from (5, 0) when all the five user classify the pair as a paraphrase, to (0, 5) when none of them identifies the pair to be a paraphrase). 3.2 Experimental Setup The neural network was setup with a hidden layer dimension of three times the input. The development set was used to tune the L2 regularization coefficient, set at γ = 0.01, as well as the learning rate and the other hyperparameters, and to have a measure of improvement against the official thresholding baseline provided for the task (Das and Smith, 2009). To implement the neural network we used THEANO Python toolkit (Bergstra et al., 2010). We train the network with all the sentences provided in the training set. The objective label of the cross-entropy objective function was set to 1.0 for pairs labeled (5, 0) and (4, 1), 0.75 for pairs labeled (3, 2), 0.5 for pairs labeled (2, 3) and 0.0 for pairs labeled (0, 5). This choice allowed a more fine training for task 2, where a continuous similarity value must be estimated, without altering too much the behavior in the binary estimation task 1. The training procedure was repeated several times, each time with a different random initialization of the weights and with a different ra</context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU Math Expression Compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Burrows</author>
<author>David J Wheeler</author>
</authors>
<title>A Blocksorting Lossless Data Compression Algorithm.</title>
<date>1994</date>
<tech>Technical report,</tech>
<marker>Burrows, Wheeler, 1994</marker>
<rawString>Michael Burrows and David J. Wheeler. A Blocksorting Lossless Data Compression Algorithm. Technical report, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Maxsim: A Maximum Similarity Metric for Machine Translation Evaluation. In</title>
<date>2008</date>
<booktitle>ACL,</booktitle>
<pages>55--62</pages>
<contexts>
<context position="8039" citStr="Chan and Ng, 2008" startWordPosition="1302" endWordPosition="1305"> (Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics also consider synonyms, stemming, simple paraphrase patterns and the syntactic structure of the n-grams. Yet another set of metrics are based instead on different principles: TER (Snover et al., 2006) and TERp (Snover et al., 2009) count the number of edits needed to transform a sentence into the other, MAXSIM (Chan and Ng, 2008) evaluates lexical similarity performing a word-by-word matching and finding out how much the aligned words are similar in each meaning, BADGER (Parker, 2008) the distance between the compression of each sentence obtained from the Burrows-Wheeler transform algorithm (Burrows and 24 Wheeler, 1994), and MEANT which, as discussed in the previous section, scores the similarity of aligned semantic frames. For each pair of sentences the scores are calculated first taking one of the sentences as the reference and the other as the sample and then vice-versa. Both scores are included as distinct featur</context>
</contexts>
<marker>Chan, Ng, 2008</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. Maxsim: A Maximum Similarity Metric for Machine Translation Evaluation. In ACL, pages 55–62, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>468--476</pages>
<contexts>
<context position="11914" citStr="Das and Smith, 2009" startWordPosition="1989" endWordPosition="1992"> labeled by five users via Amazon Mechanical Turk, hence providing a six-level classification label (from (5, 0) when all the five user classify the pair as a paraphrase, to (0, 5) when none of them identifies the pair to be a paraphrase). 3.2 Experimental Setup The neural network was setup with a hidden layer dimension of three times the input. The development set was used to tune the L2 regularization coefficient, set at γ = 0.01, as well as the learning rate and the other hyperparameters, and to have a measure of improvement against the official thresholding baseline provided for the task (Das and Smith, 2009). To implement the neural network we used THEANO Python toolkit (Bergstra et al., 2010). We train the network with all the sentences provided in the training set. The objective label of the cross-entropy objective function was set to 1.0 for pairs labeled (5, 0) and (4, 1), 0.75 for pairs labeled (3, 2), 0.5 for pairs labeled (2, 3) and 0.0 for pairs labeled (0, 5). This choice allowed a more fine training for task 2, where a continuous similarity value must be estimated, without altering too much the behavior in the binary estimation task 1. The training procedure was repeated several times, </context>
<context position="14522" citStr="Das and Smith (2009)" startWordPosition="2435" endWordPosition="2438">e pairs – we achieved an F-score of 0.651 and ranked 6th over 18 methods, the best method (ASOBEK) achieved an F-score of 0.674. In subtask 2, which was aimed at finding a similarity score in the range [0, 1], with a Pearson coefficient of 0.563 we reached the 3rd place among 13 methods (the other five provided only a binary output), with the winner (MITRE) obtaining a Pearson score of 0.619. A summary and comparison of our results with the winners of the two subtasks, with the average results and with the supervised official baseline (n-gram overlapping features with logistic regression from Das and Smith (2009)) is shown in table 1. For both tasks our results are above the average both in term of ranking and average results. Semantic features were useful to identify paraphrases, as they improved the accuracy and F-score on the development set by 0.6%. But often the shallow semantic parser failed to give an output for many sentences, limiting their potential contribution. This is due to two main reasons. The first one is the imperfect accuracy of the semantic parser itself, also observed in previous experiments where we employed it, which fails to analyze sentences containing certain patterns and pre</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A. Smith. Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1, pages 468–476, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically Constructing a Corpus of Sentential Paraphrases.</title>
<date>2005</date>
<booktitle>In Proc. of IWP,</booktitle>
<contexts>
<context position="1572" citStr="Dolan and Brockett, 2005" startWordPosition="236" endWordPosition="239"> two sentences, as required by subtask 2. With an F-score of 0.651 in the binary paraphrase classification subtask 1, and a Pearson coefficient of 0.697 for the sentence similarity subtask 2, we achieved respectively the 6th place and the 3rd place, above the average of what obtained by the other contestants. 1 Introduction Paraphrase identification is the problem to determine whether two sentences have the same meaning, and is the objective of the task 1 of SemEval 2015 workshop (Xu et al., 2015). Conventionally this task has been mainly evaluated on the Microsoft Research Paraphrase corpus (Dolan and Brockett, 2005), which consists of pairs of sentences taken out from news headlines and articles. News domain sentences are usually grammatically correct and of average to long length. The current state-of-the-art method to our knowledge on this corpus (Ji and Eisenstein, 2013) trains an SVM over latent semantic vectors, lexical and syntactic similarity features. Although their main objective was to show the effectiveness of a method based on latent semantic analysis, it is also evident that other features pertinent to different aspects of sentence similarity are able to boost the results. Previously Socher </context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B. Dolan and Chris Brockett. Automatically Constructing a Corpus of Sentential Paraphrases. In Proc. of IWP, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
</authors>
<title>Young-Sook Hwang, and Eiichiro Sumita. Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing (IWP2005),</booktitle>
<pages>17--24</pages>
<marker>Finch, 2005</marker>
<rawString>Andrew Finch, Young-Sook Hwang, and Eiichiro Sumita. Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), pages 17–24, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Zhaojun Wu</author>
<author>Yongsheng Yang</author>
<author>Dekai Wu</author>
</authors>
<title>Learning Bilingual Semantic Frames: Shallow Semantic Parsing vs. Semantic Role Projection.</title>
<date>2007</date>
<booktitle>In 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI</booktitle>
<pages>75--84</pages>
<contexts>
<context position="6340" citStr="Fung et al., 2007" startWordPosition="1024" endWordPosition="1027">asha algorithm (Zhang and Shasha, 1989; Wan et al., 2006). 2.2 Semantic Similarity Features The way we evaluate the semantic similarity of each pair of sentences is through the analysis of the semantic roles. The first feature we choose in this sense is the semantic role based MEANT machine translation score (Lo et al., 2012), effective to provide, as shown by various experiments, a translation evaluation closer to human judges. This metric first annotates each sentence with semantic roles (Pradhan et al., 2004), then aligns them and computes a similarity score only within the aligned frames (Fung et al., 2007) using the Jaccard coefficient (Tumuluru et al., 2012). Another set of features is obtained by looking at the semantic roles themselves and their alignment without looking at the content: these include the percentage of semantic roles of one sentence that are also present in the other, the percentage of correct pairs of semantic roles after the alignment operated for MEANT, and a binary feature equal to 1 in case the semantic parser fails to give any output for at least one of the sentences. In this last case all the other features based on semantic roles are 0 except the MEANT score which is </context>
</contexts>
<marker>Fung, Wu, Yang, Wu, 2007</marker>
<rawString>Pascale Fung, Zhaojun Wu, Yongsheng Yang, and Dekai Wu. Learning Bilingual Semantic Frames: Shallow Semantic Parsing vs. Semantic Role Projection. In 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI 2007), pages 75–84, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Yoshua Bengio</author>
</authors>
<title>Understanding the difficulty of training deep feedforward neural networks.</title>
<date>2010</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="10535" citStr="Glorot and Bengio, 2010" startWordPosition="1727" endWordPosition="1730">justifying our neural network choice. We choose a two layer standard configuration (hidden and output layer), where we fix the size of the hidden layer large enough at three times the size of the input layer; the hyperbolic tangent (tanh) and the sigmoid are used respectively as the non-linear activation functions of the hidden layer and the output layer. Due to this choice the output assumes values in the interval [0, 1], which is also exactly the output range required in subtask 2. The network weights, with the exception of the ones associated to the bias terms set at zero, are initialized (Glorot and Bengio, 2010) with uniform values in the range: � � 6 �1 � 6 �1 � wt=0 ∈ −α , α (1) nin + nout nin + nout Where α = 1 in case the activation function is the hyperbolic tangent, and α = 4 with the sigmoid. We train the model using standard backpropagation algorithm, taking the cross-entropy as the cost objective function: E = −l log(y) − (1 − l) log(1 − y) + R (2) where y is the network output, l the objective value (both in the range [0,1]), and R is an L2 regularization term. 3 Experiments 3.1 Corpus We made use of the corpus provided for the contest (Xu et al., 2014), made of a training set of 13063 sent</context>
</contexts>
<marker>Glorot, Bengio, 2010</marker>
<rawString>Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, pages 249–256, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Goutte</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality. Presentation at the European Community, Xerox Research Centre Europe,</title>
<date>2006</date>
<location>on</location>
<contexts>
<context position="7511" citStr="Goutte, 2006" startWordPosition="1218" endWordPosition="1219">les are 0 except the MEANT score which is set to the value of the Jaccard coefficient between the whole sentences (Lo and Wu, 2013). 2.3 Translation Metrics Previous work (Finch et al., 2005; Madnani et al., 2012) have shown that machine translation evaluation metrics are useful for the paraphrase recognition task, due to their ability to capture useful similarity information to correctly classify the sentence pairs. The various translation metrics all take into account different aspects of sentence similarities. BLEU (Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics also consider synonyms, stemming, simple paraphrase patterns and the syntactic structure of the n-grams. Yet another set of metrics are based instead on different principles: TER (Snover et al., 2006) and TERp (Snover et al., 2009) count the number of edits needed to transform a sentence into the other, MAXSIM (Chan and Ng, 2008) evaluates lexical similarity performing a word-by-word matching and fin</context>
</contexts>
<marker>Goutte, 2006</marker>
<rawString>Cyril Goutte. Automatic Evaluation of Machine Translation Quality. Presentation at the European Community, Xerox Research Centre Europe, on January, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling Sentences in the Latent Space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>864--872</pages>
<contexts>
<context position="15619" citStr="Guo and Diab (2012)" startWordPosition="2621" endWordPosition="2624">bserved in previous experiments where we employed it, which fails to analyze sentences containing certain patterns and predicates. The second reason, more specific to Twitter domain, is that some sentences lack a valid predicate or a proper grammatical structure. This prevents the semantic parser from giving an accurate output. The inclusion on latent semantic features in run 2 proved to be ineffective, as it improved subtask 1 F-score by less than 0.001, and gave a worse performance in subtask 2. During the evaluation phase other experiments were tried as using the latent semantic vectors of Guo and Diab (2012), or using the vectors as described in Ji and Eisenstein (2013) instead of the extra layer, and other modifications, all without obtaining any perceptible improvement when the system was tested on the development set. The non-perfect implementation and usage of these features, together with the fact they might not be suitable to be applied to Twitter domain, may explain this lack of improvement. 4 Conclusions We have used a neural network classifier, with a combination of multiple views of lexical, syntactic and semantic information, as the system which participated in SemEval 2015 task 1, who</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. Modeling Sentences in the Latent Space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 864–872, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ahmed Elkholy</author>
</authors>
<title>Sepia: Surface Span Extension to Syntactic Dependency Precision-based mt Evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the NIST metrics for machine translation workshop at the association for machine translation in the Americas conference, AMTA-2008.</booktitle>
<location>Waikiki, HI,</location>
<contexts>
<context position="7548" citStr="Habash and Elkholy, 2008" startWordPosition="1222" endWordPosition="1225">T score which is set to the value of the Jaccard coefficient between the whole sentences (Lo and Wu, 2013). 2.3 Translation Metrics Previous work (Finch et al., 2005; Madnani et al., 2012) have shown that machine translation evaluation metrics are useful for the paraphrase recognition task, due to their ability to capture useful similarity information to correctly classify the sentence pairs. The various translation metrics all take into account different aspects of sentence similarities. BLEU (Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics also consider synonyms, stemming, simple paraphrase patterns and the syntactic structure of the n-grams. Yet another set of metrics are based instead on different principles: TER (Snover et al., 2006) and TERp (Snover et al., 2009) count the number of edits needed to transform a sentence into the other, MAXSIM (Chan and Ng, 2008) evaluates lexical similarity performing a word-by-word matching and finding out how much the aligned words a</context>
</contexts>
<marker>Habash, Elkholy, 2008</marker>
<rawString>Nizar Habash and Ahmed Elkholy. Sepia: Surface Span Extension to Syntactic Dependency Precision-based mt Evaluation. In Proceedings of the NIST metrics for machine translation workshop at the association for machine translation in the Americas conference, AMTA-2008. Waikiki, HI, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical Normalisation of Short Text Messages: Makn Sens a #Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>368--378</pages>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. Lexical Normalisation of Short Text Messages: Makn Sens a #Twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 368–378, 2011.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kurt Hornik</author>
</authors>
<title>Maxwell Stinchcombe,</title>
<location>and Halbert</location>
<marker>Hornik, </marker>
<rawString>Kurt Hornik, Maxwell Stinchcombe, and Halbert</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
</authors>
<title>Anand Karthik Tumuluru, and Dekai Wu. Fully Automatic Semantic mt Evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>243--252</pages>
<marker>Lo, 2012</marker>
<rawString>Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. Fully Automatic Semantic mt Evaluation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 243–252, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining Machine Translation Metrics for Paraphrase Identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<contexts>
<context position="2395" citStr="Madnani et al. (2012)" startWordPosition="367" endWordPosition="371">hod to our knowledge on this corpus (Ji and Eisenstein, 2013) trains an SVM over latent semantic vectors, lexical and syntactic similarity features. Although their main objective was to show the effectiveness of a method based on latent semantic analysis, it is also evident that other features pertinent to different aspects of sentence similarity are able to boost the results. Previously Socher et al. (2011) used a recursive autoencoder to similarly obtain a vector representation of each sentence, again combining other lexical similarity features to improve the results. Other methods, such as Madnani et al. (2012) or Wan et al. (2006) used instead a more traditional supervised classification approach over different sets of features and different classifiers, most of which improved previous results. Task 1 of SemEval 2015 workshop required to evaluate paraphrases on a new corpus, consisting of sentences taken from Twitter posts (Xu et al., 2014). Twitter sentences notoriously differ from those taken from news articles: the 140 characters limit makes the sentences short, with few words, lots of different abbreviations; they also include many misspelled and invented words, and often lack a correct grammat</context>
<context position="7111" citStr="Madnani et al., 2012" startWordPosition="1158" endWordPosition="1161">nment without looking at the content: these include the percentage of semantic roles of one sentence that are also present in the other, the percentage of correct pairs of semantic roles after the alignment operated for MEANT, and a binary feature equal to 1 in case the semantic parser fails to give any output for at least one of the sentences. In this last case all the other features based on semantic roles are 0 except the MEANT score which is set to the value of the Jaccard coefficient between the whole sentences (Lo and Wu, 2013). 2.3 Translation Metrics Previous work (Finch et al., 2005; Madnani et al., 2012) have shown that machine translation evaluation metrics are useful for the paraphrase recognition task, due to their ability to capture useful similarity information to correctly classify the sentence pairs. The various translation metrics all take into account different aspects of sentence similarities. BLEU (Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics als</context>
<context position="8863" citStr="Madnani et al., 2012" startWordPosition="1437" endWordPosition="1440"> sentence obtained from the Burrows-Wheeler transform algorithm (Burrows and 24 Wheeler, 1994), and MEANT which, as discussed in the previous section, scores the similarity of aligned semantic frames. For each pair of sentences the scores are calculated first taking one of the sentences as the reference and the other as the sample and then vice-versa. Both scores are included as distinct features except in the case of BADGER, as it computes a distance between two objects without taking into account the direction. In case of BLEU and NIST we use the scores from unigrams up to 4-grams for BLEU (Madnani et al., 2012) and up to the maximum order which gives at least one result different than zero for NIST. 2.4 Classifier To classify the sentence pairs we design a feedforward neural network. One of the main properties of the neural network is its ability to learn complex functions of the input values (Hornik et al., 1989). It follows that in our task, given the combination of features, the network would learn how to combine them effectively and take advantage of their mutual interaction. The neural network can also be trained using an objective function that takes into consideration a label not just binary </context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. Re-examining Machine Translation Metrics for Paraphrase Identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="5246" citStr="Miller, 1995" startWordPosition="837" endWordPosition="838">resenting numbers of one sentences are the subset of the other (Socher et al., 2011). Two other features include the percentage of overlapping tokens, and the difference in sentence length. Another feature considers the word order: starting from one sentence we align the tokens that matches with the other sentence, and for each aligned pair we take the average of the differences of the absolute positions of the two elements, normalized by the length of the first sentence, and we do the same switching the order of the two sentences. Another group of features involves WordNet word synonym sets (Miller, 1995). We take from them, separately for nouns and verbs, the average of the path similarity scores obtained, among all word alignments, from the one which gives the maximum score. When the two words in the pair to be scored have multiple synonym sets we select the two sets that again are giving the highest score. Finally, in order to include an estimation of the level of similarity in the syntax parse tree of the sentences, we use the parse tree edit distance from the ZhangShasha algorithm (Zhang and Shasha, 1989; Wan et al., 2006). 2.2 Semantic Similarity Features The way we evaluate the semantic</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. Wordnet: A Lexical Database for English. Communications of the ACM, 38(11): 39–41, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="7445" citStr="Papineni et al., 2002" startWordPosition="1206" endWordPosition="1209">he sentences. In this last case all the other features based on semantic roles are 0 except the MEANT score which is set to the value of the Jaccard coefficient between the whole sentences (Lo and Wu, 2013). 2.3 Translation Metrics Previous work (Finch et al., 2005; Madnani et al., 2012) have shown that machine translation evaluation metrics are useful for the paraphrase recognition task, due to their ability to capture useful similarity information to correctly classify the sentence pairs. The various translation metrics all take into account different aspects of sentence similarities. BLEU (Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics also consider synonyms, stemming, simple paraphrase patterns and the syntactic structure of the n-grams. Yet another set of metrics are based instead on different principles: TER (Snover et al., 2006) and TERp (Snover et al., 2009) count the number of edits needed to transform a sentence into the other, MAXSIM (Chan and Ng, 2008) evalu</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Parker</author>
</authors>
<title>Badger: A New Machine Translation Metric. Metrics for Machine Translation Challenge,</title>
<date>2008</date>
<contexts>
<context position="8197" citStr="Parker, 2008" startWordPosition="1329" endWordPosition="1330"> source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics also consider synonyms, stemming, simple paraphrase patterns and the syntactic structure of the n-grams. Yet another set of metrics are based instead on different principles: TER (Snover et al., 2006) and TERp (Snover et al., 2009) count the number of edits needed to transform a sentence into the other, MAXSIM (Chan and Ng, 2008) evaluates lexical similarity performing a word-by-word matching and finding out how much the aligned words are similar in each meaning, BADGER (Parker, 2008) the distance between the compression of each sentence obtained from the Burrows-Wheeler transform algorithm (Burrows and 24 Wheeler, 1994), and MEANT which, as discussed in the previous section, scores the similarity of aligned semantic frames. For each pair of sentences the scores are calculated first taking one of the sentences as the reference and the other as the sample and then vice-versa. Both scores are included as distinct features except in the case of BADGER, as it computes a distance between two objects without taking into account the direction. In case of BLEU and NIST we use the </context>
</contexts>
<marker>Parker, 2008</marker>
<rawString>Steven Parker. Badger: A New Machine Translation Metric. Metrics for Machine Translation Challenge, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow Semantic Parsing using Support Vector Machines. In</title>
<date>2004</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>233--240</pages>
<contexts>
<context position="6239" citStr="Pradhan et al., 2004" startWordPosition="1006" endWordPosition="1009">milarity in the syntax parse tree of the sentences, we use the parse tree edit distance from the ZhangShasha algorithm (Zhang and Shasha, 1989; Wan et al., 2006). 2.2 Semantic Similarity Features The way we evaluate the semantic similarity of each pair of sentences is through the analysis of the semantic roles. The first feature we choose in this sense is the semantic role based MEANT machine translation score (Lo et al., 2012), effective to provide, as shown by various experiments, a translation evaluation closer to human judges. This metric first annotates each sentence with semantic roles (Pradhan et al., 2004), then aligns them and computes a similarity score only within the aligned frames (Fung et al., 2007) using the Jaccard coefficient (Tumuluru et al., 2012). Another set of features is obtained by looking at the semantic roles themselves and their alignment without looking at the content: these include the percentage of semantic roles of one sentence that are also present in the other, the percentage of correct pairs of semantic roles after the alignment operated for MEANT, and a binary feature equal to 1 in case the semantic parser fails to give any output for at least one of the sentences. In</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Daniel Jurafsky. Shallow Semantic Parsing using Support Vector Machines. In HLT-NAACL, pages 233–240, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of association for machine translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="7908" citStr="Snover et al., 2006" startWordPosition="1278" endWordPosition="1281">y classify the sentence pairs. The various translation metrics all take into account different aspects of sentence similarities. BLEU (Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics also consider synonyms, stemming, simple paraphrase patterns and the syntactic structure of the n-grams. Yet another set of metrics are based instead on different principles: TER (Snover et al., 2006) and TERp (Snover et al., 2009) count the number of edits needed to transform a sentence into the other, MAXSIM (Chan and Ng, 2008) evaluates lexical similarity performing a word-by-word matching and finding out how much the aligned words are similar in each meaning, BADGER (Parker, 2008) the distance between the compression of each sentence obtained from the Burrows-Wheeler transform algorithm (Burrows and 24 Wheeler, 1994), and MEANT which, as discussed in the previous section, scores the similarity of aligned semantic frames. For each pair of sentences the scores are calculated first taking</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of association for machine translation in the Americas, pages 223–231, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Ter-Plus: paraphrase, semantic, and alignment enhancements to Translation Edit Rate. Machine Translation,</title>
<date>2009</date>
<pages>23--2</pages>
<contexts>
<context position="7939" citStr="Snover et al., 2009" startWordPosition="1284" endWordPosition="1287">The various translation metrics all take into account different aspects of sentence similarities. BLEU (Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics also consider synonyms, stemming, simple paraphrase patterns and the syntactic structure of the n-grams. Yet another set of metrics are based instead on different principles: TER (Snover et al., 2006) and TERp (Snover et al., 2009) count the number of edits needed to transform a sentence into the other, MAXSIM (Chan and Ng, 2008) evaluates lexical similarity performing a word-by-word matching and finding out how much the aligned words are similar in each meaning, BADGER (Parker, 2008) the distance between the compression of each sentence obtained from the Burrows-Wheeler transform algorithm (Burrows and 24 Wheeler, 1994), and MEANT which, as discussed in the previous section, scores the similarity of aligned semantic frames. For each pair of sentences the scores are calculated first taking one of the sentences as the re</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. Ter-Plus: paraphrase, semantic, and alignment enhancements to Translation Edit Rate. Machine Translation, 23(2-3):117–127, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>801--809</pages>
<contexts>
<context position="2185" citStr="Socher et al. (2011)" startWordPosition="335" endWordPosition="338">, 2005), which consists of pairs of sentences taken out from news headlines and articles. News domain sentences are usually grammatically correct and of average to long length. The current state-of-the-art method to our knowledge on this corpus (Ji and Eisenstein, 2013) trains an SVM over latent semantic vectors, lexical and syntactic similarity features. Although their main objective was to show the effectiveness of a method based on latent semantic analysis, it is also evident that other features pertinent to different aspects of sentence similarity are able to boost the results. Previously Socher et al. (2011) used a recursive autoencoder to similarly obtain a vector representation of each sentence, again combining other lexical similarity features to improve the results. Other methods, such as Madnani et al. (2012) or Wan et al. (2006) used instead a more traditional supervised classification approach over different sets of features and different classifiers, most of which improved previous results. Task 1 of SemEval 2015 workshop required to evaluate paraphrases on a new corpus, consisting of sentences taken from Twitter posts (Xu et al., 2014). Twitter sentences notoriously differ from those tak</context>
<context position="4717" citStr="Socher et al., 2011" startWordPosition="747" endWordPosition="750">. The second goal was to make use of a classifier which can take full advantage of the six level labeling provided in order to have good performance in both subtasks, identified in an artificial neural network. 2.1 Lexical and Syntactic Similarity Features The first set of lexical features includes three binary indexes obtained from the analysis of the numerical tokens: the first of them is 1 if they are the same in both sentences or there are not any, the second is 1 only if they are the same, and the third is 1 if the tokens representing numbers of one sentences are the subset of the other (Socher et al., 2011). Two other features include the percentage of overlapping tokens, and the difference in sentence length. Another feature considers the word order: starting from one sentence we align the tokens that matches with the other sentence, and for each aligned pair we take the average of the differences of the absolute positions of the two elements, normalized by the length of the first sentence, and we do the same switching the order of the two sentences. Another group of features involves WordNet word synonym sets (Miller, 1995). We take from them, separately for nouns and verbs, the average of the</context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennin, Christopher D. Manning, and Andrew Y. Ng. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Advances in Neural Information Processing Systems, pages 801–809, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Karthik Tumuluru</author>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Accuracy and robustness in measuring the lexical similarity of semantic role fillers for automatic semantic mt evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation,</booktitle>
<contexts>
<context position="6394" citStr="Tumuluru et al., 2012" startWordPosition="1032" endWordPosition="1035">, 2006). 2.2 Semantic Similarity Features The way we evaluate the semantic similarity of each pair of sentences is through the analysis of the semantic roles. The first feature we choose in this sense is the semantic role based MEANT machine translation score (Lo et al., 2012), effective to provide, as shown by various experiments, a translation evaluation closer to human judges. This metric first annotates each sentence with semantic roles (Pradhan et al., 2004), then aligns them and computes a similarity score only within the aligned frames (Fung et al., 2007) using the Jaccard coefficient (Tumuluru et al., 2012). Another set of features is obtained by looking at the semantic roles themselves and their alignment without looking at the content: these include the percentage of semantic roles of one sentence that are also present in the other, the percentage of correct pairs of semantic roles after the alignment operated for MEANT, and a binary feature equal to 1 in case the semantic parser fails to give any output for at least one of the sentences. In this last case all the other features based on semantic roles are 0 except the MEANT score which is set to the value of the Jaccard coefficient between th</context>
</contexts>
<marker>Tumuluru, Lo, Wu, 2012</marker>
<rawString>Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu. Accuracy and robustness in measuring the lexical similarity of semantic role fillers for automatic semantic mt evaluation. In Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Using Dependency-Based Features to Take the Para-farce out of Paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<contexts>
<context position="2416" citStr="Wan et al. (2006)" startWordPosition="373" endWordPosition="376">his corpus (Ji and Eisenstein, 2013) trains an SVM over latent semantic vectors, lexical and syntactic similarity features. Although their main objective was to show the effectiveness of a method based on latent semantic analysis, it is also evident that other features pertinent to different aspects of sentence similarity are able to boost the results. Previously Socher et al. (2011) used a recursive autoencoder to similarly obtain a vector representation of each sentence, again combining other lexical similarity features to improve the results. Other methods, such as Madnani et al. (2012) or Wan et al. (2006) used instead a more traditional supervised classification approach over different sets of features and different classifiers, most of which improved previous results. Task 1 of SemEval 2015 workshop required to evaluate paraphrases on a new corpus, consisting of sentences taken from Twitter posts (Xu et al., 2014). Twitter sentences notoriously differ from those taken from news articles: the 140 characters limit makes the sentences short, with few words, lots of different abbreviations; they also include many misspelled and invented words, and often lack a correct grammatical structure. Anoth</context>
<context position="5779" citStr="Wan et al., 2006" startWordPosition="931" endWordPosition="934">ntences. Another group of features involves WordNet word synonym sets (Miller, 1995). We take from them, separately for nouns and verbs, the average of the path similarity scores obtained, among all word alignments, from the one which gives the maximum score. When the two words in the pair to be scored have multiple synonym sets we select the two sets that again are giving the highest score. Finally, in order to include an estimation of the level of similarity in the syntax parse tree of the sentences, we use the parse tree edit distance from the ZhangShasha algorithm (Zhang and Shasha, 1989; Wan et al., 2006). 2.2 Semantic Similarity Features The way we evaluate the semantic similarity of each pair of sentences is through the analysis of the semantic roles. The first feature we choose in this sense is the semantic role based MEANT machine translation score (Lo et al., 2012), effective to provide, as shown by various experiments, a translation evaluation closer to human judges. This metric first annotates each sentence with semantic roles (Pradhan et al., 2004), then aligns them and computes a similarity score only within the aligned frames (Fung et al., 2007) using the Jaccard coefficient (Tumulur</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. Using Dependency-Based Features to Take the Para-farce out of Paraphrase. In Proceedings of the Australasian Language Technology Workshop, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
<author>Yangfeng Ji</author>
</authors>
<title>Extracting Lexically Divergent Paraphrases from Twitter. Transactions Of The Association For Computational Linguistics,</title>
<date>2014</date>
<contexts>
<context position="2732" citStr="Xu et al., 2014" startWordPosition="420" endWordPosition="423">larity are able to boost the results. Previously Socher et al. (2011) used a recursive autoencoder to similarly obtain a vector representation of each sentence, again combining other lexical similarity features to improve the results. Other methods, such as Madnani et al. (2012) or Wan et al. (2006) used instead a more traditional supervised classification approach over different sets of features and different classifiers, most of which improved previous results. Task 1 of SemEval 2015 workshop required to evaluate paraphrases on a new corpus, consisting of sentences taken from Twitter posts (Xu et al., 2014). Twitter sentences notoriously differ from those taken from news articles: the 140 characters limit makes the sentences short, with few words, lots of different abbreviations; they also include many misspelled and invented words, and often lack a correct grammatical structure. Another important difference is the sixlevel classification labels provided, compared to the binary labels of MSRP corpus, which allows a finegrained evaluation of the similarity level between the sentences. The task was divided into two subtasks. Subtask 1 was the classical binary paraphrase classification task, where </context>
<context position="11097" citStr="Xu et al., 2014" startWordPosition="1845" endWordPosition="1848">set at zero, are initialized (Glorot and Bengio, 2010) with uniform values in the range: � � 6 �1 � 6 �1 � wt=0 ∈ −α , α (1) nin + nout nin + nout Where α = 1 in case the activation function is the hyperbolic tangent, and α = 4 with the sigmoid. We train the model using standard backpropagation algorithm, taking the cross-entropy as the cost objective function: E = −l log(y) − (1 − l) log(1 − y) + R (2) where y is the network output, l the objective value (both in the range [0,1]), and R is an L2 regularization term. 3 Experiments 3.1 Corpus We made use of the corpus provided for the contest (Xu et al., 2014), made of a training set of 13063 sentence pairs, a development set of 4727 pairs, and a test set of 972 pairs released a few days before the deadline without the labels. Each pair of sentences was labeled by five users via Amazon Mechanical Turk, hence providing a six-level classification label (from (5, 0) when all the five user classify the pair as a paraphrase, to (0, 5) when none of them identifies the pair to be a paraphrase). 3.2 Experimental Setup The neural network was setup with a hidden layer dimension of three times the input. The development set was used to tune the L2 regularizat</context>
</contexts>
<marker>Xu, Ritter, Callison-Burch, Dolan, Ji, 2014</marker>
<rawString>Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan, and Yangfeng Ji. Extracting Lexically Divergent Paraphrases from Twitter. Transactions Of The Association For Computational Linguistics, 2:435–448, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
</authors>
<title>SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT).</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval),</booktitle>
<contexts>
<context position="1449" citStr="Xu et al., 2015" startWordPosition="219" endWordPosition="222">s provided in the corpus, in order to give as output a more fine-grained estimation of the similarity level of the two sentences, as required by subtask 2. With an F-score of 0.651 in the binary paraphrase classification subtask 1, and a Pearson coefficient of 0.697 for the sentence similarity subtask 2, we achieved respectively the 6th place and the 3rd place, above the average of what obtained by the other contestants. 1 Introduction Paraphrase identification is the problem to determine whether two sentences have the same meaning, and is the objective of the task 1 of SemEval 2015 workshop (Xu et al., 2015). Conventionally this task has been mainly evaluated on the Microsoft Research Paraphrase corpus (Dolan and Brockett, 2005), which consists of pairs of sentences taken out from news headlines and articles. News domain sentences are usually grammatically correct and of average to long length. The current state-of-the-art method to our knowledge on this corpus (Ji and Eisenstein, 2013) trains an SVM over latent semantic vectors, lexical and syntactic similarity features. Although their main objective was to show the effectiveness of a method based on latent semantic analysis, it is also evident </context>
</contexts>
<marker>Xu, Callison-Burch, Dolan, 2015</marker>
<rawString>Wei Xu, Chris Callison-Burch, and William B. Dolan. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval), 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaizhong Zhang</author>
<author>Dennis Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>SIAM journal on computing,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="5760" citStr="Zhang and Shasha, 1989" startWordPosition="927" endWordPosition="930"> the order of the two sentences. Another group of features involves WordNet word synonym sets (Miller, 1995). We take from them, separately for nouns and verbs, the average of the path similarity scores obtained, among all word alignments, from the one which gives the maximum score. When the two words in the pair to be scored have multiple synonym sets we select the two sets that again are giving the highest score. Finally, in order to include an estimation of the level of similarity in the syntax parse tree of the sentences, we use the parse tree edit distance from the ZhangShasha algorithm (Zhang and Shasha, 1989; Wan et al., 2006). 2.2 Semantic Similarity Features The way we evaluate the semantic similarity of each pair of sentences is through the analysis of the semantic roles. The first feature we choose in this sense is the semantic role based MEANT machine translation score (Lo et al., 2012), effective to provide, as shown by various experiments, a translation evaluation closer to human judges. This metric first annotates each sentence with semantic roles (Pradhan et al., 2004), then aligns them and computes a similarity score only within the aligned frames (Fung et al., 2007) using the Jaccard c</context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>Kaizhong Zhang and Dennis Shasha. Simple fast algorithms for the editing distance between trees and related problems. SIAM journal on computing, 18(6):1245–1262, 1989.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>