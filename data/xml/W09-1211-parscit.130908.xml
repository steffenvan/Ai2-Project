<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001290">
<title confidence="0.996776">
Exploring Multilingual Semantic Role Labeling
</title>
<author confidence="0.999578">
Baoli Li, Martin Emms, Saturnino Luz, Carl Vogel
</author>
<affiliation confidence="0.9823865">
Department of Computer Science
Trinity College Dublin
</affiliation>
<address confidence="0.807381">
Dublin 2, Ireland
</address>
<email confidence="0.995364">
{baoli.li,mtemms,luzs,vogel}@cs.tcd.ie
</email>
<sectionHeader confidence="0.993795" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999154263157895">
This paper describes the multilingual semantic
role labeling system of Computational Lin-
guistics Group, Trinity College Dublin, for the
CoNLL-2009 SRLonly closed shared task.
The system consists of two cascaded compo-
nents: one for disambiguating predicate word
sense, and the other for identifying and classi-
fying arguments. Supervised learning tech-
niques are utilized in these two components.
As each language has its unique characteris-
tics, different parameters and strategies have
to be taken for different languages, either for
providing functions required by a language or
for meeting the tight deadline. The system ob-
tained labeled F1 69.26 averaging over seven
languages (Catalan, Chinese, Czech, English,
German, Japanese, and Spanish), which ranks
the system fourth among the seven systems
participating the SRLonly closed track.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998322375">
Semantic role labeling, which aims at computa-
tionally identifying and labeling arguments of
predicate words, has become a leading research
problem in computational linguistics with the ad-
vent of various supporting resources (e.g. corpora
and lexicons) (Màrquez et al., 2008). Word seman-
tic dependencies derived by semantic role labeling
are assumed to facilitate automated interpretation
of natural language texts. Moreover, techniques for
automatic annotation of semantic dependencies can
also play an important role in adding metadata to
corpora for the purposes of machine translation
and speech processing. We are currently investi-
gating such techniques as part of our research into
integrated language technology in the Center for
Next Generation Localization (CNGL,
</bodyText>
<page confidence="0.985946">
73
</page>
<bodyText confidence="0.999819457142857">
http://www.cngl.ie). The multilingual nature of the
CoNLL-2009 shared task on syntactic and seman-
tic dependency analysis, which includes Catalan,
Chinese, Czech, English, German, Japanese, and
Spanish (Hajic et al., 2009), makes it a good test-
bed for our research.
We decided to participate in the CoNLL-2009
shared task at the beginning of March, signed the
agreement for getting the training data on March
2nd, 2009, and obtained all the training data (espe-
cially the part from LDC) on March 4th, 2009. Due
to the tight time constraints of the task, we chose to
use existing packages to implement our system.
These time constraints also meant that we had to
resort to less computationally intensive methods to
meet the deadline, especially for some large data-
sets (such as the Czech data). In spite of these dif-
ficulties and resource limitations, we are proud to
be among the 21 teams who successfully submitted
the results1.
As a new participant, our goals in attending the
CoNLL-2009 SRLonly shared task were to gain
more thorough knowledge of this line of research
and its state-of-the-art, and to explore how well a
system quickly assembled with existing packages
can fare at this hard semantic analysis problem.
Following the successful approaches taken by
the participants of the CoNLL-2008 shared task
(Surdeanu et al., 2008) on monolingual syntactic
and semantic dependency analysis, we designed
and implemented our CoNLL-2009 SRLonly sys-
tem with pipeline architecture. Two main compo-
nents are cascaded in this system: one is for
disambiguating predicate word sense 2 , and the
other for identifying and classifying arguments for
</bodyText>
<footnote confidence="0.991071">
1 According to our correspondence with Dr. Jan Hajic, totally
31 teams among 60 registered ones signed and got the evalua-
tion data.
2 As predicate words are marked in the CoNLL-2009 datasets,
we don’t need to identify predicate words.
</footnote>
<note confidence="0.886768">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 73–78,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999941346153846">
predicate words. Different supervised learning
techniques are utilized in these two components.
For predicate word sense disambiguation (WSD),
we have experimented with three algorithms: SVM,
kNN, and Naïve Bayes. Based on experimental
results on the development datasets, we chose
SVM and kNN to produce our submitted official
results. For argument identification and classifica-
tion, we used a maximum entropy classifier for all
the seven datasets. As each language has its unique
characteristics and peculiarities within the dataset,
different parameters and strategies have to be taken
for different languages (as detailed below), either
for providing functions required by a language or
for meeting the tight deadline. Our official submis-
sion obtained 69.26 labeled F1 averaging over the
seven languages, which ranks our system fourth
among the seven systems in the SRLonly closed
track.
The rest of this paper is organized as follows.
Section 2 discusses the first component of our sys-
tem for predicate word sense disambiguation. Sec-
tion 3 explains how our system detects and
classifies arguments with respect to a predicate
word. We present experiments in Section 4, and
conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.921692" genericHeader="method">
2 Predicate Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.999925125">
This component tries to determine the sense of a
predicate word in a specific context. As a sense of
a predicate word is often associated with a unique
set of possible semantic roles, this task is also
called role set determination. Based on the charac-
teristics of different languages, we take different
strategies in this step, but the same feature set is
used for different languages.
</bodyText>
<subsectionHeader confidence="0.974232">
2.1 Methods
</subsectionHeader>
<bodyText confidence="0.999942142857143">
Intuitively, each predicate word should be treated
individually according to the list of its possible
senses. We therefore designed an initial solution
based on the traditional methods in WSD: repre-
sent each sense as a vector from its definition or
examples; describe the predicate word for disam-
biguation as a vector derived from its context; and
finally output the sense which has the highest simi-
larity with the current context. We also considered
using singular value decomposition (SVD) to over-
come the data sparseness problem. Unfortunately,
we found this solution didn’t work well in our pre-
liminary experiments. The main problem is that the
definition of each sense of a predicate word is not
available. What we have is just a few example con-
texts for one sense of a predicate word, and these
contexts are often not informative enough for
WSD. On the other hand, our limited computing
resources could not afford SVD operation on a
huge matrix.
We finally decided to take each sense tag as a
class tag across different words and transform the
disambiguation problem into a normal multi-class
categorization problem. For example, in the Eng-
lish datasets, all predicates with “01” as a sense
identifier were counted as examples for the class
“01”. With this setting, a predicate word may be
assigned an invalid sense tag. It is an indirect solu-
tion, but works well. We think there are at least
two possible reasons: firstly, most predicate words
take their popular sense in running text. For exam-
ple, in the English dataset (training and develop-
ment), 160,477 of 185,406 predicate occurrences
(about 86.55%) take their default sense “01”. Sec-
ondly, predicates may share some common role
sets, even though their senses may not be exactly
the same, e.g. “tell” and “inform”.
Unlike the datasets in other languages, the Japa-
nese dataset doesn’t have specialized sense tags
annotated for each predicate word, so we simply
copy the predicted lemma of a predicate word to its
PRED field. For other datasets, we derived a train-
ing sample for each predicate word, whose class
tag is its sense tag. Then we trained a model from
the generated training data with a supervised learn-
ing algorithm, and applied the learned model for
predicting the sense of a predicate word. This is
our base solution.
When transforming the datasets, the Czech data
needs some special processing because of its
unique annotation format. The sense annotation for
a predicate word in the Czech data does not take
the form “LEMMA.SENSE”. In most cases, no
specialized sense tags are annotated. The PRED
field of these words only contains “LEMMA”. In
other cases, the disambiguated senses are anno-
tated with an internal representation, which is
given in a predicate word lexicon. We decomposed
the internal representation of each predicate word
into two parts: word index id and sense tag. For
example, from “zvýšení v-w10004f2” we know “v-
w10004” is the index id of word “zvýšení”, and
“f2” is its sense tag. We then use these derived
</bodyText>
<page confidence="0.991953">
74
</page>
<bodyText confidence="0.999882097560976">
sense tags as class tags and add a class tag “=” for
samples without specialized sense tag.
For each predicate word, we derive a vector de-
scribing its context and attributes, each dimension
of which corresponds to a feature. We list the fea-
ture types in the next subsection. Features appear-
ing only once are removed. The TF*IDF weighting
schema is used to calculate the weight of a feature.
Three different algorithms were tried during the
development period: support vector machines
(SVM), distance-weighted k-Nearest Neighbor
(kNN) (Li et al., 2004), and Naïve Bayes with mul-
tinomial model (Mccallum and Nigam, 1998). As
to the SVM algorithm, we used the robust
LIBSVM package (Chang and Lin, 2001), with a
linear kernel and default values for other parame-
ters. The algorithms achieving the best results in
our preliminary experiments are chosen for differ-
ent languages: SVM for Catalan, Chinese, and
Spanish; kNN for German (k=20).
We used kNN for English (k=20) and Czech
(k=10) because we could not finish training with
SVM on these two datasets in limited time. Even
with kNN algorithm, we still had trouble with the
English and Czech datasets, because thousands of
training samples make the prediction for the
evaluation data unacceptably slow. We therefore
had to further constrain the search space for a new
predicate word to those samples containing the
same predicate word. If there are not samples con-
taining the same predicate word in the training data,
we will assign it the most popular sense tag (e.g.
“01” for English).
How to use the provided predicate lexicons is a
challenging issue. Lexicons for different languages
take different formats and the information included
in different lexicons is quite different. We derived
a sense list lexicon from the original predicate
lexicon for Chinese, Czech, English, and German.
Each entry in a sense list lexicon contains a predi-
cate word, its internal representation (especially for
Czech), and a list of sense tags that the predicate
can have. Then we obtained a variant of our base
solution, which uses the sense list of a predicate
word to filter impossible senses. It works as fol-
lows:
- Disambiguate a new predicate with the base
solution;
- Choose the most possible sense from all the
candidate senses obtained in step 1: if the
base classifier doesn’t output a vector of
probabilities for classes, only check
whether the predicted one is a valid sense
for the predicate;
- If there is not a valid sense for a new predi-
cate (including the cases where the predi-
cate does not have an entry in the sense list
lexicon), output the most popular sense tag;
Unfortunately, preliminary experiments on the
German and Chinese datasets didn’t support to in-
clude such a post-processing stage. The perform-
ance with this filtering became a little worse.
Therefore, we decided not to use it generally, but
one exception is for the Czech data.
With kNN algorithm, we can greatly reduce the
time for training the Czech data, but we do have
problem with prediction, as there are totally
469,754 samples in the training dataset. It’s a time-
consuming task to calculate the similarities be-
tween a new sample and all the samples in the
training dataset to find its k nearest neighbors, thus
we have to limit the search space to those samples
that contain the predicate word for disambiguation.
To process unseen predicate words, we used the
derived sense list lexicon: if a predicate word for
disambiguation is out of the sense list lexicon, we
simply copy its predicted lemma to the PRED field;
if no sample in the training dataset has the same
predicate word, we take its first possible sense in
the sense list lexicon. With this strategy, our sys-
tem can process the huge Czech dataset in short
time.
</bodyText>
<subsectionHeader confidence="0.919768">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.998319">
The features we used in this step include3:
</bodyText>
<listItem confidence="0.935011285714286">
a. [Lemma  |(Lemma with POS)] of all words in the sen-
tence;
b. Attributes of predicate word, which is obtained from
PFEAT field by splitting the field at symbol “|” and
removing the invalid attribute of “*”;
c. [Lemma  |POS] bi-grams of predicate word and its
[previous  |following] one word;
d. [Lemma  |POS] tri-grams of predicate word and its
[previous  |following] two words;
e. [Lemma  |(Lemma with POS)] of its most [left  |right]
child;
f. [(Lemma+Dependency_Relation+Lemma)  |(POS
+Dependency_Relation+POS)] of predicate word and
its most [left  |right] child;
</listItem>
<footnote confidence="0.944940333333333">
3 We referred to those CoNLL-2008 participants’ reports, e.g.
(Ciaramita et al., 2008), when we designed the feature sets for
the two components.
</footnote>
<page confidence="0.995186">
75
</page>
<listItem confidence="0.901230230769231">
g. [Lemma  |(Lemma with POS)] of the head of the pre-
dicate word;
h. [(Lemma+Dependency_Relation+Lemma)  |(POS+D-
ependency_Relation+POS)] of predicate word and its
head;
i. [Lemma  |(Lemma with POS)] of its [previous  |fol-
lowing] two brothers;
j. [Lemma  |POS  |(Dependency relation)] bi-gram of
predicate word and its [previous  |following] one
brother;
k. [Lemma  |POS  |(Dependency relation)] tri-gram of
predicate word and its [previous  |following] two
brothers.
</listItem>
<sectionHeader confidence="0.987307" genericHeader="method">
3 Argument Identification and Classifica-
tion
</sectionHeader>
<bodyText confidence="0.9999214">
The second component of our system is used to
detect and classify arguments with respect to a
predicate word. We take a joint solution rather than
solve the problem in two consecutive steps: argu-
ment identification and argument classification.
</bodyText>
<subsectionHeader confidence="0.984217">
3.1 Methods
</subsectionHeader>
<bodyText confidence="0.960644127272728">
By introducing an additional argument type tag “_”
for non-arguments, we transformed the two tasks
(i.e. argument identification and argument classifi-
cation) into one multi-class classification problem.
As a word can play different roles with respect to
different predicate words and a predicate word can
be an argument of itself, we generate a training set
by deriving a training example from each word-
predicate pair. For example, if a sentence with two
predicates has 7 words, we will derive 7*2=14
training examples. Therefore, the number of train-
ing examples generated in this step will be around
L times larger than that obtained in the previous
step, where L is the average length of sentences.
We chose to use maximum entropy algorithm in
this step because of its success in the CoNLL-2008
shared task (Surdeanu et al., 2008). Le Zhang’s
maximum entropy package (Zhang, 2006) is inte-
grated in our system.
The Czech data cause much trouble again for us,
as the training data derived by the above strategy
became even larger. We had to use a special strat-
egy for the Czech data: we selectively chose word-
predicate pairs for generating the training dataset.
In other words, not all possible combinations are
used. We chose the following words with respect
to each predicate: the first and the last two words
of a sentence; the words between the predicate and
any argument of it; two words before the predicate
or any argument; and two words after the predicate
or any argument.
In the Czech and Japanese data, some words
may play multiple roles with respect to a predicate
word. We thus have to consider multi-label classi-
fication problem (Tsoumakas and Katakis, 2007)
for these two languages’ data. We tried the follow-
ing two solutions:
• Take each role type combination as a class
and transform the multi-label problem to a
single-label classification problem;
• Classify a word with a set of binary classi-
fiers: consider each role type individually
with a binary classifier; any possible role
type will be output; if no role type is ob-
tained after considering all the role types,
the role type with the highest confidence
value will be output; and, if “_” is output
with any other role type, remove it.
We used the second solution in our official
submission, but we finally found these two solu-
tions perform almost the same. The performance
difference is very small. We found the cases with
multi-labels (actually at most two) in the training
data are very limited: 690 of 414,326 in the Czech
data and 113 of 46,663 in the Japanese data.
</bodyText>
<subsectionHeader confidence="0.977304">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.998418">
The features we used in this step include:
</bodyText>
<listItem confidence="0.816920822222222">
a. Whether the current word is a predicate;
b. [Lemma  |POS] of current word and its [previous  |fol-
lowing] one word;
c. [Lemma  |POS] bi-grams of current word and its [pre-
vious  |following] one word;
d. POS tri-grams of current word, its previous word and
its following word;
e. Dependency relation of current word to its head;
f. [Lemma  |POS] of the head of current word;
g. [Lemma  |POS] bi-grams of current word and its head;
h. [(Lemma+Dependency_Relation+Lemma)  |(POS+De
pendency_Relation+POS)] of current word and its
head;
i. [Lemma  |POS] of its most [left  |right] child;
j. [Lemma  |POS] bi-grams of current word and its most
[left  |right] child;
k. [(Lemma+Dependency_Relation+Lemma)  |(POS+De
pendency_Relation+POS) of current word and its
most [left  |right] child;
l. The number of children of the current word and the
predicate word;
m. Attributes of the current word, which is obtained from
PFEAT field by splitting the field at symbol “|” and
removing the invalid attribute of “*”;
n. The sense tag of the predicate word;
76
o. [Lemma  |POS] of the predicate word and its head;
p. Dependency relation of the predicate word to its head;
q. [Lemma  |POS] bi-grams of the predicate word and its
head;
r. [(Lemma+Dependency_Relation+Lemma)  |(POS+De
pendency_Relation+POS)] of the predicate word and
its head;
s. [Lemma  |POS] of the most [left  |right] child of the
predicate word;
t. [(Lemma+Dependency_Relation+Lemma)  |(POS+De
pendency_Relation+POS)] of predicate word and its
head;
u. [Lemma  |POS] bi-gram of the predicate word and its
most [left  |right] child;
v. [(Lemma+Dependency_Relation+Lemma)  |(POS+De
pendency_Relation+POS)] of the predicate word and
its most [left  |right] child;
w. The relative position of the current word to the predi-
cate one: before, after, or on;
</listItem>
<bodyText confidence="0.897744">
x. The distance of the current word to the predicate one;
y. The relative level (up, down, or same) and level dif-
ference on the syntactic dependency tree of the current
word to the predicate one;
z. The length of the shortest path between the current
word and the predicate word.
</bodyText>
<sectionHeader confidence="0.998332" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.970426">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9996927">
The datasets of the CoNLL-2009 shared task con-
tain seven languages: Catalan (CA), Chinese (CN),
Czech (CZ), English (EG), German (GE), Japanese
(JP), and Spanish (SP). The training and evaluation
data of each language (Taulé et al., 2008; Xue et
al., 2008; Hajic et al., 2006; Palmer et al., 2002;
Burchardt et al., 2006; Kawahara et al., 2002) have
been converted to a uniform CoNLL Shared Task
format. Each participating team is required to
process all seven language datasets.
</bodyText>
<table confidence="0.999622545454546">
Lanuage CA CN CZ EN GE JP SP
Size (KB) 48974 41340 94284 58155 41091 8948 52430
# of Sen- 14924 24039 43955 40613 38020 4643 15984
tences
# of Predi- 42536 110916 469754 185404 17988 27251 48900
cate words
Avg. # of 2.85 4.61 10.69 4.57 0.47 5.87 3.06
Predicates
per sentence
popular a2 01 = 01 1 = a2
sense tag (37%) (90%) (81%) (87%) (75%) (100%) (39%)
</table>
<tableCaption confidence="0.927423">
Table 1. Statistical information of the seven language
datasets (training and development).
</tableCaption>
<bodyText confidence="0.962743571428571">
Table 1 shows some statistical information of
both training and development data for each lan-
guage. The total size of the uncompressed original
data without lexicons is about 345MB. The Czech
dataset is the largest one containing 43,955 sen-
tences and 469,754 predicate words, while the
Japanese dataset the smallest one. On average,
10.69 predicate words appear in a Czech sentence,
while only 0.47 predicate words exist in a German
sentence. The most popular sense tag in the Czech
datasets is “=”, which means the PRED field has
the same value as the PLEMMA field or the
FORM field. About 81% of Czech predicate words
take this value.
</bodyText>
<subsectionHeader confidence="0.974599">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9797782">
F1 is used as the main evaluation metric in the
CoNLL-2009 shared task. As to the SRLonly track,
a joint semantic labeled F1, which considers predi-
cate word sense disambiguation and argument la-
beling equally, is used to rank systems.
</bodyText>
<table confidence="0.9651745">
Avg. CA CN CZ EG GE JP SP
69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
</table>
<tableCaption confidence="0.999225">
Table 2. Official results of our system.
</tableCaption>
<bodyText confidence="0.967969571428571">
Table 2 gives the official results of our system
on the evaluation data. The system obtained the
best result (74.06) on the Catalan data, but per-
formed very poor (57.46) on the Czech data. Ex-
cept the Czech data, our system performs quite
stable on the other six language data with mean of
71.23 and standard deviation of 2.42.
</bodyText>
<table confidence="0.999444866666667">
Avg. CA CN CZ EG GE JP SP
Over- 69.47 74.12 70.52 57.57 70.24 67.97 72.17 73.68
all F1
Pred. 86.9 84.42 94.54 72.23 92.98 81.09 99.07 83.96
WSD
F1
Arg 57.24 69.29 57.71 33.19 58.25 60.64 52.72 68.86
I&amp;C
F1
Arg 69.77 73.43 72.48 62.14 70.14 66.63 69.37 74.23
I&amp;C
PR
Arg 49.77 65.6 47.94 22.64 49.81 55.64 42.52 64.21
I&amp;C
RE
</table>
<tableCaption confidence="0.999584">
Table 3. Results of our system after fixing a minor bug.
</tableCaption>
<bodyText confidence="0.999171916666667">
After submitting the official results, we found
and fixed a minor bug in the implementation of the
second component. Table 3 presents the results of
our system after fixing this bug. The overall per-
formance doesn’t change much. We further ana-
lyzed the bottlenecks by checking the performance
of different components.
At the predicate WSD part, our system works
reasonable with labeled F1 86.9, but the perform-
ance on the Czech data is lower than that of a base-
line system that constantly chooses the most
popular sense tag. If we use this baseline solution,
</bodyText>
<page confidence="0.995">
77
</page>
<bodyText confidence="0.999906375">
we can get predicate WSD F1 78.66, which further
increases the overall labeled F1 on the Czech data
to 61.68 from 57.57 and the overall labeled F1
over the seven languages to 70.05 from 69.47.
From table 3, we can see our system performs
relatively poorly for argument identification and
classification (57.24 vs. 86.9). The system seems
too conservative for argument identification, which
makes the recall very lower. We explored some
strategies for improving the performance of the
second component, e.g. separating argument iden-
tification and argument classification, and using
feature selection (with DF threshold) techniques,
but none of them helps much. We are thinking the
features currently used may not be effective
enough, which deserves further study.
</bodyText>
<sectionHeader confidence="0.993992" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999848363636364">
In this paper, we describe our system for the
CoNLL-2009 shared task -- SRLonly closed track.
Our system was built on existing packages with a
pipeline architecture, which integrated two cas-
caded components: predicate word sense disam-
biguation and argument identification and
classification. Our system performs well at disam-
biguating the sense of predicate words, but poorly
at identifying and classifying arguments. In the
future, we plan to explore much effective features
for argument identification and classification.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998560625">
This research was funded by Science Foundation
Ireland under the CNGL grant. We used the IITAC
Cluster in our initial experiments. We thank IITAC,
the HEA, the National Development Plan and the
Trinity Centre for High Performance Computing
for their support. We are also obliged to John
Keeney for helping us running our system on the
CNGL servers.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852092307692">
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Padó and Manfred Pinkal. 2006.
The SALSA Corpus: a German Corpus Resource for
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2006). Genoa, Italy.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM:
a library for support vector machines. Software
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
Massimiliano Ciaramita, Giuseppe Attardi, Felice
Dell’Orletta, and Mihai Surdeanu. 2008. DeSRL: A
Linear-Time Semantic Role Labeling System. Pro-
ceedings of the CoNLL-2008.
Jan Haji6, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria Antonia Martí, Lluís
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Št6pánek, Pavel Stranák, Mihai Surdeanu,
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009
Shared Task: Syntactic and Semantic Dependencies
in Multiple Languages. Proceedings of the 13th
Conference on Computational Natural Language
Learning (CoNLL-2009). Boulder, Colorado, USA.
Jan Haji6, Jarmila Panevová, Eva Haji6ová, Petr Sgall,
Petr Pajas, Jan Št6pánek, Jiri Havelka, Marie
Mikulová and Zden6k Žabokrtský. 2006. The Prague
Dependency Treebank 2.0. Linguistic Data
Consortium, USA. ISBN 1-58563-370-4.
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida.
2002. Construction of a Japanese Relevance-tagged
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain.
Baoli Li, Qin Lu and Shiwen Yu. 2004. An Adaptive k-
Nearest Neighbor Text Categorization Strategy. ACM
Transactions on Asian Language Information
Processing, 3(4): 215-226.
Lluís Màrquez, Xavier Carreras, Kenneth C. Litkowski
and Suzanne Stevenson. 2008. Semantic Role Label-
ing: An Introduction to the Special Issue. Computa-
tional Linguistics, 34(2):145-159.
Andrew Mccallum and Kamal Nigam. 1998. A Com-
parison of Event Models for Naive Bayes Text Clas-
sification. Proceedings of AAAI/ICML-98 Workshop
on Learning for Text Categorization.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis Marquez and Joakim Nivre. 2008. The CoNLL-
2008 Shared Task on Joint Parsing of Syntactic and
Semantic Dependencies. Proceedings of the 12th
Conference on Computational Natural Language
Learning (CoNLL-2008).
Mariona Taulé, Maria Antònia Martí and Marta
Recasens. 2008. AnCora: Multilevel Annotated
Corpora for Catalan and Spanish. Proceedings of the
6th International Conference on Language Resources
and Evaluation (LREC-2008). Marrakech, Morocco.
Grigorios Tsoumakas and Ioannis Katakis. 2007. Multi-
Label Classification: An Overview. International
Journal of Data Warehousing and Mining, 3(3):1-13.
Nianwen Xue and Martha Palmer. 2009. Adding
semantic roles to the Chinese Treebank. Natural
Language Engineering, 15(1):143-172.
Le Zhang. 2006. Maximum Entropy Modeling Toolkit
for Python and C++. Software available at
http://homepages.inf.ed.ac.uk/s0450736/maxent_tool
kit.html.
</reference>
<page confidence="0.998826">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.515682">
<title confidence="0.99992">Exploring Multilingual Semantic Role Labeling</title>
<author confidence="0.997778">Baoli Li</author>
<author confidence="0.997778">Martin Emms</author>
<author confidence="0.997778">Saturnino Luz</author>
<author confidence="0.997778">Carl</author>
<affiliation confidence="0.9976205">Department of Computer Trinity College</affiliation>
<address confidence="0.531193">Dublin 2,</address>
<email confidence="0.970692">baoli.li@cs.tcd.ie</email>
<email confidence="0.970692">mtemms@cs.tcd.ie</email>
<email confidence="0.970692">luzs@cs.tcd.ie</email>
<email confidence="0.970692">vogel@cs.tcd.ie</email>
<abstract confidence="0.9993959">This paper describes the multilingual semantic role labeling system of Computational Linguistics Group, Trinity College Dublin, for the CoNLL-2009 SRLonly closed shared task. The system consists of two cascaded components: one for disambiguating predicate word sense, and the other for identifying and classifying arguments. Supervised learning techniques are utilized in these two components. As each language has its unique characteristics, different parameters and strategies have to be taken for different languages, either for providing functions required by a language or for meeting the tight deadline. The system obtained labeled F1 69.26 averaging over seven languages (Catalan, Chinese, Czech, English, German, Japanese, and Spanish), which ranks the system fourth among the seven systems participating the SRLonly closed track.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Padó</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA Corpus: a German Corpus Resource for Lexical Semantics.</title>
<date>2006</date>
<booktitle>Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006).</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="18845" citStr="Burchardt et al., 2006" startWordPosition="3046" endWordPosition="3049">e distance of the current word to the predicate one; y. The relative level (up, down, or same) and level difference on the syntactic dependency tree of the current word to the predicate one; z. The length of the shortest path between the current word and the predicate word. 4 Experiments 4.1 Datasets The datasets of the CoNLL-2009 shared task contain seven languages: Catalan (CA), Chinese (CN), Czech (CZ), English (EG), German (GE), Japanese (JP), and Spanish (SP). The training and evaluation data of each language (Taulé et al., 2008; Xue et al., 2008; Hajic et al., 2006; Palmer et al., 2002; Burchardt et al., 2006; Kawahara et al., 2002) have been converted to a uniform CoNLL Shared Task format. Each participating team is required to process all seven language datasets. Lanuage CA CN CZ EN GE JP SP Size (KB) 48974 41340 94284 58155 41091 8948 52430 # of Sen- 14924 24039 43955 40613 38020 4643 15984 tences # of Predi- 42536 110916 469754 185404 17988 27251 48900 cate words Avg. # of 2.85 4.61 10.69 4.57 0.47 5.87 3.06 Predicates per sentence popular a2 01 = 01 1 = a2 sense tag (37%) (90%) (81%) (87%) (75%) (100%) (39%) Table 1. Statistical information of the seven language datasets (training and develop</context>
</contexts>
<marker>Burchardt, Erk, Frank, Kowalski, Padó, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Padó and Manfred Pinkal. 2006. The SALSA Corpus: a German Corpus Resource for Lexical Semantics. Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="9262" citStr="Chang and Lin, 2001" startWordPosition="1462" endWordPosition="1465"> sense tag. For each predicate word, we derive a vector describing its context and attributes, each dimension of which corresponds to a feature. We list the feature types in the next subsection. Features appearing only once are removed. The TF*IDF weighting schema is used to calculate the weight of a feature. Three different algorithms were tried during the development period: support vector machines (SVM), distance-weighted k-Nearest Neighbor (kNN) (Li et al., 2004), and Naïve Bayes with multinomial model (Mccallum and Nigam, 1998). As to the SVM algorithm, we used the robust LIBSVM package (Chang and Lin, 2001), with a linear kernel and default values for other parameters. The algorithms achieving the best results in our preliminary experiments are chosen for different languages: SVM for Catalan, Chinese, and Spanish; kNN for German (k=20). We used kNN for English (k=20) and Czech (k=10) because we could not finish training with SVM on these two datasets in limited time. Even with kNN algorithm, we still had trouble with the English and Czech datasets, because thousands of training samples make the prediction for the evaluation data unacceptably slow. We therefore had to further constrain the search</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Giuseppe Attardi</author>
<author>Felice Dell’Orletta</author>
<author>Mihai Surdeanu</author>
</authors>
<title>DeSRL: A Linear-Time Semantic Role Labeling System.</title>
<date>2008</date>
<booktitle>Proceedings of the CoNLL-2008.</booktitle>
<marker>Ciaramita, Attardi, Dell’Orletta, Surdeanu, 2008</marker>
<rawString>Massimiliano Ciaramita, Giuseppe Attardi, Felice Dell’Orletta, and Mihai Surdeanu. 2008. DeSRL: A Linear-Time Semantic Role Labeling System. Proceedings of the CoNLL-2008.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Haji6</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
</authors>
<title>Maria Antonia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Št6pánek, Pavel Stranák, Mihai Surdeanu, Nianwen Xue and Yi Zhang.</title>
<date>2009</date>
<booktitle>The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009).</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Haji6, Ciaramita, Johansson, Kawahara, 2009</marker>
<rawString>Jan Haji6, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antonia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Št6pánek, Pavel Stranák, Mihai Surdeanu, Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009). Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Haji6</author>
<author>Jarmila Panevová</author>
<author>Eva Haji6ová</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
</authors>
<title>The Prague Dependency Treebank 2.0. Linguistic Data Consortium,</title>
<date></date>
<booktitle>Št6pánek, Jiri Havelka, Marie Mikulová and Zden6k Žabokrtský.</booktitle>
<pages>1--58563</pages>
<publisher>ISBN</publisher>
<location>USA.</location>
<marker>Haji6, Panevová, Haji6ová, Sgall, Pajas, </marker>
<rawString>Jan Haji6, Jarmila Panevová, Eva Haji6ová, Petr Sgall, Petr Pajas, Jan Št6pánek, Jiri Havelka, Marie Mikulová and Zden6k Žabokrtský. 2006. The Prague Dependency Treebank 2.0. Linguistic Data Consortium, USA. ISBN 1-58563-370-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Koiti Hasida</author>
</authors>
<title>Construction of a Japanese Relevance-tagged Corpus.</title>
<date>2002</date>
<booktitle>Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC2002). Las</booktitle>
<location>Palmas,</location>
<contexts>
<context position="18869" citStr="Kawahara et al., 2002" startWordPosition="3050" endWordPosition="3053">t word to the predicate one; y. The relative level (up, down, or same) and level difference on the syntactic dependency tree of the current word to the predicate one; z. The length of the shortest path between the current word and the predicate word. 4 Experiments 4.1 Datasets The datasets of the CoNLL-2009 shared task contain seven languages: Catalan (CA), Chinese (CN), Czech (CZ), English (EG), German (GE), Japanese (JP), and Spanish (SP). The training and evaluation data of each language (Taulé et al., 2008; Xue et al., 2008; Hajic et al., 2006; Palmer et al., 2002; Burchardt et al., 2006; Kawahara et al., 2002) have been converted to a uniform CoNLL Shared Task format. Each participating team is required to process all seven language datasets. Lanuage CA CN CZ EN GE JP SP Size (KB) 48974 41340 94284 58155 41091 8948 52430 # of Sen- 14924 24039 43955 40613 38020 4643 15984 tences # of Predi- 42536 110916 469754 185404 17988 27251 48900 cate words Avg. # of 2.85 4.61 10.69 4.57 0.47 5.87 3.06 Predicates per sentence popular a2 01 = 01 1 = a2 sense tag (37%) (90%) (81%) (87%) (75%) (100%) (39%) Table 1. Statistical information of the seven language datasets (training and development). Table 1 shows som</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 2002. Construction of a Japanese Relevance-tagged Corpus. Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC2002). Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baoli Li</author>
<author>Qin Lu</author>
<author>Shiwen Yu</author>
</authors>
<title>An Adaptive kNearest Neighbor Text Categorization Strategy.</title>
<date>2004</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>215--226</pages>
<contexts>
<context position="9113" citStr="Li et al., 2004" startWordPosition="1436" endWordPosition="1439">výšení”, and “f2” is its sense tag. We then use these derived 74 sense tags as class tags and add a class tag “=” for samples without specialized sense tag. For each predicate word, we derive a vector describing its context and attributes, each dimension of which corresponds to a feature. We list the feature types in the next subsection. Features appearing only once are removed. The TF*IDF weighting schema is used to calculate the weight of a feature. Three different algorithms were tried during the development period: support vector machines (SVM), distance-weighted k-Nearest Neighbor (kNN) (Li et al., 2004), and Naïve Bayes with multinomial model (Mccallum and Nigam, 1998). As to the SVM algorithm, we used the robust LIBSVM package (Chang and Lin, 2001), with a linear kernel and default values for other parameters. The algorithms achieving the best results in our preliminary experiments are chosen for different languages: SVM for Catalan, Chinese, and Spanish; kNN for German (k=20). We used kNN for English (k=20) and Czech (k=10) because we could not finish training with SVM on these two datasets in limited time. Even with kNN algorithm, we still had trouble with the English and Czech datasets, </context>
</contexts>
<marker>Li, Lu, Yu, 2004</marker>
<rawString>Baoli Li, Qin Lu and Shiwen Yu. 2004. An Adaptive kNearest Neighbor Text Categorization Strategy. ACM Transactions on Asian Language Information Processing, 3(4): 215-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluís Màrquez</author>
<author>Xavier Carreras</author>
<author>Kenneth C Litkowski</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Semantic Role Labeling: An Introduction to the Special Issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--2</pages>
<contexts>
<context position="1343" citStr="Màrquez et al., 2008" startWordPosition="186" endWordPosition="189">nt languages, either for providing functions required by a language or for meeting the tight deadline. The system obtained labeled F1 69.26 averaging over seven languages (Catalan, Chinese, Czech, English, German, Japanese, and Spanish), which ranks the system fourth among the seven systems participating the SRLonly closed track. 1 Introduction Semantic role labeling, which aims at computationally identifying and labeling arguments of predicate words, has become a leading research problem in computational linguistics with the advent of various supporting resources (e.g. corpora and lexicons) (Màrquez et al., 2008). Word semantic dependencies derived by semantic role labeling are assumed to facilitate automated interpretation of natural language texts. Moreover, techniques for automatic annotation of semantic dependencies can also play an important role in adding metadata to corpora for the purposes of machine translation and speech processing. We are currently investigating such techniques as part of our research into integrated language technology in the Center for Next Generation Localization (CNGL, 73 http://www.cngl.ie). The multilingual nature of the CoNLL-2009 shared task on syntactic and semanti</context>
</contexts>
<marker>Màrquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>Lluís Màrquez, Xavier Carreras, Kenneth C. Litkowski and Suzanne Stevenson. 2008. Semantic Role Labeling: An Introduction to the Special Issue. Computational Linguistics, 34(2):145-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Mccallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A Comparison of Event Models for Naive Bayes Text Classification.</title>
<date>1998</date>
<booktitle>Proceedings of AAAI/ICML-98 Workshop on Learning for Text Categorization.</booktitle>
<contexts>
<context position="9180" citStr="Mccallum and Nigam, 1998" startWordPosition="1447" endWordPosition="1450">ved 74 sense tags as class tags and add a class tag “=” for samples without specialized sense tag. For each predicate word, we derive a vector describing its context and attributes, each dimension of which corresponds to a feature. We list the feature types in the next subsection. Features appearing only once are removed. The TF*IDF weighting schema is used to calculate the weight of a feature. Three different algorithms were tried during the development period: support vector machines (SVM), distance-weighted k-Nearest Neighbor (kNN) (Li et al., 2004), and Naïve Bayes with multinomial model (Mccallum and Nigam, 1998). As to the SVM algorithm, we used the robust LIBSVM package (Chang and Lin, 2001), with a linear kernel and default values for other parameters. The algorithms achieving the best results in our preliminary experiments are chosen for different languages: SVM for Catalan, Chinese, and Spanish; kNN for German (k=20). We used kNN for English (k=20) and Czech (k=10) because we could not finish training with SVM on these two datasets in limited time. Even with kNN algorithm, we still had trouble with the English and Czech datasets, because thousands of training samples make the prediction for the e</context>
</contexts>
<marker>Mccallum, Nigam, 1998</marker>
<rawString>Andrew Mccallum and Kamal Nigam. 1998. A Comparison of Event Models for Naive Bayes Text Classification. Proceedings of AAAI/ICML-98 Workshop on Learning for Text Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis Marquez</author>
<author>Joakim Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</booktitle>
<contexts>
<context position="3178" citStr="Surdeanu et al., 2008" startWordPosition="475" endWordPosition="478">meet the deadline, especially for some large datasets (such as the Czech data). In spite of these difficulties and resource limitations, we are proud to be among the 21 teams who successfully submitted the results1. As a new participant, our goals in attending the CoNLL-2009 SRLonly shared task were to gain more thorough knowledge of this line of research and its state-of-the-art, and to explore how well a system quickly assembled with existing packages can fare at this hard semantic analysis problem. Following the successful approaches taken by the participants of the CoNLL-2008 shared task (Surdeanu et al., 2008) on monolingual syntactic and semantic dependency analysis, we designed and implemented our CoNLL-2009 SRLonly system with pipeline architecture. Two main components are cascaded in this system: one is for disambiguating predicate word sense 2 , and the other for identifying and classifying arguments for 1 According to our correspondence with Dr. Jan Hajic, totally 31 teams among 60 registered ones signed and got the evaluation data. 2 As predicate words are marked in the CoNLL-2009 datasets, we don’t need to identify predicate words. Proceedings of the Thirteenth Conference on Computational N</context>
<context position="14667" citStr="Surdeanu et al., 2008" startWordPosition="2351" endWordPosition="2354">rd can play different roles with respect to different predicate words and a predicate word can be an argument of itself, we generate a training set by deriving a training example from each wordpredicate pair. For example, if a sentence with two predicates has 7 words, we will derive 7*2=14 training examples. Therefore, the number of training examples generated in this step will be around L times larger than that obtained in the previous step, where L is the average length of sentences. We chose to use maximum entropy algorithm in this step because of its success in the CoNLL-2008 shared task (Surdeanu et al., 2008). Le Zhang’s maximum entropy package (Zhang, 2006) is integrated in our system. The Czech data cause much trouble again for us, as the training data derived by the above strategy became even larger. We had to use a special strategy for the Czech data: we selectively chose wordpredicate pairs for generating the training dataset. In other words, not all possible combinations are used. We chose the following words with respect to each predicate: the first and the last two words of a sentence; the words between the predicate and any argument of it; two words before the predicate or any argument; a</context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, Marquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis Marquez and Joakim Nivre. 2008. The CoNLL2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taulé</author>
</authors>
<title>Maria Antònia Martí and Marta Recasens.</title>
<date>2008</date>
<booktitle>Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008).</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Taulé, 2008</marker>
<rawString>Mariona Taulé, Maria Antònia Martí and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008). Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigorios Tsoumakas</author>
<author>Ioannis Katakis</author>
</authors>
<title>MultiLabel Classification: An Overview.</title>
<date>2007</date>
<journal>International Journal of Data Warehousing and Mining,</journal>
<pages>3--3</pages>
<contexts>
<context position="15507" citStr="Tsoumakas and Katakis, 2007" startWordPosition="2495" endWordPosition="2498"> a special strategy for the Czech data: we selectively chose wordpredicate pairs for generating the training dataset. In other words, not all possible combinations are used. We chose the following words with respect to each predicate: the first and the last two words of a sentence; the words between the predicate and any argument of it; two words before the predicate or any argument; and two words after the predicate or any argument. In the Czech and Japanese data, some words may play multiple roles with respect to a predicate word. We thus have to consider multi-label classification problem (Tsoumakas and Katakis, 2007) for these two languages’ data. We tried the following two solutions: • Take each role type combination as a class and transform the multi-label problem to a single-label classification problem; • Classify a word with a set of binary classifiers: consider each role type individually with a binary classifier; any possible role type will be output; if no role type is obtained after considering all the role types, the role type with the highest confidence value will be output; and, if “_” is output with any other role type, remove it. We used the second solution in our official submission, but we</context>
</contexts>
<marker>Tsoumakas, Katakis, 2007</marker>
<rawString>Grigorios Tsoumakas and Ioannis Katakis. 2007. MultiLabel Classification: An Overview. International Journal of Data Warehousing and Mining, 3(3):1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<pages>15--1</pages>
<marker>Xue, Palmer, 2009</marker>
<rawString>Nianwen Xue and Martha Palmer. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum Entropy Modeling Toolkit for Python and C++. Software available at http://homepages.inf.ed.ac.uk/s0450736/maxent_tool kit.html.</title>
<date>2006</date>
<marker>Le Zhang, 2006</marker>
<rawString>Le Zhang. 2006. Maximum Entropy Modeling Toolkit for Python and C++. Software available at http://homepages.inf.ed.ac.uk/s0450736/maxent_tool kit.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>