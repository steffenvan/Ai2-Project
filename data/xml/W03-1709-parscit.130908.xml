<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.982909">
Chinese Lexical Analysis Using Hierarchical Hidden Markov Model
</title>
<author confidence="0.996677">
Hua-Ping ZHANG1 Qun LIU1,2 Xue-Qi CHENG1 Hao Zhang1 Hong-Kui Yu1
</author>
<affiliation confidence="0.995652">
1Inst. of Computing Tech., The Chinese Academy of Science, Beijing, 100080 CHINA
2Inst. of Computational Linguistics, Peking University, Beijing, 100871 CHINA
</affiliation>
<email confidence="0.974034">
Email: zhanghp@software.ict.ac.cn
</email>
<sectionHeader confidence="0.995693" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99902668">
This paper presents a unified approach for
Chinese lexical analysis using hierarchical
hidden Markov model (HHMM), which
aims to incorporate Chinese word seg-
mentation, Part-Of-Speech tagging, dis-
ambiguation and unknown words
recognition into a whole theoretical frame.
A class-based HMM is applied in word
segmentation, and in this level unknown
words are treated in the same way as
common words listed in the lexicon. Un-
known words are recognized with reliabil-
ity in role-based HMM. As for
disambiguation, the authors bring forth an
n-shortest-path strategy that, in the early
stage, reserves top N segmentation results
as candidates and covers more ambiguity.
Various experiments show that each level
in HHMM contributes to lexical analysis.
An HHMM-based system ICTCLAS was
accomplished. The recent official evalua-
tion indicates that ICTCLAS is one of the
best Chinese lexical analyzers. In a word,
HHMM is effective to Chinese lexical
analysis.
</bodyText>
<sectionHeader confidence="0.998735" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998623862745098">
Word is the independent and meaningful atom
in natural language. Unlike English and Spanish,
there is no delimiter to mark word boundaries and
no explicit definition of words in some Asian lan-
guages. As for Chinese language processing, the
fundamental task is word segmentation, which
transforms Chinese character string into words se-
quence. It is prerequisite to POS tagger, parser and
other deep processing, and the lexical result is the
basis of further applications such as machine trans-
lation, information retrieval and information ex-
traction.
Since the first system CDWS appeared in
1983, word segmentation has been researched in-
tensively. Many solutions were proposed and could
be broadly categorized into rules-based approaches
that make use of linguistic knowledge and statisti-
cal approaches that train on corpus after machine
learning. The classic rule-based approaches include
maximum matching and shortest path (SP), which
achieve the minimum number of segmented words.
Zhang and Liu (2002) present an extended SP al-
gorithm named &amp;quot;n-shortest paths&amp;quot;. Some research-
ers introduce more complicated rules, such as
error-driven learning (Hockenmaier and Brew,
1998) and parsing (Wu and Jiang, 1998). Rule is
the only feasible way to segment words unless
necessary resources such as large amount of corpus
are available. With the development of hand-
corrected resource, statistical approaches became
more popular. The language models commonly
applied are n-gram (Zhang and Liu, 2002; Gao et
al., 2001), EM (Peng and Schuurmans, 2001), and
channel noise model. As far as we know, however,
there is yet neither purely rule-based system nor
purely statistical one. It tends to tackle Chinese
lexical problem with mixture of rules and statisti-
cal information. On one hand, trainable rules
(Palmer, D. 1997) seem more adaptive and effi-
cient in that rule-based approaches benefit from
frequency of rule occurrence, on the other hand,
statistical solutions employ rules to detect ambigu-
ity, numeric expression, time and other named
entities. Apart from the above approaches, we also
notice some other promising ideas such as com-
pression-based (Teahan et al., 2001), classifier-
based (Xue and Susan, 2002) and self-supervised
segmentation without lexicon. According to recent
reports, word segmentation has achieved good re-
sult in precision, especially on texts that do not
contain ambiguity or out-of-vocabulary words.
However, segmentation ambiguity and un-
known words1 cause bottlenecks and greatly de-
grade performance in word segmentation. Am-
biguous or unknown string is hard to be correctly
segmented; at the same time, it also influences on
segmenting its neighboring words. What&apos;s worse,
ambiguity often occurs with unknown words. Take
&amp;quot; A # VN XI, fel fq M g M !A &amp;quot;(Clinton said to
Netanyahu) as exemplification, &amp;quot; F j fq M g
ffl&amp;quot;(Netanyahu) is unknown transliterated personal
name, and both &amp;quot;X&amp;quot;Mj&amp;quot; (�or home) and &amp;quot;MIA&amp;quot;
(talk nonsense) has two ambiguous segmentations:
split into halves or not. Here, it&apos;s difficult to
identify unknown word&amp;quot;I ]MPi &amp;quot;M&amp;quot; because of
the ambiguities, while disambiguation is also
difficult to accomplish before unknown words
detection. Therefore, the final lexical result is very
likely to be &amp;quot;~~~~~~~~~ ~
~&amp;quot; instead
of &amp;quot;~~~~~~~~~
~~&amp;quot;.
Historically, much effort has been made in the
two sub-problems of word segmentation. Almost
all previous solutions (Chunyu et al. 2002; Zhang,
1998; Zheng, 1999) of disambiguation attempt to
cover each possible case with trivial rules, while
recently statistical approaches are applied in some
special categories of ambiguity. For instance, vec-
tor space model was applied in combinational am-
biguity (Luo et al. 2002). Concerning unknown
word, we only need focus on unknown named enti-
ties, including personal name (PER), location name
(LOC), and organization name (ORG). The moti-
vation in named entity recognition is to utilize its
components and contexts. Like word segmentation
and disambiguation, the usual approach is to apply
rules (Sun, 1993;Tan, 1999;Luo and Ji, 2001; Luo
and Song, 2001). Recognition rules are summa-
rized on name libraries or different linguistic phe-
nomena. Compared with rules-based approach,
machine learning from large corpus seems easy but
better in performance. The statistical approaches
proposed recently include hidden Markov model
(Zhang and Liu, 2002; Zhang et al. 2002), agent-
based (Ye, 2003), class-based trigram model (Sun
et al., 2002).
After nearly 20 years of hard work, rapid pro-
gresses are made on word segmentation,
disambiguation and unknown word recognition
research individually. To the best of our
knowledge, however, all the achievement has not
</bodyText>
<footnote confidence="0.8002195">
1 We define unknown words to be those neitheri n-
cluded in the core lexicon nor recognized through FSA.
</footnote>
<bodyText confidence="0.992770563636364">
ever, all the achievement has not integrated into a
unified model with a general theoretical basis. In
previous lexical analyzers, so-called word segmen-
tation algorithm actually only employs on common
words listed in the lexicon, while disambiguation
and unknown word recognition have their own in-
dependent mechanism and become distinct proc-
esses from segmentation. Without scientific
quantification, unknown words and disambiguation
result could not compete with other segmentation
candidates. In a word, previous work lacks a
whole frame incorporating the different sub-tasks
in lexical analysis, while there is also no consistent
mechanism to evaluate various lexical results
Therefore, previous lexical system is difficult
achieve better performance on real texts that con-
tain irregular character strings mentioned above.
This paper presents an HHMM-based ap-
proach for Chinese lexical analysis. It aims to util-
ize a general model to proceed all steps in lexical
analysis, including word segmentation, disam-
biguation, unknown words recognition and part-of
speech (POS) tagging. In the preprocessing, top n
segmentation candidates covering the possible am-
biguity are provided using n-shortest-path algo-
rithm (Zhang and Liu, 2002). Then, simple
unknown named entities like personal names and
location names are identified on the candidate set
using class-based HMM. Following that, a higher
level of HMM could be employed on recognizing
organization and other recursive named entity,
which includes another simple unknown word.
Unknown words recognized with credible prob-
ability are added to class-based HMM for word
segmentation. In this level of HHMM, unknown
words and ambiguity are treated in the same way
as common words. POS tagging is the top level in
HHMM. After HHMM- based approach applied,
Chinese lexical analysis system ICTCLAS
achieves well in segmentation and POS tagging.
The official evaluation, which was held by the Na-
tional Foundation of 973 Plan of China, shows that
ICTCLAS rank top and it is one of the best Chi-
nese lexical analyzers.
The structure of this paper is as follows. The
next section reviews HHMM and presents the
framework of HHMM-based Chinese lexical
analysis. Then we explain the class-based HMM
for word segmentation. Next we detail role-based
unknown words recognition and n-shortest-path
disambiguation. The following section describes
various experiments designed to evaluate lexical
analysis performance and contribution from differ-
ent level in HHMM.
2 HHMM and Chinese lexical analysis
</bodyText>
<equation confidence="0.979291">
P(qd+1 �)
�� is defined to be the probability that
i
state q initially activates its child state q.
d+1
�
</equation>
<bodyText confidence="0.9981385">
3) Only the bottom HMM can observe the symbols.
The corresponding symbol emission probabilities
</bodyText>
<subsectionHeader confidence="0.909124">
2*1 An overview of HHMM are B(qD )=(bk (qD ))) , where b (qD)=P(ok IqD )
</subsectionHeader>
<bodyText confidence="0.99757375">
Hidden Markov model (HMM, L.R. Rabiner,
1989) has become the method of choice for model-
ing stochastic processes and sequence in natural
language processing, because HMM is very rich in
mathematical structure and hence can form theo-
retical basis for use. However, compared with the
sophisticated phenomena in natural language,
traditional HMM seems hard to use due to the
multiplicity of length scales and recursive nature of
the sequences. Therefore Shai Fine et al (1998)
proposed hierarchical hidden Markov model,
which is a recursive and generalized HMM.
Based on Shai&apos;s work, we give a formal de-
scription of HHMM. An HHMM is specified by a
six-tuple (S , O , II, A, B, D), where D is the depth
of levels, S and O are the finite set of states and the
final output alphabet or intermediate output, and
II,A and B are the probabilities of the initial state,
state transitions and emissions of symbol or inter-
mediate output, respectively. The contrast between
traditional HMM and HHMM lies in:
1) The state set S can be classified into different
sub-sets according to its level. A state in S is an-
notated with � (0&lt;��D,
</bodyText>
<equation confidence="0.737568428571428">
� 0&lt;i&lt;l �
� 1), where d is
�
the level index, i is the state index and � is
� the
set of state in level d. When d=D, qd is called
�
</equation>
<bodyText confidence="0.472650666666667">
terminal state because its observation is symbols,
or else, it is called internal state whose observation
is from its child HMM in (d+1)th level.
</bodyText>
<listItem confidence="0.9960325">
2) Every internal state q (0&lt;
&lt;D)
</listItem>
<bodyText confidence="0.974403166666667">
� has its child
states, which form an independent HMM. In the
child HMM, the state transitive probabilities are
A(qd )=(a..(gd)),and,a..(gd)=P(qd+1lgd+1)
ii ii J z
And the initial distribution vector is like
</bodyText>
<equation confidence="0.98575">
n(qd) = (9d(gd+1)) = (P(qd+1 qd )) ,where
� �
</equation>
<bodyText confidence="0.999930375">
and a. is in symbol set. For the d (d&lt;D) level
HMM, state sequence in its child HMM could be
viewed as its observation. The emission probabili-
ties could be estimated as above.
All in all, HHMM includes D levels of HMM
while each level is independent HMM. Moreover,
each HMM only links with its parent and child.
The whole parameters set of HHMM is denoted by
</bodyText>
<equation confidence="0.963146166666667">
A={ (A(qd )}
={ { ( )}
� ��
.
Actually, HMM is the specific form of
HHMM with D=1.
</equation>
<bodyText confidence="0.991238481481482">
2*2 Framework of HHMM-based lexical
analysis
As illustrated in Figure 1, HHMM-based Chi-
nese lexical analysis comprises five levels: atom
segmentation, simple and recursive unknown
words recognition, class-based segmentation and
POS tagging. In the whole frame, class-based seg-
mentation graph, which is a directed graph de-
signed for word segmentation, is an essential
intermediate data structure that links disambigua-
tion, unknown words recognition with word seg-
mentation and POS tagging.
Atom segmentation, the bottom level of
HHMM, is an initial step. Here, atom is defined to
be the minimal segmentation unit that cannot be
split in any stage. The atom consists of Chinese
character, punctuation, symbol string, numeric ex-
pression and other non-Chinese char string. Any
word is made up of an atom or more. Atom seg-
mentation is to segment original text into atom se-
quence and it provides pure and simple source for
its parent HMM. For instance, a sentence like
&amp;quot;2002.9,ICTCLAS (6,nEh gffAfAtli&amp;quot; (The
free source codes of ICTCLAS was distributed in
September, 2002) would be segmented as atom
sequence &amp;quot;2002.9/,/ICTCLAS/�J/n/Eh/�/q/fF
/44M/M/�/&amp;quot;. In this HMM, the original symbol is
</bodyText>
<figure confidence="0.817355138888889">
de {1, ... ,D }
}
, { ( )}}
� � �
,{n(qd )}
1
de {1, ... ,D
� �
1
}
de {1, ... ,D
&amp;quot;&apos; #$$
W#=
arg max P(WIA)=
arg max P(W,A)/P(A)
W
W
Words sequence
On the basis of Baye&apos;s Theorem, it can
be induced that:
arg max P(WIC)P(C)
W
W# can be found with another level of
HMM if class ci is viewed as state while
word wi is output. Therefore:
W# =
W# �
arg max
II (x&apos;i I ci) (ci I ci-1)
observation while the atom is state. We skip the
String
Atom
Segmentation
Atom sequence
3 Class-based HMM for word segmenta-
tion
</figure>
<bodyText confidence="0.7229592">
We apply to word segmentation class-
based HMM, which is a generalized ap-
proach covering both common words and
unknown words.
Given a word wi, class ci is defined in
</bodyText>
<figureCaption confidence="0.94834">
Figure 2. Suppose ILEXI to be the lexicon
size, then the total number of word classes is
</figureCaption>
<figure confidence="0.97908590625">
ILEXI+9.
Simple unknown
words recognition
Recursive unknown
words recognition
Revised
SP-based rough
segmentation
Top n sequence
results
%ih#$$
PER
LOC
LOC
ORG
Class-based
segmentation
graph
&amp;&apos; #$$
Class-based
segmentation
wi iff wi is listed in the segmentation lexicon;
PER iff wi is unlisted* personal name;
LOC iff wi is unlisted location name;
ORG iff wi is unlisted organization name;
TIME iff wi is unlisted time expression;
NUM iff wi is unlisted numeric expression;
STR iff wi is unlisted symbol string;
BEG iff beginning of a sentence
END iff ending of a sentence
OTHER otherwise.
* &amp;quot;unlisted&amp;quot; is referred as being outside the lexicon
</figure>
<figureCaption confidence="0.997795">
Figure 2: Class Definition of word w;
</figureCaption>
<bodyText confidence="0.665494333333333">
Given the atom sequence A=(al,...aj,
let W=(wl,...wm) be the words sequence, C=
(cl,...cm) be a corresponding class sequence
of W, and W# be the choice of word segmen-
tation with the maximized probability, re-
spectively. Then, we could get:
</bodyText>
<figure confidence="0.973292833333333">
ci =
&apos;`h #$$
For a specific atom sequence A, P(A) is
POS sequence
Lexical
results
</figure>
<figureCaption confidence="0.999293">
Figure 1* HHMM-based Chinese lexical analysis
</figureCaption>
<bodyText confidence="0.9992212">
detail of operation in that it&apos;s a simple application
on the basis of HMM. POS tagging using HMM is
also skipped because role tagging, which presented
in section 5, is similar to it in nature. The other
levels of HHMM will be provided in the next parts.
</bodyText>
<figure confidence="0.939350058823529">
M
where c! is begin of sentence.
arg max P(W)
it
� � ... �
� 2 �
a constant and P(W,A)= P(W). So,
W#=
W
1ih #$$
POS
Tagging
(I0) (�I-T_-) (lo) (T,IM (lo) (NUMI30 (Io) (`fINUM) (1o) (1A1!�f) (Io) (1I10)
S/S
(lo) (_T__IS)
(lo) (PERIS)
:t/:t
:t43/PER
(lo) (NUMIPER)
4/4 ter,/,ter, 1893/NUM
(lo) (TIMEIPER)
(1o) (T1MEI*)
(lo) (A1-I*)
1893 */TIME 01/91
(lo) (AITIME)
(1o) (1A1IT1ME)
/ Wig 1/1
(Io) (EI&apos;l)
E/E
Figure3* Class-based word segmentation
Note:
1. The original sentence is &amp;quot;Et43T,, 1893 *01&amp;quot; (Mao Ze-Dong was born in the year of
1893). Its atom sequence is &amp;quot;~/~/~/1893/~/i9/1/&amp;quot; after atom segmentation;
2.The node format is &amp;quot;word/class&amp;quot; (wi / ci) and the weight on the node is *lo) (wi I ci);
</figure>
<listItem confidence="0.932656666666667">
3. Weight on the directed edge is —log (ci I ci-1);
4. &amp;quot;-:t43T,&amp;quot; (Mao Ze-Dong) is personal name outside the lexicon.The node &amp;quot; -:t4-3T,,/PER&amp;quot; and
the related edges with dash line is inserted after unknown words recognition.
</listItem>
<bodyText confidence="0.912983">
For convenience, we often use the negative
log probability instead of the proper form. That is:
</bodyText>
<equation confidence="0.959845">
M
W � = argmin Y_[—ln (wi I ci)— ln (Ci I Ci —1)1
W i=1
</equation>
<bodyText confidence="0.999902333333333">
According to the word class definition, if wi is
listed in lexicon, then ci is wi, and p(wiIci) is equal
to 1.0. Otherwise, p(wiIci) is probability that class
ci initially activates wi , and it could be estimated
in its child HMM for unknown words recognition.
As demonstrated in Figure 3, we provide the
process of class-based word segmentation on &amp;quot;Et
MT,, 1893&apos; 01&amp;quot; (Mao Ze-Dong was born in the
year of 1893). The significance of our method is: it
covers the possible ambiguity. Moreover, unknown
words, which are recognized in the following steps,
can be added into the segmentation graph and pro-
ceeded as any other common words.
After transformation through class-based HMM,
word segmentation becomes single-source shortest
paths problem. Hence the best choice Wo of word
segmentation is easy to find using Djikstra&apos;s algo-
rithm.
</bodyText>
<sectionHeader confidence="0.950364" genericHeader="method">
4 NSP-based disambiguation strategy
</sectionHeader>
<bodyText confidence="0.99705509375">
Segmentation ambiguous error is made mainly
because of improper decision in the earlier stage.
For example, overlapping ambiguity in &amp;quot;ffiLf-f�3&apos;-/)A/
3}&apos;T/ffl&amp;quot; (When combining into molecule) and
combining ambiguity in &amp;quot;9A_/n/1k/_T_/_L/h/
&amp;quot;(The person has naevi on his hand) are difficult
to solve only in the initial stage of word segmenta-
tion. However, it&apos;s simple to find the correct result
among the possible candidates in POS tagging or
further processes. Therefore, the initial process
should not make the final decision, but provide
candidates covering the correct segmentation.
We take n-shortest-path (NSP, Zhang and Liu,
2002) algorithm as the disambiguation strategy.
NSP, which selects n shortest paths, is an extension
of Djikstra&apos;s algorithm. The motivation in disam-
biguation using NSP is covering more ambiguity
with top n results in rough segmentation, which is
the initial step in lexical analysis and produces
candidate results.
Considering efficiency and performance, rough
segmentation coverage, which is percentage of cor-
rect results, should be much higher while the aver-
age size of candidate set should be as small as
possible. Compared with NSP, full segmentation,
which produces all the possible segmentation paths,
suffers from large amount of candidates, while
other approaches lose so many correct results. As
shown in Table 1, NSP-based rough segmentation
enjoys two good properties: higher coverage and
fewer candidates. In other word, NSP is effective
strategy for disambiguation.
</bodyText>
<table confidence="0.995984333333333">
Approach Max Size AV Size Coverage
MM 1 1 8 5.46%
SP 1 1 91.80%
ML 1 1 93. 50%
FS &gt;3,424, 507 &gt;391.79 100.00%
NSP 8 5.82 99.92%
</table>
<tableCaption confidence="0.828324">
Table 1* Comparison between NSP and other
approaches of rough segmentation
</tableCaption>
<note confidence="0.336622">
Note:
</note>
<listItem confidence="0.990909666666667">
1) MM: maximum matching; SP: shortest path; ML:
Maximum likelihood; FS: Full segmentation
2) Max size and AV size is the maximum and average
size of segmentation candidate set, respectively;
3) Coverage=# of correctly segmented/# of sen-
tence*100%
4) The size of testing set is 2 million Chinese characters.
5 Unknown words recognition using role-
based HMM
</listItem>
<bodyText confidence="0.9839351875">
The task includes: locating the boundary of a
unknown word wi, identifying the word class ci,
and computing the probability p(wilci), which is
required in class-based segmentation. Here, we
introduce two levels of HMM to recognize simple
and recursive unknown words on the rough
segmentation set.
5*1 Role set for unknown words recognition
In the same way of class-based HMM for
word segmentation, here we classify word class
into various role according to its linguistic features
shown in unknown words recognition. In table 2,
we present a simplified role set for unknown per-
sonal name recognition. Role is similar as word
class. Their difference is: a word has only a word
class, but a word class has one role or more.
</bodyText>
<table confidence="0.999761769230769">
Role Significance Sample
A Previous context 1*0/-f/�/�/�/�
B Next context A/�/ jV
C Surname *AV/00
D First token of 2- */074/8JN
Hanzi given name
E Second token of 2- */ /S8JN
Hanzi* given name
H Suffix T-/,9; JXIf/-9
L Token in transliter- /�/JA/�
ated name
Z Remote context AW/ *K/5 /+/-1
...
</table>
<tableCaption confidence="0.95946">
Table 2* Simplified role set of personal names
* Hanzi: Chinese character
</tableCaption>
<bodyText confidence="0.934684444444444">
5*2 Role tagging and Recognizing Unknown
words recognition
Given a word sequence W=(wl,...wn) , we
could get its class result C=(cl,...cn). Now we
could tag W with role +=(rl,...rn), where all roles
are from the same set. Among all the roles se-
quence, we select the sequence R# with the maxi-
mum probability as the final choice. Through the
same induction detailed in section 3, we could get
</bodyText>
<equation confidence="0.996182">
( � ) ln ( � 1)]
�� �� � �� ���
+ i
</equation>
<bodyText confidence="0.999900315789474">
It is a tagging process and we make use of Viterbi
algorithm (L.R.Rabiner, 1988) that selects the
global optimum among all the state sequences.
Here, tagging word class sequence&amp;quot; EE / 4 / T,,
/TIME/0-&apos;t&amp;quot; (Mao Ze-Dong was born in some-
time.) with personal roles, we could get R#=&amp;quot;-:E/C
4/D ,T, ,/E TIME/B 0-&apos;t/Z&amp;quot; through Vitebi selec-
tion.
Unknown words are recognized through
maximum pattern matching on role sequence. For
instance, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;,&amp;quot;E&amp;quot; is surname, first and sec-
ond token of 2-Hanzi given name, respectively. So
token sequence tagged with role &amp;quot;CDE&amp;quot; is likely to
form a traditional Chinese personal name. There-
fore, &amp;quot;~~~&amp;quot; will be recognized as a Chinese
personal name according to its roles.
Let wi be recognized unknown word and ci be
the word class, we estimate the probability (wi,ci)
with the following formula:
</bodyText>
<equation confidence="0.955432">
k � 1 k � 1
(wi I C) = H (C +j I r +j )x H Arr j I P+j-1 );
j ~ 0 j � 1
</equation>
<bodyText confidence="0.950871666666667">
where wi is made up of tokens from pth to (p+k-
1)th.
Hence (-:E4,Tr,,PER)=p(-&amp;quot;QC) p(41D) p(,T,,,E)
p(DIC)p(EID). Finally unknown word &amp;quot;���&amp;quot;
and (-:E4�,PER) can be added into the class-
based HMM, shown as dashed area in Figure 3.
5*3 Recursive unknown word recognition
Organization name like &amp;quot;Mr, A.
~~&amp;quot;(Memorial Hall of Zhou En-Lai and Deng
Yun-Chao) and some sophisticated location name
like &amp;quot;�Kn�A&amp;quot;(Zhang Zi-Zhong Road) often in-
clude one or more unknown words. We call them
&amp;quot;recursive unknown word&amp;quot;.
Our solution is: Firstly, recognizing non-
recursive unknown words in the lower level of
role-based HMM, then revising the word class se-
quence with the recognized results; next applying
another role HMM to recognize the recursive ones.
Take the original word class sequence &amp;quot;fr9-/.,/*/
/
/~/~/~~&amp;quot; as exemplification. In the
first step, &amp;quot;frCK, &amp;quot; and &amp;quot;AW-AM&amp;quot; would be recog-
nized as personal name. Then, the original class
sequence could be replaced with &amp;quot;PER/R/PER/C
</bodyText>
<figure confidence="0.9546391">
n
# �
I
�
+
ln
E
arg min
1
�
</figure>
<bodyText confidence="0.9961138">
��&amp;quot;.Based on the revised class result, the higher
role-based HMM could recognize the recursive
unknown word &amp;quot;ME, as an
organization name. Our method utilizes previous
results and greatly reduces data sparseness.
The role training set is transformed from cor-
pus tagged with POS. Zhang and Liu (2002) pro-
vided the algorithm for role data conversion, model
training, named entity recognition and the other
procedures in role-based HMM.
</bodyText>
<sectionHeader confidence="0.999211" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.921661761904762">
An HHMM-based Chinese lexical system
ICTCLAS was accomplished. The following ex-
periments are performed on ICTCLAS.
As commonly used, we conduct our evalua-
tions on terms of segmentation accuracy (SEG),
accuracy of POS tagging (TAG1) with 24 tags,
accuracy of POS tagging (TAG2) with 48 tags,
precision of named entity recognition (P), recall of
named entity recognition (R) and F-measure (F)
that is weighted combination of P and R. They are
calculated as following:
SEG= # of correctly segmented words/ # of words;
TAG1= # of correctly tagged 24-tag POS/ # of
words;
TAG2= # of correctly tagged 48-tag POS/ # of
words;
P= # of correct recognized NE/# of recognized NE;
R= # of correct recognized NE/# of NEX100%;
F_ Rx Px (&amp;quot;J62) ,here0is assigned with 1, and F
R+Px�32
is called F-1.
</bodyText>
<subsectionHeader confidence="0.69276">
6*1 Chinese Lexical analysis and HHMM
</subsectionHeader>
<bodyText confidence="0.9027555">
On 1,108,049-word news corpus from the
People &apos;s Daily, we conduct four experiments:
</bodyText>
<listItem confidence="0.997283125">
1) BASE: ICTCLAS with only class-based seg-
mentation and POS tagging;
2) +PER: Adding role-based HMM for personal
name recognition to BASE;
3) +LOC: Adding role-based HMM for location
name recognition to +PER;
4) +ORG: Adding role-based HMM for location
name recognition to +LOC.
</listItem>
<bodyText confidence="0.9994621">
Figure 4 gives the contrast among the four
experiments in performance. It indicates that:
firstly, every level in HHMM contributes to lexical
analysis. For instance, SEG increases from 96.55%
to 97.96% after personal HMM is added. If all lev-
els of HMM are integrated, ICTCLAS achieves
98.25% SEG, 95.63% TAG1 and 93.38% TAG2.
Secondly, low levels in HHMM benefits from the
higher one. After organization recognition is ap-
plied, F-1 value of organization adds by 25.91%,
furthermore, the performance of segmentation,
POS tagging and recognition of personal and loca-
tion name improves, too. It is because high level
not only solves its own problem, but also helps the
lower HMMfilter improper candidate. For exa m-
ple, in the sentence &amp;quot;CJI 7JcTRWU&amp;quot;(The water in
Liu village is sweet), &amp;quot;CJI&amp;quot;(Liu village) is very
likely to be incorrectly recognized as a personal
name in +PER experiment. However, it will be
revised as a location name in +LOC experiment.
</bodyText>
<figureCaption confidence="0.966203">
Figure 4* Contrast among 4 cases in performance
</figureCaption>
<bodyText confidence="0.3465545">
Note:
FP: F-1 value of personal name recognition;
FL: F-1 value of location name recognition;
FO: F-1 value of organization name recognition
</bodyText>
<subsectionHeader confidence="0.291292">
6*2 Official evaluation on ICTCLAS
</subsectionHeader>
<bodyText confidence="0.9964195">
On July 6, 2002, ICTCLAS participated the of-
ficial evaluation, which was held by the National
Foundation of 973 Project of China. The open
evaluation is conducted on real texts from six do-
mains. The performance of ICTCLAS lists as Ta-
ble 3.
</bodyText>
<table confidence="0.99964325">
Domain Words SEG TAG1 RTAG
Sport 33,348 97.01% 86.77% 89.31%
Int. news 59,683 97.51% 88.55% 90.78%
Literature 20,524 96.40% 87.47% 90.59%
Law 14,668 98.44% 85.26% 86.59%
Theoretics 55,225 98.12% 87.29% 88.91%
Economics 24,765 97.80% 86.25% 88.16%
Total: 208,213 97.58% 87.32% 89.42%
</table>
<tableCaption confidence="0.950564">
Table 3* Official evaluation result of ICTCLAS
Note:
</tableCaption>
<page confidence="0.47068">
1) RTAG=TAG1/SEG*100%
</page>
<bodyText confidence="0.56243525">
2) The result about POS is not comparable because
our tag set is greatly different from theirs.
Compared with other systems, ICTCLAS
ranked top in the evaluation, and it is one of the
</bodyText>
<figure confidence="0.869496090909091">
100
80
60
40
20
0
SEG TAG1 TAG2 FP FL FO
BASE +PER +�OC +ORG
best Chinese lexical analyzer.
7 Conclusion
Our contributions are:
</figure>
<listItem confidence="0.77249">
1) Applying HHMM to different lexical tasks, in-
cluding word segmentation, POS tagging, un-
known words recognition, and disambiguation.
2) Using class-based HMM for word segmentation,
which integrates common words and unknown
ones into a unified frame.
3) Proposing NSP strategy for segmentation dis-
ambiguation.
4) Bringing forth role-based HMM to recognize
simple and recursive unknown words.
</listItem>
<bodyText confidence="0.9997305">
Various experiments show that each level in
HHMM contributes to the final performance.
Evaluation on ICTCLAS confirms that HHMM-
based Chinese lexical analysis is effective.
</bodyText>
<sectionHeader confidence="0.996635" genericHeader="method">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999955777777778">
The authors wish to thank Prof. Shiwen Yu of Pe-
king University for the training corpus. And we
acknowledge our debt to Gang Zou, Dr. Bin Wang,
Dr. Jian Sun, Ji-Feng Li and other colleagues.
Huaping Zhang would especially express gratitude
to his graceful girl friend Feifei and her family for
their encouragement during the hard work. We also
thank three anonymous reviewers for their elabo-
rate and helpful comments.
</bodyText>
<sectionHeader confidence="0.976978" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856544303798">
Andi Wu ,Zixin Jiang. Word Segmentation in Sentence
Analysis. 1998 International Conference on Chinese
Information Processing, Beijing, 1998. 169-180.
Chunyu Kit,Haihua Pan and Hongbiao Chen. Learning
Case- based Knowledge for Disambiguating Chinese
Word Segmentation: A preliminary study. First
SIGHAN Workshop attached with the 19th COLING,
2002.8, pp.63-70
Dai, Y., Khoo, C.S.G. and Loh, T.E. 1999. A new statis-
tical formula for Chinese text segmentation incorpo-
rating contextual information. Proc ACM SIGIR99,
pp. 82-89.
Gao Shan, Zhang Yan. The Research on Integrated
Chinese Word Segmentation and Labeling based on
trigram statistical model, proceeding of natural lan-
guage understanding and machine translation, Beijing,
Tsinghua University Press. 2001.116-122; (in Chinese)
Hockenmaier, J. and Brew, C. .1998. Error-driven
learning of Chinese word segmentation. In J. Guo, K.
T. Lua, and J. Xu, editors, 12th Pacific Conference on
Language and Information, pp. 218-229, Singapore.
Chinese and Oriental Languages Processing Society.
Lawrence. R.Rabiner.1989. A Tutorial on Hidden
Markov Models and Selected Applications in Speech
Recognition. Proceedings of IEEE 77(2): pp.257-286.
Luo Xiao, Sun, Maosong Benjamin K Tsou. Covering
Ambuguity Resolution in Chinese Word Segmentation
Based on Contextual Information. the 19th COLING,
2002.8, pp.598-604
Luo Z. and Song R. 2001. Integrated and Fast Recog-
nition of Proper Noun in Modern Chinese Word Seg-
mentation. Proceedings of International Conference
on Chinese Computing 2001, Singapore, pp. 323-328.
Nianwen Xue and Susan P. Converse. Combining Clas-
sifiers for Chinese Word Segmentation, First SIGHAN
Workshop attached with the 19th COLING, 2002.8,
pp.63-70.
Palmer, D. 1997. .A trainable rule-based algorithm for
word segmentation. Proceedings of the 35th Annual
Meeting of the Association for Computational Lin-
guistics (ACL &apos;97), Madrid, 1997.
Peng, F. and Schuurmans, D. (2001). A hierarchical EM
approach to word segmentation. In 6th Natural Lan-
guage Processing Pacific Rim Symposium (NLPRS-
2001)
Shai Fine, Yoram Singer, and Naftali Tishby.1998. The
hierarchical Hidden Markov Model: Analysis and ap-
plications. Machine Learning, 32:41
Sun J., Gao J. F., Zhang L., Zhou M Huang, C. N.2002.
Chinese Named Entity Identification Using Class-
based Language Model, Proc. of the 19th International
Conference on Computational Linguistics, Taipei, pp
967-973
Sun M.S. (1993) English Transliteration Automatic
Recognition. In &amp;quot;Computational Language Research
and Development&amp;quot;, L. W. Chen &amp; Q. Yuan, ed., Bei-
jing Institute of Linguistic Press.
Tan H. Y. (1999) Chinese Place Automatic Recogni-
tion Research. In &amp;quot;Proceedings of Computational
Language &amp;quot;, C. N. Huang &amp; Z.D. Dong, ed., Tsinghua
Univ. Press, Beijing, China
Teahan, W. J. and Wen, Y. and McNab, R. and Witten I.
H. 2001, A Compression-based Algorithm for Chinese
Word Segmentation. In Comput. Ling., 26(3):375-393.
Xiao Luo, Maosong Sun, Benjamin K Tsou. Covering
Ambuguity Resolution in Chinese Word Segmentation
Based on Contextual Information. the 19th COLING,
2002.8, pp.598-604
Ye S.R, Chua T.S., Liu J. M. 2002. An Agent-based
Approach to Chinese Named Entity Recognition, Proc.
of the 19th International Conference on Computa-
tional Linguistics, Taipei, pp 1149-1155
Zhang Hua-Ping, Liu Qun. Model of Chinese Words
Rough Segmentation Based on N-Shortest-Paths
Method. Journal of Chinese information processing,
2002,16(5):1-7 (in Chinese)
ZHANG Hua-Ping, LIU Qun, Zhang Hao and Cheng
Xue-Qi. 2002. Automatic Recognition of Chinese Un-
known Words Recognition. Proc. of COLING 2002
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.853839">
<title confidence="0.993887">Chinese Lexical Analysis Using Hierarchical Hidden Markov Model</title>
<author confidence="0.976445">Qun Xue-Qi Hao Hong-Kui</author>
<address confidence="0.93923">of Computing Tech., The Chinese Academy of Science, Beijing, 100080 of Computational Linguistics, Peking University, Beijing, 100871</address>
<abstract confidence="0.999203230769231">This paper presents a unified approach Chinese lexical analysis using hierarchical hidden Markov model (HHMM), which to incorporate Chinese word Part-Of-Speech tagging, disambiguation and unknown words recognition into a whole theoretical A class-based HMM is applied in word segmentation, and in this level unknown words are treated in the same way words listed in the lexicon. Unwords are recognized with reliability in role-based HMM. As disambiguation, the authors bring forth an n-shortest-path strategy that, in the early stage, reserves top N segmentation as candidates and covers more ambiguity. Various experiments show that each level in HHMM contributes to lexical An HHMM-based system ICTCLAS was accomplished. The recent official evaluation indicates that ICTCLAS is one of the best Chinese lexical analyzers. In a word, HHMM is effective to Chinese lexical analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andi Wu</author>
</authors>
<title>Word Segmentation in Sentence Analysis.</title>
<date>1998</date>
<booktitle>International Conference on Chinese Information Processing,</booktitle>
<pages>169--180</pages>
<location>Beijing,</location>
<marker>Wu, 1998</marker>
<rawString>Andi Wu ,Zixin Jiang. Word Segmentation in Sentence Analysis. 1998 International Conference on Chinese Information Processing, Beijing, 1998. 169-180.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chunyu Kit</author>
<author>Haihua Pan</author>
<author>Hongbiao Chen</author>
</authors>
<title>Learning Case- based Knowledge for Disambiguating Chinese Word Segmentation: A preliminary study.</title>
<booktitle>First SIGHAN Workshop attached with the 19th COLING,</booktitle>
<volume>2002</volume>
<pages>63--70</pages>
<marker>Kit, Pan, Chen, </marker>
<rawString>Chunyu Kit,Haihua Pan and Hongbiao Chen. Learning Case- based Knowledge for Disambiguating Chinese Word Segmentation: A preliminary study. First SIGHAN Workshop attached with the 19th COLING, 2002.8, pp.63-70</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Dai</author>
<author>C S G Khoo</author>
<author>T E Loh</author>
</authors>
<title>A new statistical formula for Chinese text segmentation incorporating contextual information.</title>
<date>1999</date>
<booktitle>Proc ACM SIGIR99,</booktitle>
<pages>82--89</pages>
<marker>Dai, Khoo, Loh, 1999</marker>
<rawString>Dai, Y., Khoo, C.S.G. and Loh, T.E. 1999. A new statistical formula for Chinese text segmentation incorporating contextual information. Proc ACM SIGIR99, pp. 82-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gao Shan</author>
<author>Zhang Yan</author>
</authors>
<title>The Research on Integrated Chinese Word Segmentation and Labeling based on trigram statistical model, proceeding of natural language understanding and machine translation,</title>
<date>2001</date>
<publisher>University Press.</publisher>
<location>Beijing, Tsinghua</location>
<note>(in Chinese)</note>
<marker>Shan, Yan, 2001</marker>
<rawString>Gao Shan, Zhang Yan. The Research on Integrated Chinese Word Segmentation and Labeling based on trigram statistical model, proceeding of natural language understanding and machine translation, Beijing, Tsinghua University Press. 2001.116-122; (in Chinese)</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Hockenmaier</author>
<author>C Brew</author>
</authors>
<title>Error-driven learning of Chinese word segmentation. In</title>
<date>1998</date>
<booktitle>12th Pacific Conference on Language and Information,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>218--229</pages>
<editor>J. Guo, K. T. Lua, and J. Xu, editors,</editor>
<contexts>
<context position="2466" citStr="Hockenmaier and Brew, 1998" startWordPosition="361" endWordPosition="364">ion. Since the first system CDWS appeared in 1983, word segmentation has been researched intensively. Many solutions were proposed and could be broadly categorized into rules-based approaches that make use of linguistic knowledge and statistical approaches that train on corpus after machine learning. The classic rule-based approaches include maximum matching and shortest path (SP), which achieve the minimum number of segmented words. Zhang and Liu (2002) present an extended SP algorithm named &amp;quot;n-shortest paths&amp;quot;. Some researchers introduce more complicated rules, such as error-driven learning (Hockenmaier and Brew, 1998) and parsing (Wu and Jiang, 1998). Rule is the only feasible way to segment words unless necessary resources such as large amount of corpus are available. With the development of handcorrected resource, statistical approaches became more popular. The language models commonly applied are n-gram (Zhang and Liu, 2002; Gao et al., 2001), EM (Peng and Schuurmans, 2001), and channel noise model. As far as we know, however, there is yet neither purely rule-based system nor purely statistical one. It tends to tackle Chinese lexical problem with mixture of rules and statistical information. On one hand</context>
</contexts>
<marker>Hockenmaier, Brew, 1998</marker>
<rawString>Hockenmaier, J. and Brew, C. .1998. Error-driven learning of Chinese word segmentation. In J. Guo, K. T. Lua, and J. Xu, editors, 12th Pacific Conference on Language and Information, pp. 218-229, Singapore. Chinese and Oriental Languages Processing Society. Lawrence. R.Rabiner.1989. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. Proceedings of IEEE 77(2): pp.257-286.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Luo Xiao</author>
<author>Maosong Benjamin K Tsou Sun</author>
</authors>
<title>Covering Ambuguity Resolution</title>
<booktitle>in Chinese Word Segmentation Based on Contextual Information. the 19th COLING,</booktitle>
<volume>2002</volume>
<pages>598--604</pages>
<marker>Xiao, Sun, </marker>
<rawString>Luo Xiao, Sun, Maosong Benjamin K Tsou. Covering Ambuguity Resolution in Chinese Word Segmentation Based on Contextual Information. the 19th COLING, 2002.8, pp.598-604</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Luo</author>
<author>R Song</author>
</authors>
<title>Integrated and Fast Recognition of Proper Noun in Modern Chinese Word Segmentation.</title>
<date>2001</date>
<booktitle>Proceedings of International Conference on Chinese Computing</booktitle>
<pages>323--328</pages>
<contexts>
<context position="5357" citStr="Luo and Song, 2001" startWordPosition="813" endWordPosition="816">tion attempt to cover each possible case with trivial rules, while recently statistical approaches are applied in some special categories of ambiguity. For instance, vector space model was applied in combinational ambiguity (Luo et al. 2002). Concerning unknown word, we only need focus on unknown named entities, including personal name (PER), location name (LOC), and organization name (ORG). The motivation in named entity recognition is to utilize its components and contexts. Like word segmentation and disambiguation, the usual approach is to apply rules (Sun, 1993;Tan, 1999;Luo and Ji, 2001; Luo and Song, 2001). Recognition rules are summarized on name libraries or different linguistic phenomena. Compared with rules-based approach, machine learning from large corpus seems easy but better in performance. The statistical approaches proposed recently include hidden Markov model (Zhang and Liu, 2002; Zhang et al. 2002), agentbased (Ye, 2003), class-based trigram model (Sun et al., 2002). After nearly 20 years of hard work, rapid progresses are made on word segmentation, disambiguation and unknown word recognition research individually. To the best of our knowledge, however, all the achievement has not 1</context>
</contexts>
<marker>Luo, Song, 2001</marker>
<rawString>Luo Z. and Song R. 2001. Integrated and Fast Recognition of Proper Noun in Modern Chinese Word Segmentation. Proceedings of International Conference on Chinese Computing 2001, Singapore, pp. 323-328.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nianwen Xue</author>
<author>Susan P Converse</author>
</authors>
<title>Combining Classifiers for Chinese Word Segmentation,</title>
<booktitle>First SIGHAN Workshop attached with the 19th COLING,</booktitle>
<volume>2002</volume>
<pages>63--70</pages>
<marker>Xue, Converse, </marker>
<rawString>Nianwen Xue and Susan P. Converse. Combining Classifiers for Chinese Word Segmentation, First SIGHAN Workshop attached with the 19th COLING, 2002.8, pp.63-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Palmer</author>
</authors>
<title>A trainable rule-based algorithm for word segmentation.</title>
<date>1997</date>
<booktitle>Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL &apos;97),</booktitle>
<location>Madrid,</location>
<marker>Palmer, 1997</marker>
<rawString>Palmer, D. 1997. .A trainable rule-based algorithm for word segmentation. Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL &apos;97), Madrid, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>D Schuurmans</author>
</authors>
<title>A hierarchical EM approach to word segmentation.</title>
<date>2001</date>
<booktitle>In 6th Natural Language Processing Pacific Rim Symposium (NLPRS2001)</booktitle>
<contexts>
<context position="2832" citStr="Peng and Schuurmans, 2001" startWordPosition="419" endWordPosition="422">ortest path (SP), which achieve the minimum number of segmented words. Zhang and Liu (2002) present an extended SP algorithm named &amp;quot;n-shortest paths&amp;quot;. Some researchers introduce more complicated rules, such as error-driven learning (Hockenmaier and Brew, 1998) and parsing (Wu and Jiang, 1998). Rule is the only feasible way to segment words unless necessary resources such as large amount of corpus are available. With the development of handcorrected resource, statistical approaches became more popular. The language models commonly applied are n-gram (Zhang and Liu, 2002; Gao et al., 2001), EM (Peng and Schuurmans, 2001), and channel noise model. As far as we know, however, there is yet neither purely rule-based system nor purely statistical one. It tends to tackle Chinese lexical problem with mixture of rules and statistical information. On one hand, trainable rules (Palmer, D. 1997) seem more adaptive and efficient in that rule-based approaches benefit from frequency of rule occurrence, on the other hand, statistical solutions employ rules to detect ambiguity, numeric expression, time and other named entities. Apart from the above approaches, we also notice some other promising ideas such as compression-bas</context>
</contexts>
<marker>Peng, Schuurmans, 2001</marker>
<rawString>Peng, F. and Schuurmans, D. (2001). A hierarchical EM approach to word segmentation. In 6th Natural Language Processing Pacific Rim Symposium (NLPRS2001)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Shai Fine</author>
<author>Yoram Singer</author>
<author>Naftali Tishby 1998</author>
</authors>
<title>The hierarchical Hidden Markov Model: Analysis and applications.</title>
<booktitle>Machine Learning,</booktitle>
<pages>32--41</pages>
<marker>Fine, Singer, 1998, </marker>
<rawString>Shai Fine, Yoram Singer, and Naftali Tishby.1998. The hierarchical Hidden Markov Model: Analysis and applications. Machine Learning, 32:41</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Sun</author>
<author>J F Gao</author>
<author>L Zhang</author>
<author>Zhou M Huang</author>
<author>C N 2002</author>
</authors>
<title>Chinese Named Entity Identification Using Classbased Language Model,</title>
<booktitle>Proc. of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>967--973</pages>
<location>Taipei,</location>
<contexts>
<context position="5736" citStr="Sun et al., 2002" startWordPosition="869" endWordPosition="872">n name (ORG). The motivation in named entity recognition is to utilize its components and contexts. Like word segmentation and disambiguation, the usual approach is to apply rules (Sun, 1993;Tan, 1999;Luo and Ji, 2001; Luo and Song, 2001). Recognition rules are summarized on name libraries or different linguistic phenomena. Compared with rules-based approach, machine learning from large corpus seems easy but better in performance. The statistical approaches proposed recently include hidden Markov model (Zhang and Liu, 2002; Zhang et al. 2002), agentbased (Ye, 2003), class-based trigram model (Sun et al., 2002). After nearly 20 years of hard work, rapid progresses are made on word segmentation, disambiguation and unknown word recognition research individually. To the best of our knowledge, however, all the achievement has not 1 We define unknown words to be those neitheri ncluded in the core lexicon nor recognized through FSA. ever, all the achievement has not integrated into a unified model with a general theoretical basis. In previous lexical analyzers, so-called word segmentation algorithm actually only employs on common words listed in the lexicon, while disambiguation and unknown word recogniti</context>
</contexts>
<marker>Sun, Gao, Zhang, Huang, 2002, </marker>
<rawString>Sun J., Gao J. F., Zhang L., Zhou M Huang, C. N.2002. Chinese Named Entity Identification Using Classbased Language Model, Proc. of the 19th International Conference on Computational Linguistics, Taipei, pp 967-973</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Sun</author>
</authors>
<title>English Transliteration Automatic Recognition. In &amp;quot;Computational Language Research and Development&amp;quot;,</title>
<date>1993</date>
<booktitle>Beijing Institute of Linguistic</booktitle>
<editor>L. W. Chen &amp; Q. Yuan, ed.,</editor>
<publisher>Press.</publisher>
<contexts>
<context position="5309" citStr="Sun, 1993" startWordPosition="807" endWordPosition="808">hang, 1998; Zheng, 1999) of disambiguation attempt to cover each possible case with trivial rules, while recently statistical approaches are applied in some special categories of ambiguity. For instance, vector space model was applied in combinational ambiguity (Luo et al. 2002). Concerning unknown word, we only need focus on unknown named entities, including personal name (PER), location name (LOC), and organization name (ORG). The motivation in named entity recognition is to utilize its components and contexts. Like word segmentation and disambiguation, the usual approach is to apply rules (Sun, 1993;Tan, 1999;Luo and Ji, 2001; Luo and Song, 2001). Recognition rules are summarized on name libraries or different linguistic phenomena. Compared with rules-based approach, machine learning from large corpus seems easy but better in performance. The statistical approaches proposed recently include hidden Markov model (Zhang and Liu, 2002; Zhang et al. 2002), agentbased (Ye, 2003), class-based trigram model (Sun et al., 2002). After nearly 20 years of hard work, rapid progresses are made on word segmentation, disambiguation and unknown word recognition research individually. To the best of our k</context>
</contexts>
<marker>Sun, 1993</marker>
<rawString>Sun M.S. (1993) English Transliteration Automatic Recognition. In &amp;quot;Computational Language Research and Development&amp;quot;, L. W. Chen &amp; Q. Yuan, ed., Beijing Institute of Linguistic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Y Tan</author>
</authors>
<title>Chinese Place Automatic Recognition Research.</title>
<date>1999</date>
<booktitle>In &amp;quot;Proceedings of Computational Language &amp;quot;,</booktitle>
<editor>C. N. Huang &amp; Z.D. Dong, ed., Tsinghua Univ.</editor>
<publisher>Press,</publisher>
<location>Beijing, China</location>
<contexts>
<context position="5319" citStr="Tan, 1999" startWordPosition="808" endWordPosition="809">; Zheng, 1999) of disambiguation attempt to cover each possible case with trivial rules, while recently statistical approaches are applied in some special categories of ambiguity. For instance, vector space model was applied in combinational ambiguity (Luo et al. 2002). Concerning unknown word, we only need focus on unknown named entities, including personal name (PER), location name (LOC), and organization name (ORG). The motivation in named entity recognition is to utilize its components and contexts. Like word segmentation and disambiguation, the usual approach is to apply rules (Sun, 1993;Tan, 1999;Luo and Ji, 2001; Luo and Song, 2001). Recognition rules are summarized on name libraries or different linguistic phenomena. Compared with rules-based approach, machine learning from large corpus seems easy but better in performance. The statistical approaches proposed recently include hidden Markov model (Zhang and Liu, 2002; Zhang et al. 2002), agentbased (Ye, 2003), class-based trigram model (Sun et al., 2002). After nearly 20 years of hard work, rapid progresses are made on word segmentation, disambiguation and unknown word recognition research individually. To the best of our knowledge, </context>
</contexts>
<marker>Tan, 1999</marker>
<rawString>Tan H. Y. (1999) Chinese Place Automatic Recognition Research. In &amp;quot;Proceedings of Computational Language &amp;quot;, C. N. Huang &amp; Z.D. Dong, ed., Tsinghua Univ. Press, Beijing, China</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Teahan</author>
<author>Y Wen</author>
<author>R McNab</author>
<author>I H Witten</author>
</authors>
<title>A Compression-based Algorithm for Chinese Word Segmentation.</title>
<date>2001</date>
<booktitle>In Comput. Ling.,</booktitle>
<pages>26--3</pages>
<contexts>
<context position="3456" citStr="Teahan et al., 2001" startWordPosition="518" endWordPosition="521">d channel noise model. As far as we know, however, there is yet neither purely rule-based system nor purely statistical one. It tends to tackle Chinese lexical problem with mixture of rules and statistical information. On one hand, trainable rules (Palmer, D. 1997) seem more adaptive and efficient in that rule-based approaches benefit from frequency of rule occurrence, on the other hand, statistical solutions employ rules to detect ambiguity, numeric expression, time and other named entities. Apart from the above approaches, we also notice some other promising ideas such as compression-based (Teahan et al., 2001), classifierbased (Xue and Susan, 2002) and self-supervised segmentation without lexicon. According to recent reports, word segmentation has achieved good result in precision, especially on texts that do not contain ambiguity or out-of-vocabulary words. However, segmentation ambiguity and unknown words1 cause bottlenecks and greatly degrade performance in word segmentation. Ambiguous or unknown string is hard to be correctly segmented; at the same time, it also influences on segmenting its neighboring words. What&apos;s worse, ambiguity often occurs with unknown words. Take &amp;quot; A # VN XI, fel fq M g </context>
</contexts>
<marker>Teahan, Wen, McNab, Witten, 2001</marker>
<rawString>Teahan, W. J. and Wen, Y. and McNab, R. and Witten I. H. 2001, A Compression-based Algorithm for Chinese Word Segmentation. In Comput. Ling., 26(3):375-393.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xiao Luo</author>
<author>Maosong Sun</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Covering Ambuguity Resolution</title>
<booktitle>in Chinese Word Segmentation Based on Contextual Information. the 19th COLING,</booktitle>
<volume>2002</volume>
<pages>598--604</pages>
<marker>Luo, Sun, Tsou, </marker>
<rawString>Xiao Luo, Maosong Sun, Benjamin K Tsou. Covering Ambuguity Resolution in Chinese Word Segmentation Based on Contextual Information. the 19th COLING, 2002.8, pp.598-604</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Ye</author>
<author>T S Chua</author>
<author>J M Liu</author>
</authors>
<title>An Agent-based Approach to Chinese Named Entity Recognition,</title>
<date>2002</date>
<booktitle>Proc. of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1149--1155</pages>
<location>Taipei,</location>
<marker>Ye, Chua, Liu, 2002</marker>
<rawString>Ye S.R, Chua T.S., Liu J. M. 2002. An Agent-based Approach to Chinese Named Entity Recognition, Proc. of the 19th International Conference on Computational Linguistics, Taipei, pp 1149-1155</rawString>
</citation>
<citation valid="false">
<authors>
<author>Zhang Hua-Ping</author>
</authors>
<title>Liu Qun. Model of Chinese Words Rough Segmentation Based on N-Shortest-Paths Method.</title>
<journal>Journal of Chinese</journal>
<note>information processing, 2002,16(5):1-7 (in Chinese)</note>
<marker>Hua-Ping, </marker>
<rawString>Zhang Hua-Ping, Liu Qun. Model of Chinese Words Rough Segmentation Based on N-Shortest-Paths Method. Journal of Chinese information processing, 2002,16(5):1-7 (in Chinese)</rawString>
</citation>
<citation valid="true">
<authors>
<author>ZHANG Hua-Ping</author>
<author>LIU Qun</author>
<author>Zhang Hao</author>
<author>Cheng Xue-Qi</author>
</authors>
<title>Automatic Recognition of Chinese Unknown Words Recognition.</title>
<date>2002</date>
<booktitle>Proc. of COLING</booktitle>
<marker>Hua-Ping, Qun, Hao, Xue-Qi, 2002</marker>
<rawString>ZHANG Hua-Ping, LIU Qun, Zhang Hao and Cheng Xue-Qi. 2002. Automatic Recognition of Chinese Unknown Words Recognition. Proc. of COLING 2002</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>