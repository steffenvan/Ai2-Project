<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056440">
<title confidence="0.9994015">
Predicting the fluency of text with shallow structural features: case studies
of machine translation and human-written text
</title>
<author confidence="0.996631">
Jieun Chae
</author>
<affiliation confidence="0.998244">
University of Pennsylvania
</affiliation>
<email confidence="0.997802">
chaeji@seas.upenn.edu
</email>
<author confidence="0.987289">
Ani Nenkova
</author>
<affiliation confidence="0.996032">
University of Pennsylvania
</affiliation>
<email confidence="0.998341">
nenkova@seas.upenn.edu
</email>
<sectionHeader confidence="0.993885" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998096">
Sentence fluency is an important compo-
nent of overall text readability but few
studies in natural language processing
have sought to understand the factors that
define it. We report the results of an ini-
tial study into the predictive power of sur-
face syntactic statistics for the task; we use
fluency assessments done for the purpose
of evaluating machine translation. We
find that these features are weakly but sig-
nificantly correlated with fluency. Ma-
chine and human translations can be dis-
tinguished with accuracy over 80%. The
performance of pairwise comparison of
fluency is also very high—over 90% for a
multi-layer perceptron classifier. We also
test the hypothesis that the learned models
capture general fluency properties applica-
ble to human-written text. The results do
not support this hypothesis: prediction ac-
curacy on the new data is only 57%. This
finding suggests that developing a dedi-
cated, task-independent corpus of fluency
judgments will be beneficial for further in-
vestigations of the problem.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99943585106383">
Numerous natural language applications involve
the task of producing fluent text. This is a core
problem for surface realization in natural language
generation (Langkilde and Knight, 1998; Banga-
lore and Rambow, 2000), as well as an impor-
tant step in machine translation. Considerations
of sentence fluency are also key in sentence sim-
plification (Siddharthan, 2003), sentence compres-
sion (Jing, 2000; Knight and Marcu, 2002; Clarke
and Lapata, 2006; McDonald, 2006; Turner and
Charniak, 2005; Galley and McKeown, 2007), text
re-generation for summarization (Daum´e III and
Marcu, 2004; Barzilay and McKeown, 2005; Wan
et al., 2005) and headline generation (Banko et al.,
2000; Zajic et al., 2007; Soricut and Marcu, 2007).
Despite its importance for these popular appli-
cations, the factors contributing to sentence level
fluency have not been researched indepth. Much
more attention has been devoted to discourse-level
constraints on adjacent sentences indicative of co-
herence and good text flow (Lapata, 2003; Barzi-
lay and Lapata, 2008; Karamanis et al., to appear).
In many applications fluency is assessed in
combination with other qualities. For example, in
machine translation evaluation, approaches such
as BLEU (Papineni et al., 2002) use n-gram over-
lap comparisons with a model to judge overall
“goodness”, with higher n-grams meant to capture
fluency considerations. More sophisticated ways
to compare a system production and a model in-
volve the use of syntax, but even in these cases flu-
ency is only indirectly assessed and the main ad-
vantage of the use of syntax is better estimation of
the semantic overlap between a model and an out-
put. Similarly, the metrics proposed for text gener-
ation by (Bangalore et al., 2000) (simple accuracy,
generation accuracy) are based on string-edit dis-
tance from an ideal output.
In contrast, the work of (Wan et al., 2005)
and (Mutton et al., 2007) directly sets as a goal
the assessment of sentence-level fluency, regard-
less of content. In (Wan et al., 2005) the main
premise is that syntactic information from a parser
can more robustly capture fluency than language
models, giving more direct indications of the de-
gree of ungrammaticality. The idea is extended in
(Mutton et al., 2007), where four parsers are used
</bodyText>
<note confidence="0.9238245">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 139–147,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998577">
139
</page>
<bodyText confidence="0.999188983606558">
and artificially generated sentences with varying
level of fluency are evaluated with impressive suc-
cess. The fluency models hold promise for ac-
tual improvements in machine translation output
quality (Zwarts and Dras, 2008). In that work,
only simple parser features are used for the pre-
diction of fluency, but no actual syntactic prop-
erties of the sentences. But certainly, problems
with sentence fluency are expected to be mani-
fested in syntax. We would expect for example
that syntactic tree features that capture common
parse configurations and that are used in discrim-
inative parsing (Collins and Koo, 2005; Charniak
and Johnson, 2005; Huang, 2008) should be use-
ful for predicting sentence fluency as well. In-
deed, early work has demonstrated that syntac-
tic features, and branching properties in particular,
are helpful features for automatically distinguish-
ing human translations from machine translations
(Corston-Oliver et al., 2001). The exploration of
branching properties of human and machine trans-
lations was motivated by the observations during
failure analysis that MT system output tends to
favor right-branching structures over noun com-
pounding. Branching preference mismatch man-
ifest themselves in the English output when trans-
lating from languages whose branching properties
are radically different from English. Accuracy
close to 80% was achieved for distinguishing hu-
man translations from machine translations.
In our work we continue the investigation of
sentence level fluency based on features that cap-
ture surface statistics of the syntactic structure in
a sentence. We revisit the task of distinguishing
machine translations from human translations, but
also further our understanding of fluency by pro-
viding comprehensive analysis of the association
between fluency assessments of translations and
surface syntactic features. We also demonstrate
that based on the same class of features, it is possi-
ble to distinguish fluent machine translations from
disfluent machine translations. Finally, we test the
models on human written text in order to verify
if the classifiers trained on data coming from ma-
chine translation evaluations can be used for gen-
eral predictions of fluency and readability.
For our experiments we use the evaluations
of Chinese to English translations distributed by
LDC (catalog number LDC2003T17), for which
both machine and human translations are avail-
able. Machine translations have been assessed
by evaluators for fluency on a five point scale (5:
flawless English; 4: good English; 3: non-native
English; 2: disfluent English; 1: incomprehen-
sible). Assessments by different annotators were
averaged to assign overall fluency assessment for
each machine-translated sentence. For each seg-
ment (sentence), there are four human and three
machine translations.
In this setting we address four tasks with in-
creasing difficulty:
</bodyText>
<listItem confidence="0.9849204">
• Distinguish human and machine translations.
• Distinguish fluent machine translations from
poor machine translations.
• Distinguish the better (in terms of fluency)
translation among two translations of the
same input segment.
• Use the models trained on data from MT
evaluations to predict potential fluency prob-
lems of human-written texts (from the Wall
Street Journal).
</listItem>
<bodyText confidence="0.999689071428571">
Even for the last most challenging task results
are promising, with prediction accuracy almost
10% better than a random baseline. For the other
tasks accuracies are high, exceeding 80%.
It is important to note that the purpose of our
study is not evaluation of machine translation per
se. Our goal is more general and the interest is in
finding predictors of sentence fluency. No general
corpora exist with fluency assessments, so it seems
advantageous to use the assessments done in the
context of machine translation for preliminary in-
vestigations of fluency. Nevertheless, our findings
are also potentially beneficial for sentence-level
evaluation of machine translation.
</bodyText>
<sectionHeader confidence="0.996541" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.999861153846154">
Perceived sentence fluency is influenced by many
factors. The way the sentence fits in the con-
text of surrounding sentences is one obvious factor
(Barzilay and Lapata, 2008). Another well-known
factor is vocabulary use: the presence of uncom-
mon difficult words are known to pose problems
to readers and to render text less readable (Collins-
Thompson and Callan, 2004; Schwarm and Osten-
dorf, 2005). But these discourse- and vocabulary-
level features measure properties at granularities
different from the sentence level.
Syntactic sentence level features have not been
investigated as a stand-alone class, as has been
</bodyText>
<page confidence="0.994366">
140
</page>
<bodyText confidence="0.9998735">
done for the other types of features. This is why
we constrain our study to syntactic features alone,
and do not discuss discourse and language model
features that have been extensively studied in prior
work on coherence and readability.
In our work, instead of looking at the syntac-
tic structures present in the sentences, e.g. the
syntactic rules used, we use surface statistics of
phrase length and types of modification. The sen-
tences were parsed with Charniak’s parser (Char-
niak, 2000) in order to calculate these features.
Sentence length is the number of words in a sen-
tence. Evaluation metrics such as BLEU (Papineni
et al., 2002) have a built-in preference for shorter
translations. In general one would expect that
shorter sentences are easier to read and thus are
perceived as more fluent. We added this feature
in order to test directly the hypothesis for brevity
preference.
Parse tree depth is considered to be a measure
of sentence complexity. Generally, longer sen-
tences are syntactically more complex but when
sentences are approximately the same length the
larger parse tree depth can be indicative of in-
creased complexity that can slow processing and
lead to lower perceived fluency of the sentence.
Number offragment tags in the sentence parse
Out of the 2634 total sentences, only 165 con-
tained a fragment tag in their parse, indicating
the presence of ungrammaticality in the sentence.
Fragments occur in headlines (e.g. “Cheney will-
ing to hold bilateral talks if Arafat observes U.S.
cease-fire arrangement”) but in machine transla-
tion the presence of fragments can signal a more
serious problem.
Phrase type proportion was computed for
prepositional phrases (PP), noun phrases (NP)
and verb phrases (VP). The length in number of
words of each phrase type was counted, then di-
vided by the sentence length. Embedded phrases
were also included in the calculation: for ex-
ample a noun phrase (NP1 ... (NP2)) would
contribute length(NP1) + length(NP2) to the
phrase length count.
Average phrase length is the number of words
comprising a given type of phrase, divided by the
number of phrases of this type. It was computed
for PP, NP, VP, ADJP, ADVP. Two versions of
the features were computed—one with embedded
phrases included in the calculation and one just for
the largest phrases of a given type. Normalized av-
erage phrase length is computed for PP, NP and
VP and is equal to the average phrase length of
given type divided by the sentence length. These
were computed only for the largest phrases.
Phrase type rate was also computed for PPs,
VPs and NPs and is equal to the number of phrases
of the given type that appeared in the sentence, di-
vided by the sentence length. For example, the
sentence “The boy caught a huge fish this morn-
ing” will have NP phrase number equal to 3/8 and
VP phrase number equal to 1/8.
Phrase length The number of words in a PP,
NP, VP, without any normalization; it is computed
only for the largest phrases. Normalized phrase
length is the average phrase length (for VPs, NPs,
PPs) divided by the sentence length. This was
computed both for longest phrase (where embed-
ded phrases of the same type were counted only
once) and for each phrase regardless of embed-
ding.
Length of NPs/PPs contained in a VP The aver-
age number of words that constitute an NP or PP
within a verb phrase, divided by the length of the
verb phrase. Similarly, the length of PP in NP was
computed.
Head noun modifiers Noun phrases can be very
complex, and the head noun can be modified in va-
riety of ways—pre-modifiers, prepositional phrase
modifiers, apposition. The length in words of
these modifiers was calculated. Each feature also
had a variant in which the modifier length was di-
vided by the sentence length. Finally, two more
features on total modification were computed: one
was the sum of all modifier lengths, the other the
sum of normalized modifier length.
</bodyText>
<sectionHeader confidence="0.868313" genericHeader="method">
3 Feature analysis
</sectionHeader>
<bodyText confidence="0.9997275">
In this section, we analyze the association of the
features that we described above and fluency. Note
that the purpose of the analysis is not feature
selection—all features will be used in the later ex-
periments. Rather, the analysis is performed in or-
der to better understand which factors are predic-
tive of good fluency.
The distribution of fluency scores in the dataset
is rather skewed, with the majority of the sen-
tences rated as being of average fluency 3 as can
be seen in Table 1.
Pearson’s correlation between the fluency rat-
ings and features are shown in Table 2. First of all,
fluency and adequacy as given by MT evaluators
</bodyText>
<page confidence="0.980093">
141
</page>
<figure confidence="0.982404">
Fluency score The number of sentences
1 &lt; fluency &lt; 2 7
1 &lt; fluency &lt; 2 295
2 &lt; fluency &lt; 3 1789
3 &lt; fluency &lt; 4 521
4 &lt; fluency &lt; 5 22
</figure>
<tableCaption confidence="0.923788">
Table 1: Distribution of fluency scores.
</tableCaption>
<bodyText confidence="0.99955246875">
are highly correlated (0.7). This is surprisingly
high, given that separate fluency and adequacy as-
sessments were elicited with the idea that these
are qualities of the translations that are indepen-
dent of each other. Fluency was judged directly by
the assessors, while adequacy was meant to assess
the content of the sentence compared to a human
gold-standard. Yet, the assessments of the two
aspects were often the same—readability/fluency
of the sentence is important for understanding the
sentence. Only after the assessor has understood
the sentence can (s)he judge how it compares to
the human model. One can conclude then that a
model of fluency/readability that will allow sys-
tems to produce fluent text is key for developing a
successful machine translation system.
The next feature most strongly associated with
fluency is sentence length. Shorter sentences are
easier and perceived as more fluent than longer
ones, which is not surprising. Note though that the
correlation is actually rather weak. It is only one
of various fluency factors and has to be accommo-
dated alongside the possibly conflicting require-
ments shown by the other features. Still, length
considerations reappear at sub-sentential (phrasal)
levels as well.
Noun phrase length for example has almost the
same correlation with fluency as sentence length
does. The longer the noun phrases, the less fluent
the sentence is. Long noun phrases take longer to
interpret and reduce sentence fluency/readability.
Consider the following example:
</bodyText>
<listItem confidence="0.9996115">
• [The dog] jumped over the fence and fetched the ball.
• [The big dog in the corner] fetched the ball.
</listItem>
<bodyText confidence="0.9927805">
The long noun phrase is more difficult to read,
especially in subject position. Similarly the length
of the verb phrases signal potential fluency prob-
lems:
</bodyText>
<listItem confidence="0.99340875">
• Most of the US allies in Europe publicly [object to in-
vading Iraq]V P.
• But this [is dealing against some recent remarks of
Japanese financial minister, Masajuro Shiokawa]V P.
</listItem>
<bodyText confidence="0.999207333333333">
VP distance (the average number of words sep-
arating two verb phrases) is also negatively corre-
lated with sentence fluency. In machine transla-
tions there is the obvious problem that they might
not include a verb for long stretches of text. But
even in human written text, the presence of more
verbs can make a difference in fluency (Bailin and
Grafstein, 2001). Consider the following two sen-
tences:
</bodyText>
<listItem confidence="0.99162">
• In his state of the Union address, Putin also talked
about the national development plan for this fiscal year
and the domestic and foreign policies.
• Inside the courtyard of the television station, a recep-
</listItem>
<bodyText confidence="0.917423363636364">
tion team of 25 people was formed to attend to those
who came to make donations in person.
The next strongest correlation is with unnormal-
ized verb phrase length. In fact in terms of correla-
tions, in turned out that it was best not to normal-
ize the phrase length features at all. The normal-
ized versions were also correlated with fluency,
but the association was lower than for the direct
count without normalization.
Parse tree depth is the final feature correlated
with fluency with correlation above 0.1.
</bodyText>
<sectionHeader confidence="0.9924295" genericHeader="method">
4 Experiments with machine translation
data
</sectionHeader>
<subsectionHeader confidence="0.9261985">
4.1 Distinguishing human from machine
translations
</subsectionHeader>
<bodyText confidence="0.999952722222222">
In this section we use all the features discussed in
Section 2 for several classification tasks. Note that
while we discussed the high correlation between
fluency and adequacy, we do not use adequacy in
the experiments that we report from here on.
For all experiments we used four of the classi-
fiers in Weka—decision tree (J48), logistic regres-
sion, support vector machines (SMO), and multi-
layer perceptron. All results are for 10-fold cross
validation.
We extracted the 300 sentences with highest flu-
ency scores, 300 sentences with lowest fluency
scores among machine translations and 300 ran-
domly chosen human translations. We then tried
the classification task of distinguishing human and
machine translations with different fluency quality
(highest fluency scores vs. lowest fluency score).
We expect that low fluency MT will be more easily
</bodyText>
<page confidence="0.988537">
142
</page>
<table confidence="0.999702166666667">
adequacy sentence length unnormalized NP length VP distance
0.701(0.00) -0.132(0.00) -0.124(0.00) -0.116(0.00)
unnormalized VP length Max Tree depth phrase length avr. NP length (embedded)
-0.109(0.00) -0.106(0.00) -0.105(0.00) -0.097(0.00)
avr. VP length (embedded) SBAR length avr. largest NP length Unnormalized PP
-0.094(0.00) -0.086(0.00) -0.084(0.00) -0.082(0.00)
avr PP length (embedded) SBAR count PP length in VP Normalized PP1
-0.070(0.00) -0.069(0.001) -0.066(0.001) 0.065(0.001)
NP length in VP PP length normalized VP length PP length in NP
-0.058(0.003) -0.054(0.006) 0.054(0.005) 0.053(0.006)
Fragment avr. ADJP length (embedded) avr. largest VP length
-0.049(0.011) -0.046(0.019) -0.038(0.052)
</table>
<tableCaption confidence="0.8760725">
Table 2: Pearson’s correlation coefficient between fluency and syntactic phrasing features. P-values are
given in parenthesis.
</tableCaption>
<table confidence="0.999854">
worst 300 MT best 300 MT total MT (5920)
SMO 86.00% 78.33% 82.68%
Logistic reg. 77.16% 79.33% 82.68%
MLP 78.00% 82% 86.99%
Decision Tree(J48) 71.67 % 81.33% 86.11%
</table>
<tableCaption confidence="0.998647">
Table 3: Accuracy for the task of distinguishing machine and human translations.
</tableCaption>
<bodyText confidence="0.999824892857143">
distinguished from human translation in compari-
son with machine translations rated as having high
fluency.
Results are shown in Table 3. Overall the
best classifier is the multi-layer perceptron. On
the task using all available data of machine and
human translations, the classification accuracy is
86.99%. We expected that distinguishing the ma-
chine translations from the human ones will be
harder when the best translations are used, com-
pared to the worse translations, but this expecta-
tion is fulfilled only for the support vector machine
classifier.
The results in Table 3 give convincing evi-
dence that the surface structural statistics can dis-
tinguish very well between fluent and non-fluent
sentences when the examples come from human
and machine-produced text respectively. If this is
the case, will it be possible to distinguish between
good and bad machine translations as well? In or-
der to answer this question, we ran one more bi-
nary classification task. The two classes were the
300 machine translations with highest and lowest
fluency respectively. The results are not as good as
those for distinguishing machine and human trans-
lation, but still significantly outperform a random
baseline. All classifiers performed similarly on the
task, and achieved accuracy close to 61%.
</bodyText>
<subsectionHeader confidence="0.996984">
4.2 Pairwise fluency comparisons
</subsectionHeader>
<bodyText confidence="0.999926551724138">
We also considered the possibility of pairwise
comparisons for fluency: given two sentences,
can we distinguish which is the one scored more
highly for fluency. For every two sentences, the
feature for the pair is the difference of features of
the individual sentences.
There are two ways this task can be set up. First,
we can use all assessed translations and make pair-
ings for every two sentences with different fluency
assessment. In this setting, the question being ad-
dressed is Can sentences with differing fluency be
distinguished?, without regard to the sources of
the sentence. The harder question is Can a more
fluent translation be distinguished from a less flu-
ent translation of the same sentence?
The results from these experiments can be seen
in Table 4. When any two sentences with differ-
ent fluency assessments are paired, the prediction
accuracy is very high: 91.34% for the multi-layer
perceptron classifier. In fact all classifiers have ac-
curacy higher than 80% for this task. The surface
statistics of syntactic form are powerful enough to
distinguishing sentences of varying fluency.
The task of pairwise comparison for translations
of the same input is more difficult: doing well on
this task would be equivalent to having a reliable
measure for ranking different possible translation
variants.
In fact, the problem is much more difficult as
</bodyText>
<page confidence="0.997772">
143
</page>
<table confidence="0.999786">
Task J48 Logistic Regression SMO MLP
Any pair 89.73% 82.35% 82.38% 91.34%
Same Sentence 67.11% 70.91% 71.23% 69.18%
</table>
<tableCaption confidence="0.973565">
Table 4: Accuracy for pairwise fluency comparison. “Same sentence” are comparisons constrained
</tableCaption>
<bodyText confidence="0.981694909090909">
between different translations of the same sentences, “any pair” contains comparisons of sentences with
different fluency over the entire data set.
can be seen in the second row of Table 4. Lo-
gistic regression, support vector machines and
multi-layer perceptron perform similarly, with
support vector machine giving the best accuracy
of 71.23%. This number is impressively high, and
significantly higher than baseline performance.
The results are about 20% lower than for predic-
tion of a more fluent sentence when the task is not
constrained to translation of the same sentence.
</bodyText>
<subsectionHeader confidence="0.99938">
4.3 Feature analysis: differences among tasks
</subsectionHeader>
<bodyText confidence="0.999985565217391">
In the previous sections we presented three varia-
tions involving fluency predictions based on syn-
tactic phrasing features: distinguishing human
from machine translations, distinguishing good
machine translations from bad machine transla-
tions, and pairwise ranking of sentences with dif-
ferent fluency. The results differ considerably and
it is interesting to know whether the same kind
of features are useful in making the three distinc-
tions.
In Table 5 we show the five features with largest
weight in the support vector machine model for
each task. In many cases, certain features appear
to be important only for particular tasks. For ex-
ample the number of prepositional phrases is an
important feature only for ranking different ver-
sions of the same sentence but is not important for
other distinctions. The number of appositions is
helpful in distinguishing human translations from
machine translations, but is not that useful in the
other tasks. So the predictive power of the features
is very directly related to the variant of fluency dis-
tinctions one is interested in making.
</bodyText>
<sectionHeader confidence="0.986951" genericHeader="method">
5 Applications to human written text
</sectionHeader>
<subsectionHeader confidence="0.9982135">
5.1 Identifying hard-to-read sentences in
Wall Street Journal texts
</subsectionHeader>
<bodyText confidence="0.999957829787234">
The goal we set out in the beginning of this pa-
per was to derive a predictive model of sentence
fluency from data coming from MT evaluations.
In the previous sections, we demonstrated that
indeed structural features can enable us to per-
form this task very accurately in the context of
machine translation. But will the models conve-
niently trained on data from MT evaluation be at
all capable to identify sentences in human-written
text that are not fluent and are difficult to under-
stand?
To answer this question, we performed an ad-
ditional experiment on 30 Wall Street Journal ar-
ticles from the Penn Treebank that were previ-
ously used in experiments for assessing overall
text quality (Pitler and Nenkova, 2008). The arti-
cles were chosen at random and comprised a to-
tal of 290 sentences. One human assessor was
asked to read each sentence and mark the ones that
seemed disfluent because they were hard to com-
prehend. These were sentences that needed to be
read more than once in order to fully understand
the information conveyed in them. There were 52
such sentences. The assessments served as a gold-
standard against which the predictions of the flu-
ency models were compared.
Two models trained on machine translation data
were used to predict the status of each sentence in
the WSJ articles. One of the models was that for
distinguishing human translations from machine
translations (human vs machine MT), the other
was the model for distinguishing the 300 best from
the 300 worst machine translations (good vs bad
MT). The classifiers used were decision trees for
human vs machine distinction and support vector
machines for good vs bad MT. For the first model
sentences predicted to belong to the “human trans-
lation” class are considered fluent; for the second
model fluent sentences are the ones predicted to be
in the “best MT” class.
The results are shown in Table 6. The two
models vastly differ in performance. The model
for distinguishing machine translations from hu-
man translations is the better one, with accuracy
of 57%. For both, prediction accuracy is much
lower than when tested on data from MT evalu-
ations. These findings indicate that building a new
</bodyText>
<page confidence="0.99537">
144
</page>
<table confidence="0.995835">
MT vs HT good MT vs Bad MT Ranking Same sentence Ranking
unnormalized PP SBAR count avr. NP lengt normalized NP length
PP length in VP Unnormalized VP length normalized PP length PP count
avr. NP length post attribute length NP count normalized NP length
# apposition VP count normalized NP length max tree depth
SBAR length sentence length normalized VP length avr. phrase length
</table>
<tableCaption confidence="0.9841195">
Table 5: The five features with highest weights in the support vector machine model for the different
tasks.
</tableCaption>
<table confidence="0.992491">
Model Acc P R
human vs machine trans. 57% 0.79 0.58
good MT vs bad MT 44% 0.57 0.44
</table>
<tableCaption confidence="0.985288">
Table 6: Accuracy, precision and recall (for fluent
</tableCaption>
<bodyText confidence="0.953114361702128">
class) for each model when test on WSJ sentences.
The gold-standard is assessment by a single reader
of the text.
corpus for the finer fluency distinctions present in
human-written text is likely to be more beneficial
than trying to leverage data from existing MT eval-
uations.
Below, we show several example sentences on
which the assessor and the model for distinguish-
ing human and machine translations (dis)agreed.
Model and assessor agree that sentence is prob-
lematic:
(1.1) The Soviet legislature approved a 1990 budget yes-
terday that halves its huge deficit with cuts in defense spend-
ing and capital outlays while striving to improve supplies to
frustrated consumers.
(1.2) Officials proposed a cut in the defense budget this
year to 70.9 billion rubles (US$114.3 billion) from 77.3 bil-
lion rubles (US$125 billion) as well as large cuts in outlays
for new factories and equipment.
(1.3) Rather, the two closely linked exchanges have been
drifting apart for some years, with a nearly five-year-old
moratorium on new dual listings, separate and different list-
ing requirements, differing trading and settlement guidelines
and diverging national-policy aims.
The model predicts the sentence is good, but the
assessor finds it problematic:
(2.1) Moody’s Investors Service Inc. said it lowered the
ratings of some $145 million of Pinnacle debt because of
”accelerating deficiency in liquidity,” which it said was ev-
idenced by Pinnacle’s elimination of dividend payments.
(2.2) Sales were higher in all of the company’s business
categories, with the biggest growth coming in sales of food-
stuffs such as margarine, coffee and frozen food, which rose
6.3%.
(2.3) Ajinomoto predicted sales in the current fiscal year
ending next March 31 of 480 billion yen, compared with
460.05 billion yen in fiscal 1989.
The model predicts the sentences are bad, but
the assessor considered them fluent:
(3.1) The sense grows that modern public bureaucracies
simply don’t perform their assigned functions well.
(3.2) Amstrad PLC, a British maker of computer hardware
and communications equipment, posted a 52% plunge in pre-
tax profit for the latest year.
(3.3) At current allocations, that means EPA will be spend-
ing $300 billion on itself.
</bodyText>
<subsectionHeader confidence="0.999618">
5.2 Correlation with overall text quality
</subsectionHeader>
<bodyText confidence="0.999997458333333">
In our final experiment we focus on the relation-
ship between sentence fluency and overall text
quality. We would expect that the presence of dis-
fluent sentences in text will make it appear less
well written. Five annotators had previously as-
sess the overall text quality of each article on a
scale from 1 to 5 (Pitler and Nenkova, 2008). The
average of the assessments was taken as a single
number describing the article. The correlation be-
tween this number and the percentage of fluent
sentences in the article according to the different
models is shown in Table 7.
The correlation between the percentage of flu-
ent sentences in the article as given by the human
assessor and the overall text quality is rather low,
0.127. The positive correlation would suggest that
the more hard to read sentence appear in a text,
the higher the text would be rated overall, which
is surprising. The predictions from the model for
distinguishing good and bad machine translations
very close to zero, but negative which corresponds
better to the intuitive relationship between the two.
Note that none of the correlations are actually
significant for the small dataset of 30 points.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999145">
We presented a study of sentence fluency based on
data from machine translation evaluations. These
data allow for two types of comparisons: human
(fluent) text and (not so good) machine-generated
</bodyText>
<page confidence="0.994284">
145
</page>
<table confidence="0.917671">
Fluency given by Correlation
human 0.127
human vs machine trans. model -0.055
good MT vs bad MT model 0.076
</table>
<tableCaption confidence="0.967497">
Table 7: Correlations between text quality assess-
</tableCaption>
<bodyText confidence="0.997975583333333">
ment of the articles and the percentage of fluent
sentences according to different models.
text, and levels of fluency in the automatically pro-
duced text. The distinctions were possible even
when based solely on features describing syntac-
tic phrasing in the sentences.
Correlation analysis reveals that the structural
features are significant but weakly correlated with
fluency. Interestingly, the features correlated with
fluency levels in machine-produced text are not the
same as those that distinguish between human and
machine translations. Such results raise the need
for caution when using assessments for machine
produced text to build a general model of fluency.
The captured phenomena in this case might be
different than these from comparing human texts
with differing fluency. For future research it will
be beneficial to build a dedicated corpus in which
human-produced sentences are assessed for flu-
ency.
Our experiments show that basic fluency dis-
tinctions can be made with high accuracy. Ma-
chine translations can be distinguished from hu-
man translations with accuracy of 87%; machine
translations with low fluency can be distinguished
from machine translations with high fluency with
accuracy of 61%. In pairwise comparison of sen-
tences with different fluency, accuracy of predict-
ing which of the two is better is 90%. Results are
not as high but still promising for comparisons in
fluency of translations of the same text. The pre-
diction becomes better when the texts being com-
pared exhibit larger difference in fluency quality.
Admittedly, our pilot experiments with human
assessment of text quality and sentence level flu-
ency are small, so no big generalizations can be
made. Still, they allow some useful observations
that can guide future work. They do show that for
further research in automatic recognition of flu-
ency, new annotated corpora developed specially
for the task will be necessary. They also give
some evidence that sentence-level fluency is only
weakly correlated with overall text quality. Dis-
course apects and language model features that
have been extensively studied in prior work are in-
deed much more indicative of overall text quality
(Pitler and Nenkova, 2008). We leave direct com-
parison for future work.
</bodyText>
<sectionHeader confidence="0.990144" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999031195652174">
A. Bailin and A. Grafstein. 2001. The linguistic as-
sumptions underlying readability formulae: a cri-
tique. Language and Communication, 21:285–301.
S. Bangalore and O. Rambow. 2000. Exploiting a
probabilistic hierarchical model for generation. In
COLING, pages 42–48.
S. Bangalore, O. Rambow, and S. Whittaker. 2000.
Evaluation metrics for generation. In INLG’00:
Proceedings of the first international conference on
Natural language generation, pages 1–8.
M. Banko, V. Mittal, and M. Witbrock. 2000. Head-
line generation based on statistical translation. In
Proceedings of the 38th Annual Meeting of the As-
sociation for Co mputational Linguistics.
R. Barzilay and M. Lapata. 2008. Modeling local co-
herence: An entity-based approach. Computational
Linguistics, 34(1):1–34.
R. Barzilay and K. McKeown. 2005. Sentence fusion
for multidocument news summarization. Computa-
tional Linguistics, 31(3).
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL’05), pages 173–180.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In NAACL-2000.
J. Clarke and M. Lapata. 2006. Models for sen-
tence compression: A comparison across domains,
training requirements and evaluation measures. In
ACL:COLING’06, pages 377–384.
M. Collins and T. Koo. 2005. Discriminative rerank-
ing for natural language parsing. Comput. Linguist.,
31(1):25–70.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty.
In Proceedings of HLT/NAACL’04.
S. Corston-Oliver, M. Gamon, and C. Brockett. 2001.
A machine learning approach to the automatic eval-
uation of machine translation. In Proceedings of
39th Annual Meeting of the Association for Compu-
tational Linguistics, pages 148–155.
H. Daum´e III and D. Marcu. 2004. Generic sentence
fusion is an ill-defined summarization task. In Pro-
ceedings of the Text Summarization Branches Out
Workshop at ACL.
</reference>
<page confidence="0.987524">
146
</page>
<reference confidence="0.999911757575758">
M. Galley and K. McKeown. 2007. Lexicalized
markov grammars for sentence compression. In
Proceedings of Human Language Technologies: The
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT).
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586–594.
H. Jing. 2000. Sentence simplification in automatic
text summarization. In Proceedings of the 6th Ap-
plied NLP Conference, ANLP’2000.
N. Karamanis, M. Poesio, C. Mellish, and J. Oberlan-
der. (to appear). Evaluating centering for infor-
mation ordering using corpora. Computational Lin-
guistics.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1).
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704–710.
Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. In Proceed-
ings of ACL’03.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In EACL’06.
A. Mutton, M. Dras, S. Wan, and R. Dale. 2007. Gleu:
Automatic evaluation of sentence-level fluency. In
ACL’07, pages 344–351.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
186–195.
S. Schwarm and M. Ostendorf. 2005. Reading level
assessment using support vector machines and sta-
tistical language models. In Proceedings ofACL’05,
pages 523–530.
A. Siddharthan. 2003. Syntactic simplification and
Text Cohesion. Ph.D. thesis, University of Cam-
bridge, UK.
R. Soricut and D. Marcu. 2007. Abstractive head-
line generation using widl-expressions. Inf. Process.
Manage., 43(6):1536–1548.
J. Turner and E. Charniak. 2005. Supervised and un-
supervised learning for sentence compression. In
ACL’05.
S. Wan, R. Dale, and M. Dras. 2005. Searching
for grammaticality: Propagating dependencies in the
viterbi algorithm. In Proceedings of the Tenth Eu-
ropean Workshop on Natural Language Generation
(ENLG-05).
D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2007.
Multi-candidate reduction: Sentence compression as
a tool for document summarization tasks. Inf. Pro-
cess. Manage., 43(6):1549–1570.
S. Zwarts and M. Dras. 2008. Choosing the right
translation: A syntactically informed classification
approach. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 1153–1160.
</reference>
<page confidence="0.9981">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.886112">
<title confidence="0.980102">Predicting the fluency of text with shallow structural features: case studies of machine translation and human-written text</title>
<author confidence="0.985047">Jieun Chae</author>
<affiliation confidence="0.999907">University of Pennsylvania</affiliation>
<email confidence="0.999542">chaeji@seas.upenn.edu</email>
<author confidence="0.953708">Ani Nenkova</author>
<affiliation confidence="0.999751">University of Pennsylvania</affiliation>
<email confidence="0.999677">nenkova@seas.upenn.edu</email>
<abstract confidence="0.998991423076923">Sentence fluency is an important component of overall text readability but few studies in natural language processing have sought to understand the factors that define it. We report the results of an initial study into the predictive power of surface syntactic statistics for the task; we use fluency assessments done for the purpose of evaluating machine translation. We find that these features are weakly but significantly correlated with fluency. Machine and human translations can be distinguished with accuracy over 80%. The performance of pairwise comparison of fluency is also very high—over 90% for a multi-layer perceptron classifier. We also test the hypothesis that the learned models capture general fluency properties applicable to human-written text. The results do not support this hypothesis: prediction accuracy on the new data is only 57%. This finding suggests that developing a dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bailin</author>
<author>A Grafstein</author>
</authors>
<title>The linguistic assumptions underlying readability formulae: a critique. Language and Communication,</title>
<date>2001</date>
<pages>21--285</pages>
<contexts>
<context position="15330" citStr="Bailin and Grafstein, 2001" startWordPosition="2463" endWordPosition="2466">ct position. Similarly the length of the verb phrases signal potential fluency problems: • Most of the US allies in Europe publicly [object to invading Iraq]V P. • But this [is dealing against some recent remarks of Japanese financial minister, Masajuro Shiokawa]V P. VP distance (the average number of words separating two verb phrases) is also negatively correlated with sentence fluency. In machine translations there is the obvious problem that they might not include a verb for long stretches of text. But even in human written text, the presence of more verbs can make a difference in fluency (Bailin and Grafstein, 2001). Consider the following two sentences: • In his state of the Union address, Putin also talked about the national development plan for this fiscal year and the domestic and foreign policies. • Inside the courtyard of the television station, a reception team of 25 people was formed to attend to those who came to make donations in person. The next strongest correlation is with unnormalized verb phrase length. In fact in terms of correlations, in turned out that it was best not to normalize the phrase length features at all. The normalized versions were also correlated with fluency, but the assoc</context>
</contexts>
<marker>Bailin, Grafstein, 2001</marker>
<rawString>A. Bailin and A. Grafstein. 2001. The linguistic assumptions underlying readability formulae: a critique. Language and Communication, 21:285–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In COLING,</booktitle>
<pages>42--48</pages>
<contexts>
<context position="1503" citStr="Bangalore and Rambow, 2000" startWordPosition="220" endWordPosition="224"> perceptron classifier. We also test the hypothesis that the learned models capture general fluency properties applicable to human-written text. The results do not support this hypothesis: prediction accuracy on the new data is only 57%. This finding suggests that developing a dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem. 1 Introduction Numerous natural language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence l</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>S. Bangalore and O. Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In COLING, pages 42–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
<author>S Whittaker</author>
</authors>
<title>Evaluation metrics for generation.</title>
<date>2000</date>
<booktitle>In INLG’00: Proceedings of the first international conference on Natural language generation,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3016" citStr="Bangalore et al., 2000" startWordPosition="461" endWordPosition="464">ed in combination with other qualities. For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons with a model to judge overall “goodness”, with higher n-grams meant to capture fluency considerations. More sophisticated ways to compare a system production and a model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation of the semantic overlap between a model and an output. Similarly, the metrics proposed for text generation by (Bangalore et al., 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. In contrast, the work of (Wan et al., 2005) and (Mutton et al., 2007) directly sets as a goal the assessment of sentence-level fluency, regardless of content. In (Wan et al., 2005) the main premise is that syntactic information from a parser can more robustly capture fluency than language models, giving more direct indications of the degree of ungrammaticality. The idea is extended in (Mutton et al., 2007), where four parsers are used Proceedings of the 12th Conference of the European Chapter of the</context>
</contexts>
<marker>Bangalore, Rambow, Whittaker, 2000</marker>
<rawString>S. Bangalore, O. Rambow, and S. Whittaker. 2000. Evaluation metrics for generation. In INLG’00: Proceedings of the first international conference on Natural language generation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>V Mittal</author>
<author>M Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Co mputational Linguistics.</booktitle>
<contexts>
<context position="1962" citStr="Banko et al., 2000" startWordPosition="291" endWordPosition="294">f producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combination with other qualities. For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons wi</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>M. Banko, V. Mittal, and M. Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the 38th Annual Meeting of the Association for Co mputational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="2323" citStr="Barzilay and Lapata, 2008" startWordPosition="345" endWordPosition="349"> Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combination with other qualities. For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons with a model to judge overall “goodness”, with higher n-grams meant to capture fluency considerations. More sophisticated ways to compare a system production and a model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation of the semantic overlap between a model a</context>
<context position="7840" citStr="Barzilay and Lapata, 2008" startWordPosition="1198" endWordPosition="1201"> study is not evaluation of machine translation per se. Our goal is more general and the interest is in finding predictors of sentence fluency. No general corpora exist with fluency assessments, so it seems advantageous to use the assessments done in the context of machine translation for preliminary investigations of fluency. Nevertheless, our findings are also potentially beneficial for sentence-level evaluation of machine translation. 2 Features Perceived sentence fluency is influenced by many factors. The way the sentence fits in the context of surrounding sentences is one obvious factor (Barzilay and Lapata, 2008). Another well-known factor is vocabulary use: the presence of uncommon difficult words are known to pose problems to readers and to render text less readable (CollinsThompson and Callan, 2004; Schwarm and Ostendorf, 2005). But these discourse- and vocabularylevel features measure properties at granularities different from the sentence level. Syntactic sentence level features have not been investigated as a stand-alone class, as has been 140 done for the other types of features. This is why we constrain our study to syntactic features alone, and do not discuss discourse and language model feat</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>R. Barzilay and M. Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="1899" citStr="Barzilay and McKeown, 2005" startWordPosition="280" endWordPosition="283"> Introduction Numerous natural language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combination with other qualities. For example, in machine translation evaluation, approaches such as</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>R. Barzilay and K. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>173--180</pages>
<contexts>
<context position="4370" citStr="Charniak and Johnson, 2005" startWordPosition="679" endWordPosition="682">lly generated sentences with varying level of fluency are evaluated with impressive success. The fluency models hold promise for actual improvements in machine translation output quality (Zwarts and Dras, 2008). In that work, only simple parser features are used for the prediction of fluency, but no actual syntactic properties of the sentences. But certainly, problems with sentence fluency are expected to be manifested in syntax. We would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008) should be useful for predicting sentence fluency as well. Indeed, early work has demonstrated that syntactic features, and branching properties in particular, are helpful features for automatically distinguishing human translations from machine translations (Corston-Oliver et al., 2001). The exploration of branching properties of human and machine translations was motivated by the observations during failure analysis that MT system output tends to favor right-branching structures over noun compounding. Branching preference mismatch manifest themselves in the English output when </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In NAACL-2000.</booktitle>
<contexts>
<context position="8776" citStr="Charniak, 2000" startWordPosition="1348" endWordPosition="1350">rom the sentence level. Syntactic sentence level features have not been investigated as a stand-alone class, as has been 140 done for the other types of features. This is why we constrain our study to syntactic features alone, and do not discuss discourse and language model features that have been extensively studied in prior work on coherence and readability. In our work, instead of looking at the syntactic structures present in the sentences, e.g. the syntactic rules used, we use surface statistics of phrase length and types of modification. The sentences were parsed with Charniak’s parser (Charniak, 2000) in order to calculate these features. Sentence length is the number of words in a sentence. Evaluation metrics such as BLEU (Papineni et al., 2002) have a built-in preference for shorter translations. In general one would expect that shorter sentences are easier to read and thus are perceived as more fluent. We added this feature in order to test directly the hypothesis for brevity preference. Parse tree depth is considered to be a measure of sentence complexity. Generally, longer sentences are syntactically more complex but when sentences are approximately the same length the larger parse tr</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In NAACL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Models for sentence compression: A comparison across domains, training requirements and evaluation measures.</title>
<date>2006</date>
<booktitle>In ACL:COLING’06,</booktitle>
<pages>377--384</pages>
<contexts>
<context position="1735" citStr="Clarke and Lapata, 2006" startWordPosition="257" endWordPosition="260">. This finding suggests that developing a dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem. 1 Introduction Numerous natural language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis </context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>J. Clarke and M. Lapata. 2006. Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In ACL:COLING’06, pages 377–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="4342" citStr="Collins and Koo, 2005" startWordPosition="675" endWordPosition="678">stics 139 and artificially generated sentences with varying level of fluency are evaluated with impressive success. The fluency models hold promise for actual improvements in machine translation output quality (Zwarts and Dras, 2008). In that work, only simple parser features are used for the prediction of fluency, but no actual syntactic properties of the sentences. But certainly, problems with sentence fluency are expected to be manifested in syntax. We would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008) should be useful for predicting sentence fluency as well. Indeed, early work has demonstrated that syntactic features, and branching properties in particular, are helpful features for automatically distinguishing human translations from machine translations (Corston-Oliver et al., 2001). The exploration of branching properties of human and machine translations was motivated by the observations during failure analysis that MT system output tends to favor right-branching structures over noun compounding. Branching preference mismatch manifest themselves</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>M. Collins and T. Koo. 2005. Discriminative reranking for natural language parsing. Comput. Linguist., 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL’04.</booktitle>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>K. Collins-Thompson and J. Callan. 2004. A language modeling approach to predicting reading difficulty. In Proceedings of HLT/NAACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>M Gamon</author>
<author>C Brockett</author>
</authors>
<title>A machine learning approach to the automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>148--155</pages>
<contexts>
<context position="4672" citStr="Corston-Oliver et al., 2001" startWordPosition="722" endWordPosition="725"> actual syntactic properties of the sentences. But certainly, problems with sentence fluency are expected to be manifested in syntax. We would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008) should be useful for predicting sentence fluency as well. Indeed, early work has demonstrated that syntactic features, and branching properties in particular, are helpful features for automatically distinguishing human translations from machine translations (Corston-Oliver et al., 2001). The exploration of branching properties of human and machine translations was motivated by the observations during failure analysis that MT system output tends to favor right-branching structures over noun compounding. Branching preference mismatch manifest themselves in the English output when translating from languages whose branching properties are radically different from English. Accuracy close to 80% was achieved for distinguishing human translations from machine translations. In our work we continue the investigation of sentence level fluency based on features that capture surface sta</context>
</contexts>
<marker>Corston-Oliver, Gamon, Brockett, 2001</marker>
<rawString>S. Corston-Oliver, M. Gamon, and C. Brockett. 2001. A machine learning approach to the automatic evaluation of machine translation. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>Generic sentence fusion is an ill-defined summarization task.</title>
<date>2004</date>
<booktitle>In Proceedings of the Text Summarization Branches Out Workshop at ACL.</booktitle>
<marker>Daum´e, Marcu, 2004</marker>
<rawString>H. Daum´e III and D. Marcu. 2004. Generic sentence fusion is an ill-defined summarization task. In Proceedings of the Text Summarization Branches Out Workshop at ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
</authors>
<title>Lexicalized markov grammars for sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT).</booktitle>
<contexts>
<context position="1805" citStr="Galley and McKeown, 2007" startWordPosition="267" endWordPosition="270">t corpus of fluency judgments will be beneficial for further investigations of the problem. 1 Introduction Numerous natural language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combin</context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>M. Galley and K. McKeown. 2007. Lexicalized markov grammars for sentence compression. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="4384" citStr="Huang, 2008" startWordPosition="683" endWordPosition="684"> varying level of fluency are evaluated with impressive success. The fluency models hold promise for actual improvements in machine translation output quality (Zwarts and Dras, 2008). In that work, only simple parser features are used for the prediction of fluency, but no actual syntactic properties of the sentences. But certainly, problems with sentence fluency are expected to be manifested in syntax. We would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008) should be useful for predicting sentence fluency as well. Indeed, early work has demonstrated that syntactic features, and branching properties in particular, are helpful features for automatically distinguishing human translations from machine translations (Corston-Oliver et al., 2001). The exploration of branching properties of human and machine translations was motivated by the observations during failure analysis that MT system output tends to favor right-branching structures over noun compounding. Branching preference mismatch manifest themselves in the English output when translating fr</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL-08: HLT, pages 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
</authors>
<title>Sentence simplification in automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied NLP Conference, ANLP’2000.</booktitle>
<contexts>
<context position="1686" citStr="Jing, 2000" startWordPosition="251" endWordPosition="252">accuracy on the new data is only 57%. This finding suggests that developing a dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem. 1 Introduction Numerous natural language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (La</context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>H. Jing. 2000. Sentence simplification in automatic text summarization. In Proceedings of the 6th Applied NLP Conference, ANLP’2000.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Karamanis</author>
<author>M Poesio</author>
<author>C Mellish</author>
<author>J Oberlander</author>
</authors>
<title>(to appear). Evaluating centering for information ordering using corpora. Computational Linguistics.</title>
<marker>Karamanis, Poesio, Mellish, Oberlander, </marker>
<rawString>N. Karamanis, M. Poesio, C. Mellish, and J. Oberlander. (to appear). Evaluating centering for information ordering using corpora. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="1710" citStr="Knight and Marcu, 2002" startWordPosition="253" endWordPosition="256">the new data is only 57%. This finding suggests that developing a dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem. 1 Introduction Numerous natural language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artificial Intelligence, 139(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>704--710</pages>
<contexts>
<context position="1474" citStr="Langkilde and Knight, 1998" startWordPosition="216" endWordPosition="219">h—over 90% for a multi-layer perceptron classifier. We also test the hypothesis that the learned models capture general fluency properties applicable to human-written text. The results do not support this hypothesis: prediction accuracy on the new data is only 57%. This finding suggests that developing a dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem. 1 Introduction Numerous natural language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the facto</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998. Generation that exploits corpus-based statistical knowledge. In COLING-ACL, pages 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03.</booktitle>
<contexts>
<context position="2296" citStr="Lapata, 2003" startWordPosition="343" endWordPosition="344">00; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combination with other qualities. For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons with a model to judge overall “goodness”, with higher n-grams meant to capture fluency considerations. More sophisticated ways to compare a system production and a model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation of the semanti</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In EACL’06.</booktitle>
<contexts>
<context position="1751" citStr="McDonald, 2006" startWordPosition="261" endWordPosition="262">hat developing a dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem. 1 Introduction Numerous natural language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appea</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>R. McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In EACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mutton</author>
<author>M Dras</author>
<author>S Wan</author>
<author>R Dale</author>
</authors>
<title>Gleu: Automatic evaluation of sentence-level fluency.</title>
<date>2007</date>
<booktitle>In ACL’07,</booktitle>
<pages>344--351</pages>
<contexts>
<context position="3181" citStr="Mutton et al., 2007" startWordPosition="489" endWordPosition="492">ith a model to judge overall “goodness”, with higher n-grams meant to capture fluency considerations. More sophisticated ways to compare a system production and a model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation of the semantic overlap between a model and an output. Similarly, the metrics proposed for text generation by (Bangalore et al., 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. In contrast, the work of (Wan et al., 2005) and (Mutton et al., 2007) directly sets as a goal the assessment of sentence-level fluency, regardless of content. In (Wan et al., 2005) the main premise is that syntactic information from a parser can more robustly capture fluency than language models, giving more direct indications of the degree of ungrammaticality. The idea is extended in (Mutton et al., 2007), where four parsers are used Proceedings of the 12th Conference of the European Chapter of the ACL, pages 139–147, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 139 and artificially generated sentences with varying </context>
</contexts>
<marker>Mutton, Dras, Wan, Dale, 2007</marker>
<rawString>A. Mutton, M. Dras, S. Wan, and R. Dale. 2007. Gleu: Automatic evaluation of sentence-level fluency. In ACL’07, pages 344–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2528" citStr="Papineni et al., 2002" startWordPosition="376" endWordPosition="379">t al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combination with other qualities. For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons with a model to judge overall “goodness”, with higher n-grams meant to capture fluency considerations. More sophisticated ways to compare a system production and a model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation of the semantic overlap between a model and an output. Similarly, the metrics proposed for text generation by (Bangalore et al., 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. In contrast, the</context>
<context position="8924" citStr="Papineni et al., 2002" startWordPosition="1373" endWordPosition="1376">er types of features. This is why we constrain our study to syntactic features alone, and do not discuss discourse and language model features that have been extensively studied in prior work on coherence and readability. In our work, instead of looking at the syntactic structures present in the sentences, e.g. the syntactic rules used, we use surface statistics of phrase length and types of modification. The sentences were parsed with Charniak’s parser (Charniak, 2000) in order to calculate these features. Sentence length is the number of words in a sentence. Evaluation metrics such as BLEU (Papineni et al., 2002) have a built-in preference for shorter translations. In general one would expect that shorter sentences are easier to read and thus are perceived as more fluent. We added this feature in order to test directly the hypothesis for brevity preference. Parse tree depth is considered to be a measure of sentence complexity. Generally, longer sentences are syntactically more complex but when sentences are approximately the same length the larger parse tree depth can be indicative of increased complexity that can slow processing and lead to lower perceived fluency of the sentence. Number offragment t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>186--195</pages>
<contexts>
<context position="23537" citStr="Pitler and Nenkova, 2008" startWordPosition="3748" endWordPosition="3751">sentence fluency from data coming from MT evaluations. In the previous sections, we demonstrated that indeed structural features can enable us to perform this task very accurately in the context of machine translation. But will the models conveniently trained on data from MT evaluation be at all capable to identify sentences in human-written text that are not fluent and are difficult to understand? To answer this question, we performed an additional experiment on 30 Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). The articles were chosen at random and comprised a total of 290 sentences. One human assessor was asked to read each sentence and mark the ones that seemed disfluent because they were hard to comprehend. These were sentences that needed to be read more than once in order to fully understand the information conveyed in them. There were 52 such sentences. The assessments served as a goldstandard against which the predictions of the fluency models were compared. Two models trained on machine translation data were used to predict the status of each sentence in the WSJ articles. One of the models</context>
<context position="28225" citStr="Pitler and Nenkova, 2008" startWordPosition="4526" endWordPosition="4529"> functions well. (3.2) Amstrad PLC, a British maker of computer hardware and communications equipment, posted a 52% plunge in pretax profit for the latest year. (3.3) At current allocations, that means EPA will be spending $300 billion on itself. 5.2 Correlation with overall text quality In our final experiment we focus on the relationship between sentence fluency and overall text quality. We would expect that the presence of disfluent sentences in text will make it appear less well written. Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). The average of the assessments was taken as a single number describing the article. The correlation between this number and the percentage of fluent sentences in the article according to the different models is shown in Table 7. The correlation between the percentage of fluent sentences in the article as given by the human assessor and the overall text quality is rather low, 0.127. The positive correlation would suggest that the more hard to read sentence appear in a text, the higher the text would be rated overall, which is surprising. The predictions from the model for distinguishing good </context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>E. Pitler and A. Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 186–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schwarm</author>
<author>M Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL’05,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="8062" citStr="Schwarm and Ostendorf, 2005" startWordPosition="1233" endWordPosition="1237">to use the assessments done in the context of machine translation for preliminary investigations of fluency. Nevertheless, our findings are also potentially beneficial for sentence-level evaluation of machine translation. 2 Features Perceived sentence fluency is influenced by many factors. The way the sentence fits in the context of surrounding sentences is one obvious factor (Barzilay and Lapata, 2008). Another well-known factor is vocabulary use: the presence of uncommon difficult words are known to pose problems to readers and to render text less readable (CollinsThompson and Callan, 2004; Schwarm and Ostendorf, 2005). But these discourse- and vocabularylevel features measure properties at granularities different from the sentence level. Syntactic sentence level features have not been investigated as a stand-alone class, as has been 140 done for the other types of features. This is why we constrain our study to syntactic features alone, and do not discuss discourse and language model features that have been extensively studied in prior work on coherence and readability. In our work, instead of looking at the syntactic structures present in the sentences, e.g. the syntactic rules used, we use surface statis</context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>S. Schwarm and M. Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. In Proceedings ofACL’05, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
</authors>
<title>Syntactic simplification and Text Cohesion.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge, UK.</institution>
<contexts>
<context position="1652" citStr="Siddharthan, 2003" startWordPosition="246" endWordPosition="247">o not support this hypothesis: prediction accuracy on the new data is only 57%. This finding suggests that developing a dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem. 1 Introduction Numerous natural language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative o</context>
</contexts>
<marker>Siddharthan, 2003</marker>
<rawString>A. Siddharthan. 2003. Syntactic simplification and Text Cohesion. Ph.D. thesis, University of Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Abstractive headline generation using widl-expressions.</title>
<date>2007</date>
<journal>Inf. Process. Manage.,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="2008" citStr="Soricut and Marcu, 2007" startWordPosition="299" endWordPosition="302">problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combination with other qualities. For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons with a model to judge overall “goodness”, with h</context>
</contexts>
<marker>Soricut, Marcu, 2007</marker>
<rawString>R. Soricut and D. Marcu. 2007. Abstractive headline generation using widl-expressions. Inf. Process. Manage., 43(6):1536–1548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turner</author>
<author>E Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In ACL’05.</booktitle>
<contexts>
<context position="1778" citStr="Turner and Charniak, 2005" startWordPosition="263" endWordPosition="266"> dedicated, task-independent corpus of fluency judgments will be beneficial for further investigations of the problem. 1 Introduction Numerous natural language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fl</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression. In ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wan</author>
<author>R Dale</author>
<author>M Dras</author>
</authors>
<title>Searching for grammaticality: Propagating dependencies in the viterbi algorithm.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth European Workshop on Natural Language Generation (ENLG-05).</booktitle>
<contexts>
<context position="1918" citStr="Wan et al., 2005" startWordPosition="284" endWordPosition="287">al language applications involve the task of producing fluent text. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combination with other qualities. For example, in machine translation evaluation, approaches such as BLEU (Papineni et </context>
<context position="3155" citStr="Wan et al., 2005" startWordPosition="484" endWordPosition="487">m overlap comparisons with a model to judge overall “goodness”, with higher n-grams meant to capture fluency considerations. More sophisticated ways to compare a system production and a model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation of the semantic overlap between a model and an output. Similarly, the metrics proposed for text generation by (Bangalore et al., 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. In contrast, the work of (Wan et al., 2005) and (Mutton et al., 2007) directly sets as a goal the assessment of sentence-level fluency, regardless of content. In (Wan et al., 2005) the main premise is that syntactic information from a parser can more robustly capture fluency than language models, giving more direct indications of the degree of ungrammaticality. The idea is extended in (Mutton et al., 2007), where four parsers are used Proceedings of the 12th Conference of the European Chapter of the ACL, pages 139–147, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 139 and artificially generat</context>
</contexts>
<marker>Wan, Dale, Dras, 2005</marker>
<rawString>S. Wan, R. Dale, and M. Dras. 2005. Searching for grammaticality: Propagating dependencies in the viterbi algorithm. In Proceedings of the Tenth European Workshop on Natural Language Generation (ENLG-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zajic</author>
<author>B Dorr</author>
<author>J Lin</author>
<author>R Schwartz</author>
</authors>
<title>Multi-candidate reduction: Sentence compression as a tool for document summarization tasks.</title>
<date>2007</date>
<journal>Inf. Process. Manage.,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="1982" citStr="Zajic et al., 2007" startWordPosition="295" endWordPosition="298">ext. This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000), as well as an important step in machine translation. Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combination with other qualities. For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons with a model to judge </context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2007</marker>
<rawString>D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2007. Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Inf. Process. Manage., 43(6):1549–1570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zwarts</author>
<author>M Dras</author>
</authors>
<title>Choosing the right translation: A syntactically informed classification approach.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1153--1160</pages>
<contexts>
<context position="3954" citStr="Zwarts and Dras, 2008" startWordPosition="611" endWordPosition="614">rmation from a parser can more robustly capture fluency than language models, giving more direct indications of the degree of ungrammaticality. The idea is extended in (Mutton et al., 2007), where four parsers are used Proceedings of the 12th Conference of the European Chapter of the ACL, pages 139–147, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 139 and artificially generated sentences with varying level of fluency are evaluated with impressive success. The fluency models hold promise for actual improvements in machine translation output quality (Zwarts and Dras, 2008). In that work, only simple parser features are used for the prediction of fluency, but no actual syntactic properties of the sentences. But certainly, problems with sentence fluency are expected to be manifested in syntax. We would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008) should be useful for predicting sentence fluency as well. Indeed, early work has demonstrated that syntactic features, and branching properties in particular, are helpfu</context>
</contexts>
<marker>Zwarts, Dras, 2008</marker>
<rawString>S. Zwarts and M. Dras. 2008. Choosing the right translation: A syntactically informed classification approach. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1153–1160.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>