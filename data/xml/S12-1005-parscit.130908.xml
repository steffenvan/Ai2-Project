<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.978834">
Sentence Clustering via Projection over Term Clusters
</title>
<author confidence="0.996006">
Lili Kotlerman, Ido Dagan
</author>
<affiliation confidence="0.929565">
Bar-Ilan University
Israel
</affiliation>
<email confidence="0.9584615">
Lili.Kotlerman@biu.ac.il
dagan@cs.biu.ac.il
</email>
<author confidence="0.811295">
Maya Gorodetsky, Ezra Daya
</author>
<affiliation confidence="0.6784995">
NICE Systems Ltd.
Israel
</affiliation>
<email confidence="0.967107">
Maya.Gorodetsky@nice.com
Ezra.Daya@nice.com
</email>
<sectionHeader confidence="0.995527" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996318428571428">
This paper presents a novel sentence cluster-
ing scheme based on projecting sentences over
term clusters. The scheme incorporates exter-
nal knowledge to overcome lexical variability
and small corpus size, and outperforms com-
mon sentence clustering methods on two real-
life industrial datasets.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947882352941">
Clustering is a popular technique for unsupervised
text analysis, often used in industrial settings to ex-
plore the content of large amounts of sentences. Yet,
as may be seen from the results of our research,
widespread clustering techniques, which cluster sen-
tences directly, result in rather moderate perfor-
mance when applied to short sentences, which are
common in informal media.
In this paper we present and evaluate a novel
sentence clustering scheme based on projecting
sentences over term clusters. Section 2 briefly
overviews common sentence clustering approaches.
Our suggested clustering scheme is presented in
Section 3. Section 4 describes an implementation of
the scheme for a particular industrial task, followed
by evaluation results in Section 5. Section 6 lists
directions for future research.
</bodyText>
<sectionHeader confidence="0.98916" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99977375">
Sentence clustering aims at grouping sentences with
similar meanings into clusters. Commonly, vector
similarity measures, such as cosine, are used to de-
fine the level of similarity over bag-of-words encod-
</bodyText>
<page confidence="0.986841">
38
</page>
<bodyText confidence="0.9988665">
ing of the sentences. Then, standard clustering algo-
rithms can be applied to group sentences into clus-
ters (see Steinbach et al. (2000) for an overview).
The most common practice is representing the
sentences as vectors in term space and applying the
K-means clustering algorithm (Shen et al. (2011);
Pasquier (2010); Wang et al. (2009); Nomoto and
Matsumoto (2001); Boros et al. (2001)). An alterna-
tive approach involves partitioning a sentence con-
nectivity graph by means of a graph clustering algo-
rithm (Erkan and Radev (2004); Zha (2002)).
The main challenge for any sentence clustering
approach is language variability, where the same
meaning can be phrased in various ways. The
shorter the sentences are, the less effective becomes
exact matching of their terms. Compare the fol-
lowing newspaper sentence ”The bank is phasing out
the EZ Checking package, with no monthly fee charged
for balances over $1,500, and is instead offering cus-
tomers its Basic Banking account, which carries a fee”
with two tweets regarding the same event: ”Whats
wrong.. charging $$ for checking a/c” and ”Now they
want a monthly fee!”. Though each of the tweets can
be found similar to the long sentence by exact term
matching, they do not share any single term. Yet,
knowing that the words fee and charge are semanti-
cally related would allow discovering the similarity
between the two tweets.
External resources can be utilized to provide such
kind of knowledge, by which sentence representa-
tion can be enriched. Traditionally, WordNet (Fell-
baum, 1998) has been used for this purpose (She-
hata (2009); Chen et al. (2003); Hotho et al. (2003);
Hatzivassiloglou et al. (2001)). Yet, other resources
</bodyText>
<note confidence="0.317948">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996742">
of semantically-related terms can be beneficial, such
as WordNet::Similarity (Pedersen et al., 2004), sta-
tistical resources like that of Lin (1998) or DIRECT
(Kotlerman et al., 2010), thesauri, Wikipedia (Hu et
al., 2009), ontologies (Suchanek et al., 2007) etc.
</bodyText>
<sectionHeader confidence="0.895359" genericHeader="method">
3 Sentence Clustering via Term Clusters
</sectionHeader>
<bodyText confidence="0.999930833333333">
This section presents a generic sentence clustering
scheme, which involves two consecutive steps: (1)
generating relevant term clusters based on lexical se-
mantic relatedness and (2) projecting the sentence
set over these term clusters. Below we describe each
of the two steps.
</bodyText>
<subsectionHeader confidence="0.99754">
3.1 Step 1: Obtaining Term Clusters
</subsectionHeader>
<bodyText confidence="0.999171666666667">
In order to obtain term clusters, a term connectivity
graph is constructed for the given sentence set and is
clustered as follows:
</bodyText>
<listItem confidence="0.9760325">
1. Create initially an undirected graph with
sentence-set terms as nodes and use lexical re-
sources to extract semantically-related terms
for each node.
2. Augment the graph nodes with the extracted
terms and connect semantically-related nodes
with edges. Then, partition the graph into term
clusters through a graph clustering algorithm.
</listItem>
<bodyText confidence="0.941292741935484">
Extracting and filtering related terms. In Sec-
tion 2 we listed a number of lexical resources pro-
viding pairs of semantically-related terms. Within
the suggested scheme, any combination of resources
may be utilized.
Often resources contain terms, which are
semantically-related only in certain contexts. E.g.,
the words visa and passport are semantically-related
when talking about tourism, but cannot be consid-
ered related in the banking domain, where visa usu-
ally occurs in its credit card sense. In order to dis-
card irrelevant terms, filtering procedures can be em-
ployed. E.g., a simple filtering applicable in most
cases of sentence clustering in a specific domain
would discard candidate related terms, which do not
occur sufficiently frequently in a target-domain cor-
pus. In the example above, this procedure would
allow avoiding the insertion of passport as related to
visa, when considering the banking domain.
Clustering the graph nodes. Once the term
graph is constructed, a graph clustering algorithm
is applied resulting in a partition of the graph nodes
(terms) into clusters. The choice of a particular al-
gorithm is a parameter of the scheme. Many clus-
tering algorithms consider the graph’s edge weights.
To address this trait, different edge weights can be
assigned, reflecting the level of confidence that the
two terms are indeed validly related and the reliabil-
ity of the resource, which suggested the correspond-
ing edge (e.g. WordNet synonyms are commonly
considered more reliable than statistical thesauri).
</bodyText>
<subsectionHeader confidence="0.996973">
3.2 Step 2: Projecting Sentences to Term
Clusters
</subsectionHeader>
<bodyText confidence="0.999940285714286">
To obtain sentence clusters, the given sentence set
has to be projected in some manner over the term
clusters obtained in Step 1. Our projection pro-
cedure resembles unsupervised text categorization
(Gliozzo et al., 2005), with categories represented
by term clusters that are not predefined but rather
emerge from the analyzed data:
</bodyText>
<listItem confidence="0.6763073">
1. Represent term clusters and sentences as vec-
tors in term space and calculate the similarity
of each sentence with each of the term clusters.
2. Assign each sentence to the best-scoring term
cluster. (We focus on hard clustering, but the
procedure can be adapted for soft clustering).
Various metrics for feature weighting and vector
comparison may be chosen. The top terms of term-
cluster vectors can be regarded as labels for the cor-
responding sentence clusters.
</listItem>
<bodyText confidence="0.999974545454545">
Thus each sentence cluster corresponds to a sin-
gle coherent cluster of related terms. This is con-
trasted with common clustering methods, where if
sentence A shares a term with B, and B shares an-
other term with C, then A and C might appear in the
same cluster even if they have no related terms in
common. This behavior turns out harmful for short
sentences, where each incidental term is influential.
Our scheme ensures that each cluster contains only
sentences related to the underlying term cluster, re-
sulting in more coherent clusters.
</bodyText>
<sectionHeader confidence="0.989997" genericHeader="method">
4 Application: Clustering Customer
Interactions
</sectionHeader>
<bodyText confidence="0.999932">
In industry there’s a prominent need to obtain busi-
ness insights from customer interactions in a contact
center or social media. Though the number of key
</bodyText>
<page confidence="0.996579">
39
</page>
<bodyText confidence="0.992887">
sentences to analyze is often relatively small, such
as a couple hundred, manually analyzing just a hand-
ful of clusters is much preferable. This section de-
scribes our implementation of the scheme described
in Section 3 for the task of clustering customer in-
teractions, as well as the data used for evaluation.
Results and analysis are presented in Section 5.
</bodyText>
<subsectionHeader confidence="0.990415">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999974428571428">
We apply our clustering approach over two real-life
datasets. The first one consists of 155 sentences
containing reasons of account cancelation, retrieved
from automatic transcripts of contact center interac-
tions of an Internet Service Provider (ISP). The sec-
ond one contains 194 sentences crawled from Twit-
ter, expressing reasons for customer dissatisfaction
with a certain banking company. The sentences in
both datasets were gathered automatically by a rule-
based extraction algorithm. Each dataset is accom-
panied by a small corpus of call transcripts or tweets
from the corresponding domain.1
The goal of clustering these sentences is to iden-
tify the prominent reasons of cancelation and dissat-
isfaction. To obtain the gold-standard (GS) anno-
tation, sentences were manually grouped to clusters
according to the reasons stated in them.
Table 1 presents examples of sentences from the
ISP dataset. The sentences are short, with only one
or two words expressing the actual reason stated in
them. We see that exact term matching is not suffi-
cient to group the related sentences. Moreover, tra-
ditional clustering algorithms are likely to mix re-
lated and unrelated sentences, due to matching non-
essential terms (e.g. husband or summer). We note
that such short and noisy sentences are common
in informal media, which became a most important
channel of information in industry.
</bodyText>
<subsectionHeader confidence="0.997351">
4.2 Implementation of the Clustering Scheme
</subsectionHeader>
<bodyText confidence="0.999295428571429">
Our proposed sentence clustering scheme presented
in Section 3 includes a number of choices. Below
we describe the choices we made in our current im-
plementation.
Input sentences were tokenized, lemmatized and
cleaned from stopwords in order to extract content-
word terms. Candidate semantically-related terms
</bodyText>
<footnote confidence="0.7772335">
1The bank dataset with the output of the tested methods will
be made publicly available.
</footnote>
<tableCaption confidence="0.741381888888889">
he hasn’t been using it all summer long
it’s been sitting idle for about it almost a year
I’m getting married my husband has a computer
yeah I bought a new laptop this summer so
when I said faces my husband got laid offfrom work
well I’m them going through financial difficulties
Table 1: Example sentences expressing 3 reasons for can-
celation: the customer (1) does not use the service, (2)
acquired a computer, (3) cannot afford the service.
</tableCaption>
<bodyText confidence="0.999901523809524">
were extracted for each of the terms, using Word-
Net synonyms and derivations, as well as DIRECT2,
a directional statistical resource learnt from a news
corpus. Candidate terms that did not appear in the
accompanying domain corpus were filtered out as
described in Section 3.1.
Edges in the term graph were weighted with the
number of resources supporting the corresponding
edge. To cluster the graph we used the Chinese
Whispers clustering tool3 (Biemann, 2006), whose
algorithm does not require to pre-set the desired
number of clusters and is reported to outperform
other algorithms for several NLP tasks.
To generate the projection, sentences were rep-
resented as vectors of terms weighted by their fre-
quency in each sentence. Terms of the term-cluster
vectors were weighted by the number of sentences
in which they occur. Similarity scores were calcu-
lated using the cosine measure. Clusters were la-
beled with the top terms appearing both in the un-
derlying term cluster and in the cluster’s sentences.
</bodyText>
<sectionHeader confidence="0.999216" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<bodyText confidence="0.9692286">
In this section we present the results of evaluating
our projection approach, compared to the common
K-means clustering method4 applied to:
(A) Standard bag-of-words representation of sen-
tences;
</bodyText>
<footnote confidence="0.899727">
2Available for download at www.cs.biu.ac.il/
˜nlp/downloads/DIRECT.html. For each term we
extract from the resource the top-5 related terms.
3Available at http://wortschatz.informatik.
uni-leipzig.de/˜cbiemann/software/CW.html
</footnote>
<bodyText confidence="0.898428">
4We use the Weka (Hall et al., 2009) implementation. Due
to space limitations and for more meaningful comparison we re-
port here one value of K, which is equal to the number of clus-
ters returned by projection (60 for the ISP and 65 for the bank
dataset). For K = 20, 40 and 70 the performance was similar.
</bodyText>
<page confidence="0.99621">
40
</page>
<bodyText confidence="0.9587559">
(B) Bag-of-words representation, where sentence’s
words are augmented with semantically-related
terms (following the common scheme of prior
work, see Section 2). We use the same set of
related terms as is used by our method.
(C) Representation of sentences in term-cluster
space, using the term clusters generated by our
method as vector features. A feature is acti-
vated in a sentence vector if it contains a term
from the corresponding term cluster.
Table 2 shows the results in terms of Purity, Recall
(R), Precision (P) and F1 (see ”Evaluation of clus-
tering”, Manning et al. (2008)). Projection signifi-
cantly5 outperforms all baselines for both datasets.
grouped in one term cluster. However, adding more
resources may introduce additional noise. Such de-
pendency on coverage and accuracy of resources is
apparently a limitation of our approach. Yet, as
our experiments indicate, using only two generic re-
sources already yielded valuable results.
</bodyText>
<figure confidence="0.480386">
a. Projection
credit card, card, mastercard, visa (38 sentences)
XXX has the worst credit cards ever
XXX MasterCard is the worst credit card I’ve ever had
ntuc do not accept XXX visa now I have to redraw $150...
XXX card declined again, $40 dinner in SF...
b. K-means C
</figure>
<table confidence="0.9883792">
Dataset Algorithm Purity R P F1
Projection .74 .40 .68 .50
ISP K-means A .65 .18 .22 .20
K-means B .65 .13 .24 .17
K-means C .65 .18 .26 .22
Projection .79 .26 .53 .35
K-means A .61 .14 .14 .14
Bank
K-means B .64 .13 .19 .16
K-means C .67 .17 .21 .19
</table>
<tableCaption confidence="0.999623">
Table 2: Evaluation results.
</tableCaption>
<bodyText confidence="0.99899295">
For completeness we experimented with applying
Chinese Whispers clustering to sentence connectiv-
ity graphs, but the results were inferior to K-means.
Table 3 presents sample sentences from clusters
produced by projection and K-means for illustration.
Our initial analysis showed that our approach indeed
produces more homogenous clusters than the base-
line methods, as conjectured in Section 3.2. We con-
sider it advantageous, since it’s easier for a human to
merge clusters than to reveal sub-clusters. E.g., a GS
cluster of 20 sentences referring to fees and charges
is covered by three projection clusters labeled fee,
charge and interest rate, with 9, 8 and 2 sentences
correspondingly. On the other hand, K-means C
method places 11 out of the 20 sentences in a messy
cluster of 57 sentences (see Table 3), scattering the
remaining 9 sentences over 7 other clusters.
In our current implementation fee, charge and in-
terest rate were not detected by the lexical resources
we used as semantically similar and thus were not
</bodyText>
<note confidence="0.380159">
5p=0.001 according to McNemar test (Dietterich, 1998).
</note>
<tableCaption confidence="0.716286333333333">
fee, charge (57 sentences)
XXX playing games wit my interest
arguing w incompetent pol at XXX damansara perdana
XXX’s upper management are a bunch of rude pricks
XXX are ninjas at catching fraudulent charges.
Table 3: Excerpt from resulting clusterings for the bank
dataset. Bank name is substituted with XXX. Cluster la-
bels are given in italics. Two most frequent terms are
assigned as cluster labels for K-means C.
</tableCaption>
<sectionHeader confidence="0.988276" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999540444444444">
We presented a novel sentence clustering scheme
and evaluated its implementation, showing signifi-
cantly superior performance over common sentence
clustering techniques. We plan to further explore
the suggested scheme by utilizing additional lexical
resources and clustering algorithms. We also plan
to compare our approach with co-clustering meth-
ods used in document clustering (Xu et al. (2003),
Dhillon (2001), Slonim and Tishby (2000)).
</bodyText>
<sectionHeader confidence="0.98441" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999312111111111">
This work was partially supported by the MAGNE-
TON grant no. 43834 of the Israel Ministry of Indus-
try, Trade and Labor, the Israel Ministry of Science
and Technology, the Israel Science Foundation grant
1112/08, the PASCAL-2 Network of Excellence of
the European Community FP7-ICT-2007-1-216886
and the European Community’s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
</bodyText>
<page confidence="0.999432">
41
</page>
<sectionHeader confidence="0.921203" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.693681333333333">
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
</bodyText>
<note confidence="0.571145666666667">
of TextGraphs: the Second Workshop on Graph Based
Methods for Natural Language Processing, pages 73–
80, New York City, USA.
</note>
<reference confidence="0.99881035">
Endre Boros, Paul B. Kantor, and David J. Neu. 2001. A
clustering based approach to creating multi-document
summaries.
Hsin-Hsi Chen, June-Jei Kuo, and Tsei-Chun Su.
2003. Clustering and visualization in a multi-lingual
multi-document summarization system. In Proceed-
ings of the 25th European conference on IR re-
search, ECIR’03, pages 266–280, Berlin, Heidelberg.
Springer-Verlag.
Inderjit S. Dhillon. 2001. Co-clustering documents and
words using bipartite spectral graph partitioning. In
Proceedings of the seventh ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, KDD ’01, pages 269–274, New York, NY,
USA. ACM.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms.
G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. J. Artif. Int. Res., 22(1):457–479, Decem-
ber.
C. Fellbaum. 1998. WordNet – An Electronic Lexical
Database. MIT Press.
Alfio Massimiliano Gliozzo, Carlo Strapparava, and Ido
Dagan. 2005. Investigating unsupervised learning for
text categorization bootstrapping. In HLT/EMNLP.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDDExplor. Newsl., 11(1):10–18.
Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L.
Holcombe, Regina Barzilay, Min yen Kan, and Kath-
leen R. McKeown. 2001. Simfinder: A flexible clus-
tering tool for summarization. In In Proceedings of the
NAACL Workshop on Automatic Summarization, pages
41–49.
A. Hotho, S. Staab, and G. Stumme. 2003. Word-
net improves text document clustering. In Ying
Ding, Keith van Rijsbergen, Iadh Ounis, and Joe-
mon Jose, editors, Proceedings of the Semantic Web
Workshop of the 26th Annual International ACM SI-
GIR Conference on Research and Development in In-
formaion Retrieval (SIGIR 2003), August 1, 2003,
Toronto Canada. Published Online at http://de.
scientificcommons.org/608322.
Xiaohua Hu, Xiaodan Zhang, Caimei Lu, E. K. Park, and
Xiaohua Zhou. 2009. Exploiting wikipedia as exter-
nal knowledge for document clustering. In Proceed-
ings of the 15th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
’09, pages 389–396, New York, NY, USA. ACM.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. JNLE, 16:359–389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics - Vol-
ume 2, COLING ’98, pages 768–774, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, Juli.
Tadashi Nomoto and Yuji Matsumoto. 2001. A new ap-
proach to unsupervised text summarization. In Pro-
ceedings of the 24th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, SIGIR ’01, pages 26–34, New York, NY,
USA. ACM.
Claude Pasquier. 2010. Task 5: Single document
keyphrase extraction using sentence clustering and la-
tent dirichlet allocation. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ’10, pages 154–157, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL–Demonstrations
’04, pages 38–41, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Shady Shehata. 2009. A wordnet-based semantic model
for enhancing text clustering. Data Mining Work-
shops, International Conference on, 0:477–482.
Chao Shen, Tao Li, and Chris H. Q. Ding. 2011. Integrat-
ing clustering and multi-document summarization by
bi-mixture probabilistic latent semantic analysis (plsa)
with sentence bases. In AAAI.
Noam Slonim and Naftali Tishby. 2000. Document clus-
tering using word clusters via the information bottle-
neck method. In Proceedings of the 23rd annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ’00, pages
208–215, New York, NY, USA. ACM.
M. Steinbach, G. Karypis, and V. Kumar. 2000. A
comparison of document clustering techniques. KDD
Workshop on Text Mining.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A large ontology from
wikipedia and wordnet.
</reference>
<page confidence="0.981546">
42
</page>
<reference confidence="0.999401">
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ’09, pages 297–300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, SIGIR ’03, pages 267–273, New
York, NY, USA. ACM.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement prin-
ciple and sentence clustering. In SIGIR, pages 113–
120.
</reference>
<page confidence="0.999832">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.597671">
<title confidence="0.999723">Sentence Clustering via Projection over Term Clusters</title>
<author confidence="0.8356055">Lili Kotlerman</author>
<author confidence="0.8356055">Ido Bar-Ilan</author>
<email confidence="0.984069">dagan@cs.biu.ac.il</email>
<author confidence="0.899869">Maya Gorodetsky</author>
<author confidence="0.899869">Ezra</author>
<affiliation confidence="0.997855">NICE Systems</affiliation>
<email confidence="0.991028">Ezra.Daya@nice.com</email>
<abstract confidence="0.997239875">This paper presents a novel sentence clustering scheme based on projecting sentences over term clusters. The scheme incorporates external knowledge to overcome lexical variability and small corpus size, and outperforms common sentence clustering methods on two reallife industrial datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Endre Boros</author>
<author>Paul B Kantor</author>
<author>David J Neu</author>
</authors>
<title>A clustering based approach to creating multi-document summaries.</title>
<date>2001</date>
<contexts>
<context position="1979" citStr="Boros et al. (2001)" startWordPosition="287" endWordPosition="290">re research. 2 Background Sentence clustering aims at grouping sentences with similar meanings into clusters. Commonly, vector similarity measures, such as cosine, are used to define the level of similarity over bag-of-words encod38 ing of the sentences. Then, standard clustering algorithms can be applied to group sentences into clusters (see Steinbach et al. (2000) for an overview). The most common practice is representing the sentences as vectors in term space and applying the K-means clustering algorithm (Shen et al. (2011); Pasquier (2010); Wang et al. (2009); Nomoto and Matsumoto (2001); Boros et al. (2001)). An alternative approach involves partitioning a sentence connectivity graph by means of a graph clustering algorithm (Erkan and Radev (2004); Zha (2002)). The main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways. The shorter the sentences are, the less effective becomes exact matching of their terms. Compare the following newspaper sentence ”The bank is phasing out the EZ Checking package, with no monthly fee charged for balances over $1,500, and is instead offering customers its Basic Banking account, which carri</context>
</contexts>
<marker>Boros, Kantor, Neu, 2001</marker>
<rawString>Endre Boros, Paul B. Kantor, and David J. Neu. 2001. A clustering based approach to creating multi-document summaries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-Hsi Chen</author>
<author>June-Jei Kuo</author>
<author>Tsei-Chun Su</author>
</authors>
<title>Clustering and visualization in a multi-lingual multi-document summarization system.</title>
<date>2003</date>
<booktitle>In Proceedings of the 25th European conference on IR research, ECIR’03,</booktitle>
<pages>266--280</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="3196" citStr="Chen et al. (2003)" startWordPosition="488" endWordPosition="491">ies a fee” with two tweets regarding the same event: ”Whats wrong.. charging $$ for checking a/c” and ”Now they want a monthly fee!”. Though each of the tweets can be found similar to the long sentence by exact term matching, they do not share any single term. Yet, knowing that the words fee and charge are semantically related would allow discovering the similarity between the two tweets. External resources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic sentence clustering s</context>
</contexts>
<marker>Chen, Kuo, Su, 2003</marker>
<rawString>Hsin-Hsi Chen, June-Jei Kuo, and Tsei-Chun Su. 2003. Clustering and visualization in a multi-lingual multi-document summarization system. In Proceedings of the 25th European conference on IR research, ECIR’03, pages 266–280, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
</authors>
<title>Co-clustering documents and words using bipartite spectral graph partitioning.</title>
<date>2001</date>
<booktitle>In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’01,</booktitle>
<pages>269--274</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="15463" citStr="Dhillon (2001)" startWordPosition="2447" endWordPosition="2448">sterings for the bank dataset. Bank name is substituted with XXX. Cluster labels are given in italics. Two most frequent terms are assigned as cluster labels for K-means C. 6 Conclusions and Future Work We presented a novel sentence clustering scheme and evaluated its implementation, showing significantly superior performance over common sentence clustering techniques. We plan to further explore the suggested scheme by utilizing additional lexical resources and clustering algorithms. We also plan to compare our approach with co-clustering methods used in document clustering (Xu et al. (2003), Dhillon (2001), Slonim and Tishby (2000)). Acknowledgments This work was partially supported by the MAGNETON grant no. 43834 of the Israel Ministry of Industry, Trade and Labor, the Israel Ministry of Science and Technology, the Israel Science Foundation grant 1112/08, the PASCAL-2 Network of Excellence of the European Community FP7-ICT-2007-1-216886 and the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). 41 References Chris Biemann. 2006. Chinese whispers - an efficient graph clustering algorithm and its application to natural language process</context>
</contexts>
<marker>Dhillon, 2001</marker>
<rawString>Inderjit S. Dhillon. 2001. Co-clustering documents and words using bipartite spectral graph partitioning. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’01, pages 269–274, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<contexts>
<context position="14603" citStr="Dietterich, 1998" startWordPosition="2315" endWordPosition="2316">merge clusters than to reveal sub-clusters. E.g., a GS cluster of 20 sentences referring to fees and charges is covered by three projection clusters labeled fee, charge and interest rate, with 9, 8 and 2 sentences correspondingly. On the other hand, K-means C method places 11 out of the 20 sentences in a messy cluster of 57 sentences (see Table 3), scattering the remaining 9 sentences over 7 other clusters. In our current implementation fee, charge and interest rate were not detected by the lexical resources we used as semantically similar and thus were not 5p=0.001 according to McNemar test (Dietterich, 1998). fee, charge (57 sentences) XXX playing games wit my interest arguing w incompetent pol at XXX damansara perdana XXX’s upper management are a bunch of rude pricks XXX are ninjas at catching fraudulent charges. Table 3: Excerpt from resulting clusterings for the bank dataset. Bank name is substituted with XXX. Cluster labels are given in italics. Two most frequent terms are assigned as cluster labels for K-means C. 6 Conclusions and Future Work We presented a novel sentence clustering scheme and evaluated its implementation, showing significantly superior performance over common sentence clust</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2122" citStr="Erkan and Radev (2004)" startWordPosition="310" endWordPosition="313">asures, such as cosine, are used to define the level of similarity over bag-of-words encod38 ing of the sentences. Then, standard clustering algorithms can be applied to group sentences into clusters (see Steinbach et al. (2000) for an overview). The most common practice is representing the sentences as vectors in term space and applying the K-means clustering algorithm (Shen et al. (2011); Pasquier (2010); Wang et al. (2009); Nomoto and Matsumoto (2001); Boros et al. (2001)). An alternative approach involves partitioning a sentence connectivity graph by means of a graph clustering algorithm (Erkan and Radev (2004); Zha (2002)). The main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways. The shorter the sentences are, the less effective becomes exact matching of their terms. Compare the following newspaper sentence ”The bank is phasing out the EZ Checking package, with no monthly fee charged for balances over $1,500, and is instead offering customers its Basic Banking account, which carries a fee” with two tweets regarding the same event: ”Whats wrong.. charging $$ for checking a/c” and ”Now they want a monthly fee!”. Though eac</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res., 22(1):457–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet – An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3129" citStr="Fellbaum, 1998" startWordPosition="476" endWordPosition="478">instead offering customers its Basic Banking account, which carries a fee” with two tweets regarding the same event: ”Whats wrong.. charging $$ for checking a/c” and ”Now they want a monthly fee!”. Though each of the tweets can be found similar to the long sentence by exact term matching, they do not share any single term. Yet, knowing that the words fee and charge are semantically related would allow discovering the similarity between the two tweets. External resources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet – An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Massimiliano Gliozzo</author>
<author>Carlo Strapparava</author>
<author>Ido Dagan</author>
</authors>
<title>Investigating unsupervised learning for text categorization bootstrapping.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="6322" citStr="Gliozzo et al., 2005" startWordPosition="966" endWordPosition="969">rithms consider the graph’s edge weights. To address this trait, different edge weights can be assigned, reflecting the level of confidence that the two terms are indeed validly related and the reliability of the resource, which suggested the corresponding edge (e.g. WordNet synonyms are commonly considered more reliable than statistical thesauri). 3.2 Step 2: Projecting Sentences to Term Clusters To obtain sentence clusters, the given sentence set has to be projected in some manner over the term clusters obtained in Step 1. Our projection procedure resembles unsupervised text categorization (Gliozzo et al., 2005), with categories represented by term clusters that are not predefined but rather emerge from the analyzed data: 1. Represent term clusters and sentences as vectors in term space and calculate the similarity of each sentence with each of the term clusters. 2. Assign each sentence to the best-scoring term cluster. (We focus on hard clustering, but the procedure can be adapted for soft clustering). Various metrics for feature weighting and vector comparison may be chosen. The top terms of termcluster vectors can be regarded as labels for the corresponding sentence clusters. Thus each sentence cl</context>
</contexts>
<marker>Gliozzo, Strapparava, Dagan, 2005</marker>
<rawString>Alfio Massimiliano Gliozzo, Carlo Strapparava, and Ido Dagan. 2005. Investigating unsupervised learning for text categorization bootstrapping. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDDExplor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="11757" citStr="Hall et al., 2009" startWordPosition="1833" endWordPosition="1836">e cosine measure. Clusters were labeled with the top terms appearing both in the underlying term cluster and in the cluster’s sentences. 5 Results and Analysis In this section we present the results of evaluating our projection approach, compared to the common K-means clustering method4 applied to: (A) Standard bag-of-words representation of sentences; 2Available for download at www.cs.biu.ac.il/ ˜nlp/downloads/DIRECT.html. For each term we extract from the resource the top-5 related terms. 3Available at http://wortschatz.informatik. uni-leipzig.de/˜cbiemann/software/CW.html 4We use the Weka (Hall et al., 2009) implementation. Due to space limitations and for more meaningful comparison we report here one value of K, which is equal to the number of clusters returned by projection (60 for the ISP and 65 for the bank dataset). For K = 20, 40 and 70 the performance was similar. 40 (B) Bag-of-words representation, where sentence’s words are augmented with semantically-related terms (following the common scheme of prior work, see Section 2). We use the same set of related terms as is used by our method. (C) Representation of sentences in term-cluster space, using the term clusters generated by our method </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDDExplor. Newsl., 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Melissa L Holcombe</author>
<author>Regina Barzilay</author>
<author>Min yen Kan</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Simfinder: A flexible clustering tool for summarization. In</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL Workshop on Automatic Summarization,</booktitle>
<pages>41--49</pages>
<contexts>
<context position="3249" citStr="Hatzivassiloglou et al. (2001)" startWordPosition="496" endWordPosition="499">same event: ”Whats wrong.. charging $$ for checking a/c” and ”Now they want a monthly fee!”. Though each of the tweets can be found similar to the long sentence by exact term matching, they do not share any single term. Yet, knowing that the words fee and charge are semantically related would allow discovering the similarity between the two tweets. External resources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic sentence clustering scheme, which involves two consecutive steps: (1) gene</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Holcombe, Barzilay, Kan, McKeown, 2001</marker>
<rawString>Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L. Holcombe, Regina Barzilay, Min yen Kan, and Kathleen R. McKeown. 2001. Simfinder: A flexible clustering tool for summarization. In In Proceedings of the NAACL Workshop on Automatic Summarization, pages 41–49.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Hotho</author>
<author>S Staab</author>
<author>G Stumme</author>
</authors>
<title>Wordnet improves text document clustering.</title>
<date>2003</date>
<booktitle>Proceedings of the Semantic Web Workshop of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval (SIGIR 2003), August 1, 2003, Toronto Canada. Published Online at http://de. scientificcommons.org/608322.</booktitle>
<editor>In Ying Ding, Keith van Rijsbergen, Iadh Ounis, and Joemon Jose, editors,</editor>
<contexts>
<context position="3217" citStr="Hotho et al. (2003)" startWordPosition="492" endWordPosition="495">tweets regarding the same event: ”Whats wrong.. charging $$ for checking a/c” and ”Now they want a monthly fee!”. Though each of the tweets can be found similar to the long sentence by exact term matching, they do not share any single term. Yet, knowing that the words fee and charge are semantically related would allow discovering the similarity between the two tweets. External resources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic sentence clustering scheme, which involves</context>
</contexts>
<marker>Hotho, Staab, Stumme, 2003</marker>
<rawString>A. Hotho, S. Staab, and G. Stumme. 2003. Wordnet improves text document clustering. In Ying Ding, Keith van Rijsbergen, Iadh Ounis, and Joemon Jose, editors, Proceedings of the Semantic Web Workshop of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval (SIGIR 2003), August 1, 2003, Toronto Canada. Published Online at http://de. scientificcommons.org/608322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Hu</author>
<author>Xiaodan Zhang</author>
<author>Caimei Lu</author>
<author>E K Park</author>
<author>Xiaohua Zhou</author>
</authors>
<title>Exploiting wikipedia as external knowledge for document clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’09,</booktitle>
<pages>389--396</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3661" citStr="Hu et al., 2009" startWordPosition="553" endWordPosition="556">ich sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic sentence clustering scheme, which involves two consecutive steps: (1) generating relevant term clusters based on lexical semantic relatedness and (2) projecting the sentence set over these term clusters. Below we describe each of the two steps. 3.1 Step 1: Obtaining Term Clusters In order to obtain term clusters, a term connectivity graph is constructed for the given sentence set and is clustered as follows: 1. Create initially an undirected graph with sentence-set terms as nodes a</context>
</contexts>
<marker>Hu, Zhang, Lu, Park, Zhou, 2009</marker>
<rawString>Xiaohua Hu, Xiaodan Zhang, Caimei Lu, E. K. Park, and Xiaohua Zhou. 2009. Exploiting wikipedia as external knowledge for document clustering. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’09, pages 389–396, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>JNLE,</journal>
<pages>16--359</pages>
<contexts>
<context position="3622" citStr="Kotlerman et al., 2010" startWordPosition="547" endWordPosition="550">lized to provide such kind of knowledge, by which sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic sentence clustering scheme, which involves two consecutive steps: (1) generating relevant term clusters based on lexical semantic relatedness and (2) projecting the sentence set over these term clusters. Below we describe each of the two steps. 3.1 Step 1: Obtaining Term Clusters In order to obtain term clusters, a term connectivity graph is constructed for the given sentence set and is clustered as follows: 1. Create initially an undirected g</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. JNLE, 16:359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics - Volume 2, COLING ’98,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3587" citStr="Lin (1998)" startWordPosition="543" endWordPosition="544">l resources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic sentence clustering scheme, which involves two consecutive steps: (1) generating relevant term clusters based on lexical semantic relatedness and (2) projecting the sentence set over these term clusters. Below we describe each of the two steps. 3.1 Step 1: Obtaining Term Clusters In order to obtain term clusters, a term connectivity graph is constructed for the given sentence set and is clustered as follows: </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguistics - Volume 2, COLING ’98, pages 768–774, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, Juli.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, Cambridge, Juli.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A new approach to unsupervised text summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01,</booktitle>
<pages>26--34</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1958" citStr="Nomoto and Matsumoto (2001)" startWordPosition="283" endWordPosition="286">n 6 lists directions for future research. 2 Background Sentence clustering aims at grouping sentences with similar meanings into clusters. Commonly, vector similarity measures, such as cosine, are used to define the level of similarity over bag-of-words encod38 ing of the sentences. Then, standard clustering algorithms can be applied to group sentences into clusters (see Steinbach et al. (2000) for an overview). The most common practice is representing the sentences as vectors in term space and applying the K-means clustering algorithm (Shen et al. (2011); Pasquier (2010); Wang et al. (2009); Nomoto and Matsumoto (2001); Boros et al. (2001)). An alternative approach involves partitioning a sentence connectivity graph by means of a graph clustering algorithm (Erkan and Radev (2004); Zha (2002)). The main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways. The shorter the sentences are, the less effective becomes exact matching of their terms. Compare the following newspaper sentence ”The bank is phasing out the EZ Checking package, with no monthly fee charged for balances over $1,500, and is instead offering customers its Basic Banking</context>
</contexts>
<marker>Nomoto, Matsumoto, 2001</marker>
<rawString>Tadashi Nomoto and Yuji Matsumoto. 2001. A new approach to unsupervised text summarization. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01, pages 26–34, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Pasquier</author>
</authors>
<title>Task 5: Single document keyphrase extraction using sentence clustering and latent dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>154--157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1909" citStr="Pasquier (2010)" startWordPosition="277" endWordPosition="278">aluation results in Section 5. Section 6 lists directions for future research. 2 Background Sentence clustering aims at grouping sentences with similar meanings into clusters. Commonly, vector similarity measures, such as cosine, are used to define the level of similarity over bag-of-words encod38 ing of the sentences. Then, standard clustering algorithms can be applied to group sentences into clusters (see Steinbach et al. (2000) for an overview). The most common practice is representing the sentences as vectors in term space and applying the K-means clustering algorithm (Shen et al. (2011); Pasquier (2010); Wang et al. (2009); Nomoto and Matsumoto (2001); Boros et al. (2001)). An alternative approach involves partitioning a sentence connectivity graph by means of a graph clustering algorithm (Erkan and Radev (2004); Zha (2002)). The main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways. The shorter the sentences are, the less effective becomes exact matching of their terms. Compare the following newspaper sentence ”The bank is phasing out the EZ Checking package, with no monthly fee charged for balances over $1,500, an</context>
</contexts>
<marker>Pasquier, 2010</marker>
<rawString>Claude Pasquier. 2010. Task 5: Single document keyphrase extraction using sentence clustering and latent dirichlet allocation. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, pages 154–157, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL–Demonstrations ’04,</booktitle>
<pages>38--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3540" citStr="Pedersen et al., 2004" startWordPosition="533" endWordPosition="536"> discovering the similarity between the two tweets. External resources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic sentence clustering scheme, which involves two consecutive steps: (1) generating relevant term clusters based on lexical semantic relatedness and (2) projecting the sentence set over these term clusters. Below we describe each of the two steps. 3.1 Step 1: Obtaining Term Clusters In order to obtain term clusters, a term connectivity graph is constructed for the g</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL–Demonstrations ’04, pages 38–41, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shady Shehata</author>
</authors>
<title>A wordnet-based semantic model for enhancing text clustering.</title>
<date>2009</date>
<booktitle>Data Mining Workshops, International Conference on,</booktitle>
<pages>0--477</pages>
<contexts>
<context position="3176" citStr="Shehata (2009)" startWordPosition="485" endWordPosition="487">ount, which carries a fee” with two tweets regarding the same event: ”Whats wrong.. charging $$ for checking a/c” and ”Now they want a monthly fee!”. Though each of the tweets can be found similar to the long sentence by exact term matching, they do not share any single term. Yet, knowing that the words fee and charge are semantically related would allow discovering the similarity between the two tweets. External resources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic s</context>
</contexts>
<marker>Shehata, 2009</marker>
<rawString>Shady Shehata. 2009. A wordnet-based semantic model for enhancing text clustering. Data Mining Workshops, International Conference on, 0:477–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Tao Li</author>
<author>Chris H Q Ding</author>
</authors>
<title>Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis (plsa) with sentence bases.</title>
<date>2011</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1892" citStr="Shen et al. (2011)" startWordPosition="273" endWordPosition="276">task, followed by evaluation results in Section 5. Section 6 lists directions for future research. 2 Background Sentence clustering aims at grouping sentences with similar meanings into clusters. Commonly, vector similarity measures, such as cosine, are used to define the level of similarity over bag-of-words encod38 ing of the sentences. Then, standard clustering algorithms can be applied to group sentences into clusters (see Steinbach et al. (2000) for an overview). The most common practice is representing the sentences as vectors in term space and applying the K-means clustering algorithm (Shen et al. (2011); Pasquier (2010); Wang et al. (2009); Nomoto and Matsumoto (2001); Boros et al. (2001)). An alternative approach involves partitioning a sentence connectivity graph by means of a graph clustering algorithm (Erkan and Radev (2004); Zha (2002)). The main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways. The shorter the sentences are, the less effective becomes exact matching of their terms. Compare the following newspaper sentence ”The bank is phasing out the EZ Checking package, with no monthly fee charged for balance</context>
</contexts>
<marker>Shen, Li, Ding, 2011</marker>
<rawString>Chao Shen, Tao Li, and Chris H. Q. Ding. 2011. Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis (plsa) with sentence bases. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Slonim</author>
<author>Naftali Tishby</author>
</authors>
<title>Document clustering using word clusters via the information bottleneck method.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’00,</booktitle>
<pages>208--215</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="15489" citStr="Slonim and Tishby (2000)" startWordPosition="2449" endWordPosition="2452"> bank dataset. Bank name is substituted with XXX. Cluster labels are given in italics. Two most frequent terms are assigned as cluster labels for K-means C. 6 Conclusions and Future Work We presented a novel sentence clustering scheme and evaluated its implementation, showing significantly superior performance over common sentence clustering techniques. We plan to further explore the suggested scheme by utilizing additional lexical resources and clustering algorithms. We also plan to compare our approach with co-clustering methods used in document clustering (Xu et al. (2003), Dhillon (2001), Slonim and Tishby (2000)). Acknowledgments This work was partially supported by the MAGNETON grant no. 43834 of the Israel Ministry of Industry, Trade and Labor, the Israel Ministry of Science and Technology, the Israel Science Foundation grant 1112/08, the PASCAL-2 Network of Excellence of the European Community FP7-ICT-2007-1-216886 and the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). 41 References Chris Biemann. 2006. Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedin</context>
</contexts>
<marker>Slonim, Tishby, 2000</marker>
<rawString>Noam Slonim and Naftali Tishby. 2000. Document clustering using word clusters via the information bottleneck method. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’00, pages 208–215, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steinbach</author>
<author>G Karypis</author>
<author>V Kumar</author>
</authors>
<title>A comparison of document clustering techniques.</title>
<date>2000</date>
<booktitle>KDD Workshop on Text Mining.</booktitle>
<contexts>
<context position="1728" citStr="Steinbach et al. (2000)" startWordPosition="247" endWordPosition="250">ntence clustering approaches. Our suggested clustering scheme is presented in Section 3. Section 4 describes an implementation of the scheme for a particular industrial task, followed by evaluation results in Section 5. Section 6 lists directions for future research. 2 Background Sentence clustering aims at grouping sentences with similar meanings into clusters. Commonly, vector similarity measures, such as cosine, are used to define the level of similarity over bag-of-words encod38 ing of the sentences. Then, standard clustering algorithms can be applied to group sentences into clusters (see Steinbach et al. (2000) for an overview). The most common practice is representing the sentences as vectors in term space and applying the K-means clustering algorithm (Shen et al. (2011); Pasquier (2010); Wang et al. (2009); Nomoto and Matsumoto (2001); Boros et al. (2001)). An alternative approach involves partitioning a sentence connectivity graph by means of a graph clustering algorithm (Erkan and Radev (2004); Zha (2002)). The main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways. The shorter the sentences are, the less effective becom</context>
</contexts>
<marker>Steinbach, Karypis, Kumar, 2000</marker>
<rawString>M. Steinbach, G. Karypis, and V. Kumar. 2000. A comparison of document clustering techniques. KDD Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A large ontology from wikipedia and wordnet.</title>
<date>2007</date>
<contexts>
<context position="3697" citStr="Suchanek et al., 2007" startWordPosition="558" endWordPosition="561">n be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic sentence clustering scheme, which involves two consecutive steps: (1) generating relevant term clusters based on lexical semantic relatedness and (2) projecting the sentence set over these term clusters. Below we describe each of the two steps. 3.1 Step 1: Obtaining Term Clusters In order to obtain term clusters, a term connectivity graph is constructed for the given sentence set and is clustered as follows: 1. Create initially an undirected graph with sentence-set terms as nodes and use lexical resources to extract </context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A large ontology from wikipedia and wordnet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Shenghuo Zhu</author>
<author>Tao Li</author>
<author>Yihong Gong</author>
</authors>
<title>Multi-document summarization using sentence-based topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>297--300</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1929" citStr="Wang et al. (2009)" startWordPosition="279" endWordPosition="282">in Section 5. Section 6 lists directions for future research. 2 Background Sentence clustering aims at grouping sentences with similar meanings into clusters. Commonly, vector similarity measures, such as cosine, are used to define the level of similarity over bag-of-words encod38 ing of the sentences. Then, standard clustering algorithms can be applied to group sentences into clusters (see Steinbach et al. (2000) for an overview). The most common practice is representing the sentences as vectors in term space and applying the K-means clustering algorithm (Shen et al. (2011); Pasquier (2010); Wang et al. (2009); Nomoto and Matsumoto (2001); Boros et al. (2001)). An alternative approach involves partitioning a sentence connectivity graph by means of a graph clustering algorithm (Erkan and Radev (2004); Zha (2002)). The main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways. The shorter the sentences are, the less effective becomes exact matching of their terms. Compare the following newspaper sentence ”The bank is phasing out the EZ Checking package, with no monthly fee charged for balances over $1,500, and is instead offerin</context>
</contexts>
<marker>Wang, Zhu, Li, Gong, 2009</marker>
<rawString>Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong Gong. 2009. Multi-document summarization using sentence-based topic models. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 297–300, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Xin Liu</author>
<author>Yihong Gong</author>
</authors>
<title>Document clustering based on non-negative matrix factorization.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03,</booktitle>
<pages>267--273</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="15447" citStr="Xu et al. (2003)" startWordPosition="2443" endWordPosition="2446">from resulting clusterings for the bank dataset. Bank name is substituted with XXX. Cluster labels are given in italics. Two most frequent terms are assigned as cluster labels for K-means C. 6 Conclusions and Future Work We presented a novel sentence clustering scheme and evaluated its implementation, showing significantly superior performance over common sentence clustering techniques. We plan to further explore the suggested scheme by utilizing additional lexical resources and clustering algorithms. We also plan to compare our approach with co-clustering methods used in document clustering (Xu et al. (2003), Dhillon (2001), Slonim and Tishby (2000)). Acknowledgments This work was partially supported by the MAGNETON grant no. 43834 of the Israel Ministry of Industry, Trade and Labor, the Israel Ministry of Science and Technology, the Israel Science Foundation grant 1112/08, the PASCAL-2 Network of Excellence of the European Community FP7-ICT-2007-1-216886 and the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). 41 References Chris Biemann. 2006. Chinese whispers - an efficient graph clustering algorithm and its application to natural </context>
</contexts>
<marker>Xu, Liu, Gong, 2003</marker>
<rawString>Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based on non-negative matrix factorization. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03, pages 267–273, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyuan Zha</author>
</authors>
<title>Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering.</title>
<date>2002</date>
<booktitle>In SIGIR,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="2134" citStr="Zha (2002)" startWordPosition="314" endWordPosition="315">are used to define the level of similarity over bag-of-words encod38 ing of the sentences. Then, standard clustering algorithms can be applied to group sentences into clusters (see Steinbach et al. (2000) for an overview). The most common practice is representing the sentences as vectors in term space and applying the K-means clustering algorithm (Shen et al. (2011); Pasquier (2010); Wang et al. (2009); Nomoto and Matsumoto (2001); Boros et al. (2001)). An alternative approach involves partitioning a sentence connectivity graph by means of a graph clustering algorithm (Erkan and Radev (2004); Zha (2002)). The main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways. The shorter the sentences are, the less effective becomes exact matching of their terms. Compare the following newspaper sentence ”The bank is phasing out the EZ Checking package, with no monthly fee charged for balances over $1,500, and is instead offering customers its Basic Banking account, which carries a fee” with two tweets regarding the same event: ”Whats wrong.. charging $$ for checking a/c” and ”Now they want a monthly fee!”. Though each of the twe</context>
</contexts>
<marker>Zha, 2002</marker>
<rawString>Hongyuan Zha. 2002. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In SIGIR, pages 113– 120.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>