<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99771">
Automated Construction of Database Interfaces: Integrating
Statistical and Relational Learning for Semantic Parsing
</title>
<author confidence="0.980307">
Lappoon R. Tang and Raymond J. Mooney
</author>
<affiliation confidence="0.9980735">
Department of Computer Sciences
University of Texas at Austin
</affiliation>
<address confidence="0.759269">
Austin, TX 78712-1188
</address>
<email confidence="0.604907">
Irupert ,mooneyl@cs .utexas.edu
</email>
<sectionHeader confidence="0.974269" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954125">
The development of natural language inter-
faces (NLI&apos;s) for databases has been a chal-
lenging problem in natural language process-
ing (NLP) since the 1970&apos;s. The need for
NLI&apos;s has become more pronounced due to the
widespread access to complex databases now
available through the Internet. A challenging
problem for empirical NLP is the automated
acquisition of NLI&apos;s from training examples.
We present a method for integrating statisti-
cal and relational learning techniques for this
task which exploits the strength of both ap-
proaches. Experimental results from three dif-
ferent domains suggest that such an approach
is more robust than a previous purely logic-
based approach.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966685185185">
We use the term semantic parsing to refer
to the process of mapping a natural language
sentence to a structured meaning representa-
tion. One interesting application of semantic
parsing is building natural language interfaces
for online databases. The need for such appli-
cations is growing since when information is
delivered through the Internet, most users do
not know the underlying database access lan-
guage. An example of such an interface that
we have developed is shown in Figure 1.
Traditional (rationalist) approaches to con-
structing database interfaces require an ex-
pert to hand-craft an appropriate semantic
parser (Woods, 1970; Hendrix et al., 1978).
However, such hand-crafted parsers are time
consuming to develop and suffer from prob-
lems with robustness and incompleteness even
for domain specific applications. Neverthe-
less, very little research in empirical NLP has
explored the task of automatically acquiring
such interfaces from annotated training ex-
amples. The only exceptions of which we
are aware are a statistical approach to map-
ping airline-information queries into SQL pre-
sented in (Miller et al., 1996), a probabilistic
decision-tree method for the same task de-
scribed in (Kuhn and De Mori, 1995), and
an approach using relational learning (a.k.a.
inductive logic programming, ILP) to learn a
logic-based semantic parser described in (Zelle
and Mooney, 1996).
The existing empirical systems for this task
employ either a purely logical or purely sta-
tistical approach. The former uses a deter-
ministic parser, which can suffer from some
of the same robustness problems as rational-
ist methods. The latter constructs a prob-
abilistic grammar, which requires supplying
a sytactic parse tree as well as a semantic
representation for each training sentence, and
requires hand-crafting a small set of contex-
tual features on which to condition the pa-
rameters of the model. Combining relational
and statistical approaches can overcome the
need to supply parse-trees and hand-crafted
features while retaining the robustness of sta-
tistical parsing. The current work is based
on the CHILL logic-based parser-acquisition
framework (Zelle and Mooney, 1996), retain-
ing access to the complete parse state for mak-
ing decisions, but building a probabilistic re-
lational model that allows for statistical pars-
ing.
</bodyText>
<sectionHeader confidence="0.895425" genericHeader="method">
2 Overview of the Approach
</sectionHeader>
<bodyText confidence="0.996255333333333">
This section reviews our overall approach
using an interface developed for a U.S.
Geography database (Geoquery) as a
sample application (Zelle and Mooney,
1996) which is available on the Web (see
http://www.cs.utexas.eduhisers/m1/geo.html).
</bodyText>
<subsectionHeader confidence="0.963478">
2.1 Semantic Representation
</subsectionHeader>
<bodyText confidence="0.997513">
First-order logic is used as a semantic repre-
sentation language. CHILL has also been ap-
plied to a restaurant database in which the
logical form resembles SQL, and is translated
</bodyText>
<page confidence="0.996733">
133
</page>
<figure confidence="0.765488178571428">
KY. tatlra it Query byAkon
A LowningNatzaal-Language Interface to a N. Catifornia Remaaant
Databass
Pusul 1 Irt. ED.I,Limut °wry &apos;Zra4nrt
THE QUERY YOU POSTED:
Where it a pcoa euneee reatmsent IA Palo Alto/
RESULT:
I-4z ..-r .AuPANT, . 0,04,x, )10, L&apos; L - ..-m-
lcrunAreiouREsrntrum lei 005 37 ALF-4Wo
OEM LION PUS la cam REAL ALO ALTO
IFEEm yore morDApannain4 I1-----IIIL CAtAIN3 PEAL ALO ALIO
MA SZECHWARIAIAN 11E4E7 140 NPT :ALO ALTO
bdANTARINOOLIFSAET --.11tAACAIAIrr AIVO AMM, P----
0131VILLAIl70:I PEMMICAN%) RD IPALO ALIO &apos;3A
IMR CliAUS CHINESE FASTFOOD ItTI IEL CAM:NORM PALO ALTO 32
PERM DWXRESCAURANT pp tEL CAM/kV/REAL IPALO Arm pa
fm000nRennvann Ms IBRYARr sr jeuo &amp;To ILI
/31) MOM RBSTAITRANT 13I—rra. CAMIIV WAY ALO ALTO
OfalaCt Indesnair at niaNa &apos;TIE SOL GENERATED:
SELECT CDIEMINFO.P.EST_NAME LOCATIONS.X0315E_MO.
LIICAnONS MEE, aka:. WC:4MM are)1.IE. GEMERalaw, WING
FROM GENERALWO, LOCA1101I3
504151
GENEMLIWORaTIMG ..25.4.413
0001MUPffOJO50_Tyfl.tsncsrAaoo
LOCAIIONS CrlY_VaxE • -MO al14^ aaa
GENEnal NFU REST_O• LOCanONS frETUD
•
</figure>
<figureCaption confidence="0.999542">
Figure 1: Screenshots of a Learned Web-based NL Database Interface
</figureCaption>
<bodyText confidence="0.992667285714286">
automatically into SQL (see Figure 1). We
explain the features of the Geoquery repre-
sentation language through a sample query:
Input: &amp;quot;What is the largest city in Texas?&amp;quot;
Query: answer(C,largest(C,(city(C),loc(C,S),
const(S,stateid(texas))))).
Objects are represented as logical terms and
are typed with a semantic category using
logical functions applied to possibly ambigu-
ous English constants (e.g. stateid(Mississippi),
riverid(Mississippi)). Relationships between ob-
jects are expressed using predicates; for in-
stance, loc(X,Y) states that X is located in Y.
We also need to handle quantifiers such
as &apos;largest&apos;. We represent these using meta-
predicates for which at least one argument is a
conjunction of literals. For example, largest(X,
Goal) states that the object X satisfies Goal
and is the largest object that does so, using
the appropriate measure of size for objects of
its type (e.g. area for states, population for
cities). Finally, an unspecified object required
as an argument to a predicate can appear else-
where in the sentence, requiring the use of the
predicate const(X,C) to bind the variable X to
the constant C. Some other database queries
(or training examples) for the U.S. Geography
domain are shown below:
</bodyText>
<subsectionHeader confidence="0.68797">
What is the capital of Texas?
</subsectionHeader>
<bodyText confidence="0.877145333333333">
answer(C,(capital(C,S),const(S,stateid(texas)))).
What state has the most rivers running through it?
answer(S,most(S,R,(state(S),river(R),traverse(R,S)))).
</bodyText>
<subsectionHeader confidence="0.999893">
2.2 Parsing Actions
</subsectionHeader>
<bodyText confidence="0.99991762962963">
Our semantic parser employs a shift-reduce
architecture that maintains a stack of pre-
viously built semantic constituents and a
buffer of remaining words in the input. The
parsing actions are automatically generated
from templates given the training data. The
templates are INTRODUCE, COREF_VARS,
DROP_CONJ, LIFT_CONJ, and SHIFT. IN-
TRODUCE pushes a predicate onto the stack
based on a word appearing in the input and
information about its possible meanings in
the lexicon. COREF_VARS binds two argu-
ments of two different predicates on the stack.
DROP_CONJ (or LIFT_CONJ) takes a pred-
icate on the stack and puts it into one of the
arguments of a meta-predicate on the stack.
SHIFT simply pushes a word from the input
buffer onto the stack. The parsing actions are
tried in exactly this order. The parser also
requires a lexicon to map phrases in the in-
put into specific predicates, this lexicon can
also be learned automatically from the train-
ing data (Thompson and Mooney, 1999).
Let&apos;s go through a simple trace of parsing
the request &amp;quot;What is the capital of Texas?&amp;quot;
A lexicon that maps &apos;capital&apos; to &apos;capital(_,_)&apos;
and &apos;Texas&apos; to &apos;const(_,stateid(texas))&apos; suffices
</bodyText>
<page confidence="0.99791">
134
</page>
<bodyText confidence="0.999923">
here. Interrogatives like &amp;quot;what&amp;quot; may be
mapped to predicates in the lexicon if neces-
sary. The parser begins with an initial stack
and a buffer holding the input sentence. Each
predicate on the parse stack has an attached
buffer to hold the context in which it was
introduced; words from the input sentence
are shifted onto this buffer during parsing.
The initial parse state is shown below:
</bodyText>
<subsectionHeader confidence="0.9917835">
Parse Stack: [answer(_,_):D]
Input Buffer: [whatis,the,capital,ottexas,1
</subsectionHeader>
<bodyText confidence="0.9998558">
Since the first three words in the input
buffer do not map to any predicates, three
SHIFT actions are performed. The next is an
INTRODUCE as &apos;capital&apos; is at the head of
the input buffer:
</bodyText>
<subsectionHeader confidence="0.940314">
Parse Stack: Icapital(_,_):0, answer(_,_):[the,is,what]]
Input Buffer: [capital,of,texas,1
</subsectionHeader>
<bodyText confidence="0.9999688">
The next action is a COREF_VARS that
binds the first argument of capital(_,_) with
the first argument of answer(_,_).
The next sequence of steps are two SHIFT&apos;s,
an INTRODUCE, and then a COREF_VARS:
</bodyText>
<subsectionHeader confidence="0.77133325">
Parse Stack: [const(S,stateid(texas)):0,
capital(C,5):[of,ca pitaq,
answer(C,_):[the,is,what]]
Input Buffer: [texas,?]
</subsectionHeader>
<bodyText confidence="0.997466">
The last four steps are two DROP_CONJ&apos;s
followed by two SHIFT&apos;s:
</bodyText>
<subsectionHeader confidence="0.556191">
Parse Stack: [answer(C, (capital(C,5),
</subsectionHeader>
<bodyText confidence="0.6680898">
const(S,stateid(texas)))):
[?,texas,of,capital,the,is,whatlj
Input Buffer: [1
This is the final state and the logical query is
extracted from the stack.
</bodyText>
<subsectionHeader confidence="0.998925">
2.3 Learning Control Rules
</subsectionHeader>
<bodyText confidence="0.999973523809524">
The initially constructed parser has no con-
straints on when to apply actions, and is
therefore overly general and generates numer-
ous spurious parses. Positive and negative ex-
amples are collected for each action by parsing
each training example and recording the parse
states encountered. Parse states to which an
action should be applied (i.e. the action leads
to building the correct semantic representa-
tion) are labeled positive examples for that
action. Otherwise, a parse state is labeled a
negative example for an action if it is a posi-
tive example for another action below the cur-
rent one in the ordered list of actions. Control
conditions which decide the correct action for
a given parse state are learned for each action
from these positive and negative examples.
The initial CHILL system used ILP (Lavrac
and Dzeroski, 1994) to learn Prolog control
rules and employed deterministic parsing, us-
ing the learned rules to decide the appropriate
parse action for each state. The current ap-
proach learns a model for estimating the prob-
ability that each action should be applied to
a given state, and employs statistical parsing
(Manning and Schiitze, 1999) to try to find
the overall most probable parse, using beam
search to control the complexity. The advan-
tage of ILP is that it can perform induction
over the logical description of the complete
parse state without the need to pre-engineer a
fixed set of features (which vary greatly from
one domain to another) that are relevant to
making decisions. We maintain this advan-
tage by using ILP to learn a committee of
hypotheses, and basing probability estimates
on a weighted vote of them (Ali and Pazzani,
1996). We believe that using such a proba-
bilistic relational model (Getoor and Jensen,
2000) combines the advantages of frameworks
based on first-order logic and those based on
standard statistical techniques.
</bodyText>
<sectionHeader confidence="0.986569" genericHeader="method">
3 The TABULATE ILP Method
</sectionHeader>
<bodyText confidence="0.999836666666667">
This section discusses the ILP method used to
build a committee of logical control hypothe-
ses for each action.
</bodyText>
<subsectionHeader confidence="0.999161">
3.1 The Basic TABULATE Algorithm
</subsectionHeader>
<bodyText confidence="0.999839071428571">
Most ILP methods use a set-covering method
to learn one clause (rule) at a time and con-
struct clauses using either a strictly top-down
(general to specific) or bottom-up (specific to
general) search through the space of possi-
ble rules (Lavrac and Dzeroski, 1994). TAB-
ULATE,1 on the other hand, employs both
bottom-up and top-down methods to con-
struct potential clauses and searches through
the hypothesis space of complete logic pro-
grams (i.e. sets of clauses called theories). It
uses beam search to find a set of alternative
hypotheses guided by a theory evaluation met-
ric discussed below. The search starts with
</bodyText>
<footnote confidence="0.7355885">
1TABULATE stands for Top-down And Bottom-Up
cLAuse construction with Theory Evaluation.
</footnote>
<table confidence="0.663927">
Parse Stack: Icapital(C,_):0, answer(C,_):[the,is,what]]
Input Buffer: [capital,of,texas,?]
</table>
<page confidence="0.732593">
135
</page>
<figure confidence="0.977333222222222">
Procedure Tabulate
Input:
the target concept to learn
the 9 examples
the 9 examples
Output:
Q: a queue of learned theories
Theory° := +-I E E el /* the initial theory */
T(No) := Theom /* theory of node No */
C(No) := empty /* the clause being built */
Q := [No] /* the search queue */
Repeat
CQ := 0
For each search node Ni E Q Do
If C(Ni) = empty or C(Ni) = fail Then
Pairs := sampling of S pairs of clauses from T(Ni)
Find LGG G in Pairs with the greatest cover in e
:= Refine_Clause(t(X1,- ,X,) U
Refine_Clause(G 4--)
Else
:= flefine_Clause(C(Ni))
End If
If Ri =0 Then
CQ, := {(T(N,), fail)}
Else
CQi := {(Complete(T(N,),Gi,e+),nexti)
for each G E Ri, next&apos; = empty if G3
satisfies the noise criteria; otherwise, Gil
End If
CQ := CQ U CQi
End For
Q := the B best nodes from Q U CQ
ranked by metric M
Until termination-criteria-satisfied
Return Q
End Procedure
</figure>
<figureCaption confidence="0.999976">
Figure 2: The TABULATE algorithm
</figureCaption>
<bodyText confidence="0.999975551020408">
the most specific hypothesis (the set of posi-
tive examples each represented as a separate
clause). Each iteration of the loop attempts
to refine each of the hypotheses in the current
search queue. There are two cases in each it-
eration: 1) an existing clause in a theory is
refined or 2) a new clause is begun. Clauses
are learned using both top-down specializa-
tion using a method similar to FOIL (Quin-
lan, 1990) and bottom-up generalization using
Least General Generalizations (LGG&apos;s). Ad-
vantages of combining both ILP approaches
were explored in Cm:um (Zelle and Mooney,
1994), an ILP method which motivated the
design of TABULATE. An outline of the TAB-
ULATE algorithm is given in Figure 2.
A noise-handling criterion is used to de-
cide when an individual clause in a hypoth-
esis is sufficiently accurate to be permanently
retained. There are three possible outcomes
in a refinement: 1) the current clause satisfies
the noise-handling criterion and is simply re-
turned (nextj is set to empty), 2) the current
clause does not satisfy the noise-handling cri-
teria and all possible refinements are returned
(nextj is set to the refined clause), and 3)
the current clause does not satisfy the noise-
handling criterion but there are no further re-
finements (nexti is set to fail). If the refine-
ment is a new clause, clauses in the current
theory subsumed by it are removed. Oth-
erwise, it is a specialization of an existing
clause. Positive examples that are not cov-
ered by the resulting theory, due to special-
izing the clause, are added back into theory
as individual clauses. Hence, the theory is al-
ways maintained complete (i.e. covering all
positive examples). These final steps are per-
formed in the Complete procedure.
The termination criterion checks for two
conditions. The first is satisfied if the next
search queue does not improve the sum of
the metric score over all hypotheses in the
queue. Second, there is no clause currently
being built for each theory in the search queue
and the last finished clause of each theory sat-
isfies the noise-handling criterion. Finally, a
committee of hypotheses found by the algo-
rithm is returned.
</bodyText>
<subsectionHeader confidence="0.997437">
3.2 Compression and Accuracy
</subsectionHeader>
<bodyText confidence="0.999911">
The goal of the search is to find accurate
and yet simple hypotheses. We measure accu-
racy using the m-estimate (Cestnik, 1990), a
smoothed measure of accuracy on the training
data which in the case of a two-class problem
is defined as:
</bodyText>
<equation confidence="0.999738">
s+m-p+ (1)
accuracy(H) =
n + m
</equation>
<bodyText confidence="0.9996958">
where s is the number of positive examples
covered by the hypothesis H, n is the total
number of examples covered, p+ is the prior
probability of the class ED, and m is a smooth-
ing parameter.
We measure theory complexity using a met-
ric similar to that introduced in (Muggleton
and Buntine, 1988). The size of a Clause hav-
ing a Head and a Body is defined as follows
(ts= &amp;quot;term size&amp;quot; and ar=&amp;quot;arity&amp;quot;):
</bodyText>
<equation confidence="0.8862795">
size(Clause) =l+ts(Head)+ts(Body) (2)
136
T is a variable
ts(T) / 2 T is a constant
2 + E11`!1T) ts(argi(T)) otherwise.
(3)
</equation>
<bodyText confidence="0.9994262">
The size of a clause is roughly the number of
variables, constants, or predicate symbols it
contains. The size of a theory is the sum of
the sizes of its clauses. The metric M(H) used
as the search heuristic is defined as:
</bodyText>
<equation confidence="0.999907333333333">
accuracy(H) + C
M (H) = (4)
log2 size(H)
</equation>
<bodyText confidence="0.9997644">
where C is a constant used to control the rel-
ative weight of accuracy vs. complexity. We
assume that the most general hypothesis is as
good as the most specific hypothesis; thus, C
is determined to be:
</bodyText>
<equation confidence="0.858600666666667">
EbSt — EtSb
C = (5)
Sb—St
</equation>
<bodyText confidence="0.999436666666667">
where Et, Eb are the accuracy estimates of the
most general and most specific hypotheses re-
spectively, and St, Sb are their sizes.
</bodyText>
<subsectionHeader confidence="0.998238">
3.3 Noise Handling
</subsectionHeader>
<bodyText confidence="0.986454666666667">
A clause needs no further refinement when it
meets the following criterion (as in RIPPER
(Cohen, 1995)):
</bodyText>
<equation confidence="0.896465333333333">
p — n
&gt; (6)
p n
</equation>
<bodyText confidence="0.999969428571428">
where p is the number of positive examples
covered by the clause, n is the number of neg-
ative examples covered and —1 &lt; &lt; 1 is a
parameter. The value of 0 is decreased when-
ever the sum of the metric over the hypotheses
in the queue does not improve although some
of them still have unfinished or failed clauses.
</bodyText>
<sectionHeader confidence="0.962631" genericHeader="method">
4 Statistical Semantic Parsing
</sectionHeader>
<subsectionHeader confidence="0.584431">
4.1 The Parsing Model
</subsectionHeader>
<bodyText confidence="0.999077394736842">
A parser is a relation Parser C Sentences x
Queries where Sentences and Queries are
the sets of natural language sentences and
database queries respectively. Given a sen-
tence 1 E Sentences, the set Q(1) = {q E
Queries I (1,q) E Parser} is the set of queries
that are correct interpretations of 1.
A parse state consists of a stack of lexical-
ized predicates and a list of words from the
input sentence. S is the set of states reach-
able by the parser. Suppose our learned parser
has n different parsing actions, the ith ac-
tion ai is a function ai(s) : ISi -4 OSi where
iSi C S is the set of states to which the ac-
tion is applicable and OSi C S is the set of
states constructed by the action. The function
ao(1) : Sentences IniS maps each sentence 1
to a corresponding unique initial parse state in
IniS C S. A state is called a final state if there
exists no parsing action applicable to it. The
partial function an+i(s) : FS -+ Queries is
defined as the action that retrieves the query
from the final state s E FS C S if one exists.
Some final states may not &amp;quot;contain&amp;quot; a query
(e.g. when the parse stack contains predicates
with unbound variables) and therefore it is a
partial function. When the parser meets such
a final state, it reports a failure.
A path is a finite sequence of parsing ac-
tions. Given a sentence 1, a good state s is
one such that there exists a path from it to a
query q E Q(l). Otherwise, it is a bad state.
The set of parse states can be uniquely divided
into the set of good states and the set of bad
states given 1 and Parser. S+ and S- are the
sets of good and bad states respectively.
Given a sentence 1, the goal is to construct
the query 4 such that
</bodyText>
<equation confidence="0.694991">
4= argmax P(q E Q(1) I 1 q) (7)
</equation>
<bodyText confidence="0.999931333333333">
where 1 = q means a path exists from 1 to q.
Now, we need to estimate P(q E Q(1) I 1
q). First, we notice that:
</bodyText>
<equation confidence="0.9990665">
P(q E Q(1) I 1 = q) = (8)
P(s E FS 4&amp;quot; 11 =. $ and an+i(s) = q)
</equation>
<bodyText confidence="0.998956692307692">
where FS + = FS n SF. For notational con-
venience we drop the conditions and denote
the above probabilities as P(q E Q(1)) and
P(s E FS+) respectively, assuming these con-
ditions in the following discussion. The equa-
tion states that the probability that a given
query is a correct meaning for 1 is the same as
the probability that the final state (reached
by parsing 1) is a good state. We need to de-
termine in general the probability of having a
good resulting parse state. Given any parse
state si at the jth step of parsing and an ac-
tion ai such that si+i = ai(si), we have:
</bodyText>
<equation confidence="0.999661666666667">
P(s3+1. E OSt) = (9)
P(Sj+i E OSt I sj E /St)P(sj E /St) ±
P(sj+1 E OS t I si ISt)P(si Mt)
</equation>
<bodyText confidence="0.999378">
where /St = IS n SI&apos; and OS t = 0S n S.
Since no parsing action can produce a good
</bodyText>
<page confidence="0.993812">
137
</page>
<bodyText confidence="0.728130666666667">
parse state from a bad one, the second term
is zero. Now, we are ready to derive P(q E
Q(1)). Suppose q = an+i(sm), we have:
</bodyText>
<equation confidence="0.9998282">
P(q E Q(1))
= P(sm EFS)
= P(sm E FS+. I E
P(sj E sj_i E is,+,_,)
P(s2 E 04,J 3 E /5)P(si E IS)
</equation>
<bodyText confidence="0.99967325">
where ak denotes the index of which action
is applied at the kth step. We assume that
= P(si E /4) 0 0 (which may not be true
in general). Now, we have
</bodyText>
<equation confidence="0.912319333333333">
m-i
P(q E Q(1)) = P + Eosz Isj E /5:3 ).
(11)
</equation>
<bodyText confidence="0.9803634">
Next we describe how we estimate the proba-
bility of the goodness of each action in a given
state (P(ai(s) E OSt I s 1St)). We need
not estimate 7 since its value does not affect
the outcome of equation (7).
</bodyText>
<subsectionHeader confidence="0.999325">
4.2 Estimating Probabilities for
Parsing Actions
</subsectionHeader>
<bodyText confidence="0.9999943">
The committee of hypotheses learned by TAB-
ULATE is used to estimate the probability that
a particular action is a good one to apply to a
given parse state. Some hypotheses are more
&amp;quot;important&amp;quot; than others in the sense that they
carry more weight in the decision. A weight-
ing parameter is also included to lower the
probability estimate of actions that appear
further down the decision list. For actions ai
where 1 &lt; i &lt; n — 1:
</bodyText>
<equation confidence="0.996853333333333">
P(ai(s) E OSt I s E 1St) = (12)
E AkP(ai(s) E 05t I hk)
hkEHi
</equation>
<bodyText confidence="0.975764666666667">
where s is a given parse state, pos(i) is the
position of the action ai in the list of ac-
tions applicable to state s, Ak and 0 &lt; /.1 &lt;
1 are weighting parameters,2 Hi is the set
of hypotheses learned for the action a, and
Ek Ak = 1-
To estimate the probability for the last ac-
tion an, we devise a simple test that checks
if the maximum of the set A(s) of proba-
bility estimates for the subset of the actions
2i.s is set to 0.95 for all the experiments performed.
{ ai, , an_i } applicable to s is less than or
equal to a threshold a. If A(s) is empty, we
assume the maximum is zero. More precisely,
if max(A(s)) &lt;a
otherwise
where a is the threshold,3 c(an(s) E 04) is
the count of the number of good states pro-
duced by the last action, and c(s E /4) is the
count of the number of good states to which
the last action is applicable.
Now, let&apos;s discuss how P(ai(s) E OS? I hk)
and Ak are estimated. If h = $ (i.e. hk covers
s), we have
</bodyText>
<equation confidence="0.79869">
P(ai(s) E aSyji- I h,) = Pc + 9 • nc
Pc ± Tic
</equation>
<bodyText confidence="0.94783575">
where pc and nc are the number of positive
and negative examples covered by hk respec-
tively. Otherwise, if hk s (i.e. hk does not
cover s), we have
</bodyText>
<equation confidence="0.921129">
Pu71
P(ai(s) E OSt I hk) =+0- u (15)
</equation>
<subsectionHeader confidence="0.481425">
Pu nu
</subsectionHeader>
<bodyText confidence="0.999810769230769">
where Pu and nu are the number of positive
and negative examples rejected by hk respec-
tively. 0 is the probability that a negative
example is mislabelled and its value can be
estimated given 13 (in equation (6)) and the
total number of positive and negative exam-
ples.
One could use a variety of linear combina-
tion methods to estimate the weights Ak (e.g.
Bayesian combination (Buntine, 1990)). How-
ever, we have taken a simple approach and
weighted hypotheses based on their relative
simplicity:
</bodyText>
<equation confidence="0.85020575">
size(hk)i
— •
- size(
—1 h=1 .7
</equation>
<subsectionHeader confidence="0.996223">
4.3 Searching for a Parse
</subsectionHeader>
<bodyText confidence="0.999564333333333">
To find the most probably correct parse, the
parser employs a beam search. At each step,
the parser finds all of the parsing actions ap-
plicable to each parse state on the queue and
calculates the probability of goodness of each
of them using equations (12) and (13). It then
</bodyText>
<footnote confidence="0.956635">
3The threshold is set to 0.5 for all the experiments
performed.
</footnote>
<figure confidence="0.76616875">
(10) P(an(s) e OSZI s E IS) =
c(a. (s)E )
c(sE/Sti
(16)
</figure>
<page confidence="0.991278">
138
</page>
<bodyText confidence="0.999652857142857">
computes the probability that the resulting
state of each possible action is a good state
using equation (11), sorts the queue of possi-
ble next states accordingly, and keeps the best
B options. The parser stops when a complete
parse is found on the top of the parse queue
or a failure is reported.
</bodyText>
<sectionHeader confidence="0.977599" genericHeader="method">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.996283">
5.1 The Domains
</subsectionHeader>
<bodyText confidence="0.999988565217391">
Three different domains are used to demon-
strate the performance of the new approach.
The first is the U.S. Geography domain.
The database contains about 800 facts about
U.S. states like population, area, capital city,
neighboring states, major rivers, major cities,
and so on. A hand-crafted parser, GEOBASE
was previously constructed for this domain as
a demo product for Turbo Prolog. The second
application is the restaurant query system il-
lustrated in Figure 1. The database contains
information about thousands of restaurants
in Northern California, including the name of
the restaurant, its location, its specialty, and a
guide-book rating. The third domain consists
of a set of 300 computer-related jobs automat-
ically extracted from postings to the USENET
newsgroup aust in . jobs. The database con-
tains the following information: the job title,
the company, the recruiter, the location, the
salary, the languages and platforms used, and
required or desired years of experience and de-
grees.
</bodyText>
<subsectionHeader confidence="0.99695">
5.2 Experimental Design
</subsectionHeader>
<bodyText confidence="0.9999475">
The geography corpus contains 560 questions.
Approximately 100 of these were collected
from a log of questions submitted to the web
site and the rest were collected in studies in-
volving students in undergraduate classes at
our university. We also included results for the
subset of 250 sentences originally used in the
experiments reported in (Zelle and Mooney,
1996). The remaining questions were specif-
ically collected to be more complex than the
original 250, and generally require one or more
meta-predicates. The restaurant corpus con-
tains 250 questions automatically generated
from a hand-built grammar constructed to re-
flect typical queries in this domain. The job
corpus contains 400 questions automatically
generated in a similar fashion. The beam
width for TABULATE was set to five for all the
domains. The deterministic parser used only
the best hypothesis found. The experiments
were conducted using 10-fold cross validation.
For each domain, the average recall (a.k.a.
accuracy) and precision of the parser on dis-
joint test data are reported where:
</bodyText>
<listItem confidence="0.8539525">
# of correct queries produced
# of test sentences
# of correct queries produced
# of complete parses produced
</listItem>
<bodyText confidence="0.9998404">
A complete parse is one which contains an ex-
ecutable query (which could be incorrect). A
query is considered correct if it produces the
same answer set as the gold-standard query
supplied with the corpus.
</bodyText>
<sectionHeader confidence="0.561842" genericHeader="evaluation">
5.3 Results
</sectionHeader>
<bodyText confidence="0.999937717948718">
The results are presented in Table 1 and Fig-
ure 3. By switching from deterministic to
probabilistic parsing, the system increased the
number of correct queries it produced. Re-
call increases almost monotonically with pars-
ing beam width in most of the domains. Im-
provement is most apparent in the Jobs do-
main where probabilistic parsing significantly
outperformed the deterministic system (80%
vs 68%). However, using a beam width of
one (and thus the probabilistic parser picks
only the best action) results in worse perfor-
mance than using the original purely logic-
based deterministic parser. This suggests that
the probability esitmates could be improved
since overall they are not indicating the sin-
gle best action as well as a non-probabilistic
approach. Precision of the system decreased
with beam width, but not significantly except
for the larger Geography corpus. Since the
system conducts a more extensive search for
a complete parse, it risks increasing the num-
ber of incorrect as well as correct parses. The
importance of recall vs. precision depends on
the relative cost of providing an incorrect an-
swer versus no answer at all. Individual ap-
plications may require emphasizing one or the
other.
All of the experiments were run on a
167MHz UltraSparc work station under Sic-
stus Prolog. Although results on the parsing
time of the different systems are not formally
reported here, it was noted that the difference
between using a beam width of three and the
original system was less than two seconds in
all domains but increased to around twenty
seconds when using a beam width of twelve.
However, the current Prolog implementation
is not highly optimized.
</bodyText>
<equation confidence="0.5242945">
Recall
Precision =
</equation>
<page confidence="0.918037">
139
</page>
<table confidence="0.9996942">
Parsers \ Corpora Geo250 Geo560 Jobs400 Rest250
RP RP RP RP
Prob-Parser(12) 80.40 88.16 71.61 78.94 80.50 86.56 99.20 99.60
Prob-Parser(8) 79.60 86.90 71.07 79.76 78.75 86.54 99.20 99.60
Prob-Parser(5) 78.40 87.11 70.00 79.51 74.25 86.59 99.20 99.60
Prob-Pa,rser(3) 77.60 87.39 69.11 79.30 70.50 87.31 99.20 99.60
Prob-Parser(1) 67.60 90.37 62.86 82.05 34.25 85.63 99.20 99.60
TABULATE 75.60 92.65 69.29 89.81 68.50 87.54 99.20 99.60
Original CHILL 68.50 97.65
Hand-Built Parser 56.00 96.40
</table>
<tableCaption confidence="0.740585666666667">
Table 1: Results For All Domains- R = % Recall and P % Precision. Prob-Parser(B) is
the probabilistic parser using a beam width of B. TABULATE is CHILL using the TABULATE
induction algorithm with deterministic parsing.
</tableCaption>
<figureCaption confidence="0.976055">
Figure 3: The recall and precision of the parser using various beam widths in the different
domains
</figureCaption>
<figure confidence="0.994372222222222">
----------
----------- -----------
2 4 6 10 12
Seam Sae
Geo250
-
11e44250
100 100
90 -
80
70 --------- ----------------
40
50
Geo250
40 Jot e400 -
Rest250 --a -
30 o 2 4 6 a 10 12 75
Sewn Sae
</figure>
<bodyText confidence="0.99962784">
While there was an overall improvement in
recall using the new approach, its performance
varied significantly from domain to domain.
As a result, the recall did not always improve
dramatically by using a larger beam width.
Domain factors possibly affecting the perfor-
mance are the quality of the lexicon, the rel-
ative amount of data available for calculat-
ing probability estimates, and the problem of
&amp;quot;parser incompleteness&amp;quot; with respect to the
test data (i.e. there is not a path from a sen-
tence to a correct query which happens when
-y = 0). The performance of all systems were
basically equivalent in the restaurant domain,
where they were near perfect in both recall
and precision. This is because this corpus is
relatively easier given the restricted range of
possible questions due to the limited informa-
tion available about each restaurant. The sys-
tems achieved &gt; 90% in recall and precision
given only roughly 30% of the training data
in this domain. Finally, GEOBASE performed
the worst on the original geography queries,
since it is difficult to hand-craft a parser that
handles a sufficient variety of questions.
</bodyText>
<sectionHeader confidence="0.999549" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999979">
A probabilistic framework for semantic shift-
reduce parsing was presented. A new ILP
learning system was also introduced which
learns multiple hypotheses. These two tech-
niques were integrated to learn semantic
parsers for building NLI&apos;s to online databases.
Experimental results were presented that
demonstrate that such an approach outper-
forms a purely logical approach in terms of
the accuracy of the learned parser.
</bodyText>
<sectionHeader confidence="0.981509" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<footnote confidence="0.79509675">
This research was supported by a grant from
the Daimler-Chrysler Research and Technol-
ogy Center and by the National Science Foun-
dation under grant IRI-9704943.
</footnote>
<page confidence="0.996473">
140
</page>
<sectionHeader confidence="0.995825" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999827176470588">
K. Ali and M. Pazzani. 1996. Error reduction
through learning multiple descriptions. Ma-
chine Learning Journal, 24:3:100-132.
W. Buntine. 1990. A theory of learning classifica-
tion rules. Ph.D. thesis, University of Technol-
ogy, Sydney, Australia.
B. Cestnik. 1990. Estimating probabilities: A cru-
cial task in machine learning. In Proceedings of
the Ninth European Conference on Artificial In-
telligence, pages 147-149, Stockholm, Sweden.
W. W. Cohen. 1995. Fast effective rule induc-
tion. In Proceedings of the Twelfth Interna-
tional Conference on Machine Learning, pages
115-123.
L. Getoor and D. Jensen, editors. 2000. Papers
from the AAAI Workshop on Learning Statis-
tical Models from Relational Data, Austin, TX.
AAAI Press.
G. G. Hendrix, E. Sacerdoti, D. Sagalowicz, and
J. Slocum. 1978. Developing a natural language
interface to complex data. ACM Transactions
on Database Systems, 3(2):105-147.
R. Kuhn and R. De Mori. 1995. The application of
semantic classification trees to natural language
understanding. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 17(5):449-
460.
N. Lavrac and S. Dzeroski. 1994. Inductive Logic
Programming: Techniques and Applications.
Ellis Horwood.
C. D. Manning and H. Schiitze. 1999. Founda-
tions of Statistical Natural Language Process-
ing. MIT Press, Cambridge, MA.
Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical ap-
proach to natural language interfaces. In Pro-
ceedings of the 34th Annual Meeting of the As-
sociation for Computational Linguistics, pages
55-61, Santa Cruz, CA.
S. Muggleton and W. Buntine. 1988. Machine
invention of first-order predicates by inverting
resolution. In Proceedings of the Fifth Interna-
tional Conference on Machine Learning, pages
339-352, Ann Arbor, MI, June.
J. R. Quinlan 1990. Learning logical definitions
from relations. Machine Learning, 5(3):239--
266.
C. A. Thompson and R. J. Mooney. 1999. Au-
tomatic construction of semantic lexicons for
learning natural language interfaces. In Pro-
ceedings of the Sixteenth National Conference
on Artificial Intelligence, pages 487-493, Or-
lando, FL, July.
W. A. Woods. 1970. Transition network gram-
mars for natural language analysis. Communi-
cations of the Association for Computing Ma-
chinery, 13:591-606.
J. M. Zelle and R. J. Mooney. 1994. Combin-
ing top-down and bottom-up methods in induc-
tive logic programming In Proceedings of the
Eleventh International Conference on Machine
Learning, pages 343-351, New Brunswick, NJ,
July.
J. M. Zelle and It. J. Mooney. 1996. Learning
to parse database queries using inductive logic
programming In Proceedings of the Thirteenth
National Conference on Artificial Intelligence,
pages 1050-1055, Portland, OR, August.
</reference>
<page confidence="0.998252">
141
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.666284">
<title confidence="0.999274">Automated Construction of Database Interfaces: Statistical and Relational Learning for Semantic Parsing</title>
<author confidence="0.999416">Lappoon R Tang</author>
<author confidence="0.999416">J Raymond</author>
<affiliation confidence="0.994908">Department of Computer University of Texas at</affiliation>
<address confidence="0.773489">Austin, TX</address>
<email confidence="0.925672">,mooneyl@cs</email>
<abstract confidence="0.995137411764706">The development of natural language interfaces (NLI&apos;s) for databases has been a challenging problem in natural language processing (NLP) since the 1970&apos;s. The need for NLI&apos;s has become more pronounced due to the widespread access to complex databases now available through the Internet. A challenging problem for empirical NLP is the automated acquisition of NLI&apos;s from training examples. We present a method for integrating statistical and relational learning techniques for this task which exploits the strength of both approaches. Experimental results from three different domains suggest that such an approach is more robust than a previous purely logicbased approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Ali</author>
<author>M Pazzani</author>
</authors>
<title>Error reduction through learning multiple descriptions.</title>
<date>1996</date>
<journal>Machine Learning Journal,</journal>
<pages>24--3</pages>
<contexts>
<context position="10494" citStr="Ali and Pazzani, 1996" startWordPosition="1603" endWordPosition="1606"> that each action should be applied to a given state, and employs statistical parsing (Manning and Schiitze, 1999) to try to find the overall most probable parse, using beam search to control the complexity. The advantage of ILP is that it can perform induction over the logical description of the complete parse state without the need to pre-engineer a fixed set of features (which vary greatly from one domain to another) that are relevant to making decisions. We maintain this advantage by using ILP to learn a committee of hypotheses, and basing probability estimates on a weighted vote of them (Ali and Pazzani, 1996). We believe that using such a probabilistic relational model (Getoor and Jensen, 2000) combines the advantages of frameworks based on first-order logic and those based on standard statistical techniques. 3 The TABULATE ILP Method This section discusses the ILP method used to build a committee of logical control hypotheses for each action. 3.1 The Basic TABULATE Algorithm Most ILP methods use a set-covering method to learn one clause (rule) at a time and construct clauses using either a strictly top-down (general to specific) or bottom-up (specific to general) search through the space of possi</context>
</contexts>
<marker>Ali, Pazzani, 1996</marker>
<rawString>K. Ali and M. Pazzani. 1996. Error reduction through learning multiple descriptions. Machine Learning Journal, 24:3:100-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Buntine</author>
</authors>
<title>A theory of learning classification rules.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Technology,</institution>
<location>Sydney, Australia.</location>
<contexts>
<context position="22010" citStr="Buntine, 1990" startWordPosition="3780" endWordPosition="3781">i- I h,) = Pc + 9 • nc Pc ± Tic where pc and nc are the number of positive and negative examples covered by hk respectively. Otherwise, if hk s (i.e. hk does not cover s), we have Pu71 P(ai(s) E OSt I hk) =+0- u (15) Pu nu where Pu and nu are the number of positive and negative examples rejected by hk respectively. 0 is the probability that a negative example is mislabelled and its value can be estimated given 13 (in equation (6)) and the total number of positive and negative examples. One could use a variety of linear combination methods to estimate the weights Ak (e.g. Bayesian combination (Buntine, 1990)). However, we have taken a simple approach and weighted hypotheses based on their relative simplicity: size(hk)i — • - size( —1 h=1 .7 4.3 Searching for a Parse To find the most probably correct parse, the parser employs a beam search. At each step, the parser finds all of the parsing actions applicable to each parse state on the queue and calculates the probability of goodness of each of them using equations (12) and (13). It then 3The threshold is set to 0.5 for all the experiments performed. (10) P(an(s) e OSZI s E IS) = c(a. (s)E ) c(sE/Sti (16) 138 computes the probability that the resul</context>
</contexts>
<marker>Buntine, 1990</marker>
<rawString>W. Buntine. 1990. A theory of learning classification rules. Ph.D. thesis, University of Technology, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Cestnik</author>
</authors>
<title>Estimating probabilities: A crucial task in machine learning.</title>
<date>1990</date>
<booktitle>In Proceedings of the Ninth European Conference on Artificial Intelligence,</booktitle>
<pages>147--149</pages>
<location>Stockholm,</location>
<contexts>
<context position="14853" citStr="Cestnik, 1990" startWordPosition="2346" endWordPosition="2347">e performed in the Complete procedure. The termination criterion checks for two conditions. The first is satisfied if the next search queue does not improve the sum of the metric score over all hypotheses in the queue. Second, there is no clause currently being built for each theory in the search queue and the last finished clause of each theory satisfies the noise-handling criterion. Finally, a committee of hypotheses found by the algorithm is returned. 3.2 Compression and Accuracy The goal of the search is to find accurate and yet simple hypotheses. We measure accuracy using the m-estimate (Cestnik, 1990), a smoothed measure of accuracy on the training data which in the case of a two-class problem is defined as: s+m-p+ (1) accuracy(H) = n + m where s is the number of positive examples covered by the hypothesis H, n is the total number of examples covered, p+ is the prior probability of the class ED, and m is a smoothing parameter. We measure theory complexity using a metric similar to that introduced in (Muggleton and Buntine, 1988). The size of a Clause having a Head and a Body is defined as follows (ts= &amp;quot;term size&amp;quot; and ar=&amp;quot;arity&amp;quot;): size(Clause) =l+ts(Head)+ts(Body) (2) 136 T is a variable ts</context>
</contexts>
<marker>Cestnik, 1990</marker>
<rawString>B. Cestnik. 1990. Estimating probabilities: A crucial task in machine learning. In Proceedings of the Ninth European Conference on Artificial Intelligence, pages 147-149, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<pages>115--123</pages>
<contexts>
<context position="16261" citStr="Cohen, 1995" startWordPosition="2607" endWordPosition="2608">m of the sizes of its clauses. The metric M(H) used as the search heuristic is defined as: accuracy(H) + C M (H) = (4) log2 size(H) where C is a constant used to control the relative weight of accuracy vs. complexity. We assume that the most general hypothesis is as good as the most specific hypothesis; thus, C is determined to be: EbSt — EtSb C = (5) Sb—St where Et, Eb are the accuracy estimates of the most general and most specific hypotheses respectively, and St, Sb are their sizes. 3.3 Noise Handling A clause needs no further refinement when it meets the following criterion (as in RIPPER (Cohen, 1995)): p — n &gt; (6) p n where p is the number of positive examples covered by the clause, n is the number of negative examples covered and —1 &lt; &lt; 1 is a parameter. The value of 0 is decreased whenever the sum of the metric over the hypotheses in the queue does not improve although some of them still have unfinished or failed clauses. 4 Statistical Semantic Parsing 4.1 The Parsing Model A parser is a relation Parser C Sentences x Queries where Sentences and Queries are the sets of natural language sentences and database queries respectively. Given a sentence 1 E Sentences, the set Q(1) = {q E Querie</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>W. W. Cohen. 1995. Fast effective rule induction. In Proceedings of the Twelfth International Conference on Machine Learning, pages 115-123.</rawString>
</citation>
<citation valid="true">
<date>2000</date>
<booktitle>Papers from the AAAI Workshop on Learning Statistical Models from Relational Data,</booktitle>
<editor>L. Getoor and D. Jensen, editors.</editor>
<publisher>AAAI Press.</publisher>
<location>Austin, TX.</location>
<marker>2000</marker>
<rawString>L. Getoor and D. Jensen, editors. 2000. Papers from the AAAI Workshop on Learning Statistical Models from Relational Data, Austin, TX. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G G Hendrix</author>
<author>E Sacerdoti</author>
<author>D Sagalowicz</author>
<author>J Slocum</author>
</authors>
<title>Developing a natural language interface to complex data.</title>
<date>1978</date>
<journal>ACM Transactions on Database Systems,</journal>
<pages>3--2</pages>
<contexts>
<context position="1625" citStr="Hendrix et al., 1978" startWordPosition="243" endWordPosition="246">efer to the process of mapping a natural language sentence to a structured meaning representation. One interesting application of semantic parsing is building natural language interfaces for online databases. The need for such applications is growing since when information is delivered through the Internet, most users do not know the underlying database access language. An example of such an interface that we have developed is shown in Figure 1. Traditional (rationalist) approaches to constructing database interfaces require an expert to hand-craft an appropriate semantic parser (Woods, 1970; Hendrix et al., 1978). However, such hand-crafted parsers are time consuming to develop and suffer from problems with robustness and incompleteness even for domain specific applications. Nevertheless, very little research in empirical NLP has explored the task of automatically acquiring such interfaces from annotated training examples. The only exceptions of which we are aware are a statistical approach to mapping airline-information queries into SQL presented in (Miller et al., 1996), a probabilistic decision-tree method for the same task described in (Kuhn and De Mori, 1995), and an approach using relational lea</context>
</contexts>
<marker>Hendrix, Sacerdoti, Sagalowicz, Slocum, 1978</marker>
<rawString>G. G. Hendrix, E. Sacerdoti, D. Sagalowicz, and J. Slocum. 1978. Developing a natural language interface to complex data. ACM Transactions on Database Systems, 3(2):105-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R De Mori</author>
</authors>
<title>The application of semantic classification trees to natural language understanding.</title>
<date>1995</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>17--5</pages>
<marker>Kuhn, De Mori, 1995</marker>
<rawString>R. Kuhn and R. De Mori. 1995. The application of semantic classification trees to natural language understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(5):449-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Lavrac</author>
<author>S Dzeroski</author>
</authors>
<title>Inductive Logic Programming: Techniques and Applications. Ellis Horwood.</title>
<date>1994</date>
<contexts>
<context position="9660" citStr="Lavrac and Dzeroski, 1994" startWordPosition="1462" endWordPosition="1465">on by parsing each training example and recording the parse states encountered. Parse states to which an action should be applied (i.e. the action leads to building the correct semantic representation) are labeled positive examples for that action. Otherwise, a parse state is labeled a negative example for an action if it is a positive example for another action below the current one in the ordered list of actions. Control conditions which decide the correct action for a given parse state are learned for each action from these positive and negative examples. The initial CHILL system used ILP (Lavrac and Dzeroski, 1994) to learn Prolog control rules and employed deterministic parsing, using the learned rules to decide the appropriate parse action for each state. The current approach learns a model for estimating the probability that each action should be applied to a given state, and employs statistical parsing (Manning and Schiitze, 1999) to try to find the overall most probable parse, using beam search to control the complexity. The advantage of ILP is that it can perform induction over the logical description of the complete parse state without the need to pre-engineer a fixed set of features (which vary </context>
<context position="11131" citStr="Lavrac and Dzeroski, 1994" startWordPosition="1706" endWordPosition="1709">e that using such a probabilistic relational model (Getoor and Jensen, 2000) combines the advantages of frameworks based on first-order logic and those based on standard statistical techniques. 3 The TABULATE ILP Method This section discusses the ILP method used to build a committee of logical control hypotheses for each action. 3.1 The Basic TABULATE Algorithm Most ILP methods use a set-covering method to learn one clause (rule) at a time and construct clauses using either a strictly top-down (general to specific) or bottom-up (specific to general) search through the space of possible rules (Lavrac and Dzeroski, 1994). TABULATE,1 on the other hand, employs both bottom-up and top-down methods to construct potential clauses and searches through the hypothesis space of complete logic programs (i.e. sets of clauses called theories). It uses beam search to find a set of alternative hypotheses guided by a theory evaluation metric discussed below. The search starts with 1TABULATE stands for Top-down And Bottom-Up cLAuse construction with Theory Evaluation. Parse Stack: Icapital(C,_):0, answer(C,_):[the,is,what]] Input Buffer: [capital,of,texas,?] 135 Procedure Tabulate Input: the target concept to learn the 9 exa</context>
</contexts>
<marker>Lavrac, Dzeroski, 1994</marker>
<rawString>N. Lavrac and S. Dzeroski. 1994. Inductive Logic Programming: Techniques and Applications. Ellis Horwood.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schiitze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9986" citStr="Manning and Schiitze, 1999" startWordPosition="1515" endWordPosition="1518">s a positive example for another action below the current one in the ordered list of actions. Control conditions which decide the correct action for a given parse state are learned for each action from these positive and negative examples. The initial CHILL system used ILP (Lavrac and Dzeroski, 1994) to learn Prolog control rules and employed deterministic parsing, using the learned rules to decide the appropriate parse action for each state. The current approach learns a model for estimating the probability that each action should be applied to a given state, and employs statistical parsing (Manning and Schiitze, 1999) to try to find the overall most probable parse, using beam search to control the complexity. The advantage of ILP is that it can perform induction over the logical description of the complete parse state without the need to pre-engineer a fixed set of features (which vary greatly from one domain to another) that are relevant to making decisions. We maintain this advantage by using ILP to learn a committee of hypotheses, and basing probability estimates on a weighted vote of them (Ali and Pazzani, 1996). We believe that using such a probabilistic relational model (Getoor and Jensen, 2000) comb</context>
</contexts>
<marker>Manning, Schiitze, 1999</marker>
<rawString>C. D. Manning and H. Schiitze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>David Stallard</author>
<author>Robert Bobrow</author>
<author>Richard Schwartz</author>
</authors>
<title>A fully statistical approach to natural language interfaces.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>55--61</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="2093" citStr="Miller et al., 1996" startWordPosition="314" endWordPosition="317">alist) approaches to constructing database interfaces require an expert to hand-craft an appropriate semantic parser (Woods, 1970; Hendrix et al., 1978). However, such hand-crafted parsers are time consuming to develop and suffer from problems with robustness and incompleteness even for domain specific applications. Nevertheless, very little research in empirical NLP has explored the task of automatically acquiring such interfaces from annotated training examples. The only exceptions of which we are aware are a statistical approach to mapping airline-information queries into SQL presented in (Miller et al., 1996), a probabilistic decision-tree method for the same task described in (Kuhn and De Mori, 1995), and an approach using relational learning (a.k.a. inductive logic programming, ILP) to learn a logic-based semantic parser described in (Zelle and Mooney, 1996). The existing empirical systems for this task employ either a purely logical or purely statistical approach. The former uses a deterministic parser, which can suffer from some of the same robustness problems as rationalist methods. The latter constructs a probabilistic grammar, which requires supplying a sytactic parse tree as well as a sema</context>
</contexts>
<marker>Miller, Stallard, Bobrow, Schwartz, 1996</marker>
<rawString>Scott Miller, David Stallard, Robert Bobrow, and Richard Schwartz. 1996. A fully statistical approach to natural language interfaces. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 55-61, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muggleton</author>
<author>W Buntine</author>
</authors>
<title>Machine invention of first-order predicates by inverting resolution.</title>
<date>1988</date>
<booktitle>In Proceedings of the Fifth International Conference on Machine Learning,</booktitle>
<pages>339--352</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="15289" citStr="Muggleton and Buntine, 1988" startWordPosition="2425" endWordPosition="2428"> found by the algorithm is returned. 3.2 Compression and Accuracy The goal of the search is to find accurate and yet simple hypotheses. We measure accuracy using the m-estimate (Cestnik, 1990), a smoothed measure of accuracy on the training data which in the case of a two-class problem is defined as: s+m-p+ (1) accuracy(H) = n + m where s is the number of positive examples covered by the hypothesis H, n is the total number of examples covered, p+ is the prior probability of the class ED, and m is a smoothing parameter. We measure theory complexity using a metric similar to that introduced in (Muggleton and Buntine, 1988). The size of a Clause having a Head and a Body is defined as follows (ts= &amp;quot;term size&amp;quot; and ar=&amp;quot;arity&amp;quot;): size(Clause) =l+ts(Head)+ts(Body) (2) 136 T is a variable ts(T) / 2 T is a constant 2 + E11`!1T) ts(argi(T)) otherwise. (3) The size of a clause is roughly the number of variables, constants, or predicate symbols it contains. The size of a theory is the sum of the sizes of its clauses. The metric M(H) used as the search heuristic is defined as: accuracy(H) + C M (H) = (4) log2 size(H) where C is a constant used to control the relative weight of accuracy vs. complexity. We assume that the mos</context>
</contexts>
<marker>Muggleton, Buntine, 1988</marker>
<rawString>S. Muggleton and W. Buntine. 1988. Machine invention of first-order predicates by inverting resolution. In Proceedings of the Fifth International Conference on Machine Learning, pages 339-352, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Learning logical definitions from relations.</title>
<date>1990</date>
<booktitle>Machine Learning,</booktitle>
<pages>5--3</pages>
<contexts>
<context position="12976" citStr="Quinlan, 1990" startWordPosition="2031" endWordPosition="2033">e criteria; otherwise, Gil End If CQ := CQ U CQi End For Q := the B best nodes from Q U CQ ranked by metric M Until termination-criteria-satisfied Return Q End Procedure Figure 2: The TABULATE algorithm the most specific hypothesis (the set of positive examples each represented as a separate clause). Each iteration of the loop attempts to refine each of the hypotheses in the current search queue. There are two cases in each iteration: 1) an existing clause in a theory is refined or 2) a new clause is begun. Clauses are learned using both top-down specialization using a method similar to FOIL (Quinlan, 1990) and bottom-up generalization using Least General Generalizations (LGG&apos;s). Advantages of combining both ILP approaches were explored in Cm:um (Zelle and Mooney, 1994), an ILP method which motivated the design of TABULATE. An outline of the TABULATE algorithm is given in Figure 2. A noise-handling criterion is used to decide when an individual clause in a hypothesis is sufficiently accurate to be permanently retained. There are three possible outcomes in a refinement: 1) the current clause satisfies the noise-handling criterion and is simply returned (nextj is set to empty), 2) the current clau</context>
</contexts>
<marker>Quinlan, 1990</marker>
<rawString>J. R. Quinlan 1990. Learning logical definitions from relations. Machine Learning, 5(3):239--266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Thompson</author>
<author>R J Mooney</author>
</authors>
<title>Automatic construction of semantic lexicons for learning natural language interfaces.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence,</booktitle>
<pages>487--493</pages>
<location>Orlando, FL,</location>
<contexts>
<context position="7291" citStr="Thompson and Mooney, 1999" startWordPosition="1100" endWordPosition="1103">dicate onto the stack based on a word appearing in the input and information about its possible meanings in the lexicon. COREF_VARS binds two arguments of two different predicates on the stack. DROP_CONJ (or LIFT_CONJ) takes a predicate on the stack and puts it into one of the arguments of a meta-predicate on the stack. SHIFT simply pushes a word from the input buffer onto the stack. The parsing actions are tried in exactly this order. The parser also requires a lexicon to map phrases in the input into specific predicates, this lexicon can also be learned automatically from the training data (Thompson and Mooney, 1999). Let&apos;s go through a simple trace of parsing the request &amp;quot;What is the capital of Texas?&amp;quot; A lexicon that maps &apos;capital&apos; to &apos;capital(_,_)&apos; and &apos;Texas&apos; to &apos;const(_,stateid(texas))&apos; suffices 134 here. Interrogatives like &amp;quot;what&amp;quot; may be mapped to predicates in the lexicon if necessary. The parser begins with an initial stack and a buffer holding the input sentence. Each predicate on the parse stack has an attached buffer to hold the context in which it was introduced; words from the input sentence are shifted onto this buffer during parsing. The initial parse state is shown below: Parse Stack: [answ</context>
</contexts>
<marker>Thompson, Mooney, 1999</marker>
<rawString>C. A. Thompson and R. J. Mooney. 1999. Automatic construction of semantic lexicons for learning natural language interfaces. In Proceedings of the Sixteenth National Conference on Artificial Intelligence, pages 487-493, Orlando, FL, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Transition network grammars for natural language analysis.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<pages>13--591</pages>
<contexts>
<context position="1602" citStr="Woods, 1970" startWordPosition="241" endWordPosition="242"> parsing to refer to the process of mapping a natural language sentence to a structured meaning representation. One interesting application of semantic parsing is building natural language interfaces for online databases. The need for such applications is growing since when information is delivered through the Internet, most users do not know the underlying database access language. An example of such an interface that we have developed is shown in Figure 1. Traditional (rationalist) approaches to constructing database interfaces require an expert to hand-craft an appropriate semantic parser (Woods, 1970; Hendrix et al., 1978). However, such hand-crafted parsers are time consuming to develop and suffer from problems with robustness and incompleteness even for domain specific applications. Nevertheless, very little research in empirical NLP has explored the task of automatically acquiring such interfaces from annotated training examples. The only exceptions of which we are aware are a statistical approach to mapping airline-information queries into SQL presented in (Miller et al., 1996), a probabilistic decision-tree method for the same task described in (Kuhn and De Mori, 1995), and an approa</context>
</contexts>
<marker>Woods, 1970</marker>
<rawString>W. A. Woods. 1970. Transition network grammars for natural language analysis. Communications of the Association for Computing Machinery, 13:591-606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Combining top-down and bottom-up methods in inductive logic programming</title>
<date>1994</date>
<booktitle>In Proceedings of the Eleventh International Conference on Machine Learning,</booktitle>
<pages>343--351</pages>
<location>New Brunswick, NJ,</location>
<contexts>
<context position="13142" citStr="Zelle and Mooney, 1994" startWordPosition="2053" endWordPosition="2056">nd Procedure Figure 2: The TABULATE algorithm the most specific hypothesis (the set of positive examples each represented as a separate clause). Each iteration of the loop attempts to refine each of the hypotheses in the current search queue. There are two cases in each iteration: 1) an existing clause in a theory is refined or 2) a new clause is begun. Clauses are learned using both top-down specialization using a method similar to FOIL (Quinlan, 1990) and bottom-up generalization using Least General Generalizations (LGG&apos;s). Advantages of combining both ILP approaches were explored in Cm:um (Zelle and Mooney, 1994), an ILP method which motivated the design of TABULATE. An outline of the TABULATE algorithm is given in Figure 2. A noise-handling criterion is used to decide when an individual clause in a hypothesis is sufficiently accurate to be permanently retained. There are three possible outcomes in a refinement: 1) the current clause satisfies the noise-handling criterion and is simply returned (nextj is set to empty), 2) the current clause does not satisfy the noise-handling criteria and all possible refinements are returned (nextj is set to the refined clause), and 3) the current clause does not sat</context>
</contexts>
<marker>Zelle, Mooney, 1994</marker>
<rawString>J. M. Zelle and R. J. Mooney. 1994. Combining top-down and bottom-up methods in inductive logic programming In Proceedings of the Eleventh International Conference on Machine Learning, pages 343-351, New Brunswick, NJ, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1050--1055</pages>
<location>Portland, OR,</location>
<contexts>
<context position="2349" citStr="Mooney, 1996" startWordPosition="355" endWordPosition="356"> incompleteness even for domain specific applications. Nevertheless, very little research in empirical NLP has explored the task of automatically acquiring such interfaces from annotated training examples. The only exceptions of which we are aware are a statistical approach to mapping airline-information queries into SQL presented in (Miller et al., 1996), a probabilistic decision-tree method for the same task described in (Kuhn and De Mori, 1995), and an approach using relational learning (a.k.a. inductive logic programming, ILP) to learn a logic-based semantic parser described in (Zelle and Mooney, 1996). The existing empirical systems for this task employ either a purely logical or purely statistical approach. The former uses a deterministic parser, which can suffer from some of the same robustness problems as rationalist methods. The latter constructs a probabilistic grammar, which requires supplying a sytactic parse tree as well as a semantic representation for each training sentence, and requires hand-crafting a small set of contextual features on which to condition the parameters of the model. Combining relational and statistical approaches can overcome the need to supply parse-trees and</context>
<context position="24297" citStr="Mooney, 1996" startWordPosition="4162" endWordPosition="4163">t in . jobs. The database contains the following information: the job title, the company, the recruiter, the location, the salary, the languages and platforms used, and required or desired years of experience and degrees. 5.2 Experimental Design The geography corpus contains 560 questions. Approximately 100 of these were collected from a log of questions submitted to the web site and the rest were collected in studies involving students in undergraduate classes at our university. We also included results for the subset of 250 sentences originally used in the experiments reported in (Zelle and Mooney, 1996). The remaining questions were specifically collected to be more complex than the original 250, and generally require one or more meta-predicates. The restaurant corpus contains 250 questions automatically generated from a hand-built grammar constructed to reflect typical queries in this domain. The job corpus contains 400 questions automatically generated in a similar fashion. The beam width for TABULATE was set to five for all the domains. The deterministic parser used only the best hypothesis found. The experiments were conducted using 10-fold cross validation. For each domain, the average </context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>J. M. Zelle and It. J. Mooney. 1996. Learning to parse database queries using inductive logic programming In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1050-1055, Portland, OR, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>