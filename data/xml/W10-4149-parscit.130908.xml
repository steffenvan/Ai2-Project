<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.984764">
Treebank Conversion based Self-training Strategy for Parsing
</title>
<author confidence="0.997767">
Zhiguo Wang and Chengqing Zong
</author>
<affiliation confidence="0.9956355">
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
</affiliation>
<email confidence="0.992268">
{zgwang, cqzong}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.993782" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946333333333">
In this paper, we propose a novel self-
training strategy for parsing which is
based on Treebank conversion (SSPTC).
In SSPTC, we make full use of the
strong points of Treebank conversion
and self-training, and offset their
weaknesses with each other. To provide
good parse selection strategies which are
needed in self-training, we score the
automatically generated parse trees with
parse trees in source Treebank as a
reference. To maintain the constituency
between source Treebank and conversion
Treebank which is needed in Treebank
conversion, we get the conversion trees
with the help of self-training. In our
experiments, SSPTC strategy is utilized
to parse Tsinghua Chinese Treebank
with the help of Penn Chinese Treebank.
The results significantly outperform the
baseline parser.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945743243243">
Syntax parsing is one of the most fundamental
tasks in natural language processing (NLP) and
has attracted extensive attention during the past
few decades. In statistical area, according to the
type of data used in training stage, the parsing
approaches can be classified into three
categories: supervised, semi-supervised and
unsupervised. In supervised parsing approach, a
high-performance parser can be built when given
sufficient labeled data (Charniak, 2000; Collins,
2003; Henderson, 2004). The semi-supervised
approach utilizes some labeled data to annotate
unlabeled data, then uses the annotated data to
improve original model, e.g., self-training
(McClosky et al., 2006) and co-training (Hwa et
al., 2003). In unsupervised parsing, the labeled
data was not employed and all annotations and
grammars are discovered automatically from
unlabeled data.
State-of-the-art supervised parsers (Charniak,
2000; Collins, 2003; Henderson, 2004) require
large amounts of manually annotated training
data, such as the Penn Treebank (Marcus et al.,
1993), to achieve high performance. However, it
is quite costly and time-consuming to create
high quality labeled data. So it becomes a key
bottleneck for supervised approach to acquire
sufficient labeled training data. Self-training is
an effective strategy to overcome this shortage.
It tries to enlarge the training set with
automatically annotated unlabeled data and
trains a parser with the enlarged training set.
During the last few decades, many Treebanks
annotated with different grammar formalisms
are released (Zhou, 2004; Xue et al., 2005).
Although they are annotated with different
schemes, they have some linguistic consistency
in some extent. Intuitively, we can convert
Treebank annotated with one grammar
formalisms into another Treebank annotated
with grammar formalism that we are interested
in. For simplicity, we call the first source
Treebank, and the second target Treebank. And
we call this strategy as Treebank conversion.
Although both self-training and Treebank
conversion can overcome the limitation of
labeled data shortage for supervised parsing in
some extent, they all have drawbacks. For self-
training, the quality of automatically annotated
unlabeled data will affect the performance of
semi-supervised parsers highly. For example,
McClosky et al. (2006) shows that when the
parser-best list is used for self-training, the
parsing performance isn’t improved, but after
using reranker-best list, the retrained parser
achieves an absolute 1.1% improvement. For
Treebank conversion, different types among
Treebanks make the converting procedure very
complicated, and it is very hard to get a
conversion Treebank constituent with target
Treebank.
To overcome the limitations mentioned above,
we propose a Treebank conversion based self-
training strategy for parsing, which tries to
combine self-training and Treebank conversion
together.
Remainder of this paper is organized as
follows. In Section 2, we introduce some related
work. Section 3 describes details of our SSPTC
strategy. In Section 4, we propose a head finding
method for Task21 in CLP2010. The
experiments and analysis is given in Section 5.
The last section draws conclusions and describes
the future work.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.993863703125">
With the development of statistical parsing
approaches, large scale corpus has become an
indispensable resource. Because of the limited
amount of existing labeled training data and the
hardness of constructing corpus, many strategies
have been proposed and experimented to
overcome the contradiction.
Self-training is one of the most successful
strategies. McClosky et al. (2006) shows that
self-training effectively improves the accuracy
of English parsing. First, they trained a two-
stage reranking parser(Charniak and Johnson,
2005) using Penn Treebank (PTB)(Marcus et al.,
1993) and parsed 1,750k unlabeled sentences
from North American News Text corpus
(NANC). Then they combined the labeled
NANC sentences with PTB together as training
set and retrained the first stage of the parser. The
final result got a 1.1% improvement over the
previous best parser for section 23 of the Penn
Treebank. Huang and Harper (2009) combined
self-training into a PCFG-LA based parser both
for English and Chinese. Experimental result
showed that self-training contributed 0.83%
absolute improvement using only 210k
unlabeled sentences with a single generative
parser. For the Chinese parsing, self-training
contributed 1.03% absolute improvement.
Treebank Conversion is another potential
strategy to reuse existing source Treebanks for
the study of target grammar parsing. Wang et al.
(1994) proposed a Treebank conversion
algorithm for corpus sharing. They employed a
parser with target grammar formalism to get N-
best parse list for each sentence in source
Treebank, selected the best conversion tree from
the list using their algorithm, then inserted the
conversion trees into training set, and finally
retrained the parser with the enlarged training set.
Experimental result shows their algorithm is
effective. Collins et al. (1999) performed
statistical constituency parsing of Czech on a
Treebank that was converted from the Prague
Dependency Treebank under the guidance of
conversion rules and heuristic rules, and the final
performance was also improved. Xia and Palmer
(2001) proposed three methods to convert
dependency trees into phrase structure trees with
some hand-written heuristic rules. For
acquisition of better conversion rules, Xia et al.
(2008) proposed a method to automatically
extract conversion rules from a target Treebank.
Niu et al. (2009) tried to exploit heterogeneous
Treebanks for parsing. They proposed a
grammar formalism conversion algorithm to
convert dependency formalism Treebank into
phrase structure formalism, and did phrase
structure parsing with the conversion trees. Their
experiments are done in Chinese parsing, and the
final performance is improved indeed.
In summary, from the existing work we are
confident that the strategies of self-training and
Treebank conversion are effective to improve
the performance of parser.
</bodyText>
<sectionHeader confidence="0.996228" genericHeader="method">
3 Our Strategy
</sectionHeader>
<subsectionHeader confidence="0.99997">
3.1 Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.999923434782609">
Although self-training and Treebank Conversion
are effective for training set enlarging, they all
have drawbacks. Self-training needs some parse
selection strategies to select higher quality
parsers. Treebank Conversion needs us to
maintain the consistency between conversed
Treebank and target Treebank. On the other
hand, self-training strategy provides us a good
idea to get annotated trees consistent with target
grammar formalism, and the parse trees in
source side provide a reference for higher
quality parsers selecting. So we can combine
self-training and Treebank Conversion together,
use self-training strategy to get converted
candidates for sentences in source Treebank, and
select higher quality parses according to trees in
source Treebank. We call this strategy Treebank
Conversion based Self-training, and show more
details in Algorithm 1.
In Algorithm 1, target Treebank Tt and source
Treebank Ts are input first (line 1). Then Tt is
split into two parts: training set Ttrain and
development set Tdev (line 3). And we train an
</bodyText>
<construct confidence="0.336855">
Algorithm 1
</construct>
<listItem confidence="0.9901749375">
1: Input: Tt and Ts
2: D initialize
3: {Ttrain,Tdev} m Split(Tt )
4: Parser0 m Train(Ttrain , Tdev)
5: D Iter iterations
6: for i m 1... Iter do
7: i
T s o t m I
8: for k m 1... N do
9: ParseListk m Nbest(Parseri_1, sk )
10: pk arg max p ParseListk Score(ps,k , pj )
i
11: Ts ot m p k
ˆ
12: Parseri m Train(Ttrain,Td v,Ti t)
13: return ParserIter
</listItem>
<bodyText confidence="0.984529315789474">
initial parser with Ttrain and Tdev in line 4. From
line 6 to line 12, we train parsers with SSPTC
strategy Iter times iteratively. Let i
T s ot be the
automatically converted Treebank from source
Treebank to target Treebank grammar formalism
during the i-th iteration. From line 8 to line 11,
we try to get a conversion tree with target
grammar for each of the N sentences in source
Treebank. We get N-best parse list ParseListk for
sentence sk with Parser_i1 (line 9), select the
parse pˆ k with the highest score from ParseListk
(line 10), and insert it into i
T s ot (line 11). This
procedure runs iteratively until all the trees in
source Treebank have been converted, finally,
we train a new parser Parser i with Ttrain , Tdev and
T s o t (line 12).
i
</bodyText>
<subsectionHeader confidence="0.999511">
3.2 Parse selection
</subsectionHeader>
<bodyText confidence="0.93500024">
In line 10 of Algorithm 1, we select the highest
quality parse pˆ k from ParseListk according to
function Score(ps, psot) , where ps denotes a tree
in source Treebank and psot denotes a
conversion tree with target Treebank grammar
formalism for ps . Score(ps, psot) compares
psot with ps and computes a score for psot
taken ps as a reference. According to the idea
proposed in Wang et al. (1994), we use the
number of aligned constituents in the source and
target trees to construct Score(ps ,psot) . We
propose two types of Score(ps, psot) as follows.
(1) Unlabeled aligned constituents F1 score
(UAF)
First, we define a constituent as tag[i,j], which
represents a non-terminal node labeled with tag
and spanning words from positions i to j of the
input sentence. A non-terminal node in psot
aligns with a non-terminal node in ps when they
span the same words. If two nodes are aligned,
we call them an aligned constituent and denote
the aligned relationship as tags [i,j]  tagt [i, j] .
For example in Figure 1, there are three aligned
constituents between the source Treebank tree
and the conversion tree, and we can denote them
</bodyText>
<equation confidence="0.7947276">
as [0, 7]
IP  dj [0, 7] , NR [0, 2]  sp [0, 2] and
s t s t
NR [2,6]  np [2, 6] , respectively.
s t
</equation>
<bodyText confidence="0.9858102">
When given ps and psot , we can easily
collect all the aligned constituents. So we define
Unlabeled aligned constituents Precision (UAP)
and Unlabeled aligned constituents Recall (UAR)
as follows.
</bodyText>
<figure confidence="0.756683869565217">
(tags[i,j]  tagt[i,j])
UAP
(tagt[i, j])
(tags[i,j]  tagt[i,j])
UAR
(tags [i, j])
¦
Count
ij,
¦
Count
i j
,
¦
Count
i j
,
¦
Count
i j
,
NR [0,2] NR [2,6] VP [6,7]
NR NR NN CC NN NN VV
</figure>
<bodyText confidence="0.9991476">
Given the source Treebank and N-best
conversion trees, first we align all the
constituents, then collect all the aligned tags and
compute the converting probability as the
following equation.
</bodyText>
<figure confidence="0.997335058823529">
IP [0,7]
VP [0,6]
Ϟ⍋ ⌺ϰ ᓔথ Ϣ ⊩ࠊ ᓎ䆒 ৠℹ
(a) parse tree in source Treebank
p(tags o tag)
Count
( tags  tag)
(2)
Count
( )
tags
dj [0,7]
vp [2,7]
np [2,6]
sp [0,2] np [4,6]
nS nS vN cC n vN v
(b) conversion tree with target Treebank grammar
</figure>
<figureCaption confidence="0.8936445">
Figure 1: source tree and its conversion
tree with target grammar formalism
Then Unlabeled aligned constituents F1 score
(UAF) is defined as:
</figureCaption>
<table confidence="0.986865166666667">
Score ( , ) 2u UAP UAR
p p u
s s t
o
UAP UAR
�
</table>
<equation confidence="0.9429046">
2 u ¦Count(tags[i, j]  tagt [i, j])
i j
,
(Count(tags[i, j]) + Count(tagt [i, j]))
(1)
</equation>
<listItem confidence="0.8456865">
(2) Labeled aligned constituents F1 score
(LAF)
</listItem>
<bodyText confidence="0.991905833333333">
In the last subsection, we define Score(ps, psot)
according to UAF. In fact, the tags of
constituents bring us much information to score
conversion trees. So we
define Score(ps,psot) with Labeled aligned
constituents F1 score (LAF) in this subsection.
Because the annotation schemes are different,
constituent tags in source Treebank may be
much more different from target Treebank. The
number of such tags may be drastically different
and the mapping may not be one-to-one. To
eliminate the contradiction, we assume that each
tag in source Treebank can be converted into
every tag in target Treebank with various
probabilities. So there is a converting matrix
representing the converting probabilities, and we
can calculate the converting matrix through
source Treebank and N-best conversion trees.
</bodyText>
<equation confidence="0.992285">
¦ ( ( [ , ])
Count tag i j Count tag i j
� ( [ , ]))
s t
i j
,
(3)
</equation>
<bodyText confidence="0.999784">
In equation (3), J is a tunable variable, which
is used to weight the converting probability.
Especially, LAF will be transferred into UAF
when J =0.
</bodyText>
<subsectionHeader confidence="0.998501">
3.3 Corpus weighting technique
</subsectionHeader>
<bodyText confidence="0.990191961538461">
In line 12 of Algorithm 1, we train a new parser
with target Treebank and conversion trees.
However, the errors in automatically conversion
trees are unavoidable and they would limit the
accuracy of the self-trained model. So we have
to take some measures to weight the gold target
Treebank and the automatically conversion trees.
McClosky et al. (2006) and Niu et al. (2009)
take the strategy that duplicates the gold
Treebank data many times. However, this
strategy isn’t suitable for PCFG-LA
parser 1(Matsuzaki et al., 2005; Petrov et al.,
2006), because PCFG-LA employs an EM
algorithm in training stage, so duplicating gold
Treebank would increase the training time
tremendously. Instead, according to Huang and
Harper (2009), we weight the posterior
probabilities computed for the gold and
automatically converted trees to balance their
importance.
Let count(A o E |t) be the count of rule
A oE in a parse tree t . Tt and Tso t are the sets
of target Treebank and automatically converted
trees from source Treebank respectively. The
posterior probability of rule A o E (with
weighting parameterD ) can be expressed as:
</bodyText>
<footnote confidence="0.985414">
1 We will use BerkeleyParser as our baseline parser,
which is a PCFG-LA based parser.
</footnote>
<figure confidence="0.620973692307692">
Ϟ⍋ ⌺ϰ ᓔথ Ϣ ⊩ࠊ ᓎ䆒 ৠℹ
¦
i j
,
Finally, we modify UAF computed by
equation (1) into LAF as below.
Score( ps, psot)
2 (1
u ¦ + u
J p (tags o tagt)) u Count(tags [i, j ]  tagt [i, j ])
i j
,
Feature templates
</figure>
<bodyText confidence="0.816575428571429">
The label of the current constituent;
The label of the left most child, the middle child and the right most child;
The head word of the left most child, the middle child and the right most child;
The POS tag of the head word of the left most child, the middle child and the right most child;
Bigram of label, head word and POS tag of head word of the children: L/M, M/R;
Trigram of label, head word and POS tag of head word of the children: L/M/R;
The number of children;
</bodyText>
<tableCaption confidence="0.983844">
Table 1: Feature Templates for Head Finding
</tableCaption>
<equation confidence="0.991654909090909">
p (A o E)
( A o E  |t) + D ¦ Count(A oE  |t)
t T
 t T

t s o t
¦ ¦
( Count(A o E  |t) + D ¦ Count(A oE  |t))
t T
 tTot
(4)
</equation>
<sectionHeader confidence="0.986359" genericHeader="method">
4 Head Finding
</sectionHeader>
<bodyText confidence="0.995960393939394">
In Task21 of CLP2010, we are required to find
heads for each constituent. Our method is to
make head finding as a post procedure after
parsing.
We treat head finding problem as a
classification problem, which is to classify each
context-free production into categories labelled
with their heads. For example, there are three
types of heads: -0, -0-2 and -2 for
vp o vp wP vp , so we try to classify this
production into categories labelled with -0, -0-2
and -2. First, we scan the train set and collect all
the heads for each context-free production. Then
we train a Maxent classifier to classify each
context-free production into categories. We take
the same feature templates for the classification
as Chen et al. (2009) did, which is described in
Table 1.
The head finding procedure proceeds in a
bottom-up fashion, so that we can make use of
heads of productions in lower layers as features
for classification of the higher layers.
To evaluate the accuracy of our head finding
method, we randomly select a development set,
remove all head information and use our Maxent
classifier to retrieve the heads. Experimental
results show the accuracy has reached 98.28%.
However, the final performance would drop
much when the parse trees are generated
automatically. Because the automatically
generated parse trees would bring many errors,
and the post procedure of head finding can’t
correct the errors.
</bodyText>
<sectionHeader confidence="0.983349" genericHeader="method">
5 Experiments and Analysis
</sectionHeader>
<subsectionHeader confidence="0.991496">
5.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999974">
In order to evaluate the effectiveness of our
approach, we do experiments for Chinese
parsing using Tsinghua Chinese Treebank
(TCTB) on target side and Penn Chinese
Treebank (PCTB) on source side. We divide the
training portion of the Tsinghua Chinese
Treebank provided by CLP2010 into three parts
as follows: 500 trees are randomly extracted as
development set, another 500 as validating set
and the rest trees are taken as training set. For
trees in PCTB, all the empty-node and function
tag information are removed. All the ParseVal
measures reported in this paper are evaluated by
the EVALB tool2.
</bodyText>
<subsectionHeader confidence="0.985446">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999984363636364">
In order to get a good final accuracy, we choose
BerkeleyParser 3 , which is a state-of-the-art
unlexicalized parser, and train a model with the
training set as our baseline. The F1 score of
validating set parsed by baseline parser is
85.72%. In the following of this subsection, we
try to combine our strategies into the baseline
parser and evaluate the effectiveness. Because
mult-time iterations can’t improve parsing
performance tremendously but cost much time
during our experiments, we take Iter=1 here.
</bodyText>
<listItem confidence="0.530961">
(1) Corpus weighting experiment
</listItem>
<bodyText confidence="0.969036428571429">
To evaluate the corpus weighting strategy, we
take sentences (ignore the tree structure) in
PCTB as unlabeled data, and train a parser with
self-training strategy. F1 scores of validating set
varying with D in equation (4) are shown in
Figure 2. From Figure 2, we find that the F1
score varies with D , and reaches 86.46%
</bodyText>
<footnote confidence="0.9873405">
2 http://nlp.cs.nyu.edu/evalb/
3 http://code.google.com/p/berkeleyparser/
</footnote>
<figure confidence="0.657747">
¦
Count
E
</figure>
<bodyText confidence="0.9920345">
when a =1. The 0.74 absolute improvement
comparing with the baseline certifies the
effectiveness of our corpus weighting strategy.
more improvement. So we can conclude that the
parse selection strategy with LAF is much more
effective.
</bodyText>
<figureCaption confidence="0.991029">
Figure 2: F1 score of self-training
</figureCaption>
<bodyText confidence="0.951901416666667">
(2) Parse selection experiments
In this subsection we evaluate our parse
selection strategies with the help of PCTB.
According to Algorithm 1, we train an initial
parser with training set and development set.
Then we generate 50-best parses list with the
initial parser for each sentence in PCTB, and
select a higher-score parse for each sentence
through our parse selection strategies to build a
conversion Treebank. Finally, we retrain a parser
with training set and the conversion Treebank
with the help of corpus weighting strategy.
</bodyText>
<figureCaption confidence="0.992217">
Figure 3: F1 score of UAF strategy
</figureCaption>
<bodyText confidence="0.998645266666667">
Figure 3 shows F1 scores of validating set
using UAF to select higher quality parses.
When a =0.3, F1 score reaches 86.92%. The
improvement over baseline is 1.2 percentage
points. Comparing with the highest F1 score of
self-training, we got 0.46 more improvement. So
our parse selection strategy with UAF is
effective.
Because the highest F1 score is at the point
a =0.3 in Figure 3, we choose a =0.3 to
evaluating LAF strategy. Figure 4 shows F1
scores on validating set using LAF. The highest
F1 score is 87.44% at the pointY =6, and it gets
1.72 percentage points improvement over
baseline. Comparing with UAF, LAF gets 0.52
</bodyText>
<figureCaption confidence="0.987189">
Figure 4: F1 score of LAF strategy
</figureCaption>
<subsectionHeader confidence="0.728507">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.9995826">
Table 2 reports the highest performances of
various strategies. From the table we can easily
find that all strategies outperform the baseline
parser. Corpus weighting experiment tells us that
balancing the importance of gold target
Treebank and conversion trees is helpful for the
final performance. Using UAF to select
conversion trees can get more improvement than
self-training which just selects the best-first trees.
This fact proves that our SSPTC strategy is
reasonable and effective. Making use of LAF,
we get more improvement than UAF. It tells us
that exploiting source Treebank deeply can bring
us more useful knowledge which is helpful to
develop high-performance parser.
</bodyText>
<table confidence="0.9984118">
Strategy F1 score
Baseline 85.72%
Corpus weighting 86.46%
UAF 86.92%
LAF 87.44%
</table>
<tableCaption confidence="0.995424">
Table 2: F1 scores of various strategies
</tableCaption>
<sectionHeader confidence="0.999527" genericHeader="method">
6 Experiments for Task 2 of CLP2010
</sectionHeader>
<bodyText confidence="0.9999052">
Task 2 of CLP2010 includes two sub-tasks: sub-
sentence parsing and complete sentence parsing.
For each sub-task, there are two tracks: closed
track and open track. To accomplish tasks in
closed track, we make use of our baseline parser
shown in section 5 and train it with different
parameters and data set. For open track, we
make use of our SSPTC strategy and train it with
different parameters and data set. We tuned the
parameters on the development set and selected
</bodyText>
<figure confidence="0.944247416666666">
Sub-task Track ID Parser Parameters Train data
Sub-task 1 Closed a Berkeley -- TS
b Berkeley -- TS &amp;&amp; VS
Open a SSPTC 5 &amp;&amp; PTCB
a 0.3 y ==TS
b SSPTC a 0.3==y 5 TS &amp;&amp; VS &amp;&amp; PTCB
Sub-task 2 Closed a Berkeley -- TS
b Berkeley -- TS &amp;&amp; VS
Open a SSPTC a = 0.3 y=6 TS &amp;&amp; PTCB
b SSPTC a 0.3 ==y 5 TS &amp;&amp; VS &amp;&amp; PTCB
c SSPTC a = 0.3 y=5 TS &amp;&amp; PTCB
d SSPTC a = 0.3 y=3 TS &amp;&amp; PTCB
</figure>
<tableCaption confidence="0.811428">
Table 3: The configurations of our systems. The abbreviations in the last column mean
training set(TS) and validating set(VS) explaining in section 5.1.
</tableCaption>
<bodyText confidence="0.99964375">
some configurations which achieve higher
performance on the development set(more
details have been shown in section 5). The final
parameters and training data of our systems are
shown in Table 34. We also make use of the
approach explained in section 4 for the head
finding procedure.
The parsing results of our systems on the test
set can be found on the official ranking report.
Our systems training with SSPTC strategy bring
us an amazing performance which outperforms
other systems in both the two sub-tasks.
</bodyText>
<sectionHeader confidence="0.987887" genericHeader="conclusions">
7 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.9999435">
In this paper, we propose a novel self-training
strategy for parsing which is based on Treebank
conversion. Benefiting from SSPTC strategy, we
have gotten higher quality parse trees with the
help of source Treebank, and gotten conversion
Treebank with target Treebank grammar
formalism simply and consistently. The parsing
results on validating set show SSPTC is
effective. We apply SSPTC to the test set of
Task 2 in CLP2010, and get 1.275 percentage
points improvement over baseline parser using
the parameters tuned on validating set.
</bodyText>
<footnote confidence="0.988710666666667">
4 The parsing result for system b in open track of sub-
task1 has been submitted mistakenly, so the figures of
this system on the official ranking report have no
reference value.
5 The F1 score of baseline parser is 75.24%, and it
reaches 76.51% using TCBS strategy.
</footnote>
<bodyText confidence="0.999915555555556">
All the delightful results tell us that SSPTC is
a promoting strategy for parsing. However, there
is much knowledge in source Treebank remained
to further exploit, e.g. the POS tags in source
Treebank is a good resource to improve the POS
tagging accuracy of target Treebank. So, in the
next step we will exploit source Treebank deeply
and try to get more knowledge from it for
parsing.
</bodyText>
<sectionHeader confidence="0.979908" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999275545454546">
The research work has been partially funded by
the Natural Science Foundation of China under
Grant No. 6097 5053, 90820303 and 60736014,
the National Key Technology R&amp;D Program
under Grant No. 2006BAH03B02, the Hi-Tech
Research and Development Program (“863”
Program) of China under Grant No.
2006AA010108-4, and also supported by the
China-Singapore Institute of Digital Media
(CSIDM) project under grant No. CSIDM-
200804.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999924156862745">
Eugene Charniak, 2000. A maximum-entropy-
inspired parser. In NAACL-2000
Eugene Charniak and Mark Johnson, 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL-05.
Xiao Chen, Changning Huang, Mu Li and Chunyu
Kit, 2009. Better Parser Combination. In CIPS.
Michael Collins, 2003. Head-driven statistical models
for natural language parsing. Computational
Linguistics, 29 (4). pages 589-637.
M Collins, J Hajic, L Ramshaw and C Tillman, 1999.
A statistical parser for Czech. In ACL-99. J
Henderson, 2004. Discriminative training of a
neural network statistical parser.
Zhongqiang Huang and Mary Harper, 2009. Self-
Training PCFG grammars with latent annotations
across languages. ACL-09.
R Hwa, M Osborne, A Sarkar and M Steedman, 2003.
Corrected co-training for statistical parsers.
Citeseer.
MP Marcus, B Santorini and MA Marcinkiewicz,
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19
(2). pages 313-330.
Takuya Matsuzaki, Yusuke Miyao and Jun&apos;ichi Tsujii,
2005. Probabilistic CFG with latent annotations. In
ACL-05.
David McClosky, Eugene Charniak and Mark
Johnson, 2006. Effective self-training for parsing.
In ACL-06.
Zheng-Yu Niu, Haifeng Wang and Hua Wu, 2009.
Exploiting heterogeneous treebanks for parsing. In
ACL-09, pages 46-54.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein, 2006. Learning accurate, compact, and
interpretable tree annotation. In ACL-06.
Jong-Nae Wang, Jing-Shin Chang and Keh-Yih Su,
1994. An automatic treebank conversion algorithm
for corpus sharing. In ACL-94.
Fei Xia and Martha Palmer, 2001. Converting
dependency structures to phrase structures. In The
1st Human Language Technology Conference
(HLT-2001).
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha
Palmer and Dipti Misra Sharma, 2008. Towards a
multi-representational treebank. Proc. of the 7th
Int&apos;lWorkshop on Treebanks and Linguistic
Theories (TLT-7). pages 207-238.
Qiang Zhou, 2004. Annotation Scheme for Chinese
Treebank. Journal of Chinese Information
Processing, 18 (004).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.711879">
<title confidence="0.999903">Treebank Conversion based Self-training Strategy for Parsing</title>
<author confidence="0.964288">Wang</author>
<affiliation confidence="0.9681835">National Laboratory of Pattern Institute of Automation, Chinese Academy of</affiliation>
<email confidence="0.781104">zgwang@nlpr.ia.ac.cn</email>
<email confidence="0.781104">cqzong@nlpr.ia.ac.cn</email>
<abstract confidence="0.999217">In this paper, we propose a novel selftraining strategy for parsing which is based on Treebank conversion (SSPTC). In SSPTC, we make full use of the strong points of Treebank conversion and self-training, and offset their weaknesses with each other. To provide good parse selection strategies which are needed in self-training, we score the automatically generated parse trees with parse trees in source Treebank as a reference. To maintain the constituency between source Treebank and conversion Treebank which is needed in Treebank conversion, we get the conversion trees with the help of self-training. In our experiments, SSPTC strategy is utilized to parse Tsinghua Chinese Treebank with the help of Penn Chinese Treebank. The results significantly outperform the baseline parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In NAACL-2000</booktitle>
<contexts>
<context position="1491" citStr="Charniak, 2000" startWordPosition="214" endWordPosition="215">zed to parse Tsinghua Chinese Treebank with the help of Penn Chinese Treebank. The results significantly outperform the baseline parser. 1 Introduction Syntax parsing is one of the most fundamental tasks in natural language processing (NLP) and has attracted extensive attention during the past few decades. In statistical area, according to the type of data used in training stage, the parsing approaches can be classified into three categories: supervised, semi-supervised and unsupervised. In supervised parsing approach, a high-performance parser can be built when given sufficient labeled data (Charniak, 2000; Collins, 2003; Henderson, 2004). The semi-supervised approach utilizes some labeled data to annotate unlabeled data, then uses the annotated data to improve original model, e.g., self-training (McClosky et al., 2006) and co-training (Hwa et al., 2003). In unsupervised parsing, the labeled data was not employed and all annotations and grammars are discovered automatically from unlabeled data. State-of-the-art supervised parsers (Charniak, 2000; Collins, 2003; Henderson, 2004) require large amounts of manually annotated training data, such as the Penn Treebank (Marcus et al., 1993), to achieve</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak, 2000. A maximum-entropyinspired parser. In NAACL-2000</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL-05.</booktitle>
<contexts>
<context position="4823" citStr="Charniak and Johnson, 2005" startWordPosition="697" endWordPosition="700"> Section 5. The last section draws conclusions and describes the future work. 2 Related Work With the development of statistical parsing approaches, large scale corpus has become an indispensable resource. Because of the limited amount of existing labeled training data and the hardness of constructing corpus, many strategies have been proposed and experimented to overcome the contradiction. Self-training is one of the most successful strategies. McClosky et al. (2006) shows that self-training effectively improves the accuracy of English parsing. First, they trained a twostage reranking parser(Charniak and Johnson, 2005) using Penn Treebank (PTB)(Marcus et al., 1993) and parsed 1,750k unlabeled sentences from North American News Text corpus (NANC). Then they combined the labeled NANC sentences with PTB together as training set and retrained the first stage of the parser. The final result got a 1.1% improvement over the previous best parser for section 23 of the Penn Treebank. Huang and Harper (2009) combined self-training into a PCFG-LA based parser both for English and Chinese. Experimental result showed that self-training contributed 0.83% absolute improvement using only 210k unlabeled sentences with a sing</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson, 2005. Coarseto-fine n-best parsing and MaxEnt discriminative reranking. In ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Chen</author>
<author>Changning Huang</author>
<author>Mu Li</author>
<author>Chunyu Kit</author>
</authors>
<title>Better Parser Combination.</title>
<date>2009</date>
<booktitle>In CIPS.</booktitle>
<contexts>
<context position="15577" citStr="Chen et al. (2009)" startWordPosition="2575" endWordPosition="2578">ding as a post procedure after parsing. We treat head finding problem as a classification problem, which is to classify each context-free production into categories labelled with their heads. For example, there are three types of heads: -0, -0-2 and -2 for vp o vp wP vp , so we try to classify this production into categories labelled with -0, -0-2 and -2. First, we scan the train set and collect all the heads for each context-free production. Then we train a Maxent classifier to classify each context-free production into categories. We take the same feature templates for the classification as Chen et al. (2009) did, which is described in Table 1. The head finding procedure proceeds in a bottom-up fashion, so that we can make use of heads of productions in lower layers as features for classification of the higher layers. To evaluate the accuracy of our head finding method, we randomly select a development set, remove all head information and use our Maxent classifier to retrieve the heads. Experimental results show the accuracy has reached 98.28%. However, the final performance would drop much when the parse trees are generated automatically. Because the automatically generated parse trees would brin</context>
</contexts>
<marker>Chen, Huang, Li, Kit, 2009</marker>
<rawString>Xiao Chen, Changning Huang, Mu Li and Chunyu Kit, 2009. Better Parser Combination. In CIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<pages>589--637</pages>
<contexts>
<context position="1506" citStr="Collins, 2003" startWordPosition="216" endWordPosition="217">nghua Chinese Treebank with the help of Penn Chinese Treebank. The results significantly outperform the baseline parser. 1 Introduction Syntax parsing is one of the most fundamental tasks in natural language processing (NLP) and has attracted extensive attention during the past few decades. In statistical area, according to the type of data used in training stage, the parsing approaches can be classified into three categories: supervised, semi-supervised and unsupervised. In supervised parsing approach, a high-performance parser can be built when given sufficient labeled data (Charniak, 2000; Collins, 2003; Henderson, 2004). The semi-supervised approach utilizes some labeled data to annotate unlabeled data, then uses the annotated data to improve original model, e.g., self-training (McClosky et al., 2006) and co-training (Hwa et al., 2003). In unsupervised parsing, the labeled data was not employed and all annotations and grammars are discovered automatically from unlabeled data. State-of-the-art supervised parsers (Charniak, 2000; Collins, 2003; Henderson, 2004) require large amounts of manually annotated training data, such as the Penn Treebank (Marcus et al., 1993), to achieve high performan</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins, 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29 (4). pages 589-637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Hajic</author>
<author>L Ramshaw</author>
<author>C Tillman</author>
</authors>
<title>A statistical parser for Czech.</title>
<date>1999</date>
<booktitle>In ACL-99. J Henderson,</booktitle>
<contexts>
<context position="6112" citStr="Collins et al. (1999)" startWordPosition="890" endWordPosition="893">uted 1.03% absolute improvement. Treebank Conversion is another potential strategy to reuse existing source Treebanks for the study of target grammar parsing. Wang et al. (1994) proposed a Treebank conversion algorithm for corpus sharing. They employed a parser with target grammar formalism to get Nbest parse list for each sentence in source Treebank, selected the best conversion tree from the list using their algorithm, then inserted the conversion trees into training set, and finally retrained the parser with the enlarged training set. Experimental result shows their algorithm is effective. Collins et al. (1999) performed statistical constituency parsing of Czech on a Treebank that was converted from the Prague Dependency Treebank under the guidance of conversion rules and heuristic rules, and the final performance was also improved. Xia and Palmer (2001) proposed three methods to convert dependency trees into phrase structure trees with some hand-written heuristic rules. For acquisition of better conversion rules, Xia et al. (2008) proposed a method to automatically extract conversion rules from a target Treebank. Niu et al. (2009) tried to exploit heterogeneous Treebanks for parsing. They proposed </context>
</contexts>
<marker>Collins, Hajic, Ramshaw, Tillman, 1999</marker>
<rawString>M Collins, J Hajic, L Ramshaw and C Tillman, 1999. A statistical parser for Czech. In ACL-99. J Henderson, 2004. Discriminative training of a neural network statistical parser.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>SelfTraining PCFG grammars with latent annotations across languages.</title>
<date>2009</date>
<pages>09</pages>
<contexts>
<context position="5209" citStr="Huang and Harper (2009)" startWordPosition="760" endWordPosition="763">on. Self-training is one of the most successful strategies. McClosky et al. (2006) shows that self-training effectively improves the accuracy of English parsing. First, they trained a twostage reranking parser(Charniak and Johnson, 2005) using Penn Treebank (PTB)(Marcus et al., 1993) and parsed 1,750k unlabeled sentences from North American News Text corpus (NANC). Then they combined the labeled NANC sentences with PTB together as training set and retrained the first stage of the parser. The final result got a 1.1% improvement over the previous best parser for section 23 of the Penn Treebank. Huang and Harper (2009) combined self-training into a PCFG-LA based parser both for English and Chinese. Experimental result showed that self-training contributed 0.83% absolute improvement using only 210k unlabeled sentences with a single generative parser. For the Chinese parsing, self-training contributed 1.03% absolute improvement. Treebank Conversion is another potential strategy to reuse existing source Treebanks for the study of target grammar parsing. Wang et al. (1994) proposed a Treebank conversion algorithm for corpus sharing. They employed a parser with target grammar formalism to get Nbest parse list fo</context>
<context position="13515" citStr="Huang and Harper (2009)" startWordPosition="2170" endWordPosition="2173">rors in automatically conversion trees are unavoidable and they would limit the accuracy of the self-trained model. So we have to take some measures to weight the gold target Treebank and the automatically conversion trees. McClosky et al. (2006) and Niu et al. (2009) take the strategy that duplicates the gold Treebank data many times. However, this strategy isn’t suitable for PCFG-LA parser 1(Matsuzaki et al., 2005; Petrov et al., 2006), because PCFG-LA employs an EM algorithm in training stage, so duplicating gold Treebank would increase the training time tremendously. Instead, according to Huang and Harper (2009), we weight the posterior probabilities computed for the gold and automatically converted trees to balance their importance. Let count(A o E |t) be the count of rule A oE in a parse tree t . Tt and Tso t are the sets of target Treebank and automatically converted trees from source Treebank respectively. The posterior probability of rule A o E (with weighting parameterD ) can be expressed as: 1 We will use BerkeleyParser as our baseline parser, which is a PCFG-LA based parser. Ϟ⍋ ⌺ϰ ᓔথ Ϣ ⊩ ᓎ䆒 ৠℹ ¦ i j , Finally, we modify UAF computed by equation (1) into LAF as below. Score( ps, psot) 2 (1 u </context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper, 2009. SelfTraining PCFG grammars with latent annotations across languages. ACL-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
<author>M Osborne</author>
<author>A Sarkar</author>
<author>M Steedman</author>
</authors>
<title>Corrected co-training for statistical parsers.</title>
<date>2003</date>
<journal>Citeseer.</journal>
<contexts>
<context position="1744" citStr="Hwa et al., 2003" startWordPosition="248" endWordPosition="251">ttracted extensive attention during the past few decades. In statistical area, according to the type of data used in training stage, the parsing approaches can be classified into three categories: supervised, semi-supervised and unsupervised. In supervised parsing approach, a high-performance parser can be built when given sufficient labeled data (Charniak, 2000; Collins, 2003; Henderson, 2004). The semi-supervised approach utilizes some labeled data to annotate unlabeled data, then uses the annotated data to improve original model, e.g., self-training (McClosky et al., 2006) and co-training (Hwa et al., 2003). In unsupervised parsing, the labeled data was not employed and all annotations and grammars are discovered automatically from unlabeled data. State-of-the-art supervised parsers (Charniak, 2000; Collins, 2003; Henderson, 2004) require large amounts of manually annotated training data, such as the Penn Treebank (Marcus et al., 1993), to achieve high performance. However, it is quite costly and time-consuming to create high quality labeled data. So it becomes a key bottleneck for supervised approach to acquire sufficient labeled training data. Self-training is an effective strategy to overcome</context>
</contexts>
<marker>Hwa, Osborne, Sarkar, Steedman, 2003</marker>
<rawString>R Hwa, M Osborne, A Sarkar and M Steedman, 2003. Corrected co-training for statistical parsers. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MP Marcus</author>
<author>B Santorini</author>
<author>MA Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="2079" citStr="Marcus et al., 1993" startWordPosition="294" endWordPosition="297">ent labeled data (Charniak, 2000; Collins, 2003; Henderson, 2004). The semi-supervised approach utilizes some labeled data to annotate unlabeled data, then uses the annotated data to improve original model, e.g., self-training (McClosky et al., 2006) and co-training (Hwa et al., 2003). In unsupervised parsing, the labeled data was not employed and all annotations and grammars are discovered automatically from unlabeled data. State-of-the-art supervised parsers (Charniak, 2000; Collins, 2003; Henderson, 2004) require large amounts of manually annotated training data, such as the Penn Treebank (Marcus et al., 1993), to achieve high performance. However, it is quite costly and time-consuming to create high quality labeled data. So it becomes a key bottleneck for supervised approach to acquire sufficient labeled training data. Self-training is an effective strategy to overcome this shortage. It tries to enlarge the training set with automatically annotated unlabeled data and trains a parser with the enlarged training set. During the last few decades, many Treebanks annotated with different grammar formalisms are released (Zhou, 2004; Xue et al., 2005). Although they are annotated with different schemes, t</context>
<context position="4870" citStr="Marcus et al., 1993" startWordPosition="704" endWordPosition="707">cribes the future work. 2 Related Work With the development of statistical parsing approaches, large scale corpus has become an indispensable resource. Because of the limited amount of existing labeled training data and the hardness of constructing corpus, many strategies have been proposed and experimented to overcome the contradiction. Self-training is one of the most successful strategies. McClosky et al. (2006) shows that self-training effectively improves the accuracy of English parsing. First, they trained a twostage reranking parser(Charniak and Johnson, 2005) using Penn Treebank (PTB)(Marcus et al., 1993) and parsed 1,750k unlabeled sentences from North American News Text corpus (NANC). Then they combined the labeled NANC sentences with PTB together as training set and retrained the first stage of the parser. The final result got a 1.1% improvement over the previous best parser for section 23 of the Penn Treebank. Huang and Harper (2009) combined self-training into a PCFG-LA based parser both for English and Chinese. Experimental result showed that self-training contributed 0.83% absolute improvement using only 210k unlabeled sentences with a single generative parser. For the Chinese parsing, </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>MP Marcus, B Santorini and MA Marcinkiewicz, 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19 (2). pages 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
</authors>
<title>Yusuke Miyao and Jun&apos;ichi Tsujii,</title>
<date>2005</date>
<booktitle>In ACL-05.</booktitle>
<marker>Matsuzaki, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao and Jun&apos;ichi Tsujii, 2005. Probabilistic CFG with latent annotations. In ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In ACL-06.</booktitle>
<contexts>
<context position="1709" citStr="McClosky et al., 2006" startWordPosition="242" endWordPosition="245">ural language processing (NLP) and has attracted extensive attention during the past few decades. In statistical area, according to the type of data used in training stage, the parsing approaches can be classified into three categories: supervised, semi-supervised and unsupervised. In supervised parsing approach, a high-performance parser can be built when given sufficient labeled data (Charniak, 2000; Collins, 2003; Henderson, 2004). The semi-supervised approach utilizes some labeled data to annotate unlabeled data, then uses the annotated data to improve original model, e.g., self-training (McClosky et al., 2006) and co-training (Hwa et al., 2003). In unsupervised parsing, the labeled data was not employed and all annotations and grammars are discovered automatically from unlabeled data. State-of-the-art supervised parsers (Charniak, 2000; Collins, 2003; Henderson, 2004) require large amounts of manually annotated training data, such as the Penn Treebank (Marcus et al., 1993), to achieve high performance. However, it is quite costly and time-consuming to create high quality labeled data. So it becomes a key bottleneck for supervised approach to acquire sufficient labeled training data. Self-training i</context>
<context position="3361" citStr="McClosky et al. (2006)" startWordPosition="482" endWordPosition="485">vely, we can convert Treebank annotated with one grammar formalisms into another Treebank annotated with grammar formalism that we are interested in. For simplicity, we call the first source Treebank, and the second target Treebank. And we call this strategy as Treebank conversion. Although both self-training and Treebank conversion can overcome the limitation of labeled data shortage for supervised parsing in some extent, they all have drawbacks. For selftraining, the quality of automatically annotated unlabeled data will affect the performance of semi-supervised parsers highly. For example, McClosky et al. (2006) shows that when the parser-best list is used for self-training, the parsing performance isn’t improved, but after using reranker-best list, the retrained parser achieves an absolute 1.1% improvement. For Treebank conversion, different types among Treebanks make the converting procedure very complicated, and it is very hard to get a conversion Treebank constituent with target Treebank. To overcome the limitations mentioned above, we propose a Treebank conversion based selftraining strategy for parsing, which tries to combine self-training and Treebank conversion together. Remainder of this pap</context>
<context position="4668" citStr="McClosky et al. (2006)" startWordPosition="676" endWordPosition="679">escribes details of our SSPTC strategy. In Section 4, we propose a head finding method for Task21 in CLP2010. The experiments and analysis is given in Section 5. The last section draws conclusions and describes the future work. 2 Related Work With the development of statistical parsing approaches, large scale corpus has become an indispensable resource. Because of the limited amount of existing labeled training data and the hardness of constructing corpus, many strategies have been proposed and experimented to overcome the contradiction. Self-training is one of the most successful strategies. McClosky et al. (2006) shows that self-training effectively improves the accuracy of English parsing. First, they trained a twostage reranking parser(Charniak and Johnson, 2005) using Penn Treebank (PTB)(Marcus et al., 1993) and parsed 1,750k unlabeled sentences from North American News Text corpus (NANC). Then they combined the labeled NANC sentences with PTB together as training set and retrained the first stage of the parser. The final result got a 1.1% improvement over the previous best parser for section 23 of the Penn Treebank. Huang and Harper (2009) combined self-training into a PCFG-LA based parser both fo</context>
<context position="13138" citStr="McClosky et al. (2006)" startWordPosition="2112" endWordPosition="2115">st conversion trees. ¦ ( ( [ , ]) Count tag i j Count tag i j � ( [ , ])) s t i j , (3) In equation (3), J is a tunable variable, which is used to weight the converting probability. Especially, LAF will be transferred into UAF when J =0. 3.3 Corpus weighting technique In line 12 of Algorithm 1, we train a new parser with target Treebank and conversion trees. However, the errors in automatically conversion trees are unavoidable and they would limit the accuracy of the self-trained model. So we have to take some measures to weight the gold target Treebank and the automatically conversion trees. McClosky et al. (2006) and Niu et al. (2009) take the strategy that duplicates the gold Treebank data many times. However, this strategy isn’t suitable for PCFG-LA parser 1(Matsuzaki et al., 2005; Petrov et al., 2006), because PCFG-LA employs an EM algorithm in training stage, so duplicating gold Treebank would increase the training time tremendously. Instead, according to Huang and Harper (2009), we weight the posterior probabilities computed for the gold and automatically converted trees to balance their importance. Let count(A o E |t) be the count of rule A oE in a parse tree t . Tt and Tso t are the sets of tar</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak and Mark Johnson, 2006. Effective self-training for parsing. In ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
</authors>
<title>Exploiting heterogeneous treebanks for parsing.</title>
<date>2009</date>
<booktitle>In ACL-09,</booktitle>
<pages>46--54</pages>
<contexts>
<context position="6643" citStr="Niu et al. (2009)" startWordPosition="969" endWordPosition="972">ing set. Experimental result shows their algorithm is effective. Collins et al. (1999) performed statistical constituency parsing of Czech on a Treebank that was converted from the Prague Dependency Treebank under the guidance of conversion rules and heuristic rules, and the final performance was also improved. Xia and Palmer (2001) proposed three methods to convert dependency trees into phrase structure trees with some hand-written heuristic rules. For acquisition of better conversion rules, Xia et al. (2008) proposed a method to automatically extract conversion rules from a target Treebank. Niu et al. (2009) tried to exploit heterogeneous Treebanks for parsing. They proposed a grammar formalism conversion algorithm to convert dependency formalism Treebank into phrase structure formalism, and did phrase structure parsing with the conversion trees. Their experiments are done in Chinese parsing, and the final performance is improved indeed. In summary, from the existing work we are confident that the strategies of self-training and Treebank conversion are effective to improve the performance of parser. 3 Our Strategy 3.1 Parsing Algorithm Although self-training and Treebank Conversion are effective </context>
<context position="13160" citStr="Niu et al. (2009)" startWordPosition="2117" endWordPosition="2120">[ , ]) Count tag i j Count tag i j � ( [ , ])) s t i j , (3) In equation (3), J is a tunable variable, which is used to weight the converting probability. Especially, LAF will be transferred into UAF when J =0. 3.3 Corpus weighting technique In line 12 of Algorithm 1, we train a new parser with target Treebank and conversion trees. However, the errors in automatically conversion trees are unavoidable and they would limit the accuracy of the self-trained model. So we have to take some measures to weight the gold target Treebank and the automatically conversion trees. McClosky et al. (2006) and Niu et al. (2009) take the strategy that duplicates the gold Treebank data many times. However, this strategy isn’t suitable for PCFG-LA parser 1(Matsuzaki et al., 2005; Petrov et al., 2006), because PCFG-LA employs an EM algorithm in training stage, so duplicating gold Treebank would increase the training time tremendously. Instead, according to Huang and Harper (2009), we weight the posterior probabilities computed for the gold and automatically converted trees to balance their importance. Let count(A o E |t) be the count of rule A oE in a parse tree t . Tt and Tso t are the sets of target Treebank and autom</context>
</contexts>
<marker>Niu, Wang, Wu, 2009</marker>
<rawString>Zheng-Yu Niu, Haifeng Wang and Hua Wu, 2009. Exploiting heterogeneous treebanks for parsing. In ACL-09, pages 46-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL-06.</booktitle>
<contexts>
<context position="13333" citStr="Petrov et al., 2006" startWordPosition="2144" endWordPosition="2147"> will be transferred into UAF when J =0. 3.3 Corpus weighting technique In line 12 of Algorithm 1, we train a new parser with target Treebank and conversion trees. However, the errors in automatically conversion trees are unavoidable and they would limit the accuracy of the self-trained model. So we have to take some measures to weight the gold target Treebank and the automatically conversion trees. McClosky et al. (2006) and Niu et al. (2009) take the strategy that duplicates the gold Treebank data many times. However, this strategy isn’t suitable for PCFG-LA parser 1(Matsuzaki et al., 2005; Petrov et al., 2006), because PCFG-LA employs an EM algorithm in training stage, so duplicating gold Treebank would increase the training time tremendously. Instead, according to Huang and Harper (2009), we weight the posterior probabilities computed for the gold and automatically converted trees to balance their importance. Let count(A o E |t) be the count of rule A oE in a parse tree t . Tt and Tso t are the sets of target Treebank and automatically converted trees from source Treebank respectively. The posterior probability of rule A o E (with weighting parameterD ) can be expressed as: 1 We will use BerkeleyP</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein, 2006. Learning accurate, compact, and interpretable tree annotation. In ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Nae Wang</author>
<author>Jing-Shin Chang</author>
<author>Keh-Yih Su</author>
</authors>
<title>An automatic treebank conversion algorithm for corpus sharing.</title>
<date>1994</date>
<booktitle>In ACL-94.</booktitle>
<contexts>
<context position="5668" citStr="Wang et al. (1994)" startWordPosition="822" endWordPosition="825">e first stage of the parser. The final result got a 1.1% improvement over the previous best parser for section 23 of the Penn Treebank. Huang and Harper (2009) combined self-training into a PCFG-LA based parser both for English and Chinese. Experimental result showed that self-training contributed 0.83% absolute improvement using only 210k unlabeled sentences with a single generative parser. For the Chinese parsing, self-training contributed 1.03% absolute improvement. Treebank Conversion is another potential strategy to reuse existing source Treebanks for the study of target grammar parsing. Wang et al. (1994) proposed a Treebank conversion algorithm for corpus sharing. They employed a parser with target grammar formalism to get Nbest parse list for each sentence in source Treebank, selected the best conversion tree from the list using their algorithm, then inserted the conversion trees into training set, and finally retrained the parser with the enlarged training set. Experimental result shows their algorithm is effective. Collins et al. (1999) performed statistical constituency parsing of Czech on a Treebank that was converted from the Prague Dependency Treebank under the guidance of conversion r</context>
<context position="9760" citStr="Wang et al. (1994)" startWordPosition="1498" endWordPosition="1501">t into i T s ot (line 11). This procedure runs iteratively until all the trees in source Treebank have been converted, finally, we train a new parser Parser i with Ttrain , Tdev and T s o t (line 12). i 3.2 Parse selection In line 10 of Algorithm 1, we select the highest quality parse pˆ k from ParseListk according to function Score(ps, psot) , where ps denotes a tree in source Treebank and psot denotes a conversion tree with target Treebank grammar formalism for ps . Score(ps, psot) compares psot with ps and computes a score for psot taken ps as a reference. According to the idea proposed in Wang et al. (1994), we use the number of aligned constituents in the source and target trees to construct Score(ps ,psot) . We propose two types of Score(ps, psot) as follows. (1) Unlabeled aligned constituents F1 score (UAF) First, we define a constituent as tag[i,j], which represents a non-terminal node labeled with tag and spanning words from positions i to j of the input sentence. A non-terminal node in psot aligns with a non-terminal node in ps when they span the same words. If two nodes are aligned, we call them an aligned constituent and denote the aligned relationship as tags [i,j]  tagt [i, j] . For e</context>
</contexts>
<marker>Wang, Chang, Su, 1994</marker>
<rawString>Jong-Nae Wang, Jing-Shin Chang and Keh-Yih Su, 1994. An automatic treebank conversion algorithm for corpus sharing. In ACL-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
</authors>
<title>Converting dependency structures to phrase structures.</title>
<date>2001</date>
<booktitle>In The 1st Human Language Technology Conference (HLT-2001).</booktitle>
<contexts>
<context position="6360" citStr="Xia and Palmer (2001)" startWordPosition="927" endWordPosition="930">oyed a parser with target grammar formalism to get Nbest parse list for each sentence in source Treebank, selected the best conversion tree from the list using their algorithm, then inserted the conversion trees into training set, and finally retrained the parser with the enlarged training set. Experimental result shows their algorithm is effective. Collins et al. (1999) performed statistical constituency parsing of Czech on a Treebank that was converted from the Prague Dependency Treebank under the guidance of conversion rules and heuristic rules, and the final performance was also improved. Xia and Palmer (2001) proposed three methods to convert dependency trees into phrase structure trees with some hand-written heuristic rules. For acquisition of better conversion rules, Xia et al. (2008) proposed a method to automatically extract conversion rules from a target Treebank. Niu et al. (2009) tried to exploit heterogeneous Treebanks for parsing. They proposed a grammar formalism conversion algorithm to convert dependency formalism Treebank into phrase structure formalism, and did phrase structure parsing with the conversion trees. Their experiments are done in Chinese parsing, and the final performance </context>
</contexts>
<marker>Xia, Palmer, 2001</marker>
<rawString>Fei Xia and Martha Palmer, 2001. Converting dependency structures to phrase structures. In The 1st Human Language Technology Conference (HLT-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Owen Rambow</author>
<author>Rajesh Bhatt</author>
<author>Martha Palmer</author>
<author>Dipti Misra Sharma</author>
</authors>
<title>Towards a multi-representational treebank.</title>
<date>2008</date>
<booktitle>Proc. of the 7th Int&apos;lWorkshop on Treebanks and Linguistic Theories (TLT-7).</booktitle>
<pages>207--238</pages>
<contexts>
<context position="6541" citStr="Xia et al. (2008)" startWordPosition="953" endWordPosition="956">erted the conversion trees into training set, and finally retrained the parser with the enlarged training set. Experimental result shows their algorithm is effective. Collins et al. (1999) performed statistical constituency parsing of Czech on a Treebank that was converted from the Prague Dependency Treebank under the guidance of conversion rules and heuristic rules, and the final performance was also improved. Xia and Palmer (2001) proposed three methods to convert dependency trees into phrase structure trees with some hand-written heuristic rules. For acquisition of better conversion rules, Xia et al. (2008) proposed a method to automatically extract conversion rules from a target Treebank. Niu et al. (2009) tried to exploit heterogeneous Treebanks for parsing. They proposed a grammar formalism conversion algorithm to convert dependency formalism Treebank into phrase structure formalism, and did phrase structure parsing with the conversion trees. Their experiments are done in Chinese parsing, and the final performance is improved indeed. In summary, from the existing work we are confident that the strategies of self-training and Treebank conversion are effective to improve the performance of pars</context>
</contexts>
<marker>Xia, Rambow, Bhatt, Palmer, Sharma, 2008</marker>
<rawString>Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer and Dipti Misra Sharma, 2008. Towards a multi-representational treebank. Proc. of the 7th Int&apos;lWorkshop on Treebanks and Linguistic Theories (TLT-7). pages 207-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Zhou</author>
</authors>
<title>Annotation Scheme for Chinese Treebank.</title>
<date>2004</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>18</volume>
<pages>004</pages>
<contexts>
<context position="2605" citStr="Zhou, 2004" startWordPosition="374" endWordPosition="375">of manually annotated training data, such as the Penn Treebank (Marcus et al., 1993), to achieve high performance. However, it is quite costly and time-consuming to create high quality labeled data. So it becomes a key bottleneck for supervised approach to acquire sufficient labeled training data. Self-training is an effective strategy to overcome this shortage. It tries to enlarge the training set with automatically annotated unlabeled data and trains a parser with the enlarged training set. During the last few decades, many Treebanks annotated with different grammar formalisms are released (Zhou, 2004; Xue et al., 2005). Although they are annotated with different schemes, they have some linguistic consistency in some extent. Intuitively, we can convert Treebank annotated with one grammar formalisms into another Treebank annotated with grammar formalism that we are interested in. For simplicity, we call the first source Treebank, and the second target Treebank. And we call this strategy as Treebank conversion. Although both self-training and Treebank conversion can overcome the limitation of labeled data shortage for supervised parsing in some extent, they all have drawbacks. For selftraini</context>
</contexts>
<marker>Zhou, 2004</marker>
<rawString>Qiang Zhou, 2004. Annotation Scheme for Chinese Treebank. Journal of Chinese Information Processing, 18 (004).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>