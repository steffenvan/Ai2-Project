<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000064">
<title confidence="0.978959">
AutoLearn’s authoring tool: a piece of cake for teachers
</title>
<author confidence="0.985145">
Martí Quixal1, Susanne Preuß3, David García-Narbona2, Jose R. Boullosa2
</author>
<affiliation confidence="0.904651">
1 Voice and Language Group,
</affiliation>
<address confidence="0.844075333333333">
2 Advanced Development Group
Barcelona Media Centre d’Innovació
Diagonal, 177, E-08018 Barcelona, Spain
</address>
<email confidence="0.853279">
{marti.quixal,david.garcian,
beto.boullosa}@barcelonamedia.org
</email>
<sectionHeader confidence="0.998442" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999587375">
This paper1 presents AutoLearn’s authoring
tool: AutoTutor, a software solution that en-
ables teachers (content creators) to develop
language learning activities including auto-
matic feedback generation without the need of
being a programmer. The software has been
designed and implemented on the basis of
processing pipelines developed in previous
work. A group of teachers has been trained to
use the technology and the accompanying
methodology, and has used materials created
by them in their courses in real instruction set-
tings, which served as an initial evaluation.
The paper is structured in four sections: Sec-
tion 1 introduces and contextualizes the re-
search work. Section 2 describes the solution,
its architecture and its components, and spe-
cifically the way the NLP resources are cre-
ated automatically with teacher input. Section
3 describes and analyses a case study using
the tool to create and test a language learning
activity. Finally Section 4 concludes with re-
marks on the work done and connections to
related work, and with future work.
</bodyText>
<sectionHeader confidence="0.999619" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9960914">
Over the past four decades there have been several
hundreds of CALL (Computer-Aided Language
Learning) projects, often linked to CALL practice
(Levy 1997), and within the last twenty years a
considerable number of them focused on the use of
</bodyText>
<footnote confidence="0.9935645">
1 Research funded by the Lifelong Learning Programme 2007-
2013 (AUTOLEARN, 2007-3625/001-001).
</footnote>
<page confidence="0.995749">
19
</page>
<sectionHeader confidence="0.869604" genericHeader="introduction">
3 GFAI
</sectionHeader>
<bodyText confidence="0.971696676470588">
Martin-Luther-Str. 14
Saarbrücken, Germany
susannep@iai.uni-sb.de
NLP in the context of CALL (Amaral and Meur-
ers, in preparation). Despite this, there is an appall-
ing absence of parser-based CALL in real
instruction settings, which has been partially at-
tributed to a certain negligence of the pedagogical
needs (Amaral and Meurers, in preparation). In
contrast, projects and systems that were pedagogi-
cally informed succeeded, yielded and are yielding
interesting results, and are evolving for over a dec-
ade now (Nagata 2002; Nagata 2009; Heift 2001;
Heift 2003; Heift 2005; Amaral and Meurers, in
preparation). According to Amaral and Meurers
successful projects were able to restrict learner
production in terms of NLP complexity by limiting
the scope of the learning activities to language-
oriented (as opposed to communicative-oriented)
or translation exercises, or by providing feedback
on formal aspects of language in content oriented
activities, always under pedagogical considerations
–focus on form.
Our proposal is a step forward in this direction
in two ways: a) it allows for feedback generation
focusing both on formal and content (communica-
tive-oriented) aspects of language learning activi-
ties, and b) it provides teachers with a tool and a
methodology –both evolving– for them to gain
autonomy in the creation of parser-based CALL
activities –which by the way has a long tradition in
CALL (Levy 1997, chap. 2). The goal is to shape
language technologies to the needs of the teachers,
and truly ready-to-hand.
</bodyText>
<subsectionHeader confidence="0.583863">
1.1 Related work and research context
</subsectionHeader>
<bodyText confidence="0.994302333333333">
The extent to which pedagogues appreciate and
require autonomy in the design and creation of
CALL activities can be traced in the historical
</bodyText>
<note confidence="0.984777">
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 19–27,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.982279">
overview offered by (Levy 1997, 16, 17, 19, 23
and 38). Moreover, parallel research shows that the
integration of CALL in the learning context is
</bodyText>
<note confidence="0.6275635">
2 AutoTutor: AutoLearn’s authoring
software
</note>
<figureCaption confidence="0.999676">
Figure 1. AutoTutor software architecture.
</figureCaption>
<bodyText confidence="0.99988244">
critical to ensure the success of whatever materials
are offered to learners (Levy 1997, 200-203; Polis-
ca 2006).
AutoTutor goes beyond tools such as Hot Pota-
toes, eXelearning or JClic2 in that it offers the pos-
sibility of authoring NLP-based CALL activities. It
is also more ambitious than other authoring tools
developed for the creation of activities in intelli-
gent tutoring systems. Chen and Tokuda (2003)
and Rösener (2009) present authoring tools for
translation exercises, where expected learner input
is much more controlled (by the sentence in the
source language).
Heift and Toole (2002) present Tutor Assistant,
which enables to create activities such as build-a-
sentence, drag-and-drop and fill-in-the-blank. An
important difference between AutoTutor and Tutor
Assistant is that the latter is a bit more restrictive in
terms of the linguistic objects that can be used. It
also presents a lighter complexity in the modelling
of the underlying correction modules. However,
the system underlying Tutor Assistant provides
with more complex student adaptation functional-
ities (Heift 2003) and would be complementary in
terms of overall system functionalities.
</bodyText>
<footnote confidence="0.965446">
2 http://hotpot.uvic.ca/, http://sourceforge.net/apps/trac/exe/wiki,
http://clic.xtec.cat/es/jclic/index.htm.
</footnote>
<bodyText confidence="0.996808551724138">
AutoTutor is a web-based software solution to
assist non-NLP experts in the creation of language
learning activities using NLP-intensive processing
techniques. The process includes a simplified
specification of the means to automatically create
the resources used to analyse learner input for each
exercise. The goal is to use computational devices
to analyse learner production and to be able to go
beyond “yes-or-no” answers providing additional
feedback focused both on form and content.
This research work is framed within the
AutoLearn project, a follow up of the ALLES pro-
ject (Schmidt et al., 2004, Quixal et al., 2006).
AutoLearn’s aim was to exploit in a larger scale a
subset of the technologies developed in ALLES in
real instruction settings. Estrada et al. (2009) de-
scribe how, in AutoLearn’s first evaluation phase,
the topics of the activities were not attractive
enough for learners and how learner activity de-
creased within the same learning unit across exer-
cises. Both observations –together with what it has
been shown with respect to the integration of inde-
pendent language learning, see above – impelled
us to develop AutoTutor, which allows teachers to
create their own learning units.
As reflected in Figure 1, AutoTutor consists
primarily of two pieces of software: AutoTutor
Activity Creation Kit (ATACK) and AutoTutor
Activity Player (ATAP). ATACK, an authoring
</bodyText>
<page confidence="0.99022">
20
</page>
<bodyText confidence="0.998656142857143">
tool, provides teachers with the ability to create
parser-based CALL exercises and define the corre-
sponding exercise specifications for the generation
of automated feedback. ATAP allows teachers to
insert, track and manage those exercises in Moodle
(http://moodle.org), giving learners the possibility
to visualize and answer them. Both ATACK and
</bodyText>
<listItem confidence="0.96696875">
1. {The Hagia Sophia/The old mosque} is
famous for its massive dome.
2. The reputation of {the Hagia Sophia/the
old mosque} is due to its massive dome.
</listItem>
<bodyText confidence="0.896388333333333">
To model these possible answers, one would
use four blocks (see Figure 2) corresponding to
WHO (Hagia Sophia), WHAT (Famousness), and
</bodyText>
<figure confidence="0.9969738">
B2 (FAMOUSNESS)
A: B:
is famous for the reputation of
B1 (SOPHIA)
the Hagia Sophia
the old mosque
B3 (DUE)
is due to
B4 (CAUSE)
its massive dome
</figure>
<figureCaption confidence="0.999679">
Figure 2 Blocks as specified in AutoTutor GUI.
</figureCaption>
<bodyText confidence="0.9948955">
ATAP share a common infrastructure of NLP ser-
vices which provides the basic methods for gener-
ating, storing and using NLP tools. Access to those
methods is made through XML-RPC calls.
</bodyText>
<subsectionHeader confidence="0.993326">
2.1 AutoTutor Activity Creation Kit
</subsectionHeader>
<bodyText confidence="0.999983714285714">
ATACK is divided in two components: a GUI that
allows content creators to enter the text, questions
and instructions to be presented to learners in order
to elicit answers from them; and an NLP resource
creation module that automatically generates the
resources that will be used for the automated feed-
back. Through the GUI, teachers are also able to
define a set of expected correct answers for each
question, and, optionally, specific customized
feedback and sample answers.
To encode linguistic and conceptual variation in
the expected answers, teachers are required to turn
them into linguistic patterns using blocks. Blocks
represent abstract concepts, and contain the con-
crete chunks linked to those concepts. Within a
block one can define alternative linguistic struc-
tures representing the same concept. By combining
and ordering blocks, teachers can define the se-
quences of text that correspond to the expected
correct answers –i.e., they can provide the seeds
for answer modelling.
</bodyText>
<subsectionHeader confidence="0.917319">
Modelling answers
</subsectionHeader>
<bodyText confidence="0.999447333333333">
Given an exercise where learners are required
to answer the question “From an architecture point
of view, what makes Hagia Sophia in Istanbul so
famous according to its Wikipedia entry?”, the
following answers would be accepted:
WHY (Dome), and complementary linguistic ex-
pressions such as “is due to”. Thus, the possible
correct block sequences would be (indices corre-
sponding to Figure 2):
</bodyText>
<equation confidence="0.995586">
a) B1 B2.A B4
b) B2.B B1 B3 B4
</equation>
<bodyText confidence="0.992853375">
Block B1 is an example of interchangeable al-
ternatives (the Hagia Sophia or the old mosque),
which do not require any further condition to ap-
ply. In contrast, block B2 is an instance of a syn-
tactic variation of the concept. Famousness can be
expressed through an adjective or through a verb
(in our example), but each of the choices requires a
different sentence structure.
Alternative texts in a block with no variants (as
in B1) exploit the paradigmatic properties of lan-
guage, while alternative texts in a block with two
variants as in B2 account for its syntagmatic prop-
erties, reflected in the block sequences. Interest-
ingly, this sort of splitting of a sentence into blocks
is information-driven and simplifies the linguistic
expertise needed for the exercise specifications.
</bodyText>
<subsectionHeader confidence="0.987846">
2.2 Automatic generation of exercise-specific
NLP-resources
</subsectionHeader>
<bodyText confidence="0.986786571428571">
Figure 3 shows how the teacher’s input is con-
verted into NLP-components. Predefined system
components present plain borders, and the result-
ing ones present hyphenised borders. The figure
also reflects the need for answer and error model-
ling resources.
NLP resource generation process
</bodyText>
<page confidence="0.998811">
21
</page>
<figureCaption confidence="0.995749">
Figure 3. Processing schema and components of the customizable NLP resources of ATACK
</figureCaption>
<figure confidence="0.99879116">
Morph.
analysis
Morph.
analysis
Disam. of
base form
Teacher input (GUI)
ERROR MODEL
ANSWER MODEL
Blocks (word
chunks)
Teacher defined
error modelling
Block order
Exercise-specific lexicon
Exercise-specific
error checking
General content
evaluation
Content mat-
ching
Customized
feedback
Match
settings
</figure>
<bodyText confidence="0.978998352941176">
The generation of the NLP resources is possible
through the processing of the teacher’s input with
three modules: the morphological analysis module
performs a lexicon lookup and determines un-
known words that are entered into the exercise-
specific lexicon; the disambiguation of base form
module, disambiguates base forms, e.g. “better” is
disambiguated between verb and adjective depend-
ing on the context in preparation of customized
feedback.
The last and most important module in the ar-
chitecture is the match settings component, which
determines the linguistic features and structures to
be used by the content matching and the exercise-
specific error checking modules (see Figure 4).
Using relaxation techniques, the parsing of learner
input is flexible enough to recognize structures
including incorrect word forms and incorrect,
missing or additional items such as determiners,
prepositions or digits, or even longish chunks of
text with no correspondence the specified answers.
The match settings component contains rules that
later on trigger the input for the exercise-specific
error checking.
The match settings component consists of
KURD rules (Carl et al. 1998). Thus it can be
modified and extended by a computational linguist
any time without the need of a programmer.
Once the exercise’s questions and expected an-
swers have been defined, ATACK allows for the
generation of the NLP resources needed for the
automatic correction of that exercise. The right-
hand side of Figure 3 shows which the generated
resources are:
</bodyText>
<listItem confidence="0.985099944444445">
• An exercise-specific lexicon to handle un-
known words
• A content matching module based on the
KURD formalism to define several lin-
guistically-motivated layers with different
levels of relaxation (using word, lemma,
and grammatical features) for determining
the matching between the learner input and
the expected answers
• A customized feedback module for teacher-
defined exercise-specific feedback
• An exercise-specific error checking mod-
ule for context-dependent errors linked to
language aspects in the expected answers
• A general content evaluation component
that checks whether the analysis performed
by the content matching module conforms
to the specified block orders
</listItem>
<subsectionHeader confidence="0.993109">
2.3 AutoTutor Activity Player (ATAP)
</subsectionHeader>
<bodyText confidence="0.999956571428571">
With ATAP learners have access to the contents
enhanced with automatic tutoring previously cre-
ated by teachers. ATAP consists of a) a client GUI
for learners, integrated in Moodle, to answer exer-
cises and track their own activity; b) a client GUI
for teachers, also integrated in Moodle, used to
manage and track learning resources and learner
</bodyText>
<page confidence="0.982497">
22
</page>
<bodyText confidence="0.987652444444444">
activity; and c) a backend module, integrated into
the AutoTutor NLP Services Infrastructure, re-
sponsible for parsing the learner’s input and gener-
ating feedback messages.
Figure 4 describes the two steps involved in the
NLP-based feedback generation: the NLP compo-
nents created through ATACK –in hyphenised
rectangles– are combined with general built-in
NLP-based correction modules.
</bodyText>
<subsectionHeader confidence="0.992269">
2.4 The feedback generation software
</subsectionHeader>
<bodyText confidence="0.999881875">
Feedback is provided to learners in two steps,
which is reflected in Figure 4 by the two parts, the
upper and lower part, called General Checking and
Exercise Specific Checking respectively. The for-
mer consists in the application of standard spell
and grammar checkers. The latter consists in the
application of the NLP resources automatically
generated with the teacher’s input.
</bodyText>
<subsectionHeader confidence="0.78242">
Content matching module
</subsectionHeader>
<bodyText confidence="0.999964785714286">
The text chunks (blocks) that the teacher has en-
tered into ATACK’s GUI are converted into
KURD rules. KURD provides with sophisticated
linguistically-oriented matching and action opera-
tors. These operators are used to model (predict-
able) learner text. The content matching module is
designed to be able to parse learners input with
different degrees of correctness combining both
relaxation techniques and mal-rules. For instance,
it detects the presence of both correct and incorrect
word forms, but it also detects incorrect words
belonging to a range of closed or open word
classes –mainly prepositions, determiners, modal
verbs and digits– which can be used to issue a cor-
responding linguistically motivated error messages
like “Preposition wrong in this context”, in a con-
text where the preposition is determined by the
relevant communicative situation.
Error types that are more complex to handle in
technical terms involve mismatches between the
amount of expected elements and the actual
amount of informational elements in the learner’s
answer. Such mismatches arise on the grammatical
level if a composite verb form is used instead of a
simple one, or when items such as determiners or
commas are missing or redundant. The system also
accounts for additional modifiers and other words
interspersed in the learner’s answer.
The matching strategy uses underspecified
empty slots to fit in textual material in between the
correct linguistic structures. Missing words are
handled by a layer of matching in which certain
elements, mainly grammatical function words such
as determiners or auxiliary verbs, are optional.
Incorrect word choice in open and closed word
classes is handled by matching on more abstract
linguistic features instead of lexeme features.
The interaction between KURD-based linguis-
tically-driven triggers in the content matching
module and the rules in the exercise-specific error
checking (see below) module allows for specific
mal-rule based error correction.
</bodyText>
<subsectionHeader confidence="0.577469">
Customized feedback
</subsectionHeader>
<bodyText confidence="0.9987532">
Teachers can create specific error messages for
simple linguistic patterns (containing errors or
searching for missing items) ranging from one or
two word structures to more complex word-based
linguistic structures. Technically, error patterns are
</bodyText>
<figure confidence="0.849564">
GENERAL CHECKING (ONE)
EXERCISE-SPECIFIC CHECKING (TWO)
</figure>
<figureCaption confidence="0.996836">
Figure 4. Processing schema of the NLP resources to generate automatic feedback.
</figureCaption>
<page confidence="0.863641">
23
</page>
<figure confidence="0.807466692307692">
Grammar
checking
Morph. Spell
analysis checking
Lexicon
Exercise-specific lexicon
Content
matching
Customized
feedback
Exercise-specific
error checking
General content
</figure>
<page confidence="0.287608">
evaluation
</page>
<bodyText confidence="0.962636538461538">
implemented as KURD rules linked to a specific
error message. These rules have preference over
the rules applied by any other later module.
Exercise-specific error checking
Teachers do not encode all the exercise-specific
errors themselves because a set of KURD rules for
the detection of prototypical errors is encoded –this
module uses the triggers set by the content match-
ing component. Exercise-specific linguistic errors
handled in this module have in common that they
result in sentences that are likely to be wrong ei-
ther from a formal (but context-dependent) point of
view or from an informational point of view.
</bodyText>
<subsectionHeader confidence="0.523126">
General content evaluation
</subsectionHeader>
<bodyText confidence="0.99970325">
Since the contents are specified by the blocks cre-
ated by teachers, the evaluation has a final step in
which the system checks whether the learner’s
answer contains all the necessary information that
belongs to a valid block sequence.
This module checks for correct order in infor-
mation blocks, for blending structures (mixtures of
two possible correct structures), missing informa-
tion and extra words (which do not always imply
an error). The messages generated with this com-
ponent pertain to the level of completeness and
adequacy of the answer in terms of content.
</bodyText>
<sectionHeader confidence="0.95745" genericHeader="method">
3 Usage and evaluation
</sectionHeader>
<bodyText confidence="0.999988">
AutoTutor has been used by a group of seven
content creators –university and school teachers–
for a period of three months. They developed over
20 activities for learning units on topics such as
business and finance, sustainable production and
consumption, and new technologies. Those activi-
ties contain listening and reading comprehension
activities, short-text writing activities, enabling
tasks on composition writing aspects, etc. whose
answers must be expressed in relatively free an-
swers consisting of one sentence. In November
2009, these activities were used in real instruction
settings with approximately 600 learners of Eng-
lish and German. Furthermore, an evaluation of
both teacher and learner satisfaction and system
performance was carried out.
We briefly describe the process of creating the
materials by one of the (secondary school) teachers
participating in the content creation process and
evaluate the results of system performance in one
activity created by this same teacher.
</bodyText>
<subsectionHeader confidence="0.997697">
3.1 Content creation: training and practice
</subsectionHeader>
<bodyText confidence="0.999975272727273">
To start the process teachers received a 4-hour
training course (in two sessions) where they were
taught how to plan, pedagogically speaking, a
learning sequence including activities to be cor-
rected using automatically generated feedback. We
required them to develop autonomous learning
units if possible. And we invited them to get hold
of any available technology or platform functional-
ity to implement their ideas (and partially offered
support to them too), convinced that technology
had to be a means rather than a goal in itself. The
course also included an overview of NLP tech-
niques and a specific course on the mechanics of
ATACK (the authoring tool) and ATAP (the activ-
ity management and deployment tool).
During this training we learned that most teach-
ers do not plan how activities will be assessed: that
is, they often do not think of the concrete answers
to the possible questions they will pose to learners.
They do not need to, since they have all the knowl-
edge required to correct learner production any
place, any time in their heads (the learner, the ac-
tivity and the expert model) no matter if the learner
production is written or oral. This is crucial since it
requires a change in normal working routine.
After the initial training they created learning
materials. During creation we interacted with them
to make sure that they were not designing activities
whose answers were simply impossible to model.
For instance, the secondary school teacher who
prepared the activity on sustainable production and
consumption provided us with a listening compre-
hension activity including questions such as:
</bodyText>
<construct confidence="0.814874857142857">
1) Which is your attitude concerning respon-
sible consumption? How do you deal with
recycling? Do you think yours is an eco-
logical home? Are you doing your best to
reduce your ecological footprint? Make a
list with 10 things you could do at home to
reduce, reuse o recycle waste at home.
</construct>
<bodyText confidence="0.999930666666667">
All these things were asked in one sole instruc-
tion, to be answered in one sole text area. We then
talked to the teacher and argued with her the kinds
of things that could be modelled using simple one-
sentence answers. We ended up reducing the input
provided to learners to perform the activity to one
</bodyText>
<page confidence="0.99719">
24
</page>
<bodyText confidence="0.9978935">
video (initially a text and a video) and prompting
learners with the following three questions:
</bodyText>
<listItem confidence="0.996718333333333">
1) Explain in your words what the ecological
footprint is.
2) What should be the role of retailers accord-
ing to Timo Mäkelä?
3) Why should producers and service provid-
ers use the Ecolabel?
</listItem>
<bodyText confidence="0.99958975">
Similar interventions were done in other activi-
ties created by other content creators. But some of
them were able to create activities which could be
used almost straightforwardly.
</bodyText>
<subsectionHeader confidence="0.999859">
3.2 System evaluation
</subsectionHeader>
<bodyText confidence="0.990043951219512">
The materials created by teachers were then
used in their courses. In the setting that we analyse
learners of English as a second language were
Catalan and Spanish native speakers between 15
and 17 years old that attended a regular first year
of Batxillerat (first course for those preparing to
enter university studies). They had all been learn-
ing English for more than five years, and according
to their teacher their CEF level was between A2
and B1. They were all digital literates and they all
used the computer on a weekly basis for their stud-
ies or leisure (80% daily).
We analyse briefly the results obtained for two
of the questions in one of the activities created by
the school teacher who authored the learning unit
on sustainable production and consumption,
namely questions 1) and 2) above. This learning
unit was offered to a group of 25 learners.
Overall system performance
Table 1 reflects the number of attempts performed
by learners trying to answer the two questions
evaluated here: correct, partially correct and incor-
rect answers are almost equally distributed (around
30% each) and non-evaluated answers are roughly
10%. In non-evaluated answers we include basi-
cally answers where learners made a bad use of the
system (e.g., answers in a language other than the
one learned) or answers which were exactly the
same as the previous one for two attempts in a row,
which can interpreted in several ways (misunder-
standing of the feedback, usability problems with
the interface, problems with pop-up windows, etc.)
that fall out of the scope of the current analysis.
Table 2 and Table 3 show the number of mes-
sages issued by the system for correct, partially
correct and incorrect answers for each of the two
questions analyzed. The tables distinguish between
Form Messages and Content Messages, and Real
Form Errors and Real Content Errors –a crucial
distinction given our claim that using AutoTutor
more open questions could be tackled.3
</bodyText>
<table confidence="0.997562">
QST CORR. PART. INCORR. INV. TOT
1ST 36 23 12 2 73
2ND 14 29 36 21 100
ALL 50 (29 %) 52(30 %) 48(28 %) 23(13 %) 173
</table>
<tableCaption confidence="0.999978">
Table 1. Correct, partially correct and incorrect answers.
</tableCaption>
<bodyText confidence="0.979829142857143">
Table 2 and Table 3 show that the contrast be-
tween issued feedback messages (most commonly
error messages, but sometimes rather pieces of
advice or suggestions) and real problems found in
the answers is generally balanced in formal prob-
lems (31:15, 8:7 and 41:39 for Table 2; and 6:8,
29:18, and 20:21 for Table 3) independently of the
correctness of the answer.
On the contrary, the contrast between issued
messages and content problems is much more un-
balanced in correct and partially correct answers
(139:71 and 84:42 for Table 2; and 45:20 and
110:57 for Table 3) and more balanced for incor-
rect answers (30:18 for Table 2; and 93:77 for
</bodyText>
<tableCaption confidence="0.593914">
Table 3).
</tableCaption>
<table confidence="0.992576">
MESSAGES REAL ERRORS
Form Cont Form Cont
CORRECT ANSWERS 31 139 15 71
PARTIALLY CORRECT 8 84 7 42
INCORRECT ANSWERS 41 30 39 18
TOTAL ANSWERS 80 253 61 131
</table>
<tableCaption confidence="0.996981">
Table 2. Messages issued vs. real errors for question 1
in the answers produced by learners.
</tableCaption>
<table confidence="0.999264">
MESSAGES REAL ERRORS
Form Cont Form Cont
CORRECT ANSWERS 6 45 8 20
PARTIALLY CORRECT 29 110 18 57
INCORRECT ANSWERS 20 93 21 77
TOTAL ANSWERS 55 248 47 154
</table>
<tableCaption confidence="0.9616335">
Table 3. Messages issued vs. real errors for question 2
in the answers produced by learners.
</tableCaption>
<bodyText confidence="0.996038">
This indicates that generally speaking the sys-
tem behaved more confidently in the detection of
formal errors than in the detection of content er-
rors.
</bodyText>
<footnote confidence="0.983916">
3 A proper evaluation would require manual correction of the
activities by a number of teachers and the corresponding
evaluation process.
</footnote>
<page confidence="0.997322">
25
</page>
<subsectionHeader confidence="0.979671">
System feedback analysis
</subsectionHeader>
<bodyText confidence="0.998987833333333">
To analyze the system’s feedback we looked into
the answers and the feedback proposed by the sys-
tem and annotated each answer with one or more
of the tags corresponding to a possible cause of
misbehaviour. The possible causes and its absolute
frequency are listed in Table 4.
The less frequent ones are bad use of the system
on the learner side, bad guidance (misleading the
learner to an improper answer or to a more com-
plex way of getting to it), connection failure, and
message drawing attention on form when the error
was on content.
</bodyText>
<table confidence="0.9993618">
MISBEHAVIOUR QUESTION 1 QUESTION 2
CONN-FAIL 1 0
BAD-USE 1 1
FRM-INSTOF-CONT 2 1
BAD-GUIDE 4 2
OOV 11 13
WRNG-DIAG 11 20
FRM-STRICT 33 20
ARTIF-SEP 0 61
SPECS-POOR 1 62
</table>
<tableCaption confidence="0.99982">
Table 4. Frequent sources of system errors.
</tableCaption>
<bodyText confidence="0.999951625">
The most frequent causes of system misbehav-
iour are out-of-vocabulary words, wrong diagno-
ses, and corrections too restrictive with respect to
form.
Two interesting causes of misbehaviour and in
fact the most frequent ones were artificial separa-
tion and poor specifications. The former refers to
the system dividing answer parts into smaller parts
(and therefore generation of a larger number of
issued messages). For instance in a sentence like
(as an answer to question 2)
The retailers need to make sure that whatever
they label or they put in shelf is understandable
to consumers.4
the system would generate six different feedback
messages informing that some words were not
expected (even if correct) and some were found but
not in the expected location or form.
In this same sentence above we find examples
of too poor specifications, where, for instance, it
was not foreseen that retailers was used in the
answer. These two kinds of errors reflect the flaws
of the current system: artificial separation reflects a
lack of generalization capacity of the underlying
</bodyText>
<footnote confidence="0.979473666666667">
4 One of the expected possible answers was “They need to
make sure that whatever they label and whatever they put in
the shelves is understood by consumers”.
</footnote>
<bodyText confidence="0.999827333333333">
parser, and poor specifications reflect the incom-
pleteness of the information provided by novice
users, teachers acting as material designers.
</bodyText>
<sectionHeader confidence="0.985781" genericHeader="method">
4 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999969222222222">
This paper describes software that provides
non-NLP experts with a means to utilize and cus-
tomize NLP-intensive resources using an authoring
tool for language instruction activities. Its usability
and usefulness have been tested in real instruction
settings and are currently being evaluated and ana-
lyzed. Initial analyses show that the technology
and methodology proposed allow teachers to create
contents including automatic generation feedback
without the need of being neither a programmer
nor an NLP expert.
Moreover, system performance shows a reason-
able confidence in error detection given the imma-
turity of the tool and of its users –following
Shneiderman and Plaisant’s terminology (2006).
There is room for improvement in the way to re-
duce false positives related with poor specifica-
tions. It is quite some work for exercise designers
to foresee a reasonable range of linguistic alterna-
tives for each answer. One could further support
them in the design of materials with added func-
tionalities –using strategies such as shallow seman-
tic parsing, as in (Bailey and Meurers, 2008), or
adding functionalities on the user interface that
allow teachers to easily feed exercise models or
specific feedback messages using learner answers.
The architecture presented allows for portability
into other languages (English and German already
available), with a relative simplicity provided that
the lexicon for the language exists and contains
basic morpho-syntactic information. Moreover,
having developed it as a Moodle extension makes
it available to a wide community of teachers and
learners. The modularity of ATACK and ATAP
makes them easy to integrate in other Learning
Management Systems.
In the longer term we plan to improve AutoTu-
tor’s configurability so that its behaviour can be
defined following pedagogical criteria. One of the
aspects to be improved is that a computational
linguist is needed to add new global error types to
be handled or new linguistic phenomena to be con-
sidered in terms of block order. If such a system is
used by wider audiences, then statistically driven
techniques might be employed gradually, probably
</bodyText>
<page confidence="0.983728">
26
</page>
<bodyText confidence="0.999902375">
in combination with symbolic techniques –the
usage of the tool will provide with invaluable
learner corpora. In the meantime AutoTutor pro-
vides with a means to have automatic correction
and feedback generation for those areas and text
genres where corpus or native speaker text is
scarce, and experiments show it could be realisti-
cally used in real instruction settings.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999971222222222">
We want to thank the secondary school teachers
who enthusiastically volunteered in the creation
and usage of AutoLearn materials: Eli Garrabou
(Fundació Llor), Mònica Castanyer, Montse Pada-
reda (Fundació GEM) and Anna Campillo (Escola
Sant Gervasi). We also want to thank their learn-
ers, who took the time and made the effort to go
through them. We also thank two anonymous re-
viewers for their useful comments.
</bodyText>
<sectionHeader confidence="0.999115" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999950321428572">
Amaral, Luiz A., and Detmar Meurers. On Using Intel-
ligent Computer-Assisted Language Learning in
Real-Life Foreign Language Teaching and Learning
(Submitted).
Bailey, Stacey and Detmar Meurers (2008) Diagnosing
meaning errors in short answers to reading compre-
hension question. In Proceedings of the Third ACL
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 107–115, Columbus,
Ohio, USA, June 2008.
Carl, Michael, and Antje Schmidt-Wigger (1998). Shal-
low Post Morphological Processing with KURD. In
Proceedings of NeMLaP&apos;98, Sydney.
Chen, Liang and Naoyuki Tokuda (2003) A New Tem-
plate-Template-enhanced ICALL System for a Sec-
ond Language Composition Course. CALICO
Journal, Vol. 20, No. 3: May 2003.
Estrada, M., R. Navarro-Prieto, M. Quixal (2009) Com-
bined evaluation of a virtual learning environment:
use of qualitative methods and log interpretation to
evaluate a computer mediated language course. In
Proceedings of International Conference on Educa-
tion and New Learning Technologies, EDULEARN
09. Barcelona (Spain), 6th-8th July, 2009.
Heift, Trude. 2001. Intelligent Language Tutoring Sys-
tems for Grammar Practice. Zeitschrift für Interkul-
turellen Fremdsprachenunterricht 6, no. 2.
http://www.ualberta.ca/~german/ejournal/ heift2.htm.
———. 2003. Multiple learner errors and meaningful
feedback: A challenge for ICALL systems. CALICO
Journal 20, no. 3: 533-548.
———. 2005. Corrective Feedback and Learner Uptake
in CALL. ReCALL Journal 17, no. 1: 32-46.
Heift, Trude, and Mathias Schulze. 2007. Errors and
Intelligence in Computer-Assisted Language Learn-
ing: Parsers and Pedagogues. New York: Routledge.
Levy, Michael. 1997. Computer-Assisted Language
Learning. Context and Conceptualization. Oxford:
Oxford University Press.
Nagata, Noriko. 2002. BANZAI: An Application of
Natural Language Processingto Web based
Language Learning. CALICO Journal 19, no. 3: 583-
599.
———. 2009. Robo-Sensei’s NLP-Based Error Detec-
tion and Feedback Generation. CALICO Journal 26,
no. 3: 562-579.
Polisca, Elena. 2006. Facilitating the Learning Process:
An Evaluation of the Use and Benefits of a Virtual
Learning Environment (VLE)-enhanced Independent
Language-learning Program (ILLP). CALICO Jour-
nal 23, no.3: 499-51.
Quixal, M., T. Badia, B. Boullosa, L. Díaz, and A. Rug-
gia. (2006). Strategies for the Generation of Indi-
vidualised Feedback in Distance Language Learning.
In Proceedings of the Workshop on Language-
Enabled Technology and Development and Evalua-
tion of Robust Spoken Dialogue Systems of ECAI
2006. Riva del Garda, Italy, Sept. 2006.
Rösener, C.: “A linguistic intelligent system for tech-
nology enhanced learning in vocational training – the
ILLU project”. In Cress, U.; Dimitrova, V.; Specht,
M. (Eds.): Learning in the Synergy of Multiple Dis-
ciplines. 4th European Conference on Technology
Enhanced Learning, EC-TEL 2009 Nice, France,
Sept. 29 – Oct. 2, 2009. Lecture Notes in Computer
Science. Programming and Software Engineering,
Vol. 5794, 2009, XVIII, p. 813, Springer, Berlin.
Schmidt, P., S. Garnier, M. Sharwood, T. Badia, L.
Díaz, M. Quixal, A. Ruggia, A. S. Valderrabanos, A.
J. Cruz, E. Torrejon, C. Rico, J. Jimenez. (2004)
ALLES: Integrating NLP in ICALL Applications. In
Proceedings of Fourth International Conference on
Language Resources and Evaluation. Lisbon, vol. VI
p. 1888-1891. ISBN: 2-9517408-1-6.
Shneiderman, B. and C. Plaisant. (2006) Strategies for
evaluating information visualization tools: multi-
dimensional in-depth long-term case studies. BELIV
’06: Proceedings of the 2006 AVI workshop on Be-
yond time and errors: novel evaluation methods for
information visualization, May 2006.
Toole, J. &amp; Heift, T. (2002). The Tutor Assistant: An
Authoring System for a Web-based Intelligent Lan-
guage Tutor. Computer Assisted Language Learning,
15(4), 373-86.
</reference>
<page confidence="0.998809">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.751242">
<title confidence="0.997322">AutoLearn’s authoring tool: a piece of cake for teachers</title>
<author confidence="0.98802">Susanne David Jose R</author>
<affiliation confidence="0.940725">and Language Development Barcelona Media Centre</affiliation>
<address confidence="0.965716">Diagonal, 177, E-08018 Barcelona,</address>
<email confidence="0.999054">beto.boullosa}@barcelonamedia.org</email>
<abstract confidence="0.99618724">presents AutoLearn’s authoring tool: AutoTutor, a software solution that enables teachers (content creators) to develop language learning activities including automatic feedback generation without the need of being a programmer. The software has been designed and implemented on the basis of processing pipelines developed in previous work. A group of teachers has been trained to use the technology and the accompanying methodology, and has used materials created by them in their courses in real instruction settings, which served as an initial evaluation. The paper is structured in four sections: Section 1 introduces and contextualizes the research work. Section 2 describes the solution, its architecture and its components, and specifically the way the NLP resources are created automatically with teacher input. Section 3 describes and analyses a case study using the tool to create and test a language learning activity. Finally Section 4 concludes with remarks on the work done and connections to related work, and with future work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Luiz A Amaral</author>
<author>Detmar Meurers</author>
</authors>
<title>On Using Intelligent Computer-Assisted Language Learning in Real-Life Foreign Language Teaching and Learning (Submitted).</title>
<marker>Amaral, Meurers, </marker>
<rawString>Amaral, Luiz A., and Detmar Meurers. On Using Intelligent Computer-Assisted Language Learning in Real-Life Foreign Language Teaching and Learning (Submitted).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stacey Bailey</author>
<author>Detmar Meurers</author>
</authors>
<title>Diagnosing meaning errors in short answers to reading comprehension question.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>107--115</pages>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="28359" citStr="Bailey and Meurers, 2008" startWordPosition="4465" endWordPosition="4468"> of being neither a programmer nor an NLP expert. Moreover, system performance shows a reasonable confidence in error detection given the immaturity of the tool and of its users –following Shneiderman and Plaisant’s terminology (2006). There is room for improvement in the way to reduce false positives related with poor specifications. It is quite some work for exercise designers to foresee a reasonable range of linguistic alternatives for each answer. One could further support them in the design of materials with added functionalities –using strategies such as shallow semantic parsing, as in (Bailey and Meurers, 2008), or adding functionalities on the user interface that allow teachers to easily feed exercise models or specific feedback messages using learner answers. The architecture presented allows for portability into other languages (English and German already available), with a relative simplicity provided that the lexicon for the language exists and contains basic morpho-syntactic information. Moreover, having developed it as a Moodle extension makes it available to a wide community of teachers and learners. The modularity of ATACK and ATAP makes them easy to integrate in other Learning Management S</context>
</contexts>
<marker>Bailey, Meurers, 2008</marker>
<rawString>Bailey, Stacey and Detmar Meurers (2008) Diagnosing meaning errors in short answers to reading comprehension question. In Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 107–115, Columbus, Ohio, USA, June 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Carl</author>
</authors>
<title>and Antje Schmidt-Wigger</title>
<date>1998</date>
<booktitle>In Proceedings of NeMLaP&apos;98,</booktitle>
<location>Sydney.</location>
<marker>Carl, 1998</marker>
<rawString>Carl, Michael, and Antje Schmidt-Wigger (1998). Shallow Post Morphological Processing with KURD. In Proceedings of NeMLaP&apos;98, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Chen</author>
<author>Naoyuki Tokuda</author>
</authors>
<title>A New Template-Template-enhanced ICALL System for a Second Language Composition Course.</title>
<date>2003</date>
<journal>CALICO Journal,</journal>
<volume>20</volume>
<contexts>
<context position="4306" citStr="Chen and Tokuda (2003)" startWordPosition="652" endWordPosition="655"> (Levy 1997, 16, 17, 19, 23 and 38). Moreover, parallel research shows that the integration of CALL in the learning context is 2 AutoTutor: AutoLearn’s authoring software Figure 1. AutoTutor software architecture. critical to ensure the success of whatever materials are offered to learners (Levy 1997, 200-203; Polisca 2006). AutoTutor goes beyond tools such as Hot Potatoes, eXelearning or JClic2 in that it offers the possibility of authoring NLP-based CALL activities. It is also more ambitious than other authoring tools developed for the creation of activities in intelligent tutoring systems. Chen and Tokuda (2003) and Rösener (2009) present authoring tools for translation exercises, where expected learner input is much more controlled (by the sentence in the source language). Heift and Toole (2002) present Tutor Assistant, which enables to create activities such as build-asentence, drag-and-drop and fill-in-the-blank. An important difference between AutoTutor and Tutor Assistant is that the latter is a bit more restrictive in terms of the linguistic objects that can be used. It also presents a lighter complexity in the modelling of the underlying correction modules. However, the system underlying Tutor</context>
</contexts>
<marker>Chen, Tokuda, 2003</marker>
<rawString>Chen, Liang and Naoyuki Tokuda (2003) A New Template-Template-enhanced ICALL System for a Second Language Composition Course. CALICO Journal, Vol. 20, No. 3: May 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Estrada</author>
<author>R Navarro-Prieto</author>
<author>M Quixal</author>
</authors>
<title>Combined evaluation of a virtual learning environment: use of qualitative methods and log interpretation to evaluate a computer mediated language course.</title>
<date>2009</date>
<booktitle>In Proceedings of International Conference on Education and New Learning Technologies, EDULEARN 09.</booktitle>
<location>Barcelona</location>
<contexts>
<context position="5949" citStr="Estrada et al. (2009)" startWordPosition="888" endWordPosition="891">ues. The process includes a simplified specification of the means to automatically create the resources used to analyse learner input for each exercise. The goal is to use computational devices to analyse learner production and to be able to go beyond “yes-or-no” answers providing additional feedback focused both on form and content. This research work is framed within the AutoLearn project, a follow up of the ALLES project (Schmidt et al., 2004, Quixal et al., 2006). AutoLearn’s aim was to exploit in a larger scale a subset of the technologies developed in ALLES in real instruction settings. Estrada et al. (2009) describe how, in AutoLearn’s first evaluation phase, the topics of the activities were not attractive enough for learners and how learner activity decreased within the same learning unit across exercises. Both observations –together with what it has been shown with respect to the integration of independent language learning, see above – impelled us to develop AutoTutor, which allows teachers to create their own learning units. As reflected in Figure 1, AutoTutor consists primarily of two pieces of software: AutoTutor Activity Creation Kit (ATACK) and AutoTutor Activity Player (ATAP). ATACK, a</context>
</contexts>
<marker>Estrada, Navarro-Prieto, Quixal, 2009</marker>
<rawString>Estrada, M., R. Navarro-Prieto, M. Quixal (2009) Combined evaluation of a virtual learning environment: use of qualitative methods and log interpretation to evaluate a computer mediated language course. In Proceedings of International Conference on Education and New Learning Technologies, EDULEARN 09. Barcelona (Spain), 6th-8th July, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trude Heift</author>
</authors>
<title>Intelligent Language Tutoring Systems for Grammar Practice.</title>
<date>2001</date>
<journal>Zeitschrift für Interkulturellen Fremdsprachenunterricht</journal>
<volume>6</volume>
<note>http://www.ualberta.ca/~german/ejournal/ heift2.htm.</note>
<contexts>
<context position="2299" citStr="Heift 2001" startWordPosition="343" endWordPosition="344">ogramme 2007- 2013 (AUTOLEARN, 2007-3625/001-001). 19 3 GFAI Martin-Luther-Str. 14 Saarbrücken, Germany susannep@iai.uni-sb.de NLP in the context of CALL (Amaral and Meurers, in preparation). Despite this, there is an appalling absence of parser-based CALL in real instruction settings, which has been partially attributed to a certain negligence of the pedagogical needs (Amaral and Meurers, in preparation). In contrast, projects and systems that were pedagogically informed succeeded, yielded and are yielding interesting results, and are evolving for over a decade now (Nagata 2002; Nagata 2009; Heift 2001; Heift 2003; Heift 2005; Amaral and Meurers, in preparation). According to Amaral and Meurers successful projects were able to restrict learner production in terms of NLP complexity by limiting the scope of the learning activities to languageoriented (as opposed to communicative-oriented) or translation exercises, or by providing feedback on formal aspects of language in content oriented activities, always under pedagogical considerations –focus on form. Our proposal is a step forward in this direction in two ways: a) it allows for feedback generation focusing both on formal and content (comm</context>
</contexts>
<marker>Heift, 2001</marker>
<rawString>Heift, Trude. 2001. Intelligent Language Tutoring Systems for Grammar Practice. Zeitschrift für Interkulturellen Fremdsprachenunterricht 6, no. 2. http://www.ualberta.ca/~german/ejournal/ heift2.htm.</rawString>
</citation>
<citation valid="true">
<title>Multiple learner errors and meaningful feedback: A challenge for ICALL systems.</title>
<date>2003</date>
<journal>CALICO Journal</journal>
<volume>20</volume>
<pages>533--548</pages>
<contexts>
<context position="4306" citStr="(2003)" startWordPosition="655" endWordPosition="655"> 17, 19, 23 and 38). Moreover, parallel research shows that the integration of CALL in the learning context is 2 AutoTutor: AutoLearn’s authoring software Figure 1. AutoTutor software architecture. critical to ensure the success of whatever materials are offered to learners (Levy 1997, 200-203; Polisca 2006). AutoTutor goes beyond tools such as Hot Potatoes, eXelearning or JClic2 in that it offers the possibility of authoring NLP-based CALL activities. It is also more ambitious than other authoring tools developed for the creation of activities in intelligent tutoring systems. Chen and Tokuda (2003) and Rösener (2009) present authoring tools for translation exercises, where expected learner input is much more controlled (by the sentence in the source language). Heift and Toole (2002) present Tutor Assistant, which enables to create activities such as build-asentence, drag-and-drop and fill-in-the-blank. An important difference between AutoTutor and Tutor Assistant is that the latter is a bit more restrictive in terms of the linguistic objects that can be used. It also presents a lighter complexity in the modelling of the underlying correction modules. However, the system underlying Tutor</context>
</contexts>
<marker>2003</marker>
<rawString>———. 2003. Multiple learner errors and meaningful feedback: A challenge for ICALL systems. CALICO Journal 20, no. 3: 533-548.</rawString>
</citation>
<citation valid="true">
<title>Corrective Feedback and Learner Uptake in CALL.</title>
<date>2005</date>
<journal>ReCALL Journal</journal>
<volume>17</volume>
<pages>32--46</pages>
<marker>2005</marker>
<rawString>———. 2005. Corrective Feedback and Learner Uptake in CALL. ReCALL Journal 17, no. 1: 32-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trude Heift</author>
<author>Mathias Schulze</author>
</authors>
<title>Errors and Intelligence in Computer-Assisted Language Learning: Parsers and Pedagogues.</title>
<date>2007</date>
<publisher>Routledge.</publisher>
<location>New York:</location>
<marker>Heift, Schulze, 2007</marker>
<rawString>Heift, Trude, and Mathias Schulze. 2007. Errors and Intelligence in Computer-Assisted Language Learning: Parsers and Pedagogues. New York: Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Levy</author>
</authors>
<title>Computer-Assisted Language Learning. Context and Conceptualization.</title>
<date>1997</date>
<publisher>University Press.</publisher>
<location>Oxford: Oxford</location>
<contexts>
<context position="1557" citStr="Levy 1997" startWordPosition="229" endWordPosition="230">ection 1 introduces and contextualizes the research work. Section 2 describes the solution, its architecture and its components, and specifically the way the NLP resources are created automatically with teacher input. Section 3 describes and analyses a case study using the tool to create and test a language learning activity. Finally Section 4 concludes with remarks on the work done and connections to related work, and with future work. 1 Introduction Over the past four decades there have been several hundreds of CALL (Computer-Aided Language Learning) projects, often linked to CALL practice (Levy 1997), and within the last twenty years a considerable number of them focused on the use of 1 Research funded by the Lifelong Learning Programme 2007- 2013 (AUTOLEARN, 2007-3625/001-001). 19 3 GFAI Martin-Luther-Str. 14 Saarbrücken, Germany susannep@iai.uni-sb.de NLP in the context of CALL (Amaral and Meurers, in preparation). Despite this, there is an appalling absence of parser-based CALL in real instruction settings, which has been partially attributed to a certain negligence of the pedagogical needs (Amaral and Meurers, in preparation). In contrast, projects and systems that were pedagogically </context>
<context position="3165" citStr="Levy 1997" startWordPosition="477" endWordPosition="478">pposed to communicative-oriented) or translation exercises, or by providing feedback on formal aspects of language in content oriented activities, always under pedagogical considerations –focus on form. Our proposal is a step forward in this direction in two ways: a) it allows for feedback generation focusing both on formal and content (communicative-oriented) aspects of language learning activities, and b) it provides teachers with a tool and a methodology –both evolving– for them to gain autonomy in the creation of parser-based CALL activities –which by the way has a long tradition in CALL (Levy 1997, chap. 2). The goal is to shape language technologies to the needs of the teachers, and truly ready-to-hand. 1.1 Related work and research context The extent to which pedagogues appreciate and require autonomy in the design and creation of CALL activities can be traced in the historical Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 19–27, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics overview offered by (Levy 1997, 16, 17, 19, 23 and 38). Moreover, parallel research shows that the i</context>
</contexts>
<marker>Levy, 1997</marker>
<rawString>Levy, Michael. 1997. Computer-Assisted Language Learning. Context and Conceptualization. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Nagata</author>
</authors>
<title>BANZAI: An Application of Natural Language Processingto Web based Language Learning.</title>
<date>2002</date>
<journal>CALICO Journal</journal>
<volume>19</volume>
<pages>583--599</pages>
<contexts>
<context position="2274" citStr="Nagata 2002" startWordPosition="339" endWordPosition="340">y the Lifelong Learning Programme 2007- 2013 (AUTOLEARN, 2007-3625/001-001). 19 3 GFAI Martin-Luther-Str. 14 Saarbrücken, Germany susannep@iai.uni-sb.de NLP in the context of CALL (Amaral and Meurers, in preparation). Despite this, there is an appalling absence of parser-based CALL in real instruction settings, which has been partially attributed to a certain negligence of the pedagogical needs (Amaral and Meurers, in preparation). In contrast, projects and systems that were pedagogically informed succeeded, yielded and are yielding interesting results, and are evolving for over a decade now (Nagata 2002; Nagata 2009; Heift 2001; Heift 2003; Heift 2005; Amaral and Meurers, in preparation). According to Amaral and Meurers successful projects were able to restrict learner production in terms of NLP complexity by limiting the scope of the learning activities to languageoriented (as opposed to communicative-oriented) or translation exercises, or by providing feedback on formal aspects of language in content oriented activities, always under pedagogical considerations –focus on form. Our proposal is a step forward in this direction in two ways: a) it allows for feedback generation focusing both on</context>
</contexts>
<marker>Nagata, 2002</marker>
<rawString>Nagata, Noriko. 2002. BANZAI: An Application of Natural Language Processingto Web based Language Learning. CALICO Journal 19, no. 3: 583-599.</rawString>
</citation>
<citation valid="true">
<title>Robo-Sensei’s NLP-Based Error Detection and Feedback Generation.</title>
<date>2009</date>
<journal>CALICO Journal</journal>
<volume>26</volume>
<pages>562--579</pages>
<contexts>
<context position="4325" citStr="(2009)" startWordPosition="658" endWordPosition="658">. Moreover, parallel research shows that the integration of CALL in the learning context is 2 AutoTutor: AutoLearn’s authoring software Figure 1. AutoTutor software architecture. critical to ensure the success of whatever materials are offered to learners (Levy 1997, 200-203; Polisca 2006). AutoTutor goes beyond tools such as Hot Potatoes, eXelearning or JClic2 in that it offers the possibility of authoring NLP-based CALL activities. It is also more ambitious than other authoring tools developed for the creation of activities in intelligent tutoring systems. Chen and Tokuda (2003) and Rösener (2009) present authoring tools for translation exercises, where expected learner input is much more controlled (by the sentence in the source language). Heift and Toole (2002) present Tutor Assistant, which enables to create activities such as build-asentence, drag-and-drop and fill-in-the-blank. An important difference between AutoTutor and Tutor Assistant is that the latter is a bit more restrictive in terms of the linguistic objects that can be used. It also presents a lighter complexity in the modelling of the underlying correction modules. However, the system underlying Tutor Assistant provides</context>
<context position="5949" citStr="(2009)" startWordPosition="891" endWordPosition="891">s includes a simplified specification of the means to automatically create the resources used to analyse learner input for each exercise. The goal is to use computational devices to analyse learner production and to be able to go beyond “yes-or-no” answers providing additional feedback focused both on form and content. This research work is framed within the AutoLearn project, a follow up of the ALLES project (Schmidt et al., 2004, Quixal et al., 2006). AutoLearn’s aim was to exploit in a larger scale a subset of the technologies developed in ALLES in real instruction settings. Estrada et al. (2009) describe how, in AutoLearn’s first evaluation phase, the topics of the activities were not attractive enough for learners and how learner activity decreased within the same learning unit across exercises. Both observations –together with what it has been shown with respect to the integration of independent language learning, see above – impelled us to develop AutoTutor, which allows teachers to create their own learning units. As reflected in Figure 1, AutoTutor consists primarily of two pieces of software: AutoTutor Activity Creation Kit (ATACK) and AutoTutor Activity Player (ATAP). ATACK, a</context>
</contexts>
<marker>2009</marker>
<rawString>———. 2009. Robo-Sensei’s NLP-Based Error Detection and Feedback Generation. CALICO Journal 26, no. 3: 562-579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Polisca</author>
</authors>
<title>Facilitating the Learning Process: An Evaluation of the Use and Benefits of a Virtual Learning Environment (VLE)-enhanced Independent Language-learning Program (ILLP).</title>
<date>2006</date>
<journal>CALICO Journal</journal>
<volume>23</volume>
<pages>499--51</pages>
<contexts>
<context position="4009" citStr="Polisca 2006" startWordPosition="605" endWordPosition="607">eation of CALL activities can be traced in the historical Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 19–27, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics overview offered by (Levy 1997, 16, 17, 19, 23 and 38). Moreover, parallel research shows that the integration of CALL in the learning context is 2 AutoTutor: AutoLearn’s authoring software Figure 1. AutoTutor software architecture. critical to ensure the success of whatever materials are offered to learners (Levy 1997, 200-203; Polisca 2006). AutoTutor goes beyond tools such as Hot Potatoes, eXelearning or JClic2 in that it offers the possibility of authoring NLP-based CALL activities. It is also more ambitious than other authoring tools developed for the creation of activities in intelligent tutoring systems. Chen and Tokuda (2003) and Rösener (2009) present authoring tools for translation exercises, where expected learner input is much more controlled (by the sentence in the source language). Heift and Toole (2002) present Tutor Assistant, which enables to create activities such as build-asentence, drag-and-drop and fill-in-the</context>
</contexts>
<marker>Polisca, 2006</marker>
<rawString>Polisca, Elena. 2006. Facilitating the Learning Process: An Evaluation of the Use and Benefits of a Virtual Learning Environment (VLE)-enhanced Independent Language-learning Program (ILLP). CALICO Journal 23, no.3: 499-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Quixal</author>
<author>T Badia</author>
<author>B Boullosa</author>
<author>L Díaz</author>
<author>A Ruggia</author>
</authors>
<title>Strategies for the Generation of Individualised Feedback in Distance Language Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on LanguageEnabled Technology and Development and Evaluation of Robust Spoken Dialogue Systems of ECAI 2006. Riva del Garda,</booktitle>
<location>Italy,</location>
<contexts>
<context position="5799" citStr="Quixal et al., 2006" startWordPosition="863" endWordPosition="866">tor is a web-based software solution to assist non-NLP experts in the creation of language learning activities using NLP-intensive processing techniques. The process includes a simplified specification of the means to automatically create the resources used to analyse learner input for each exercise. The goal is to use computational devices to analyse learner production and to be able to go beyond “yes-or-no” answers providing additional feedback focused both on form and content. This research work is framed within the AutoLearn project, a follow up of the ALLES project (Schmidt et al., 2004, Quixal et al., 2006). AutoLearn’s aim was to exploit in a larger scale a subset of the technologies developed in ALLES in real instruction settings. Estrada et al. (2009) describe how, in AutoLearn’s first evaluation phase, the topics of the activities were not attractive enough for learners and how learner activity decreased within the same learning unit across exercises. Both observations –together with what it has been shown with respect to the integration of independent language learning, see above – impelled us to develop AutoTutor, which allows teachers to create their own learning units. As reflected in Fi</context>
</contexts>
<marker>Quixal, Badia, Boullosa, Díaz, Ruggia, 2006</marker>
<rawString>Quixal, M., T. Badia, B. Boullosa, L. Díaz, and A. Ruggia. (2006). Strategies for the Generation of Individualised Feedback in Distance Language Learning. In Proceedings of the Workshop on LanguageEnabled Technology and Development and Evaluation of Robust Spoken Dialogue Systems of ECAI 2006. Riva del Garda, Italy, Sept. 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Rösener</author>
</authors>
<title>A linguistic intelligent system for technology enhanced learning in vocational training – the ILLU project”. In</title>
<date>2009</date>
<booktitle>4th European Conference on Technology Enhanced Learning, EC-TEL</booktitle>
<volume>29</volume>
<pages>813</pages>
<publisher>Springer,</publisher>
<location>Nice, France,</location>
<contexts>
<context position="4325" citStr="Rösener (2009)" startWordPosition="657" endWordPosition="658"> and 38). Moreover, parallel research shows that the integration of CALL in the learning context is 2 AutoTutor: AutoLearn’s authoring software Figure 1. AutoTutor software architecture. critical to ensure the success of whatever materials are offered to learners (Levy 1997, 200-203; Polisca 2006). AutoTutor goes beyond tools such as Hot Potatoes, eXelearning or JClic2 in that it offers the possibility of authoring NLP-based CALL activities. It is also more ambitious than other authoring tools developed for the creation of activities in intelligent tutoring systems. Chen and Tokuda (2003) and Rösener (2009) present authoring tools for translation exercises, where expected learner input is much more controlled (by the sentence in the source language). Heift and Toole (2002) present Tutor Assistant, which enables to create activities such as build-asentence, drag-and-drop and fill-in-the-blank. An important difference between AutoTutor and Tutor Assistant is that the latter is a bit more restrictive in terms of the linguistic objects that can be used. It also presents a lighter complexity in the modelling of the underlying correction modules. However, the system underlying Tutor Assistant provides</context>
</contexts>
<marker>Rösener, 2009</marker>
<rawString>Rösener, C.: “A linguistic intelligent system for technology enhanced learning in vocational training – the ILLU project”. In Cress, U.; Dimitrova, V.; Specht, M. (Eds.): Learning in the Synergy of Multiple Disciplines. 4th European Conference on Technology Enhanced Learning, EC-TEL 2009 Nice, France, Sept. 29 – Oct. 2, 2009. Lecture Notes in Computer Science. Programming and Software Engineering, Vol. 5794, 2009, XVIII, p. 813, Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schmidt</author>
<author>S Garnier</author>
<author>M Sharwood</author>
<author>T Badia</author>
<author>L Díaz</author>
<author>M Quixal</author>
<author>A Ruggia</author>
<author>A S Valderrabanos</author>
<author>A J Cruz</author>
<author>E Torrejon</author>
<author>C Rico</author>
<author>J Jimenez</author>
</authors>
<title>ALLES: Integrating NLP in ICALL Applications.</title>
<date>2004</date>
<booktitle>In Proceedings of Fourth International Conference on Language Resources and Evaluation. Lisbon,</booktitle>
<volume>vol. VI</volume>
<pages>1888--1891</pages>
<contexts>
<context position="5777" citStr="Schmidt et al., 2004" startWordPosition="859" endWordPosition="862">clic/index.htm. AutoTutor is a web-based software solution to assist non-NLP experts in the creation of language learning activities using NLP-intensive processing techniques. The process includes a simplified specification of the means to automatically create the resources used to analyse learner input for each exercise. The goal is to use computational devices to analyse learner production and to be able to go beyond “yes-or-no” answers providing additional feedback focused both on form and content. This research work is framed within the AutoLearn project, a follow up of the ALLES project (Schmidt et al., 2004, Quixal et al., 2006). AutoLearn’s aim was to exploit in a larger scale a subset of the technologies developed in ALLES in real instruction settings. Estrada et al. (2009) describe how, in AutoLearn’s first evaluation phase, the topics of the activities were not attractive enough for learners and how learner activity decreased within the same learning unit across exercises. Both observations –together with what it has been shown with respect to the integration of independent language learning, see above – impelled us to develop AutoTutor, which allows teachers to create their own learning uni</context>
</contexts>
<marker>Schmidt, Garnier, Sharwood, Badia, Díaz, Quixal, Ruggia, Valderrabanos, Cruz, Torrejon, Rico, Jimenez, 2004</marker>
<rawString>Schmidt, P., S. Garnier, M. Sharwood, T. Badia, L. Díaz, M. Quixal, A. Ruggia, A. S. Valderrabanos, A. J. Cruz, E. Torrejon, C. Rico, J. Jimenez. (2004) ALLES: Integrating NLP in ICALL Applications. In Proceedings of Fourth International Conference on Language Resources and Evaluation. Lisbon, vol. VI p. 1888-1891. ISBN: 2-9517408-1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Shneiderman</author>
<author>C Plaisant</author>
</authors>
<title>Strategies for evaluating information visualization tools: multidimensional in-depth long-term case studies.</title>
<date>2006</date>
<booktitle>BELIV ’06: Proceedings of the</booktitle>
<marker>Shneiderman, Plaisant, 2006</marker>
<rawString>Shneiderman, B. and C. Plaisant. (2006) Strategies for evaluating information visualization tools: multidimensional in-depth long-term case studies. BELIV ’06: Proceedings of the 2006 AVI workshop on Beyond time and errors: novel evaluation methods for information visualization, May 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Toole</author>
<author>T Heift</author>
</authors>
<title>The Tutor Assistant: An Authoring System for a Web-based Intelligent Language Tutor.</title>
<date>2002</date>
<journal>Computer Assisted Language Learning,</journal>
<volume>15</volume>
<issue>4</issue>
<pages>373--86</pages>
<marker>Toole, Heift, 2002</marker>
<rawString>Toole, J. &amp; Heift, T. (2002). The Tutor Assistant: An Authoring System for a Web-based Intelligent Language Tutor. Computer Assisted Language Learning, 15(4), 373-86.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>