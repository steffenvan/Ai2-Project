<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000300">
<title confidence="0.993805">
Improved Tree-to-string Transducer for Machine Translation
</title>
<author confidence="0.999428">
Ding Liu and Daniel Gildea
</author>
<affiliation confidence="0.9985065">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.337922">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.976684" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885461538462">
We propose three enhancements to the tree-
to-string (TTS) transducer for machine trans-
lation: first-level expansion-based normaliza-
tion for TTS templates, a syntactic align-
ment framework integrating the insertion of
unaligned target words, and subtree-based n-
gram model addressing the tree decomposi-
tion probability. Empirical results show that
these methods improve the performance of a
TTS transducer based on the standard BLEU-
4 metric. We also experiment with semantic
labels in a TTS transducer, and achieve im-
provement over our baseline system.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999382527272728">
Syntax-based statistical machine translation
(SSMT) has achieved significant progress during
recent years, with two threads developing simul-
taneously: the synchronous parsing-based SSMT
(Galley et al., 2006; May and Knight, 2007) and
the tree-to-string (TTS) transducer (Liu et al.,
2006; Huang et al., 2006). Synchronous SSMT
here denotes the systems which accept a source
sentence as the input and generate the translation
and the syntactic structure for both the source and
the translation simultaneously. Such systems are
sometimes also called TTS transducers, but in this
paper, TTS transducer refers to the system which
starts with the syntax tree of a source sentence and
recursively transforms the tree to the target language
based on TTS templates.
In synchronous SSMT, TTS templates are used
similar to the context free grammar used in the stan-
dard CYK parser, thus the syntax is part of the output
and can be thought of as a constraint on the transla-
tion process. In the TTS transducer, since the parse
tree is given, syntax can be thought of as an addi-
tional feature of the input to be used in the transla-
tion. The idea of synchronous SSMT can be traced
back to Wu (1997)’s Stochastic Inversion Transduc-
tion Grammars. A systematic method for extract-
ing TTS templates from parallel corpora was pro-
posed by Galley et al. (2004), and later binarized
by Zhang et al. (2006) for high efficiency and ac-
curacy. In the other track, the TTS transducer orig-
inated from the tree transducer proposed by Rounds
(1970) and Thatcher (1970) independently. Graehl
and Knight (2004) generalized the tree transducer
to the TTS transducer and introduced an EM al-
gorithm to estimate the probability of TTS tem-
plates based on a bilingual corpus with one side
parsed. Liu et al. (2006) and Huang et al. (2006)
then used the TTS transducer on the task of Chinese-
to-English and English-to-Chinese translation, re-
spectively, and achieved decent performance.
Despite the progress SSMT has achieved, it is
still a developing field with many problems un-
solved. For example, the word alignment com-
puted by GIZA++ and used as a basis to extract
the TTS templates in most SSMT systems has been
observed to be a problem for SSMT (DeNero and
Klein, 2007; May and Knight, 2007), due to the
fact that the word-based alignment models are not
aware of the syntactic structure of the sentences and
could produce many syntax-violating word align-
ments. Approaches have been proposed recently to-
wards getting better word alignment and thus bet-
ter TTS templates, such as encoding syntactic struc-
ture information into the HMM-based word align-
ment model DeNero and Klein (2007), and build-
</bodyText>
<page confidence="0.99094">
62
</page>
<note confidence="0.737246">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 62–69,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9983496">
ing a syntax-based word alignment model May
and Knight (2007) with TTS templates. Unfortu-
nately, neither approach reports end-to-end MT per-
formance based on the syntactic alignment. DeN-
ero and Klein (2007) focus on alignment and do not
present MT results, while May and Knight (2007)
takes the syntactic re-alignment as an input to an EM
algorithm where the unaligned target words are in-
serted into the templates and minimum templates are
combined into bigger templates (Galley et al., 2006).
Thus the improvement they reported is rather indi-
rect, leading us to wonder how much improvement
the syntactic alignment model can directly bring to a
SSMT system. Some other issues of SSMT not fully
addressed before are highlighted below:
</bodyText>
<listItem confidence="0.890797580645161">
1. Normalization of TTS templates. Galley et
al. (2006) mentioned that with only the mini-
mum templates extracted from GHKM (Galley
et al., 2004), normalizing the template proba-
bility based on its tree pattern “can become ex-
tremely biased”, due to the fact that bigger tem-
plates easily get high probabilities. They in-
stead use a joint model where the templates are
normalized based on the root of their tree pat-
terns and show empirical results for that. There
is no systematic comparison of different nor-
malization methods.
2. Decomposition model of a TTS transducer
(or syntactic language model in synchronous
SSMT). There is no explicit modeling for the
decomposition of a syntax tree in the TTS
transducer (or the probability of the syntactic
tree in a synchronous SSMT). Most systems
simply use a uniform model (Liu et al., 2006;
Huang et al., 2006) or implicitly consider it
with a joint model producing both syntax trees
and the translations (Galley et al., 2006).
3. Use of semantics. Using semantic features in
a SSMT is a natural step along the way to-
wards generating more refined models across
languages. The statistical approach to semantic
role labeling has been well studied (Xue and
Palmer, 2004; Ward et al., 2004; Toutanova et
al., 2005), but there is no work attempting to
use such information in SSMT, to our limited
knowledge.
</listItem>
<bodyText confidence="0.999861033333334">
This paper proposes novel methods towards solv-
ing these problems. Specifically, we compare three
ways of normalizing the TTS templates based on the
tree pattern, the root of the tree pattern, and the first-
level expansion of the tree pattern respectively, in
the context of hard counting and EM estimation; we
present a syntactic alignment framework integrating
both the template re-estimation and insertion of un-
aligned target words; we use a subtree-based n-gram
model to address the decomposition of the syntax
trees in TTS transducer (or the syntactic language
model for synchronous SSMT); we use a statistical
classifier to label the semantic roles defined by Prop-
Bank (Palmer et al., 2005) and try different ways of
using the semantic features in a TTS transducer.
We chose the TTS transducer instead of syn-
chronous SSMT for two reasons. First, the decoding
algorithm for the TTS transducer has lower compu-
tational complexity, which makes it easier to inte-
grate a complex decomposition model. Second, the
TTS Transducer can be easily integrated with se-
mantic role features since the syntax tree is present,
and it’s not clear how to do this in a synchronous
SSMT system. The remainder of the paper will
focus on introducing the improved TTS transducer
and is organized as follows: Section 2 describes the
implementation of a basic TTS transducer; Section
3 describes the components of the improved TTS
transducer; Section 4 presents the empirical results
and Section 5 gives the conclusion.
</bodyText>
<sectionHeader confidence="0.7270745" genericHeader="method">
2 A Basic Tree-to-string Transducer for
Machine Translation
</sectionHeader>
<bodyText confidence="0.999990142857143">
The TTS transducer, as a generalization to the finite
state transducer, receives a tree structure as its input
and recursively applies TTS templates to generate
the target string. For simplicity, usually only one
state is used in the TTS transducer, i.e., a TTS tem-
plate will always lead to the same outcome wher-
ever it is used. A TTS template is composed of a
left-hand side (LHS) and a right-hand side (RHS),
where LHS is a subtree pattern and RHS is a se-
quence of the variables and translated words. The
variables in the RHS of a template correspond to the
bottom level non-terminals in the LHS’s subtree pat-
tern, and their relative order indicates the permuta-
tion desired at the point where the template is ap-
</bodyText>
<page confidence="0.967735">
63
</page>
<equation confidence="0.899079">
SQ
AUX NP1 RB VP2 ?3
Is not
NP1 没有 VP2 ?3
</equation>
<bodyText confidence="0.995217">
the source syntax trees and the target translations.
In theory, the translation model should sum over
all possible derivations generating the target transla-
tion, but in practice, usually only the best derivation
is considered:
</bodyText>
<equation confidence="0.965941714285714">
Is the job not finished ?
工作 没有 完成 ?
(SQ (AUX is) NP1 (RB not) VP2 ?3) _&gt; NP1 没有 VP2 ?3
(NP (DT the) (NN job)) _&gt; 工作
(VP VBN1) _&gt; VBN1
(VBN finished) _&gt; 完成
(? ?) _&gt; ?
</equation>
<figureCaption confidence="0.999108">
Figure 1: A TTS Template Example
</figureCaption>
<figure confidence="0.97583825">
SQ
AUX NP RB VP ?
DT NN
VBN
</figure>
<bodyText confidence="0.997345090909091">
Here, S denotes the target translation, T denotes the
source syntax tree, and D* denotes the best deriva-
tion of T. The implementation of a TTS trans-
ducer can be done either top down with memoiza-
tion to the visited subtrees (Huang et al., 2006), or
with a bottom-up dynamic programming (DP) algo-
rithm (Liu et al., 2006). This paper uses the lat-
ter approach, and the algorithm is sketched in Fig-
ure 3. For the baseline approach, only the translation
model and n-gram model for the target language are
used:
</bodyText>
<equation confidence="0.897467">
Pr(S|T, D*) = � Weight(t)
tED*
</equation>
<figureCaption confidence="0.9510025">
Figure 2: Derivation Example S* = argmax Pr(T|S) = argmax Pr(S)Pr(S|T)
S S
</figureCaption>
<bodyText confidence="0.999848925925926">
plied to translate one language to another. The vari-
ables are further transformed and the recursive pro-
cess goes on until there are no variables left. The
formal description of a TTS transducer is described
in Graehl and Knight (2004), and our baseline ap-
proach follows the Extended Tree-to-String Trans-
ducer defined in (Huang et al., 2006). Figure 1 gives
an example of the English-to-Chinese TTS template,
which shows how to translate a skeleton YES/NO
question from English to Chinese. NP1 and V P2
are the variables whose relative position in the trans-
lation are determined by the template while their ac-
tual translations are still unknown and dependent on
the subtrees rooted at them; and the English words Is
and not are translated into the Chinese word MeiYou
in the context of the template. The superscripts at-
tached on the variables are used to distinguish the
non-terminals with identical names (if there is any).
Figure 2 shows the steps of transforming the English
sentence “Is the job not finished ?” to the corre-
sponding Chinese.
For a given derivation (decomposition) of a syn-
tax tree, the translation probability is computed as
the product of the templates which generate both
Since the n-gram model tends to favor short transla-
tions, a penalty is added to the translation templates
with fewer RHS symbols than LHS leaf symbols:
</bodyText>
<equation confidence="0.992371">
Penalty(t) = exp(|t.RHS |− |t.LHSLeaf|)
</equation>
<bodyText confidence="0.9999165">
where |t.RHS |denotes the number of symbols in
the RHS of t, and |t.LHSLeaf |denotes the num-
ber of leaves in the LHS of t. The length penalty is
analogous to the length feature widely used in log-
linear models for MT (Huang et al., 2006; Liu et al.,
2006; Och and Ney, 2004). Here we distribute the
penalty into TTS templates for the convenience of
DP, so that we don’t have to generate the N-best list
and do re-ranking. To speed up the decoding, stan-
dard beam search is used.
In Figure 3, BinaryCombine denotes the target-
size binarization (Huang et al., 2006) combination.
The translation candidates of the template’s vari-
ables, as well as its terminals, are combined pair-
wise in the order they appear in the RHS of the
template. fz denotes a combined translation, whose
probability is equal to the product of the probabili-
ties of the component translations, the probability of
the rule, the n-gram probability of connecting the
component translations, and the length penalty of
</bodyText>
<page confidence="0.997033">
64
</page>
<bodyText confidence="0.9108822">
Match(v, t): the descendant tree nodes of v, which match the variables in template t
v.sk: the stack associated with tree node v
In(cj, fi): the translation candidate of cj which is chosen to combine fi
for all tree node v in bottom-up order do
for all template t applicable at v do
</bodyText>
<equation confidence="0.941056666666667">
{c1, c2, ..., cl}=Match(v, t);
{f1, f2,..., f,..} = BinaryCombine(c1.sk, c2.sk, ..., cn.sk, t);
for i=1:m do
Pr(fi) = Hlj_1Pr(In(cj, fi)) · Weight(t)p· Lang(v, t, fi)ry· Penalty(t)α;
Add (fi, Pr(fi)) to v.sk;
Prune v.sk;
</equation>
<figureCaption confidence="0.997928">
Figure 3: Decoding Algorithm
</figureCaption>
<bodyText confidence="0.999951">
the template. α, Q and -y are the weights of the length
penalty, the translation model, and the n-gram lan-
guage model, respectively. Each state in the DP
chart denotes the best translation of a tree node with
a certain prefix and suffix. The length of the pre-
fix and the suffix is equal to the length of the n-gram
model minus one. Without the beam pruning, the de-
coding algorithm runs in O(N4(n−1)RPQ), where
N is the vocabulary size of the target language, n is
the length of the n-gram model, R is the maximum
number of templates applicable to one tree node, P
is the maximum number of variables in a template,
and Q is the number of tree nodes in the syntax tree.
The DP algorithm works for most systems in the pa-
per, and only needs to be slightly modified to en-
code the subtree-based n-gram model described in
Section 3.3.
</bodyText>
<sectionHeader confidence="0.9832835" genericHeader="method">
3 Improved Tree-to-string Transducer for
Machine Translation
</sectionHeader>
<subsectionHeader confidence="0.99992">
3.1 Normalization of TTS Templates
</subsectionHeader>
<bodyText confidence="0.99997975">
Given the story that translations are generated based
on the source syntax trees, the weight of the template
is computed as the probability of the target strings
given the source subtree:
</bodyText>
<equation confidence="0.996995">
#(t)
Weight(t) = #(t0 : LHS(t0) = LHS(t))
</equation>
<bodyText confidence="0.989543714285714">
Such normalization, denoted here as TREE, is used
in most tree-to-string template-based MT systems
(Liu et al., 2007; Liu et al., 2006; Huang et al.,
2006). Galley et al. (2006) proposed an alteration
in synchronous SSMT which addresses the proba-
bility of both the source subtree and the target string
given the root of the source subtree:
</bodyText>
<equation confidence="0.997273833333333">
#(t)
Weight(t) = #(t0 : root(t0) = root(t))
This method is denoted as ROOT. Here, we propose
another modification:
#(t)
Weight(t) = #(t0 : cfg(t0) = cfg(t)) (1)
</equation>
<bodyText confidence="0.999863470588235">
cfg in Equation 1 denotes the first level expansion
of the source subtree and the method is denoted as
CFG. CFG can be thought of as generating both the
source subtree and the target string given the first
level expansion of the source subtree. TREE focuses
on the conditional probability of the target string
given the source subtree, ROOT focuses on the joint
probability of both the source subtree and the target
string, while CFG, as something of a compromise
between TREE and ROOT, hopefully can achieve a
combined effect of both of them. Compared with
TREE, CFG favors the one-level context-free gram-
mar like templates and gives penalty to the templates
bigger (in terms of the depth of the source subtree)
than that. It makes sense considering that the big
templates, due to their sparseness in the corpus, are
often assigned unduly large probabilities by TREE.
</bodyText>
<subsectionHeader confidence="0.994501">
3.2 Syntactic Word Alignment
</subsectionHeader>
<bodyText confidence="0.999915285714286">
The idea of building a syntax-based word alignment
model has been explored by May and Knight (2007),
with an algorithm working from the root tree node
down to the leaves, recursively replacing the vari-
ables in the matched tree-to-string templates until
there are no such variables left. The TTS tem-
plates they use are initially gathered using GHKM
</bodyText>
<page confidence="0.995069">
65
</page>
<listItem confidence="0.687055291666667">
1. Run GIZA++ to get the initial word alignment, use
GHKM to gather translation templates, and com-
pute the initial probability as their normalized fre-
quency.
2. Collect all the one-level subtrees in the training cor-
pus containing only non-terminals and create TTS
templates addressing all the permutations of the
subtrees’ leaves if its spanning factor is not greater
than four, or only the monotonic translation tem-
plate if its spanning factor is greater than four. Col-
lect all the terminal rules in the form of A → B
where A is one source word, B is the consecutive
target word sequence up to three words long, and
A, B occurs in some sentence pairs. These extra
templates are assigned a small probability 10−6.
3. Run the EM algorithm described in (Graehl and
Knight, 2004) with templates obtained in step 1 and
step 2 to re-estimate their probabilities.
4. Use the templates from step 3 to compute the viterbi
word alignment.
5. The templates not occurring in the viterbi deriva-
tion are ignored and the probability of the remain-
ing ones are re-normalized based on their frequency
in the viterbi derivation.
</listItem>
<figureCaption confidence="0.999009">
Figure 4: Steps generating the refined TTS templates
</figureCaption>
<bodyText confidence="0.995262181818182">
(Galley et al., 2004) with the word alignment com-
puted by GIZA++ and re-estimated using EM, ig-
noring the alignment from Giza++. The refined
word alignment is then fed to the expanded GHKM
(Galley et al., 2006), where the TTS templates will
be combined with the unaligned target words and
re-estimated in another EM framework. The syn-
tactic alignment proposed here shares the essence of
May and Knight’s approach, but combines the re-
estimation of the TTS templates and insertion of the
unaligned target words into a single EM framework.
The process is described in Figure 4. The inser-
tion of the unaligned target words is done implicitly
as we include the extra terminal templates in Fig-
ure 4, and the extra non-terminal templates ensure
that we can get a complete derivation forest in the
EM training. The last viterbi alignment step may
seem unnecessary given that we already have the
EM-estimated templates, but in experiments we find
that it produces better result by cutting off the noisy
(usually very big) templates resulting from the poor
alignments of GIZA++.
</bodyText>
<subsectionHeader confidence="0.983059">
3.3 Tree Decomposition Model
</subsectionHeader>
<bodyText confidence="0.999951">
A deficiency of the translation model for tree-to-
string transducer is that it cannot fully address
the decomposition probability of the source syntax
trees. Though we can say that ROOT/CFG implic-
itly includes the decomposition model, a more di-
rect and explicit modeling of the decomposition is
still desired. Here we propose a novel n-gram-like
model to solve this problem. The probability of a
decomposition (derivation) of a syntax tree is com-
puted as the product of the n-gram probability of
the decomposed subtrees conditioned on their ascen-
dant subtrees. The formal description of the model
is in Equation 2, where D denotes the derivation and
PT(st) denotes the direct parent subtree of st.
</bodyText>
<equation confidence="0.995878666666667">
Pr(D|T) = � Pr(st|PT(st), PT(PT(st)), ...)
subtrees
stED
</equation>
<bodyText confidence="0.9400645">
(2)
Now, with the decomposition model added in, the
probability of the target string given the source syn-
tax tree is computed as:
</bodyText>
<equation confidence="0.998004">
Pr(S|T) = Pr(D*|T) x Pr(S|T,D*)
</equation>
<bodyText confidence="0.999966772727273">
To encode this n-gram probability of the subtrees
in the decoding process, we need to expand the
state space of the dynamic programming algorithm
in Figure 3, so that each state represents not only
the prefix/suffix of the partial translation, but also
the decomposition history of a tree node. For ex-
ample, with a bigram tree model, the states should
include the different subtrees in the LHS of the tem-
plates used to translate a tree node. With bigger n-
grams, more complex history information should be
encoded in the states, and this leads to higher com-
putational complexity. In this paper, we only con-
sider the tree n-gram up to size 2. It is not practi-
cal to search the full state space; instead, we mod-
ify the beam search algorithm in Figure 3 to encode
the decomposition history information. The mod-
ified algorithm for the tree bigram creates a stack
for each tree pattern occurring in the templates ap-
plicable to a tree node. This ensures that for each
tree node, the decompositions headed with differ-
ent subtrees have equal number of translation can-
didates surviving to the upper phase. The function
</bodyText>
<page confidence="0.94982">
66
</page>
<figureCaption confidence="0.9911465">
Figure 5: Flow graph of the system with all components
integrated
</figureCaption>
<bodyText confidence="0.999873363636364">
BinaryCombine is almost the same as in Figure 3,
except that the translation candidates (states) of each
tree node are grouped according to their associated
subtrees. The bigram probabilities of the subtrees
can be easily computed with the viterbi derivation in
last subsection. Also, a weight should be assigned
to this component. This tree n-gram model can be
easily adapted and used in synchronous SSMT sys-
tems such as May and Knight (2007), Galley et al.
(2006). The flow graph of the final system with all
the components integrated is shown in Figure 5.
</bodyText>
<subsectionHeader confidence="0.999158">
3.4 Use of Semantic Roles
</subsectionHeader>
<bodyText confidence="0.9999645">
Statistical approaches to MT have gone through
word-based systems, phrase-based systems, and
syntax-based systems. The next generation would
seem to be semantic-based systems. We use Prop-
Bank (Palmer et al., 2005) as the semantic driver in
our TTS transducer because it is built upon the same
corpus (the Penn Treebank) used to train the statisti-
cal parser, and its shallow semantic roles are more
easily integrated into a TTS transducer. A Max-
Entropy classifier, with features following Xue and
Palmer (2004) and Ward et al. (2004), is used to gen-
erate the semantic roles for each verb in the syntax
trees. We then replace the syntactic labels with the
semantic roles so that we have more general tree la-
bels, or combine the semantic roles with the syntac-
tic labels to generate more refined tree node labels.
Though semantic roles are associated with the verbs,
it is not feasible to differentiate the roles of different
</bodyText>
<table confidence="0.475416">
NP VP VP NP
(S NP-agent VP)
(S NP-patient VP)
</table>
<tableCaption confidence="0.9879435">
Table 1: The TREE-based weights of the skeleton tem-
plates with NP in different roles
</tableCaption>
<bodyText confidence="0.99982475">
verbs due to the data sparseness problem. If some
tree nodes are labeled different roles for different
verbs, those semantic roles will be ignored.
A simple example demonstrating the need for se-
mantics in the TTS transducer is that in English-
Chinese translation, the NP VP skeleton phrase is
more likely to be inverted when NP is in a patient
role than when it is in an agent role. Table 1 shows
the TREE-based weights of the 4 translation tem-
plates, computed based on our training corpus. This
shows that the difference caused by the roles of NP
is significant.
</bodyText>
<sectionHeader confidence="0.997623" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<bodyText confidence="0.99999575">
We used 74,597 pairs of English and Chinese sen-
tences in the FBIS data set as our experimental
data, which are further divided into 500 test sen-
tence pairs, 500 development sentence pairs and
73597 training sentence pairs. The test set and de-
velopment set are selected as those sentences hav-
ing fewer than 25 words on the Chinese side. The
translation is from English to Chinese, and Char-
niak (2000)’s parser, trained on the Penn Treebank,
is used to generate the syntax trees for the English
side. The weights of the MT components are op-
timized based on the development set using a grid-
based line search. The Chinese sentence from the se-
lected pair is used as the single reference to tune and
evaluate the MT system with word-based BLEU-4
(Papineni et al., 2002). Huang et al. (2006) used
character-based BLEU as a way of normalizing in-
consistent Chinese word segmentation, but we avoid
this problem as the training, development, and test
data are from the same source.
</bodyText>
<subsectionHeader confidence="0.99796">
4.1 Syntax-Based System
</subsectionHeader>
<bodyText confidence="0.998565">
The decoding algorithm described in Figure 3 is
used with the different normalization methods de-
scribed in Section 3.1 and the results are summa-
rized in Table 2. The TTS templates are extracted
using GHKM based on the many-to-one alignment
</bodyText>
<figure confidence="0.995952136363636">
Parsed
Parallel
Corpus
Giza++
GHKM
Templates
Extra Templates
EM
viterbi
Templates
Sub-tree
Bigram
SRILM
decoder
decompositon model
translation model
language model
Normalize
0.983
0.857
0.017
0.143
</figure>
<page confidence="0.992012">
67
</page>
<table confidence="0.999222666666667">
Baseline Syntactic Alignment Subtree bigram
dev test dev test dev test
TREE 12.29 8.90 13.25 9.65 14.84 10.61
ROOT 12.41 9.66 13.72 10.16 14.24 10.66
CFG 13.27 9.69 14.32 10.29 15.30 10.99
PHARAOH 9.04 7.84
</table>
<tableCaption confidence="0.985369">
Table 2: BLEU-4 scores of various systems with the syntactic alignment and subtree bigram improvements added
incrementally.
</tableCaption>
<bodyText confidence="0.999897953488372">
from Chinese to English obtained from GIZA++.
We have tried using alignment in the reverse direc-
tion and the union of both directions, but neither
of them is better than the Chinese-to-English align-
ment. The reason, based on the empirical result,
is simply that the Chinese-to-English alignments
lead to the maximum number of templates using
GHKM. A modified Kneser-Ney bigram model of
the Chinese sentence is trained using SRILM (Stol-
cke, 2002) using the training set. For comparison,
results for Pharaoh (Koehn, 2004), trained and tuned
under the same condition, are also shown in Table 2.
The phrases used in Pharaoh are extracted as the pair
of longest continuous spans in English and Chinese
based on the union of the alignments in both direc-
tion. We tried using alignments of different direc-
tions with Pharaoh, and find that the union gives
the maximum number of phrase pairs and the best
BLEU scores. The results show that the TTS trans-
ducers all outperform Pharaoh, and among them, the
one with CFG normalization works better than the
other two.
We tried the three normalization methods in the
syntactic alignment process in Figure 4, and found
that the initialization (step 1) and viterbi alignment
(step 3 and 4) based on the least biased model
ROOT gave the best performance. Table 2 shows
the results with the final template probability re-
normalized (step 5) using TREE, ROOT and CFG
respectively. We can see that the syntactic align-
ment brings a reasonable improvement for the TTS
transducer no matter what normalization method is
used. To test the effect of the subtree-based n-
gram model, SRILM is used to compute a modi-
fied Kneser-Ney bigram model for the subtree pat-
terns used in the viterbi alignment. The last 3 lines
in Table 2 show the improved results by further in-
corporating the subtree-based bigram model. We
can see that the difference of the three normaliza-
tion methods is lessened and TREE, the weakest nor-
malization in terms of addressing the decomposition
probability, gets the biggest improvement with the
subtree-based bigram model added in.
</bodyText>
<subsectionHeader confidence="0.971912">
4.2 Semantic-Based System
</subsectionHeader>
<bodyText confidence="0.999938136363636">
Following the standard division, our max-entropy
based SRL classifier is trained and tuned using sec-
tions 2-21 and section 24 of PropBank, respectively.
The F-score we achieved on section 23 is 88.70%.
We repeated the experiments in last section with
the semantic labels generated by the SRL classi-
fier. Table 3 shows the results, comparing the non-
semantic-based systems with similar systems us-
ing the refined and general semantic labels, respec-
tively. Unfortunately, semantic based systems do
not always outperform the syntactic based systems.
We can see that for the baseline systems based on
TREE and ROOT, semantic labels improve the re-
sults, while for the other systems, they are not re-
ally better than the syntactic labels. Our approach
to semantic roles is preliminary; possible improve-
ments include associating role labels with verbs and
backing off to the syntactic-label based models from
semantic-label based TTS templates. In light of our
results, we are optimistic that more sophisticated
use of semantic features can further improve a TTS
transducer’s performance.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999932285714286">
This paper first proposes three enhancements to the
TTS transducer: first-level expansion-based normal-
ization for TTS templates, a syntactic alignment
framework integrating the insertion of unaligned tar-
get words, and a subtree-based n-gram model ad-
dressing the tree decomposition probability. The ex-
periments show that the first-level expansion-based
</bodyText>
<page confidence="0.997468">
68
</page>
<table confidence="0.998278333333333">
No Semantic Labels Refined Labels General Labels
Baseline Syntactic Subtree Syntactic Subtree Syntactic Subtree
Alignment Bigram Baseline Alignment Bigram Baseline Alignment Bigram
TREE 8.90 9.65 10.61 9.40 10.25 10.42 9.40 10.02 10.47
ROOT 9.66 10.16 10.66 9.89 10.32 10.43 9.82 10.17 10.42
CFG 9.69 10.29 10.99 9.66 10.16 10.33 9.58 10.25 10.59
</table>
<tableCaption confidence="0.9983185">
Table 3: BLEU-4 scores of semantic-based systems on test data. As in Table 2, the syntactic alignment and subtree
bigram improvements are added incrementally within each condition.
</tableCaption>
<bodyText confidence="0.999697888888889">
normalization for TTS templates is better than the
root-based one and the tree-based one; the syntactic
alignment framework and the n-gram based tree de-
composition model both improve a TTS transducer’s
performance. Our experiments using PropBank se-
mantic roles in the TTS transducer show that the ap-
proach has potential, improving on our baseline sys-
tem. However, adding semantic roles does not im-
prove our best TTS system.
</bodyText>
<sectionHeader confidence="0.999445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936514285715">
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 132–139.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL-07, pages 17–24.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of NAACL-04.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL-06, pages 961–968, July.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of NAACL-04.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In The Sixth Conference of the Association for
Machine Translation in the Americas, pages 115–124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING/ACL-06, Sydney,
Australia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of ACL-07, Prague.
J. May and K. Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of
EMNLP.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings ofACL-
02.
William C. Rounds. 1970. Mappings and grammars on
trees. Mathematical Systems Theory, 4(3):257–287.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, volume 2, pages 901–904.
J. W. Thatcher. 1970. Generalized sequential machine
maps. J. Comput. System Sci., 4:339–367.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL-05, pages 589–596.
Wayne Ward, Kadri Hacioglu, James Martin, , and Dan
Jurafsky. 2004. Shallow semantic parsing using sup-
port vector machines. In Proceedings of EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings of
EMNLP.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of NAACL-06, pages 256–
263.
</reference>
<page confidence="0.999313">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929316">
<title confidence="0.996889">Improved Tree-to-string Transducer for Machine Translation</title>
<author confidence="0.969708">Liu</author>
<affiliation confidence="0.9998185">Department of Computer University of</affiliation>
<address confidence="0.999061">Rochester, NY 14627</address>
<abstract confidence="0.997032571428571">We propose three enhancements to the treeto-string (TTS) transducer for machine translation: first-level expansion-based normalization for TTS templates, a syntactic alignment framework integrating the insertion of target words, and subtree-based gram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU- 4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In The Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="21787" citStr="Charniak (2000)" startWordPosition="3689" endWordPosition="3691"> is in an agent role. Table 1 shows the TREE-based weights of the 4 translation templates, computed based on our training corpus. This shows that the difference caused by the roles of NP is significant. 4 Experiment We used 74,597 pairs of English and Chinese sentences in the FBIS data set as our experimental data, which are further divided into 500 test sentence pairs, 500 development sentence pairs and 73597 training sentence pairs. The test set and development set are selected as those sentences having fewer than 25 words on the Chinese side. The translation is from English to Chinese, and Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the syntax trees for the English side. The weights of the MT components are optimized based on the development set using a gridbased line search. The Chinese sentence from the selected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. 4.1 Syntax-Based System The </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In The Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="2965" citStr="DeNero and Klein, 2007" startWordPosition="475" endWordPosition="478">to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved. For example, the word alignment computed by GIZA++ and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT (DeNero and Klein, 2007; May and Knight, 2007), due to the fact that the word-based alignment models are not aware of the syntactic structure of the sentences and could produce many syntax-violating word alignments. Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and build62 Proceedings of the Third Workshop on Statistical Machine Translation, pages 62–69, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ing a syn</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In Proceedings of ACL-07, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-04.</booktitle>
<contexts>
<context position="2071" citStr="Galley et al. (2004)" startWordPosition="323" endWordPosition="326"> target language based on TTS templates. In synchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax is part of the output and can be thought of as a constraint on the translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent perfor</context>
<context position="4433" citStr="Galley et al., 2004" startWordPosition="708" endWordPosition="711">hile May and Knight (2007) takes the syntactic re-alignment as an input to an EM algorithm where the unaligned target words are inserted into the templates and minimum templates are combined into bigger templates (Galley et al., 2006). Thus the improvement they reported is rather indirect, leading us to wonder how much improvement the syntactic alignment model can directly bring to a SSMT system. Some other issues of SSMT not fully addressed before are highlighted below: 1. Normalization of TTS templates. Galley et al. (2006) mentioned that with only the minimum templates extracted from GHKM (Galley et al., 2004), normalizing the template probability based on its tree pattern “can become extremely biased”, due to the fact that bigger templates easily get high probabilities. They instead use a joint model where the templates are normalized based on the root of their tree patterns and show empirical results for that. There is no systematic comparison of different normalization methods. 2. Decomposition model of a TTS transducer (or syntactic language model in synchronous SSMT). There is no explicit modeling for the decomposition of a syntax tree in the TTS transducer (or the probability of the syntactic</context>
<context position="16004" citStr="Galley et al., 2004" startWordPosition="2701" endWordPosition="2704">ve target word sequence up to three words long, and A, B occurs in some sentence pairs. These extra templates are assigned a small probability 10−6. 3. Run the EM algorithm described in (Graehl and Knight, 2004) with templates obtained in step 1 and step 2 to re-estimate their probabilities. 4. Use the templates from step 3 to compute the viterbi word alignment. 5. The templates not occurring in the viterbi derivation are ignored and the probability of the remaining ones are re-normalized based on their frequency in the viterbi derivation. Figure 4: Steps generating the refined TTS templates (Galley et al., 2004) with the word alignment computed by GIZA++ and re-estimated using EM, ignoring the alignment from Giza++. The refined word alignment is then fed to the expanded GHKM (Galley et al., 2006), where the TTS templates will be combined with the unaligned target words and re-estimated in another EM framework. The syntactic alignment proposed here shares the essence of May and Knight’s approach, but combines the reestimation of the TTS templates and insertion of the unaligned target words into a single EM framework. The process is described in Figure 4. The insertion of the unaligned target words is </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of NAACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-06,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="940" citStr="Galley et al., 2006" startWordPosition="128" endWordPosition="131">s, a syntactic alignment framework integrating the insertion of unaligned target words, and subtree-based ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years, with two threads developing simultaneously: the synchronous parsing-based SSMT (Galley et al., 2006; May and Knight, 2007) and the tree-to-string (TTS) transducer (Liu et al., 2006; Huang et al., 2006). Synchronous SSMT here denotes the systems which accept a source sentence as the input and generate the translation and the syntactic structure for both the source and the translation simultaneously. Such systems are sometimes also called TTS transducers, but in this paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates. In synchronous SSMT, TTS templates are used simi</context>
<context position="4047" citStr="Galley et al., 2006" startWordPosition="645" endWordPosition="648">n Statistical Machine Translation, pages 62–69, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ing a syntax-based word alignment model May and Knight (2007) with TTS templates. Unfortunately, neither approach reports end-to-end MT performance based on the syntactic alignment. DeNero and Klein (2007) focus on alignment and do not present MT results, while May and Knight (2007) takes the syntactic re-alignment as an input to an EM algorithm where the unaligned target words are inserted into the templates and minimum templates are combined into bigger templates (Galley et al., 2006). Thus the improvement they reported is rather indirect, leading us to wonder how much improvement the syntactic alignment model can directly bring to a SSMT system. Some other issues of SSMT not fully addressed before are highlighted below: 1. Normalization of TTS templates. Galley et al. (2006) mentioned that with only the minimum templates extracted from GHKM (Galley et al., 2004), normalizing the template probability based on its tree pattern “can become extremely biased”, due to the fact that bigger templates easily get high probabilities. They instead use a joint model where the template</context>
<context position="13246" citStr="Galley et al. (2006)" startWordPosition="2233" endWordPosition="2236">the paper, and only needs to be slightly modified to encode the subtree-based n-gram model described in Section 3.3. 3 Improved Tree-to-string Transducer for Machine Translation 3.1 Normalization of TTS Templates Given the story that translations are generated based on the source syntax trees, the weight of the template is computed as the probability of the target strings given the source subtree: #(t) Weight(t) = #(t0 : LHS(t0) = LHS(t)) Such normalization, denoted here as TREE, is used in most tree-to-string template-based MT systems (Liu et al., 2007; Liu et al., 2006; Huang et al., 2006). Galley et al. (2006) proposed an alteration in synchronous SSMT which addresses the probability of both the source subtree and the target string given the root of the source subtree: #(t) Weight(t) = #(t0 : root(t0) = root(t)) This method is denoted as ROOT. Here, we propose another modification: #(t) Weight(t) = #(t0 : cfg(t0) = cfg(t)) (1) cfg in Equation 1 denotes the first level expansion of the source subtree and the method is denoted as CFG. CFG can be thought of as generating both the source subtree and the target string given the first level expansion of the source subtree. TREE focuses on the conditional</context>
<context position="16192" citStr="Galley et al., 2006" startWordPosition="2734" endWordPosition="2737">raehl and Knight, 2004) with templates obtained in step 1 and step 2 to re-estimate their probabilities. 4. Use the templates from step 3 to compute the viterbi word alignment. 5. The templates not occurring in the viterbi derivation are ignored and the probability of the remaining ones are re-normalized based on their frequency in the viterbi derivation. Figure 4: Steps generating the refined TTS templates (Galley et al., 2004) with the word alignment computed by GIZA++ and re-estimated using EM, ignoring the alignment from Giza++. The refined word alignment is then fed to the expanded GHKM (Galley et al., 2006), where the TTS templates will be combined with the unaligned target words and re-estimated in another EM framework. The syntactic alignment proposed here shares the essence of May and Knight’s approach, but combines the reestimation of the TTS templates and insertion of the unaligned target words into a single EM framework. The process is described in Figure 4. The insertion of the unaligned target words is done implicitly as we include the extra terminal templates in Figure 4, and the extra non-terminal templates ensure that we can get a complete derivation forest in the EM training. The las</context>
<context position="19639" citStr="Galley et al. (2006)" startWordPosition="3313" endWordPosition="3316">ave equal number of translation candidates surviving to the upper phase. The function 66 Figure 5: Flow graph of the system with all components integrated BinaryCombine is almost the same as in Figure 3, except that the translation candidates (states) of each tree node are grouped according to their associated subtrees. The bigram probabilities of the subtrees can be easily computed with the viterbi derivation in last subsection. Also, a weight should be assigned to this component. This tree n-gram model can be easily adapted and used in synchronous SSMT systems such as May and Knight (2007), Galley et al. (2006). The flow graph of the final system with all the components integrated is shown in Figure 5. 3.4 Use of Semantic Roles Statistical approaches to MT have gone through word-based systems, phrase-based systems, and syntax-based systems. The next generation would seem to be semantic-based systems. We use PropBank (Palmer et al., 2005) as the semantic driver in our TTS transducer because it is built upon the same corpus (the Penn Treebank) used to train the statistical parser, and its shallow semantic roles are more easily integrated into a TTS transducer. A MaxEntropy classifier, with features fo</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING/ACL-06, pages 961–968, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-04.</booktitle>
<contexts>
<context position="2310" citStr="Graehl and Knight (2004)" startWordPosition="362" endWordPosition="365">e translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved. For example, the word alignment computed by GIZA++ and used as a basis to extract the TTS templates in most SSMT systems has been ob</context>
<context position="9248" citStr="Graehl and Knight (2004)" startWordPosition="1543" endWordPosition="1546">s (Huang et al., 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al., 2006). This paper uses the latter approach, and the algorithm is sketched in Figure 3. For the baseline approach, only the translation model and n-gram model for the target language are used: Pr(S|T, D*) = � Weight(t) tED* Figure 2: Derivation Example S* = argmax Pr(T|S) = argmax Pr(S)Pr(S|T) S S plied to translate one language to another. The variables are further transformed and the recursive process goes on until there are no variables left. The formal description of a TTS transducer is described in Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al., 2006). Figure 1 gives an example of the English-to-Chinese TTS template, which shows how to translate a skeleton YES/NO question from English to Chinese. NP1 and V P2 are the variables whose relative position in the translation are determined by the template while their actual translations are still unknown and dependent on the subtrees rooted at them; and the English words Is and not are translated into the Chinese word MeiYou in the context of the template. The superscripts attached on the va</context>
<context position="15595" citStr="Graehl and Knight, 2004" startWordPosition="2633" endWordPosition="2636">ency. 2. Collect all the one-level subtrees in the training corpus containing only non-terminals and create TTS templates addressing all the permutations of the subtrees’ leaves if its spanning factor is not greater than four, or only the monotonic translation template if its spanning factor is greater than four. Collect all the terminal rules in the form of A → B where A is one source word, B is the consecutive target word sequence up to three words long, and A, B occurs in some sentence pairs. These extra templates are assigned a small probability 10−6. 3. Run the EM algorithm described in (Graehl and Knight, 2004) with templates obtained in step 1 and step 2 to re-estimate their probabilities. 4. Use the templates from step 3 to compute the viterbi word alignment. 5. The templates not occurring in the viterbi derivation are ignored and the probability of the remaining ones are re-normalized based on their frequency in the viterbi derivation. Figure 4: Steps generating the refined TTS templates (Galley et al., 2004) with the word alignment computed by GIZA++ and re-estimated using EM, ignoring the alignment from Giza++. The refined word alignment is then fed to the expanded GHKM (Galley et al., 2006), w</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proceedings of NAACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1042" citStr="Huang et al., 2006" startWordPosition="145" endWordPosition="148">sed ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years, with two threads developing simultaneously: the synchronous parsing-based SSMT (Galley et al., 2006; May and Knight, 2007) and the tree-to-string (TTS) transducer (Liu et al., 2006; Huang et al., 2006). Synchronous SSMT here denotes the systems which accept a source sentence as the input and generate the translation and the syntactic structure for both the source and the translation simultaneously. Such systems are sometimes also called TTS transducers, but in this paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates. In synchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax is part of the output</context>
<context position="2532" citStr="Huang et al. (2006)" startWordPosition="402" endWordPosition="405">997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved. For example, the word alignment computed by GIZA++ and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT (DeNero and Klein, 2007; May and Knight, 2007), due to the fact that the word-based alignment models are not aware of the syntactic structure of the sentences and could produce many syntax-v</context>
<context position="5141" citStr="Huang et al., 2006" startWordPosition="828" endWordPosition="831">ased”, due to the fact that bigger templates easily get high probabilities. They instead use a joint model where the templates are normalized based on the root of their tree patterns and show empirical results for that. There is no systematic comparison of different normalization methods. 2. Decomposition model of a TTS transducer (or syntactic language model in synchronous SSMT). There is no explicit modeling for the decomposition of a syntax tree in the TTS transducer (or the probability of the syntactic tree in a synchronous SSMT). Most systems simply use a uniform model (Liu et al., 2006; Huang et al., 2006) or implicitly consider it with a joint model producing both syntax trees and the translations (Galley et al., 2006). 3. Use of semantics. Using semantic features in a SSMT is a natural step along the way towards generating more refined models across languages. The statistical approach to semantic role labeling has been well studied (Xue and Palmer, 2004; Ward et al., 2004; Toutanova et al., 2005), but there is no work attempting to use such information in SSMT, to our limited knowledge. This paper proposes novel methods towards solving these problems. Specifically, we compare three ways of no</context>
<context position="8646" citStr="Huang et al., 2006" startWordPosition="1438" endWordPosition="1441"> model should sum over all possible derivations generating the target translation, but in practice, usually only the best derivation is considered: Is the job not finished ? 工作 没有 完成 ? (SQ (AUX is) NP1 (RB not) VP2 ?3) _&gt; NP1 没有 VP2 ?3 (NP (DT the) (NN job)) _&gt; 工作 (VP VBN1) _&gt; VBN1 (VBN finished) _&gt; 完成 (? ?) _&gt; ? Figure 1: A TTS Template Example SQ AUX NP RB VP ? DT NN VBN Here, S denotes the target translation, T denotes the source syntax tree, and D* denotes the best derivation of T. The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al., 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al., 2006). This paper uses the latter approach, and the algorithm is sketched in Figure 3. For the baseline approach, only the translation model and n-gram model for the target language are used: Pr(S|T, D*) = � Weight(t) tED* Figure 2: Derivation Example S* = argmax Pr(T|S) = argmax Pr(S)Pr(S|T) S S plied to translate one language to another. The variables are further transformed and the recursive process goes on until there are no variables left. The formal description of a TTS transducer is described in Graehl and Knight (200</context>
<context position="10634" citStr="Huang et al., 2006" startWordPosition="1780" endWordPosition="1783">finished ?” to the corresponding Chinese. For a given derivation (decomposition) of a syntax tree, the translation probability is computed as the product of the templates which generate both Since the n-gram model tends to favor short translations, a penalty is added to the translation templates with fewer RHS symbols than LHS leaf symbols: Penalty(t) = exp(|t.RHS |− |t.LHSLeaf|) where |t.RHS |denotes the number of symbols in the RHS of t, and |t.LHSLeaf |denotes the number of leaves in the LHS of t. The length penalty is analogous to the length feature widely used in loglinear models for MT (Huang et al., 2006; Liu et al., 2006; Och and Ney, 2004). Here we distribute the penalty into TTS templates for the convenience of DP, so that we don’t have to generate the N-best list and do re-ranking. To speed up the decoding, standard beam search is used. In Figure 3, BinaryCombine denotes the targetsize binarization (Huang et al., 2006) combination. The translation candidates of the template’s variables, as well as its terminals, are combined pairwise in the order they appear in the RHS of the template. fz denotes a combined translation, whose probability is equal to the product of the probabilities of the</context>
<context position="13224" citStr="Huang et al., 2006" startWordPosition="2229" endWordPosition="2232"> for most systems in the paper, and only needs to be slightly modified to encode the subtree-based n-gram model described in Section 3.3. 3 Improved Tree-to-string Transducer for Machine Translation 3.1 Normalization of TTS Templates Given the story that translations are generated based on the source syntax trees, the weight of the template is computed as the probability of the target strings given the source subtree: #(t) Weight(t) = #(t0 : LHS(t0) = LHS(t)) Such normalization, denoted here as TREE, is used in most tree-to-string template-based MT systems (Liu et al., 2007; Liu et al., 2006; Huang et al., 2006). Galley et al. (2006) proposed an alteration in synchronous SSMT which addresses the probability of both the source subtree and the target string given the root of the source subtree: #(t) Weight(t) = #(t0 : root(t0) = root(t)) This method is denoted as ROOT. Here, we propose another modification: #(t) Weight(t) = #(t0 : cfg(t0) = cfg(t)) (1) cfg in Equation 1 denotes the first level expansion of the source subtree and the method is denoted as CFG. CFG can be thought of as generating both the source subtree and the target string given the first level expansion of the source subtree. TREE focu</context>
<context position="22172" citStr="Huang et al. (2006)" startWordPosition="3757" endWordPosition="3760">lopment sentence pairs and 73597 training sentence pairs. The test set and development set are selected as those sentences having fewer than 25 words on the Chinese side. The translation is from English to Chinese, and Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the syntax trees for the English side. The weights of the MT components are optimized based on the development set using a gridbased line search. The Chinese sentence from the selected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. 4.1 Syntax-Based System The decoding algorithm described in Figure 3 is used with the different normalization methods described in Section 3.1 and the results are summarized in Table 2. The TTS templates are extracted using GHKM based on the many-to-one alignment Parsed Parallel Corpus Giza++ GHKM Templates Extra Templates EM viterbi Templates Sub-tree Bigram SRILM decoder decompositon model translation model </context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In The Sixth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="23674" citStr="Koehn, 2004" startWordPosition="3994" endWordPosition="3995">us systems with the syntactic alignment and subtree bigram improvements added incrementally. from Chinese to English obtained from GIZA++. We have tried using alignment in the reverse direction and the union of both directions, but neither of them is better than the Chinese-to-English alignment. The reason, based on the empirical result, is simply that the Chinese-to-English alignments lead to the maximum number of templates using GHKM. A modified Kneser-Ney bigram model of the Chinese sentence is trained using SRILM (Stolcke, 2002) using the training set. For comparison, results for Pharaoh (Koehn, 2004), trained and tuned under the same condition, are also shown in Table 2. The phrases used in Pharaoh are extracted as the pair of longest continuous spans in English and Chinese based on the union of the alignments in both direction. We tried using alignments of different directions with Pharaoh, and find that the union gives the maximum number of phrase pairs and the best BLEU scores. The results show that the TTS transducers all outperform Pharaoh, and among them, the one with CFG normalization works better than the other two. We tried the three normalization methods in the syntactic alignme</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In The Sixth Conference of the Association for Machine Translation in the Americas, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-06,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="1021" citStr="Liu et al., 2006" startWordPosition="141" endWordPosition="144">ds, and subtree-based ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years, with two threads developing simultaneously: the synchronous parsing-based SSMT (Galley et al., 2006; May and Knight, 2007) and the tree-to-string (TTS) transducer (Liu et al., 2006; Huang et al., 2006). Synchronous SSMT here denotes the systems which accept a source sentence as the input and generate the translation and the syntactic structure for both the source and the translation simultaneously. Such systems are sometimes also called TTS transducers, but in this paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates. In synchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax </context>
<context position="2508" citStr="Liu et al. (2006)" startWordPosition="397" endWordPosition="400">e traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved. For example, the word alignment computed by GIZA++ and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT (DeNero and Klein, 2007; May and Knight, 2007), due to the fact that the word-based alignment models are not aware of the syntactic structure of the sentences and cou</context>
<context position="5120" citStr="Liu et al., 2006" startWordPosition="824" endWordPosition="827">ecome extremely biased”, due to the fact that bigger templates easily get high probabilities. They instead use a joint model where the templates are normalized based on the root of their tree patterns and show empirical results for that. There is no systematic comparison of different normalization methods. 2. Decomposition model of a TTS transducer (or syntactic language model in synchronous SSMT). There is no explicit modeling for the decomposition of a syntax tree in the TTS transducer (or the probability of the syntactic tree in a synchronous SSMT). Most systems simply use a uniform model (Liu et al., 2006; Huang et al., 2006) or implicitly consider it with a joint model producing both syntax trees and the translations (Galley et al., 2006). 3. Use of semantics. Using semantic features in a SSMT is a natural step along the way towards generating more refined models across languages. The statistical approach to semantic role labeling has been well studied (Xue and Palmer, 2004; Ward et al., 2004; Toutanova et al., 2005), but there is no work attempting to use such information in SSMT, to our limited knowledge. This paper proposes novel methods towards solving these problems. Specifically, we com</context>
<context position="8721" citStr="Liu et al., 2006" startWordPosition="1451" endWordPosition="1454">tion, but in practice, usually only the best derivation is considered: Is the job not finished ? 工作 没有 完成 ? (SQ (AUX is) NP1 (RB not) VP2 ?3) _&gt; NP1 没有 VP2 ?3 (NP (DT the) (NN job)) _&gt; 工作 (VP VBN1) _&gt; VBN1 (VBN finished) _&gt; 完成 (? ?) _&gt; ? Figure 1: A TTS Template Example SQ AUX NP RB VP ? DT NN VBN Here, S denotes the target translation, T denotes the source syntax tree, and D* denotes the best derivation of T. The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al., 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al., 2006). This paper uses the latter approach, and the algorithm is sketched in Figure 3. For the baseline approach, only the translation model and n-gram model for the target language are used: Pr(S|T, D*) = � Weight(t) tED* Figure 2: Derivation Example S* = argmax Pr(T|S) = argmax Pr(S)Pr(S|T) S S plied to translate one language to another. The variables are further transformed and the recursive process goes on until there are no variables left. The formal description of a TTS transducer is described in Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transduce</context>
<context position="10652" citStr="Liu et al., 2006" startWordPosition="1784" endWordPosition="1787">orresponding Chinese. For a given derivation (decomposition) of a syntax tree, the translation probability is computed as the product of the templates which generate both Since the n-gram model tends to favor short translations, a penalty is added to the translation templates with fewer RHS symbols than LHS leaf symbols: Penalty(t) = exp(|t.RHS |− |t.LHSLeaf|) where |t.RHS |denotes the number of symbols in the RHS of t, and |t.LHSLeaf |denotes the number of leaves in the LHS of t. The length penalty is analogous to the length feature widely used in loglinear models for MT (Huang et al., 2006; Liu et al., 2006; Och and Ney, 2004). Here we distribute the penalty into TTS templates for the convenience of DP, so that we don’t have to generate the N-best list and do re-ranking. To speed up the decoding, standard beam search is used. In Figure 3, BinaryCombine denotes the targetsize binarization (Huang et al., 2006) combination. The translation candidates of the template’s variables, as well as its terminals, are combined pairwise in the order they appear in the RHS of the template. fz denotes a combined translation, whose probability is equal to the product of the probabilities of the component transla</context>
<context position="13203" citStr="Liu et al., 2006" startWordPosition="2225" endWordPosition="2228">DP algorithm works for most systems in the paper, and only needs to be slightly modified to encode the subtree-based n-gram model described in Section 3.3. 3 Improved Tree-to-string Transducer for Machine Translation 3.1 Normalization of TTS Templates Given the story that translations are generated based on the source syntax trees, the weight of the template is computed as the probability of the target strings given the source subtree: #(t) Weight(t) = #(t0 : LHS(t0) = LHS(t)) Such normalization, denoted here as TREE, is used in most tree-to-string template-based MT systems (Liu et al., 2007; Liu et al., 2006; Huang et al., 2006). Galley et al. (2006) proposed an alteration in synchronous SSMT which addresses the probability of both the source subtree and the target string given the root of the source subtree: #(t) Weight(t) = #(t0 : root(t0) = root(t)) This method is denoted as ROOT. Here, we propose another modification: #(t) Weight(t) = #(t0 : cfg(t0) = cfg(t)) (1) cfg in Equation 1 denotes the first level expansion of the source subtree and the method is denoted as CFG. CFG can be thought of as generating both the source subtree and the target string given the first level expansion of the sour</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of COLING/ACL-06, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-string statistical translation rules.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07,</booktitle>
<location>Prague.</location>
<contexts>
<context position="13185" citStr="Liu et al., 2007" startWordPosition="2221" endWordPosition="2224"> syntax tree. The DP algorithm works for most systems in the paper, and only needs to be slightly modified to encode the subtree-based n-gram model described in Section 3.3. 3 Improved Tree-to-string Transducer for Machine Translation 3.1 Normalization of TTS Templates Given the story that translations are generated based on the source syntax trees, the weight of the template is computed as the probability of the target strings given the source subtree: #(t) Weight(t) = #(t0 : LHS(t0) = LHS(t)) Such normalization, denoted here as TREE, is used in most tree-to-string template-based MT systems (Liu et al., 2007; Liu et al., 2006; Huang et al., 2006). Galley et al. (2006) proposed an alteration in synchronous SSMT which addresses the probability of both the source subtree and the target string given the root of the source subtree: #(t) Weight(t) = #(t0 : root(t0) = root(t)) This method is denoted as ROOT. Here, we propose another modification: #(t) Weight(t) = #(t0 : cfg(t0) = cfg(t)) (1) cfg in Equation 1 denotes the first level expansion of the source subtree and the method is denoted as CFG. CFG can be thought of as generating both the source subtree and the target string given the first level exp</context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Proceedings of ACL-07, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J May</author>
<author>K Knight</author>
</authors>
<title>Syntactic re-alignment models for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="963" citStr="May and Knight, 2007" startWordPosition="132" endWordPosition="135">ent framework integrating the insertion of unaligned target words, and subtree-based ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system. 1 Introduction Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years, with two threads developing simultaneously: the synchronous parsing-based SSMT (Galley et al., 2006; May and Knight, 2007) and the tree-to-string (TTS) transducer (Liu et al., 2006; Huang et al., 2006). Synchronous SSMT here denotes the systems which accept a source sentence as the input and generate the translation and the syntactic structure for both the source and the translation simultaneously. Such systems are sometimes also called TTS transducers, but in this paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates. In synchronous SSMT, TTS templates are used similar to the context free</context>
<context position="2988" citStr="May and Knight, 2007" startWordPosition="479" endWordPosition="482">d introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved. For example, the word alignment computed by GIZA++ and used as a basis to extract the TTS templates in most SSMT systems has been observed to be a problem for SSMT (DeNero and Klein, 2007; May and Knight, 2007), due to the fact that the word-based alignment models are not aware of the syntactic structure of the sentences and could produce many syntax-violating word alignments. Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and build62 Proceedings of the Third Workshop on Statistical Machine Translation, pages 62–69, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ing a syntax-based word alignmen</context>
<context position="14567" citStr="May and Knight (2007)" startWordPosition="2456" endWordPosition="2459"> both the source subtree and the target string, while CFG, as something of a compromise between TREE and ROOT, hopefully can achieve a combined effect of both of them. Compared with TREE, CFG favors the one-level context-free grammar like templates and gives penalty to the templates bigger (in terms of the depth of the source subtree) than that. It makes sense considering that the big templates, due to their sparseness in the corpus, are often assigned unduly large probabilities by TREE. 3.2 Syntactic Word Alignment The idea of building a syntax-based word alignment model has been explored by May and Knight (2007), with an algorithm working from the root tree node down to the leaves, recursively replacing the variables in the matched tree-to-string templates until there are no such variables left. The TTS templates they use are initially gathered using GHKM 65 1. Run GIZA++ to get the initial word alignment, use GHKM to gather translation templates, and compute the initial probability as their normalized frequency. 2. Collect all the one-level subtrees in the training corpus containing only non-terminals and create TTS templates addressing all the permutations of the subtrees’ leaves if its spanning fa</context>
<context position="19617" citStr="May and Knight (2007)" startWordPosition="3309" endWordPosition="3312">th different subtrees have equal number of translation candidates surviving to the upper phase. The function 66 Figure 5: Flow graph of the system with all components integrated BinaryCombine is almost the same as in Figure 3, except that the translation candidates (states) of each tree node are grouped according to their associated subtrees. The bigram probabilities of the subtrees can be easily computed with the viterbi derivation in last subsection. Also, a weight should be assigned to this component. This tree n-gram model can be easily adapted and used in synchronous SSMT systems such as May and Knight (2007), Galley et al. (2006). The flow graph of the final system with all the components integrated is shown in Figure 5. 3.4 Use of Semantic Roles Statistical approaches to MT have gone through word-based systems, phrase-based systems, and syntax-based systems. The next generation would seem to be semantic-based systems. We use PropBank (Palmer et al., 2005) as the semantic driver in our TTS transducer because it is built upon the same corpus (the Penn Treebank) used to train the statistical parser, and its shallow semantic roles are more easily integrated into a TTS transducer. A MaxEntropy classi</context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>J. May and K. Knight. 2007. Syntactic re-alignment models for machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="10672" citStr="Och and Ney, 2004" startWordPosition="1788" endWordPosition="1791">se. For a given derivation (decomposition) of a syntax tree, the translation probability is computed as the product of the templates which generate both Since the n-gram model tends to favor short translations, a penalty is added to the translation templates with fewer RHS symbols than LHS leaf symbols: Penalty(t) = exp(|t.RHS |− |t.LHSLeaf|) where |t.RHS |denotes the number of symbols in the RHS of t, and |t.LHSLeaf |denotes the number of leaves in the LHS of t. The length penalty is analogous to the length feature widely used in loglinear models for MT (Huang et al., 2006; Liu et al., 2006; Och and Ney, 2004). Here we distribute the penalty into TTS templates for the convenience of DP, so that we don’t have to generate the N-best list and do re-ranking. To speed up the decoding, standard beam search is used. In Figure 3, BinaryCombine denotes the targetsize binarization (Huang et al., 2006) combination. The translation candidates of the template’s variables, as well as its terminals, are combined pairwise in the order they appear in the RHS of the template. fz denotes a combined translation, whose probability is equal to the product of the probabilities of the component translations, the probabili</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="6328" citStr="Palmer et al., 2005" startWordPosition="1022" endWordPosition="1025">ally, we compare three ways of normalizing the TTS templates based on the tree pattern, the root of the tree pattern, and the firstlevel expansion of the tree pattern respectively, in the context of hard counting and EM estimation; we present a syntactic alignment framework integrating both the template re-estimation and insertion of unaligned target words; we use a subtree-based n-gram model to address the decomposition of the syntax trees in TTS transducer (or the syntactic language model for synchronous SSMT); we use a statistical classifier to label the semantic roles defined by PropBank (Palmer et al., 2005) and try different ways of using the semantic features in a TTS transducer. We chose the TTS transducer instead of synchronous SSMT for two reasons. First, the decoding algorithm for the TTS transducer has lower computational complexity, which makes it easier to integrate a complex decomposition model. Second, the TTS Transducer can be easily integrated with semantic role features since the syntax tree is present, and it’s not clear how to do this in a synchronous SSMT system. The remainder of the paper will focus on introducing the improved TTS transducer and is organized as follows: Section </context>
<context position="19972" citStr="Palmer et al., 2005" startWordPosition="3366" endWordPosition="3369">probabilities of the subtrees can be easily computed with the viterbi derivation in last subsection. Also, a weight should be assigned to this component. This tree n-gram model can be easily adapted and used in synchronous SSMT systems such as May and Knight (2007), Galley et al. (2006). The flow graph of the final system with all the components integrated is shown in Figure 5. 3.4 Use of Semantic Roles Statistical approaches to MT have gone through word-based systems, phrase-based systems, and syntax-based systems. The next generation would seem to be semantic-based systems. We use PropBank (Palmer et al., 2005) as the semantic driver in our TTS transducer because it is built upon the same corpus (the Penn Treebank) used to train the statistical parser, and its shallow semantic roles are more easily integrated into a TTS transducer. A MaxEntropy classifier, with features following Xue and Palmer (2004) and Ward et al. (2004), is used to generate the semantic roles for each verb in the syntax trees. We then replace the syntactic labels with the semantic roles so that we have more general tree labels, or combine the semantic roles with the syntactic labels to generate more refined tree node labels. Tho</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL02.</booktitle>
<contexts>
<context position="22151" citStr="Papineni et al., 2002" startWordPosition="3753" endWordPosition="3756">sentence pairs, 500 development sentence pairs and 73597 training sentence pairs. The test set and development set are selected as those sentences having fewer than 25 words on the Chinese side. The translation is from English to Chinese, and Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the syntax trees for the English side. The weights of the MT components are optimized based on the development set using a gridbased line search. The Chinese sentence from the selected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. 4.1 Syntax-Based System The decoding algorithm described in Figure 3 is used with the different normalization methods described in Section 3.1 and the results are summarized in Table 2. The TTS templates are extracted using GHKM based on the many-to-one alignment Parsed Parallel Corpus Giza++ GHKM Templates Extra Templates EM viterbi Templates Sub-tree Bigram SRILM decoder decompositon mod</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings ofACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Rounds</author>
</authors>
<title>Mappings and grammars on trees.</title>
<date>1970</date>
<booktitle>Mathematical Systems Theory,</booktitle>
<pages>4--3</pages>
<contexts>
<context position="2250" citStr="Rounds (1970)" startWordPosition="356" endWordPosition="357">utput and can be thought of as a constraint on the translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved. For example, the word alignment computed by GIZA++ and used as a basis t</context>
</contexts>
<marker>Rounds, 1970</marker>
<rawString>William C. Rounds. 1970. Mappings and grammars on trees. Mathematical Systems Theory, 4(3):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="23600" citStr="Stolcke, 2002" startWordPosition="3982" endWordPosition="3984">69 14.32 10.29 15.30 10.99 PHARAOH 9.04 7.84 Table 2: BLEU-4 scores of various systems with the syntactic alignment and subtree bigram improvements added incrementally. from Chinese to English obtained from GIZA++. We have tried using alignment in the reverse direction and the union of both directions, but neither of them is better than the Chinese-to-English alignment. The reason, based on the empirical result, is simply that the Chinese-to-English alignments lead to the maximum number of templates using GHKM. A modified Kneser-Ney bigram model of the Chinese sentence is trained using SRILM (Stolcke, 2002) using the training set. For comparison, results for Pharaoh (Koehn, 2004), trained and tuned under the same condition, are also shown in Table 2. The phrases used in Pharaoh are extracted as the pair of longest continuous spans in English and Chinese based on the union of the alignments in both direction. We tried using alignments of different directions with Pharaoh, and find that the union gives the maximum number of phrase pairs and the best BLEU scores. The results show that the TTS transducers all outperform Pharaoh, and among them, the one with CFG normalization works better than the ot</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In International Conference on Spoken Language Processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Thatcher</author>
</authors>
<title>Generalized sequential machine maps.</title>
<date>1970</date>
<journal>J. Comput. System Sci.,</journal>
<pages>4--339</pages>
<contexts>
<context position="2270" citStr="Thatcher (1970)" startWordPosition="359" endWordPosition="360">hought of as a constraint on the translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved. For example, the word alignment computed by GIZA++ and used as a basis to extract the TTS te</context>
</contexts>
<marker>Thatcher, 1970</marker>
<rawString>J. W. Thatcher. 1970. Generalized sequential machine maps. J. Comput. System Sci., 4:339–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05,</booktitle>
<pages>589--596</pages>
<contexts>
<context position="5541" citStr="Toutanova et al., 2005" startWordPosition="895" endWordPosition="898">explicit modeling for the decomposition of a syntax tree in the TTS transducer (or the probability of the syntactic tree in a synchronous SSMT). Most systems simply use a uniform model (Liu et al., 2006; Huang et al., 2006) or implicitly consider it with a joint model producing both syntax trees and the translations (Galley et al., 2006). 3. Use of semantics. Using semantic features in a SSMT is a natural step along the way towards generating more refined models across languages. The statistical approach to semantic role labeling has been well studied (Xue and Palmer, 2004; Ward et al., 2004; Toutanova et al., 2005), but there is no work attempting to use such information in SSMT, to our limited knowledge. This paper proposes novel methods towards solving these problems. Specifically, we compare three ways of normalizing the TTS templates based on the tree pattern, the root of the tree pattern, and the firstlevel expansion of the tree pattern respectively, in the context of hard counting and EM estimation; we present a syntactic alignment framework integrating both the template re-estimation and insertion of unaligned target words; we use a subtree-based n-gram model to address the decomposition of the s</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher Manning. 2005. Joint learning improves semantic role labeling. In Proceedings of ACL-05, pages 589–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5516" citStr="Ward et al., 2004" startWordPosition="891" endWordPosition="894">SSMT). There is no explicit modeling for the decomposition of a syntax tree in the TTS transducer (or the probability of the syntactic tree in a synchronous SSMT). Most systems simply use a uniform model (Liu et al., 2006; Huang et al., 2006) or implicitly consider it with a joint model producing both syntax trees and the translations (Galley et al., 2006). 3. Use of semantics. Using semantic features in a SSMT is a natural step along the way towards generating more refined models across languages. The statistical approach to semantic role labeling has been well studied (Xue and Palmer, 2004; Ward et al., 2004; Toutanova et al., 2005), but there is no work attempting to use such information in SSMT, to our limited knowledge. This paper proposes novel methods towards solving these problems. Specifically, we compare three ways of normalizing the TTS templates based on the tree pattern, the root of the tree pattern, and the firstlevel expansion of the tree pattern respectively, in the context of hard counting and EM estimation; we present a syntactic alignment framework integrating both the template re-estimation and insertion of unaligned target words; we use a subtree-based n-gram model to address t</context>
<context position="20291" citStr="Ward et al. (2004)" startWordPosition="3421" endWordPosition="3424">em with all the components integrated is shown in Figure 5. 3.4 Use of Semantic Roles Statistical approaches to MT have gone through word-based systems, phrase-based systems, and syntax-based systems. The next generation would seem to be semantic-based systems. We use PropBank (Palmer et al., 2005) as the semantic driver in our TTS transducer because it is built upon the same corpus (the Penn Treebank) used to train the statistical parser, and its shallow semantic roles are more easily integrated into a TTS transducer. A MaxEntropy classifier, with features following Xue and Palmer (2004) and Ward et al. (2004), is used to generate the semantic roles for each verb in the syntax trees. We then replace the syntactic labels with the semantic roles so that we have more general tree labels, or combine the semantic roles with the syntactic labels to generate more refined tree node labels. Though semantic roles are associated with the verbs, it is not feasible to differentiate the roles of different NP VP VP NP (S NP-agent VP) (S NP-patient VP) Table 1: The TREE-based weights of the skeleton templates with NP in different roles verbs due to the data sparseness problem. If some tree nodes are labeled differ</context>
</contexts>
<marker>Ward, Hacioglu, Martin, 2004</marker>
<rawString>Wayne Ward, Kadri Hacioglu, James Martin, , and Dan Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1917" citStr="Wu (1997)" startWordPosition="301" endWordPosition="302">is paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates. In synchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax is part of the output and can be thought of as a constraint on the translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huan</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5497" citStr="Xue and Palmer, 2004" startWordPosition="887" endWordPosition="890"> model in synchronous SSMT). There is no explicit modeling for the decomposition of a syntax tree in the TTS transducer (or the probability of the syntactic tree in a synchronous SSMT). Most systems simply use a uniform model (Liu et al., 2006; Huang et al., 2006) or implicitly consider it with a joint model producing both syntax trees and the translations (Galley et al., 2006). 3. Use of semantics. Using semantic features in a SSMT is a natural step along the way towards generating more refined models across languages. The statistical approach to semantic role labeling has been well studied (Xue and Palmer, 2004; Ward et al., 2004; Toutanova et al., 2005), but there is no work attempting to use such information in SSMT, to our limited knowledge. This paper proposes novel methods towards solving these problems. Specifically, we compare three ways of normalizing the TTS templates based on the tree pattern, the root of the tree pattern, and the firstlevel expansion of the tree pattern respectively, in the context of hard counting and EM estimation; we present a syntactic alignment framework integrating both the template re-estimation and insertion of unaligned target words; we use a subtree-based n-gram</context>
<context position="20268" citStr="Xue and Palmer (2004)" startWordPosition="3416" endWordPosition="3419">ow graph of the final system with all the components integrated is shown in Figure 5. 3.4 Use of Semantic Roles Statistical approaches to MT have gone through word-based systems, phrase-based systems, and syntax-based systems. The next generation would seem to be semantic-based systems. We use PropBank (Palmer et al., 2005) as the semantic driver in our TTS transducer because it is built upon the same corpus (the Penn Treebank) used to train the statistical parser, and its shallow semantic roles are more easily integrated into a TTS transducer. A MaxEntropy classifier, with features following Xue and Palmer (2004) and Ward et al. (2004), is used to generate the semantic roles for each verb in the syntax trees. We then replace the syntactic labels with the semantic roles so that we have more general tree labels, or combine the semantic roles with the syntactic labels to generate more refined tree node labels. Though semantic roles are associated with the verbs, it is not feasible to differentiate the roles of different NP VP VP NP (S NP-agent VP) (S NP-patient VP) Table 1: The TREE-based weights of the skeleton templates with NP in different roles verbs due to the data sparseness problem. If some tree n</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL-06,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="2115" citStr="Zhang et al. (2006)" startWordPosition="331" endWordPosition="334">ynchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax is part of the output and can be thought of as a constraint on the translation process. In the TTS transducer, since the parse tree is given, syntax can be thought of as an additional feature of the input to be used in the translation. The idea of synchronous SSMT can be traced back to Wu (1997)’s Stochastic Inversion Transduction Grammars. A systematic method for extracting TTS templates from parallel corpora was proposed by Galley et al. (2004), and later binarized by Zhang et al. (2006) for high efficiency and accuracy. In the other track, the TTS transducer originated from the tree transducer proposed by Rounds (1970) and Thatcher (1970) independently. Graehl and Knight (2004) generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. Liu et al. (2006) and Huang et al. (2006) then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance. Despite the progress SSMT has achieve</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of NAACL-06, pages 256– 263.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>