<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000270">
<title confidence="0.999743">
A Supervised Learning based Chunking in Thai
using Categorial Grammar
</title>
<author confidence="0.916244">
Thepchai Supnithi, Peerachet Porkaew,
Taneth Ruangrajitpakorn, Kanokorn
</author>
<affiliation confidence="0.81999425">
Trakultaweekool
Human Language Technology,
National Electronics and Computer
Technology Center
</affiliation>
<address confidence="0.495064">
{thepchai.sup, peera-
chet.por, taneth.rua, ka-
</address>
<email confidence="0.965373">
nokorn.tra}@nectec.or.th
</email>
<author confidence="0.6719155">
Chanon Onman, Asanee Kaw-
trakul
</author>
<affiliation confidence="0.9111915">
Department of Computer Engineer-
ing, Kasetsart University and
National Electronics and Computer
Technology Center
</affiliation>
<email confidence="0.991978">
chanon.onman@gmail.com,
asanee.kaw@nectec.or.th
</email>
<sectionHeader confidence="0.995499" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937588235294">
One of the challenging problems in Thai
NLP is to manage a problem on a syn-
tactical analysis of a long sentence.
This paper applies conditional random
field and categorical grammar to devel-
op a chunking method, which can group
words into larger unit. Based on the ex-
periment, we found the impressive re-
sults. We gain around 74.17% on sen-
tence level chunking. Furthermore we
got a more correct parsed tree based on
our technique. Around 50% of tree can
be added. Finally, we solved the prob-
lem on implicit sentential NP which is
one of the difficult Thai language pro-
cessing. 58.65% of sentential NP is cor-
rectly detected.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939790697674">
Recently, many languages applied chunking, or
shallow parsing, using supervised learning ap-
proaches. Basili (1999) utilized clause boundary
recognition for shallow parsing. Osborne (2000)
and McCallum et al. (2000) applied Maximum
Entropy tagger for chunking. Lafferty (2001)
proposed Conditional Random Fields for se-
quence labeling. CRF can be recognized as a
generative model that is able to reach global
optimum while other sequential classifiers focus
on making the best local decision. Sha and Pe-
reira (2003) compared CRF to other supervised
learning in CoNLL task. They achieved results
better than other approaches. Molina et al.
(2002) improved the accuracy of HMM-based
shallow parser by introducing the specialized
HMMs.
In Thai language processing, many research-
es focus on fundamental level of NLP, such as
word segmentation, POS tagging. For example,
Kruengkrai et al. (2006) introduced CRF for
word segmentation and POS tagging trained
over Orchid corpus (Sornlertlamvanich et al.,
1998.). However, the number of tagged texts in
Orchid is specific on a technical report, which is
difficult to be applied to other domains such as
news, document, etc. Furthermore, very little
researches on other fundamental tools, such as
chunking, unknown word detection and parser,
have been done. Pengphon et al. (2002) ana-
lyzed chunks of noun phrase in Thai for infor-
mation retrieval task. All researches assume that
sentence segmentation has been primarily done
in corpus. Since Thai has no explicit sentence
boundary, defining a concrete concept of sen-
tence break is extremely difficult.
Most sentence segmentation researches con-
centrate on &amp;quot;space&amp;quot; and apply to Orchid corpus
(Meknavin 1987, Pradit 2002). Because of am-
biguities on using space, the accuracy is not im-
pressive when we apply into a real application.
Let consider the following paragraph which
is a practical usage from news:
</bodyText>
<page confidence="0.981057">
129
</page>
<note confidence="0.93254">
Proceedings of the 8th Workshop on Asian Language Resources, pages 129–136,
Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing
</note>
<construct confidence="0.86441">
&amp;quot;สําหรับการวางกําลังของคนเสื�อแดง ได้มีกา ร ว า ง บังเกอร์โดยรอบWWri1Jที�ชุมนม
และใช้นํ�ามันราด  |รวม ทั�งมียางรถยนต์  |ขณะการจราจรยังเปิดเllนปFlติ &amp;quot;ุ
lit: “The red shirts have put bunkers around
the assembly area and put oil and tires. The
traffic is opened normally.”
</construct>
<bodyText confidence="0.9998353">
We found that three events are described in
this paragraph. We found that both the first and
second event do not contain a subject. The third
event does not semantically relate to the previ-
ous two events. With a literal translation to Eng-
lish, the first and second can be combined into
one sentence; however, the third events should
be separated.
As we survey in BEST corpus (Kosawat
2009), a ten-million word Thai segmented cor-
pus. It contains twelve genres. The number of
word in sentence is varied from one word to
2,633 words and the average word per line is
40.07 words. Considering to a News domain,
which is the most practical usage in BEST, we
found that the number of words are ranged from
one to 415 words, and the average word length
in sentence is 53.20. It is obvious that there is a
heavy burden load for parser when these long
texts are applied.
</bodyText>
<figureCaption confidence="0.995433">
Figure 1. Examples of compounds in Thai
</figureCaption>
<bodyText confidence="0.99995875">
Two issues are raised in this paper. The first
question is &amp;quot;How to separate a long paragraph
into a larger unit than word effectively?&amp;quot; We are
looking at the possibility of combining words
into a larger grain size. It enables the system to
understand the complicate structure in Thai as
explained in the example. Chunking approach in
this paper is closely similar to the work of Sha
and Pereira (2003). Second question is &amp;quot;How to
analyze the compound noun structure in Thai?&amp;quot;
Thai allows a compound construction for a noun
and its structures can be either a sequence of
nouns or a combination of nouns and verbs. The
second structure is unique since the word order
is as same as a word order of a sentence. We
call this compound noun structure as a “senten-
tial NP”.
Let us exemplify some Thai examples related to
compound word and serial construction problem
in Figure 1. The example 1 shows a sentence
which contains a combination of nouns and
verbs. It can be ambiguously represented into
two structures. The first alternative is that this
sentence shows an evidence of a serial verb
construction. The first word serves as a subject
of the two following predicates. Another alter-
native is that the first three word can be formed
together as a compound noun and they refer to
“a taxi driver” which serve as a subject of the
following verb and noun. The second alternative
is more commonly used in practical language.
However, to set the “N V N” pattern as a noun
can be very ambiguous since in the example 1
can be formed a sentential NP from either the
first three words or the last three words.
From the Example 2, an auxiliary verb serial-
ization is represented. It is a combination of
auxiliary verbs and verb. The word order is
shown in Aux Aux Aux Aux V N sequence.
The given examples show complex cases that
require chunking to reduce an ambiguity while
Thai text is applied into a syntactical analysis
such as parsing. Moreover, there is more chance
to get a syntactically incorrect result from either
rule-based parser or statistical parser with a high
amount of word per input.
This paper is organized as follows. Section 2
explains Thai categorial grammar. Section 3
</bodyText>
<equation confidence="0.846327454545455">
็
ั
Example 1:
man(n) drive(v) taxi(n) find(v) wallet(n)
lit1: A man drove a taxi and found a wallet.
lit2: A taxi chauffeur found a wallet.
คน ข
บ รถแท
กซี� พบ กระเป๋ าสตางค์
Example 2:
น่า จะ ต้อง สามารถ พัฒนา ประเทศ
</equation>
<bodyText confidence="0.797527">
should will must can develop(v) country(n)
lit: possibly have to develop country.
</bodyText>
<page confidence="0.875565">
130
</page>
<bodyText confidence="0.999900166666667">
illustrates CRF, which is supervised method
applied in this work. Section 4 explains the
methodology and experiment framework. Sec-
tion 5 shows experiments setting and result.
Section 6 shows discussion. Conclusion and
future work are illustrated in section 7.
</bodyText>
<sectionHeader confidence="0.932382" genericHeader="method">
2 Linguistic Knowledge
</sectionHeader>
<subsectionHeader confidence="0.991512">
2.1 Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.9998793125">
Categorial grammar (Aka. CG or classical cate-
gorial grammar) (Ajdukiewicz, 1935; Bar-
Hillel, 1953; Carpenter, 1992; Buszkowski,
1998; Steedman, 2000) is formalism in natural
language syntax motivated by the principle of
constitutionality and organized according to the
syntactic elements. The syntactic elements are
categorised in terms of their ability to combine
with one another to form larger constituents as
functions or according to a function-argument
relationship. All syntactic categories in CG are
distinguished by a syntactic category identifying
them as one of the following two types:
example, intransitive verb is needed to combine
with a subject to complete a sentence therefore
intransitive verb is written as s\np which means
</bodyText>
<figureCaption confidence="0.920114">
Figure 2 Example of Thai CG-parsed Tree.
</figureCaption>
<bodyText confidence="0.999542375">
it needs a noun phrase from the left side to
complete a sentence. If there is a noun phrase
exists on the left side, the rule of fraction can-
cellation is applied as np*s\np = s. With CG,
each constituent is annotated with its own syn-
tactic category as its function in text. Currently
there are 79 categories in Thai. An example of
CG derivation from Thai is shown in Figure 2.
</bodyText>
<listItem confidence="0.948763818181818">
1. Argument: this type is a basic category,
such as s (sentence) and np (noun
phrase).
2. Functor (or function category): this cat-
egory type is a combination of argu-
ment and operator(s) &apos;/&apos; and &apos;\&apos;. Functor
is marked to a complex constituent to
assist argument to complete sentence
such as s\np (intransitive verb) requires
noun phrase from the left side to com-
plete a sentence.
</listItem>
<bodyText confidence="0.998810538461539">
CG captures the same information by associ-
ating a functional type or category with all
grammatical entities. The notation α/β is a
rightward-combining functor over a domain of α
into a range of β. The notation α\β is a leftward-
combining functor over β into α. α and β are
both argument syntactic categories
(Hockenmaier and Steedman, 2002; Baldridge
and Kruijff, 2003).
The basic concept is to find the core of the
combination and replace the grammatical modi-
fier and complement with set of categories
based on the same concept with fractions. For
</bodyText>
<subsectionHeader confidence="0.502403">
2.2 CG-Set
</subsectionHeader>
<bodyText confidence="0.5011468">
CG-Set are used as a feature when no CG are
tagged to the input. We aim to apply our chunk-
er to a real world application. Therefore, in case
that we have only sentence without CG tags, we
will use CG-Set instead.
</bodyText>
<table confidence="0.996391142857143">
Cat- Cat-Set Member
Set
Index
0 np คุณสมบัติ
2 s\np/pp,s\np/np,s\np/pp/np,s\np เก็บ, กรอง
3 (np\np)/(np\np), วงจร,
((s\np)\(s\np))/spnum, สัญญาณ
np,
(np\np)\num,np\num,
(np\np)/spnum,
((s\np)\(s\np))\num
62 (s\np)\(s\np),s\s มั&apos;ย, มั&apos;ง, ล่ะ
134 np/(s\np), การ, ความ
np/((s\np)/np)
</table>
<tableCaption confidence="0.99483">
Table 1 Example of CG-Set
</tableCaption>
<page confidence="0.99663">
131
</page>
<bodyText confidence="0.998324428571429">
The concept of CG-Set is to group words that
their all possible CGs are equivalent to the
other. Therefore every word will be assigned to
only one CG-Set. By using CG-Set we use the
lookup table for tagging the input. Table 1
shows examples of CG-set. Currently, there are
183 CG set.
</bodyText>
<sectionHeader confidence="0.994041" genericHeader="method">
3 Conditional Random Field (CRF)
</sectionHeader>
<bodyText confidence="0.9997099375">
CRF is an undirected graph model in which
each vertex represents a random variable whose
distribution is to be inferred, and edge
represents a dependency between two random
variables. It is a supervised framework for
labeling a sequence data such as POS tagging
and chunking. Let X is a random variable of
observed input sequence, such as sequence of
words, and Y is a random variable of label
sequence corresponding to X , such as sequence
of POS or CG. The most probable label
sequence ( yˆ ) can be obtain by
Where x = x1, x2,..., xn and y = y1 , y2,..., yn
p(y  |x) is the conditional probability
distribution of a label sequence given by an
input sequence. CRF defines p(y  |x) as
</bodyText>
<equation confidence="0.9949025">
n
� �
exp EF(y,x,i)
=�i1 �
</equation>
<bodyText confidence="0.9996755">
where Z = E exp(En 1F(y,x,i)) is a
normalization factor over all state sequences.
F(y, x, i) is the global feature vector of CRF
for sequence x and y at position i . F(y, x, i)
can be calculated by using summation of local
features.
</bodyText>
<equation confidence="0.990374333333333">
F y x i
( , , ) = E Aif (yi−1 ,yi , t) +E Ajgj (x, y, t)
i j
</equation>
<bodyText confidence="0.909600888888889">
Each local feature consists of transition feature
function fi (yi−1, yi, t) and per-state feature
function gj (x, y, t) . Where Ai and Aj are
weight vectors of transition feature function and
per-state feature function respectively.
The parameter of CRF can be calculated by
maximizing the likelihood function on the
training data. Viterbi algorithm is normally
applied for searching the most suitable output.
</bodyText>
<sectionHeader confidence="0.999039" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.9991625">
Figure 3 shows the methodology of our
experiments. To prepare the training set, we
start with our corpus annotated with CG tag.
Then, each sentence in the corpus was parsed by
</bodyText>
<equation confidence="0.9955944">
ˆ
(y  |x)
argmax
p
y=
1
=
Z
P y x
(  |)
</equation>
<figureCaption confidence="0.823076">
Figure 3 Experimental Framework
</figureCaption>
<page confidence="0.98915">
132
</page>
<bodyText confidence="0.950270142857143">
with five chunk types. &amp;quot;NP&amp;quot; stands for noun
phrase, &amp;quot;VP&amp;quot; stands for verb phrase, &amp;quot;PP&amp;quot; stands
for preposition phrase, &amp;quot;ADVP&amp;quot; stands for
adverb phrase and S-BAR stands for
complementizer that link two phrases.
Surface and CG-set are developed from CG
dictionary. CG is retrieved from CG tagged
corpus. IOB is developed by parsing tree. We
apply Thai CG parser to obtain the parsed tree.
Figure 4 shows an example of our prepared
data. We provide 4,201 sentences as a training
data in CRF to obtain a chunked model. In this
experiment, we use 5-fold cross validation to
evaluation the model in term of F-measure.
</bodyText>
<figure confidence="0.915996357142857">
surface cg_set cg chunk_label
ใu 74 s/s/np B-ADVP
วัu 3 np I-ADVP
ที� 180 (np\np)/(s\np) I-ADVP
ไม่ 54 (s\np)/(s\np) I-ADVP
หuาว 7 s\np I-ADVP
หรือ 130 ((s/s)\(s/s))/(s/s) I-ADVP
ใu 74 s/s/np I-ADVP
ฤดูร้อu 0 np I-ADVP
dขา 0 np B-NP
สวม 8 s\np/np B-VP
dสื�อ 0 np B-NP
มา 148 (s\np)/(s\np) B-VP
dข้าdฝ้า 2 s\np I-VP
</figure>
<figureCaption confidence="0.998789">
Figure 4 An example of prepared data
</figureCaption>
<bodyText confidence="0.99994425">
our Thai CG parser, developed by GLR tech-
technique. However, not all sentences can be
parsed successfully due to the complexity of the
sentence. We kept parsable sentences and
unparsable sentences separately. The parsable
sentences were selected to be the training set.
There are four features – surface, CG, CG-set
and chunk marker – in our experiments. CRF is
applied using 5-fold cross validation over
combination of these features. Accuracy in term
of averaged precision and recall are reported.
We select the best model from the experiment
to implement the chunker. To investigate
performance of the chunker, we feed the
unparsable sentences to the chunker and
evaluate them manually.
After that, the sentences which are correctly
chunked will be sent to our Thai CG parser. We
calculate the number of successfully-parsed
sentences and the number of correct chunks.
</bodyText>
<sectionHeader confidence="0.969723" genericHeader="method">
5 Experiment Settings and Results
</sectionHeader>
<subsectionHeader confidence="0.98656">
5.1 Experiment on chunking
5.1.1 Experiment setting
</subsectionHeader>
<bodyText confidence="0.999635272727273">
To develop chunker, we apply CG Dictionary
and CG tagged corpus as input. Four features
are provided to CRF. Surface is a word surface.
CG is a categorial grammar of the word. CG-set
is a combination of CG of the word. IOB
represents a method to mark chunk in a
sentence. &amp;quot;I&amp;quot; means &amp;quot;inner&amp;quot; which represents
the word within the chunk. &amp;quot;O&amp;quot; means &amp;quot;outside&amp;quot;
which represents the word outside the chunk.
&amp;quot;B&amp;quot; means &amp;quot;boundary&amp;quot; which represents the
word as a boundary position. It accompanied
</bodyText>
<tableCaption confidence="0.902113">
Table 2 Chunking accuracy of each chunk
</tableCaption>
<page confidence="0.995507">
133
</page>
<tableCaption confidence="0.992935">
Table 3 Chunking accuracy based on
word and sentence.
</tableCaption>
<subsubsectionHeader confidence="0.527197">
5.1.2 Experiment result
</subsubsectionHeader>
<bodyText confidence="0.999982466666667">
From Table 2, considering on chunk based lev-
el, we found that CG gives the best result
among surface, CG-set, CG and their combina-
tion. The average on three types in terms of F-
measure is 86.20. When we analyze infor-
mation in detail, we found that NP, VP and PP
show the same results. Using CG shows the F-
measure for each of them, 81.15, 90.96 and
99.56 respectively.
From Table 3, considering in both word level
and sentence level, we got the similar results,
CG gives the best results. F-measure is 93.24 in
word level and 74.17 in sentence level. This
shows the evidence that CG plays an important
role to improve the accuracy on chunking.
</bodyText>
<subsectionHeader confidence="0.9963615">
5.2 Experiment on parsing
5.2.1 Experiment setting
</subsectionHeader>
<bodyText confidence="0.999967111111111">
We investigate the improvement of parsing con-
sidering unparsable sentences. There are 14,885
unparsable sentences from our CG parser. These
sentences are inputted in chunked model to ob-
tain a chunked corpus. We manually evaluate
the results by linguist. Linguists evaluate the
chunked output in three types. 0 means incorrect
chunk. 1 means correct chunk and 2 represents a
special case for Thai NP, a sentential NP.
</bodyText>
<subsectionHeader confidence="0.828436">
5.2.2 Experiment result
</subsectionHeader>
<bodyText confidence="0.999978357142857">
From the experiment, we got an impressive re-
sult. We found that 11,698 sentences (78.59%)
are changed from unparsable to parsable sen-
tence. Only 3,187 (21.41%) are unparsable. We
manually evaluate the parsable sentence by ran-
domly select 7,369 sentences. Linguists found
3,689 correct sentences (50.06%). In addition,
we investigate the number of parsable chunk
calculated from the parsable result and found
37,743 correct chunks from 47,718 chunks
(78.47%). We also classified chunk into three
types NN VP and PP and gain the accuracy in
each type 79.14% ,74.66% and 92.57% respec-
tively.
</bodyText>
<sectionHeader confidence="0.99967" genericHeader="method">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999781">
6.1 Error analysis
</subsectionHeader>
<bodyText confidence="0.999904">
From the experiment results, we found the fol-
lowing errors.
</bodyText>
<subsectionHeader confidence="0.952462">
6.1.1 Chunking Type missing
</subsectionHeader>
<bodyText confidence="0.981177666666667">
Some chunk missing types are found in experi-
ment results. For example, [PP vu��� (rec-
ord)][NP �i�&amp;Ns14&apos;flsdj,tu (character about)]. [PP
</bodyText>
<figureCaption confidence="0.769794">
Figure 4 An Example of sentential NP
</figureCaption>
<page confidence="0.990003">
134
</page>
<bodyText confidence="0.991895">
บันทึก (record)] should be defined as VP instead
of PP.
</bodyText>
<subsectionHeader confidence="0.963944">
6.1.2 Over-grouping
</subsectionHeader>
<bodyText confidence="0.9998019">
In the sentence “[VP ใช้ (Using)][NP
(medicine)][VP รักษา (treat) ][NP โรคแต่ละครั&apos;งต้อง
เป็นไป (each disease have to)][PP ตาม (follow) ]
[NP คําแนะนําของแพทย์ (doctor’s instruction)] “, we
found that “NP โรคแต่ละครั&apos;งต้องเป็นไป (each disease
have to) “ has over-grouping. IT is necessary to
breakdown to NP โรคแต่ละครั&apos;ง(each disease) and
VP ต้องเป็นไป(have to). The reason of this error is
due to allow the sentential structure NP VP NP,
and then NP and VP are combined.
</bodyText>
<subsectionHeader confidence="0.513678">
6.1.3 Sentential NP
</subsectionHeader>
<bodyText confidence="0.999997125">
We investigated the number of sentential NP. If
the number of chunk equal to 1, sentence should
not be recognized as NP. Other cases are de-
fined as NP. We found that 929 from 1,584 sen-
tences (58.65 % of sentences) are correct sen-
tential NP. This evidence shows the impressive
results to solve implicit NP in Thai. Figure 4
shows an example of sentential NP.
</bodyText>
<subsectionHeader confidence="0.77247">
6.1.4 CG-set
</subsectionHeader>
<bodyText confidence="0.999906166666667">
Since CG-set is another representation of word
and can only detect from CG dictionary. It is
very easy to develop a tag sequence using CG-
set. We found that CG-set is more powerful than
surface. It might be another alternative for less
language resource situation.
</bodyText>
<subsectionHeader confidence="0.863092">
6.2 The Effect of Linguistic Knowledge on
chunking
</subsectionHeader>
<bodyText confidence="0.99995935">
Since CG is formalism in natural language syn-
tax motivated by the principle of constitutionali-
ty and organised according to the syntactic ele-
ments, we would like to find out whether lin-
guistic knowledge effects to the model. We
grouped 89 categorial grammars into 17 groups,
called CG-17.
It is categorized into Noun, Prep, Noun
Modifier, Number modifier for noun, Number
modifier for verb, Number, Clause Marker,
Verb with no argument, Verb with 1 argument,
Verb with 2 or more arguments, Prefix noun,
Prefix predicate, Prefix predicate modifier,
Noun linker, Predicate Modification, Predicate
linker, and Sentence Modifier.
We found that F-measure is slightly improved
from 74.17% to 75.06%. This shows the evi-
dence that if we carefully categorized data based
on linguistics viewpoint, it may improve more
accuracy.
</bodyText>
<sectionHeader confidence="0.998052" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999966473684211">
In this paper, we stated Thai language problems
on the long sentence pattern and find the novel
method to chunk sentence into smaller unit,
which larger than word. We concluded that us-
ing CRF accompanied with categorical grammar
show the impressive results. The accuracy of
chunking in sentence level is 74.17%. We are
possible to collect 50% more on correct tree.
This technique enables us to solve the implicit
sentential NP problem. With our technique, we
found 58% of implicit sentential NP. In the fu-
ture work, there are several issues to be im-
proved. First, we have to trade-off between
over-grouping problem and implicit sentential
problem. Second, we plan to consider ADVP,
SBAR, which has a very small size of data. It is
not adequate to train for a good result. Finally,
we plan to apply more linguistics knowledge to
assist more accuracy.
</bodyText>
<sectionHeader confidence="0.999277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991300444444444">
Abney S., and Tenny C., editors, 1991. Parsing
by chunks, Priciple-based Parsing. Kluwer
Academic Publishers.
Awasthi P., Rao D., Ravindram B., 2006. Part
of Speech Tagging and Chunking with HMM
and CRF, Proceeding of the NLPAI Machine
Learning Competition.
Basili R., Pazienza T., and Massio F., 1999.
Lexicalizing a shallow parser, Proceedings of
</reference>
<page confidence="0.985569">
135
</page>
<reference confidence="0.996046777777778">
Traitement Automatique du Langage Naturel
1999. Corgese, Corsica.
Charoenporn Thatsanee, Sornlertlamvanich Vi-
rach, and Isahara Hitoshi. 1997. Building A
Large Thai Text Corpus - Part-Of-Speech
Tagged Corpus: ORCHID. Proceedings of
Natural Language Processing Pacific Rim
Symposium.
Kosawat Krit, Boriboon Monthika, Chootrakool
Patcharika, Chotimongkol Ananlada, Klaithin
Supon, Kongyoung Sarawoot, Kriengket
Kanyanut, Phaholphinyo Sitthaa, Puroda-
kananda Sumonmas,Thanakulwarapas
Tipraporn, and Wutiwiwatchai Chai. 2009.
BEST 2009: Thai Word Segmentation Soft-
ware Contest. The Eigth International Sym-
posium on Natural Language Processing :
83-88.
Kruengkrai C., Sornlertlumvanich V., Isahara H,
2006. A Conditional Random Field Frame-
work for Thai Morphological Analysis, Pro-
ceedings of 5th International Conference on
Language Resources and Evaluation (LREC-
2006).
Kudo T., and Matsumoto Y., 2001. Chunking
with support vector machines, Proceeding of
NAACL.
Lafferty J., McCallum A., and Pereira F., 2001.
Conditional Random Fields : Probabilistic
models for segmenting and labeling sequence
data. In Proceeding of ICML-01, 282-289.
McCallum A., Freitag D., and Pereira F. 2000.
Maximum entropy markov model for infor-
mation extraction and segmentation. Pro-
ceedings of ICML.
Molina A., and Pla F., 2002. Shallow Parsing
using Specialized HMMs, Journal of Machine
Learning Research 2,595-613
Nguyen L. Minh, Nguyen H. Thao, and Nguyen
P., Thai. 2009. An Empirical Study of Viet-
namese Noun Phrase Chunking with Discrim-
inative Sequence Models, Proceedings of the
7th Workshop on Asian Language Resources,
ACL-IJCNLP 2009,9-16
Osborne M. 2000. Shallow Parsing as Part-of-
Speech Tagging. Proceedings of CoNLL-
2000 and LLL-2000, Lisbon, Portugal.
Pengphon N., Kawtrakul A., Suktarachan M.,
2002. Word Formation Approach to Noun
Phrase Analysis for Thai, Proceedings of
SNLP2002.
Sha F. and Pereira F., 2003. Shallow Parsing
with Conditional Random Fields, Proceeding
of HLT-NAACL.
</reference>
<page confidence="0.998774">
136
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.012040">
<title confidence="0.7961575">A Supervised Learning based Chunking in Thai using Categorial Grammar Taneth Ruangrajitpakorn, Human Language</title>
<affiliation confidence="0.814312">National Electronics and Technology Center</affiliation>
<email confidence="0.782066">taneth.rua,</email>
<keyword confidence="0.185292">nokorn.tra}@nectec.or.th Onman, Asanee</keyword>
<email confidence="0.60847">trakul</email>
<affiliation confidence="0.97506175">of Computer ing, Kasetsart University National Electronics and Technology Center</affiliation>
<email confidence="0.8209595">chanon.onman@gmail.com,asanee.kaw@nectec.or.th</email>
<abstract confidence="0.995917166666667">One of the challenging problems in Thai NLP is to manage a problem on a syntactical analysis of a long sentence. This paper applies conditional random field and categorical grammar to develop a chunking method, which can group words into larger unit. Based on the experiment, we found the impressive results. We gain around 74.17% on sentence level chunking. Furthermore we got a more correct parsed tree based on our technique. Around 50% of tree can be added. Finally, we solved the problem on implicit sentential NP which is one of the difficult Thai language processing. 58.65% of sentential NP is correctly detected.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>C Tenny</author>
<author>editors</author>
</authors>
<title>Parsing by chunks, Priciple-based Parsing.</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Abney, Tenny, editors, 1991</marker>
<rawString>Abney S., and Tenny C., editors, 1991. Parsing by chunks, Priciple-based Parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Awasthi</author>
<author>D Rao</author>
<author>B Ravindram</author>
</authors>
<date>2006</date>
<booktitle>Part of Speech Tagging and Chunking with HMM and CRF, Proceeding of the NLPAI Machine Learning Competition.</booktitle>
<marker>Awasthi, Rao, Ravindram, 2006</marker>
<rawString>Awasthi P., Rao D., Ravindram B., 2006. Part of Speech Tagging and Chunking with HMM and CRF, Proceeding of the NLPAI Machine Learning Competition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>T Pazienza</author>
<author>F Massio</author>
</authors>
<title>Lexicalizing a shallow parser,</title>
<date>1999</date>
<journal>Corgese, Corsica.</journal>
<booktitle>Proceedings of Traitement Automatique du Langage Naturel</booktitle>
<marker>Basili, Pazienza, Massio, 1999</marker>
<rawString>Basili R., Pazienza T., and Massio F., 1999. Lexicalizing a shallow parser, Proceedings of Traitement Automatique du Langage Naturel 1999. Corgese, Corsica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charoenporn Thatsanee</author>
<author>Sornlertlamvanich Virach</author>
<author>Isahara Hitoshi</author>
</authors>
<title>Building A Large Thai Text Corpus - Part-Of-Speech Tagged Corpus: ORCHID.</title>
<date>1997</date>
<booktitle>Proceedings of Natural Language Processing Pacific Rim Symposium.</booktitle>
<marker>Thatsanee, Virach, Hitoshi, 1997</marker>
<rawString>Charoenporn Thatsanee, Sornlertlamvanich Virach, and Isahara Hitoshi. 1997. Building A Large Thai Text Corpus - Part-Of-Speech Tagged Corpus: ORCHID. Proceedings of Natural Language Processing Pacific Rim Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kosawat Krit</author>
</authors>
<title>Boriboon Monthika, Chootrakool Patcharika, Chotimongkol Ananlada, Klaithin Supon, Kongyoung Sarawoot, Kriengket Kanyanut, Phaholphinyo Sitthaa, Purodakananda Sumonmas,Thanakulwarapas Tipraporn, and Wutiwiwatchai Chai.</title>
<date>2009</date>
<booktitle>BEST 2009: Thai Word Segmentation Software Contest. The Eigth International Symposium on Natural Language Processing :</booktitle>
<pages>83--88</pages>
<marker>Krit, 2009</marker>
<rawString>Kosawat Krit, Boriboon Monthika, Chootrakool Patcharika, Chotimongkol Ananlada, Klaithin Supon, Kongyoung Sarawoot, Kriengket Kanyanut, Phaholphinyo Sitthaa, Purodakananda Sumonmas,Thanakulwarapas Tipraporn, and Wutiwiwatchai Chai. 2009. BEST 2009: Thai Word Segmentation Software Contest. The Eigth International Symposium on Natural Language Processing : 83-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kruengkrai</author>
<author>V Sornlertlumvanich</author>
<author>H Isahara</author>
</authors>
<title>A Conditional Random Field Framework for Thai Morphological Analysis,</title>
<date>2006</date>
<booktitle>Proceedings of 5th International Conference on Language Resources and Evaluation (LREC2006).</booktitle>
<contexts>
<context position="2032" citStr="Kruengkrai et al. (2006)" startWordPosition="297" endWordPosition="300">erty (2001) proposed Conditional Random Fields for sequence labeling. CRF can be recognized as a generative model that is able to reach global optimum while other sequential classifiers focus on making the best local decision. Sha and Pereira (2003) compared CRF to other supervised learning in CoNLL task. They achieved results better than other approaches. Molina et al. (2002) improved the accuracy of HMM-based shallow parser by introducing the specialized HMMs. In Thai language processing, many researches focus on fundamental level of NLP, such as word segmentation, POS tagging. For example, Kruengkrai et al. (2006) introduced CRF for word segmentation and POS tagging trained over Orchid corpus (Sornlertlamvanich et al., 1998.). However, the number of tagged texts in Orchid is specific on a technical report, which is difficult to be applied to other domains such as news, document, etc. Furthermore, very little researches on other fundamental tools, such as chunking, unknown word detection and parser, have been done. Pengphon et al. (2002) analyzed chunks of noun phrase in Thai for information retrieval task. All researches assume that sentence segmentation has been primarily done in corpus. Since Thai ha</context>
</contexts>
<marker>Kruengkrai, Sornlertlumvanich, Isahara, 2006</marker>
<rawString>Kruengkrai C., Sornlertlumvanich V., Isahara H, 2006. A Conditional Random Field Framework for Thai Morphological Analysis, Proceedings of 5th International Conference on Language Resources and Evaluation (LREC2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with support vector machines, Proceeding of NAACL.</title>
<date>2001</date>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Kudo T., and Matsumoto Y., 2001. Chunking with support vector machines, Proceeding of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields : Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceeding of ICML-01,</booktitle>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty J., McCallum A., and Pereira F., 2001. Conditional Random Fields : Probabilistic models for segmenting and labeling sequence data. In Proceeding of ICML-01, 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy markov model for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>Proceedings of ICML.</booktitle>
<contexts>
<context position="1358" citStr="McCallum et al. (2000)" startWordPosition="193" endWordPosition="196">rds into larger unit. Based on the experiment, we found the impressive results. We gain around 74.17% on sentence level chunking. Furthermore we got a more correct parsed tree based on our technique. Around 50% of tree can be added. Finally, we solved the problem on implicit sentential NP which is one of the difficult Thai language processing. 58.65% of sentential NP is correctly detected. 1 Introduction Recently, many languages applied chunking, or shallow parsing, using supervised learning approaches. Basili (1999) utilized clause boundary recognition for shallow parsing. Osborne (2000) and McCallum et al. (2000) applied Maximum Entropy tagger for chunking. Lafferty (2001) proposed Conditional Random Fields for sequence labeling. CRF can be recognized as a generative model that is able to reach global optimum while other sequential classifiers focus on making the best local decision. Sha and Pereira (2003) compared CRF to other supervised learning in CoNLL task. They achieved results better than other approaches. Molina et al. (2002) improved the accuracy of HMM-based shallow parser by introducing the specialized HMMs. In Thai language processing, many researches focus on fundamental level of NLP, suc</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>McCallum A., Freitag D., and Pereira F. 2000. Maximum entropy markov model for information extraction and segmentation. Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Molina</author>
<author>F Pla</author>
</authors>
<title>Shallow Parsing using Specialized HMMs,</title>
<date>2002</date>
<journal>Journal of Machine Learning Research</journal>
<pages>2--595</pages>
<marker>Molina, Pla, 2002</marker>
<rawString>Molina A., and Pla F., 2002. Shallow Parsing using Specialized HMMs, Journal of Machine Learning Research 2,595-613</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen L Minh</author>
<author>Nguyen H Thao</author>
<author>P Nguyen</author>
<author>Thai</author>
</authors>
<title>An Empirical Study of Vietnamese Noun Phrase Chunking with Discriminative Sequence Models,</title>
<date>2009</date>
<booktitle>Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP</booktitle>
<pages>2009--9</pages>
<marker>Minh, Thao, Nguyen, Thai, 2009</marker>
<rawString>Nguyen L. Minh, Nguyen H. Thao, and Nguyen P., Thai. 2009. An Empirical Study of Vietnamese Noun Phrase Chunking with Discriminative Sequence Models, Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009,9-16</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Osborne</author>
</authors>
<title>Shallow Parsing as Part-ofSpeech Tagging.</title>
<date>2000</date>
<booktitle>Proceedings of CoNLL2000 and LLL-2000,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1331" citStr="Osborne (2000)" startWordPosition="190" endWordPosition="191"> which can group words into larger unit. Based on the experiment, we found the impressive results. We gain around 74.17% on sentence level chunking. Furthermore we got a more correct parsed tree based on our technique. Around 50% of tree can be added. Finally, we solved the problem on implicit sentential NP which is one of the difficult Thai language processing. 58.65% of sentential NP is correctly detected. 1 Introduction Recently, many languages applied chunking, or shallow parsing, using supervised learning approaches. Basili (1999) utilized clause boundary recognition for shallow parsing. Osborne (2000) and McCallum et al. (2000) applied Maximum Entropy tagger for chunking. Lafferty (2001) proposed Conditional Random Fields for sequence labeling. CRF can be recognized as a generative model that is able to reach global optimum while other sequential classifiers focus on making the best local decision. Sha and Pereira (2003) compared CRF to other supervised learning in CoNLL task. They achieved results better than other approaches. Molina et al. (2002) improved the accuracy of HMM-based shallow parser by introducing the specialized HMMs. In Thai language processing, many researches focus on fu</context>
</contexts>
<marker>Osborne, 2000</marker>
<rawString>Osborne M. 2000. Shallow Parsing as Part-ofSpeech Tagging. Proceedings of CoNLL2000 and LLL-2000, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Pengphon</author>
<author>A Kawtrakul</author>
<author>M Suktarachan</author>
</authors>
<title>Word Formation Approach to Noun Phrase Analysis for Thai,</title>
<date>2002</date>
<booktitle>Proceedings of SNLP2002.</booktitle>
<contexts>
<context position="2463" citStr="Pengphon et al. (2002)" startWordPosition="364" endWordPosition="367">introducing the specialized HMMs. In Thai language processing, many researches focus on fundamental level of NLP, such as word segmentation, POS tagging. For example, Kruengkrai et al. (2006) introduced CRF for word segmentation and POS tagging trained over Orchid corpus (Sornlertlamvanich et al., 1998.). However, the number of tagged texts in Orchid is specific on a technical report, which is difficult to be applied to other domains such as news, document, etc. Furthermore, very little researches on other fundamental tools, such as chunking, unknown word detection and parser, have been done. Pengphon et al. (2002) analyzed chunks of noun phrase in Thai for information retrieval task. All researches assume that sentence segmentation has been primarily done in corpus. Since Thai has no explicit sentence boundary, defining a concrete concept of sentence break is extremely difficult. Most sentence segmentation researches concentrate on &amp;quot;space&amp;quot; and apply to Orchid corpus (Meknavin 1987, Pradit 2002). Because of ambiguities on using space, the accuracy is not impressive when we apply into a real application. Let consider the following paragraph which is a practical usage from news: 129 Proceedings of the 8th</context>
</contexts>
<marker>Pengphon, Kawtrakul, Suktarachan, 2002</marker>
<rawString>Pengphon N., Kawtrakul A., Suktarachan M., 2002. Word Formation Approach to Noun Phrase Analysis for Thai, Proceedings of SNLP2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow Parsing with Conditional Random Fields, Proceeding of HLT-NAACL.</title>
<date>2003</date>
<contexts>
<context position="1657" citStr="Sha and Pereira (2003)" startWordPosition="239" endWordPosition="243">one of the difficult Thai language processing. 58.65% of sentential NP is correctly detected. 1 Introduction Recently, many languages applied chunking, or shallow parsing, using supervised learning approaches. Basili (1999) utilized clause boundary recognition for shallow parsing. Osborne (2000) and McCallum et al. (2000) applied Maximum Entropy tagger for chunking. Lafferty (2001) proposed Conditional Random Fields for sequence labeling. CRF can be recognized as a generative model that is able to reach global optimum while other sequential classifiers focus on making the best local decision. Sha and Pereira (2003) compared CRF to other supervised learning in CoNLL task. They achieved results better than other approaches. Molina et al. (2002) improved the accuracy of HMM-based shallow parser by introducing the specialized HMMs. In Thai language processing, many researches focus on fundamental level of NLP, such as word segmentation, POS tagging. For example, Kruengkrai et al. (2006) introduced CRF for word segmentation and POS tagging trained over Orchid corpus (Sornlertlamvanich et al., 1998.). However, the number of tagged texts in Orchid is specific on a technical report, which is difficult to be app</context>
<context position="4778" citStr="Sha and Pereira (2003)" startWordPosition="747" endWordPosition="750">ds are ranged from one to 415 words, and the average word length in sentence is 53.20. It is obvious that there is a heavy burden load for parser when these long texts are applied. Figure 1. Examples of compounds in Thai Two issues are raised in this paper. The first question is &amp;quot;How to separate a long paragraph into a larger unit than word effectively?&amp;quot; We are looking at the possibility of combining words into a larger grain size. It enables the system to understand the complicate structure in Thai as explained in the example. Chunking approach in this paper is closely similar to the work of Sha and Pereira (2003). Second question is &amp;quot;How to analyze the compound noun structure in Thai?&amp;quot; Thai allows a compound construction for a noun and its structures can be either a sequence of nouns or a combination of nouns and verbs. The second structure is unique since the word order is as same as a word order of a sentence. We call this compound noun structure as a “sentential NP”. Let us exemplify some Thai examples related to compound word and serial construction problem in Figure 1. The example 1 shows a sentence which contains a combination of nouns and verbs. It can be ambiguously represented into two struct</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Sha F. and Pereira F., 2003. Shallow Parsing with Conditional Random Fields, Proceeding of HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>