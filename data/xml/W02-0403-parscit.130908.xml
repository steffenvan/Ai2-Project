<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006248">
<note confidence="0.968601">
Proceedings of the Workshop on Automatic Summarization (including DUC 2002),
Philadelphia, July 2002, pp. 19-26. Association for Computational Linguistics.
</note>
<bodyText confidence="0.999927375">
provided with GATE. These are able to perform
basic language processing tasks such as POS
tagging and semantic tagging. This eliminates
the need for users to keep reinventing the same
resources, and provides a good starting point for
new applications. We used these components as
a basis for building the HaSIE summarisation
system described below (see Section 2.2).
</bodyText>
<sectionHeader confidence="0.902951" genericHeader="abstract">
2.1 Finite-state transduction support in
GATE
</sectionHeader>
<bodyText confidence="0.999986791666667">
In order to make it easier to build new process-
ing resources, GATE comes with built-in finite-
state transduction capabilities, which we have
used as the core of the summarisation process
(see Section 3). The transducer runs on gram-
mars written in the JAPE (Java Annotations
Pattern Engine) language (Cunningham et al.,
2002), which describes patterns to match and
annotations to be created as a result.
A JAPE grammar consists of a set of phases,
each of which consists of a set of pattern/action
rules, and which run sequentially. Patterns can
be specified by describing a specific text string,
or existing annotations (e.g. annotations cre-
ated by the tokeniser, gazetteer, part-of-speech
tagger, or document format analysis). Rule pri-
oritisation (if activated) prevents multiple as-
signment of annotations to the same text string.
Creating new modules and applications on the
basis of JAPE, such as a summarisation compo-
nent for another domain, is a low-overhead task,
because the user only needs to be concerned with
writing new grammar rules (though other mod-
ules such as the gazetteer may also need to be
updated or modified). The amount of tuning
necessary for a new domain can vary depending
on the type of information that needs to be ex-
tracted, and how similar the domain and text
structure is to that which existing components
are designed for. For example, we reuse much of
the same information about companies, dates,
numbers, etc from the default Information Ex-
traction components, so very little tuning was
needed in this case, especially since the patterns
we aim to identify for health and safety are quite
easy to define, but this cannot be guaranteed for
all other domains or applications.
So far, we have successfully used JAPE for
named entity recognition, sentence splitting,
and summarisation, and we intend to experi-
ment with it in other fields such as shallow syn-
tactic parsing. Although at the moment we are
using hand-crafted rules, it would be possible
for an application to learn rules automatically
for the Jape transducers in a manner similar to
(Day et al., 1997). These rules could then be
verified or amended by a human if necessary, as
they are human readable.
</bodyText>
<subsectionHeader confidence="0.981857">
2.2 HaSIE NE Modules
</subsectionHeader>
<bodyText confidence="0.9165915">
HaSIE uses a set of Named Entity Recognition
modules adapted from the ANNIE information
extraction system. HaSIE uses the following
processing resources:
</bodyText>
<listItem confidence="0.997778461538462">
• Tokeniser - which splits the text into indi-
vidual word, number and punctuation to-
kens.
• Sentence Splitter - which splits the text into
individual sentences.
• Part-of-Speech Tagger (Hepple, 2000) -
which produces a part-of-speech (POS) tag
on each token.
• Gazetteer - which contains lists of proper
names and keywords used by the grammar.
• Semantic Tagger - a JAPE Transducer
which annotates text with information such
as entity types.
</listItem>
<bodyText confidence="0.999966631578947">
The first three components are taken directly
from ANNIE; the gazetteer and JAPE trans-
ducer are modified versions of ANNIE com-
ponents, which reflect the specific information
needs of the project. The HaSIE gazetteer con-
tains additional information about words related
specifically to the field of health and safety, such
as lists of accident keywords (e.g. &amp;quot;road traf-
fic accident&amp;quot;, &amp;quot;fatality&amp;quot;, &amp;quot;falls from heights&amp;quot;&apos;),
health and safety keywords (e.g. &amp;quot;Lost Work-
day Cases&amp;quot;, &amp;quot;Medical Treatment Cases&amp;quot;, &amp;quot;RID-
DOR&amp;quot;), and various other lists of more general
keywords such as &amp;quot;monitoring&amp;quot;, &amp;quot;report&amp;quot;, &amp;quot;pol-
icy&amp;quot; etc. The HaSIE semantic tagger contains
hand-coded rules to identify textual patterns
which are to be annotated, for example, to find
noun phrases which contain information about
accidents. These will be described in more detail
in the next section.
</bodyText>
<sectionHeader confidence="0.9069015" genericHeader="introduction">
3 Extracting the relevant
information
</sectionHeader>
<bodyText confidence="0.999428">
The summarisation system was built according
to the following steps:
</bodyText>
<subsectionHeader confidence="0.999892">
3.1 Corpus analysis
</subsectionHeader>
<bodyText confidence="0.999971625">
A corpus was collected consisting of company re-
ports downloaded from the Web and converted
from PDF into HTML. After consultation with
experts in the field of health and safety as to
which types of information were relevant, the
reports were analysed manually and guidelines
were drawn up about relevant sentences and
phrases to be extracted. These were then anal-
ysed further by the system developers (compu-
tational linguists) in order to establish which en-
tities should be recognised, and how the relevant
material could be extracted. This list included
key incident and accident words and phrases,
health and safety-related words and phrases,
names of organizations, job titles, locations, per-
centages, figures, and dates.
</bodyText>
<subsectionHeader confidence="0.999962">
3.2 Extraction of entities
</subsectionHeader>
<bodyText confidence="0.99400046">
The entity extraction was performed by adapt-
ing the gazetteer and semantic tagger mod-
ules from ANNIE detailed in Section 2.2. The
gazetteer lists were updated with keywords, and
new rules were added to the semantic tag-
ger. These modules annotate specific words
and phrases related to health and safety (e.g.
&amp;quot;HSE&amp;quot;, &amp;quot;Occupational Health&amp;quot;) with an HSE
label, accidents (e.g. &amp;quot;accident&amp;quot;, &amp;quot;injury&amp;quot;,
&amp;quot;death&amp;quot;) with an Accident label, and dates,
company names, etc. with similar relevant la-
bels. Some of these annotations, such as Com-
pany Name, are output directly (see Section
3.4); others are used in the sentence generation
phase (see Section 3.3). Figure 1 shows part of
a sample text (the Debenhams company report)
annotated with such entities.
The annotation types extracted in this phase
are: Company, Organization, Jobtitle&apos;, Acci-
dent, HSE, Date, Percent, Money and Number.
With the exception of Company, Accident and
HSE, these types (and the rules used to extract
them) are the same as those used in the default
ANNIE system. The difference between Com-
pany and Organization is that Company depicts
the name of the company which has produced
the report, while Organization depicts any or-
ganisation mentioned in the document (which
may or may not be a company name).
The following rule shows how we extract an
HSE annotation, using the result of gazetteer
lookup and part-of-speech tagging. We look for
a Jobtitle annotation (found in a previous phase
of the grammar), followed by one or more num-
bers, common nouns or adjectives (in any com-
bination) followed by something which has been
found in a gazetteer list of HSE keywords. (The
POS tag is specified by the feature &amp;quot;category&amp;quot;
on the token.) If this pattern is matched, then
the whole pattern is annotated as an HSE.
Rule: HSE2
(
({Jobtitle} I
{Token. category — CD} I
{Token. category — MO I
{Token. category — JJ})+
({Lookup .maj orType — hse})+
) :key
--&gt;
:key .HSE = {rule= HSE2}
</bodyText>
<subsectionHeader confidence="0.995863">
3.3 Extracting sentences
</subsectionHeader>
<bodyText confidence="0.999981444444444">
The next step uses another JAPE transducer
to define rules which extract sentences or para-
graphs which describe health and safety infor-
mation. The transducer depends on previous
annotations such as the result of sentence split-
ting, tokenisation, and the initial annotations
produced in the entity extraction phase. We ex-
tract sentences describing specific information,
such as accident and incident rates, and also
</bodyText>
<footnote confidence="0.879691333333333">
1-We identify this because we want to know whether
there is somebody in the company whose role is to look
after health and safety matters.
</footnote>
<table confidence="0.988333307692308">
Gate 2.0ra build 830 IiIiFi
File Options Tools Help
IP_Gate :1&apos;±iii:i debenhams 1
9 i....4. Applications j Annotations &amp;quot;Annotation Sets Prin r9
a. hse—slac Health and safety Default annotations ....
9 M Language Resources The Company is committed to operating health and safety assessment, Ei ■Prit
M hse corpus training, monitoring and implementation arrangements reaching every part of 2
Adebenhams the Company to ensure so far as reasonably practicable the heal&amp;quot;, and El =
A Coats welfare of its employees, customers and the public. 2 HSE
T it Processing Resources Its commitment to health Msafety is implemented by its Environmental D HSEContext
,L1hse grammar Health Managers, who undertake risk assessments, develop policy, liaise with El JobTitle
Z tag all divisions on matters concerning health and safety, carry out visits to 0 Lookup
Z split stores, collate and monitor centrally reports of accidents and serious incidents E Money
</table>
<bodyText confidence="0.767632866666667">
\ tokie and carry out training. It is also implemented through the framework of a
Health and Safety Committee comprising representatives from all areas of the
business, Health and Safety Committees in stores, and through the store
managers, operations managers and technical services advisers.The Audit
nhse gaz Committee receives regular reports on the operation Ei
of health and safety management within the Company.
.dj reset signed a lead authority agreement with Luton Borough Council to 2 Organization IN
ngaz ° improve liaison between the Company and local enforcement agencies and to 0 Sentence
9 Data stores improve information flows. Representatives from both parties meet on a El
a file:/share/n1p.1S/ quarterly basis to discuss health and safety issues, consultative documents El =
and new developments. D Temp
Charitable and political donations If MIEI
The Group made no political donations during the year.
Payments for by Company during
charitable purposes made the the year ended
</bodyText>
<table confidence="0.842035">
MEIMI /. If Token ....
I
26 August amounted to f 153,000 1159,000) Major charitable I 01
JIM (TM: .
/ Annotations Editor LFeatures Editor 1
Hides this view
</table>
<figureCaption confidence="0.997203">
Figure 1: Debenhams report annotated with entities
</figureCaption>
<bodyText confidence="0.9999925">
paragraphs which describe more general issues
about health and safety. The reason we extract
whole paragraphs is that there is often relevant
information in the whole paragraph which would
be missed if we only extracted the sentences re-
lating specifically to health and safety.
</bodyText>
<subsectionHeader confidence="0.897034">
3.3.1 Paragraph Annotations
</subsectionHeader>
<bodyText confidence="0.999981108108108">
We explain below 3 typical rules used in the
generation of HSE paragraphs.
The first rule tries to match paragraphs con-
taining an HSE entity annotation, followed by
one or more Accident annotations, Environment
annotations, or HSE Keys (possibly separated
by other words). An HSE Key is a word which
in itself does not represent something related
to health and safety, but when combined with
an HSE annotation suggests more strongly the
presence of something related to health and
safety. For example &amp;quot;report&amp;quot; on its own does
not provide conclusive evidence, but when found
with &amp;quot;health&amp;quot; provides a much stronger clue.
&amp;quot;Environment&amp;quot; annotations on their own again
do not provide much evidence, but when com-
bined with an HSE annotation are again much
more strongly linked with health and safety. If
the rule finds a successful match, the whole para-
graph is annotated.
The second rule tries to match paragraphs
containing an HSE annotation, followed by one
or more other HSE annotations or Organiza-
tion annotations (possibly separated by other
words). An HSE annotation refers to a word
of phrase directly related to health and safety,
an Organization annotation refers to a company
name or name of another organization such as
&amp;quot;Health and Safety Executive&amp;quot; or &amp;quot;EC&amp;quot;. If the
rule finds a successful match, the whole para-
graph is annotated.
The third rule looks for an HSE Key fol-
lowed by an HSE annotation (possibly separated
by other words). If the rule finds a successful
match, the whole paragraph is annotated. Since
this is quite a general rule, it only gets fired if
both the other rules fail to match.
</bodyText>
<subsectionHeader confidence="0.944897">
3.3.2 Sentence Annotations
</subsectionHeader>
<bodyText confidence="0.988947807692308">
We also identify sentences about more specific
facts such as accidents and illnesses, e.g.
&amp;quot;The accident frequency ratio for
construction projects was 0.4 (0.49)
per 100,000 hours worked, less than
a third of the national accident
frequency rate in the construction
sector.&amp;quot;
These are identified using rules based on en-
tities such as percentages, accident entities, and
numbers.
We currently use two rules to identify accident
statistics. The first rule tries to match a Number
annotation (written in figures) followed imme-
diately by an Accident annotation. The second
rule is more complicated, and tries to match an
Accident annotation, followed immediately by
one or more Number annotations (in figures) or
Percentage annotations, followed optionally by
further Accident, number or percentage annota-
tions. Number and Percentage annotations are
produced during earlier phases of the grammar,
and are based largely on gazetteer lookup and
string identification (e.g. looking for the &amp;quot;%&amp;quot;
sign). If this pattern is matched, the whole sen-
tence is again identified.
</bodyText>
<subsectionHeader confidence="0.999628">
3.4 Populating a database
</subsectionHeader>
<bodyText confidence="0.999933692307692">
Finally, the relevant annotated parts of the doc-
ument are output by the system in a comma
separated file format, which is then imported in
an Access database. The users inspect the sum-
marisation results using a form interface (see
Figure 2) and also SQL queries (e.g., to find
out how many companies do not discuss health
and safety issues in their annual reports). Prior
to using the IE-based summarisation system,
such analysis was performed manually, which in-
volved locating the necessary information in sev-
eral hundred reports, each between 50 and 100
pages long.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999954903225806">
Evaluations of text summarization systems can
be intrinsic or extrinsic (Sparck Jones and Gal-
hers, 1995): intrinsic evaluation measures the
content of the summary by a comparison with
an &amp;quot;ideal&amp;quot; or &amp;quot;target&amp;quot; summary. Extrinsic eval-
uation measures how helpful summaries are in
the completion of a given task, for example
in question answering or text categorization.
When the evaluation is done by comparing ex-
tracted textual units (sentences or paragraphs)
to a set of ideal textual units, then co-selection
is measured by precision, recall and F-score
(Firmin and Chrzanowski, 1999). While many
researchers have resisted these measures because
in generic summarization it is recognized that
there is no &amp;quot;ideal summary&amp;quot; (Jing et al., 1998),
in our domain-dependent IE-based summariza-
tion, there is a clear idea of what information
should be included in the summary and which
articles deal with health and safety information.
First, we carried out formative evaluation, in-
volving input from our users, as part of the
system development cycle. We processed 36
company reports (8.9 MB of HTML text) with
HaSIE and asked one of our users to mark up
in the resulting summaries (97.6 KB) the sen-
tences which they found highly relevant, moder-
ately relevant, and completely irrelevant.
In an example taken from the Burmah Cas-
trol company report, the following section was
marked as highly relevant:
</bodyText>
<subsectionHeader confidence="0.427258">
Burmah Castrol is committed to
</subsectionHeader>
<bodyText confidence="0.997954777777778">
effective occupational health practices
and procedures. The larger sites
in the businesses have their own
arrangements for monitoring their
health and safety performance and,
where appropriate, health surveillance.
The next section was marked as moderately
relevant:
Underpinning the reduction of
environmental impacts has been the
introduction of training programmes
to raise awareness of environmental
issues. Two international Safety
Health and Environment (SHE) meetings
were held in 1999 involving SHE
specialists, business managers,
marketers and customers.
Finally, this section was marked as irrelevant:
</bodyText>
<figure confidence="0.967821166666667">
Companctme
I-ISEParagraphs
Awards
Accidents
Hse
BAA
</figure>
<bodyText confidence="0.972593111111111">
sustainability management system.... BAA has &apos;received a RoSPA
gold award for occupational safety for the bull year running. The
award is given o-nly if a consistently good o-r continuously improvir g
performance can be demonstrated over a four-year periocl. The
accident-frequency ratio for construction projects was 0.1(0.19) per
100,000 Incurs worked, less than one 11.*d ofte national accident
frequency rate in the construction sector. The company is rwroing
?One in a Mon? campaign to raise safety consckpcsness emd
staridards in construction ancl reduce the accident frequency Fate skill-
furter to one for every mHio11 man hours wofked.... We have no higher
priority than the safety and security of the passengers, staff and
organisations that use our airports. In order to ensu-re that our systems
and practices are continually assessed and wagracled, we work
BAA has received a RoSPA gold award
1-
The accident frequency ratio for construction projects was 0.4 (0.49)
per 100.000 hours worked, less than one third of the national
accident frequency rate in the construction sector.
</bodyText>
<figure confidence="0.464069">
Record: 11 *I 41D.*] of *
</figure>
<figureCaption confidence="0.999712">
Figure 2: The Access form used for report summary analysis
</figureCaption>
<bodyText confidence="0.999909666666667">
Working with the charity &amp;quot;Riders for
Health&amp;quot;, our contribuit ion to a school
for excellence in Zimbabwe will provide
outreach health workers with training
in motorcycle riding and maintenance.
This will help maintain vital services
to rural areas.
The principle behind this was that the users
considered it top priority for the system to find
the highly relevant sections; ideally, the system
should also find the moderately relevant sec-
tions, but this was not so important; the sys-
tem should not identify any of the irrelevant
sections, although this was again less important
than finding the relevant sections. This meant
that the system should be tuned slightly more
towards recall than Precision. To avoid high
recall errors, however, the system deliberately
identified some non-relevant sections (in order
to ensure that they are not annotated as rele-
vant).
</bodyText>
<subsectionHeader confidence="0.989475">
4.1 Evaluation tools
</subsectionHeader>
<bodyText confidence="0.999972708333334">
A vital part of any language processing ap-
plication is the evaluation of its performance,
and a development environment for this pur-
pose would not be complete without some mech-
anisms for its measurement in a large number
of test cases. GATE contains two such mech-
anisms: an evaluation tool (AnnotationDiff)
which enables automated performance measure-
ment and visualisation of the results, and a
benchmarking tool, which enables the tracking
of a system&apos;s progress and regression testing.
Gate&apos;s AnnotationDiff tool enables two sets
of annotations on a document to be compared,
in order to either compare a system-annotated
text with a reference (hand-annotated) text, or
to compare the output of two different versions
of the system (or two different systems). For
each annotation type, figures are generated for
precision, recall, F-measure and false positives.
The AnnotationDiff viewer displays the two
sets of annotations, marked with different
colours (similar to &apos;visual diff&apos; implementations
such as in the MKS Toolkit or TkDiff). Anno-
tations in the key set have two possible colours
depending on their state: white for annotations
which have a compatible (or partially compat-
ible) annotation in the response set, and or-
ange for annotations which are missing in the
response set. Annotations in the response set
have three possible colours: green if they are
compatible with the key annotation, blue if they
are partially compatible, and red if they are spu-
rious. In the viewer, two annotations will be po-
sitioned on the same row if they are co-extensive,
and on different rows if not.
GATE&apos;s benchmarking tool differs from the
AnnotationDiff in that it enables evaluation to
be carried out over a whole corpus rather than
a single document. It also enables tracking
of the system&apos;s performance over time. Fur-
thermore, the system can be run in verbose
mode, where for each performance figure below
a certain threshold (set by the user), the non-
coextensive annotations (and their correspond-
ing text) will be displayed. This information is
useful e.g., when developing new JAPE gram-
mar rules to cover cases currently missed by the
system.
</bodyText>
<sectionHeader confidence="0.537453" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999959320754717">
Preliminary results showed that the system had
identified correctly that 8 of the reports did not
contain relevant information. From the remain-
ing 28 summaries, only 10 had some irrelevant
sentences, but these sentences never constituted
more than 50% of the summary. These re-
sults led to changing HaSIE to account better
for ambiguous keywords, such as &amp;quot;health&amp;quot;, be-
cause often such words occur in irrelevant terms
like &amp;quot;health care&amp;quot; and &amp;quot;health insurance&amp;quot;. By
recognising such terms as spurious, we prevented
them from leading to the (erroneous) recogni-
tion of sentences containing them as relevant.
After these changes were implemented, 7 of the
10 problematic summaries no longer contained
the irrelevant senteces.
After improving the system to deal with key-
word ambiguities, we then created a manually
annotated HSE summary corpus for 80 of the
company reports. To this end, we used HaSIE to
bootstrap the annotation process and then cor-
rect its results manually, using the visual docu-
ment annotation editor from GATE. This corpus
was used to measure the system&apos;s performance,
using GATE&apos;s evaluation tools described in Sec-
tion 4.1.
Results show that overall the system achieved
78% precision and 80% recall, which is con-
sistent with the performance estimates we ob-
tained from the formative evaluation, after we
discarded the irrelevant sentences which were
no longer included after the ambiguity improve-
ments. Furthermore, in 70% of the reports,
the system identified the presence of health and
safety data correctly, and in 15% of these, the
system found this data correctly where humans
had failed to find it. The system only missed
this data in 1 report, and in all the remaining re-
ports, the system correctly identified that there
was no health and safety data present. The sys-
tem generated 70 pages output from approxi-
mately 6000 pages of reports, thereby creating
a substantial reduction in the workload of the
experts who wished to make use of the relevant
data.
Although 80 reports might seem a small num-
ber of texts on which to perform an evaluation,
this is the current scale at which human an-
alysts are performing this task manually each
year. The creation of a reliable automatic sys-
tem will allow the process to scale up into the
thousands and ultimately be able to analyse the
HSE data in all reports of public UK companies.
</bodyText>
<sectionHeader confidence="0.99987" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99967745">
Unlike other knowledge-based approaches to
summarization that depend on rich conceptual
structures (Hahn, 1990; Rau et al., 1989), we
rely on domain-specific terminology and robust
Information Extraction techniques. Closely re-
lated to our approach is Concept Based Ab-
stracting (CBA) (Paice and Oakes, 1999) where
shallow processing is used to instantiate a se-
mantic frame containing the most important
concepts of the source document. Unlike CBA,
which produces a short textual abstract, our sys-
tem produces a set of text passages and instan-
tiates a number of slots (e.g. company, number
of employees, etc.) which are used by an in-
formation analyst to produce health and safety
reports. Our rules for passage extraction rely
on concept co-occurrence, and so are similar to
the contextual templates employed in other IE
approaches to summarization (Paice and Jones,
1993; Saggion and Lapalme, 2000).
</bodyText>
<sectionHeader confidence="0.985999" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999725307692308">
In this paper we have presented how GATE
was used as a development environment for a
summarization system. Our focus was on the
adaptation of general natural language engineer-
ing tools to an specific summarization problem.
While we have relied completely on IE for the
purpose of this application, GATE also allows
the programmer to easily deploy methods within
the framework for computing surface level indi-
cators of salience. In fact, several such GATE-
based modules (e.g., sentence position, t f * idf )
are currently being developed by one of the au-
thors.
</bodyText>
<sectionHeader confidence="0.998247" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993889">
H. Cunningham, D. Maynard, K. Bontcheva,
V. Tablan, and C. Ursu. 2002. The GATE User
Guide. http: //gate . ac .uk/.
D. Day, J. Aberdeen, L. Hirschman, R. Kozierok,
P. Robinson, and M. Vilain. 1997. Mixed-
Initiative Development of Language Processing
Systems. In Proceedings of the 5th Conference on
Applied Natural Language Processing (ANLP-97).
T. Firmin and M.J. Chrzanowski. 1999. An Evalu-
ation of Automatic Text Summarization Systems.
In I. Mani and M.T. Maybury, editors, Advances
in Automatic Text Summarization, pages 325-336.
Udo Hahn. 1990. Topic Parsing: Accounting for
Text Macro Structures in Full-Text Analysis. In-
formation Processing &amp; Management, 26(1):135-
170.
Mark Hepple. 2000. Independence and commitment:
Assumptions for rapid training and execution of
rule-based POS taggers. In Proceedings of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2000), Hong Kong, Octo-
ber.
Hongyan Jing, Kathleen McKeown, Regina Barzi-
lay, and Michael Elhadad. 1998. Summarization
Evaluation Methods: Experiments and Analysis.
In Intelligent Text Summarization. Papers from
the 1998 AAAI Spring Symposium. Technical Re-
port SS-98-06, pages 60-68, Standford (CA), USA,
March 23-25. The AAAI Press.
Inderjeet Mani. 2000. Automatic Text Summariza-
tion. John Benjamins Publishing Company.
Chris D. Paice and Paul A. Jones. 1993. The Iden-
tification of Important Concepts in Highly Struc-
tured Technical Papers. In R. Korfhage, E. Ras-
mussen, and P. Willett, editors, Proc. of the 16th
ACM-SIGIR Conference, pages 69-78.
Chris D. Paice and Michael P. Oakes. 1999. A
Concept-Based Method for Automatic Abstract-
ing. Technical Report Research Report 27, Li-
brary and Information Commission.
Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989.
Information Extraction and Text Summarization
using Linguistic Knowledge Acquisition. Informa-
tion Processing &amp; Management, 25(4):419-428.
H. Saggion and G. Lapalme. 2000. Concept Identi-
fication and Presentation in the Context of Tech-
nical Text Summarization. In Proceedings of the
Workshop on Automatic Summarization. ANLP-
NAACL2000, Seattle, WA, USA, 30 April. Asso-
ciation for Computational Linguistics.
K. Sparck Jones and J.R. Galliers. 1995. Evaluating
Natural Language Processing Systems: An Analy-
sis and Review. Number 1083 in Lecture Notes in
Artificial Intelligence. Springer.
V. Tablan, C. Ursu, K. Bontcheva, H. Cunningham,
D. Maynard, 0. Hamza, Tony McEnery, Paul
Baker, and Mark Leisher. 2002. A unicode-based
environment for creation and use of language re-
sources. In Proceedings of 3rd Language Resources
and Evaluation Conference. forthcoming.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.072320">
<note confidence="0.966725">Proceedings of the Workshop on Automatic Summarization (including DUC 2002), Philadelphia, July 2002, pp. 19-26. Association for Computational Linguistics. provided with GATE. These are able to perform</note>
<abstract confidence="0.984197151785716">basic language processing tasks such as POS tagging and semantic tagging. This eliminates the need for users to keep reinventing the same resources, and provides a good starting point for new applications. We used these components as a basis for building the HaSIE summarisation system described below (see Section 2.2). 2.1 Finite-state transduction support in GATE In order to make it easier to build new processing resources, GATE comes with built-in finitestate transduction capabilities, which we have used as the core of the summarisation process (see Section 3). The transducer runs on grammars written in the JAPE (Java Annotations Pattern Engine) language (Cunningham et al., 2002), which describes patterns to match and annotations to be created as a result. A JAPE grammar consists of a set of phases, each of which consists of a set of pattern/action rules, and which run sequentially. Patterns can be specified by describing a specific text string, or existing annotations (e.g. annotations created by the tokeniser, gazetteer, part-of-speech tagger, or document format analysis). Rule prioritisation (if activated) prevents multiple assignment of annotations to the same text string. Creating new modules and applications on the basis of JAPE, such as a summarisation component for another domain, is a low-overhead task, because the user only needs to be concerned with writing new grammar rules (though other modules such as the gazetteer may also need to be updated or modified). The amount of tuning necessary for a new domain can vary depending on the type of information that needs to be extracted, and how similar the domain and text structure is to that which existing components are designed for. For example, we reuse much of the same information about companies, dates, numbers, etc from the default Information Extraction components, so very little tuning was needed in this case, especially since the patterns we aim to identify for health and safety are quite easy to define, but this cannot be guaranteed for all other domains or applications. So far, we have successfully used JAPE for named entity recognition, sentence splitting, and summarisation, and we intend to experiment with it in other fields such as shallow syntactic parsing. Although at the moment we are using hand-crafted rules, it would be possible for an application to learn rules automatically for the Jape transducers in a manner similar to (Day et al., 1997). These rules could then be verified or amended by a human if necessary, as they are human readable. 2.2 HaSIE NE Modules HaSIE uses a set of Named Entity Recognition modules adapted from the ANNIE information extraction system. HaSIE uses the following processing resources: • Tokeniser which splits the text into individual word, number and punctuation tokens. • Sentence Splitter which splits the text into individual sentences. • Part-of-Speech Tagger (Hepple, 2000) which produces a part-of-speech (POS) tag on each token. • Gazetteer which contains lists of proper names and keywords used by the grammar. • Semantic Tagger a JAPE Transducer which annotates text with information such as entity types. The first three components are taken directly from ANNIE; the gazetteer and JAPE transducer are modified versions of ANNIE components, which reflect the specific information needs of the project. The HaSIE gazetteer contains additional information about words related specifically to the field of health and safety, such as lists of accident keywords (e.g. &amp;quot;road traffic accident&amp;quot;, &amp;quot;fatality&amp;quot;, &amp;quot;falls from heights&amp;quot;&apos;), health and safety keywords (e.g. &amp;quot;Lost Workday Cases&amp;quot;, &amp;quot;Medical Treatment Cases&amp;quot;, &amp;quot;RID- DOR&amp;quot;), and various other lists of more general keywords such as &amp;quot;monitoring&amp;quot;, &amp;quot;report&amp;quot;, &amp;quot;policy&amp;quot; etc. The HaSIE semantic tagger contains hand-coded rules to identify textual patterns which are to be annotated, for example, to find noun phrases which contain information about accidents. These will be described in more detail in the next section. 3 Extracting the relevant information The summarisation system was built according to the following steps: 3.1 Corpus analysis A corpus was collected consisting of company reports downloaded from the Web and converted from PDF into HTML. After consultation with experts in the field of health and safety as to which types of information were relevant, the reports were analysed manually and guidelines were drawn up about relevant sentences and phrases to be extracted. These were then analysed further by the system developers (computational linguists) in order to establish which entities should be recognised, and how the relevant material could be extracted. This list included key incident and accident words and phrases, health and safety-related words and phrases, names of organizations, job titles, locations, percentages, figures, and dates. 3.2 Extraction of entities The entity extraction was performed by adapting the gazetteer and semantic tagger modules from ANNIE detailed in Section 2.2. The gazetteer lists were updated with keywords, and new rules were added to the semantic tagger. These modules annotate specific words and phrases related to health and safety (e.g. &amp;quot;HSE&amp;quot;, &amp;quot;Occupational Health&amp;quot;) with an HSE label, accidents (e.g. &amp;quot;accident&amp;quot;, &amp;quot;death&amp;quot;) with an Accident label, and dates, company names, etc. with similar relevant labels. Some of these annotations, such as Company Name, are output directly (see Section 3.4); others are used in the sentence generation phase (see Section 3.3). Figure 1 shows part of a sample text (the Debenhams company report) annotated with such entities. The annotation types extracted in this phase are: Company, Organization, Jobtitle&apos;, Accident, HSE, Date, Percent, Money and Number. With the exception of Company, Accident and HSE, these types (and the rules used to extract them) are the same as those used in the default ANNIE system. The difference between Company and Organization is that Company depicts the name of the company which has produced the report, while Organization depicts any organisation mentioned in the document (which may or may not be a company name). The following rule shows how we extract an HSE annotation, using the result of gazetteer lookup and part-of-speech tagging. We look for a Jobtitle annotation (found in a previous phase of the grammar), followed by one or more numbers, common nouns or adjectives (in any combination) followed by something which has been found in a gazetteer list of HSE keywords. (The POS tag is specified by the feature &amp;quot;category&amp;quot; on the token.) If this pattern is matched, then the whole pattern is annotated as an HSE. Rule: HSE2 ( ({Jobtitle} I {Token. category — CD} I {Token. category — MO I {Token. category — JJ})+ ({Lookup .maj orType — hse})+ ) :key --&gt; :key .HSE = {rule= HSE2} 3.3 Extracting sentences The next step uses another JAPE transducer to define rules which extract sentences or paragraphs which describe health and safety information. The transducer depends on previous annotations such as the result of sentence splitting, tokenisation, and the initial annotations produced in the entity extraction phase. We extract sentences describing specific information, such as accident and incident rates, and also identify this because we want to know whether there is somebody in the company whose role is to look after health and safety matters. 2.0ra build 830IiIiFi File Options Tools Help IP_Gate debenhams1 9 i....4. Applications j Annotations &amp;quot;Annotation Sets Prin r9 hse—slac Health and safety Default annotations .... M Resources The Company is committed to operating health and safety assessment, training, monitoring and implementation arrangements reaching every part of Ei■Prit 2 M hse corpus Company to ensure so far as reasonably practicable theheal&amp;quot;, El TProcessing Resources welfare of its employees, customers and the public. grammar commitment to healthMsafetyis implemented by its Environmental Health Managers, who undertake risk assessments, develop policy, liaise with all divisions on matters concerning health and safety, carry out visits to stores, collate and monitor centrally reports of accidents and serious incidents and carry out training. It is also implemented through the framework of a Health and Safety Committee comprising representatives from all areas of the business, Health and Safety Committees in stores, and through the store managers, operations managers and technical services advisers.The Audit Z tag El Z split 0 Lookup \tokie gaz Committee receives regular reports on the operation of health and safety management within the Company. Ei reset signed a lead authority agreement with Luton Borough Council to 0 Sentence El IN ngaz ° improve liaison between the Company and local enforcement agencies and El stores improve information flows. Representatives from both parties meet on a a file:/share/n1p.1S/ quarterly basis to discuss health and safety issues, consultative documents IfMIEI and new developments. Charitable and political donations The Group made no political donations during the year. Payments for by Company during charitable purposes made the the year ended MEIMI/. Token .... I August amounted to 153,000 1159,000) charitable I (TM:. / Annotations Editor LFeatures Editor 1 Hides this view Figure 1: Debenhams report annotated with entities paragraphs which describe more general issues about health and safety. The reason we extract whole paragraphs is that there is often relevant information in the whole paragraph which would be missed if we only extracted the sentences relating specifically to health and safety. 3.3.1 Paragraph Annotations We explain below 3 typical rules used in the generation of HSE paragraphs. The first rule tries to match paragraphs containing an HSE entity annotation, followed by one or more Accident annotations, Environment annotations, or HSE Keys (possibly separated by other words). An HSE Key is a word which in itself does not represent something related to health and safety, but when combined with an HSE annotation suggests more strongly the presence of something related to health and safety. For example &amp;quot;report&amp;quot; on its own does not provide conclusive evidence, but when found with &amp;quot;health&amp;quot; provides a much stronger clue. &amp;quot;Environment&amp;quot; annotations on their own again do not provide much evidence, but when combined with an HSE annotation are again much more strongly linked with health and safety. If the rule finds a successful match, the whole paragraph is annotated. The second rule tries to match paragraphs containing an HSE annotation, followed by one or more other HSE annotations or Organization annotations (possibly separated by other words). An HSE annotation refers to a word of phrase directly related to health and safety, an Organization annotation refers to a company name or name of another organization such as &amp;quot;Health and Safety Executive&amp;quot; or &amp;quot;EC&amp;quot;. If the rule finds a successful match, the whole paragraph is annotated. The third rule looks for an HSE Key followed by an HSE annotation (possibly separated by other words). If the rule finds a successful match, the whole paragraph is annotated. Since this is quite a general rule, it only gets fired if both the other rules fail to match. 3.3.2 Sentence Annotations We also identify sentences about more specific facts such as accidents and illnesses, e.g. &amp;quot;The accident frequency ratio for construction projects was 0.4 (0.49) per 100,000 hours worked, less than a third of the national accident frequency rate in the construction sector.&amp;quot; These are identified using rules based on entities such as percentages, accident entities, and numbers. We currently use two rules to identify accident statistics. The first rule tries to match a Number annotation (written in figures) followed immediately by an Accident annotation. The second rule is more complicated, and tries to match an Accident annotation, followed immediately by one or more Number annotations (in figures) or Percentage annotations, followed optionally by further Accident, number or percentage annotations. Number and Percentage annotations are produced during earlier phases of the grammar, and are based largely on gazetteer lookup and string identification (e.g. looking for the &amp;quot;%&amp;quot; sign). If this pattern is matched, the whole sentence is again identified. 3.4 Populating a database Finally, the relevant annotated parts of the document are output by the system in a comma separated file format, which is then imported in an Access database. The users inspect the summarisation results using a form interface (see Figure 2) and also SQL queries (e.g., to find out how many companies do not discuss health and safety issues in their annual reports). Prior to using the IE-based summarisation system, such analysis was performed manually, which involved locating the necessary information in several hundred reports, each between 50 and 100 pages long. 4 Evaluation Evaluations of text summarization systems can intrinsic or extrinsic (Sparck Jones and Galhers, 1995): intrinsic evaluation measures the content of the summary by a comparison with an &amp;quot;ideal&amp;quot; or &amp;quot;target&amp;quot; summary. Extrinsic evaluation measures how helpful summaries are in the completion of a given task, for example in question answering or text categorization. When the evaluation is done by comparing extracted textual units (sentences or paragraphs) to a set of ideal textual units, then co-selection is measured by precision, recall and F-score (Firmin and Chrzanowski, 1999). While many researchers have resisted these measures because in generic summarization it is recognized that there is no &amp;quot;ideal summary&amp;quot; (Jing et al., 1998), in our domain-dependent IE-based summarization, there is a clear idea of what information should be included in the summary and which articles deal with health and safety information. First, we carried out formative evaluation, involving input from our users, as part of the system development cycle. We processed 36 company reports (8.9 MB of HTML text) with HaSIE and asked one of our users to mark up in the resulting summaries (97.6 KB) the sentences which they found highly relevant, moderately relevant, and completely irrelevant. In an example taken from the Burmah Castrol company report, the following section was marked as highly relevant: Burmah Castrol is committed to effective occupational health practices and procedures. The larger sites in the businesses have their own arrangements for monitoring their health and safety performance and, where appropriate, health surveillance. The next section was marked as moderately relevant: Underpinning the reduction of environmental impacts has been the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
<author>C Ursu</author>
</authors>
<title>The GATE User Guide. http: //gate . ac .uk/.</title>
<date>2002</date>
<contexts>
<context position="892" citStr="Cunningham et al., 2002" startWordPosition="132" endWordPosition="135">agging. This eliminates the need for users to keep reinventing the same resources, and provides a good starting point for new applications. We used these components as a basis for building the HaSIE summarisation system described below (see Section 2.2). 2.1 Finite-state transduction support in GATE In order to make it easier to build new processing resources, GATE comes with built-in finitestate transduction capabilities, which we have used as the core of the summarisation process (see Section 3). The transducer runs on grammars written in the JAPE (Java Annotations Pattern Engine) language (Cunningham et al., 2002), which describes patterns to match and annotations to be created as a result. A JAPE grammar consists of a set of phases, each of which consists of a set of pattern/action rules, and which run sequentially. Patterns can be specified by describing a specific text string, or existing annotations (e.g. annotations created by the tokeniser, gazetteer, part-of-speech tagger, or document format analysis). Rule prioritisation (if activated) prevents multiple assignment of annotations to the same text string. Creating new modules and applications on the basis of JAPE, such as a summarisation componen</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, Ursu, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, V. Tablan, and C. Ursu. 2002. The GATE User Guide. http: //gate . ac .uk/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Day</author>
<author>J Aberdeen</author>
<author>L Hirschman</author>
<author>R Kozierok</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<title>MixedInitiative Development of Language Processing Systems.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP-97).</booktitle>
<contexts>
<context position="2661" citStr="Day et al., 1997" startWordPosition="426" endWordPosition="429">ction components, so very little tuning was needed in this case, especially since the patterns we aim to identify for health and safety are quite easy to define, but this cannot be guaranteed for all other domains or applications. So far, we have successfully used JAPE for named entity recognition, sentence splitting, and summarisation, and we intend to experiment with it in other fields such as shallow syntactic parsing. Although at the moment we are using hand-crafted rules, it would be possible for an application to learn rules automatically for the Jape transducers in a manner similar to (Day et al., 1997). These rules could then be verified or amended by a human if necessary, as they are human readable. 2.2 HaSIE NE Modules HaSIE uses a set of Named Entity Recognition modules adapted from the ANNIE information extraction system. HaSIE uses the following processing resources: • Tokeniser - which splits the text into individual word, number and punctuation tokens. • Sentence Splitter - which splits the text into individual sentences. • Part-of-Speech Tagger (Hepple, 2000) - which produces a part-of-speech (POS) tag on each token. • Gazetteer - which contains lists of proper names and keywords us</context>
</contexts>
<marker>Day, Aberdeen, Hirschman, Kozierok, Robinson, Vilain, 1997</marker>
<rawString>D. Day, J. Aberdeen, L. Hirschman, R. Kozierok, P. Robinson, and M. Vilain. 1997. MixedInitiative Development of Language Processing Systems. In Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP-97).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Firmin</author>
<author>M J Chrzanowski</author>
</authors>
<title>An Evaluation of Automatic Text Summarization Systems.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>325--336</pages>
<editor>In I. Mani and M.T. Maybury, editors,</editor>
<contexts>
<context position="13961" citStr="Firmin and Chrzanowski, 1999" startWordPosition="2236" endWordPosition="2239">ch between 50 and 100 pages long. 4 Evaluation Evaluations of text summarization systems can be intrinsic or extrinsic (Sparck Jones and Galhers, 1995): intrinsic evaluation measures the content of the summary by a comparison with an &amp;quot;ideal&amp;quot; or &amp;quot;target&amp;quot; summary. Extrinsic evaluation measures how helpful summaries are in the completion of a given task, for example in question answering or text categorization. When the evaluation is done by comparing extracted textual units (sentences or paragraphs) to a set of ideal textual units, then co-selection is measured by precision, recall and F-score (Firmin and Chrzanowski, 1999). While many researchers have resisted these measures because in generic summarization it is recognized that there is no &amp;quot;ideal summary&amp;quot; (Jing et al., 1998), in our domain-dependent IE-based summarization, there is a clear idea of what information should be included in the summary and which articles deal with health and safety information. First, we carried out formative evaluation, involving input from our users, as part of the system development cycle. We processed 36 company reports (8.9 MB of HTML text) with HaSIE and asked one of our users to mark up in the resulting summaries (97.6 KB) t</context>
</contexts>
<marker>Firmin, Chrzanowski, 1999</marker>
<rawString>T. Firmin and M.J. Chrzanowski. 1999. An Evaluation of Automatic Text Summarization Systems. In I. Mani and M.T. Maybury, editors, Advances in Automatic Text Summarization, pages 325-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hahn</author>
</authors>
<title>Topic Parsing: Accounting for Text Macro Structures in Full-Text Analysis.</title>
<date>1990</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<pages>26--1</pages>
<contexts>
<context position="22148" citStr="Hahn, 1990" startWordPosition="3544" endWordPosition="3545">hereby creating a substantial reduction in the workload of the experts who wished to make use of the relevant data. Although 80 reports might seem a small number of texts on which to perform an evaluation, this is the current scale at which human analysts are performing this task manually each year. The creation of a reliable automatic system will allow the process to scale up into the thousands and ultimately be able to analyse the HSE data in all reports of public UK companies. 5 Related Work Unlike other knowledge-based approaches to summarization that depend on rich conceptual structures (Hahn, 1990; Rau et al., 1989), we rely on domain-specific terminology and robust Information Extraction techniques. Closely related to our approach is Concept Based Abstracting (CBA) (Paice and Oakes, 1999) where shallow processing is used to instantiate a semantic frame containing the most important concepts of the source document. Unlike CBA, which produces a short textual abstract, our system produces a set of text passages and instantiates a number of slots (e.g. company, number of employees, etc.) which are used by an information analyst to produce health and safety reports. Our rules for passage e</context>
</contexts>
<marker>Hahn, 1990</marker>
<rawString>Udo Hahn. 1990. Topic Parsing: Accounting for Text Macro Structures in Full-Text Analysis. Information Processing &amp; Management, 26(1):135-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hepple</author>
</authors>
<title>Independence and commitment: Assumptions for rapid training and execution of rule-based POS taggers.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000),</booktitle>
<location>Hong Kong,</location>
<contexts>
<context position="3135" citStr="Hepple, 2000" startWordPosition="504" endWordPosition="505">es, it would be possible for an application to learn rules automatically for the Jape transducers in a manner similar to (Day et al., 1997). These rules could then be verified or amended by a human if necessary, as they are human readable. 2.2 HaSIE NE Modules HaSIE uses a set of Named Entity Recognition modules adapted from the ANNIE information extraction system. HaSIE uses the following processing resources: • Tokeniser - which splits the text into individual word, number and punctuation tokens. • Sentence Splitter - which splits the text into individual sentences. • Part-of-Speech Tagger (Hepple, 2000) - which produces a part-of-speech (POS) tag on each token. • Gazetteer - which contains lists of proper names and keywords used by the grammar. • Semantic Tagger - a JAPE Transducer which annotates text with information such as entity types. The first three components are taken directly from ANNIE; the gazetteer and JAPE transducer are modified versions of ANNIE components, which reflect the specific information needs of the project. The HaSIE gazetteer contains additional information about words related specifically to the field of health and safety, such as lists of accident keywords (e.g. </context>
</contexts>
<marker>Hepple, 2000</marker>
<rawString>Mark Hepple. 2000. Independence and commitment: Assumptions for rapid training and execution of rule-based POS taggers. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000), Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen McKeown</author>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Summarization Evaluation Methods: Experiments and Analysis.</title>
<date>1998</date>
<booktitle>In Intelligent Text Summarization. Papers from the</booktitle>
<tech>Technical Report SS-98-06,</tech>
<pages>60--68</pages>
<publisher>The AAAI Press.</publisher>
<location>Standford (CA), USA,</location>
<contexts>
<context position="14117" citStr="Jing et al., 1998" startWordPosition="2260" endWordPosition="2263">uation measures the content of the summary by a comparison with an &amp;quot;ideal&amp;quot; or &amp;quot;target&amp;quot; summary. Extrinsic evaluation measures how helpful summaries are in the completion of a given task, for example in question answering or text categorization. When the evaluation is done by comparing extracted textual units (sentences or paragraphs) to a set of ideal textual units, then co-selection is measured by precision, recall and F-score (Firmin and Chrzanowski, 1999). While many researchers have resisted these measures because in generic summarization it is recognized that there is no &amp;quot;ideal summary&amp;quot; (Jing et al., 1998), in our domain-dependent IE-based summarization, there is a clear idea of what information should be included in the summary and which articles deal with health and safety information. First, we carried out formative evaluation, involving input from our users, as part of the system development cycle. We processed 36 company reports (8.9 MB of HTML text) with HaSIE and asked one of our users to mark up in the resulting summaries (97.6 KB) the sentences which they found highly relevant, moderately relevant, and completely irrelevant. In an example taken from the Burmah Castrol company report, t</context>
</contexts>
<marker>Jing, McKeown, Barzilay, Elhadad, 1998</marker>
<rawString>Hongyan Jing, Kathleen McKeown, Regina Barzilay, and Michael Elhadad. 1998. Summarization Evaluation Methods: Experiments and Analysis. In Intelligent Text Summarization. Papers from the 1998 AAAI Spring Symposium. Technical Report SS-98-06, pages 60-68, Standford (CA), USA, March 23-25. The AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Text Summarization.</title>
<date>2000</date>
<publisher>John Benjamins Publishing Company.</publisher>
<marker>Mani, 2000</marker>
<rawString>Inderjeet Mani. 2000. Automatic Text Summarization. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
<author>Paul A Jones</author>
</authors>
<title>The Identification of Important Concepts in Highly Structured Technical Papers. In</title>
<date>1993</date>
<booktitle>Proc. of the 16th ACM-SIGIR Conference,</booktitle>
<pages>69--78</pages>
<editor>R. Korfhage, E. Rasmussen, and P. Willett, editors,</editor>
<contexts>
<context position="22907" citStr="Paice and Jones, 1993" startWordPosition="3664" endWordPosition="3667">is Concept Based Abstracting (CBA) (Paice and Oakes, 1999) where shallow processing is used to instantiate a semantic frame containing the most important concepts of the source document. Unlike CBA, which produces a short textual abstract, our system produces a set of text passages and instantiates a number of slots (e.g. company, number of employees, etc.) which are used by an information analyst to produce health and safety reports. Our rules for passage extraction rely on concept co-occurrence, and so are similar to the contextual templates employed in other IE approaches to summarization (Paice and Jones, 1993; Saggion and Lapalme, 2000). 6 Conclusion and Future Work In this paper we have presented how GATE was used as a development environment for a summarization system. Our focus was on the adaptation of general natural language engineering tools to an specific summarization problem. While we have relied completely on IE for the purpose of this application, GATE also allows the programmer to easily deploy methods within the framework for computing surface level indicators of salience. In fact, several such GATEbased modules (e.g., sentence position, t f * idf ) are currently being developed by on</context>
</contexts>
<marker>Paice, Jones, 1993</marker>
<rawString>Chris D. Paice and Paul A. Jones. 1993. The Identification of Important Concepts in Highly Structured Technical Papers. In R. Korfhage, E. Rasmussen, and P. Willett, editors, Proc. of the 16th ACM-SIGIR Conference, pages 69-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
<author>Michael P Oakes</author>
</authors>
<title>A Concept-Based Method for Automatic Abstracting.</title>
<date>1999</date>
<tech>Technical Report Research Report 27,</tech>
<institution>Library and Information Commission.</institution>
<contexts>
<context position="22344" citStr="Paice and Oakes, 1999" startWordPosition="3572" endWordPosition="3575">erform an evaluation, this is the current scale at which human analysts are performing this task manually each year. The creation of a reliable automatic system will allow the process to scale up into the thousands and ultimately be able to analyse the HSE data in all reports of public UK companies. 5 Related Work Unlike other knowledge-based approaches to summarization that depend on rich conceptual structures (Hahn, 1990; Rau et al., 1989), we rely on domain-specific terminology and robust Information Extraction techniques. Closely related to our approach is Concept Based Abstracting (CBA) (Paice and Oakes, 1999) where shallow processing is used to instantiate a semantic frame containing the most important concepts of the source document. Unlike CBA, which produces a short textual abstract, our system produces a set of text passages and instantiates a number of slots (e.g. company, number of employees, etc.) which are used by an information analyst to produce health and safety reports. Our rules for passage extraction rely on concept co-occurrence, and so are similar to the contextual templates employed in other IE approaches to summarization (Paice and Jones, 1993; Saggion and Lapalme, 2000). 6 Concl</context>
</contexts>
<marker>Paice, Oakes, 1999</marker>
<rawString>Chris D. Paice and Michael P. Oakes. 1999. A Concept-Based Method for Automatic Abstracting. Technical Report Research Report 27, Library and Information Commission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa F Rau</author>
<author>Paul S Jacobs</author>
<author>Uri Zernik</author>
</authors>
<title>Information Extraction and Text Summarization using Linguistic Knowledge Acquisition.</title>
<date>1989</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<pages>25--4</pages>
<contexts>
<context position="22167" citStr="Rau et al., 1989" startWordPosition="3546" endWordPosition="3549">ing a substantial reduction in the workload of the experts who wished to make use of the relevant data. Although 80 reports might seem a small number of texts on which to perform an evaluation, this is the current scale at which human analysts are performing this task manually each year. The creation of a reliable automatic system will allow the process to scale up into the thousands and ultimately be able to analyse the HSE data in all reports of public UK companies. 5 Related Work Unlike other knowledge-based approaches to summarization that depend on rich conceptual structures (Hahn, 1990; Rau et al., 1989), we rely on domain-specific terminology and robust Information Extraction techniques. Closely related to our approach is Concept Based Abstracting (CBA) (Paice and Oakes, 1999) where shallow processing is used to instantiate a semantic frame containing the most important concepts of the source document. Unlike CBA, which produces a short textual abstract, our system produces a set of text passages and instantiates a number of slots (e.g. company, number of employees, etc.) which are used by an information analyst to produce health and safety reports. Our rules for passage extraction rely on c</context>
</contexts>
<marker>Rau, Jacobs, Zernik, 1989</marker>
<rawString>Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. Information Extraction and Text Summarization using Linguistic Knowledge Acquisition. Information Processing &amp; Management, 25(4):419-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>G Lapalme</author>
</authors>
<title>Concept Identification and Presentation in the Context of Technical Text Summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization. ANLPNAACL2000,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, WA, USA,</location>
<contexts>
<context position="22935" citStr="Saggion and Lapalme, 2000" startWordPosition="3668" endWordPosition="3671">cting (CBA) (Paice and Oakes, 1999) where shallow processing is used to instantiate a semantic frame containing the most important concepts of the source document. Unlike CBA, which produces a short textual abstract, our system produces a set of text passages and instantiates a number of slots (e.g. company, number of employees, etc.) which are used by an information analyst to produce health and safety reports. Our rules for passage extraction rely on concept co-occurrence, and so are similar to the contextual templates employed in other IE approaches to summarization (Paice and Jones, 1993; Saggion and Lapalme, 2000). 6 Conclusion and Future Work In this paper we have presented how GATE was used as a development environment for a summarization system. Our focus was on the adaptation of general natural language engineering tools to an specific summarization problem. While we have relied completely on IE for the purpose of this application, GATE also allows the programmer to easily deploy methods within the framework for computing surface level indicators of salience. In fact, several such GATEbased modules (e.g., sentence position, t f * idf ) are currently being developed by one of the authors. References</context>
</contexts>
<marker>Saggion, Lapalme, 2000</marker>
<rawString>H. Saggion and G. Lapalme. 2000. Concept Identification and Presentation in the Context of Technical Text Summarization. In Proceedings of the Workshop on Automatic Summarization. ANLPNAACL2000, Seattle, WA, USA, 30 April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
<author>J R Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review.</title>
<date>1995</date>
<journal>Number</journal>
<booktitle>in Lecture Notes in Artificial Intelligence.</booktitle>
<volume>1083</volume>
<publisher>Springer.</publisher>
<marker>Jones, Galliers, 1995</marker>
<rawString>K. Sparck Jones and J.R. Galliers. 1995. Evaluating Natural Language Processing Systems: An Analysis and Review. Number 1083 in Lecture Notes in Artificial Intelligence. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Tablan</author>
<author>C Ursu</author>
<author>K Bontcheva</author>
<author>H Cunningham</author>
<author>D Maynard</author>
</authors>
<title>A unicode-based environment for creation and use of language resources.</title>
<date>2002</date>
<booktitle>In Proceedings of 3rd Language Resources and Evaluation Conference. forthcoming.</booktitle>
<marker>Tablan, Ursu, Bontcheva, Cunningham, Maynard, 2002</marker>
<rawString>V. Tablan, C. Ursu, K. Bontcheva, H. Cunningham, D. Maynard, 0. Hamza, Tony McEnery, Paul Baker, and Mark Leisher. 2002. A unicode-based environment for creation and use of language resources. In Proceedings of 3rd Language Resources and Evaluation Conference. forthcoming.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>