<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9979255">
The complexity of finding the maximum spanning DAG and other
restrictions for DAG parsing of natural language
</title>
<author confidence="0.996446">
Natalie Schluter
</author>
<affiliation confidence="0.919238333333333">
Center for Language Technology
University of Copenhagen
Copenhagen, Denmark
</affiliation>
<email confidence="0.996242">
natalie.elaine.schluter@jur.ku.dk
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998444">
Recently, there has been renewed interest in
semantic dependency parsing, among which
one of the paradigms focuses on parsing di-
rected acyclic graphs (DAGs). Considera-
tion of the decoding problem in natural lan-
guage semantic dependency parsing as find-
ing a maximum spanning DAG of a weighted
directed graph carries many complexities. In
particular, the computational complexity (and
approximability) of the problem has not been
addressed in the literature to date. This pa-
per helps to fill this gap, showing that this
general problem is APX-hard, and is NP-hard
even under the planar restriction, in the graph-
theoretic sense. On the other hand, we show
that under the restriction of projectivity, the
problem has a straightforward O(n3) algo-
rithm. We also give some empirical evidence
of the algorithmic importance of these graph
restrictions, on data from the SemEval 2014
task 8 on Broad Coverage Semantic Depen-
dency Parsing.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996656097561">
Consideration of the decoding problem in natural
language semantic dependency parsing as finding
a maximum spanning DAG of a weighted directed
graph carries many complexities that have not been
addressed in the literature to date. Amongst these
are the problem’s computational complexity (and its
approximability). The decoding problem for seman-
tic dependency parsing was first introduced as the
maximum spanning directed acyclic graph problem
(MSDAG) by McDonald and Pereira (2006), where
it is stated to be NP-hard.1 The MSDAG problem
asks for the highest weighted spanning sub-DAG of
an input weighted digraph.
In this paper, we explain the APX-hardness of
MSDAG, by relating it to the almost identical min-
imum weighted feedback arc set and maximum
weighted acyclic subgraph problems. The proof
of MSDAG’s APX-hardness seems to discourage
its use for decoding in semantic dependency pars-
ing. However, unlike in syntactic dependency (tree)
parse decoding, where projective decoding given
by Eisner (1996)’s algorithm has a slightly higher
computational complexity (O(n3)) than the non-
projective (Tarjan) maximum spanning tree algo-
rithm (O(n2)) (Tarjan, 1977; Chu and Liu, 1965;
Edmonds, 1967; McDonald et al., 2005), finding the
maximum spanning projective dependency DAG is
tractable and can be found in time O(n3), contrary
to its APX-hard non-projective counterpart.
Projective MSDAG has been referred to in the
semantic dependency parsing literature as “planar
MSDAG”, which is an unfortunate mismatch with
long-established graph theoretical terminology, that
we need in this paper. As we discuss below, the
planar MSDAG problem is in fact NP-hard, where
“planar” is used in the graph theoretical sense. We
generalise the definition of projectivity from tree
models of syntax theory, which forbids crossing
edges, to digraphs.
The projectivity restriction itself has not been lin-
guistically motivated to date. However, an efficient
</bodyText>
<note confidence="0.665978">
1McDonald and Pereira (2006) provide a reference to
(Heckerman et al., 1995) for this fact. This fact is actually indi-
rectly shown much earlier, as we discuss in Section 4.
</note>
<page confidence="0.957047">
259
</page>
<note confidence="0.9543665">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 259–268,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.985347029850746">
exact algorithm for this restriction would pose a Satta, 2007; Carreras, 2007) from the tree decoding
starting point for various relaxations of the definition literature, based on the algorithm presented here is
(also known as mild non-projectivity) that reflect lin- straightforward. And we believe that it is these lat-
guistic description of the data, as has been done for ter algorithms, rather, that would provide the basis
the Eisner algorithm (in for example (Bodirsky et for empirical studies based on the generally theoret-
al., 2005; G´omez-Rodr´ıguez et al., 2011; Pitler et ical research presented here.
al., 2012; Pitler et al., 2013; Satta and Kuhlmann, 2 Preliminaries
2013)). This projectivity restriction has been inher- A graph is called planar if it can be drawn in the
ent in transition-based approaches to semantic de- plane with no crossing edges. Each maximal region
pendency parsing (for example, in (Sagae and Tsujii, of the plane surrounded by edges of the planar graph
2008; Titov et al., 2009)), without any study of the drawn in the plane is called a face. There is one
complexity nor proof of the power of the automaton outer or unbounded face and some number of in-
models in recognising all projective DAGs. So, in ner or bounded faces. If a connected planar graph
terms of computational efficiency, we provide the- can be written in the plane so that all vertices are on
oretical justification for the already used restriction the outer face, then we call the graph outerplanar.
of projectivity in DAG parsing, exhibiting a dynamic A connected component of a graph is a maximal
programming algorithm for this task, which runs in subgraph in which any two vertices are connected
polynomial time. to each other by a path. A digraph is a directed
Previous automaton approaches to DAG parsing graph (where edges have an orientation). A DAG
can roughly be separated into two camps: one simi- is a digraph without any directed cycles. Consider
lar to (Sagae and Tsujii, 2008), which assumes pro- the underlying undirected graph H of a digraph G.
jectivity of graphs in the data and parses without car- A weakly connected component of G is a maximal
rying out any transformation to relax the constraint sub-digraph whose underlying undirected graph is a
of projectivity, and another similar to (Titov et al., connected component of H.
2009), which attempts (online) to find a re-ordering Notation. We put [i, j] := {i, i + 1, ... , j − 1, j},
of the words in the sentence such that the resulting for i G j, and [i] := [1, i].
graph is projective. The latter approach assumes, In this paper, edge weights can be positive or neg-
as we will explain, precisely outerplanarity of the ative and not zero, unless otherwise stated.
graphs, which, it turns out, is also NP-hard (Cf.
§2 and §7). With respect to the data that we con-
sider here, it turns out that the assumption of graph
outerplanarity is well represented (almost all graphs
among three data sets being outerplanar), whereas
the percentage of projective graphs differs greatly
from one dataset to another (from 57% to 84%, Cf.
Section 5).
The projective MSDAG algorithm presented here
is a first-order decoding algorithm, and empirical re-
search on semantic parsing seems to have already
gone beyond this (for example, in (Martins and
Almeida, 2014)); moreover, first-order decoding us-
ing MSDAG in general seems not to be appropri-
ate (Schluter, 2014), though the empirical work pre-
sented by Martins and Almeida (2014) on digraph
decoding using a second order model suggests the
relevance of higher-order DAG decoding in seman-
tic dependency parsing. Manufacturing higher-order
parsing algorithms in the sense of (McDonald and
260
Hardness of approximability. APX is the class of
all NP optimisation problems that can be solved in
polynomial time with approximation ratio bounded
by some constant. A problem is APX-hard if there
is a PTAS-reduction from every problem in APX
to that problem.2 In this paper, we use a simpler
type of PTAS-reduction called an L-reduction (lin-
ear reduction), which intuitively is a mapping be-
tween problems so that (approximate) solutions dif-
fer only by some constant factor. Any L-reduction is
also a PTAS-reduction (but not vice versa).
Definition 1. An L-reduction from problem A to
problem B with respective cost functions cA and
2A discussion of PTAS-reductions is out of the scope of this
paper. The definition of a PTAS-reduction can be found, for
example, in (Wegener, 2005).
cB consists of a pair ofpolynomial-time computable
functions f and g such that:
</bodyText>
<listItem confidence="0.861739461538461">
• if x ∈ A then f(x) ∈ B,
• if y is a solution to f(x), then g(y) is a solution
to x,
• there exists a positive constant α such that the
optimal solution for f(x) ∈ B (optB(f(x))) is
bounded by a factor α of the optimal solution
for x ∈ A (optA(x)), and
optB(f(x)) ≤ α · optA(x)
• there exists a positive constant β bounding dif-
ferences between solutions and optimal solu-
tions
|optA(x)−cA(g(y)) |≤ β·|optB(f(x))−cB(y)|.
3 Generalising projectivity to DAGs
</listItem>
<bodyText confidence="0.919205447368421">
In statistical natural language syntactic or semantic
dependency parsing decoding problems, the input is
a sequence of n words, W = hw1, ... , wni, called
the sentence and a further set of weighted asymmet-
ric binary relations (directed edges) between these
words (nodes). The task is to output a most likely
connected and spanning digraph over those words,
where the formal expressivity of the structure is de-
fined with respect to the linguistic theory in ques-
tion.
The order of words in the sentence is essential
for the description of important restrictions of trees
and DAGs for natural language. We therefore in-
clude the order in the sentence digraph structure, so
GW = (V, E, ≤W) is a dependency digraph for the
sentence W, where V is the set of words tokens, E
is the set of directed binary relations between words,
and ≤W describes the order of the words in the sen-
tence W (≤W is the sentential order). For the re-
mainder of this paper, when we talk about depen-
dency digraphs (or dependency DAGs or dependency
trees) the nodes of the underlying digraphs are asso-
ciated with some fixed total order. Also we use the
terms “word” and “node” synonymously in this con-
text.
In linguistic terms, if (wi, wj) is an edge, then we
say that wi is a head of wj and that wj is a dependent
of wi. We can also write the edge (wi, wj) as wi →
−→ wj is the reflexive transitive closure of
∗
the dependency relation.
A dependency tree then is just a connected de-
pendency digraph in which every node has a unique
head, except for a special node called the root, which
has no head.
An interesting property yielding good coverage
of some natural languages (for example, English) is
that resulting dependency trees should be projective.
Definition 2. A dependency tree T = (V, E, ≤W
) is projective if for all edges (wi, wj) ∈ E,
for all intervening words, wk such that k ∈
[min{i, j}, max{i, j}], we have wi−→ wk.
∗
It turns out that the edges of a projective depen-
dency tree can be written above the sentence (i.e.,
words written on a line segment in sentential order)
without any crossing edges. This notion of avoiding
crossing edges in “desirable” spanning DAGs has
been considered in recent natural language parsing
research, however the projectivity of edges as given
in Definition 3 for dependency trees is no longer a
sufficient condition to ensure this property in DAGs.
As such, NLP researchers have adopted the unfortu-
nate term “planar”. Rather than assign a new mean-
ing to the term planar, we generalise the definition
of projectivity using the notion of crossing edges so
that it applies to dependency digraphs, adopting this
definition for the remainder of the paper. We then
provide the correct restriction of planar digraphs that
projective digraphs correspond to.
Definition 3. For a dependency digraph GW =
(V, E, ≤W), an edge (wi, wj) or (wj, wi), with i ≤
j is projective if and only if for all words wk such
that i &lt; k &lt; j, there are no edges (wl, wk) or
(wk, wl) such that l &lt; i or l &gt; j.
GW is projective if and only if all its edges are pro-
jective.
Since the definition of projectivity excludes cross-
ing edges when nodes are laid out on a line segment
(sentential order), we see that the underlying digraph
of a projective digraph is outerplanar. Moreover, it
is easy to prove that for any planar digraph with all
nodes on its outer face, we can choose a first node
and define an ordering ≤W on the nodes by follow-
ing the order of nodes (in a fixed direction) along an
outer face, skipping repeats, until the original node
</bodyText>
<equation confidence="0.389422">
wj. wi
</equation>
<page confidence="0.940264">
261
</page>
<bodyText confidence="0.988338944444445">
is met (recalling that one can find this outer face in
linear time). So, there is a correspondence between
the sets of projective digraphs and outerplanar di-
graphs.
Proposition 4. For the projective digraph GW =
(V, E, &lt;W), (V,E) is an outerplanar digraph.
Also, every outerplanar digraph corresponds to
some projective digraph with a sentential ordering
defined by node traversal in a fixed direction along
its outer face.
On the other hand, given an outerplanar graph and
some random sentential order, we of course do not
necessarily have a projective digraph. In particu-
lar, an outerplanar drawing in the plane of a digraph
does not necessarily have the specific desired order
of vertices on its outer face. So, for example, finding
the outerplanar MSDAG and the projective MSDAG
are two different problems.
</bodyText>
<sectionHeader confidence="0.56688" genericHeader="method">
4 APX-hardness of MSDAG and its dual
</sectionHeader>
<bodyText confidence="0.999871095890411">
In this section we give L-reductions from the APX-
hard problems maximum weighted directed acyclic
subgraph and its dual minimum weighted feedback
set to MSDAG and its dual.
Minimum weighted feedback arc set. The dual
problem of the MSDAG problem is almost identi-
cal with that of finding a minimum weighted feed-
back arc set. Given a directed graph, G = (V, E),
a feedback arc set (FAS) is a subset S of G’s
edges whose removal leaves a DAG (i.e., such that
(V, E(G)\E(S)) is a DAG). A minimum feedback
arc set (MFAS) is the smallest among all possible
feedback arc sets and a minimum weighted feedback
arc set (MWFAS) is a feedback arc set of minimum
weight. We call an MWFAS whose removal leaves a
connected DAG a nice MWFAS. Finding a nice MW-
FAS is the dual of the MSDAG problem.
Already, the decision version of the FAS problem,
which asks whether there is a feedback arc set of
size k was listed as one of Karp’s original 21 NP-
complete problems (Karp, 1972), which shows the
NP-hardness of the optimisation version. In fact,
this optimisation problem is also shown to be APX-
hard (Kann, 1992). The best approximation algo-
rithm in the literature to date for this problem has an
approximation guarantee of O(log n log log n) and
the solution is NP-hard to approximate to within any
factor smaller than 10.\/5 − 21 pt� 1.36 (Even et al.,
1998).
An L-reduction from MWFAS to MSDAG for a
weighted directed digraph G goes as follows. We
first show that if the G has no edge cut consist-
ing only of edges of negative weights, then the nice
MWFAS is simply the MWFAS. Let D* be an MS-
DAG for G. We show that the nice MWFAS F :=
E(G) − E(D*) must also form an MWFAS. Sup-
pose otherwise, then there is some other F&apos; of lower
weight and such that D&apos; := (V (G), E(G) − F&apos;) is
acyclic. So D&apos; is disconnected and has at least two
weakly connected components; we suppose without
loss of generality that it has precisely two weakly
connected components, C1 and C2. But then there
must not be any positive weighted edge e in E(G)
between C1 and C2; otherwise we could remove
that edge from F&apos; to achieve an MWFAS of lower
weight, since C1 U C2 U {e} is still acyclic.
Now suppose that there is some edge cut con-
sisting solely of edges of negative weight in G and
therefore in D*. Without loss of generality, we can
suppose that there is only one such edge cut. Clearly,
D* contains at most one edge from this edge cut,
the rest of them being in the nice MWFAS. An MW-
FAS would contain every negative weighted edge.
So, the difference between a nice MWFAS and an
MWFAS is just this one edge, for G. In particu-
lar, we have given an L-reduction from MWFAS to
nice MWFAS, where the optimal solution for MW-
FAS is just the nice MWFAS with negative weighted
edges removed (so both α and Q from Definition 1
are equals to 1).
This shows the APX-hardness of the dual of the
MSDAG problem, nice MWFAS.
Maximum weighted directed acyclic subgraph.
An almost identical problem to MSDAG is the max-
imum weighted directed acyclic subgraph problem
(MWDAS), which aims to find the (not necessar-
ily spanning) DAG of highest weight. The decision
version of the maximum directed acyclic subgraph
problem (MDAS) problem which asks whether there
is a directed acyclic subgraph on k edges can be
solved by the decision version of the FAS prob-
lem, with m − k as the parameter (where m is the
number of edges in the input graph), and hence has
</bodyText>
<page confidence="0.983973">
262
</page>
<bodyText confidence="0.990843354166667">
long been shown to be NP-complete (Karp, 1972).
By the same token, the optimisation problem has
been shown to be APX-hard (Kann, 1992). The
best approximation algorithm in the literature to date
for this problem has an approximation guarantee of
2 + Ω \ √dmaxwhere dmax is the maximum vertex
degree for the graph (Berger and Shor, 1997). More-
over, the solution is Unique Games-hard to approxi-
mate to within any factor smaller than 1/2, which is
a tight bound (Guruswami et al., 2008).
Again, if there are no edge cuts in the graph con-
sisting only of negative weighted edges, then the
MSDAG and MWDAS solutions are identical. Oth-
erwise, the MWDAS is just the MSDAG without its
negative weighted edges, by the discussion above L-
reducing MWFAS to nice MWFAS.
We have therefore shown the following fact.
Theorem 5. MSDAG and its dual, nice MWFAS, are
APX-hard.
5 Planarity, outerplanarity and
projectivity of DAGs in English data
APX-hardness of the MSDAG problem not only
means that the problem is essentially infeasible, but
also that it theoretically cannot be very well approxi-
mated. However, with some structural assumptions,
such as the planarity, outerplanarity or projectivity
of the objective DAGS, either approximation algo-
rithms with a good approximation guarantee or even
feasible algorithms might be achievable. The au-
thors are not aware of any specific theoretical lin-
guistic evidence for the planarity or outerplanarity
of semantic dependency DAGs. However, we con-
sider the three datasets from SemEval 2014 task
8 on Broad Coverage Semantic Dependency Pars-
ing (Oepen et al., 2014), referred to as PAS, DM
and PCEDT, following the packing conversion into
DAGs described in (Schluter et al., 2014), without
the actual label packing or edge removal heuristic,
finding that almost all DAGs are outerplanar (and
therefore also planar). Moreover, the datasets con-
sist of a majority of projective DAGs, with those
graphs that are not projective having proportion-
ally on average a large projective subgraph (respec-
tively 0.555, 0.580,and 0.605 for the PAS, DM, and
PCEDT datasets). This large projective proportion
suggests future avenues for algorithms for mildly
projective DAGs, based on the projective algorithm
presented in this paper.
</bodyText>
<table confidence="0.99855025">
PAS DM PCEDT
% projective 58.461 56.840 84.192
% outerplanar 97.796 99.160 95.390
% planar 99.997 100 99.904
</table>
<tableCaption confidence="0.996159">
Table 1: Percentage of projective, planar, and outerplanar
DAGs in the data.
</tableCaption>
<sectionHeader confidence="0.975506" genericHeader="method">
6 The NP-hardness of finding a planar or
outerplanar MSDAG
</sectionHeader>
<bodyText confidence="0.998733">
By a similar discussion to that in Section 4 on
the relationship between MWDAS and MSDAG,
finding the both the maximum weighted planar or
outerplanar spanning DAG of a directed weighted
graph can simply be shown to be NP-hard, where
“planar” (and “outerplanar”) is used in the graph-
theoretical sense (Garey and Johnson, 1979). To our
knowledge, approximability of finding a maximum
weighted planar or outerplanar acyclic directed sub-
graph is still an open problem. The NP-hardness of
finding a maximum weighted outerplanar spanning
DAG is somewhat discouraging. But the next sec-
tion provides a polynomial algorithm if there is a re-
striction on the the order of nodes on the outer face
of the output (which is just projective MSDAG).
7 Finding a projective MSDAG or digraph
in O(n3) time
We now turn our attention to the projective MS-
DAG problem, which can be solved efficiently. Our
algorithm employs bottom-up dynamic program-
ming across spans of words, where a span con-
sists of a segment of words of the input sentence
W = (w1, w2, ... , wn) along with any attributed
edges, similarly to the CKY-algorithm for context-
free language parsing (Cocke, 1969; Kasami, 1965;
Younger, 1967) and projective maximum spanning
tree algorithms (Eisner, 1996), though the proof of
correctness is slightly more complex. Following
this, we explain how to simplify the algorithm to the
task of finding the maximum weighted spanning di-
graph of digraph within the same time complexity.
Let GW = (V, E, ≤W) be a weighted de-
pendency digraph over the input sentence W =
</bodyText>
<page confidence="0.99631">
263
</page>
<bodyText confidence="0.984857514285714">
(w1, w2, ... , wn) and we suppose without loss of (A3) Connect two sub-DAGs Hai,k and Hak+1,j from
generality that |E |= n(n −1). An (i, j)-span (with among the graphs in the cells ai,k and ak+1,j
i &lt; j) for S is the subsequence of consecutive words respectively (i &lt; k &lt; j) with a single edge
wi, wi+1, ... , wj−1, wj. We construct an algorithm e E {(i, j), (j, i)}, creating the graph
proj-MSDAG (Algorithm 1) which takes GW as in- ({wi, ... , wj}, {e} U E(Hai,k) U E(Hak+1,j)).
put and outputs a highest weighted projective span- Lemma 6. The projective DAGs ai,j.G1, ai,j.G2,
ning dependency DAG for GW: a projective MS- and ai,j.G3 can be de-constructed, by reversing a
DAG for GW. For simplicity, instead of GW, we single operation (A1), (A2) or (A3), to obtain two
just write G. sub-DAGs D1 and D2 which span their vertices and
We call a directed path from node i to node j, an where either
i − j path. The algorithm constructs the upper tri- • D1 is on vertices {wi, ... , wk} and D2 is on
angular square matrix A = {ai,j} (i, j E [n]), from vertices {wk, ... , wj}, for i &lt; k &lt; j, or
left to right and from the diagonal upwards, where • D1 is on vertices {wi, ... , wk} and D2 is on
component ai,j contains at most three different re- vertices {wk+1, ... , wj}, for i &lt; k &lt; j.
strictions of optimal projective spanning DAGs for Furthermore, for p E [2] and 1 &lt; a &lt; b &lt; n, let
the (i, j) span along with their associated weights: G[a, b] be the spanning subgraph of G on the ver-
1. ai,j.G1 is a projective MSDAG of the spanning tices {wa, wa+1, ... , wb−1, wb}, let Dp be a DAG
subgraph G[i, j], and ai,j.w1 is its weight on the vertices wa, wa+1, ... , wb−1, wb.
2. ai,j.G2 is a maximum projective spanning 1. Dp is a projective MSDAG for G[a, b], or
DAG for which there is no i − j path, and 2. Dp is the a highest projective spanning DAG
ai,j.w2 is its weight, and with no a − b path for G[a, b], or
3. ai,j.G3 is a maximum projective spanning 3. Dp is the a highest projective spanning DAG
DAG for which there is no j − i path, and with no b − a path for G[a, b].
ai,j.w3 is its weight. Proof. We separate the proof into two parts: Part
The solution to the problem is then a1,n.G1. The 1 for the graph ai,j.G1 and Part 2 for the graphs
motivation for distinguishing between these three ai,j.G2, and ai,j.G3.
types of graphs is to allow restricted combinations Part 1. Let us denote ai,j.G1 by D for ease in nota-
of them which ensure that no cycles are introduced. tion. We denote by ED(u) the set of edges in E(D)
We claim that these three restrictions on projec- having u for some endpoint. Consider the word wi.
tive MSDAGs for the span (i, j) can be constructed By D’s connectivity, ED(wi) is non-empty. Let e
from those of shorter spans within (i, j) using the be the edge in ED(wi) of longest span, and sup-
three following operations, (A1), (A2), and (A3) pose without loss of generality in edge direction that
(Lemma 6). e = (wi, wk) for some k E {i + 1, ... , j}.
(A1) Concatenate sub-DAGs Hai,k and Hak,j from There are two cases to consider. Either k &lt; j in
among the graphs in the cells ai,k and ak,j re- which case we can reverse (A1) (Case 1), or k = j
spectively (i &lt; k &lt; j), creating the graph in which case we can reverse (A2) or (A3) (Case 2).
({wi, ... , wj}, E(Hai,k) U E(Hak,j)), Consider first Case 1, where k &lt; j. By D’s
(A2) Concatenate a single edge e E {(i, j), (j, i)} projectivity and the fact that e has the longest span
with the sub-DAGs Hai,k and Hak,j from in ED(w1), there are no edges with one endpoint
among the graphs in the cells ai,k and ak,j re- among wi, ... , wk−1 and the other endpoint among
spectively (i &lt; k &lt; j), creating the graph wk+1, ... , wj. So we can partition D into two
({wi, ... , wj}, {e} U E(Hai,k) U E(Hak,j)),
264
sub-DAGs D[i, k], which is a spanning subgraph
over the nodes wi, ... , wk and D[k, j], which is
a spanning subgraph over the nodes wk,..., wj.
Both D[i, k] and D[k, j] are projective MSDAGs for
G[i, k] and G[k, j] respectively, otherwise we can
construct a projective MSDAG D&apos; for G of higher
weight than D, by taking the respective projective
MSDAGs to form D&apos;.
Otherwise, we have Case 2, with k = j. Note that
there must not be any wj −wi path in D (for acyclic-
ity). We remove the edge (wi, wj) from D, the result
of which is either connected or disconnected.
If D − {(wi, wj)} is connected, it must be the
maximum spanning DAG for G not containing any
wj − wi path. In the same manner as for the case
where k &lt; j, we can partition D − {(wi,wj)}
into two sub-DAGs D[i, k], which is a spanning sub-
DAG over the nodes wi, ... , wk and D[k, j], which
is a spanning sub-DAG over the nodes wk,... , wj,
where either D[i, k] does not have a k − i path or
D[k, j] does not have a j − k path. Clearly D[i, k]
and D[k, j] are the maximum weighted projective
spanning DAGs with this property for G[i, k] and
G[k, j] respectively. This is the reversal of (A2).
Otherwise D − {(wi, wj)} is disconnected into
two weakly connected components D[i, k] and
D[k + 1,j], with k E [j − 1], such that D[i, k] is
a projective MSDAG for G[i, k] and D[k + 1, j] is
a projective MSDAG for G[k + 1, j] (which is the
reversal of (A3)).
Part 2. We prove, without loss of generality, the
result for the graph ai,j.G3, the proof for the graph
ai,j.G2 being symmetric. The proof follows almost
exactly the one in Part 1, so we simply indicate the
differences here. Again, for ease in notation, we de-
note ai,j.G2 by D, and carry out the same partition
of D as in Part 1.
The difference for Case 1 is that either the result-
ing D[i, k] must be a maximum weighted projec-
tive spanning DAG for G[i, k] with no k − i path,
or D[k, j] must be a maximum weighted projective
spanning DAG for G[k, j] with no j − k path.
Case 2 is precisely the same if the result of remov-
ing the edge (wi, wj) is disconnected. If the result
is connected, then the difference is that D[i, k] must
be a maximum projective weighted spanning DAG
for G[i, k] with no k − i path, or D[k, j] must be
a maximum weighted projective spanning DAG for
G[k, j] with no j − k path.
Algorithm 1 uses the operations (A1), (A2) and
(A3) to fill the matrix A. In Figure 1, we define
three different subroutines corresponding to each of
these operations, which take the matrix A, the span,
and the forbidden direction for the edge of the span,
if there is one (and the empty set otherwise). Clearly
each of these runs in O(n) time.
We observe that Lines 9 through 14 in Algo-
rithm 1 dominate the time complexity, taking O(n3).
Lemma 7 shows that Algorithm 1 fills the table cells
with the correct projective MSDAG restrictions.
Lemma 7. For i &lt; j, Algorithm 1 fills matrix entry
ai,j with:
</bodyText>
<listItem confidence="0.9986132">
1. a projective MSDAG for G[i, j],
2. a projective MSDAG for G[i, j] with no i − j
path, and
3. a projective MSDAG for G[i, j] with no j − i
path.
</listItem>
<bodyText confidence="0.9571978">
Proof. The proof is by induction on the span size.
For the table cells ai,i for i E [n], we have ai,i.G1 =
ai,i.G2 = ai,i.G3 = ({wi}, 0); the graphs are all
single nodes. For our base case, ai,i+1, i E [n − 1],
we construct the cell contents as follows:
</bodyText>
<listItem confidence="0.55547075">
1. ai,i+1.G1 is the highest weighted edge among
(wi, wi+1) and (wi+1, wi).
2. ai,i+1.G2 is (wi, wi+1) if this edge is in G.
3. ai,i+1.G3 is (wi+1, wi) if this edge is in G.
</listItem>
<bodyText confidence="0.993445">
This is Lines 1 through 8 in Algorithm 1.
Suppose now that the entries ai,k and ak,j con-
tain the appropriate graphs G1, G2 and G3, for
i &lt; k &lt; j. For each k (i &lt; k &lt; j), Lines
9 through 14 in Algorithm 1 construct the three
graphs, ai,j.G1(k),ai,j.G2(k), and ai,j.G3(k), as
follows, which is possible according to Lemma 6:
</bodyText>
<listItem confidence="0.950361571428571">
1. ai,j.G1(k) is a highest weighted projective de-
pendency DAG for G[i, j] that can be con-
structed from the graphs stored in ai,k and ak,j.
2. ai,j.G2(k) is a highest weighted projective de-
pendency DAG for G[i, j] that can be con-
structed from the graphs stored in ai,k, ai,k+1
and ak,j, which avoids an i − j path.
</listItem>
<page confidence="0.987669">
265
</page>
<reference confidence="0.357844894736842">
A1(G, A, i, j, ∅) = arg max w(H)
{H|H=({wi,...,wj},E(ai,k.G1)UE(ak,j.G1)), i&lt;k&lt;j}
A1(G, A, i, j, →) = arg max w(H)
{H|H=({wi,...,wj},E(ai,k.G2)UE(ak,j.G1)), i&lt;k&lt;j}
U{H|H=({wi,...,wj},E(ai,k.G1)UE(ak,j.G2)), i&lt;k&lt;j}
A1(G, A, i, j, ←) = arg max w(H)
{H|H=({wi,...,wj},E(ai,k.G3)UE(ak,j.G1)), i&lt;k&lt;j}
U{H|H=({wi,...,wj},E(ai,k.G1)UE(ak,j.G3)), i&lt;k&lt;j}
A2(G, A, i, j, ∅) = arg max w(H)
{H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G1)UE(ak,j.G1)), i&lt;k&lt;j}
A2(G, A, i, j, →) = arg max w(H)
{H|H=({wi,...,wj}, {(wj,wi)}UE(ai,k.G2)UE(ak,j.G1)), i&lt;k&lt;j}
U{H|H=({wi,...,wj}, {(wj,wi)}UE(ai,k.G1)UE(ak,j.G2)), i&lt;k&lt;j}
A2(G, A, i, j, ←) = arg max w(H)
{H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G3)UE(ak,j.G1)), i&lt;k&lt;j}
U{H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G1)UE(ak,j.G3)), i&lt;k&lt;j}
A3(G, A, i, j, ∅) = arg max w(H)
{H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G1)UE(ak+1,j.G1)), i&lt;k&lt;j}
A3(G, A, i, j, →) = arg max w(H)
</reference>
<construct confidence="0.9702278">
{H|H=({wi,...,wj}, {(wj,wi)}UE(ai,k.G2)UE(ak+1,j.G1)), i&lt;k&lt;j}
U{H|H=({wi,...,wj}, {(wj,wi)}UE(ai,k.G1)UE(ak+1,j.G2)), i&lt;k&lt;j}
A3(G, A, i, j, ←) = arg max w(H)
{H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G3)UE(ak+1,j.G1)), i&lt;k&lt;j}
U{H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G1)UE(ak+1,j.G3)), i&lt;k&lt;j}
</construct>
<figureCaption confidence="0.993633">
Figure 1: Subroutines corresponding to the operations (A1), (A2), and (A3) as used by Algorithm 1.
</figureCaption>
<bodyText confidence="0.419489">
Algorithm 1 proj-MSDAG(G)
</bodyText>
<listItem confidence="0.8898056">
1: for i ← 1,...,n do
2: ai,i.G1 ← ai,i.G2 ← ai,i.G3 = ({i}, ∅)
3: if i &lt; n then
4: ai,i+1.G1 ← ({wi, wi+1}, {arg maxeE{(wi,wi+1),(wi+1,wi)} w(e)})
5: ai,i+1.G2 ← ({wi, wi+1}, {(wi+1, wi)})
6: ai,i+1.G3 ← ({wi, wi+1}, {(wi, wi+1)})
7: end if
8: end for
9: for i ← 1,...,n do
10: for j ← i − 1,...,1 do
</listItem>
<figure confidence="0.476958">
11: ai,j.G1 ← argmaxHE{A1(G,A,i,j,O),A2(G,A,i,j,O),A3(G,A,i,j,O)} w(H()
j
12: ai.G2 ← arg maxHE{A1(G,A,i,j,→),A2(G,A,i,j,→),A3(G,A,i,j,→)} w(H)
13: ai,j.G3 ← arg maxHE{A1(G,A,i,j,←),A2(G,A,i,j,←),A3(G,A,i,j,←)} w(H)
14: end for
15: end for
16: return a1,n.G1
</figure>
<footnote confidence="0.5034355">
3. ai,j.G3(k) is a highest weighted projective de- structed from the graphs stored in ai,k, ai,k+1
pendency DAG for G[i, j] that can be con- and ak,j, which avoids an j − i path.
</footnote>
<page confidence="0.991797">
266
</page>
<table confidence="0.783605208333333">
Y.J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396–
1400.
John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Technical Report
B0007F4UOA, Courant Institute of Mathematical Sci-
ences, New York University.
Jack Edmonds. 1967. Optimum branchings. J. Res. Nat.
Bur. Standards, 71B:233–240.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING-96), pages 340–345, Copenhagen, Den-
mark.
Guy Even, Joseph Naor, Baruch Schieber, and Madhu
Sudan. 1998. Approximating minimum feedback
sets and multicuts in directed graphs. Algorithmica,
20:151–174.
Michael R. Garey and David S. Johnson. 1979. Comput-
ers and Intractability: A Guide to the Theory of NP-
completeness. Freeman &amp; Co., San Francisco.
Carlos G´omez-Rodr´ıguez, John Carroll, and David Weir.
2011. Dependency parsing schemata and mildly non-
projective dependency parsing. Computational Lin-
guistics, 37:541–586.
Venkatesan Guruswami, Rajsekar Manokaran, and
Prasad Raghavendra. 2008. Beating the random or-
dering is hard: Inapproximability of maximum acyclic
subgraph. In Proc. of FOCS.
D. Heckerman, D. Geiger, and D. M. Chickering. 1995.
Learning bayesian networks: The combination of
knowledge and statistical data. Technical report, Mi-
crosoft Research. MSR-TR-94-09.
Viggo Kann. 1992. On the approximability on NP-
complete Optimization Problems. Ph.D. thesis, De-
partment of Numerical Analysis and Computing Sci-
ence, Royal Institute of Technology, Stockholm, Swe-
den.
Richard M. Karp. 1972. Reducibility among combinato-
rial problems. In Complexity of Computer Computa-
tions, Proc. Sympos. IBM Thomas J. Watson Res. Cen-
ter, pages 85–103, New York, NY.
Tadao Kasami. 1965. An efficient recognition and
syntax-analysis algorithm for context-free languages.
Technical Report AFCRL-65-758, Air Force Cam-
bridge Research Lab, Bedford, MA.
Andr´e F. T. Martins and Mariana S. C. Almeida. 2014.
Priberam: A turbo semantic parser with second order
features. In Proc of SemEval, Dublin, Ireland.
</table>
<bodyText confidence="0.927729">
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proc. of EACL, pages 81–88.
With Lemmata 6 and 7, the proof of correctness
of the following theorem is complete.
Theorem 8. There is an algorithm for finding a
projective MSDAG of a weighted digraph in O(n3)
time, where n is the number of word tokens of the
input sentence.
The proof of Theorem 8 can be simplified to adapt
it to that of projective digraphs. We simply substi-
tute the word “DAG” by “digraph”, and disregard all
graphs in table entries avoiding certain paths, since
cycles are permitted: in the entry ai,j, we need only
construct and record the graph ai,j.G1. We have thus
also shown the following fact.
Theorem 9. There is an algorithm for finding a pro-
jective maximum weighted spanning digraph of a
weighted digraph in O(n3) time, where n is the num-
ber of word tokens of the input sentence.
8 Concluding Remarks
Understanding the complexity of the problem of
finding a maximum spanning DAG as well as im-
portant restrictions provides a basis for both theo-
retical and empirical studies using restrictions or re-
laxations of the DAG parsing paradigm. We have
provided the first direct discussion of this problem’s
complexity, showing that the problem is APX-hard
as well as the first algorithm for the projective MS-
DAG problem proven to be exact and polynomial
time. Additionally, we briefly discussed the com-
plexity of finding a planar and outerplanar MSDAG,
the approximability of which remains open.
References
Bonnie Berger and Peter W. Shor. 1997. Tight bounds
for the maximum acyclic subgraph problem. Journal
</bodyText>
<reference confidence="0.997348152542373">
of Algorithms, 25:1–18.
Manuel Bodirsky, Marco Kuhlmann, and Mathias M¨ohl.
2005. Well-nested drawings as models of syntactic
structure. In Proc. of the Tenth Conference on FOr-
mal Grammar and Ninth Meeting on Mathematics of
Language.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of CoNLL
Shared Task Session of EMNLP-CoNLL, pages 957–
961, Prague, Czech Republic.
267
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In Proceedings of the Tenth International Confer-
ence on Parsing Technologies, pages 121–132, Prague,
Czech Republic, June.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP, pages 523–530, Vancouver, BC, Canada.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajiˇc, Angelina
Ivanova, and Yi Zhang. 2014. Semeval 2014 task
8: Broad-coverage semantic dependency parsing. In
Proc. of SemEval, pages 63–71, Dublin, Ireland.
Emily Pitler, Sampath Kannan, and Mitchell Mar-
cus. 2012. Dynamic programming for higher or-
der parsing of gap-minding trees. In Proceedings of
EMNLP/CoNLL, pages 478–488, Jeju Island, Repub-
lic of Korea.
Emily Pitler, Sampath Kannan, and Mitchell Marcus.
2013. Finding optimal 1-endpoint-crossing trees.
Transactions of the Association for Computational
Linguistics.
Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce de-
pendency dag parsing. In 22nd International Con-
ference on Computational Linguistics (Coling 2008),
Manchester, UK.
Giorgio Satta and Marco Kuhlmann. 2013. Efficient
parsing for head-split dependency trees. Trans. ACL,
pages 267–278.
Natalie Schluter, Anders Søgaard, Jakob Elming, Dirk
Hovy, Barbara Plank, Hector Martinez Alonso, Anders
Johanssen, and Sigrid Klerke. 2014. Copenhagen:
Tree approximations of semantic parsing problems. In
Proceedings of SemEval, Dublin, Ireland.
Natalie Schluter. 2014. On maximum spanning dag algo-
rithms for semantic dag parsing. In ACL 2014 Work-
shop on Semantic Parsing.
Robert Tarjan. 1977. Finding optimum branchings. Net-
works, pages 25–35.
Ivan Titov, James Henderson, Paola Merlo, and Gabrielle
Musillo. 2009. Online graph planarization for syn-
chronous parsing of semantic and synactic dependen-
cies. In Proceedings of IJCAI 2009, pages 1562–1567.
Ingo Wegener. 2005. Complexity Theory. Springer.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189– 208.
</reference>
<page confidence="0.996862">
268
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.289465">
<title confidence="0.9818955">The complexity of finding the maximum spanning DAG and restrictions for DAG parsing of natural language</title>
<author confidence="0.974642">Natalie</author>
<affiliation confidence="0.997823">Center for Language University of</affiliation>
<address confidence="0.6654">Copenhagen,</address>
<email confidence="0.995163">natalie.elaine.schluter@jur.ku.dk</email>
<abstract confidence="0.973935608695652">Recently, there has been renewed interest in semantic dependency parsing, among which one of the paradigms focuses on parsing directed acyclic graphs (DAGs). Consideration of the decoding problem in natural language semantic dependency parsing as finding a maximum spanning DAG of a weighted directed graph carries many complexities. In particular, the computational complexity (and approximability) of the problem has not been addressed in the literature to date. This paper helps to fill this gap, showing that this problem is and is even under the planar restriction, in the graphtheoretic sense. On the other hand, we show that under the restriction of projectivity, the has a straightforward algorithm. We also give some empirical evidence of the algorithmic importance of these graph restrictions, on data from the SemEval 2014 task 8 on Broad Coverage Semantic Dependency Parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>A1</author>
</authors>
<title>arg max w(H)</title>
<booktitle>H|H=({wi,...,wj},E(ai,k.G1)UE(ak,j.G1)), i&lt;k&lt;j} A1(G, A, i, j, →) = arg max w(H) {H|H=({wi,...,wj},E(ai,k.G2)UE(ak,j.G1)), i&lt;k&lt;j} U{H|H=({wi,...,wj},E(ai,k.G1)UE(ak,j.G2)),</booktitle>
<pages>i&lt;k&lt;j}</pages>
<marker>A1, </marker>
<rawString>A1(G, A, i, j, ∅) = arg max w(H) {H|H=({wi,...,wj},E(ai,k.G1)UE(ak,j.G1)), i&lt;k&lt;j} A1(G, A, i, j, →) = arg max w(H) {H|H=({wi,...,wj},E(ai,k.G2)UE(ak,j.G1)), i&lt;k&lt;j} U{H|H=({wi,...,wj},E(ai,k.G1)UE(ak,j.G2)), i&lt;k&lt;j}</rawString>
</citation>
<citation valid="false">
<authors>
<author>A1</author>
</authors>
<note>arg max w(H) {H|H=({wi,...,wj},E(ai,k.G3)UE(ak,j.G1)), i&lt;k&lt;j} U{H|H=({wi,...,wj},E(ai,k.G1)UE(ak,j.G3)), i&lt;k&lt;j</note>
<marker>A1, </marker>
<rawString>A1(G, A, i, j, ←) = arg max w(H) {H|H=({wi,...,wj},E(ai,k.G3)UE(ak,j.G1)), i&lt;k&lt;j} U{H|H=({wi,...,wj},E(ai,k.G1)UE(ak,j.G3)), i&lt;k&lt;j}</rawString>
</citation>
<citation valid="false">
<authors>
<author>A2</author>
</authors>
<title>arg max w(H) {H|H=({wi,...,wj},</title>
<booktitle>wi,wj)}UE(ai,k.G1)UE(ak,j.G1)), i&lt;k&lt;j} A2(G, A, i, j, →) = arg max w(H) {H|H=({wi,...,wj}, {(wj,wi)}UE(ai,k.G2)UE(ak,j.G1)), i&lt;k&lt;j} U{H|H=({wi,...,wj}, {(wj,wi)}UE(ai,k.G1)UE(ak,j.G2)),</booktitle>
<pages>i&lt;k&lt;j}</pages>
<marker>A2, </marker>
<rawString>A2(G, A, i, j, ∅) = arg max w(H) {H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G1)UE(ak,j.G1)), i&lt;k&lt;j} A2(G, A, i, j, →) = arg max w(H) {H|H=({wi,...,wj}, {(wj,wi)}UE(ai,k.G2)UE(ak,j.G1)), i&lt;k&lt;j} U{H|H=({wi,...,wj}, {(wj,wi)}UE(ai,k.G1)UE(ak,j.G2)), i&lt;k&lt;j}</rawString>
</citation>
<citation valid="false">
<authors>
<author>A2</author>
</authors>
<note>arg max w(H) {H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G3)UE(ak,j.G1)), i&lt;k&lt;j} U{H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G1)UE(ak,j.G3)), i&lt;k&lt;j</note>
<marker>A2, </marker>
<rawString>A2(G, A, i, j, ←) = arg max w(H) {H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G3)UE(ak,j.G1)), i&lt;k&lt;j} U{H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G1)UE(ak,j.G3)), i&lt;k&lt;j}</rawString>
</citation>
<citation valid="false">
<authors>
<author>A3</author>
</authors>
<title>i, j, ∅) = arg max w(H) {H|H=({wi,...,wj},</title>
<note>wi,wj)}UE(ai,k.G1)UE(ak+1,j.G1)), i&lt;k&lt;j} A3(G, A, i, j, →) = arg max w(H)</note>
<marker>A3, </marker>
<rawString>A3(G, A, i, j, ∅) = arg max w(H) {H|H=({wi,...,wj}, {(wi,wj)}UE(ai,k.G1)UE(ak+1,j.G1)), i&lt;k&lt;j} A3(G, A, i, j, →) = arg max w(H)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>