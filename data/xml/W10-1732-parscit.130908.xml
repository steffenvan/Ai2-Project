<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001931">
<title confidence="0.996437">
Hierarchical Phrase-Based MT at the Charles University
for the WMT 2010 Shared Task
</title>
<author confidence="0.994972">
Daniel Zeman
</author>
<affiliation confidence="0.985776">
Charles University in Prague, Institute of Formal and Applied Linguistics (ÚFAL)
</affiliation>
<address confidence="0.641072">
Univerzita Karlova v Praze, Ústav formální a aplikované lingvistiky (ÚFAL)
Malostranské náměstí 25, Praha, CZ-11800, Czechia
</address>
<email confidence="0.997181">
zeman@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.993847" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999924">
We describe our experiments with hier-
archical phrase-based machine translation
for WMT 2010 Shared Task. We provide
a detailed description of our configuration
and data so the results are replicable. For
English-to-Czech translation, we experi-
ment with several datasets of various sizes
and with various preprocessing sequences.
For the other 7 translation directions, we
just present the baseline results.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973514285714">
Czech is a language with rich morphology (both
inflectional and derivational) and relatively free
word order. In fact, the predicate-argument struc-
ture, often encoded by fixed word order in English,
is usually captured by inflection (especially the
system of 7 grammatical cases) in Czech. While
the free word order of Czech is a problem when
translating to English (the text should be parsed
first in order to determine the syntactic functions
and the English word order), generating correct in-
flectional affixes is indeed a challenge for English-
to-Czech systems. Furthermore, the multitude
of possible Czech word forms (at least order of
magnitude higher than in English) makes the data
sparseness problem really severe, hindering both
directions.
There are numerous ways how these issues
could be addressed. For instance, parsing and
syntax-aware reordering of the source-language
sentences can help with the word order differ-
ences (same goal could be achieved by a reorder-
ing model or a synchronous context-free grammar
in a hierarchical system). Factored translation, a
secondary language model of morphological tags
or even a morphological generator are some of the
possible solutions to the poor-to-rich translation is-
sues.
Our submission to the shared task should reveal
where a pure hierarchical system stands in this jun-
gle and what of the above mentioned ideas match
the phenomena the system suffers from. Although
our primary focus lies on English-to-Czech trans-
lation, we also report the accuracy of the same
system on moderately-sized corpora for the other
three languages and seven translation directions.
</bodyText>
<sectionHeader confidence="0.943466" genericHeader="method">
2 The Translation System
</sectionHeader>
<bodyText confidence="0.99947548">
Our translation system belongs to the hierarchi-
cal phrase-based class (Chiang, 2007), i.e. phrase
pairs with nonterminals (rules of a synchronous
context-free grammar) are extracted from sym-
metrized word alignments and subsequently used
by the decoder. We use Joshua, a Java-based open-
source implementation of the hierarchical decoder
(Li et al., 2009), release 1.1.1
Word alignment was computed using the first
three steps of the train-factored-phrase-
model.perl script packed with Moses2 (Koehn et
al., 2007). This includes the usual combination of
word clustering using mkcls3 (Och, 1999), two-
way word alignment using GIZA++4 (Och and
Ney, 2003), and alignment symmetrization using
the grow-diag-final-and heuristic (Koehn et al.,
2003).
For language modeling we use the SRILM
toolkit5 (Stolcke, 2002) with modified Kneser-
Ney smoothing (Kneser and Ney, 1995; Chen and
Goodman, 1998).
We use the Z-MERT implementation of mini-
mum error rate training (Zaidan, 2009). The fol-
lowing settings have been used for Joshua and Z-
MERT:
</bodyText>
<footnote confidence="0.9999874">
1http://sourceforge.net/projects/joshua/
2http://www.statmt.org/moses/
3http://fjoch.com/mkcls.html
4http://fjoch.com/GIZA++.html
5http://www-speech.sri.com/projects/srilm/
</footnote>
<page confidence="0.950615">
212
</page>
<note confidence="0.4154885">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 212–215,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<listItem confidence="0.992045090909091">
• Grammar extraction:
--maxPhraseLength=5
• Decoding: span_limit=10 fuzz1=0.1
fuzz2=0.1 max_n_items=30 rela-
tive_threshold=10.0 max_n_rules=50
rule_relative_threshold=10.0
• N-best decoding: use_unique_nbest=true
use_tree_nbest=false
add_combined_cost=true top_n=300
• Z-MERT: -m BLEU 4 closest -maxIt 5
-ipi 20
</listItem>
<sectionHeader confidence="0.650647" genericHeader="method">
3 Data and Pre-processing Pipeline
</sectionHeader>
<subsectionHeader confidence="0.991583">
3.1 Baseline Experiments
</subsectionHeader>
<bodyText confidence="0.963526310344827">
We applied our system to all eight language pairs.
However, for all but one we ran only a baseline ex-
periment. From the data point of view the baseline
experiments were even more constrained than the
organizers of the shared task suggested. We did not
use the Europarl corpus, we only used the News
Commentary corpus6 for training. The target side
of the News Commentary corpus was also the only
source to train the language model. Table 1 shows
the size of the corpus.
Corpus SentPairs Tokens xx Tokens en
cs-en 94,742 2,077,947 2,327,656
de-en 100,269 2,524,909 2,484,445
es-en 98,598 2,742,935 2,472,860
fr-en 84,624 2,595,165 2,137,407
Table 1: Number of sentence pairs and tokens for
every language pair in the News Commentary cor-
pus. Unlike the organizers of the shared task, we
stick with the standard ISO 639 language codes: cs
= Czech, de = German, en = English, es = Spanish,
fr = French.
Note that in some cases the grammar extraction
algorithm in Joshua fails if the training corpus con-
tains sentences that are too long. Removing sen-
tences of 100 or more tokens (per advice by Joshua
developers) effectively healed all failures. Unfor-
tunately, for the baseline corpora the loss of train-
ing material was still considerable and resulted in
drop of BLEU score, though usually insignificant.7
</bodyText>
<footnote confidence="0.9908224">
6Available for download at http://www.statmt.org/
wmt10/translation-task.html using the link “Parallel
corpus training data”.
7Table 1 and Table 2 present statistics before removing the
long sentences.
</footnote>
<bodyText confidence="0.998620222222222">
The News Test 2008 data set (2051 sentences
in each language) was used as development data
for MERT. BLEU scores reported in this paper
were computed on the News Test 2009 set (2525
sentences each language). The official scores on
News Test 2010 are given only in the main WMT
2010 paper.
Only lowercased data were used for the baseline
experiments.
</bodyText>
<subsectionHeader confidence="0.999586">
3.2 English-to-Czech
</subsectionHeader>
<bodyText confidence="0.9946152">
A separate set of experiments has been conducted
for the English-to-Czech direction and larger data
were used. We used CzEng 0.9 (Bojar and
Žabokrtský, 2009)8 as our main parallel corpus.
Following CzEng authors’ request, we did not use
sections 8* and 9* reserved for evaluation pur-
poses.
As the baseline training dataset (“Small” in the
following) only the news section of CzEng was
used. For large-scale experiments (“Large” in the
following), we used all CzEng together with the
EMEA corpus9 (Tiedemann, 2009).10
As our monolingual data we use the mono-
lingual data provided by WMT10 organizers for
Czech. Table 2 shows the sizes of these corpora.
</bodyText>
<table confidence="0.99767525">
Corpus SentPairs Tokens cs Tokens en
Small 126,144 2,645,665 2,883,893
Large 7,543,152 79,057,403 89,018,033
Mono 13,042,040 210,507,305
</table>
<tableCaption confidence="0.9743715">
Table 2: Number of sentences and tokens in the
Czech-English corpora.
</tableCaption>
<bodyText confidence="0.981088181818182">
Again, the official WMT 201011 development
set (News Test 2008, 2051 sentences each lan-
guage) and test set (News Test 2009, 2525 sen-
tences each language) are used for MERT and eval-
uation, respectively. The official scores on News
Test 2010 are given only in the main WMT 2010
paper.
We use a slightly modified tokenization rules
compared to CzEng export format. Most notably,
we normalize English abbreviated negation and
auxiliary verbs (“couldn’t” —* “could not”) and
</bodyText>
<footnote confidence="0.985085571428571">
8http://ufal.mff.cuni.cz/czeng/
9http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php
10Unfortunately, the EMEA corpus is badly tokenized on
the Czech side with fractional numbers split into several to-
kens (e.g. “3, 14”). We attempted to reconstruct the original
detokenized form using a small set of regular expressions.
11http://www.statmt.org/wmt10
</footnote>
<page confidence="0.998504">
213
</page>
<bodyText confidence="0.9991751">
attempt at normalizing quotation marks to distin-
guish between opening and closing one following
proper typesetting rules.
The rest of our pre-processing pipeline matches
the processing employed in CzEng (Bojar and
Žabokrtský, 2009).12 We use “supervised truecas-
ing”, meaning that we cast the case of the lemma
to the form, relying on our morphological analyz-
ers and taggers to identify proper names, all other
words are lowercased.
</bodyText>
<sectionHeader confidence="0.999582" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99994425">
All BLEU scores were computed directly by
Joshua on the News Test 2009 set. Note that
they differ from what the official evaluation script
would report, due to different tokenization.
</bodyText>
<subsectionHeader confidence="0.974988">
4.1 Baseline Experiments
</subsectionHeader>
<bodyText confidence="0.999814857142857">
The set of baseline experiments with all translation
directions involved running the system on lower-
cased News Commentary corpora. Word align-
ments were computed on 4-character stems (in-
cluding the en-cs and cs-en directions). A trigram
language model was trained on the target side of
the parallel corpus.
</bodyText>
<sectionHeader confidence="0.381994" genericHeader="method">
Direction BLEU
</sectionHeader>
<bodyText confidence="0.8727785">
0.0905
0.1114
Word alignments were computed on lemmatized
version of the parallel corpus. Hexagram language
model was trained on the monolingual data. True-
cased data were used for training, as described
above; the BLEU scores of these experiments in
Table 4 are computed on truecased system output.
</bodyText>
<table confidence="0.97561475">
Setup BLEU
Baseline 0.0905
Small 0.1012
Large 0.1300
</table>
<tableCaption confidence="0.985572">
Table 4: BLEU scores (lowercased baseline, true-
</tableCaption>
<bodyText confidence="0.946906684210526">
cased rest) of the English-to-Czech experiments,
including the baseline experiment with News
Commentary, mentioned earlier.
As for the official evaluation on News Test
2010, we used the Small setup as our primary sub-
mission, and the Large setup as secondary despite
its better results. The reason was that it was not
clear whether the experiment would be finished in
time for the official evaluation.13
An interesting perspective on the three en-cs
models is provided by the feature weights opti-
mized during MERT. We can see in Table 5 that the
small and relatively weak baseline LM is trusted
less than the most influential translation feature
while for large parallel data and even much larger
LM the weights are distributed more evenly.
en-cs
en-de
cs-en
</bodyText>
<table confidence="0.831031266666667">
0.1471
0.1617
0.1966
0.2001
0.2020
0.2025
Setup LM Pt0 Pt1 Pte WP
Baseline 1.0 1.55 0.51 0.63 −2.63
Small 1.0 1.03 0.72 −0.09 −0.34
Large 1.0 0.98 0.97 −0.02 −0.82
de-en
en-es
en-fr
fr-en
es-en
</table>
<tableCaption confidence="0.975938">
Table 3: Lowercased BLEU scores of the baseline
experiments on News Test 2009 data.
</tableCaption>
<subsectionHeader confidence="0.995628">
4.2 English-to-Czech
</subsectionHeader>
<bodyText confidence="0.960287066666667">
The extended (non-baseline) English-to-Czech ex-
periments were trained on larger parallel and
monolingual data, described in Section 3.2. Note
that the dataset denoted as “Small” still falls into
the constrained task because it only uses CzEng
0.9 and the WMT 2010 monolingual data.
12Due to the subsequent processing, incl. parsing, the tok-
enization of English follows PennTreebenk style. The rather
unfortunate convention of treating hyphenated words as sin-
gle tokens increases our out-of-vocabulary rate.
Table 5: Feature weights are relative to the weight
of LM, the score by the language model. Then
there are the three translation features: Pt0 =
P(e|f), Pt1 = Plex(f|e) and Pte = Plex(e|f).
WP is the word penalty.
</bodyText>
<subsectionHeader confidence="0.996272">
4.3 Efficiency
</subsectionHeader>
<bodyText confidence="0.9982356">
The machines on which the experiments were con-
ducted are 64bit Intel Xeon dual core 2.8 GHz
CPUs with 32 GB RAM.
Word alignment of each baseline corpus took
about 1 hour, time needed for data preprocessing
</bodyText>
<footnote confidence="0.989867666666667">
13In fact, it was not finished in time. Due to a failure of
a MERT run, we used feature weights from the primary sub-
mission for the secondary one, too.
</footnote>
<page confidence="0.998148">
214
</page>
<bodyText confidence="0.999970611111112">
and training of the language model was negligible.
Grammar extraction took about four hours but it
could be parallelized. For decoding the test data
were split into 20 chunks that were processed in
parallel. One MERT iteration, including decoding,
took from 30 minutes to 1 hour.
Training the large en-cs models requires more
careful engineering. The grammar extraction eas-
ily consumes over 20 GB memory so it is impor-
tant to make sure Java really has access to it. We
parallelized the extraction in the same way as we
had done with the decoding; even so, about 5 hours
were needed to complete the extraction. The de-
coder now must use the SWIG-linked SRILM li-
brary because Java-based language modeling is too
slow and memory-consuming. Otherwise, the de-
coding times are comparable to the baseline exper-
iments.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999976888888889">
We have described the hierarchical phrase-based
SMT system we used for the WMT 2010 shared
task. For English-to-Czech translation, we dis-
cussed experiments with large data from the point
of view of both the translation accuracy and effi-
ciency.
This has been our first attempt to switch to hier-
archical SMT and we have not gone too far beyond
just putting together the infrastructure and apply-
ing it to the available data. Nevertheless, our en-cs
experiments not only confirm that more data helps;
in the Small and Large setup, the data was not only
larger than in Baseline, it also underwent a more
refined preprocessing. In particular, we took ad-
vantage of the Czeng corpus being lemmatized to
produce better word alignment; also, the truecas-
ing technique helped to better target named enti-
ties.
</bodyText>
<sectionHeader confidence="0.994947" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999697333333333">
The work on this project was supported by the
grant MSM0021620838 by the Czech Ministry of
Education.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999462048387097">
Ondřej Bojar and Zdeněk Žabokrtský. 2009. Czeng
0.9: Large parallel treebank with rich annotation.
The Prague Bulletin of Mathematical Linguistics,
92:63–83.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. In Technical report TR-10-98, Computer
Science Group, Harvard, MA, USA, August. Har-
vard University.
David Chiang. 2007. Hierarchical Phrase-
Based Translation. Computational Linguistics,
33(2):201–228.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181–184, Los Alamitos, California, USA.
IEEE Computer Society Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Compan-
ion Volume Proceedings ofthe Demo and Poster Ses-
sions, pages 177–180, Praha, Czechia, June. Associ-
ation for Computational Linguistics.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009. Decoding in Joshua:
Open Source, Parsing-Based Machine Translation.
The Prague Bulletin of Mathematical Linguistics,
91:47–56, 1.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
Ninth Conference of the European Chapter ofthe As-
sociation for Computational Linguistics (EACL’99),
pages 71–76, Bergen, Norway, June. Association for
Computational Linguistics.
Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, Den-
ver, Colorado, USA.
Jörg Tiedemann. 2009. News from opus – a collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing (vol. V), pages 237–248. John Benjamins.
Omar F. Zaidan. 2009. Z-mert: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
</reference>
<page confidence="0.999146">
215
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.881709">
<title confidence="0.99405">Hierarchical Phrase-Based MT at the Charles for the WMT 2010 Shared Task</title>
<author confidence="0.967211">Daniel</author>
<affiliation confidence="0.989243">Charles University in Prague, Institute of Formal and Applied Linguistics Univerzita Karlova v Praze, Ústav formální a aplikované lingvistiky</affiliation>
<address confidence="0.947069">Malostranské náměstí 25, Praha, CZ-11800,</address>
<email confidence="0.986673">zeman@ufal.mff.cuni.cz</email>
<abstract confidence="0.997641272727273">We describe our experiments with hierarchical phrase-based machine translation for WMT 2010 Shared Task. We provide a detailed description of our configuration and data so the results are replicable. For English-to-Czech translation, we experiment with several datasets of various sizes and with various preprocessing sequences. For the other 7 translation directions, we just present the baseline results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondřej Bojar</author>
<author>Zdeněk Žabokrtský</author>
</authors>
<title>Czeng 0.9: Large parallel treebank with rich annotation.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>92--63</pages>
<contexts>
<context position="6196" citStr="Bojar and Žabokrtský, 2009" startWordPosition="914" endWordPosition="917">corpus training data”. 7Table 1 and Table 2 present statistics before removing the long sentences. The News Test 2008 data set (2051 sentences in each language) was used as development data for MERT. BLEU scores reported in this paper were computed on the News Test 2009 set (2525 sentences each language). The official scores on News Test 2010 are given only in the main WMT 2010 paper. Only lowercased data were used for the baseline experiments. 3.2 English-to-Czech A separate set of experiments has been conducted for the English-to-Czech direction and larger data were used. We used CzEng 0.9 (Bojar and Žabokrtský, 2009)8 as our main parallel corpus. Following CzEng authors’ request, we did not use sections 8* and 9* reserved for evaluation purposes. As the baseline training dataset (“Small” in the following) only the news section of CzEng was used. For large-scale experiments (“Large” in the following), we used all CzEng together with the EMEA corpus9 (Tiedemann, 2009).10 As our monolingual data we use the monolingual data provided by WMT10 organizers for Czech. Table 2 shows the sizes of these corpora. Corpus SentPairs Tokens cs Tokens en Small 126,144 2,645,665 2,883,893 Large 7,543,152 79,057,403 89,018,0</context>
<context position="7946" citStr="Bojar and Žabokrtský, 2009" startWordPosition="1174" endWordPosition="1177">auxiliary verbs (“couldn’t” —* “could not”) and 8http://ufal.mff.cuni.cz/czeng/ 9http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php 10Unfortunately, the EMEA corpus is badly tokenized on the Czech side with fractional numbers split into several tokens (e.g. “3, 14”). We attempted to reconstruct the original detokenized form using a small set of regular expressions. 11http://www.statmt.org/wmt10 213 attempt at normalizing quotation marks to distinguish between opening and closing one following proper typesetting rules. The rest of our pre-processing pipeline matches the processing employed in CzEng (Bojar and Žabokrtský, 2009).12 We use “supervised truecasing”, meaning that we cast the case of the lemma to the form, relying on our morphological analyzers and taggers to identify proper names, all other words are lowercased. 4 Experiments All BLEU scores were computed directly by Joshua on the News Test 2009 set. Note that they differ from what the official evaluation script would report, due to different tokenization. 4.1 Baseline Experiments The set of baseline experiments with all translation directions involved running the system on lowercased News Commentary corpora. Word alignments were computed on 4-character </context>
</contexts>
<marker>Bojar, Žabokrtský, 2009</marker>
<rawString>Ondřej Bojar and Zdeněk Žabokrtský. 2009. Czeng 0.9: Large parallel treebank with rich annotation. The Prague Bulletin of Mathematical Linguistics, 92:63–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. In</title>
<date>1998</date>
<tech>Technical report TR-10-98,</tech>
<publisher>Harvard University.</publisher>
<institution>Computer Science Group,</institution>
<location>Harvard, MA, USA,</location>
<contexts>
<context position="3287" citStr="Chen and Goodman, 1998" startWordPosition="487" endWordPosition="490">oshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: 1http://sourceforge.net/projects/joshua/ 2http://www.statmt.org/moses/ 3http://fjoch.com/mkcls.html 4http://fjoch.com/GIZA++.html 5http://www-speech.sri.com/projects/srilm/ 212 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 212–215, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics • Grammar extraction: --maxPhraseLength=5 • Decoding: span_limit=10 fuzz1=0.1 fuzz2=0.1 max_n</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. In Technical report TR-10-98, Computer Science Group, Harvard, MA, USA, August. Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical PhraseBased Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2487" citStr="Chiang, 2007" startWordPosition="370" endWordPosition="371">tags or even a morphological generator are some of the possible solutions to the poor-to-rich translation issues. Our submission to the shared task should reveal where a pure hierarchical system stands in this jungle and what of the above mentioned ideas match the phenomena the system suffers from. Although our primary focus lies on English-to-Czech translation, we also report the accuracy of the same system on moderately-sized corpora for the other three languages and seven translation directions. 2 The Translation System Our translation system belongs to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical PhraseBased Translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<publisher>IEEE Computer Society Press.</publisher>
<location>Los Alamitos, California, USA.</location>
<contexts>
<context position="3262" citStr="Kneser and Ney, 1995" startWordPosition="483" endWordPosition="486"> the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: 1http://sourceforge.net/projects/joshua/ 2http://www.statmt.org/moses/ 3http://fjoch.com/mkcls.html 4http://fjoch.com/GIZA++.html 5http://www-speech.sri.com/projects/srilm/ 212 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 212–215, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics • Grammar extraction: --maxPhraseLength=5 • Decoding: span_limit=10 </context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181–184, Los Alamitos, California, USA. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3141" citStr="Koehn et al., 2003" startWordPosition="464" endWordPosition="467">ls (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: 1http://sourceforge.net/projects/joshua/ 2http://www.statmt.org/moses/ 3http://fjoch.com/mkcls.html 4http://fjoch.com/GIZA++.html 5http://www-speech.sri.com/projects/srilm/ 212 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 212–215, Uppsala, Sweden, 15-16 July 20</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondřej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings ofthe Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Praha, Czechia,</location>
<contexts>
<context position="2912" citStr="Koehn et al., 2007" startWordPosition="431" endWordPosition="434">rately-sized corpora for the other three languages and seven translation directions. 2 The Translation System Our translation system belongs to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: 1http://sourceforge.net/projects/joshua/ 2http://www.statmt.org/moses/ 3http://fjoc</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings ofthe Demo and Poster Sessions, pages 177–180, Praha, Czechia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Sanjeev Khudanpur</author>
<author>Wren Thornton</author>
</authors>
<title>Decoding in Joshua: Open Source, Parsing-Based Machine Translation. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2009</date>
<volume>91</volume>
<pages>1</pages>
<contexts>
<context position="2755" citStr="Li et al., 2009" startWordPosition="408" endWordPosition="411">phenomena the system suffers from. Although our primary focus lies on English-to-Czech translation, we also report the accuracy of the same system on moderately-sized corpora for the other three languages and seven translation directions. 2 The Translation System Our translation system belongs to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Z</context>
</contexts>
<marker>Li, Callison-Burch, Khudanpur, Thornton, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur, and Wren Thornton. 2009. Decoding in Joshua: Open Source, Parsing-Based Machine Translation. The Prague Bulletin of Mathematical Linguistics, 91:47–56, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3050" citStr="Och and Ney, 2003" startWordPosition="453" endWordPosition="456">s to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: 1http://sourceforge.net/projects/joshua/ 2http://www.statmt.org/moses/ 3http://fjoch.com/mkcls.html 4http://fjoch.com/GIZA++.html 5http://www-speech.sri.com/projects/srilm/ 212 Proceedings of the Joint 5th Workshop on Sta</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter ofthe Association for Computational Linguistics (EACL’99),</booktitle>
<pages>71--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Bergen, Norway,</location>
<contexts>
<context position="2993" citStr="Och, 1999" startWordPosition="445" endWordPosition="446"> Translation System Our translation system belongs to the hierarchical phrase-based class (Chiang, 2007), i.e. phrase pairs with nonterminals (rules of a synchronous context-free grammar) are extracted from symmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: 1http://sourceforge.net/projects/joshua/ 2http://www.statmt.org/moses/ 3http://fjoch.com/mkcls.html 4http://fjoch.com/GIZA++.html 5http://www-speech.sri.com/project</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In Proceedings of the Ninth Conference of the European Chapter ofthe Association for Computational Linguistics (EACL’99), pages 71–76, Bergen, Norway, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="3206" citStr="Stolcke, 2002" startWordPosition="476" endWordPosition="477">mmetrized word alignments and subsequently used by the decoder. We use Joshua, a Java-based opensource implementation of the hierarchical decoder (Li et al., 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: 1http://sourceforge.net/projects/joshua/ 2http://www.statmt.org/moses/ 3http://fjoch.com/mkcls.html 4http://fjoch.com/GIZA++.html 5http://www-speech.sri.com/projects/srilm/ 212 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 212–215, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics • Grammar ex</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jörg Tiedemann</author>
</authors>
<title>News from opus – a collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>In Recent Advances in Natural Language Processing (vol. V),</booktitle>
<pages>237--248</pages>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="6552" citStr="Tiedemann, 2009" startWordPosition="973" endWordPosition="974">main WMT 2010 paper. Only lowercased data were used for the baseline experiments. 3.2 English-to-Czech A separate set of experiments has been conducted for the English-to-Czech direction and larger data were used. We used CzEng 0.9 (Bojar and Žabokrtský, 2009)8 as our main parallel corpus. Following CzEng authors’ request, we did not use sections 8* and 9* reserved for evaluation purposes. As the baseline training dataset (“Small” in the following) only the news section of CzEng was used. For large-scale experiments (“Large” in the following), we used all CzEng together with the EMEA corpus9 (Tiedemann, 2009).10 As our monolingual data we use the monolingual data provided by WMT10 organizers for Czech. Table 2 shows the sizes of these corpora. Corpus SentPairs Tokens cs Tokens en Small 126,144 2,645,665 2,883,893 Large 7,543,152 79,057,403 89,018,033 Mono 13,042,040 210,507,305 Table 2: Number of sentences and tokens in the Czech-English corpora. Again, the official WMT 201011 development set (News Test 2008, 2051 sentences each language) and test set (News Test 2009, 2525 sentences each language) are used for MERT and evaluation, respectively. The official scores on News Test 2010 are given only </context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>Jörg Tiedemann. 2009. News from opus – a collection of multilingual parallel corpora with tools and interfaces. In Recent Advances in Natural Language Processing (vol. V), pages 237–248. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-mert: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="3367" citStr="Zaidan, 2009" startWordPosition="502" endWordPosition="503">), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrasemodel.perl script packed with Moses2 (Koehn et al., 2007). This includes the usual combination of word clustering using mkcls3 (Och, 1999), twoway word alignment using GIZA++4 (Och and Ney, 2003), and alignment symmetrization using the grow-diag-final-and heuristic (Koehn et al., 2003). For language modeling we use the SRILM toolkit5 (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We use the Z-MERT implementation of minimum error rate training (Zaidan, 2009). The following settings have been used for Joshua and ZMERT: 1http://sourceforge.net/projects/joshua/ 2http://www.statmt.org/moses/ 3http://fjoch.com/mkcls.html 4http://fjoch.com/GIZA++.html 5http://www-speech.sri.com/projects/srilm/ 212 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 212–215, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics • Grammar extraction: --maxPhraseLength=5 • Decoding: span_limit=10 fuzz1=0.1 fuzz2=0.1 max_n_items=30 relative_threshold=10.0 max_n_rules=50 rule_relative_threshold=10.0 • </context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-mert: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>