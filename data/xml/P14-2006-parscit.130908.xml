<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000051">
<title confidence="0.999364">
Scoring Coreference Partitions of Predicted Mentions:
A Reference Implementation
</title>
<author confidence="0.9889685">
Sameer Pradhan&apos;, Xiaoqiang Luo2, Marta Recasens3,
Eduard Hovy4, Vincent Ng5 and Michael Strube6
</author>
<affiliation confidence="0.996969666666667">
&apos;Harvard Medical School, Boston, MA, 2Google Inc., New York, NY
3Google Inc., Mountain View, CA, 4Carnegie Mellon University, Pittsburgh, PA
5HLTRI, University of Texas at Dallas, Richardson, TX, 6HITS, Heidelberg, Germany
</affiliation>
<email confidence="0.945022">
sameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com,
hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.org
</email>
<sectionHeader confidence="0.993943" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831583333333">
The definitions of two coreference scoring
metrics—B3 and CEAF—are underspeci-
fied with respect to predicted, as opposed
to key (or gold) mentions. Several varia-
tions have been proposed that manipulate
either, or both, the key and predicted men-
tions in order to get a one-to-one mapping.
On the other hand, the metric BLANC was,
until recently, limited to scoring partitions
of key mentions. In this paper, we (i) ar-
gue that mention manipulation for scoring
predicted mentions is unnecessary, and po-
tentially harmful as it could produce unin-
tuitive results; (ii) illustrate the application
of all these measures to scoring predicted
mentions; (iii) make available an open-
source, thoroughly-tested reference imple-
mentation of the main coreference eval-
uation measures; and (iv) rescore the re-
sults of the CoNLL-2011/2012 shared task
systems with this implementation. This
will help the community accurately mea-
sure and compare new end-to-end corefer-
ence resolution algorithms.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997051923077">
Coreference resolution is a key task in natural
language processing (Jurafsky and Martin, 2008)
aiming to detect the referential expressions (men-
tions) in a text that point to the same entity.
Roughly over the past two decades, research in
coreference (for the English language) had been
plagued by individually crafted evaluations based
on two central corpora—MUC (Hirschman and
Chinchor, 1997; Chinchor and Sundheim, 2003;
Chinchor, 2001) and ACE (Doddington et al.,
2004). Experimental parameters ranged from us-
ing perfect (gold, or key) mentions as input for
purely testing the quality of the entity linking al-
gorithm, to an end-to-end evaluation where pre-
dicted mentions are used. Given the range of
evaluation parameters and disparity between the
annotation standards for the two corpora, it was
very hard to grasp the state of the art for the
task of coreference. This has been expounded in
Stoyanov et al. (2009). The activity in this sub-
field of NLP can be gauged by: (i) the contin-
ual addition of corpora manually annotated for
coreference—The OntoNotes corpus (Pradhan et
al., 2007; Weischedel et al., 2011) in the general
domain, as well as the i2b2 (Uzuner et al., 2012)
and THYME (Styler et al., 2014) corpora in the
clinical domain would be a few examples of such
emerging corpora; and (ii) ongoing proposals for
refining the existing metrics to make them more
informative (Holen, 2013; Chen and Ng, 2013).
The CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus (Prad-
han et al., 2011; Pradhan et al., 2012) were an
attempt to standardize the evaluation settings by
providing a benchmark annotated corpus, scorer,
and state-of-the-art system results that would al-
low future systems to compare against them. Fol-
lowing the timely emphasis on end-to-end evalu-
ation, the official track used predicted mentions
and measured performance using five coreference
measures: MUC (Vilain et al., 1995), B3 (Bagga
and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm
(Luo, 2005), and BLANC (Recasens and Hovy,
2011). The arithmetic mean of the first three was
the task’s final score.
An unfortunate setback to these evaluations had
its root in three issues: (i) the multiple variations
of two of the scoring metrics—B3 and CEAF—
used by the community to handle predicted men-
tions; (ii) a buggy implementation of the Cai and
Strube (2010) proposal that tried to reconcile these
variations; and (iii) the erroneous computation of
</bodyText>
<page confidence="0.979772">
30
</page>
<bodyText confidence="0.965940315789474">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30–35,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
the BLANC metric for partitions of predicted men-
tions. Different interpretations as to how to com-
pute B3 and CEAF scores for coreference systems
when predicted mentions do not perfectly align
with key mentions—which is usually the case—
led to variations of these metrics that manipulate
the gold standard and system output in order to
get a one-to-one mention mapping (Stoyanov et
al., 2009; Cai and Strube, 2010). Some of these
variations arguably produce rather unintuitive re-
sults, while others are not faithful to the original
measures.
In this paper, we address the issues in scor-
ing coreference partitions of predicted mentions.
Specifically, we justify our decision to go back
to the original scoring algorithms by arguing that
manipulation of key or predicted mentions is un-
necessary and could in fact produce unintuitive re-
sults. We demonstrate the use of our recent ex-
tension of BLANC that can seamlessly handle pre-
dicted mentions (Luo et al., 2014). We make avail-
able an open-source, thoroughly-tested reference
implementation of the main coreference evalua-
tion measures that do not involve mention manip-
ulation and is faithful to the original intentions of
the proposers of these metrics. We republish the
CoNLL-2011/2012 results based on this scorer, so
that future systems can use it for evaluation and
have the CoNLL results available for comparison.
The rest of the paper is organized as follows.
Section 2 provides an overview of the variations
of the existing measures. We present our newly
updated coreference scoring package in Section 3
together with the rescored CoNLL-2011/2012 out-
puts. Section 4 walks through a scoring example
for all the measures, and we conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.569782" genericHeader="method">
2 Variations of Scoring Measures
</sectionHeader>
<bodyText confidence="0.988758045454546">
Two commonly used coreference scoring metrics
—B3 and CEAF—are underspecified in their ap-
plication for scoring predicted, as opposed to key
mentions. The examples in the papers describing
these metrics assume perfect mentions where pre-
dicted mentions are the same set of mentions as
key mentions. The lack of accompanying refer-
ence implementation for these metrics by its pro-
posers made it harder to fill the gaps in the speci-
fication. Subsequently, different interpretations of
how one can evaluate coreference systems when
predicted mentions do not perfectly align with key
mentions led to variations of these metrics that ma-
nipulate the gold and/or predicted mentions (Stoy-
anov et al., 2009; Cai and Strube, 2010). All these
variations attempted to generate a one-to-one map-
ping between the key and predicted mentions, as-
suming that the original measures cannot be ap-
plied to predicted mentions. Below we first pro-
vide an overview of these variations and then dis-
cuss the unnecessity of this assumption.
Coining the term twinless mentions for those
mentions that are either spurious or missing from
the predicted mention set, Stoyanov et al. (2009)
proposed two variations to B3 — B3all and B30—to
handle them. In the first variation, all predicted
twinless mentions are retained, whereas the lat-
ter discards them and penalizes recall for twin-
less predicted mentions. Rahman and Ng (2009)
proposed another variation by removing “all and
only those twinless system mentions that are sin-
gletons before applying B3 and CEAF.” Follow-
ing upon this line of research, Cai and Strube
(2010) proposed a unified solution for both B3 and
CEAFm, leaving the question of handling CEAFe
as future work because “it produces unintuitive
results.” The essence of their solution involves
manipulating twinless key and predicted mentions
by adding them either from the predicted parti-
tion to the key partition or vice versa, depend-
ing on whether one is computing precision or re-
call. The Cai and Strube (2010) variation was used
by the CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus, and
by the i2b2 2011 shared task on coreference res-
olution using an assortment of clinical notes cor-
pora (Uzuner et al., 2012).1 It was later identified
by Recasens et al. (2013) that there was a bug in
the implementation of this variation in the scorer
used for the CoNLL-2011/2012 tasks. We have
not tested the correctness of this variation in the
scoring package used for the i2b2 shared task.
However, it turns out that the CEAF metric (Luo,
2005) was always intended to work seamlessly on
predicted mentions, and so has been the case with
the B3 metric.2 In a latter paper, Rahman and Ng
(2011) correctly state that “CEAF can compare par-
titions with twinless mentions without any modifi-
cation.” We will look at this further in Section 4.3.
We argue that manipulations of key and re-
sponse mentions/entities, as is done in the exist-
ing B3 variations, not only confound the evalu-
ation process, but are also subject to abuse and
can seriously jeopardize the fidelity of the evalu-
1Personal communication with Andreea Bodnari, and
contents of the i2b2 scorer code.
</bodyText>
<footnote confidence="0.770173">
2Personal communication with Breck Baldwin.
</footnote>
<page confidence="0.999293">
31
</page>
<bodyText confidence="0.999954433333333">
ation. Given space constraints we use an exam-
ple worked out in Cai and Strube (2010). Let
the key contain an entity with mentions {a, b, c}
and the prediction contain an entity with mentions
{a, b, d}. As detailed in Cai and Strube (2010,
p. 29-30, Tables 1–3), B30 assigns a perfect pre-
cision of 1.00 which is unintuitive as the system
has wrongly predicted a mention d as belonging to
the entity. For the same prediction, B3all assigns a
precision of 0.556. But, if the prediction contains
two entities {a, b, d} and {c} (i.e., the mention c
is added as a spurious singleton), then B3all preci-
sion increases to 0.667 which is counter-intuitive
as it does not penalize the fact that c is erroneously
placed in its own entity. The version illustrated in
Section 4.2, which is devoid of any mention ma-
nipulations, gives a precision of 0.444 in the first
scenario and the precision drops to 0.333 in the
second scenario with the addition of a spurious
singleton entity {c}. This is a more intuitive be-
havior.
Contrary to both B3 and CEAF, the BLANC mea-
sure (Recasens and Hovy, 2011) was never de-
signed to handle predicted mentions. However, the
implementation used for the SemEval-2010 shared
task as well as the one for the CoNLL-2011/2012
shared tasks accepted predicted mentions as input,
producing undefined results. In Luo et al. (2014)
we have extended the BLANC metric to deal with
predicted mentions
</bodyText>
<sectionHeader confidence="0.996734" genericHeader="method">
3 Reference Implementation
</sectionHeader>
<bodyText confidence="0.9991398">
Given the potential unintuitive outcomes of men-
tion manipulation and the misunderstanding that
the original measures could not handle twinless
predicted mentions (Section 2), we redesigned the
CoNLL scorer. The new implementation:
</bodyText>
<listItem confidence="0.991957916666667">
• is faithful to the original measures;
• removes any prior mention manipulation,
which might depend on specific annotation
guidelines among other problems;
• has been thoroughly tested to ensure that it
gives the expected results according to the
original papers, and all test cases are included
as part of the release;
• is free of the reported bugs that the CoNLL
scorer (v4) suffered (Recasens et al., 2013);
• includes the extension of BLANC to handle
predicted mentions (Luo et al., 2014).
</listItem>
<bodyText confidence="0.928863">
This is the open source scoring package3 that
we present as a reference implementation for the
</bodyText>
<equation confidence="0.851241571428571">
3http://code.google.com/p/reference-coreference-scorers/
B3
SYSTEM MD MUC CEAF BLANC CONLL
M a AVERAGE
F1 F2 F3
F1 F1
1 1 1
</equation>
<table confidence="0.945406333333333">
CoNLL-2011; English
CoNLL-2012; English
fernandes 64.8 46.5 42.5 49.2 46.5 38.0 45.2
bjorkelund 60.6 47.8 41.6 46.7 41.2 37.9 43.5
uryupina 55.4 41.5 36.1 41.4 35.0 33.0 37.5
stamborg 59.5 41.2 35.9 40.0 32.9 34.5 36.7
chen 59.8 39.0 32.1 34.7 26.0 30.8 32.4
zhekova 41.0 29.9 22.7 31.1 25.9 18.5 26.2
li 29.7 18.1 13.1 21.0 17.3 8.4 16.2
</table>
<tableCaption confidence="0.996328">
Table 1: Performance on the official, closed track
</tableCaption>
<bodyText confidence="0.837170833333333">
in percentages using all predicted information for
the CoNLL-2011 and 2012 shared tasks.
community to use. It is written in perl and stems
from the scorer that was initially used for the
SemEval-2010 shared task (Recasens et al., 2010)
and later modified for the CoNLL-2011/2012
shared tasks.4
Partitioning detected mentions into entities (or
equivalence classes) typically comprises two dis-
tinct tasks: (i) mention detection; and (ii) coref-
erence resolution. A typical two-step coreference
algorithm uses mentions generated by the best
</bodyText>
<footnote confidence="0.9574975">
4We would like to thank Emili Sapena for writing the first
version of the scoring package.
</footnote>
<table confidence="0.999878541666667">
sapena 68.4 59.5 46.5 51.3 44.0 44.5 50.0
nugues 69.0 58.6 45.0 48.4 40.0 46.0 47.9
chang 64.9 57.2 46.0 50.7 40.0 45.5 47.7
stoyanov 67.8 58.4 40.1 43.3 36.9 34.6 45.1
santos 65.5 56.7 42.9 45.1 35.6 41.3 45.0
song 67.3 60.0 41.4 41.0 33.1 30.9 44.8
sobha 64.8 50.5 39.5 44.2 39.4 36.3 43.1
yang 63.9 52.3 39.4 43.2 35.5 36.1 42.4
charton 64.3 52.5 38.0 42.6 34.5 35.7 41.6
hao 64.3 54.5 37.7 41.9 31.6 37.0 41.3
zhou 62.3 49.0 37.0 40.6 35.0 35.0 40.3
kobdani 61.0 53.5 34.8 38.1 34.1 32.6 38.7
xinxin 61.9 46.6 34.9 37.7 31.7 35.0 37.7
kummerfeld 62.7 42.7 34.2 38.8 35.5 31.0 37.5
zhang 61.1 47.9 34.4 37.8 29.2 35.7 37.2
zhekova 48.3 24.1 23.7 23.4 20.5 15.4 22.8
irwin 26.7 20.0 11.7 18.5 14.7 6.3 15.5
lee 70.7 59.6 48.9 53.0 46.1 48.8 51.5
fernandes 77.7 70.5 57.6 61.4 53.9 58.8 60.7
martschat 75.2 67.0 54.6 58.8 51.5 55.0 57.7
bjorkelund 75.4 67.6 54.5 58.2 50.2 55.4 57.4
chen 73.8 63.7 51.8 55.8 48.1 52.9 54.5
chunyang 73.7 63.8 51.2 55.1 47.6 52.7 54.2
stamborg 73.9 65.1 51.7 55.1 46.6 54.4 54.2
yuan 72.5 62.6 50.1 54.5 46.0 52.1 52.9
xu 72.0 66.2 50.3 51.3 41.3 46.5 52.6
shou 73.7 62.9 49.4 53.2 46.7 50.4 53.0
uryupina 70.9 60.9 46.2 49.3 42.9 46.0 50.0
songyang 68.8 59.8 45.9 49.6 42.4 45.1 49.4
zhekova 67.1 53.5 35.7 39.7 32.2 34.8 40.5
xinxin 62.8 48.3 35.7 38.0 31.9 36.5 38.6
li 59.9 50.8 32.3 36.3 25.2 31.9 36.1
CoNLL-2012; Chinese
chen 71.6 62.2 55.7 60.0 55.0 54.1 57.6
yuan 68.2 60.3 52.4 55.8 50.2 43.2 54.3
bjorkelund 66.4 58.6 51.1 54.2 47.6 44.2 52.5
xu 65.2 58.1 49.5 51.9 46.6 38.5 51.4
fernandes 66.1 60.3 49.6 54.4 44.5 49.6 51.5
stamborg 64.0 57.8 47.4 51.6 41.9 45.9 49.0
uryupina 59.0 53.0 41.7 46.9 37.6 41.9 44.1
martschat 58.6 52.4 40.8 46.0 38.2 37.9 43.8
chunyang 61.6 49.8 39.6 44.2 37.3 36.8 42.2
xinxin 55.9 48.1 38.8 42.9 34.5 37.9 40.5
li 51.5 44.7 31.5 36.7 25.3 30.4 33.8
chang 47.6 37.9 28.8 36.1 29.6 25.7 32.1
zhekova 47.3 40.6 28.1 31.4 21.2 22.9 30.0
CoNLL-2012; Arabic
chang 74.3 66.4 53.0 57.1 48.9 53.9 56.1
</table>
<page confidence="0.992553">
32
</page>
<figureCaption confidence="0.945833">
Figure 1: Example key and response entities along
with the partitions for computing the MUC score.
</figureCaption>
<bodyText confidence="0.999325793103448">
possible mention detection algorithm as input to
the coreference algorithm. Therefore, ideally one
would want to score the two steps independently
of each other. A peculiarity of the OntoNotes
corpus is that singleton referential mentions are
not annotated, thereby preventing the computation
of a mention detection score independently of the
coreference resolution score. In corpora where all
referential mentions (including singletons) are an-
notated, the mention detection score generated by
this implementation is independent of the corefer-
ence resolution score.
We used this reference implementation to
rescore the CoNLL-2011/2012 system outputs for
the official task to enable future comparisons with
these benchmarks. The new CoNLL-2011/2012
results are in Table 1. We found that the over-
all system ranking remained largely unchanged for
both shared tasks, except for some of the lower
ranking systems that changed one or two places.
However, there was a considerable drop in the
magnitude of all B3 scores owing to the combi-
nation of two things: (i) mention manipulation, as
proposed by Cai and Strube (2010), adds single-
tons to account for twinless mentions; and (ii) the
B3 metric allows an entity to be used more than
once as pointed out by Luo (2005). This resulted
in a drop in the CoNLL averages (B3 is one of the
three measures that make the average).
</bodyText>
<sectionHeader confidence="0.991798" genericHeader="method">
4 An Illustrative Example
</sectionHeader>
<bodyText confidence="0.979353210526316">
This section walks through the process of com-
puting each of the commonly used metrics for
an example where the set of predicted mentions
has some missing key mentions and some spu-
rious mentions. While the mathematical formu-
lae for these metrics can be found in the original
papers (Vilain et al., 1995; Bagga and Baldwin,
1998; Luo, 2005), many misunderstandings dis-
cussed in Section 2 are due to the fact that these
papers lack an example showing how a metric is
computed on predicted mentions. A concrete ex-
ample goes a long way to prevent similar misun-
derstandings in the future. The example is adapted
from Vilain et al. (1995) with some slight modifi-
cations so that the total number of mentions in the
key is different from the number of mentions in
the prediction. The key (K) contains two entities
with mentions {a, b, c} and {d, e, f, g} and the re-
sponse (R) contains three entities with mentions
</bodyText>
<equation confidence="0.968707571428571">
{a, b}; {c, d} and {f, g, h, i}:
K2
z } |{
{d, e, f, g} (1)
R3
z } |{
{f, g, h, i}. (2)
</equation>
<bodyText confidence="0.987974">
Mention e is missing from the response, and men-
tions h and i are spurious in the response. The fol-
lowing sections use R to denote recall and P for
precision.
</bodyText>
<subsectionHeader confidence="0.955911">
4.1 MUC
</subsectionHeader>
<bodyText confidence="0.9999395">
The main step in the MUC scoring is creating the
partitions with respect to the key and response re-
spectively, as shown in Figure 1. Once we have
the partitions, then we compute the MUC score by:
</bodyText>
<equation confidence="0.999898692307692">
R = Pt i(|Ki |− |p(Ki)|)
PNk
i=1(|Ki |− 1)
(3 − 2) + (4 − 3) = 0.40 = 0.40,
=
(3 − 1) + (4 − 1)
PNr
P i=1(|Ri |− |P (Ri)|)
=
PNr 1)
i=1(|Ri |− + (4 − 3)
(2 − 1) + (2 − 2)
(2 − 1) + (2 − 1) + (4 − 1)
</equation>
<bodyText confidence="0.999699111111111">
where Ki is the i1h key entity and p(Ki) is the
set of partitions created by intersecting Ki with
response entities (cf. the middle sub-figure in Fig-
ure 1); Ri is the i1h response entity and p&apos;(Ri) is
the set of partitions created by intersecting Ri with
key entities (cf. the right-most sub-figure in Fig-
ure 1); and Nk and Nr are the number of key and
response entities, respectively.
The MUC F1 score in this case is 0.40.
</bodyText>
<subsectionHeader confidence="0.944099">
4.2 B3
</subsectionHeader>
<bodyText confidence="0.999824">
For computing B3 recall, each key mention is as-
signed a credit equal to the ratio of the number of
correct mentions in the predicted entity contain-
ing the key mention to the size of the key entity to
which the mention belongs, and the recall is just
</bodyText>
<figure confidence="0.9834469">
Solid: Key Solid: Key Solid: Partition wrt Key
Dashed: Response Dashed: partition wrt Response Dashed: Response
d
e f
g
a b
c
h
i
i
h
d
e
a b
c
f g f g
a b
h i
c
d
</figure>
<equation confidence="0.861725333333333">
K1
z } |{
K = {a, b, c}
R1
z } |{
R = {a, b}
R2
z } |{
{c, d}
</equation>
<page confidence="0.989364">
33
</page>
<bodyText confidence="0.99974075">
the sum of credits over all key mentions normal-
ized over the number of key mentions. B3 preci-
sion is computed similarly, except switching the
role of key and response. Applied to the example:
</bodyText>
<equation confidence="0.99855675">
R =
=
P = Pi=1 Pj=1 |Rj|
Nk Nr |KinRj |2
PNr
i=1 |Rj|
1 22 12 12 22 1 4
= 8 x ( 2 + 2 + 2 + 4 ) = 8 x 1 = 0.50
</equation>
<bodyText confidence="0.986623">
Note that terms with 0 value are omitted. The B3
F1 score is 0.46.
</bodyText>
<subsectionHeader confidence="0.991708">
4.3 CEAF
</subsectionHeader>
<bodyText confidence="0.9565845">
The first step in the CEAF computation is getting
the best scoring alignment between the key and
response entities. In this case the alignment is
straightforward. Entity R1 aligns with K1 and R3
aligns with K2. R2 remains unaligned.
CEAFm
CEAFm recall is the number of aligned mentions
divided by the number of key mentions, and preci-
sion is the number of aligned mentions divided by
the number of response mentions:
</bodyText>
<equation confidence="0.9999805">
R = |K1 n R1 |+ |K2 n R3|
|K1 |+ |K2|
P = |K1 n R1 |+ |K2 n R3|
|R1 |+ |R2 |+ |R3|
</equation>
<bodyText confidence="0.9157735">
The CEAFm F1 score is 0.53.
CEAFe
We use the same notation as in Luo (2005):
04(Ki, Rj) to denote the similarity between a key
entity Ki and a response entity Rj. 04(Ki, Rj) is
defined as:
</bodyText>
<equation confidence="0.9591065">
04(Ki, Rj) = 2 x |Ki n Rj|
|Ki |+ |Rj |.
</equation>
<bodyText confidence="0.9164155">
CEAFe recall and precision, when applied to this
example, are:
</bodyText>
<equation confidence="0.9815975">
04(K1, R1) + 04(K2, R3)
R =
Nk
04(K1, R1) + 04(K2, R3)
P =
Nr
</equation>
<bodyText confidence="0.731441">
The CEAFe F1 score is 0.52.
</bodyText>
<subsectionHeader confidence="0.967377">
4.4 BLANC
</subsectionHeader>
<bodyText confidence="0.9992187">
The BLANC metric illustrated here is the one in
our implementation which extends the original
BLANC (Recasens and Hovy, 2011) to predicted
mentions (Luo et al., 2014).
Let Ck and Cr be the set of coreference links
in the key and response respectively, and Nk and
Nr be the set of non-coreference links in the key
and response respectively. A link between a men-
tion pair m and n is denoted by mn; then for the
example in Figure 1, we have
</bodyText>
<equation confidence="0.995881083333333">
Ck = {ab, ac, bc, de, df, dg, ef, eg, fg}
Nk = {ad, ae, af, ag, bd, be, bf, bg, cd, ce, cf, cg}
Cr = {ab, cd, fg, fh, fi, gh, gi, hi}
Nr = {ac, ad, af, ag, ah, ai, bc, bd, bf, bg, bh, bi,
cf, cg, ch, ci, df, dg, dh, di}.
Recall and precision for coreference links are:
|Ck n Cr|
Rc =
|Ck|
|Ck n Cr|
Pc =
|Cr|
</equation>
<bodyText confidence="0.988958333333333">
and the coreference F-measure, Fc ≈ 0.23. Sim-
ilarly, recall and precision for non-coreference
links are:
</bodyText>
<equation confidence="0.967809666666667">
Rn =|Nk|
I Nk n Nr |
Pn = |Nr|
</equation>
<bodyText confidence="0.970209666666667">
and the non-coreference F-measure, Fn = 0.50.
So the BLANC score is Fc+Fn
2 ≈0.36.
</bodyText>
<sectionHeader confidence="0.998819" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999973294117647">
We have cleared several misunderstandings about
coreference evaluation metrics, especially when a
response contains imperfect predicted mentions,
and have argued against mention manipulations
during coreference evaluation. These misunder-
standings are caused partially by the lack of il-
lustrative examples to show how a metric is com-
puted on predicted mentions not aligned perfectly
with key mentions. Therefore, we provide detailed
steps for computing all four metrics on a represen-
tative example. Furthermore, we have a reference
implementation of these metrics that has been rig-
orously tested and has been made available to the
public as open source software. We reported new
scores on the CoNLL 2011 and 2012 data sets,
which can serve as the benchmarks for future re-
search work.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99844625">
This work was partially supported by grants
R01LM10090 from the National Library of
Medicine and IIS-1219142 from the National Sci-
ence Foundation.
</bodyText>
<figure confidence="0.918152085714286">
Nk Nr |KinRj |2
Pi=1 Pj=1 |Ki|
PNk
i=1 |Ki|
1 22 12 12 22 1 35
x ( + + + 4 ) = 7 x 12 - 0.42
7 3 3 4
= (2 + 2) - 0.57
(3 + 4)
= (2 + 2) = 0.50
(2 + 2 + 4)
=
(2x2)(2x2)
(3+2) + (4+4)
= 0.65
2
=
(2x2) (2x2)
(3+2) + (4+4)
- 0.43
3
2
= - 0.22
9
2
= = 0.25
8
|Nk n Nr |8
=
12
8
=
20
- 0.67
= 0.40,
</figure>
<page confidence="0.994741">
34
</page>
<sectionHeader confidence="0.988336" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991840057142857">
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
LREC, pages 563–566.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In
Proceedings of SIGDIAL, pages 28–36.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Pro-
ceedings of the Sixth IJCNLP, pages 1366–1374,
Nagoya, Japan, October.
Nancy Chinchor and Beth Sundheim. 2003. Mes-
sage understanding conference (MUC) 6. In
LDC2003T13.
Nancy Chinchor. 2001. Message understanding con-
ference (MUC) 7. In LDC2001T02.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ACE) program-tasks, data, and evaluation. In
Proceedings of LREC.
Lynette Hirschman and Nancy Chinchor. 1997. Coref-
erence task definition (v3.0, 13 jul 97). In Proceed-
ings of the 7th Message Understanding Conference.
Gordana Ilic Holen. 2013. Critical reflections on
evaluation practices in coreference resolution. In
Proceedings of the NAACL-HLT Student Research
Workshop, pages 1–7, Atlanta, Georgia, June.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall. Second
Edition.
Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An extension of BLANC to
system mentions. In Proceedings of ACL, Balti-
more, Maryland, June.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT-EMNLP,
pages 25–32.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A Unified Rela-
tional Semantic Representation. International Jour-
nal of Semantic Computing, 1(4):405–419.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of CoNLL: Shared Task, pages 1–27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of CoNLL: Shared Task, pages 1–40.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of
EMNLP, pages 968–977.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of
ACL, pages 814–824.
Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand Index for coreference eval-
uation. Natural Language Engineering, 17(4):485–
510.
Marta Recasens, Llu´ıs M`arquez, Emili Sapena,
M. Ant`onia Mart´ı, Mariona Taul´e, V´eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of SemEval,
pages 1–8.
Marta Recasens, Marie-Catherine de Marneffe, and
Chris Potts. 2013. The life and death of discourse
entities: Identifying singleton mentions. In Pro-
ceedings of NAACL-HLT, pages 627–633.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP, pages
656–664.
William F. Styler, Steven Bethard an Sean Finan,
Martha Palmer, Sameer Pradhan, Piet C de Groen,
Brad Erickson, Timothy Miller, Chen Lin, Guergana
Savova, and James Pustejovsky. 2014. Temporal
annotation in the clinical domain. Transactions of
Computational Linguistics, 2(April):143–154.
Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. Journal of
American Medical Informatics Association, 19(5),
September.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model theo-
retic coreference scoring scheme. In Proceedings of
the 6th Message Understanding Conference, pages
45–52.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Robert Belvin, Sameer Pradhan,
Lance Ramshaw, and Nianwen Xue. 2011.
OntoNotes: A large training corpus for enhanced
processing. In Joseph Olive, Caitlin Christian-
son, and John McCary, editors, Handbook of Natu-
ral Language Processing and Machine Translation:
DARPA Global Autonomous Language Exploitation.
Springer.
</reference>
<page confidence="0.999321">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.240839">
<title confidence="0.847762666666667">Scoring Coreference Partitions of Predicted A Reference Implementation Xiaoqiang Marta</title>
<author confidence="0.869929">Vincent</author>
<author confidence="0.869929">Michael</author>
<affiliation confidence="0.946292">Medical School, Boston, MA, Inc., New York, Inc., Mountain View, CA, Mellon University, Pittsburgh,</affiliation>
<address confidence="0.547926">University of Texas at Dallas, Richardson, TX, Heidelberg, Germany</address>
<email confidence="0.999453">hovy@cmu.edu,vince@hlt.utdallas.edu,michael.strube@h-its.org</email>
<abstract confidence="0.99926304">The definitions of two coreference scoring and underspeciwith respect to as opposed mentions. Several variations have been proposed that manipulate either, or both, the key and predicted mentions in order to get a one-to-one mapping. the other hand, the metric until recently, limited to scoring partitions of key mentions. In this paper, we (i) argue that mention manipulation for scoring predicted mentions is unnecessary, and potentially harmful as it could produce unintuitive results; (ii) illustrate the application of all these measures to scoring predicted mentions; (iii) make available an opensource, thoroughly-tested reference implementation of the main coreference evaluation measures; and (iv) rescore the results of the CoNLL-2011/2012 shared task systems with this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="3490" citStr="Bagga and Baldwin, 1998" startWordPosition="522" endWordPosition="525"> existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of 30 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages</context>
<context position="16330" citStr="Bagga and Baldwin, 1998" startWordPosition="2670" endWordPosition="2673">ai and Strube (2010), adds singletons to account for twinless mentions; and (ii) the B3 metric allows an entity to be used more than once as pointed out by Luo (2005). This resulted in a drop in the CoNLL averages (B3 is one of the three measures that make the average). 4 An Illustrative Example This section walks through the process of computing each of the commonly used metrics for an example where the set of predicted mentions has some missing key mentions and some spurious mentions. While the mathematical formulae for these metrics can be found in the original papers (Vilain et al., 1995; Bagga and Baldwin, 1998; Luo, 2005), many misunderstandings discussed in Section 2 are due to the fact that these papers lack an example showing how a metric is computed on predicted mentions. A concrete example goes a long way to prevent similar misunderstandings in the future. The example is adapted from Vilain et al. (1995) with some slight modifications so that the total number of mentions in the key is different from the number of mentions in the prediction. The key (K) contains two entities with mentions {a, b, c} and {d, e, f, g} and the response (R) contains three entities with mentions {a, b}; {c, d} and {f</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of LREC, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Cai</author>
<author>Michael Strube</author>
</authors>
<title>Evaluation metrics for end-to-end coreference resolution systems.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="3887" citStr="Cai and Strube (2010)" startWordPosition="589" endWordPosition="592">ainst them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of 30 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30–35, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the BLANC metric for partitions of predicted mentions. Different interpretations as to how to compute B3 and CEAF scores for coreference systems when predicted mentions do not perfectly align with key mentions—which is usually the case— led to variations of these metrics that manipulate the gold </context>
<context position="6644" citStr="Cai and Strube, 2010" startWordPosition="1024" endWordPosition="1027">ion for scoring predicted, as opposed to key mentions. The examples in the papers describing these metrics assume perfect mentions where predicted mentions are the same set of mentions as key mentions. The lack of accompanying reference implementation for these metrics by its proposers made it harder to fill the gaps in the specification. Subsequently, different interpretations of how one can evaluate coreference systems when predicted mentions do not perfectly align with key mentions led to variations of these metrics that manipulate the gold and/or predicted mentions (Stoyanov et al., 2009; Cai and Strube, 2010). All these variations attempted to generate a one-to-one mapping between the key and predicted mentions, assuming that the original measures cannot be applied to predicted mentions. Below we first provide an overview of these variations and then discuss the unnecessity of this assumption. Coining the term twinless mentions for those mentions that are either spurious or missing from the predicted mention set, Stoyanov et al. (2009) proposed two variations to B3 — B3all and B30—to handle them. In the first variation, all predicted twinless mentions are retained, whereas the latter discards them</context>
<context position="7921" citStr="Cai and Strube (2010)" startWordPosition="1233" endWordPosition="1236">ahman and Ng (2009) proposed another variation by removing “all and only those twinless system mentions that are singletons before applying B3 and CEAF.” Following upon this line of research, Cai and Strube (2010) proposed a unified solution for both B3 and CEAFm, leaving the question of handling CEAFe as future work because “it produces unintuitive results.” The essence of their solution involves manipulating twinless key and predicted mentions by adding them either from the predicted partition to the key partition or vice versa, depending on whether one is computing precision or recall. The Cai and Strube (2010) variation was used by the CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus, and by the i2b2 2011 shared task on coreference resolution using an assortment of clinical notes corpora (Uzuner et al., 2012).1 It was later identified by Recasens et al. (2013) that there was a bug in the implementation of this variation in the scorer used for the CoNLL-2011/2012 tasks. We have not tested the correctness of this variation in the scoring package used for the i2b2 shared task. However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly</context>
<context position="9226" citStr="Cai and Strube (2010)" startWordPosition="1455" endWordPosition="1458">per, Rahman and Ng (2011) correctly state that “CEAF can compare partitions with twinless mentions without any modification.” We will look at this further in Section 4.3. We argue that manipulations of key and response mentions/entities, as is done in the existing B3 variations, not only confound the evaluation process, but are also subject to abuse and can seriously jeopardize the fidelity of the evalu1Personal communication with Andreea Bodnari, and contents of the i2b2 scorer code. 2Personal communication with Breck Baldwin. 31 ation. Given space constraints we use an example worked out in Cai and Strube (2010). Let the key contain an entity with mentions {a, b, c} and the prediction contain an entity with mentions {a, b, d}. As detailed in Cai and Strube (2010, p. 29-30, Tables 1–3), B30 assigns a perfect precision of 1.00 which is unintuitive as the system has wrongly predicted a mention d as belonging to the entity. For the same prediction, B3all assigns a precision of 0.556. But, if the prediction contains two entities {a, b, d} and {c} (i.e., the mention c is added as a spurious singleton), then B3all precision increases to 0.667 which is counter-intuitive as it does not penalize the fact that </context>
<context position="15727" citStr="Cai and Strube (2010)" startWordPosition="2561" endWordPosition="2564">this implementation is independent of the coreference resolution score. We used this reference implementation to rescore the CoNLL-2011/2012 system outputs for the official task to enable future comparisons with these benchmarks. The new CoNLL-2011/2012 results are in Table 1. We found that the overall system ranking remained largely unchanged for both shared tasks, except for some of the lower ranking systems that changed one or two places. However, there was a considerable drop in the magnitude of all B3 scores owing to the combination of two things: (i) mention manipulation, as proposed by Cai and Strube (2010), adds singletons to account for twinless mentions; and (ii) the B3 metric allows an entity to be used more than once as pointed out by Luo (2005). This resulted in a drop in the CoNLL averages (B3 is one of the three measures that make the average). 4 An Illustrative Example This section walks through the process of computing each of the commonly used metrics for an example where the set of predicted mentions has some missing key mentions and some spurious mentions. While the mathematical formulae for these metrics can be found in the original papers (Vilain et al., 1995; Bagga and Baldwin, 1</context>
</contexts>
<marker>Cai, Strube, 2010</marker>
<rawString>Jie Cai and Michael Strube. 2010. Evaluation metrics for end-to-end coreference resolution systems. In Proceedings of SIGDIAL, pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Linguistically aware coreference evaluation metrics.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth IJCNLP,</booktitle>
<pages>1366--1374</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="2946" citStr="Chen and Ng, 2013" startWordPosition="440" endWordPosition="443">very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recas</context>
</contexts>
<marker>Chen, Ng, 2013</marker>
<rawString>Chen Chen and Vincent Ng. 2013. Linguistically aware coreference evaluation metrics. In Proceedings of the Sixth IJCNLP, pages 1366–1374, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>Beth Sundheim</author>
</authors>
<date>2003</date>
<booktitle>Message understanding conference (MUC) 6. In LDC2003T13.</booktitle>
<contexts>
<context position="1949" citStr="Chinchor and Sundheim, 2003" startWordPosition="271" endWordPosition="274">e results of the CoNLL-2011/2012 shared task systems with this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms. 1 Introduction Coreference resolution is a key task in natural language processing (Jurafsky and Martin, 2008) aiming to detect the referential expressions (mentions) in a text that point to the same entity. Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora—MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from using perfect (gold, or key) mentions as input for purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually</context>
</contexts>
<marker>Chinchor, Sundheim, 2003</marker>
<rawString>Nancy Chinchor and Beth Sundheim. 2003. Message understanding conference (MUC) 6. In LDC2003T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<date>2001</date>
<booktitle>Message understanding conference (MUC) 7. In LDC2001T02.</booktitle>
<contexts>
<context position="1966" citStr="Chinchor, 2001" startWordPosition="275" endWordPosition="276">012 shared task systems with this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms. 1 Introduction Coreference resolution is a key task in natural language processing (Jurafsky and Martin, 2008) aiming to detect the referential expressions (mentions) in a text that point to the same entity. Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora—MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from using perfect (gold, or key) mentions as input for purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for co</context>
</contexts>
<marker>Chinchor, 2001</marker>
<rawString>Nancy Chinchor. 2001. Message understanding conference (MUC) 7. In LDC2001T02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The automatic content extraction (ACE) program-tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="2000" citStr="Doddington et al., 2004" startWordPosition="279" endWordPosition="282">ith this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms. 1 Introduction Coreference resolution is a key task in natural language processing (Jurafsky and Martin, 2008) aiming to detect the referential expressions (mentions) in a text that point to the same entity. Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora—MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from using perfect (gold, or key) mentions as input for purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pr</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction (ACE) program-tasks, data, and evaluation. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Nancy Chinchor</author>
</authors>
<title>Coreference task definition (v3.0, 13 jul 97).</title>
<date>1997</date>
<booktitle>In Proceedings of the 7th Message Understanding Conference.</booktitle>
<contexts>
<context position="1920" citStr="Hirschman and Chinchor, 1997" startWordPosition="267" endWordPosition="270"> measures; and (iv) rescore the results of the CoNLL-2011/2012 shared task systems with this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms. 1 Introduction Coreference resolution is a key task in natural language processing (Jurafsky and Martin, 2008) aiming to detect the referential expressions (mentions) in a text that point to the same entity. Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora—MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from using perfect (gold, or key) mentions as input for purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual</context>
</contexts>
<marker>Hirschman, Chinchor, 1997</marker>
<rawString>Lynette Hirschman and Nancy Chinchor. 1997. Coreference task definition (v3.0, 13 jul 97). In Proceedings of the 7th Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordana Ilic Holen</author>
</authors>
<title>Critical reflections on evaluation practices in coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the NAACL-HLT Student Research Workshop,</booktitle>
<pages>1--7</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2926" citStr="Holen, 2013" startWordPosition="438" endWordPosition="439">pora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 200</context>
</contexts>
<marker>Holen, 2013</marker>
<rawString>Gordana Ilic Holen. 2013. Critical reflections on evaluation practices in coreference resolution. In Proceedings of the NAACL-HLT Student Research Workshop, pages 1–7, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition.</title>
<date>2008</date>
<publisher>Prentice Hall. Second Edition.</publisher>
<contexts>
<context position="1621" citStr="Jurafsky and Martin, 2008" startWordPosition="221" endWordPosition="224">redicted mentions is unnecessary, and potentially harmful as it could produce unintuitive results; (ii) illustrate the application of all these measures to scoring predicted mentions; (iii) make available an opensource, thoroughly-tested reference implementation of the main coreference evaluation measures; and (iv) rescore the results of the CoNLL-2011/2012 shared task systems with this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms. 1 Introduction Coreference resolution is a key task in natural language processing (Jurafsky and Martin, 2008) aiming to detect the referential expressions (mentions) in a text that point to the same entity. Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora—MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from using perfect (gold, or key) mentions as input for purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall. Second Edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Sameer Pradhan</author>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>An extension of BLANC to system mentions.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="5150" citStr="Luo et al., 2014" startWordPosition="787" endWordPosition="790">ne-to-one mention mapping (Stoyanov et al., 2009; Cai and Strube, 2010). Some of these variations arguably produce rather unintuitive results, while others are not faithful to the original measures. In this paper, we address the issues in scoring coreference partitions of predicted mentions. Specifically, we justify our decision to go back to the original scoring algorithms by arguing that manipulation of key or predicted mentions is unnecessary and could in fact produce unintuitive results. We demonstrate the use of our recent extension of BLANC that can seamlessly handle predicted mentions (Luo et al., 2014). We make available an open-source, thoroughly-tested reference implementation of the main coreference evaluation measures that do not involve mention manipulation and is faithful to the original intentions of the proposers of these metrics. We republish the CoNLL-2011/2012 results based on this scorer, so that future systems can use it for evaluation and have the CoNLL results available for comparison. The rest of the paper is organized as follows. Section 2 provides an overview of the variations of the existing measures. We present our newly updated coreference scoring package in Section 3 t</context>
<context position="10481" citStr="Luo et al. (2014)" startWordPosition="1674" endWordPosition="1677">ity. The version illustrated in Section 4.2, which is devoid of any mention manipulations, gives a precision of 0.444 in the first scenario and the precision drops to 0.333 in the second scenario with the addition of a spurious singleton entity {c}. This is a more intuitive behavior. Contrary to both B3 and CEAF, the BLANC measure (Recasens and Hovy, 2011) was never designed to handle predicted mentions. However, the implementation used for the SemEval-2010 shared task as well as the one for the CoNLL-2011/2012 shared tasks accepted predicted mentions as input, producing undefined results. In Luo et al. (2014) we have extended the BLANC metric to deal with predicted mentions 3 Reference Implementation Given the potential unintuitive outcomes of mention manipulation and the misunderstanding that the original measures could not handle twinless predicted mentions (Section 2), we redesigned the CoNLL scorer. The new implementation: • is faithful to the original measures; • removes any prior mention manipulation, which might depend on specific annotation guidelines among other problems; • has been thoroughly tested to ensure that it gives the expected results according to the original papers, and all te</context>
<context position="19907" citStr="Luo et al., 2014" startWordPosition="3426" endWordPosition="3429"> |K1 |+ |K2| P = |K1 n R1 |+ |K2 n R3| |R1 |+ |R2 |+ |R3| The CEAFm F1 score is 0.53. CEAFe We use the same notation as in Luo (2005): 04(Ki, Rj) to denote the similarity between a key entity Ki and a response entity Rj. 04(Ki, Rj) is defined as: 04(Ki, Rj) = 2 x |Ki n Rj| |Ki |+ |Rj |. CEAFe recall and precision, when applied to this example, are: 04(K1, R1) + 04(K2, R3) R = Nk 04(K1, R1) + 04(K2, R3) P = Nr The CEAFe F1 score is 0.52. 4.4 BLANC The BLANC metric illustrated here is the one in our implementation which extends the original BLANC (Recasens and Hovy, 2011) to predicted mentions (Luo et al., 2014). Let Ck and Cr be the set of coreference links in the key and response respectively, and Nk and Nr be the set of non-coreference links in the key and response respectively. A link between a mention pair m and n is denoted by mn; then for the example in Figure 1, we have Ck = {ab, ac, bc, de, df, dg, ef, eg, fg} Nk = {ad, ae, af, ag, bd, be, bf, bg, cd, ce, cf, cg} Cr = {ab, cd, fg, fh, fi, gh, gi, hi} Nr = {ac, ad, af, ag, ah, ai, bc, bd, bf, bg, bh, bi, cf, cg, ch, ci, df, dg, dh, di}. Recall and precision for coreference links are: |Ck n Cr| Rc = |Ck| |Ck n Cr| Pc = |Cr| and the coreference</context>
</contexts>
<marker>Luo, Pradhan, Recasens, Hovy, 2014</marker>
<rawString>Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and Eduard Hovy. 2014. An extension of BLANC to system mentions. In Proceedings of ACL, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="3509" citStr="Luo, 2005" startWordPosition="527" endWordPosition="528">re informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of 30 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30–35, Baltimore, </context>
<context position="8482" citStr="Luo, 2005" startWordPosition="1331" endWordPosition="1332">ng precision or recall. The Cai and Strube (2010) variation was used by the CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus, and by the i2b2 2011 shared task on coreference resolution using an assortment of clinical notes corpora (Uzuner et al., 2012).1 It was later identified by Recasens et al. (2013) that there was a bug in the implementation of this variation in the scorer used for the CoNLL-2011/2012 tasks. We have not tested the correctness of this variation in the scoring package used for the i2b2 shared task. However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly on predicted mentions, and so has been the case with the B3 metric.2 In a latter paper, Rahman and Ng (2011) correctly state that “CEAF can compare partitions with twinless mentions without any modification.” We will look at this further in Section 4.3. We argue that manipulations of key and response mentions/entities, as is done in the existing B3 variations, not only confound the evaluation process, but are also subject to abuse and can seriously jeopardize the fidelity of the evalu1Personal communication with Andreea Bodnari, and contents of the i2b2 </context>
<context position="15873" citStr="Luo (2005)" startWordPosition="2591" endWordPosition="2592">or the official task to enable future comparisons with these benchmarks. The new CoNLL-2011/2012 results are in Table 1. We found that the overall system ranking remained largely unchanged for both shared tasks, except for some of the lower ranking systems that changed one or two places. However, there was a considerable drop in the magnitude of all B3 scores owing to the combination of two things: (i) mention manipulation, as proposed by Cai and Strube (2010), adds singletons to account for twinless mentions; and (ii) the B3 metric allows an entity to be used more than once as pointed out by Luo (2005). This resulted in a drop in the CoNLL averages (B3 is one of the three measures that make the average). 4 An Illustrative Example This section walks through the process of computing each of the commonly used metrics for an example where the set of predicted mentions has some missing key mentions and some spurious mentions. While the mathematical formulae for these metrics can be found in the original papers (Vilain et al., 1995; Bagga and Baldwin, 1998; Luo, 2005), many misunderstandings discussed in Section 2 are due to the fact that these papers lack an example showing how a metric is compu</context>
<context position="19423" citStr="Luo (2005)" startWordPosition="3334" endWordPosition="3335"> The B3 F1 score is 0.46. 4.3 CEAF The first step in the CEAF computation is getting the best scoring alignment between the key and response entities. In this case the alignment is straightforward. Entity R1 aligns with K1 and R3 aligns with K2. R2 remains unaligned. CEAFm CEAFm recall is the number of aligned mentions divided by the number of key mentions, and precision is the number of aligned mentions divided by the number of response mentions: R = |K1 n R1 |+ |K2 n R3| |K1 |+ |K2| P = |K1 n R1 |+ |K2 n R3| |R1 |+ |R2 |+ |R3| The CEAFm F1 score is 0.53. CEAFe We use the same notation as in Luo (2005): 04(Ki, Rj) to denote the similarity between a key entity Ki and a response entity Rj. 04(Ki, Rj) is defined as: 04(Ki, Rj) = 2 x |Ki n Rj| |Ki |+ |Rj |. CEAFe recall and precision, when applied to this example, are: 04(K1, R1) + 04(K2, R3) R = Nk 04(K1, R1) + 04(K2, R3) P = Nr The CEAFe F1 score is 0.52. 4.4 BLANC The BLANC metric illustrated here is the one in our implementation which extends the original BLANC (Recasens and Hovy, 2011) to predicted mentions (Luo et al., 2014). Let Ck and Cr be the set of coreference links in the key and response respectively, and Nk and Nr be the set of no</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of HLT-EMNLP, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: A Unified Relational Semantic Representation.</title>
<date>2007</date>
<journal>International Journal of Semantic Computing,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="2618" citStr="Pradhan et al., 2007" startWordPosition="383" endWordPosition="386">4). Experimental parameters ranged from using perfect (gold, or key) mentions as input for purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system resul</context>
</contexts>
<marker>Pradhan, Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2007</marker>
<rawString>Sameer Pradhan, Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2007. OntoNotes: A Unified Relational Semantic Representation. International Journal of Semantic Computing, 1(4):405–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL: Shared Task,</booktitle>
<pages>1--27</pages>
<contexts>
<context position="3055" citStr="Pradhan et al., 2011" startWordPosition="456" endWordPosition="460"> al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setbac</context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes. In Proceedings of CoNLL: Shared Task, pages 1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Proceedings of CoNLL: Shared Task,</booktitle>
<pages>1--40</pages>
<contexts>
<context position="3078" citStr="Pradhan et al., 2012" startWordPosition="461" endWordPosition="464">ity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations </context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Proceedings of CoNLL: Shared Task, pages 1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised models for coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>968--977</pages>
<contexts>
<context position="7319" citStr="Rahman and Ng (2009)" startWordPosition="1134" endWordPosition="1137">ne mapping between the key and predicted mentions, assuming that the original measures cannot be applied to predicted mentions. Below we first provide an overview of these variations and then discuss the unnecessity of this assumption. Coining the term twinless mentions for those mentions that are either spurious or missing from the predicted mention set, Stoyanov et al. (2009) proposed two variations to B3 — B3all and B30—to handle them. In the first variation, all predicted twinless mentions are retained, whereas the latter discards them and penalizes recall for twinless predicted mentions. Rahman and Ng (2009) proposed another variation by removing “all and only those twinless system mentions that are singletons before applying B3 and CEAF.” Following upon this line of research, Cai and Strube (2010) proposed a unified solution for both B3 and CEAFm, leaving the question of handling CEAFe as future work because “it produces unintuitive results.” The essence of their solution involves manipulating twinless key and predicted mentions by adding them either from the predicted partition to the key partition or vice versa, depending on whether one is computing precision or recall. The Cai and Strube (201</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Altaf Rahman and Vincent Ng. 2009. Supervised models for coreference resolution. In Proceedings of EMNLP, pages 968–977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>814--824</pages>
<contexts>
<context position="8630" citStr="Rahman and Ng (2011)" startWordPosition="1356" endWordPosition="1359">he OntoNotes corpus, and by the i2b2 2011 shared task on coreference resolution using an assortment of clinical notes corpora (Uzuner et al., 2012).1 It was later identified by Recasens et al. (2013) that there was a bug in the implementation of this variation in the scorer used for the CoNLL-2011/2012 tasks. We have not tested the correctness of this variation in the scoring package used for the i2b2 shared task. However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly on predicted mentions, and so has been the case with the B3 metric.2 In a latter paper, Rahman and Ng (2011) correctly state that “CEAF can compare partitions with twinless mentions without any modification.” We will look at this further in Section 4.3. We argue that manipulations of key and response mentions/entities, as is done in the existing B3 variations, not only confound the evaluation process, but are also subject to abuse and can seriously jeopardize the fidelity of the evalu1Personal communication with Andreea Bodnari, and contents of the i2b2 scorer code. 2Personal communication with Breck Baldwin. 31 ation. Given space constraints we use an example worked out in Cai and Strube (2010). Le</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Coreference resolution with world knowledge. In Proceedings of ACL, pages 814–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>BLANC: Implementing the Rand Index for coreference evaluation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>510</pages>
<contexts>
<context position="3565" citStr="Recasens and Hovy, 2011" startWordPosition="534" endWordPosition="537">2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of 30 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30–35, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for C</context>
<context position="10222" citStr="Recasens and Hovy, 2011" startWordPosition="1634" endWordPosition="1637">56. But, if the prediction contains two entities {a, b, d} and {c} (i.e., the mention c is added as a spurious singleton), then B3all precision increases to 0.667 which is counter-intuitive as it does not penalize the fact that c is erroneously placed in its own entity. The version illustrated in Section 4.2, which is devoid of any mention manipulations, gives a precision of 0.444 in the first scenario and the precision drops to 0.333 in the second scenario with the addition of a spurious singleton entity {c}. This is a more intuitive behavior. Contrary to both B3 and CEAF, the BLANC measure (Recasens and Hovy, 2011) was never designed to handle predicted mentions. However, the implementation used for the SemEval-2010 shared task as well as the one for the CoNLL-2011/2012 shared tasks accepted predicted mentions as input, producing undefined results. In Luo et al. (2014) we have extended the BLANC metric to deal with predicted mentions 3 Reference Implementation Given the potential unintuitive outcomes of mention manipulation and the misunderstanding that the original measures could not handle twinless predicted mentions (Section 2), we redesigned the CoNLL scorer. The new implementation: • is faithful to</context>
<context position="19866" citStr="Recasens and Hovy, 2011" startWordPosition="3419" endWordPosition="3422"> of response mentions: R = |K1 n R1 |+ |K2 n R3| |K1 |+ |K2| P = |K1 n R1 |+ |K2 n R3| |R1 |+ |R2 |+ |R3| The CEAFm F1 score is 0.53. CEAFe We use the same notation as in Luo (2005): 04(Ki, Rj) to denote the similarity between a key entity Ki and a response entity Rj. 04(Ki, Rj) is defined as: 04(Ki, Rj) = 2 x |Ki n Rj| |Ki |+ |Rj |. CEAFe recall and precision, when applied to this example, are: 04(K1, R1) + 04(K2, R3) R = Nk 04(K1, R1) + 04(K2, R3) P = Nr The CEAFe F1 score is 0.52. 4.4 BLANC The BLANC metric illustrated here is the one in our implementation which extends the original BLANC (Recasens and Hovy, 2011) to predicted mentions (Luo et al., 2014). Let Ck and Cr be the set of coreference links in the key and response respectively, and Nk and Nr be the set of non-coreference links in the key and response respectively. A link between a mention pair m and n is denoted by mn; then for the example in Figure 1, we have Ck = {ab, ac, bc, de, df, dg, ef, eg, fg} Nk = {ad, ae, af, ag, bd, be, bf, bg, cd, ce, cf, cg} Cr = {ab, cd, fg, fh, fi, gh, gi, hi} Nr = {ac, ad, af, ag, ah, ai, bc, bd, bf, bg, bh, bi, cf, cg, ch, ci, df, dg, dh, di}. Recall and precision for coreference links are: |Ck n Cr| Rc = |Ck</context>
</contexts>
<marker>Recasens, Hovy, 2011</marker>
<rawString>Marta Recasens and Eduard Hovy. 2011. BLANC: Implementing the Rand Index for coreference evaluation. Natural Language Engineering, 17(4):485– 510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Llu´ıs M`arquez</author>
<author>Emili Sapena</author>
<author>M</author>
</authors>
<title>Ant`onia Mart´ı, Mariona Taul´e, V´eronique Hoste, Massimo Poesio, and Yannick Versley.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>1--8</pages>
<marker>Recasens, M`arquez, Sapena, M, 2010</marker>
<rawString>Marta Recasens, Llu´ıs M`arquez, Emili Sapena, M. Ant`onia Mart´ı, Mariona Taul´e, V´eronique Hoste, Massimo Poesio, and Yannick Versley. 2010. Semeval-2010 task 1: Coreference resolution in multiple languages. In Proceedings of SemEval, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Chris Potts</author>
</authors>
<title>The life and death of discourse entities: Identifying singleton mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>627--633</pages>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Chris Potts. 2013. The life and death of discourse entities: Identifying singleton mentions. In Proceedings of NAACL-HLT, pages 627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>656--664</pages>
<contexts>
<context position="2446" citStr="Stoyanov et al. (2009)" startWordPosition="354" endWordPosition="357"> individually crafted evaluations based on two central corpora—MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from using perfect (gold, or key) mentions as input for purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et </context>
<context position="4581" citStr="Stoyanov et al., 2009" startWordPosition="694" endWordPosition="697">us computation of 30 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30–35, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the BLANC metric for partitions of predicted mentions. Different interpretations as to how to compute B3 and CEAF scores for coreference systems when predicted mentions do not perfectly align with key mentions—which is usually the case— led to variations of these metrics that manipulate the gold standard and system output in order to get a one-to-one mention mapping (Stoyanov et al., 2009; Cai and Strube, 2010). Some of these variations arguably produce rather unintuitive results, while others are not faithful to the original measures. In this paper, we address the issues in scoring coreference partitions of predicted mentions. Specifically, we justify our decision to go back to the original scoring algorithms by arguing that manipulation of key or predicted mentions is unnecessary and could in fact produce unintuitive results. We demonstrate the use of our recent extension of BLANC that can seamlessly handle predicted mentions (Luo et al., 2014). We make available an open-sou</context>
<context position="6621" citStr="Stoyanov et al., 2009" startWordPosition="1019" endWordPosition="1023">ified in their application for scoring predicted, as opposed to key mentions. The examples in the papers describing these metrics assume perfect mentions where predicted mentions are the same set of mentions as key mentions. The lack of accompanying reference implementation for these metrics by its proposers made it harder to fill the gaps in the specification. Subsequently, different interpretations of how one can evaluate coreference systems when predicted mentions do not perfectly align with key mentions led to variations of these metrics that manipulate the gold and/or predicted mentions (Stoyanov et al., 2009; Cai and Strube, 2010). All these variations attempted to generate a one-to-one mapping between the key and predicted mentions, assuming that the original measures cannot be applied to predicted mentions. Below we first provide an overview of these variations and then discuss the unnecessity of this assumption. Coining the term twinless mentions for those mentions that are either spurious or missing from the predicted mention set, Stoyanov et al. (2009) proposed two variations to B3 — B3all and B30—to handle them. In the first variation, all predicted twinless mentions are retained, whereas t</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art. In Proceedings of ACL-IJCNLP, pages 656–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William F Styler</author>
<author>Steven Bethard an Sean Finan</author>
<author>Martha Palmer</author>
<author>Sameer Pradhan</author>
<author>Piet C de Groen</author>
<author>Brad Erickson</author>
<author>Timothy Miller</author>
<author>Chen Lin</author>
<author>Guergana Savova</author>
<author>James Pustejovsky</author>
</authors>
<title>Temporal annotation in the clinical domain.</title>
<date>2014</date>
<journal>Transactions of Computational Linguistics,</journal>
<pages>2--143</pages>
<marker>Styler, Finan, Palmer, Pradhan, de Groen, Erickson, Miller, Lin, Savova, Pustejovsky, 2014</marker>
<rawString>William F. Styler, Steven Bethard an Sean Finan, Martha Palmer, Sameer Pradhan, Piet C de Groen, Brad Erickson, Timothy Miller, Chen Lin, Guergana Savova, and James Pustejovsky. 2014. Temporal annotation in the clinical domain. Transactions of Computational Linguistics, 2(April):143–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozlem Uzuner</author>
<author>Andreea Bodnari</author>
<author>Shuying Shen</author>
<author>Tyler Forbush</author>
<author>John Pestian</author>
<author>Brett R South</author>
</authors>
<title>Evaluating the state of the art in coreference resolution for electronic medical records.</title>
<date>2012</date>
<journal>Journal of American Medical Informatics Association,</journal>
<volume>19</volume>
<issue>5</issue>
<contexts>
<context position="2709" citStr="Uzuner et al., 2012" startWordPosition="400" endWordPosition="403">purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis o</context>
<context position="8157" citStr="Uzuner et al., 2012" startWordPosition="1272" endWordPosition="1275">tion for both B3 and CEAFm, leaving the question of handling CEAFe as future work because “it produces unintuitive results.” The essence of their solution involves manipulating twinless key and predicted mentions by adding them either from the predicted partition to the key partition or vice versa, depending on whether one is computing precision or recall. The Cai and Strube (2010) variation was used by the CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus, and by the i2b2 2011 shared task on coreference resolution using an assortment of clinical notes corpora (Uzuner et al., 2012).1 It was later identified by Recasens et al. (2013) that there was a bug in the implementation of this variation in the scorer used for the CoNLL-2011/2012 tasks. We have not tested the correctness of this variation in the scoring package used for the i2b2 shared task. However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly on predicted mentions, and so has been the case with the B3 metric.2 In a latter paper, Rahman and Ng (2011) correctly state that “CEAF can compare partitions with twinless mentions without any modification.” We will look at this furth</context>
</contexts>
<marker>Uzuner, Bodnari, Shen, Forbush, Pestian, South, 2012</marker>
<rawString>Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler Forbush, John Pestian, and Brett R South. 2012. Evaluating the state of the art in coreference resolution for electronic medical records. Journal of American Medical Informatics Association, 19(5), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A model theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="3460" citStr="Vilain et al., 1995" startWordPosition="517" endWordPosition="520">proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of 30 Proceedings of the 52nd Annual Meeting of the Association for Computational Lin</context>
<context position="16305" citStr="Vilain et al., 1995" startWordPosition="2666" endWordPosition="2669">ion, as proposed by Cai and Strube (2010), adds singletons to account for twinless mentions; and (ii) the B3 metric allows an entity to be used more than once as pointed out by Luo (2005). This resulted in a drop in the CoNLL averages (B3 is one of the three measures that make the average). 4 An Illustrative Example This section walks through the process of computing each of the commonly used metrics for an example where the set of predicted mentions has some missing key mentions and some spurious mentions. While the mathematical formulae for these metrics can be found in the original papers (Vilain et al., 1995; Bagga and Baldwin, 1998; Luo, 2005), many misunderstandings discussed in Section 2 are due to the fact that these papers lack an example showing how a metric is computed on predicted mentions. A concrete example goes a long way to prevent similar misunderstandings in the future. The example is adapted from Vilain et al. (1995) with some slight modifications so that the total number of mentions in the key is different from the number of mentions in the prediction. The key (K) contains two entities with mentions {a, b, c} and {d, e, f, g} and the response (R) contains three entities with menti</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A model theoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference, pages 45–52.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ralph Weischedel</author>
</authors>
<title>Eduard Hovy,</title>
<location>Mitchell Marcus, Martha Palmer, Robert Belvin, Sameer Pradhan,</location>
<marker>Weischedel, </marker>
<rawString>Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha Palmer, Robert Belvin, Sameer Pradhan,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Nianwen Xue</author>
</authors>
<title>OntoNotes: A large training corpus for enhanced processing.</title>
<date>2011</date>
<booktitle>Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation.</booktitle>
<editor>In Joseph Olive, Caitlin Christianson, and John McCary, editors,</editor>
<publisher>Springer.</publisher>
<marker>Ramshaw, Xue, 2011</marker>
<rawString>Lance Ramshaw, and Nianwen Xue. 2011. OntoNotes: A large training corpus for enhanced processing. In Joseph Olive, Caitlin Christianson, and John McCary, editors, Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>