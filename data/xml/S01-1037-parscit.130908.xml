<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003819">
<title confidence="0.9797905">
WASP-Bench: a Lexicographic Tool Supporting Word Sense
Disambiguation
</title>
<author confidence="0.992615">
David Tugwell 86 Adam Kilgarriff
</author>
<affiliation confidence="0.997755">
ITRI, University of Brighton
</affiliation>
<address confidence="0.501331">
Lewes Road, Brighton BN2 4GJ, UK
</address>
<email confidence="0.785927">
{David.Tugwell,Adam.Kilgarriff}Oitri.bton.ac.uk
</email>
<sectionHeader confidence="0.990909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955928571429">
We present WASP-Bench: a novel approach to
Word Sense Disambiguation, also providing a
semi-automatic environment for a lexicographer
to compose dictionary entries based on corpus
evidence. For WSD, involving lexicographers
tackles the twin obstacles to high accuracy:
paucity of training data and insufficiently ex-
plicit dictionaries. For lexicographers, the com-
putational environment fills the need for a cor-
pus workbench which supports WSD. Results
under simulated lexicographic use on the En-
glish lexical-sample task show precision compa-
rable with supervised systems&apos;, without using
the laboriously-prepared training data.
</bodyText>
<sectionHeader confidence="0.99844" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999611">
WASP-Bench2 is a web-based tool support-
ing both corpus-based lexicography and Word
Sense Disambiguation. The central premise be-
hind the initiative is that deciding what the
senses for a word are, and developing a WSD
program for it, should be tightly coupled. In the
course of the corpus analysis, the lexicographer
explores the textual clues that indicate a word
is being used in one sense or another; given an
appropriate computational environment, these
clues can be gathered and used to seed a boot-
strapping WSD program.
This strategy clearly requires human input for
each word to be disambiguated, which may raise
</bodyText>
<footnote confidence="0.8856835">
&apos;It should be noted that the lower figure for recall
reflects solely the fact that not all words were attempted
due to time constraints.
2The system has been developed under EP-
SRC project M54971. A demo is available at
http://wasps.itri.bton.ac.uk. The second author was also
a co-ordinator for the SENSEVAL-2 evaluation exercise-to
limit any conflict of interest only the first author was in-
volved applying the system to the SENSEVAL-2 task, and
had no prior knowledge of the format of the task.
</footnote>
<bodyText confidence="0.999741">
the objection that the lexicon is far too large
for any word-by-word work to be viable. How-
ever, the amount of human interaction needed
is far less than that involved in preparing train-
ing data3 and lexicographers are already in the
position of having to inspect every word in the
vocabulary. If they use a interactive tool such
as the WASP-Bench to help them in this, then
total coverage becomes a feasible proposition.
</bodyText>
<sectionHeader confidence="0.972327" genericHeader="method">
2 WASP-Bench Methodology
</sectionHeader>
<bodyText confidence="0.9998115">
The workbench is implemented in perl and uses
cgi-scripts and a browser for user interaction.
</bodyText>
<subsectionHeader confidence="0.98594">
2.1 Grammatical relations database
</subsectionHeader>
<bodyText confidence="0.999979318181818">
The central resource is a collection of all gram-
matical relations holding between words in the
corpus. The corpus currently used in WASP-
Bench is the British National Corpus4 (BNC):
. Using finite-state techniques operating over
part-of-speech tags, we process the whole cor-
pus finding quintuples of the form: {Rel, W1,
W2, Prep, Position}, where Rel is a relation,
W1 is the lemma of the word for which Rel
holds, W2 is the lemma of the other open-class
word involved, Prep is the preposition or parti-
cle involved and Position is the position of W1
in the corpus. Relations may have null values
for W2 and Prep. The database contains 70
million quintuples.
The current inventory of relations is shown
in Table 1. All inverse relations, ie. subject-of
etc, found by taking W2 as the head word in-
stead of W1 are explicitly represented, to give a
total of twenty-six distinct relations. These pro-
vide a flexible resource to be used as the basis
of the computations of the workbench. Keeping
</bodyText>
<footnote confidence="0.999114666666667">
3See results section for details.
4 100 million words of contemporary British English.
see http://info.ox.ac.uk/bnc
</footnote>
<page confidence="0.995323">
151
</page>
<figureCaption confidence="0.98210735">
relation example
bare-noun the angle of bank&apos;
possessive my bank&apos;
plural the banks&apos;
passive was seen&apos;
reflexive see&amp;quot; herself
ing-comp love&apos; eating fish
finite-comp know&apos; he came
inf-comp decision&apos; to eat fish
wh-comp know&apos; why he came
subject the bank&apos; refused&apos;
object climb&amp;quot; the bank&apos;
adj-comp grow&apos; certain&apos;
noun-modifier merchant&apos; bank&apos;
modifier a big&apos; bank&apos;
and-or banks&apos; and mounds&apos;
predicate banks&apos; are barriers&apos;
particle growl upP
Prep+gerund tired&apos; ofP eating fish
PP-comp/mod banks&apos; of the river&apos;
</figureCaption>
<tableCaption confidence="0.965795">
Table 1: Grammatical Relations
</tableCaption>
<bodyText confidence="0.999874">
the position numbers of examples allows us to
find associations between relations and to dis-
play examples.
</bodyText>
<subsectionHeader confidence="0.996525">
2.2 Word Sketches
</subsectionHeader>
<bodyText confidence="0.954874391304348">
The user enters the word and using the gram-
matical relations database, the system com-
poses a word sketch for this word. This is
a page of data such as Table 2, which shows,
for the word in question (W1), ordered lists
of high-salience grammatical relations, relation-
W2 pairs, and relation-W2-Prep triples for the
word.
The number of patterns shown is set by the
user, but will typically be over 200. These are
listed for each relation in order of salience, with
the count of corpus instances. The instances
can be instantly retrieved and shown in a con-
cordance window. Producing a word sketch for
a medium-to-high frequency word takes in the
order of ten seconds.
Salience is calculated as the product of Mu-
tual Information / (Church and Hanks, 1989)
and log frequency. / for a word W1 in a gram-
matical relation Rel5 with a second word W2 is
calculated as:
5 { Grammatical-relation, preposition} pairs are
treated as atomic relations in calculating MI.
</bodyText>
<equation confidence="0.938281666666667">
,)&lt;W
I (W1, Rel, W2) = log( e il * Ret
iWl Rei:w w 221)
</equation>
<bodyText confidence="0.9999829375">
The notation here is adopted from (Lin, 1998)
(who also spells out the derivation from the
definition of /). 11W1, Rel, W2li denotes the
frequency count of the triple {W1, Rel, W2}6
in the grammatical relations database. Where
Wl, Rel or W2 is the wild card (*), the fre-
quency is of all the dependency triples that
match the remainder of the pattern.
The word sketches are presented to the user
as a list of relations, with items in each list or-
dered according to salience. Our experience of
working lexicographers&apos; use of Mutual Informa-
tion or log-likelihood lists shows that, for lex-
icographic purposes, these over-emphasise low
frequency items, and that multiplying by log
frequency is an appropriate adjustment.
</bodyText>
<subsectionHeader confidence="0.96639">
2.3 Matching patterns with senses
</subsectionHeader>
<bodyText confidence="0.999943545454546">
The next task is to enter a preliminary list
of senses for the word, possibly in the form
of some arbitrary mnemonics: for example,
MONEY, CLOUD and RIVER for three senses of
bank.7 This inventory may be drawn from the
user&apos;s knowledge, from a perusal of the word
sketch, or from a pre-existing dictionary entry.
As Table 2 shows, and in keeping with &amp;quot;one
sense per collocation&amp;quot; (Yarowsky, 1993) in most
cases, high-salience patterns or clues indicate
just one of the word&apos;s senses. The user then
has the task of associating, by selecting from
a pop-up menu, the required sense for unam-
biguous clues. The number of relations marked
will depend on the time available, as well as
the complexity of the sense division to be made.
The act of assigning senses to patterns may very
well lead the user to discover fresh, unconsid-
ered senses usages of the word.
The pattern-sense associations are then sub-
mitted to the next stage: automatic disam-
biguation.
</bodyText>
<subsectionHeader confidence="0.969027">
2.4 The Disambiguation Algorithm
</subsectionHeader>
<bodyText confidence="0.999153666666667">
The workbench currently uses Yarowsky&apos;s de-
cision list approach to WSD (Yarowsky, 1995).
This is a bootstrapping algorithm that, given
</bodyText>
<footnote confidence="0.9917772">
60r, strictly, of the quintuple {Wl, Rel — part —
1, W2, Rel — part 2, ANY}.
-7WASP-Bench can also be used for Machine Transla-
tion lexicography, where arbitrary mnemonics would be
replaced by target language translations.
</footnote>
<page confidence="0.99019">
152
</page>
<table confidence="0.9989716">
subj-of num sal obj-of num sal modifier num sal n-mod num sal
lend 95 21.2 burst 27 16.4 central 755 25.5 merchant 213 29.4
issue 60 11.8 rob 31 15.3 Swiss 87 18.7 clearing 127 27.0
charge 29 9.5 overflow 7 10.2 commercial 231 18.6 river 217 25.4
operate 45 8.9 line 13 8.4 grassy 42 18.5 creditor 52 22.8
modifies PP iv-PP and-or
holiday 404 32.6 of England 988 37.5 governor of 108 26.2 society 287 24.6
account 503 32.0 of Scotland 242 26.9 balance at 25 20.2 bank 107 17.7
loan 108 27.5 of river 111 22.1 borrow from 42 19.1 institution 82 16.0
lending 68 26.1 of Thames 41 20.1 account with 30 18.4 Lloyds 11 14.1
</table>
<tableCaption confidence="0.999697">
Table 2: Extract of word sketch for bank
</tableCaption>
<bodyText confidence="0.999899318181818">
some initial seeding, iteratively divides the
corpus examples into the different senses.
Yarowsky notes that the most effective ini-
tial seeding option he considered was labelling
salient corpus collocates with different senses.
The user&apos;s first interaction with the workbench
is just this.
At the user-input stage, only clues involving
grammatical relations are used. At the WSD al-
gorithm stage, some &amp;quot;bag-of-words&amp;quot; and n-gram
clues are also considered. Any content word
(lemmatised) occurring within a k-word window
of the nodeword is a bag-of-words clue.8 N-
gram clues capture local context which may not
be covered by any grammatical relation. The
n-gram clues are all bigrams and trigrams in-
cluding the nodeword. N-grams and context-
word clues frequently duplicate the grammati-
cal relations already found, but the merit of the
decision list approach is that probabilities are
not combined, so such dependencies are not a
problem.
</bodyText>
<subsectionHeader confidence="0.976738">
2.5 Sense Profiles
</subsectionHeader>
<bodyText confidence="0.999973090909091">
The output of the algorithm is both a sense dis-
ambiguated corpus, and a decision list. The de-
cision list can be viewed as a lexical entry or
as a WSD program. It will contain {Rel, W2}
pairs (as in the original word sketch), bag-of-
words words, and n-grams. The components
of the decision list which assign to a particular
sense can be displayed as &amp;quot;sense profiles&amp;quot;, in a
manner comparable to the original word sketch.
They will contain new clues, not originally seen
in the word sketch and may point to new senses
</bodyText>
<footnote confidence="0.985741">
8The user can set the value of k. The default is cur-
rently 30.
</footnote>
<bodyText confidence="0.9982582">
or usages needing addition to the lexical entry.
Users can then re-run the WSD algorithm, it-
erating until they are satisfied with the sense
inventory, and with the accuracy of the disam-
biguation performed.
</bodyText>
<sectionHeader confidence="0.963641" genericHeader="method">
3 Evaluating the workbench
</sectionHeader>
<subsectionHeader confidence="0.996858">
3.1 Lexicographic evaluation
</subsectionHeader>
<bodyText confidence="0.999995923076923">
For the last two years, a set of 6000 word
sketches has been used in a large dictio-
nary project (Rundell, 2002), with a team of
thirty professional lexicographers covering ev-
ery medium-to-high frequency noun, verb and
adjective of English. The feedback received is
that they are hugely useful, and transform the
way the lexicographer uses the corpus. They
radically reduce the amount of time the lex-
icographers need to spend reading individual
instances, and give the dictionary improved
claims to completeness, as common patterns are
far less likely to be missed.
</bodyText>
<subsectionHeader confidence="0.763478">
3.2 Results for senseval-2
</subsectionHeader>
<bodyText confidence="0.999653533333334">
Performance as a WSD system was evaluated on
the SENSEVAL-2 English lexical sample exercise.
The words to be tested were divided between
the first author and one paid volunteer, who had
no previous experience of WASP-Bench. We
carried out the procedure as above, with the
difference that instead of having to establish a
sense inventory, the inventory was already given
as that of WordNet. After assigning sufficient
clues to cover the various senses, these assign-
ments were submitted as seeds to the disam-
biguation algorithm Using the example sen-
tences from the BNC this gave us a decision list
of clues, which could then be used to disam-
biguate the test sentences.
</bodyText>
<page confidence="0.99817">
153
</page>
<bodyText confidence="0.99954396">
The marking of senses took anywhere from
3 to 35 minutes, depending upon the subtlety
of the sense divisions to be made. The average
time was around 15 minutes per word. A sub-
stantial part of this was taken up by reading and
understanding the dictionary entry even before
patterns were marked. Crucially we made no
use of the training data,&apos; although this would
certainly have been of use as a reference in clar-
ifying the sense distinctions to be made. Unfor-
tunately, due to severe time constraints, it only
proved possible to carry out analysis for the 29
nouns and 15 adjectives in the lexical sample,
and there was no time to carry out the analysis
of the verbs.10
Results on the task were 66.1% for coarse-
grained precision and 58.1% for fine-grained.&amp;quot;
This was significantly higher than other systems
which did not use the training data (the best
scores being 51.8% for coarse-grained and 40.2%
for fine-grained precision), demonstrating that
the relatively small amount of human interac-
tion is very beneficial. Indeed, the system&apos;s per-
formance was similar to the majority of systems
which had used the training data.
</bodyText>
<subsubsectionHeader confidence="0.507751">
3.2.1 Significant problems
</subsubsectionHeader>
<bodyText confidence="0.999852333333333">
The most pervasive problem was the difficulty
of getting a clear conception of the sense dis-
tinctions made in the inventory, here WordNet.
Without this, assigning putative senses to clues
could be an exasperating and painful task.
To illustrate, for the adjective simple there
were no less than 13 sense distinctions to be
made, the first two of which were particularly
hard to distinguish:
</bodyText>
<listItem confidence="0.8207914">
1. simple (vs. complex) - (not complex or
complicated or involved): a simple problem
2. elementary, simple, uncomplicated, un-
problematic - (not involved or compli-
cated): an elementary problem in statistics
</listItem>
<footnote confidence="0.4662908">
9In fact, we had to download the data to find out the
words to be tested, but made no other use of it.
1mAlso no results were returned for the noun day, as
processing the 93,000+ examples in the BNC led to an
processing delay that could not be fixed in time.
&amp;quot;Due to the limited number of words attempted the
figures for recall were 36.3% and 31.9%. It should be
understood that there was no precision/recall tradeoff
here the system returned an answer for all sentences in
the words it covered.
</footnote>
<bodyText confidence="0.999965333333333">
Unsurprisingly, the system fared particularly
badly here with 37.9% precision, while inter-
annotator agreement was also low at 67.8%.
</bodyText>
<subsubsectionHeader confidence="0.840479">
3.2.2 Previous results
</subsubsectionHeader>
<bodyText confidence="0.99994875">
We previously measured the performance of the
system on the dataset from the SENSEVAL-1 ex-
ercise (Kilgarriff and Palmer, 2000) under sim-
ilar conditions of use. Results for the WASP-
Bench here were significantly higher at 74.9%
precision which was very close to the best super-
vised system (within 1%). This was undoubt-
edly due to the clearer sense distinctions and
greater number of examples to be found in the
sense inventory used for this task in SENSEVAL-
1, which made it possible to assign senses to
clues with more confidence.
</bodyText>
<sectionHeader confidence="0.998067" genericHeader="method">
4 Summary
</sectionHeader>
<bodyText confidence="0.999968">
The results for the WASP-Bench show that
high-quality disambiguation can be achieved
with much less human interaction than is
needed for preparing a training corpus. Further-
more, this interaction can be motivated since it
has been shown to be of proven benefit for the
users of the system: lexicographers. Establish-
ing this synergy may prove to be of great im-
portance for both camps.
</bodyText>
<sectionHeader confidence="0.997911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99340815">
Kenneth Church and Patrick Hanks. 1989.
Word association norms, mutual information
and lexicography. In ACL Proceedings, 27th
Annual Meeting, pages 76-83, Vancouver.
Adam Kilgarriff and Martha Palmer. 2000.
Introduction, Special Issue on SENSEVAL:
Evaluating Word Sense Disambiguation Pro-
grams. Computers and the Humanities, 34(1-
2):1-13.
Dekang Lin. 1998. Automatic retrieval and
clustering of similar words. In COLING-
ACL, pages 768-774, Montreal.
Michael Rundell. 2002. Macmillan English Dic-
tionary for Advanced Learners. Macmillan.
David Yarowsky. 1993. One sense per colloca-
tion. In Proc. ARPA Human Language Tech-
nology Workshop, Princeton.
David Yarowsky. 1995. Unsupervised word
sense disambiguation rivalling supervised
methods. In ACL 95, pages 189-196, MIT.
</reference>
<page confidence="0.999771">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.640725">
<title confidence="0.9996095">WASP-Bench: a Lexicographic Tool Supporting Word Disambiguation</title>
<author confidence="0.993374">David Tugwell Adam</author>
<affiliation confidence="0.834487">ITRI, University of Lewes Road, Brighton BN2 4GJ,</affiliation>
<email confidence="0.963313">David.TugwellOitri.bton.ac.uk</email>
<email confidence="0.963313">Adam.KilgarriffOitri.bton.ac.uk</email>
<abstract confidence="0.999604666666667">We present WASP-Bench: a novel approach to Word Sense Disambiguation, also providing a semi-automatic environment for a lexicographer to compose dictionary entries based on corpus evidence. For WSD, involving lexicographers tackles the twin obstacles to high accuracy: paucity of training data and insufficiently explicit dictionaries. For lexicographers, the computational environment fills the need for a corpus workbench which supports WSD. Results under simulated lexicographic use on the English lexical-sample task show precision comparable with supervised systems&apos;, without using the laboriously-prepared training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1989</date>
<booktitle>In ACL Proceedings, 27th Annual Meeting,</booktitle>
<pages>76--83</pages>
<location>Vancouver.</location>
<contexts>
<context position="5075" citStr="Church and Hanks, 1989" startWordPosition="806" endWordPosition="809"> a page of data such as Table 2, which shows, for the word in question (W1), ordered lists of high-salience grammatical relations, relationW2 pairs, and relation-W2-Prep triples for the word. The number of patterns shown is set by the user, but will typically be over 200. These are listed for each relation in order of salience, with the count of corpus instances. The instances can be instantly retrieved and shown in a concordance window. Producing a word sketch for a medium-to-high frequency word takes in the order of ten seconds. Salience is calculated as the product of Mutual Information / (Church and Hanks, 1989) and log frequency. / for a word W1 in a grammatical relation Rel5 with a second word W2 is calculated as: 5 { Grammatical-relation, preposition} pairs are treated as atomic relations in calculating MI. ,)&lt;W I (W1, Rel, W2) = log( e il * Ret iWl Rei:w w 221) The notation here is adopted from (Lin, 1998) (who also spells out the derivation from the definition of /). 11W1, Rel, W2li denotes the frequency count of the triple {W1, Rel, W2}6 in the grammatical relations database. Where Wl, Rel or W2 is the wild card (*), the frequency is of all the dependency triples that match the remainder of the</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Church and Patrick Hanks. 1989. Word association norms, mutual information and lexicography. In ACL Proceedings, 27th Annual Meeting, pages 76-83, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Martha Palmer</author>
</authors>
<date>2000</date>
<booktitle>Introduction, Special Issue on SENSEVAL: Evaluating Word Sense Disambiguation Programs. Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="13663" citStr="Kilgarriff and Palmer, 2000" startWordPosition="2263" endWordPosition="2266">y, as processing the 93,000+ examples in the BNC led to an processing delay that could not be fixed in time. &amp;quot;Due to the limited number of words attempted the figures for recall were 36.3% and 31.9%. It should be understood that there was no precision/recall tradeoff here the system returned an answer for all sentences in the words it covered. Unsurprisingly, the system fared particularly badly here with 37.9% precision, while interannotator agreement was also low at 67.8%. 3.2.2 Previous results We previously measured the performance of the system on the dataset from the SENSEVAL-1 exercise (Kilgarriff and Palmer, 2000) under similar conditions of use. Results for the WASPBench here were significantly higher at 74.9% precision which was very close to the best supervised system (within 1%). This was undoubtedly due to the clearer sense distinctions and greater number of examples to be found in the sense inventory used for this task in SENSEVAL1, which made it possible to assign senses to clues with more confidence. 4 Summary The results for the WASP-Bench show that high-quality disambiguation can be achieved with much less human interaction than is needed for preparing a training corpus. Furthermore, this int</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>Adam Kilgarriff and Martha Palmer. 2000. Introduction, Special Issue on SENSEVAL: Evaluating Word Sense Disambiguation Programs. Computers and the Humanities, 34(1-2):1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLINGACL,</booktitle>
<pages>768--774</pages>
<location>Montreal.</location>
<contexts>
<context position="5379" citStr="Lin, 1998" startWordPosition="866" endWordPosition="867">of salience, with the count of corpus instances. The instances can be instantly retrieved and shown in a concordance window. Producing a word sketch for a medium-to-high frequency word takes in the order of ten seconds. Salience is calculated as the product of Mutual Information / (Church and Hanks, 1989) and log frequency. / for a word W1 in a grammatical relation Rel5 with a second word W2 is calculated as: 5 { Grammatical-relation, preposition} pairs are treated as atomic relations in calculating MI. ,)&lt;W I (W1, Rel, W2) = log( e il * Ret iWl Rei:w w 221) The notation here is adopted from (Lin, 1998) (who also spells out the derivation from the definition of /). 11W1, Rel, W2li denotes the frequency count of the triple {W1, Rel, W2}6 in the grammatical relations database. Where Wl, Rel or W2 is the wild card (*), the frequency is of all the dependency triples that match the remainder of the pattern. The word sketches are presented to the user as a list of relations, with items in each list ordered according to salience. Our experience of working lexicographers&apos; use of Mutual Information or log-likelihood lists shows that, for lexicographic purposes, these over-emphasise low frequency item</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In COLINGACL, pages 768-774, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Rundell</author>
</authors>
<title>Macmillan English Dictionary for Advanced Learners.</title>
<date>2002</date>
<publisher>Macmillan.</publisher>
<contexts>
<context position="9987" citStr="Rundell, 2002" startWordPosition="1655" endWordPosition="1656">isplayed as &amp;quot;sense profiles&amp;quot;, in a manner comparable to the original word sketch. They will contain new clues, not originally seen in the word sketch and may point to new senses 8The user can set the value of k. The default is currently 30. or usages needing addition to the lexical entry. Users can then re-run the WSD algorithm, iterating until they are satisfied with the sense inventory, and with the accuracy of the disambiguation performed. 3 Evaluating the workbench 3.1 Lexicographic evaluation For the last two years, a set of 6000 word sketches has been used in a large dictionary project (Rundell, 2002), with a team of thirty professional lexicographers covering every medium-to-high frequency noun, verb and adjective of English. The feedback received is that they are hugely useful, and transform the way the lexicographer uses the corpus. They radically reduce the amount of time the lexicographers need to spend reading individual instances, and give the dictionary improved claims to completeness, as common patterns are far less likely to be missed. 3.2 Results for senseval-2 Performance as a WSD system was evaluated on the SENSEVAL-2 English lexical sample exercise. The words to be tested wer</context>
</contexts>
<marker>Rundell, 2002</marker>
<rawString>Michael Rundell. 2002. Macmillan English Dictionary for Advanced Learners. Macmillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proc. ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton.</location>
<contexts>
<context position="6479" citStr="Yarowsky, 1993" startWordPosition="1052" endWordPosition="1053">Information or log-likelihood lists shows that, for lexicographic purposes, these over-emphasise low frequency items, and that multiplying by log frequency is an appropriate adjustment. 2.3 Matching patterns with senses The next task is to enter a preliminary list of senses for the word, possibly in the form of some arbitrary mnemonics: for example, MONEY, CLOUD and RIVER for three senses of bank.7 This inventory may be drawn from the user&apos;s knowledge, from a perusal of the word sketch, or from a pre-existing dictionary entry. As Table 2 shows, and in keeping with &amp;quot;one sense per collocation&amp;quot; (Yarowsky, 1993) in most cases, high-salience patterns or clues indicate just one of the word&apos;s senses. The user then has the task of associating, by selecting from a pop-up menu, the required sense for unambiguous clues. The number of relations marked will depend on the time available, as well as the complexity of the sense division to be made. The act of assigning senses to patterns may very well lead the user to discover fresh, unconsidered senses usages of the word. The pattern-sense associations are then submitted to the next stage: automatic disambiguation. 2.4 The Disambiguation Algorithm The workbench</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In Proc. ARPA Human Language Technology Workshop, Princeton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivalling supervised methods.</title>
<date>1995</date>
<booktitle>In ACL 95,</booktitle>
<pages>189--196</pages>
<publisher>MIT.</publisher>
<contexts>
<context position="7152" citStr="Yarowsky, 1995" startWordPosition="1164" endWordPosition="1165">ust one of the word&apos;s senses. The user then has the task of associating, by selecting from a pop-up menu, the required sense for unambiguous clues. The number of relations marked will depend on the time available, as well as the complexity of the sense division to be made. The act of assigning senses to patterns may very well lead the user to discover fresh, unconsidered senses usages of the word. The pattern-sense associations are then submitted to the next stage: automatic disambiguation. 2.4 The Disambiguation Algorithm The workbench currently uses Yarowsky&apos;s decision list approach to WSD (Yarowsky, 1995). This is a bootstrapping algorithm that, given 60r, strictly, of the quintuple {Wl, Rel — part — 1, W2, Rel — part 2, ANY}. -7WASP-Bench can also be used for Machine Translation lexicography, where arbitrary mnemonics would be replaced by target language translations. 152 subj-of num sal obj-of num sal modifier num sal n-mod num sal lend 95 21.2 burst 27 16.4 central 755 25.5 merchant 213 29.4 issue 60 11.8 rob 31 15.3 Swiss 87 18.7 clearing 127 27.0 charge 29 9.5 overflow 7 10.2 commercial 231 18.6 river 217 25.4 operate 45 8.9 line 13 8.4 grassy 42 18.5 creditor 52 22.8 modifies PP iv-PP an</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivalling supervised methods. In ACL 95, pages 189-196, MIT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>