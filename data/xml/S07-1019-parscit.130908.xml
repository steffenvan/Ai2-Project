<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.046252">
<title confidence="0.9835905">
AUG: A combined classification and clustering approach for web people
disambiguation
</title>
<author confidence="0.807572">
Els Lefever and V´eronique Hoste
</author>
<affiliation confidence="0.8643815">
LT3 Language and Translation Technology
Ghent University Association
</affiliation>
<address confidence="0.921945">
Groot-Brittanni¨elaan 45, 9000 Gent
</address>
<email confidence="0.983053">
els.lefever@hogent.be
veronique.hoste@hogent.be
</email>
<author confidence="0.559846">
Timur Fayruzov
</author>
<affiliation confidence="0.760841">
Computational Web Intelligence
Ghent University Association
</affiliation>
<address confidence="0.840333">
Krijgslaan 281, 9000 Gent
</address>
<email confidence="0.970579">
Timur.Fayruzov@UGent.be
</email>
<sectionHeader confidence="0.994945" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999876875">
This paper presents a combined super-
vised and unsupervised approach for multi-
document person name disambiguation.
Based on feature vectors reflecting pairwise
comparisons between web pages, a classifi-
cation algorithm provides linking informa-
tion about document pairs, which leads to
initial clusters. In addition, two different
clustering algorithms are fed with matrices
of weighted keywords. In a final step the
“seed” clusters are combined with the results
of the clustering algorithms. Results on the
validation data show that a combined classi-
fication and clustering approach doesn’t al-
ways compare favorably to those obtained
by the different algorithms separately.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973347826087">
Finding information about people on the World
Wide Web is one of the most popular activities of
Internet users. Given the high ambiguity of person
names and the increasing amount of information on
the web, it becomes very important to organize this
large amount of information into meaningful clus-
ters referring each to one single individual.
The problem of resolving name ambiguity on
the Internet has been approached from different an-
gles. Mann and Yarowsky (2003) have proposed a
Web based clustering technique relying on a fea-
ture space combining biographic facts and associ-
ated names, whereas Bagga and Baldwin (1998)
have looked for coreference chains within each doc-
ument, take the context of these chains for creating
summaries about each entity and convert these sum-
maries into a bag of words. Documents get clustered
using the standard vector space model. Other re-
searchers have taken this search for distinctive key-
words one step further and tried to come up with
“concepts” describing the documents. Fleischman
and Hovy (2004) introduce the “maximum entropy
model”: a binary classifier determines whether two
concept-instance pairs refer to the same individ-
ual. Pedersen (2006) presented an unsupervised ap-
proach using bigrams in the contexts to be clustered,
thus aiming at a concept level semantic space instead
of a word level feature space.
For the semeval contest, we approached the task
from a double supervised and unsupervised perspec-
tive. For the supervised classification, the task was
redefined in the form of feature vectors containing
disambiguating information on pairs of documents.
In addition to this, different clustering approaches
were applied on matrices of keywords. These results
were then merged by taking the classification output
as basic ”seed” clusters, which were then enhanced
by the results from the clustering experiments.
In the remainder of this paper, Section 2 intro-
duces the data sets and describes the construction of
the feature vectors and the keyword matrices. The
classification and clustering experiments, and the
final combination of the different outputs are dis-
cussed in Section 3. Section 4 gives an overview of
the results on the test data and Section 5 summarizes
the main findings of the paper.
</bodyText>
<page confidence="0.988987">
105
</page>
<bodyText confidence="0.7846215">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 105–108,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.352679" genericHeader="method">
2 Data sets and feature construction
</sectionHeader>
<bodyText confidence="0.999979655172414">
The data we have used for training our system were
made available in the framework of the SemEval
(task 13: Web People Search) competition (Artiles
et al., 2007). As preliminary training corpus (re-
ferred to as “trial data” in our article), we used the
WePS corpus (Web People Search corpus), available
at http://nlp.uned.es/weps. For the real training set,
this trial set was expanded in order to cover differ-
ent degrees of ambiguity (very common names, un-
common names and celebrity names which tend to
monopolize search results). The training corpus is
composed of 40 sets of 100 web pages, each set
corresponding to the first 100 results for a person
name query. The documents were manually clus-
tered. Documents that couldn’t be clustered prop-
erly have been put in a “discarded” section. Test
data have been constructed in a similar way (30 sets
of 100 web pages).
The content of the web pages has been prepro-
cessed by means of a memory-based shallow parser
(MBSP) (Daelemans and van den Bosch, 2005).
From the MBSP, we used the regular expression
based tokenizer, the part-of-speech tagger and text
chunker using the memory-based tagger MBT. On
the basis of the preprocessed data we construct a rich
feature space that combines biographic facts and dis-
tinctive characteristics for a given person, a list of
weighted keywords and meta data information about
the web page.
</bodyText>
<subsectionHeader confidence="0.996761">
2.1 Feature vector construction
</subsectionHeader>
<bodyText confidence="0.9999895">
The following biographic facts and related named
entities were extracted from the preprocessed data.
Information on date and place of birth, and on date
and place of death were extracted by means of a rule-
based component. Furthermore, three named en-
tity features were extracted on the basis of the shal-
low syntactic information provided by the memory-
based shallow parser and additional gazetteer infor-
mation. Furthermore, a “name” feature was aimed
at the extraction of further interesting name infor-
mation (E.g other surnames, family names) on the
person in focus, leading to the extraction of for ex-
ample “Ann Hill Carter Lee” and “Jo Ann Hill” for
the document collection on “Ann Hill”. The “loca-
tion” feature informs on the overlap between all lo-
cations named in the different documents. In a simi-
lar way, the “NE” feature returns the inter-document
overlap between all other named entities.
Starting with the assumption that overlapping
URL and email addresses usually point to the same
individual, we have also extracted URL, email and
domain addresses from the web pages. Therefore we
have combined pattern matching rules and markup
information (HTML &lt;href&gt; tag). The link of the
document itself has been added to the set of URL
links. Some filtering on the list has been performed
concerning length (to exclude garbage) and content
(to exclude non-distinctive URL addresses such as
index.html). Pair-wise comparison of documents
with respect to overlapping URL, email and domain
names resulted in 3 binary features.
Another binary feature we have extracted is the
location, based on our simple supposition that if
two documents are hosted in the same city, they
most probably refer to the same person (but not
vice versa). For converting IP-addresses to city lo-
cations, we have used MaxMind GeoIP(tm) open
source database2, which was sufficient for our needs.
</bodyText>
<subsectionHeader confidence="0.996574">
2.2 A bag of weighted keywords
</subsectionHeader>
<bodyText confidence="0.9996938">
The input source for extracting our distinctive key-
words is double: both the entire (preprocessed) con-
tent of the web pages as well as snippets and titles of
documents are used. Keywords extracted from snip-
pets and titles get a predefined -rather high- score,
as we consider them quite important. For determin-
ing the keyword relevance of the words extracted
from the content of the web pages, we have applied
Term Frequency Inverse Document Frequency (TF-
IDF) (Berger et al., 2000).
Once all scores are calculated, all weighted key-
words get stored in a matrix, which serve as input
for the clustering experiments. The calculated key-
word weight is also used, in case of overlapping key-
words, as a feature in our pairwise comparison vec-
tor. In case two keywords occurring in two different
documents are identical or recognized as synonyms
(information we obtain by using WordNet3), we sum
up the different weights of these keywords and store
this value in the feature vector.
</bodyText>
<footnote confidence="0.999918">
2http://www.maxmind.com/app/geolitecity
3http://wordnet.princeton.edu/
</footnote>
<page confidence="0.995052">
106
</page>
<sectionHeader confidence="0.979489" genericHeader="method">
3 Classification and Clustering algorithms
</sectionHeader>
<subsectionHeader confidence="0.97833">
3.1 Classification
</subsectionHeader>
<bodyText confidence="0.9999535">
For the classification experiments, we used the ea-
ger RIPPER rule learner (Cohen, 1995) which in-
duces a set of easily understandable if-then classi-
fication rules for the minority class and a default
rule for the remaining class. The ruler learner was
trained and validated on the trial and training data.
Given the completely different class distribution of
the trial and training data, viz. 10.6% positive in-
stances in the trial data versus 66.7% in the train-
ing data, we decided to omit the trial data and opti-
mize the learner on the basis of the more balanced
training data set. There was an optimization of the
class ordering parameter, the two-valued negative
tests parameter, the hypothesis simplification param-
eter, the example coverage parameter, the parameter
expressing the number of optimization passes and
the loss ratio parameter. The predicted positive pair-
wise classifications were then combined using a for
coreference resolution developed counting mecha-
nism (Hoste, 2005).
</bodyText>
<subsectionHeader confidence="0.999665">
3.2 Clustering Algorithms
</subsectionHeader>
<bodyText confidence="0.99997119047619">
We experimented with several clustering algorithms
and settings on the trial and training data to de-
cide on our list of parameter settings. We validated
the following three clustering algorithms. First,
we compared output from k-means and hierarchical
clustering algorithms. Next to that, we have run ex-
periments for agglomerative clustering4. with differ-
ent parameter combinations (2 similarity measures
and 5 clustering functions). All clustering experi-
ments take the weighted keywords matrix as input.
Based on the validation experiments, hierarchical
and agglomerative clustering were further evaluated
to find out the optimal parameter settings. For hier-
archical clustering, this led to the choice of the co-
sine distance metric, single-link hierarchical cluster-
ing and a 50% cluster size. For agglomerative clus-
tering, clustering accuracy was very dependent on
the structure of the document set. This has made us
use different strategies for clustering sets containing
“famous” and “non famous” people. As a distinction
criterion we have chosen the presence/non-presence
</bodyText>
<footnote confidence="0.745482">
4http://glaros.dtc.umn.edu/gkhome/views/cluto
</footnote>
<bodyText confidence="0.99996844">
of the person in Wikipedia. We started with the as-
sumption that sets containing famous people (found
in Wikipedia) most probably contain a small amount
of bigger clusters than sets describing “ordinary”
persons. According to this assumption, two differ-
ent parameter sets were used for clustering. For
Wikipedia people we have used the correlation co-
efficient and g1 clustering type, for ordinary people
we have used the cosine similarity measure and sin-
gle link clustering. For both categories the number
of target output clusters equals (number of RIPPER
output clusters + the number of documents*0.2).
Although the clustering results with the best set-
tings for hierarchical and agglomerative clustering
were very close with regard to F-score (combining
purity and inverse purity, see (Artiles et al., 2007)
for a more detailed description), manual inspection
of the content of the clusters has revealed big dif-
ferences between the two approaches. Clusters that
are output by our hierarchical algorithm look more
homogeneous (higher purity), whereas inverse pu-
rity seems better for the agglomerative clustering.
Therefor we have decided to take the best of two
worlds and combined resulting clusters of both al-
gorithms.
</bodyText>
<subsectionHeader confidence="0.999979">
3.3 Merging of clustering results
</subsectionHeader>
<bodyText confidence="0.99994619047619">
Classification and clustering with optimal settings
resulted in three sets of clusters, one based on pair-
wise similarity vectors and two based on keyword
matrices. Since the former set tends to have better
precision, which seems logical because more evi-
dent features are used for classification, we used this
set as “seed” clusters. The two remaining sets were
used to improve recall.
Merging was done in the following way: first we
compare the initial set with the result of the agglom-
erative clustering by trying to find the biggest inter-
section. We remove the intersection from the small-
est cluster and add both clusters to the final set. The
resulting set of clusters is further improved by us-
ing the result of the hierarchical clustering. Here we
apply another combining strategy: if two documents
form one cluster in the initial set, but are in separate
clusters in the other set, we merge these two clusters.
Table 1 lists all results of the separate clustering al-
gorithms as well as the final clustering results for
the Wikipedia person names. Second half of the ta-
</bodyText>
<page confidence="0.993334">
107
</page>
<table confidence="0.999786454545454">
Person Name Ripper agglom. hierarch. merged
Wikipedia
Alexander Macomb .69/.63 .64/.56 .57/.47 .79/.80
David Lodge .69/.65 .69/.64 .43/.33 .79/.85
George Clinton .65/.62 .64/.59 .54/.45 .75/.80
John Kennedy .67/.62 .70/.66 .49/.39 .76/.80
Michael Howard .56/.54 .63/.62 .65/.58 .62/.75
Paul Collins .54/.57 .64/.62 .63/.56 .55/.62
Tony Abbott .63/.59 .67/.63 .62/.54 .77/.83
Average Scores .73/.76 .67/.72 .62/.60 .66/.75
all Training Data
</table>
<tableCaption confidence="0.999973">
Table 1: Results on Training Data
</tableCaption>
<bodyText confidence="0.9995385">
ble shows the average results for the separate and
combined algorithms. The first score always refers
to Fα = 0.5, the second score refers to Fα = 0.2.
The average scores, that were calculated on the
complete training set, show that RIPPER outperforms
the combined clusters.
</bodyText>
<sectionHeader confidence="0.998073" genericHeader="evaluation">
4 Results on the test data
</sectionHeader>
<subsectionHeader confidence="0.938901">
4.1 Final settings
</subsectionHeader>
<bodyText confidence="0.99970425">
For our classification algorithm, we have finally not
kept the best settings for the training data, as this
led to an alarming over-assignment of the positive
class, thus linking nearly every document to each
other. Therefore, we were forced to define a more
strict rule set. For the clustering algorithms, we have
used the optimal parameter settings as described in
Section 3.
</bodyText>
<subsectionHeader confidence="0.998714">
4.2 Test results
</subsectionHeader>
<bodyText confidence="0.9999518">
Table 2 lists the results for the separate and merged
clustering for SET 1 in the test data (participants
in the ACL conference) and the average for all al-
gorithms. The average score, that has been calcu-
lated on the complete test set, shows that the com-
bined clusters outperform the separate algorithms
for Fα = 0.2, but the hierarchical algorithm out-
performs the others for Fα = 0.5. Table 3 lists the
average results for purity, inverse purity and the F-
measures.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999775">
We proposed and validated a combined classifica-
tion and clustering approach for resolving web peo-
ple ambiguity. In future work we plan to experiment
with clustering algorithms that don’t require a prede-
fined number of clusters, as our tests revealed a big
impact of the cluster size on our results. We will also
</bodyText>
<table confidence="0.999541714285714">
Person Name Ripper agglom. hierarch. merged
ACL
Chris Brockett .49/.39 .74/.69 .70/.61 .79/.80
Dekang Lin .69/.58 .76/.67 .59/.47 .93/.89
Frank Keller .48/.41 .68/.75 .64/.62 .56/.71
James Curran .53/.50 .64/.77 .75/.78 .54/.72
Jerry Hobbs .50/.39 .02/.01 .58/.47 .74/.70
Leon Barrett .47/.40 .67/.74 .65/.66 .57/.73
Mark Johnson .45/.42 .55/.70 .65/.77 .44/.65
Robert Moore .39/.37 .60/.71 .66/.68 .46/.65
Sharon Goldwater .60/.49 .72/.61 .40/.29 .91/.86
Stephen Clark .41/.42 .53/.67 .68/.75 .46/.67
Average Scores .49/.45 .58/.63 .69/.69 .61/.74
all Test Data
</table>
<tableCaption confidence="0.996865">
Table 2: Results on Test Data
</tableCaption>
<table confidence="0.999511333333333">
Test set Purity Inverse F = F =
Purity α = 0.5 α = 0.2
Set1 .57 .85 .64 .73
Set2 .45 .91 .58 .73
Set3 .48 .89 .60 .73
Global .50 .88 .60 .73
</table>
<tableCaption confidence="0.999789">
Table 3: Purity/Inverse Purity Results on Test Data
</tableCaption>
<bodyText confidence="0.9982772">
experiment with meta-learning, other merging tech-
niques and evaluation metrics. Furthermore, we will
investigate the impact of intra-document and inter-
document coreference resolution on web people dis-
ambiguation.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="references">
6 References
</sectionHeader>
<reference confidence="0.999209766666667">
J. Artiles and J. Gonzalo and S. Sekine. 2007. The SemEval-
2007 WePS Evaluation: Establishing a benchmark for the Web
People Search Task, Proceedings of Semeval 2007, Association
for Computational Linguistics.
A. Bagga and B. Baldwin. 1998. Entity-based cross-document
co-referencing using the vector space model, Proceedings of
the 17th international conference on Computational linguistics,
75–85.
A. Berger and R. Caruana and D. Cohn and D. Freitag and V.
Mittal. 2000. Bridging the Lexical Chasm: Statistical Ap-
proaches to Answer Finding, Proc. Int. Conf. Reasearch and
Development in Information Retrieval, 192–199.
William W. Cohen. 1995. Fast Effective Rule Induction,
Proceedings of the 12th International Conference on Machine
Learning, 115–123. Tahoe City, CA.
Walter Daelemans and Antal van den Bosch. 2005. Memory-
Based Language Processing. Cambridge University Press.
Veronique Hoste. 2005. Optimization Issues in Machine Learn-
ing of Coreference Resolution. Phd dissertation, Antwerp Uni-
versity.
M.B. Fleischman and E. Hovy. 2004. Multi-document per-
son name resolution, Proceedings of 42nd Annual Meeting of
the Association for Computational Linguistics (ACL), Reference
Resolution Workshop.
G. Mann and D. Yarowsky. 2003. Unsupervised personal name
disambiguation, Proceedings of CoNLL-2003, 33–40. Edmon-
ton, Canada.
T. Pedersen and A. Purandare and A. Kulkarni. 2006. Name
Discrimination by Clustering Similar Contexts, Proceedings of
the World Wide Web Conference (WWW).
</reference>
<page confidence="0.998328">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.177893">
<title confidence="0.981868">AUG: A combined classification and clustering approach for web people disambiguation</title>
<author confidence="0.998902">Lefever Hoste</author>
<affiliation confidence="0.9827885">LT3 Language and Translation Technology Ghent University Association</affiliation>
<address confidence="0.958488">Groot-Brittanni¨elaan 45, 9000 Gent</address>
<email confidence="0.777877">els.lefever@hogent.beveronique.hoste@hogent.be</email>
<author confidence="0.643793">Timur Fayruzov</author>
<affiliation confidence="0.693142">Computational Web Intelligence Ghent University Association</affiliation>
<address confidence="0.953815">Krijgslaan 281, 9000 Gent</address>
<email confidence="0.880463">Timur.Fayruzov@UGent.be</email>
<abstract confidence="0.999413294117647">This paper presents a combined supervised and unsupervised approach for multidocument person name disambiguation. Based on feature vectors reflecting pairwise comparisons between web pages, a classification algorithm provides linking information about document pairs, which leads to initial clusters. In addition, two different clustering algorithms are fed with matrices of weighted keywords. In a final step the “seed” clusters are combined with the results of the clustering algorithms. Results on the validation data show that a combined classification and clustering approach doesn’t always compare favorably to those obtained by the different algorithms separately.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Artiles</author>
<author>J Gonzalo</author>
<author>S Sekine</author>
</authors>
<title>The SemEval2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task,</title>
<date>2007</date>
<booktitle>Proceedings of Semeval 2007, Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3720" citStr="Artiles et al., 2007" startWordPosition="554" endWordPosition="557">rices. The classification and clustering experiments, and the final combination of the different outputs are discussed in Section 3. Section 4 gives an overview of the results on the test data and Section 5 summarizes the main findings of the paper. 105 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 105–108, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Data sets and feature construction The data we have used for training our system were made available in the framework of the SemEval (task 13: Web People Search) competition (Artiles et al., 2007). As preliminary training corpus (referred to as “trial data” in our article), we used the WePS corpus (Web People Search corpus), available at http://nlp.uned.es/weps. For the real training set, this trial set was expanded in order to cover different degrees of ambiguity (very common names, uncommon names and celebrity names which tend to monopolize search results). The training corpus is composed of 40 sets of 100 web pages, each set corresponding to the first 100 results for a person name query. The documents were manually clustered. Documents that couldn’t be clustered properly have been p</context>
<context position="10920" citStr="Artiles et al., 2007" startWordPosition="1688" endWordPosition="1691">bing “ordinary” persons. According to this assumption, two different parameter sets were used for clustering. For Wikipedia people we have used the correlation coefficient and g1 clustering type, for ordinary people we have used the cosine similarity measure and single link clustering. For both categories the number of target output clusters equals (number of RIPPER output clusters + the number of documents*0.2). Although the clustering results with the best settings for hierarchical and agglomerative clustering were very close with regard to F-score (combining purity and inverse purity, see (Artiles et al., 2007) for a more detailed description), manual inspection of the content of the clusters has revealed big differences between the two approaches. Clusters that are output by our hierarchical algorithm look more homogeneous (higher purity), whereas inverse purity seems better for the agglomerative clustering. Therefor we have decided to take the best of two worlds and combined resulting clusters of both algorithms. 3.3 Merging of clustering results Classification and clustering with optimal settings resulted in three sets of clusters, one based on pairwise similarity vectors and two based on keyword</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>J. Artiles and J. Gonzalo and S. Sekine. 2007. The SemEval2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task, Proceedings of Semeval 2007, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based cross-document co-referencing using the vector space model,</title>
<date>1998</date>
<booktitle>Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>75--85</pages>
<contexts>
<context position="1714" citStr="Bagga and Baldwin (1998)" startWordPosition="241" endWordPosition="244">n Finding information about people on the World Wide Web is one of the most popular activities of Internet users. Given the high ambiguity of person names and the increasing amount of information on the web, it becomes very important to organize this large amount of information into meaningful clusters referring each to one single individual. The problem of resolving name ambiguity on the Internet has been approached from different angles. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. Documents get clustered using the standard vector space model. Other researchers have taken this search for distinctive keywords one step further and tried to come up with “concepts” describing the documents. Fleischman and Hovy (2004) introduce the “maximum entropy model”: a binary classifier determines whether two concept-instance pairs refer to the same individual. Pedersen (2006) presented an unsupervised approa</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-based cross-document co-referencing using the vector space model, Proceedings of the 17th international conference on Computational linguistics, 75–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>R Caruana</author>
<author>D Cohn</author>
<author>D Freitag</author>
<author>V Mittal</author>
</authors>
<title>Bridging the Lexical Chasm: Statistical Approaches to Answer Finding,</title>
<date>2000</date>
<booktitle>Proc. Int. Conf. Reasearch and Development in Information Retrieval,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="7344" citStr="Berger et al., 2000" startWordPosition="1150" endWordPosition="1153">to city locations, we have used MaxMind GeoIP(tm) open source database2, which was sufficient for our needs. 2.2 A bag of weighted keywords The input source for extracting our distinctive keywords is double: both the entire (preprocessed) content of the web pages as well as snippets and titles of documents are used. Keywords extracted from snippets and titles get a predefined -rather high- score, as we consider them quite important. For determining the keyword relevance of the words extracted from the content of the web pages, we have applied Term Frequency Inverse Document Frequency (TFIDF) (Berger et al., 2000). Once all scores are calculated, all weighted keywords get stored in a matrix, which serve as input for the clustering experiments. The calculated keyword weight is also used, in case of overlapping keywords, as a feature in our pairwise comparison vector. In case two keywords occurring in two different documents are identical or recognized as synonyms (information we obtain by using WordNet3), we sum up the different weights of these keywords and store this value in the feature vector. 2http://www.maxmind.com/app/geolitecity 3http://wordnet.princeton.edu/ 106 3 Classification and Clustering </context>
</contexts>
<marker>Berger, Caruana, Cohn, Freitag, Mittal, 2000</marker>
<rawString>A. Berger and R. Caruana and D. Cohn and D. Freitag and V. Mittal. 2000. Bridging the Lexical Chasm: Statistical Approaches to Answer Finding, Proc. Int. Conf. Reasearch and Development in Information Retrieval, 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Fast Effective Rule Induction,</title>
<date>1995</date>
<booktitle>Proceedings of the 12th International Conference on Machine Learning, 115–123. Tahoe</booktitle>
<location>City, CA.</location>
<contexts>
<context position="8061" citStr="Cohen, 1995" startWordPosition="1259" endWordPosition="1260">the clustering experiments. The calculated keyword weight is also used, in case of overlapping keywords, as a feature in our pairwise comparison vector. In case two keywords occurring in two different documents are identical or recognized as synonyms (information we obtain by using WordNet3), we sum up the different weights of these keywords and store this value in the feature vector. 2http://www.maxmind.com/app/geolitecity 3http://wordnet.princeton.edu/ 106 3 Classification and Clustering algorithms 3.1 Classification For the classification experiments, we used the eager RIPPER rule learner (Cohen, 1995) which induces a set of easily understandable if-then classification rules for the minority class and a default rule for the remaining class. The ruler learner was trained and validated on the trial and training data. Given the completely different class distribution of the trial and training data, viz. 10.6% positive instances in the trial data versus 66.7% in the training data, we decided to omit the trial data and optimize the learner on the basis of the more balanced training data set. There was an optimization of the class ordering parameter, the two-valued negative tests parameter, the h</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William W. Cohen. 1995. Fast Effective Rule Induction, Proceedings of the 12th International Conference on Machine Learning, 115–123. Tahoe City, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
</authors>
<title>MemoryBased Language Processing.</title>
<date>2005</date>
<publisher>Cambridge University Press. Veronique</publisher>
<institution>Antwerp University.</institution>
<marker>Daelemans, van den Bosch, 2005</marker>
<rawString>Walter Daelemans and Antal van den Bosch. 2005. MemoryBased Language Processing. Cambridge University Press. Veronique Hoste. 2005. Optimization Issues in Machine Learning of Coreference Resolution. Phd dissertation, Antwerp University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Fleischman</author>
<author>E Hovy</author>
</authors>
<title>Multi-document person name resolution,</title>
<date>2004</date>
<booktitle>Proceedings of 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Reference Resolution Workshop.</booktitle>
<contexts>
<context position="2130" citStr="Fleischman and Hovy (2004)" startWordPosition="309" endWordPosition="312">oached from different angles. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. Documents get clustered using the standard vector space model. Other researchers have taken this search for distinctive keywords one step further and tried to come up with “concepts” describing the documents. Fleischman and Hovy (2004) introduce the “maximum entropy model”: a binary classifier determines whether two concept-instance pairs refer to the same individual. Pedersen (2006) presented an unsupervised approach using bigrams in the contexts to be clustered, thus aiming at a concept level semantic space instead of a word level feature space. For the semeval contest, we approached the task from a double supervised and unsupervised perspective. For the supervised classification, the task was redefined in the form of feature vectors containing disambiguating information on pairs of documents. In addition to this, differe</context>
</contexts>
<marker>Fleischman, Hovy, 2004</marker>
<rawString>M.B. Fleischman and E. Hovy. 2004. Multi-document person name resolution, Proceedings of 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Reference Resolution Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation,</title>
<date>2003</date>
<booktitle>Proceedings of CoNLL-2003,</booktitle>
<pages>33--40</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1558" citStr="Mann and Yarowsky (2003)" startWordPosition="216" endWordPosition="219">t a combined classification and clustering approach doesn’t always compare favorably to those obtained by the different algorithms separately. 1 Introduction Finding information about people on the World Wide Web is one of the most popular activities of Internet users. Given the high ambiguity of person names and the increasing amount of information on the web, it becomes very important to organize this large amount of information into meaningful clusters referring each to one single individual. The problem of resolving name ambiguity on the Internet has been approached from different angles. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. Documents get clustered using the standard vector space model. Other researchers have taken this search for distinctive keywords one step further and tried to come up with “concepts” describing the documents. Fleischman and Hovy (2004) introduce the “maximum entr</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>G. Mann and D. Yarowsky. 2003. Unsupervised personal name disambiguation, Proceedings of CoNLL-2003, 33–40. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>A Purandare</author>
<author>A Kulkarni</author>
</authors>
<title>Name Discrimination by Clustering Similar Contexts,</title>
<date>2006</date>
<booktitle>Proceedings of the World Wide Web Conference (WWW).</booktitle>
<marker>Pedersen, Purandare, Kulkarni, 2006</marker>
<rawString>T. Pedersen and A. Purandare and A. Kulkarni. 2006. Name Discrimination by Clustering Similar Contexts, Proceedings of the World Wide Web Conference (WWW).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>