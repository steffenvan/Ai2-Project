<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036215">
<title confidence="0.986955">
LSTC System for Chinese Word Sense Induction
</title>
<author confidence="0.998284">
Peng Jin, Yihao Zhang, Rui Sun
</author>
<affiliation confidence="0.8915135">
Laboratory of Intelligent Information Processing and Application
Leshan Teachers’ College
</affiliation>
<email confidence="0.995518">
jandp@pku.edu.cn,yhaozhang@163.com,dram_218@163.com
</email>
<sectionHeader confidence="0.99733" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807117647059">
This paper presents the Chinese word
sense Induction system of Leshan
Teachers’ College. The system
participates in the Chinese word sense
Induction of task 4 in Back offs
organized by the Chinese Information
Processing Society of China (CIPS) and
SIGHAN. The system extracts neighbor
words and their POSs centered in the
target words and selected the best one of
four cluster algorithms: Simple KMeans,
EM, Farthest First and Hierarchical
Cluster based on training data. We
obtained the F-Score of 60.5% on the
training data otherwise the F-Score is
57.89% on the test data provided by
organizers.
</bodyText>
<sectionHeader confidence="0.998938" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99994465625">
Automatically obtain the intended sense of
polysemous words according to its context has
been shown to improve performance in
information retrieval,information extraction
and machine translation. There are two ways to
resolve this problem in view of machine
learning, one is supervised classification and
the other is unsupervised classification i.e.
clustering. The former is word sense
disambiguation (WSD) which relies on large
scale, high quality manually annotated sense
corpus, but building a sense-annotated corpus
is a time-consuming and expensive project.
Even the corpus were constructed, the system
trained from this corpus show the low
performance on different domain test corpus.
The later is word sense induction (WSI) which
needs not any training data, and it has become
one of the most important topics in current
computational linguistics.
Chinese Information Processing Society of
China (CIPS) and SIGHAN organized a task is
intended to promote the research on Chinese
WSI. We built a WSI system named
LSTC-WSI system for this task. This system
tried four cluster algorithms, i.e. Simple
KMeans,EM,Farthest First and Hierarchical
Cluster implemented by weak 3.7.1 [6], and
found Simple KMeans compete the other three
ones according to their performances on
training data. Finally, the results returned by
Simple KMeans were submitted.
</bodyText>
<sectionHeader confidence="0.996098" genericHeader="method">
2. Features Selection
</sectionHeader>
<bodyText confidence="0.998789416666667">
Following the feature selection in word sense
disambiguation, we extract neighbor words and
their POSs centered in the target words. Word
segmented and POS-tag tool adapted Chinese
Lexical Analysis System developed by Institute
of Computing Technology. No other resource is
used in the system. The window size of the
context is set to 5 around the ambiguous word.
The neighbor words which occur only once
were removed. Each sample is represented as a
vector, and feature form is binary: if it occurs
in is 1 otherwise is 0.
</bodyText>
<sectionHeader confidence="0.935494" genericHeader="method">
3. Clusters Algorithms
</sectionHeader>
<bodyText confidence="0.999842777777778">
Four cluster algorithms were tried in our
system. I will introduce them simply in the
next respectively.
K-means clustering [1] is one of the simplest
unsupervised learning algorithms that solve the
well known clustering problem. The main idea
is to define k centroids, one for each cluster.
These centroids should be placed in a cunning
way because of different location causes
different result. So, the better choice is to place
them as much as possible far away from each
other.
EM algorithm[2] is a method for finding
maximum likelihood estimates of parameters in
statistical models, where the model depends on
unobserved latent variables. EM is an iterative
method which alternates between performing
an expectation (E) step, which computes the
expectation of the log-likelihood evaluated
using the current estimate for the latent
variables, and maximization (M) step, which
computes parameters maximizing the expected
log-likelihood found on the E step.
The Farthest First algorithm [3] is an
implementation of the “Farthest First Traversal
Algorithm” by Hochbaum and Shmoys (1985).
It finds fast, approximate clusters and may be
useful as an initialiser for k-means.
A hierarchical clustering [4] is the guarantee
that for every k, the induced k clustering has
cost at most eight times that of the optimal
k-clustering. A hierarchical clustering of n data
points is a recursive partitioning of the data
into 2, 3, 4, . . . and finally n, clusters. Each
intermediate clustering is made more
fine-grained by dividing one of its clusters.
</bodyText>
<sectionHeader confidence="0.995402" genericHeader="method">
4. Development
</sectionHeader>
<subsectionHeader confidence="0.998757">
4.1 Evaluation method
</subsectionHeader>
<bodyText confidence="0.9992884">
We consider the gold standard as a solution to
the clustering problem. All examples tagged
with a given sense in the gold standard form a
class. For the system output, the clusters are
formed by instances assigned to the same sense
tag. We will compare clusters output by the
system with the classes in the gold standard
and compute F-score as usual [5]. F-score is
computed with the formula below.
Suppose is a class of the gold standard,
</bodyText>
<equation confidence="0.87710125">
Cr
and Si is a cluster of the system generated, then
F−Score C r S i = P R P+R
( , ) 2 * * /( ) (1)
Then for a given class Cr,
F score
− (Cr ) max(F score(Cr, Si ))
= −
Si
F − Score =∑nrFScore(Cr)
c
=1 n (2)
</equation>
<bodyText confidence="0.757684">
where c is total number of classes, is the size
</bodyText>
<equation confidence="0.6158285">
nr
n
</equation>
<bodyText confidence="0.998029">
of class Cr, and is the total size. Participants
will be required to induce the senses of the
target word using only the dataset provided by
the organizers.
</bodyText>
<subsectionHeader confidence="0.988708">
4.2 Data Set
</subsectionHeader>
<bodyText confidence="0.999967">
The organizers provide 50 Chinese training
data of SIGHAN2010-WSI-SampleData. The
training data contain 50 Chinese words; each
word has 50 example sentences, and gives each
word the total number of sense. The total
number of sense is ranging from 2 to 21, but
more cases are 2. In order to facilitate the team
participating in the contest to do experiment,
the organizers also provide answer to each
</bodyText>
<figure confidence="0.980925222222222">
the number of correctly labeled examples for
a cluster
R=
total cluster size
the number of correctly labeled examples for
total cluster size
a cluster
=
p
</figure>
<bodyText confidence="0.967471285714286">
word.
In order to evaluating the system’s
performance of all participating team, the
organizers provide 100 test word and each
word have 50 example sentences, the system of
each participating team need to run out the
results which the organizers need.
</bodyText>
<subsectionHeader confidence="0.998758">
4.3 System Setup
</subsectionHeader>
<bodyText confidence="0.99976475">
We developed the LSTC-WSI system based on
Weka. Firstly, we implemented the evaluation
algorithm described in section 4.1. Then, the
instances were represented as vectors according
to the feature selection. Thirdly, four cluster
algorithms from Weka were tried and set
different thresholds for feature frequency.
Because of paper length constraints, we could
not list all the experience data we get. Table 1
listed system performance when frequency
threshold set two and without POS
information.
</bodyText>
<tableCaption confidence="0.99956">
Table 1: The Performance on test data
</tableCaption>
<table confidence="0.999821692307693">
Target Simple EM Farthest Hierar
word Kmeans First chical
ru�A 0.618 0.680 0.538 0.649
4EP 0.404 0.365 0.400 0.327
j9 0.711 0.557 0.672 0.636
j9% 0.626 0.700 0.536 0.570
�� 0.571 0.555 0.572 0.573
r 0.789 0.596 0.680 0.548
r� 0.704 0.617 0.704 0.682
JR�h 0.568 0.495 0.461 0.583
tTJ� 0.5679 0.679 0.625 0.688
9 0.601 0.590 0.648 0.603
AFM 0.578 0.554 0.662 0.616
Rg-ft 0.621 0.537 0.615 0.627
OR 0.560 0.429 0.466 0.527
�� 0.627 0.537 0.643 0.603
��JA 0.610 0.538 0.643 0.638
�7 0.643 0.607 0.648 0.632
VJT- 0.615 0.545 0.662 0.603
M 0.621 0.616 0.615 0.658
11M 0.538 0.583 0.569 0.609
�Pbt 0.603 0.540 0.632 0.569
MI 0.653 0.557 0.657 0.603
AFt� 0.627 0.622 0.652 0.690
#Q 0.421 0.438 0.454 0.453
�� 0.609 0.528 0.583 0.627
�� 0.634 0.667 0.486 0.652
V 0.574 0.546 0.577 0.584
4T 0.462 0.429 0.518 0.501
4TWT 0.661 0.584 0.584 0.602
4T3f 0.430 0.501 0.549 0.418
4TT&amp; 0.596 0.644 0.647 0.654
4TI 0.614 0.580 0.672 0.708
)Q2r 0.666 0.600 0.615 0.595
)Q� 0.638 0.590 0.540 0.678
)Q 0.841 0.734 0.662 0.618
)Q� 0.613 0.562 0.670 0.568
�� 0.635 0.617 0.646 0.649
49 0.603 0.594 0.615 0.577
3T,AL 0.644 0.635 0.661 0.560
3T,�j 0.599 0.595 0.624 0.638
3T,A 0.588 0.575 0.587 0.508
M)1 0.699 0.723 0.673 0.643
flRq 0.585 0.596 0.666 0.603
WT 0.643 0.639 0.666 0.656
WTI 0.624 0.537 0.663 0.608
NA, 0.632 0.525 0.629 0.617
ACJ 0.451 0.472 0.490 0.477
AR 0.613 0.625 0.6723 0.625
Af 0.601 0.640 0.646 0.661
R#f 0.591 0.585 0.663 0.639
AM 0.536 0.505 0.477 0.532
</table>
<bodyText confidence="0.9999085">
We tried two ways for feature selection: the
frequency of features and neighbor words’ POS
were taken into account or not. Table 2 shows
the average performance on the test data via
varying the parameter setting. Observing the
results returned by Hierarchical cluster is very
imbalance, we set the options “-L WARD” in
order to balance the number.
</bodyText>
<tableCaption confidence="0.995442">
Table 2: The Average Performance of 50 Training Data
</tableCaption>
<table confidence="0.996560761904762">
Features Simple EM Farthest Hierar
Kmeans First chical
Word, 0.555 0.566 0.607 0.558
Windows 5
Word, 0.583 0.567 0.599 0.582
Windows 5,
Frequency 1
Word, 0.605 0.575 0.605 0.598
Windows 5,
Frequency 2
Word, 0.598 0.590 0.600 0.599
Windows 5,
Frequency 3
Word+POSs, 0.562 0.582 0.618 0.569
Windows 5
Word+POSs, 0.589 0.580 0.610 0.594
Windows 5,
Frequency 1
Word+POSs, 0.589 0.580 0.610 0.594
Windows 5,
Frequency 2
</table>
<bodyText confidence="0.9998858">
Compared with the average performance of the
50 test data, we find the performance is best1
when considering word only and setting the
frequency is two at the same time simple
KMeans was adapted. So, we use the same
parameters setting and clustered the test data
by simple KMeans. As table 2 shows, the
F-Score is 60.5% on training data. But on test
data, our system’s F-Score is 57.89% officially
evaluated by task organizers.
</bodyText>
<sectionHeader confidence="0.993747" genericHeader="conclusions">
5. Conclusion and Future Works
</sectionHeader>
<reference confidence="0.83201875">
Four cluster algorithms are tried for Chinese
word sense induction: Simple KMeans, EM,
1 Although “Farthest First” got the highest score, the
results of “Farthest First” are too imbalance.
</reference>
<bodyText confidence="0.970076416666667">
Farthest First and Hierarchical Cluster. We
construct different feature spaces and select out
the best combination of cluster and feature
space. Finally, we apply the best system to the
test data.
In the future, we will look for better cluster
algorithms for word sense induction.
Furthermore, we observe that it is different
from word sense disambiguation, different part
of speech will cause the polysemy. We will
make use of this character to improve our
system.
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.981768428571429">
This work is supported by the Open
Projects Program of Key Laboratory of
Computational Linguistics(Peking
University),Ministry of Education, Grant
No. KLCL-1002. This work is also
supported by Leshan Teachers’ College,
Grant No. Z1046.
</reference>
<sectionHeader confidence="0.959772" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998737947368421">
[1] Dekang Lin, Xiaoyun Wu. Phrase Clustering for
Discriminative Learning. Proceedings of
ACL ,2009.
[2]Neal R, &amp; Hinton G. A view of the EM algorithm
that justifies incremental, sparse, and other
variants. Learning in Graphical Models, 89,
355–368.
[3] Jon Gibson, Firat Tekiner, Peter Halfpenny.
NCeSS Project: Data Mining for Social Scientists.
Research Computing Services, University of
Manchester, U.K.
[4] Sanjoy Dasgupta, Philip M. Long. Performance
guarantees for hierarchical clustering. Journal of
Computer and System Sciences, 555–569, 2005.
[5] Eneko Agirre, Aitor Soroa. Semeval-2007 Task
02:Evaluating Word Sense Induction and
Discrimination Systems. Proceedings of
SemEval-2007, pages 7–12, 2007.
[6] http://www.cs.waikato.ac.nz/ml/weka/
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.806468">
<title confidence="0.998742">LSTC System for Chinese Word Sense Induction</title>
<author confidence="0.999884">Peng Jin</author>
<author confidence="0.999884">Yihao Zhang</author>
<author confidence="0.999884">Rui Sun</author>
<affiliation confidence="0.9989485">Laboratory of Intelligent Information Processing and Leshan Teachers’ College</affiliation>
<email confidence="0.999093">jandp@pku.edu.cn,yhaozhang@163.com,dram_218@163.com</email>
<abstract confidence="0.986690666666667">This paper presents the Chinese word sense Induction system of Leshan Teachers’ College. The system participates in the Chinese word sense Induction of task 4 in Back offs organized by the Chinese Information Processing Society of China (CIPS) and SIGHAN. The system extracts neighbor words and their POSs centered in the target words and selected the best one of four cluster algorithms: Simple KMeans, EM, Farthest First and Hierarchical Cluster based on training data. We obtained the F-Score of 60.5% on the training data otherwise the F-Score is 57.89% on the test data provided by organizers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Four cluster algorithms are tried for Chinese word sense induction: Simple KMeans, EM, 1 Although “Farthest First” got the highest score, the results of “Farthest First” are too imbalance. This work is supported by the</title>
<date>1046</date>
<booktitle>Open Projects Program of Key Laboratory of Computational Linguistics(Peking University),Ministry of Education, Grant No. KLCL-1002. This work</booktitle>
<location>Grant No.</location>
<marker>1046</marker>
<rawString> Four cluster algorithms are tried for Chinese word sense induction: Simple KMeans, EM, 1 Although “Farthest First” got the highest score, the results of “Farthest First” are too imbalance. This work is supported by the Open Projects Program of Key Laboratory of Computational Linguistics(Peking University),Ministry of Education, Grant No. KLCL-1002. This work is also supported by Leshan Teachers’ College, Grant No. Z1046.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Xiaoyun Wu. Phrase Clustering for Discriminative Learning.</title>
<date>2009</date>
<booktitle>Proceedings of ACL</booktitle>
<contexts>
<context position="2891" citStr="[1]" startWordPosition="437" endWordPosition="437">t neighbor words and their POSs centered in the target words. Word segmented and POS-tag tool adapted Chinese Lexical Analysis System developed by Institute of Computing Technology. No other resource is used in the system. The window size of the context is set to 5 around the ambiguous word. The neighbor words which occur only once were removed. Each sample is represented as a vector, and feature form is binary: if it occurs in is 1 otherwise is 0. 3. Clusters Algorithms Four cluster algorithms were tried in our system. I will introduce them simply in the next respectively. K-means clustering [1] is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The main idea is to define k centroids, one for each cluster. These centroids should be placed in a cunning way because of different location causes different result. So, the better choice is to place them as much as possible far away from each other. EM algorithm[2] is a method for finding maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. EM is an iterative method which alternates between performing an expectation (E) st</context>
</contexts>
<marker>[1]</marker>
<rawString>Dekang Lin, Xiaoyun Wu. Phrase Clustering for Discriminative Learning. Proceedings of ACL ,2009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Neal</author>
<author>G Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<journal>Learning in Graphical Models,</journal>
<volume>89</volume>
<pages>355--368</pages>
<contexts>
<context position="3261" citStr="[2]" startWordPosition="499" endWordPosition="499">nted as a vector, and feature form is binary: if it occurs in is 1 otherwise is 0. 3. Clusters Algorithms Four cluster algorithms were tried in our system. I will introduce them simply in the next respectively. K-means clustering [1] is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The main idea is to define k centroids, one for each cluster. These centroids should be placed in a cunning way because of different location causes different result. So, the better choice is to place them as much as possible far away from each other. EM algorithm[2] is a method for finding maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. EM is an iterative method which alternates between performing an expectation (E) step, which computes the expectation of the log-likelihood evaluated using the current estimate for the latent variables, and maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. The Farthest First algorithm [3] is an implementation of the “Farthest First Traversal Algorithm” by Hochbaum and Shmoys (1985). It finds</context>
</contexts>
<marker>[2]</marker>
<rawString>Neal R, &amp; Hinton G. A view of the EM algorithm that justifies incremental, sparse, and other variants. Learning in Graphical Models, 89, 355–368.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jon Gibson</author>
</authors>
<title>Firat Tekiner, Peter Halfpenny. NCeSS Project: Data Mining for Social Scientists. Research Computing Services,</title>
<institution>University of Manchester, U.K.</institution>
<contexts>
<context position="3756" citStr="[3]" startWordPosition="570" endWordPosition="570">result. So, the better choice is to place them as much as possible far away from each other. EM algorithm[2] is a method for finding maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. EM is an iterative method which alternates between performing an expectation (E) step, which computes the expectation of the log-likelihood evaluated using the current estimate for the latent variables, and maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. The Farthest First algorithm [3] is an implementation of the “Farthest First Traversal Algorithm” by Hochbaum and Shmoys (1985). It finds fast, approximate clusters and may be useful as an initialiser for k-means. A hierarchical clustering [4] is the guarantee that for every k, the induced k clustering has cost at most eight times that of the optimal k-clustering. A hierarchical clustering of n data points is a recursive partitioning of the data into 2, 3, 4, . . . and finally n, clusters. Each intermediate clustering is made more fine-grained by dividing one of its clusters. 4. Development 4.1 Evaluation method We consider </context>
</contexts>
<marker>[3]</marker>
<rawString>Jon Gibson, Firat Tekiner, Peter Halfpenny. NCeSS Project: Data Mining for Social Scientists. Research Computing Services, University of Manchester, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
<author>Philip M Long</author>
</authors>
<title>Performance guarantees for hierarchical clustering.</title>
<date>2005</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>555</volume>
<contexts>
<context position="3967" citStr="[4]" startWordPosition="602" endWordPosition="602">del depends on unobserved latent variables. EM is an iterative method which alternates between performing an expectation (E) step, which computes the expectation of the log-likelihood evaluated using the current estimate for the latent variables, and maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. The Farthest First algorithm [3] is an implementation of the “Farthest First Traversal Algorithm” by Hochbaum and Shmoys (1985). It finds fast, approximate clusters and may be useful as an initialiser for k-means. A hierarchical clustering [4] is the guarantee that for every k, the induced k clustering has cost at most eight times that of the optimal k-clustering. A hierarchical clustering of n data points is a recursive partitioning of the data into 2, 3, 4, . . . and finally n, clusters. Each intermediate clustering is made more fine-grained by dividing one of its clusters. 4. Development 4.1 Evaluation method We consider the gold standard as a solution to the clustering problem. All examples tagged with a given sense in the gold standard form a class. For the system output, the clusters are formed by instances assigned to the sa</context>
</contexts>
<marker>[4]</marker>
<rawString>Sanjoy Dasgupta, Philip M. Long. Performance guarantees for hierarchical clustering. Journal of Computer and System Sciences, 555–569, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Aitor Soroa. Semeval-2007 Task 02:Evaluating Word Sense Induction and Discrimination Systems.</title>
<date>2007</date>
<booktitle>Proceedings of SemEval-2007,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="4697" citStr="[5]" startWordPosition="730" endWordPosition="730">ierarchical clustering of n data points is a recursive partitioning of the data into 2, 3, 4, . . . and finally n, clusters. Each intermediate clustering is made more fine-grained by dividing one of its clusters. 4. Development 4.1 Evaluation method We consider the gold standard as a solution to the clustering problem. All examples tagged with a given sense in the gold standard form a class. For the system output, the clusters are formed by instances assigned to the same sense tag. We will compare clusters output by the system with the classes in the gold standard and compute F-score as usual [5]. F-score is computed with the formula below. Suppose is a class of the gold standard, Cr and Si is a cluster of the system generated, then F−Score C r S i = P R P+R ( , ) 2 * * /( ) (1) Then for a given class Cr, F score − (Cr ) max(F score(Cr, Si )) = − Si F − Score =∑nrFScore(Cr) c =1 n (2) where c is total number of classes, is the size nr n of class Cr, and is the total size. Participants will be required to induce the senses of the target word using only the dataset provided by the organizers. 4.2 Data Set The organizers provide 50 Chinese training data of SIGHAN2010-WSI-SampleData. The </context>
</contexts>
<marker>[5]</marker>
<rawString>Eneko Agirre, Aitor Soroa. Semeval-2007 Task 02:Evaluating Word Sense Induction and Discrimination Systems. Proceedings of SemEval-2007, pages 7–12, 2007.</rawString>
</citation>
<citation valid="false">
<note>http://www.cs.waikato.ac.nz/ml/weka/</note>
<contexts>
<context position="2027" citStr="[6]" startWordPosition="298" endWordPosition="298">ere constructed, the system trained from this corpus show the low performance on different domain test corpus. The later is word sense induction (WSI) which needs not any training data, and it has become one of the most important topics in current computational linguistics. Chinese Information Processing Society of China (CIPS) and SIGHAN organized a task is intended to promote the research on Chinese WSI. We built a WSI system named LSTC-WSI system for this task. This system tried four cluster algorithms, i.e. Simple KMeans,EM,Farthest First and Hierarchical Cluster implemented by weak 3.7.1 [6], and found Simple KMeans compete the other three ones according to their performances on training data. Finally, the results returned by Simple KMeans were submitted. 2. Features Selection Following the feature selection in word sense disambiguation, we extract neighbor words and their POSs centered in the target words. Word segmented and POS-tag tool adapted Chinese Lexical Analysis System developed by Institute of Computing Technology. No other resource is used in the system. The window size of the context is set to 5 around the ambiguous word. The neighbor words which occur only once were </context>
</contexts>
<marker>[6]</marker>
<rawString>http://www.cs.waikato.ac.nz/ml/weka/</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>