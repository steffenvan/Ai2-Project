<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017841">
<title confidence="0.9990105">
Accelerating the Annotation of Sparse Named Entities
by Dynamic Sentence Selection
</title>
<author confidence="0.99981">
Yoshimasa Tsuruoka1, Jun’ichi Tsujii1,2,3 and Sophia Ananiadou1,3
</author>
<affiliation confidence="0.987094">
1 School of Computer Science, The University of Manchester, UK
2 Department of Computer Science, The University of Tokyo, Japan
</affiliation>
<address confidence="0.793124">
3 National Centre for Text Mining (NaCTeM), Manchester, UK
</address>
<email confidence="0.994614">
yoshimasa.tsuruoka@manchester.ac.uk
tsujii@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
</email>
<sectionHeader confidence="0.995555" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923727272727">
This paper presents an active learning-like
framework for reducing the human effort for
making named entity annotations in a corpus.
In this framework, the annotation work is per-
formed as an iterative and interactive process
between the human annotator and a proba-
bilistic named entity tagger. At each itera-
tion, sentences that are most likely to con-
tain named entities of the target category are
selected by the probabilistic tagger and pre-
sented to the annotator. This iterative anno-
tation process is repeated until the estimated
coverage reaches the desired level. Unlike ac-
tive learning approaches, our framework pro-
duces a named entity corpus that is free from
the sampling bias introduced by the active
strategy. We evaluated our framework by
simulating the annotation process using two
named entity corpora and show that our ap-
proach could drastically reduce the number
of sentences to be annotated when applied to
sparse named entities.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937622222223">
Named entities play a central role in conveying im-
portant domain specific information in text, and
good named entity recognizers are often required
in building practical information extraction systems.
Previous studies have shown that automatic named
entity recognition can be performed with a reason-
able level of accuracy by using various machine
learning models such as support vector machines
(SVMs) or conditional random fields (CRFs) (Tjong
Kim Sang and De Meulder, 2003; Settles, 2004;
Okanohara et al., 2006).
However, the lack of annotated corpora, which are
indispensable for training machine learning models,
makes it difficult to broaden the scope of text mining
applications. In the biomedical domain, for exam-
ple, several annotated corpora such as GENIA (Kim
et al., 2003), PennBioIE (Kulick et al., 2004), and
GENETAG (Tanabe et al., 2005) have been created
and made publicly available, but the named entity
categories annotated in these corpora are tailored to
their specific needs and not always sufficient or suit-
able for text mining tasks that other researchers need
to address.
Active learning is a framework which can be used
for reducing the amount of human effort required to
create a training corpus (Dagan and Engelson, 1995;
Engelson and Dagan, 1996; Thompson et al., 1999;
Shen et al., 2004). In active learning, samples that
need to be annotated by the human annotator are
picked up by a machine learning model in an iter-
ative and interactive manner, considering the infor-
mativeness of the samples. Active learning has been
shown to be effective in several natural language
processing tasks including named entity recognition.
The problem with active learning is, however, that
the resulting annotated data is highly dependent on
the machine learning algorithm and the sampling
strategy employed, because active learning anno-
tates only a subset of the given corpus. This sam-
pling bias is not a serious problem if one is to use the
annotated corpus only for their own machine learn-
ing purpose and with the same machine learning al-
gorithm. However, the existence of bias is not desir-
able if one also wants the corpus to be used by other
applications or researchers. For the same reason, ac-
</bodyText>
<page confidence="0.978678">
30
</page>
<note confidence="0.7801055">
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 30–37,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999729375">
tive learning approaches cannot be used to enrich an
existing linguistic corpus with a new named entity
category.
In this paper, we present a framework that enables
one to make named entity annotations for a given
corpus with a reduced cost. Unlike active learn-
ing approaches, our framework aims to annotate all
named entities of the target category contained in
the corpus. Obviously, if we were to ensure 100%
coverage of annotation, there is no way of reducing
the annotation cost, i.e. the human annotator has to
go through every sentence in the corpus. However,
we show in this paper that it is possible to reduce
the cost by slightly relaxing the requirement for the
coverage, and the reduction can be drastic when the
target named entities are sparse.
We should note here that the purpose of this pa-
per is not to claim that our approach is superior to
existing active learning approaches. The goals are
different—while active learning aims at optimizing
the performance of the resulting machine learning-
based tagger, our framework aims to help develop
an unbiased named entity-annotated corpus.
This paper is organized as follows. Section 2 de-
scribes the overall annotation flow in our framework.
Section 3 presents how to select sentences using the
output of a probabilistic tagger. Section 4 describes
how to estimate the coverage during the course of
annotation. Experimental results using two named
entity corpora are presented in section 5. Section 6
describes related work and discussions. Concluding
remarks are given in section 7.
</bodyText>
<subsectionHeader confidence="0.826575">
2 Annotating Named Entities by Dynamic
Sentence Selection
</subsectionHeader>
<bodyText confidence="0.999538416666667">
Figure 1 shows the overall flow of our annotation
framework. The framework is an iterative process
between the human annotator and a named entity
tagger based on CRFs. In each iteration, the CRF
tagger is trained using all annotated sentences avail-
able and is applied to the unannotated sentences to
select sentences that are likely to contain named
entities of the target category. The selected sen-
tences are then annotated by the human annotator
and moved to the pool of annotated sentences.
This overall flow of annotation framework is very
similar to that of active learning. In fact, the only
</bodyText>
<listItem confidence="0.999526">
1. Select the first n sentences from the corpus and
annotate the named entities of the target cate-
gory.
2. Train a CRF tagger using all annotated sen-
tences.
3. Apply the CRF tagger to the unannotated sen-
tences in the corpus and select the top n sen-
tences that are most likely to contain target
named entities.
4. Annotate the selected sentences.
5. Go back to 2 (repeat until the estimated cover-
age reaches a satisfactory level).
</listItem>
<figureCaption confidence="0.9979365">
Figure 1: Annotating named entities by dynamic sentence
selection.
</figureCaption>
<bodyText confidence="0.999875352941177">
differences are the criterion of sentence selection
and the fact that our framework uses the estimated
coverage as the stopping condition. In active learn-
ing, sentences are selected according to their infor-
mativeness to the machine learning algorithm. Our
approach, in contrast, selects sentences that are most
likely to contain named entities of the target cate-
gory. Section 3 elaborates on how to select sentences
using the output of the CRF-based tagger.
The other key in this annotation framework is
when to stop the annotation work. If we repeat the
process until all sentences are annotated, then obvi-
ously there is not merit of using this approach. We
show in section 4 that we can quite accurately esti-
mate how much of the entities in the corpus are al-
ready annotated and use this estimated coverage as
the stopping condition.
</bodyText>
<sectionHeader confidence="0.968749" genericHeader="method">
3 Selecting Sentences using the CRF
tagger
</sectionHeader>
<bodyText confidence="0.997708166666667">
Our annotation framework takes advantage of the
ability of CRFs to output multiple probabilistic hy-
potheses. This section describes how we obtain
named entity candidates and their probabilities from
CRFs in order to compute the expected number of
named entities contained in a sentence 1.
</bodyText>
<footnote confidence="0.993085">
1We could use other machine learning algorithms for this
purpose as long as they can produce probabilistic output. For
</footnote>
<page confidence="0.999872">
31
</page>
<subsectionHeader confidence="0.989872">
3.1 The CRF tagger
</subsectionHeader>
<bodyText confidence="0.999833733333333">
CRFs (Lafferty et al., 2001) can be used for named
entity recognition by representing the spans of
named entities using the “BIO” tagging scheme, in
which ‘B’ represents the beginning of a named en-
tity, ‘I’ the inside, and ‘O’ the outside (See Table 2
for example). This representation converts the task
of named entity recognition into a sequence tagging
task.
A linear chain CRF defines a single log-linear
probabilistic distribution over the possible tag se-
quences y for a sentence x:
where fk(t, yt, yt−1, xt) is typically a binary func-
tion indicating the presence of feature k, λk is the
weight of the feature, and Z(X) is a normalization
function:
</bodyText>
<equation confidence="0.62569">
λkfk(t, yt, yt−1, xt).
</equation>
<bodyText confidence="0.983044571428571">
This modeling allows us to define features on states
(“BIO” tags) and edges (pairs of adjacent “BIO”
tags) combined with observations (e.g. words and
part-of-speech (POS) tags).
The weights of the features are determined
in such a way that they maximize the condi-
tional log-likelihood of the training data2 L(θ) _
PZ_&apos;1 log pe(y(i)|x(i)). We use the L-BFGS algo-
rithm (Nocedal, 1980) to compute those parameters.
Table 1 lists the feature templates used in the CRF
tagger. We used unigrams of words/POS tags, and
prefixes and suffixes of the current word. The cur-
rent word is also normalized by lowering capital let-
ters and converting all numerals into ‘#’, and used
as a feature. We created a word shape feature from
the current word by converting consecutive capital
letters into ‘A’, small letters ‘a’, and numerals ‘#’.
example, maximum entropy Markov models are a possible al-
ternative. We chose the CRF model because it has been proved
to deliver state-of-the-art performance for named entity recog-
nition tasks by previous studies.
</bodyText>
<footnote confidence="0.7488505">
2In the actual implementation, we used L2 norm penalty for
regularization.
</footnote>
<table confidence="0.999678">
Word Unigram wi, wi−1, wi+1 &amp; yi
POS Unigram pi, pi−1, pi+1 &amp; yi
Prefix, Suffix prefixes of wi &amp; yi
suffixes of wi &amp; yi
(up to length 3)
Normalized Word N(wi) &amp; yi
Word Shape S(wi) &amp; yi
Tag Bi-gram true &amp; yi−1yi
</table>
<tableCaption confidence="0.99974">
Table 1: Feature templates used in the CRF tagger.
</tableCaption>
<subsectionHeader confidence="0.987692">
3.2 Computing the expected number of named
entities
</subsectionHeader>
<bodyText confidence="0.999792129032258">
To select sentences that are most likely to contain
named entities of the target category, we need to
obtain the expected number of named entities con-
tained in each sentence. CRFs are well-suited for
this task as the output is fully probabilistic.
Suppose, for example, that the sentence is “Tran-
scription factor GATA-1 and the estrogen receptor”.
Table 2 shows an example of the 5-best sequences
output by the CRF tagger. The sequences are rep-
resented by the aforementioned “BIO” representa-
tion. For example, the first sequence indicates that
there is one named entity ‘Transcription factor’ in
the sequence. By summing up these probabilistic se-
quences, we can compute the probabilities for pos-
sible named entities in a sentence. From the five se-
quences in Table 2, we obtain the following three
named entities and their corresponding probabilities.
‘Transcription factor’ (0.677 + 0.242 = 0.916)
‘estrogen receptor’ (0.242 + 0.009 = 0.251)
‘Transcription factor GATA-1’ (0.012 + 0.009 =
0.021)
The expected number of named entities in this
sentence can then be calculated as 0.916 + 0.251 +
0.021 = 1.188.
In this example, we used 5-best sequences as an
approximation of all possible sequences output by
the tagger, which are needed to compute the exact
expected number of entities. One possible way to
achieve a good approximation is to use a large N for
N-best sequences, but there is a simpler and more
efficient way 3, which directly produces the exact
</bodyText>
<footnote confidence="0.59795">
3We thank an anonymous reviewer for pointing this out.
</footnote>
<equation confidence="0.999366764705882">
XXx) exp
t=1 k=1
λkfk(t, yt, yt−1, xt),
T K
1
p(y|x) _
Z(
X
Z(x) _
y
T
X
t=1
exp
K
X
k=1
</equation>
<page confidence="0.991762">
32
</page>
<table confidence="0.999665142857143">
Probability Transcription factor GATA-1 and the estrogen receptor
0.677 B I O O O O O
0.242 B I O O O B I
0.035 O O O O O O O
0.012 B I I O O O O
0.009 B I I O O B I
: : : : : : : :
</table>
<tableCaption confidence="0.998347">
Table 2: N-best sequences output by the CRF tagger.
</tableCaption>
<bodyText confidence="0.998628454545455">
expected number of entities. Recall that named enti-
ties are represented with the “BIO” tags. Since one
entity always contains one “B” tag, we can compute
the number of expected entities by simply summing
up the marginal probabilities for the “B” tag on each
token in the sentence4.
Once we compute the expected number of enti-
ties for every unannotated sentence in the corpus,
we sort the sentences in descending order of the ex-
pected number of entities and choose the top n sen-
tences to be presented to the human annotator.
</bodyText>
<sectionHeader confidence="0.992833" genericHeader="method">
4 Coverage Estimation
</sectionHeader>
<bodyText confidence="0.999975588235294">
To ensure the quality of the resulting annotated cor-
pus, it is crucial to be able to know the current cov-
erage of annotation at each iteration in the annota-
tion process. To compute the coverage, however,
one needs to know the total number of target named
entities in the corpus. The problem is that it is not
known until all sentences are annotated.
In this paper, we solve this dilemma by using
an estimated value for the total number of entities.
Then, the estimated coverage can be computed as
follows:
where m is the number of entities actually annotated
so far and EZ is the expected number of entities in
sentence i, and U is the set of unannotated sentences
in the corpus. At any iteration, m is always known
and EZ is obtained from the output of the CRF tagger
as explained in the previous section.
</bodyText>
<footnote confidence="0.816477">
4The marginal probabilities on each token can be computed
by the forward-backward algorithm, which is much more effi-
cient than computing N-best sequences for a large N.
</footnote>
<table confidence="0.999976">
# Entities Sentences (%)
CoNLL: LOC 7,140 5,127 (36.5%)
CoNLL: MISC 3,438 2,698 (19.2%)
CoNLL: ORG 6,321 4,587 (32.7%)
CoNLL: PER 6,600 4,373 (31.1%)
GENIA: DNA 2,017 5,251 (28.3%)
GENIA: RNA 225 810 ( 4.4%)
GENIA: cell line 835 2,880 (15.5%)
GENIA: cell type 1,104 5,212 (28.1%)
GENIA: protein 5,272 13,040 (70.3%)
</table>
<tableCaption confidence="0.999536">
Table 3: Statistics of named entities.
</tableCaption>
<sectionHeader confidence="0.998128" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999974625">
We carried out experiments to see how our method
can improve the efficiency of annotation process
for sparse named entities. We evaluate our method
by simulating the annotation process using existing
named entity corpora. In other words, we use the
gold-standard annotations in the corpus as the anno-
tations that would be made by the human annotator
during the annotation process.
</bodyText>
<subsectionHeader confidence="0.972581">
5.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999929428571429">
We used two named entity corpora for the exper-
iments. One is the training data provided for the
CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003), which consists of 14,041 sen-
tences and includes four named entity categories
(LOC, MISC, ORG, and PER) for the general do-
main. The other is the training data provided for
the NLPBA shared task (Kim et al., 2004), which
consists of 18,546 sentences and five named entity
categories (DNA, RNA, cell line, cell type, and pro-
tein) for the biomedical domain. This corpus is cre-
ated from the GENIA corpus (Kim et al., 2003) by
merging the original fine-grained named entity cate-
gories.
</bodyText>
<equation confidence="0.99382875">
m
(estimated coverage) =
(1)
m + &amp;EU EZ
</equation>
<page confidence="0.997279">
33
</page>
<figure confidence="0.9989745">
0 2000 4000 6000 8000 10000
Number of Sentences
</figure>
<figureCaption confidence="0.999008">
Figure 3: Annotation of MISC in the CoNLL corpus.
</figureCaption>
<figure confidence="0.9990535">
0 2000 4000 6000 8000 10000
Number of Sentences
</figure>
<figureCaption confidence="0.999391">
Figure 5: Annotation of PER in the CoNLL corpus.
</figureCaption>
<figure confidence="0.998707666666667">
Coverage
Estimated Coverage
Baseline
</figure>
<figureCaption confidence="0.987403">
Figure 2: Annotation of LOC in the CoNLL corpus.
</figureCaption>
<figure confidence="0.999833857142857">
0 2000 4000 6000 8000 10000
Number of Sentences
0.8
0.6
0.4
0.2
0
1
Coverage
Estimated Coverage
Baseline
Coverage
Estimated Coverage
Baseline
</figure>
<figureCaption confidence="0.998808">
Figure 4: Annotation of ORG in the CoNLL corpus.
</figureCaption>
<figure confidence="0.999629956521739">
0 2000 4000 6000 8000 10000
Number of Sentences
0.8
0.6
0.4
0.2
0
1
Coverage
Estimated Coverage
Baseline
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
</figure>
<bodyText confidence="0.997659857142857">
Table 3 shows statistics of the named entities in-
cluded in the corpora. The first column shows the
number of named entities for each category. The
second column shows the number of the sentences
that contain the named entities of each category. We
can see that some of the named entity categories are
very sparse. For example, named entities of “RNA”
appear only in 4.4% of the sentences in the corpus.
In contrast, named entities of “protein” appear in
more than 70% of the sentences in the corpus.
In the experiments reported in the following sec-
tions, we do not use the “protein” category because
there is no merit of using our framework when most
sentences are relevant to the target category.
</bodyText>
<subsectionHeader confidence="0.688845">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.998026076923077">
We carried out eight sets of experiments, each of
which corresponds to one of those named entity cat-
egories shown in Table 3 (excluding the “protein”
category). The number of sentences selected in each
iteration (the value of n in Figure 1) was set to 100
throughout all experiments.
Figures 2 to 5 show the results obtained on the
CoNLL data. The figures show how the coverage
increases as the annotation process proceeds. The
x-axis shows the number of annotated sentences.
Each figure contains three lines. The normal line
represents the coverage actually achieved, which is
computed as follows:
</bodyText>
<equation confidence="0.87335">
entities annotated
(coverage) = total number of entities.
(2)
</equation>
<bodyText confidence="0.9999565">
The dashed line represents the coverage estimated
by using equation 1. For the purpose of comparison,
the dotted line shows the coverage achieved by the
baseline annotation strategy in which sentences are
selected sequentially from the beginning to the end
in the corpus.
The figures clearly show that our method can
drastically accelerate the annotation process in com-
parison to the baseline annotation strategy. The im-
provement is most evident in Figure 3, in which
</bodyText>
<page confidence="0.993696">
34
</page>
<figure confidence="0.9990125">
0 2000 4000 6000 8000 10000
Number of Sentences
</figure>
<figureCaption confidence="0.999765">
Figure 7: Annotation of RNA in the GENIA corpus.
</figureCaption>
<figure confidence="0.9990175">
0 2000 4000 6000 8000 10000
Number of Sentences
</figure>
<figureCaption confidence="0.999495">
Figure 9: Annotation of cell type in the GENIA corpus.
</figureCaption>
<figure confidence="0.998696333333333">
Coverage
Estimated Coverage
Baseline
</figure>
<figureCaption confidence="0.997929">
Figure 6: Annotation of DNA in the GENIA corpus.
</figureCaption>
<figure confidence="0.999825285714286">
0 2000 4000 6000 8000 10000
Number of Sentences
0.8
0.6
0.4
0.2
0
1
Coverage
Estimated Coverage
Baseline
Coverage
Estimated Coverage
Baseline
</figure>
<figureCaption confidence="0.998429">
Figure 8: Annotation of cell line in the GENIA corpus.
</figureCaption>
<figure confidence="0.999617086956522">
0 2000 4000 6000 8000 10000
Number of Sentences
0.8
0.6
0.4
0.2
0
1
Coverage
Estimated Coverage
Baseline
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
</figure>
<bodyText confidence="0.9943001">
named entities of the category “MISC” are anno-
tated.
We should also note that coverage estimation was
surprisingly accurate. In all experiments, the differ-
ence between the estimated coverage and the real
coverage was very small. This means that we can
safely use the estimated coverage as the stopping
condition for the annotation work.
Figures 6 to 9 show the experimental results on
the GENIA data. The figures show the same char-
acteristics observed in the CoNLL data. The accel-
eration by our framework was most evident for the
“RNA” category.
Table 4 shows how much we can save the annota-
tion cost if we stop the annotation process when the
estimated coverage reaches 99%. The first column
shows the coverage actually achieved and the second
column shows the number and ratio of the sentences
annotated in the corpus. This table shows that, on
average, we can achieve a coverage of 99.0% by an-
notating 52.4% of the sentences in the corpus. In
other words, we could roughly halve the annotation
cost by accepting the missing rate of 1.0%.
As expected, the cost reduction was most drastic
when “RNA”, which is the most sparse named entity
category (see Table 3), was targeted. The cost reduc-
tion was more than seven-fold. These experimental
results confirm that our annotation framework is par-
ticularly useful when applied to sparse named enti-
ties.
Table 4 also shows the timing information on the
experiments 5. One of the potential problems with
this kind of active learning-like framework is the
computation time required to retrain the tagger at
each iteration. Since the human annotator has to
wait while the tagger is being retrained, the compu-
tation time required for retraining the tagger should
not be very long. In our experiments, the worst
case (i.e. DNA) required 443 seconds for retrain-
ing the tagger at the last iteration, but in most cases
</bodyText>
<footnote confidence="0.998743">
5We used AMD Opteron 2.2GHz servers for the experiments
and our CRF tagger is implemented in C++.
</footnote>
<page confidence="0.988683">
35
</page>
<table confidence="0.9997301">
Coverage Sentences Annotated (%) Cumulative Time (second) Last Interval (second)
CoNLL: LOC 99.1% 7,600 (54.1%) 3,362 92
CoNLL: MISC 96.9% 5,400 (38.5%) 1,818 61
CoNLL: ORG 99.7% 8,900 (63.4%) 5,201 104
CoNLL: PER 98.0% 6,200 (44.2%) 2,300 75
GENIA: DNA 99.8% 11,900 (64.2%) 33,464 443
GENIA: RNA 99.2% 2,500 (13.5%) 822 56
GENIA: cell line 99.6% 9,400 (50.7%) 15,870 284
GENIA: cell type 99.3% 8,600 (46.4%) 13,487 295
Average 99.0% - (52.4%) - -
</table>
<tableCaption confidence="0.999944">
Table 4: Coverage achieved when the estimated coverage reached 99%.
</tableCaption>
<bodyText confidence="0.998276714285714">
the training time for each iteration was kept under
several minutes.
In this work, we used the BFGS algorithm for
training the CRF model, but it is probably possible to
further reduce the training time by using more recent
parameter estimation algorithms such as exponenti-
ated gradient algorithms (Globerson et al., 2007).
</bodyText>
<sectionHeader confidence="0.99786" genericHeader="evaluation">
6 Discussion and Related Work
</sectionHeader>
<bodyText confidence="0.999989912280702">
Our annotation framework is, by definition, not
something that can ensure a coverage of 100%. The
seriousness of a missing rate of, for example, 1% is
not entirely clear—it depends on the application and
the purpose of annotation. In general, however, it
is hard to achieve a coverage of 100% in real an-
notation work even if the human annotator scans
through all sentences, because there is often ambi-
guity in deciding whether a particular named entity
should be annotated or not. Previous studies report
that inter-annotator agreement rates with regards to
gene/protein name annotation are f-scores around
90% (Morgan et al., 2004; Vlachos and Gasperin,
2006). We believe that the missing rate of 1% can be
an acceptable level of sacrifice, given the cost reduc-
tion achieved and the unavoidable discrepancy made
by the human annotator.
At the same time, we should also note that our
framework could be used in conjunction with ex-
isting methods for semi-supervised learning to im-
prove the performance of the CRF tagger, which
in turn will improve the coverage. It is also pos-
sible to improve the performance of the tagger by
using external dictionaries or using more sophis-
ticated probabilistic models such as semi-Markov
CRFs (Sarawagi and Cohen, 2004). These enhance-
ments should further improve the coverage, keeping
the same degree of cost reduction.
The idea of improving the efficiency of annota-
tion work by using automatic taggers is certainly not
new. Tanabe et al. (2005) applied a gene/protein
name tagger to the target sentences and modified
the results manually. Culotta and McCallum (2005)
proposed to have the human annotator select the
correct annotation from multiple choices produced
by a CRF tagger for each sentence. Tomanek et
al. (2007) discuss the reusability of named entity-
annotated corpora created by an active learning ap-
proach and show that it is possible to build a cor-
pus that is useful to different machine learning algo-
rithms to a certain degree.
The limitation of our framework is that it is use-
ful only when the target named entities are sparse
because the upper bound of cost saving is limited
by the proportion of the relevant sentences in the
corpus. Our framework may therefore not be suit-
able for a situation where one wants to make an-
notations for named entities of many categories si-
multaneously (e.g. creating a corpus like GENIA
from scratch). In contrast, our framework should be
useful in a situation where one needs to modify or
enrich named entity annotations in an existing cor-
pus, because the target named entities are almost al-
ways sparse in such cases. We should also note that
named entities in full papers, which recently started
to attract much attention, tend to be more sparse than
those in abstracts.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999911">
We have presented a simple but powerful framework
for reducing the human effort for making name en-
tity annotations in a corpus. The proposed frame-
work allows us to annotate almost all named entities
</bodyText>
<page confidence="0.993686">
36
</page>
<bodyText confidence="0.999981294117647">
of the target category in the given corpus without
having to scan through all the sentences. The frame-
work also allows us to know when to stop the anno-
tation process by consulting the estimated coverage
of annotation.
Experimental results demonstrated that the frame-
work can reduce the number of sentences to be anno-
tated almost by half, achieving a coverage of 99.0%.
Our framework was particularly effective when the
target named entities were very sparse.
Unlike active learning, this work enables us to
create a named entity corpus that is free from the
sampling bias introduced by the active learning strat-
egy. This work will therefore be especially useful
when one needs to enrich an existing linguistic cor-
pus (e.g. WSJ, GENIA, or PennBioIE) with named
entity annotations for a new semantic category.
</bodyText>
<sectionHeader confidence="0.982742" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.996174333333333">
This work is partially supported by BBSRC grant
BB/E004431/1. The UK National Centre for Text
Mining is sponsored by the JISC/BBSRC/EPSRC.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998676620253165">
Aron Culotta and Andrew McCallum. 2005. Reducing
labeling effort for structured prediction tasks. In Pro-
ceedings ofAAAI-05, pages 746–751.
Ido Dagan and Sean P. Engelson. 1995. Committee-
based sampling for training probabilistic classifiers. In
Proceedings ofICML, pages 150–157.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In Proceedings ofACL, pages 319–326.
A. Globerson, T. Koo, X. Carreras, and M. Collins. 2007.
Exponentiated gradient algorithms for log-linear struc-
tured prediction. In Proceedings ofICML, pages 305–
312.
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus–a semantically annotated corpus for bio-
textmining. Bioinformatics, 19 (Suppl. 1):180–182.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recogni-
tion task at JNLPBA. In Proceedings of the Interna-
tional Joint Workshop on Natural Language Process-
ing in Biomedicine and its Applications (JNLPBA),
pages 70–75.
Seth Kulick, Ann Bies, Mark Libeman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein, and
Lyle Ungar. 2004. Integrated annotation for biomed-
ical information extraction. In Proceedings of HLT-
NAACL 2004 Workshop: Biolink 2004, pages 61–68.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings ofICML, pages 282–289.
Alexander A. Morgan, Lynette Hirschman, Marc
Colosimo, Alexander S. Yeh, and Jeff B. Colombe.
2004. Gene name identification and normalization us-
ing a model organism database. Journal ofBiomedical
Informatics, 37:396–410.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773–782.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun’ichi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. In Proceedings of COL-
ING/ACL, pages 465–472.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In Proceedings ofNIPS.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In COLING 2004 International Joint workshop
on Natural Language Processing in Biomedicine and
its Applications (NLPBA/BioNLP) 2004, pages 107–
110.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proceedings ofACL,
pages 589–596, Barcelona, Spain.
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne
Matten, and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMCBioinformatics, 6(Suppl 1):S3.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Pro-
ceedings ofICML, pages 406–414.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003, pages 142–147.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of EMNLP-CoNLL,
pages 486–495.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and evaluating named entity recognition in
the biomedical domain. In Proceedings of the HLT-
NAACL BioNLP Workshop on Linking Natural Lan-
guage and Biology, pages 138–145.
</reference>
<page confidence="0.999601">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.508050">
<title confidence="0.975099">Accelerating the Annotation of Sparse Named by Dynamic Sentence Selection</title>
<author confidence="0.882223">Jun’ichi</author>
<author confidence="0.882223">Sophia</author>
<affiliation confidence="0.942151">1School of Computer Science, The University of Manchester, UK 2Department of Computer Science, The University of Tokyo, Japan</affiliation>
<address confidence="0.990129">3National Centre for Text Mining (NaCTeM), Manchester, UK</address>
<email confidence="0.842494">yoshimasa.tsuruoka@manchester.ac.uktsujii@is.s.u-tokyo.ac.jpsophia.ananiadou@manchester.ac.uk</email>
<abstract confidence="0.999569956521739">This paper presents an active learning-like framework for reducing the human effort for making named entity annotations in a corpus. In this framework, the annotation work is performed as an iterative and interactive process between the human annotator and a probabilistic named entity tagger. At each iteration, sentences that are most likely to contain named entities of the target category are selected by the probabilistic tagger and presented to the annotator. This iterative annotation process is repeated until the estimated coverage reaches the desired level. Unlike active learning approaches, our framework produces a named entity corpus that is free from the sampling bias introduced by the active strategy. We evaluated our framework by simulating the annotation process using two named entity corpora and show that our approach could drastically reduce the number of sentences to be annotated when applied to sparse named entities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>Reducing labeling effort for structured prediction tasks.</title>
<date>2005</date>
<booktitle>In Proceedings ofAAAI-05,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="22212" citStr="Culotta and McCallum (2005)" startWordPosition="3731" endWordPosition="3734">to improve the performance of the CRF tagger, which in turn will improve the coverage. It is also possible to improve the performance of the tagger by using external dictionaries or using more sophisticated probabilistic models such as semi-Markov CRFs (Sarawagi and Cohen, 2004). These enhancements should further improve the coverage, keeping the same degree of cost reduction. The idea of improving the efficiency of annotation work by using automatic taggers is certainly not new. Tanabe et al. (2005) applied a gene/protein name tagger to the target sentences and modified the results manually. Culotta and McCallum (2005) proposed to have the human annotator select the correct annotation from multiple choices produced by a CRF tagger for each sentence. Tomanek et al. (2007) discuss the reusability of named entityannotated corpora created by an active learning approach and show that it is possible to build a corpus that is useful to different machine learning algorithms to a certain degree. The limitation of our framework is that it is useful only when the target named entities are sparse because the upper bound of cost saving is limited by the proportion of the relevant sentences in the corpus. Our framework m</context>
</contexts>
<marker>Culotta, McCallum, 2005</marker>
<rawString>Aron Culotta and Andrew McCallum. 2005. Reducing labeling effort for structured prediction tasks. In Proceedings ofAAAI-05, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Sean P Engelson</author>
</authors>
<title>Committeebased sampling for training probabilistic classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>150--157</pages>
<contexts>
<context position="2649" citStr="Dagan and Engelson, 1995" startWordPosition="397" endWordPosition="400"> it difficult to broaden the scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. Active learning has been shown to be effective in several natural language processing tasks including named entity recognition. The problem with active learning is, however, that the resulting annotated data is highly dependent on the machine learning algorithm and the sampling strategy employed, because active learning </context>
</contexts>
<marker>Dagan, Engelson, 1995</marker>
<rawString>Ido Dagan and Sean P. Engelson. 1995. Committeebased sampling for training probabilistic classifiers. In Proceedings ofICML, pages 150–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Engelson</author>
<author>Ido Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>319--326</pages>
<contexts>
<context position="2675" citStr="Engelson and Dagan, 1996" startWordPosition="401" endWordPosition="404">he scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. Active learning has been shown to be effective in several natural language processing tasks including named entity recognition. The problem with active learning is, however, that the resulting annotated data is highly dependent on the machine learning algorithm and the sampling strategy employed, because active learning annotates only a subset of</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Sean Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. In Proceedings ofACL, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Globerson</author>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Exponentiated gradient algorithms for log-linear structured prediction.</title>
<date>2007</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>305--312</pages>
<contexts>
<context position="20579" citStr="Globerson et al., 2007" startWordPosition="3464" endWordPosition="3467">98.0% 6,200 (44.2%) 2,300 75 GENIA: DNA 99.8% 11,900 (64.2%) 33,464 443 GENIA: RNA 99.2% 2,500 (13.5%) 822 56 GENIA: cell line 99.6% 9,400 (50.7%) 15,870 284 GENIA: cell type 99.3% 8,600 (46.4%) 13,487 295 Average 99.0% - (52.4%) - - Table 4: Coverage achieved when the estimated coverage reached 99%. the training time for each iteration was kept under several minutes. In this work, we used the BFGS algorithm for training the CRF model, but it is probably possible to further reduce the training time by using more recent parameter estimation algorithms such as exponentiated gradient algorithms (Globerson et al., 2007). 6 Discussion and Related Work Our annotation framework is, by definition, not something that can ensure a coverage of 100%. The seriousness of a missing rate of, for example, 1% is not entirely clear—it depends on the application and the purpose of annotation. In general, however, it is hard to achieve a coverage of 100% in real annotation work even if the human annotator scans through all sentences, because there is often ambiguity in deciding whether a particular named entity should be annotated or not. Previous studies report that inter-annotator agreement rates with regards to gene/prote</context>
</contexts>
<marker>Globerson, Koo, Carreras, Collins, 2007</marker>
<rawString>A. Globerson, T. Koo, X. Carreras, and M. Collins. 2007. Exponentiated gradient algorithms for log-linear structured prediction. In Proceedings ofICML, pages 305– 312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Tsujii</author>
</authors>
<title>GENIA corpus–a semantically annotated corpus for biotextmining.</title>
<date>2003</date>
<journal>Bioinformatics,</journal>
<volume>19</volume>
<pages>1--180</pages>
<contexts>
<context position="2186" citStr="Kim et al., 2003" startWordPosition="321" endWordPosition="324">l information extraction systems. Previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). However, the lack of annotated corpora, which are indispensable for training machine learning models, makes it difficult to broaden the scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human</context>
<context position="14591" citStr="Kim et al., 2003" startWordPosition="2442" endWordPosition="2445">during the annotation process. 5.1 Corpus We used two named entity corpora for the experiments. One is the training data provided for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), which consists of 14,041 sentences and includes four named entity categories (LOC, MISC, ORG, and PER) for the general domain. The other is the training data provided for the NLPBA shared task (Kim et al., 2004), which consists of 18,546 sentences and five named entity categories (DNA, RNA, cell line, cell type, and protein) for the biomedical domain. This corpus is created from the GENIA corpus (Kim et al., 2003) by merging the original fine-grained named entity categories. m (estimated coverage) = (1) m + &amp;EU EZ 33 0 2000 4000 6000 8000 10000 Number of Sentences Figure 3: Annotation of MISC in the CoNLL corpus. 0 2000 4000 6000 8000 10000 Number of Sentences Figure 5: Annotation of PER in the CoNLL corpus. Coverage Estimated Coverage Baseline Figure 2: Annotation of LOC in the CoNLL corpus. 0 2000 4000 6000 8000 10000 Number of Sentences 0.8 0.6 0.4 0.2 0 1 Coverage Estimated Coverage Baseline Coverage Estimated Coverage Baseline Figure 4: Annotation of ORG in the CoNLL corpus. 0 2000 4000 6000 8000 </context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GENIA corpus–a semantically annotated corpus for biotextmining. Bioinformatics, 19 (Suppl. 1):180–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>T Ohta</author>
<author>Y Tsuruoka</author>
<author>Y Tateisi</author>
<author>N Collier</author>
</authors>
<title>Introduction to the bio-entity recognition task at JNLPBA.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA),</booktitle>
<pages>70--75</pages>
<contexts>
<context position="14385" citStr="Kim et al., 2004" startWordPosition="2406" endWordPosition="2409">hod by simulating the annotation process using existing named entity corpora. In other words, we use the gold-standard annotations in the corpus as the annotations that would be made by the human annotator during the annotation process. 5.1 Corpus We used two named entity corpora for the experiments. One is the training data provided for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), which consists of 14,041 sentences and includes four named entity categories (LOC, MISC, ORG, and PER) for the general domain. The other is the training data provided for the NLPBA shared task (Kim et al., 2004), which consists of 18,546 sentences and five named entity categories (DNA, RNA, cell line, cell type, and protein) for the biomedical domain. This corpus is created from the GENIA corpus (Kim et al., 2003) by merging the original fine-grained named entity categories. m (estimated coverage) = (1) m + &amp;EU EZ 33 0 2000 4000 6000 8000 10000 Number of Sentences Figure 3: Annotation of MISC in the CoNLL corpus. 0 2000 4000 6000 8000 10000 Number of Sentences Figure 5: Annotation of PER in the CoNLL corpus. Coverage Estimated Coverage Baseline Figure 2: Annotation of LOC in the CoNLL corpus. 0 2000 </context>
</contexts>
<marker>Kim, Ohta, Tsuruoka, Tateisi, Collier, 2004</marker>
<rawString>J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier. 2004. Introduction to the bio-entity recognition task at JNLPBA. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA), pages 70–75.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Seth Kulick</author>
</authors>
<location>Ann Bies, Mark Libeman, Mark Mandel,</location>
<marker>Kulick, </marker>
<rawString>Seth Kulick, Ann Bies, Mark Libeman, Mark Mandel,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Martha Palmer</author>
<author>Andrew Schein</author>
<author>Lyle Ungar</author>
</authors>
<title>Integrated annotation for biomedical information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL</booktitle>
<pages>61--68</pages>
<marker>McDonald, Palmer, Schein, Ungar, 2004</marker>
<rawString>Ryan McDonald, Martha Palmer, Andrew Schein, and Lyle Ungar. 2004. Integrated annotation for biomedical information extraction. In Proceedings of HLTNAACL 2004 Workshop: Biolink 2004, pages 61–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="7818" citStr="Lafferty et al., 2001" startWordPosition="1251" endWordPosition="1254"> how much of the entities in the corpus are already annotated and use this estimated coverage as the stopping condition. 3 Selecting Sentences using the CRF tagger Our annotation framework takes advantage of the ability of CRFs to output multiple probabilistic hypotheses. This section describes how we obtain named entity candidates and their probabilities from CRFs in order to compute the expected number of named entities contained in a sentence 1. 1We could use other machine learning algorithms for this purpose as long as they can produce probabilistic output. For 31 3.1 The CRF tagger CRFs (Lafferty et al., 2001) can be used for named entity recognition by representing the spans of named entities using the “BIO” tagging scheme, in which ‘B’ represents the beginning of a named entity, ‘I’ the inside, and ‘O’ the outside (See Table 2 for example). This representation converts the task of named entity recognition into a sequence tagging task. A linear chain CRF defines a single log-linear probabilistic distribution over the possible tag sequences y for a sentence x: where fk(t, yt, yt−1, xt) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander A Morgan</author>
<author>Lynette Hirschman</author>
<author>Marc Colosimo</author>
<author>Alexander S Yeh</author>
<author>Jeff B Colombe</author>
</authors>
<title>Gene name identification and normalization using a model organism database.</title>
<date>2004</date>
<journal>Journal ofBiomedical Informatics,</journal>
<pages>37--396</pages>
<contexts>
<context position="21242" citStr="Morgan et al., 2004" startWordPosition="3572" endWordPosition="3575">on framework is, by definition, not something that can ensure a coverage of 100%. The seriousness of a missing rate of, for example, 1% is not entirely clear—it depends on the application and the purpose of annotation. In general, however, it is hard to achieve a coverage of 100% in real annotation work even if the human annotator scans through all sentences, because there is often ambiguity in deciding whether a particular named entity should be annotated or not. Previous studies report that inter-annotator agreement rates with regards to gene/protein name annotation are f-scores around 90% (Morgan et al., 2004; Vlachos and Gasperin, 2006). We believe that the missing rate of 1% can be an acceptable level of sacrifice, given the cost reduction achieved and the unavoidable discrepancy made by the human annotator. At the same time, we should also note that our framework could be used in conjunction with existing methods for semi-supervised learning to improve the performance of the CRF tagger, which in turn will improve the coverage. It is also possible to improve the performance of the tagger by using external dictionaries or using more sophisticated probabilistic models such as semi-Markov CRFs (Sar</context>
</contexts>
<marker>Morgan, Hirschman, Colosimo, Yeh, Colombe, 2004</marker>
<rawString>Alexander A. Morgan, Lynette Hirschman, Marc Colosimo, Alexander S. Yeh, and Jeff B. Colombe. 2004. Gene name identification and normalization using a model organism database. Journal ofBiomedical Informatics, 37:396–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
</authors>
<title>Updating quasi-newton matrices with limited storage.</title>
<date>1980</date>
<journal>Mathematics of Computation,</journal>
<volume>35</volume>
<issue>151</issue>
<contexts>
<context position="8849" citStr="Nocedal, 1980" startWordPosition="1426" endWordPosition="1427">possible tag sequences y for a sentence x: where fk(t, yt, yt−1, xt) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: λkfk(t, yt, yt−1, xt). This modeling allows us to define features on states (“BIO” tags) and edges (pairs of adjacent “BIO” tags) combined with observations (e.g. words and part-of-speech (POS) tags). The weights of the features are determined in such a way that they maximize the conditional log-likelihood of the training data2 L(θ) _ PZ_&apos;1 log pe(y(i)|x(i)). We use the L-BFGS algorithm (Nocedal, 1980) to compute those parameters. Table 1 lists the feature templates used in the CRF tagger. We used unigrams of words/POS tags, and prefixes and suffixes of the current word. The current word is also normalized by lowering capital letters and converting all numerals into ‘#’, and used as a feature. We created a word shape feature from the current word by converting consecutive capital letters into ‘A’, small letters ‘a’, and numerals ‘#’. example, maximum entropy Markov models are a possible alternative. We chose the CRF model because it has been proved to deliver state-of-the-art performance fo</context>
</contexts>
<marker>Nocedal, 1980</marker>
<rawString>Jorge Nocedal. 1980. Updating quasi-newton matrices with limited storage. Mathematics of Computation, 35(151):773–782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Okanohara</author>
<author>Yusuke Miyao</author>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Improving the scalability of semi-markov conditional random fields for named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>465--472</pages>
<contexts>
<context position="1916" citStr="Okanohara et al., 2006" startWordPosition="279" endWordPosition="282">ically reduce the number of sentences to be annotated when applied to sparse named entities. 1 Introduction Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). However, the lack of annotated corpora, which are indispensable for training machine learning models, makes it difficult to broaden the scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is</context>
</contexts>
<marker>Okanohara, Miyao, Tsuruoka, Tsujii, 2006</marker>
<rawString>Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsuruoka, and Jun’ichi Tsujii. 2006. Improving the scalability of semi-markov conditional random fields for named entity recognition. In Proceedings of COLING/ACL, pages 465–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings ofNIPS.</booktitle>
<contexts>
<context position="21864" citStr="Sarawagi and Cohen, 2004" startWordPosition="3676" endWordPosition="3679">004; Vlachos and Gasperin, 2006). We believe that the missing rate of 1% can be an acceptable level of sacrifice, given the cost reduction achieved and the unavoidable discrepancy made by the human annotator. At the same time, we should also note that our framework could be used in conjunction with existing methods for semi-supervised learning to improve the performance of the CRF tagger, which in turn will improve the coverage. It is also possible to improve the performance of the tagger by using external dictionaries or using more sophisticated probabilistic models such as semi-Markov CRFs (Sarawagi and Cohen, 2004). These enhancements should further improve the coverage, keeping the same degree of cost reduction. The idea of improving the efficiency of annotation work by using automatic taggers is certainly not new. Tanabe et al. (2005) applied a gene/protein name tagger to the target sentences and modified the results manually. Culotta and McCallum (2005) proposed to have the human annotator select the correct annotation from multiple choices produced by a CRF tagger for each sentence. Tomanek et al. (2007) discuss the reusability of named entityannotated corpora created by an active learning approach </context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. Semimarkov conditional random fields for information extraction. In Proceedings ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Biomedical named entity recognition using conditional random fields and rich feature sets.</title>
<date>2004</date>
<booktitle>In COLING 2004 International Joint workshop on Natural Language Processing in Biomedicine and its Applications</booktitle>
<pages>107--110</pages>
<contexts>
<context position="1891" citStr="Settles, 2004" startWordPosition="277" endWordPosition="278">ach could drastically reduce the number of sentences to be annotated when applied to sparse named entities. 1 Introduction Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). However, the lack of annotated corpora, which are indispensable for training machine learning models, makes it difficult to broaden the scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to ad</context>
</contexts>
<marker>Settles, 2004</marker>
<rawString>Burr Settles. 2004. Biomedical named entity recognition using conditional random fields and rich feature sets. In COLING 2004 International Joint workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP) 2004, pages 107– 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
<author>ChewLim Tan</author>
</authors>
<title>Multi-criteria-based active learning for named entity recognition.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>589--596</pages>
<location>Barcelona,</location>
<contexts>
<context position="2718" citStr="Shen et al., 2004" startWordPosition="409" endWordPosition="412">edical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. Active learning has been shown to be effective in several natural language processing tasks including named entity recognition. The problem with active learning is, however, that the resulting annotated data is highly dependent on the machine learning algorithm and the sampling strategy employed, because active learning annotates only a subset of the given corpus. This sampling bias is no</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and ChewLim Tan. 2004. Multi-criteria-based active learning for named entity recognition. In Proceedings ofACL, pages 589–596, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorraine Tanabe</author>
<author>Natalie Xie</author>
<author>Lynne H Thom</author>
<author>Wayne Matten</author>
<author>W John Wilbur</author>
</authors>
<title>GENETAG: a tagged corpus for gene/protein named entity recognition. BMCBioinformatics, 6(Suppl 1):S3.</title>
<date>2005</date>
<contexts>
<context position="2254" citStr="Tanabe et al., 2005" startWordPosition="332" endWordPosition="335">t automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). However, the lack of annotated corpora, which are indispensable for training machine learning models, makes it difficult to broaden the scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative</context>
<context position="22090" citStr="Tanabe et al. (2005)" startWordPosition="3713" endWordPosition="3716">hould also note that our framework could be used in conjunction with existing methods for semi-supervised learning to improve the performance of the CRF tagger, which in turn will improve the coverage. It is also possible to improve the performance of the tagger by using external dictionaries or using more sophisticated probabilistic models such as semi-Markov CRFs (Sarawagi and Cohen, 2004). These enhancements should further improve the coverage, keeping the same degree of cost reduction. The idea of improving the efficiency of annotation work by using automatic taggers is certainly not new. Tanabe et al. (2005) applied a gene/protein name tagger to the target sentences and modified the results manually. Culotta and McCallum (2005) proposed to have the human annotator select the correct annotation from multiple choices produced by a CRF tagger for each sentence. Tomanek et al. (2007) discuss the reusability of named entityannotated corpora created by an active learning approach and show that it is possible to build a corpus that is useful to different machine learning algorithms to a certain degree. The limitation of our framework is that it is useful only when the target named entities are sparse be</context>
</contexts>
<marker>Tanabe, Xie, Thom, Matten, Wilbur, 2005</marker>
<rawString>Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne Matten, and W. John Wilbur. 2005. GENETAG: a tagged corpus for gene/protein named entity recognition. BMCBioinformatics, 6(Suppl 1):S3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>406--414</pages>
<contexts>
<context position="2698" citStr="Thompson et al., 1999" startWordPosition="405" endWordPosition="408">plications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. Active learning has been shown to be effective in several natural language processing tasks including named entity recognition. The problem with active learning is, however, that the resulting annotated data is highly dependent on the machine learning algorithm and the sampling strategy employed, because active learning annotates only a subset of the given corpus. This</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Cynthia A. Thompson, Mary Elaine Califf, and Raymond J. Mooney. 1999. Active learning for natural language parsing and information extraction. In Proceedings ofICML, pages 406–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>142--147</pages>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of CoNLL-2003, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<title>An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>486--495</pages>
<contexts>
<context position="22367" citStr="Tomanek et al. (2007)" startWordPosition="3756" endWordPosition="3759">rnal dictionaries or using more sophisticated probabilistic models such as semi-Markov CRFs (Sarawagi and Cohen, 2004). These enhancements should further improve the coverage, keeping the same degree of cost reduction. The idea of improving the efficiency of annotation work by using automatic taggers is certainly not new. Tanabe et al. (2005) applied a gene/protein name tagger to the target sentences and modified the results manually. Culotta and McCallum (2005) proposed to have the human annotator select the correct annotation from multiple choices produced by a CRF tagger for each sentence. Tomanek et al. (2007) discuss the reusability of named entityannotated corpora created by an active learning approach and show that it is possible to build a corpus that is useful to different machine learning algorithms to a certain degree. The limitation of our framework is that it is useful only when the target named entities are sparse because the upper bound of cost saving is limited by the proportion of the relevant sentences in the corpus. Our framework may therefore not be suitable for a situation where one wants to make annotations for named entities of many categories simultaneously (e.g. creating a corp</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007. An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data. In Proceedings of EMNLP-CoNLL, pages 486–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Caroline Gasperin</author>
</authors>
<title>Bootstrapping and evaluating named entity recognition in the biomedical domain.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLTNAACL BioNLP Workshop on Linking Natural Language and Biology,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="21271" citStr="Vlachos and Gasperin, 2006" startWordPosition="3576" endWordPosition="3579">efinition, not something that can ensure a coverage of 100%. The seriousness of a missing rate of, for example, 1% is not entirely clear—it depends on the application and the purpose of annotation. In general, however, it is hard to achieve a coverage of 100% in real annotation work even if the human annotator scans through all sentences, because there is often ambiguity in deciding whether a particular named entity should be annotated or not. Previous studies report that inter-annotator agreement rates with regards to gene/protein name annotation are f-scores around 90% (Morgan et al., 2004; Vlachos and Gasperin, 2006). We believe that the missing rate of 1% can be an acceptable level of sacrifice, given the cost reduction achieved and the unavoidable discrepancy made by the human annotator. At the same time, we should also note that our framework could be used in conjunction with existing methods for semi-supervised learning to improve the performance of the CRF tagger, which in turn will improve the coverage. It is also possible to improve the performance of the tagger by using external dictionaries or using more sophisticated probabilistic models such as semi-Markov CRFs (Sarawagi and Cohen, 2004). These</context>
</contexts>
<marker>Vlachos, Gasperin, 2006</marker>
<rawString>Andreas Vlachos and Caroline Gasperin. 2006. Bootstrapping and evaluating named entity recognition in the biomedical domain. In Proceedings of the HLTNAACL BioNLP Workshop on Linking Natural Language and Biology, pages 138–145.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>