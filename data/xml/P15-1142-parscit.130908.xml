<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.986708">
Compositional Semantic Parsing on Semi-Structured Tables
</title>
<author confidence="0.998254">
Panupong Pasupat Percy Liang
</author>
<affiliation confidence="0.9993555">
Computer Science Department Computer Science Department
Stanford University Stanford University
</affiliation>
<email confidence="0.996753">
ppasupat@cs.stanford.edu pliang@cs.stanford.edu
</email>
<sectionHeader confidence="0.993842" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968863636364">
Two important aspects of semantic pars-
ing for question answering are the breadth
of the knowledge source and the depth of
logical compositionality. While existing
work trades off one aspect for another, this
paper simultaneously makes progress on
both fronts through a new task: answering
complex questions on semi-structured ta-
bles using question-answer pairs as super-
vision. The central challenge arises from
two compounding factors: the broader do-
main results in an open-ended set of re-
lations, and the deeper compositionality
results in a combinatorial explosion in
the space of logical forms. We propose
a logical-form driven parsing algorithm
guided by strong typing constraints and
show that it obtains significant improve-
ments over natural baselines. For evalua-
tion, we created a new dataset of 22,033
complex questions on Wikipedia tables,
which is made publicly available.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9991834">
In semantic parsing for question answering, nat-
ural language questions are converted into logi-
cal forms, which can be executed on a knowl-
edge source to obtain answer denotations. Early
semantic parsing systems were trained to answer
highly compositional questions, but the knowl-
edge sources were limited to small closed-domain
databases (Zelle and Mooney, 1996; Wong and
Mooney, 2007; Zettlemoyer and Collins, 2007;
Kwiatkowski et al., 2011). More recent work
sacrifices compositionality in favor of using more
open-ended knowledge bases such as Freebase
(Cai and Yates, 2013; Berant et al., 2013; Fader
et al., 2014; Reddy et al., 2014). However, even
these broader knowledge sources still define a
</bodyText>
<table confidence="0.985283375">
Year City Country Nations
1896 Athens Greece 14
1900 Paris France 24
1904 St. Louis USA 12
. . . . . . . . . . . .
2004 Athens Greece 201
2008 Beijing China 204
2012 London UK 204
</table>
<figure confidence="0.753465727272727">
x1: “Greece held its last Summer Olympics in which year?”
y1: {2004}
x2: “In which city’s the first time with at least 20 nations?”
y2: {Paris}
x3: “Which years have the most participating countries?”
y3: {2008, 2012}
x4: “How many events were in Athens, Greece?”
y4: {2}
x5: “How many more participants were there in 1900 than
in the first year?”
y5: {10}
</figure>
<figureCaption confidence="0.98692575">
Figure 1: Our task is to answer a highly composi-
tional question from an HTML table. We learn
a semantic parser from question-table-answer
triples {(xil til yi)}.
</figureCaption>
<bodyText confidence="0.99973755">
rigid schema over entities and relation types, thus
restricting the scope of answerable questions.
To simultaneously increase both the breadth of
the knowledge source and the depth of logical
compositionality, we propose a new task (with an
associated dataset): answering a question using an
HTML table as the knowledge source. Figure 1
shows several question-answer pairs and an ac-
companying table, which are typical of those in
our dataset. Note that the questions are logically
quite complex, involving a variety of operations
such as comparison (x2), superlatives (x3), aggre-
gation (x4), and arithmetic (x5).
The HTML tables are semi-structured and not
normalized. For example, a cell might contain
multiple parts (e.g., “Beijing, China” or “200
km”). Additionally, we mandate that the train-
ing and test tables are disjoint, so at test time,
we will see relations (column headers; e.g., “Na-
tions”) and entities (table cells; e.g., “St. Louis”)
</bodyText>
<page confidence="0.914426">
1470
</page>
<note confidence="0.975313666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1470–1480,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999950085714286">
that were not observed during training. This is in
contrast to knowledge bases like Freebase, which
have a global fixed relation schema with normal-
ized entities and relations.
Our task setting produces two main challenges.
Firstly, the increased breadth in the knowledge
source requires us to generate logical forms from
novel tables with previously unseen relations and
entities. We therefore cannot follow the typical
semantic parsing strategy of constructing or learn-
ing a lexicon that maps phrases to relations ahead
of time. Secondly, the increased depth in com-
positionality and additional logical operations ex-
acerbate the exponential growth of the number of
possible logical forms.
We trained a semantic parser for this task from
question-answer pairs based on the framework il-
lustrated in Figure 2. First, relations and entities
from the semi-structured HTML table are encoded
in a graph. Then, the system parses the question
into candidate logical forms with a high-coverage
grammar, reranks the candidates with a log-linear
model, and then executes the highest-scoring logi-
cal form to produce the answer denotation. We use
beam search with pruning strategies based on type
and denotation constraints to control the combina-
torial explosion.
To evaluate the system, we created a new
dataset, WIKITABLEQUESTIONS, consisting of
2,108 HTML tables from Wikipedia and 22,033
question-answer pairs. When tested on unseen ta-
bles, the system achieves an accuracy of 37.1%,
which is significantly higher than the information
retrieval baseline of 12.7% and a simple semantic
parsing baseline of 24.3%.
</bodyText>
<sectionHeader confidence="0.992699" genericHeader="introduction">
2 Task
</sectionHeader>
<bodyText confidence="0.9719218">
Our task is as follows: given a table t and a ques-
tion x about the table, output a list of values y
that answers the question according to the table.
Example inputs and outputs are shown in Fig-
ure 1. The system has access to a training set
D = {(xi, ti, yi)}Ni=1 of questions, tables, and an-
swers, but the tables in test data do not appear dur-
ing training.
The only restriction on the question x is that a
person must be able to answer it using just the ta-
ble t. Other than that, the question can be of any
type, ranging from a simple table lookup question
to a more complicated one that involves various
logical operations.
A[Year ... ].argmax(... Greece, Index) {2004}
</bodyText>
<figureCaption confidence="0.87848">
Figure 2: The prediction framework: (1) the table
</figureCaption>
<bodyText confidence="0.9910130625">
t is deterministically converted into a knowledge
graph w as shown in Figure 3; (2) with informa-
tion from w, the question x is parsed into candi-
date logical forms in ix; (3) the highest-scoring
candidate z E ix is chosen; and (4) z is executed
on w, yielding the answer y.
Dataset. We created a new dataset, WIK-
ITABLEQUESTIONS, of question-answer pairs on
HTML tables as follows. We randomly selected
data tables from Wikipedia with at least 8 rows and
5 columns. We then created two Amazon Mechan-
ical Turk tasks. The first task asks workers to write
trivia questions about the table. For each question,
we put one of the 36 generic prompts such as “The
question should require calculation” or “contains
the word ‘first’ or its synonym” to encourage more
complex utterances. Next, we submit the result-
ing questions to the second task where the work-
ers answer each question based on the given table.
We only keep the answers that are agreed upon by
at least two workers. After this filtering, approxi-
mately 69% of the questions remains.
The final dataset contains 22,033 examples on
2,108 tables. We set aside 20% of the tables and
their associated questions as the test set and de-
velop on the remaining examples. Simple pre-
processing was done on the tables: We omit all
non-textual contents of the tables, and if there is a
merged cell spanning many rows or columns, we
unmerge it and duplicate its content into each un-
merged cell. Section 7.2 analyzes various aspects
of the dataset and compares it to other datasets.
</bodyText>
<sectionHeader confidence="0.996287" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999778666666667">
We now describe our semantic parsing framework
for answering a given question and for training the
model with question-answer pairs.
</bodyText>
<figure confidence="0.972078357142858">
Greece held the last
Summer Olympics in
which year?
X
(2) Parsing
Z.
W
(3) Ranking
(4) Execution
Z
Y
xxx xxxx xxxxx xxxxx
xx
xx
xx
...
xxx
xxx
xxx
xxxxxx
xxxxx
xxxxxx
...
xxxxxx
xxxxxxx
xxxxxx
xxxxxx
xxxxxx
xxx
...
xxxxxx
xxxxx
xxx
xxxx
xxxx
xxxx
..
xxxx
xxxx
xxxx
(1) Conversion
t
</figure>
<page confidence="0.991584">
1471
</page>
<bodyText confidence="0.993676076923077">
Prediction. Given a table t and a question x,
we predict an answer y using the framework il-
lustrated in Figure 2. We first convert the table
t into a knowledge graph w (“world”) which en-
codes different relations in the table (Section 4).
Next, we generate a set of candidate logical forms
ix by parsing the question x using the informa-
tion from w (Section 6.1). Each generated logical
form z E ix is a graph query that can be exe-
cuted on the knowledge graph w to get a denota-
tion Qz�w. We extract a feature vector φ(x, w, z)
for each z E ix (Section 6.2) and define a log-
linear distribution over the candidates:
</bodyText>
<equation confidence="0.966024">
pθ(z  |x, w) a exp{θTφ(x, w, z)}, (1)
</equation>
<bodyText confidence="0.9972479">
where θ is the parameter vector. Finally, we
choose the logical form z with the highest model
probability and execute it on w to get the answer
denotation y = Hw.
Training. Given training examples D =
{(xi, ti, yi)}Ni=1, we seek a parameter vector θ
that maximizes the regularized log-likelihood of
the correct denotation yi marginalized over logi-
cal forms z. Formally, we maximize the objective
function
</bodyText>
<equation confidence="0.999699">
N log pθ(yi  |xi, wi) − λ 11θ111 , (2)
1
J(θ) =
N
i=1
</equation>
<bodyText confidence="0.997939">
where wi is deterministically generated from ti,
and
</bodyText>
<equation confidence="0.976282">
pθ(y  |x, w) = � pθ(z  |x, w). (3)
</equation>
<bodyText confidence="0.941073285714286">
zEZx;y=Qzlw
We optimize θ using AdaGrad (Duchi et al.,
2010), running 3 passes over the data. We use L1
regularization with λ = 3 x 10−5 obtained from
cross-validation.
The following sections explain individual sys-
tem components in more detail.
</bodyText>
<sectionHeader confidence="0.984108" genericHeader="method">
4 Knowledge graph
</sectionHeader>
<bodyText confidence="0.999689857142857">
Inspired by the graph representation of knowledge
bases, we preprocess the table t by deterministi-
cally converting it into a knowledge graph w as
illustrated in Figure 3. In the most basic form, ta-
ble rows become row nodes, strings in table cells
become entity nodes,1 and table columns become
directed edges from the row nodes to the entity
</bodyText>
<footnote confidence="0.886802">
1Two occurrences of the same string constitute one node.
</footnote>
<figureCaption confidence="0.97154">
Figure 3: Part of the knowledge graph correspond-
</figureCaption>
<bodyText confidence="0.980641487804878">
ing to the table in Figure 1. Circular nodes are row
nodes. We augment the graph with different en-
tity normalization nodes such as Number and Date
(red) and additional row node relations Next and
Index (blue).
nodes of that column. The column headers are
used as edge labels for these row-entity relations.
The knowledge graph representation is conve-
nient for three reasons. First, we can encode dif-
ferent forms of entity normalization in the graph.
Some entity strings (e.g., “1900”) can be inter-
preted as a number, a date, or a proper name de-
pending on the context, while some other strings
(e.g., “200 km”) have multiple parts. Instead of
committing to one normalization scheme, we in-
troduce edges corresponding to different normal-
ization methods from the entity nodes. For exam-
ple, the node 1900 will have an edge called Date
to another node 1900-XX-XX of type date. Apart
from type checking, these normalization nodes
also aid learning by providing signals on the ap-
propriate answer type. For instance, we can define
a feature that associates the phrase “how many”
with a logical form that says “traverse a row-entity
edge, then a Number edge” instead of just “traverse
a row-entity edge.”
The second benefit of the graph representation
is its ability to handle various logical phenomena
via graph augmentation. For example, to answer
questions of the form “What is the next ... ?” or
“Who came before ... ?”, we augment each row
node with an edge labeled Next pointing to the
next row node, after which the questions can be
answered by traversing the Next edge. In this
work, we choose to add two special edges on each
row node: the Next edge mentioned above and
an Index edge pointing to the row index number
(0,1,2,... ).
Finally, with a graph representation, we can
query it directly using a logical formalism for
knowledge graphs, which we turn to next.
</bodyText>
<figure confidence="0.975061636363636">
· · ·
Year City Country
Index
0
Next
1896
Athens
Greece
· · ·
Year City Country
Index
1
Next
Paris
France
1900
Number Date
1900.0 1900-XX-XX
1472
Name Example
Join City.Athens
(row nodes with a City edge to Athens)
Union City.(Athens U Beijing)
Intersection City.Athens fl Year.Number.&lt;.1990
Reverse R[Year].City.Athens
(entities where a row in City.Athens has a Year edge to)
Aggregation count(City.Athens)
(the number of rows with city Athens)
Superlative argmax(City.Athens, Index)
(the last row with city Athens)
Arithmetic sub(204, 201) (= 204 − 201)
Lambda Ax[Year.Date.x]
(a binary: composition of two relations)
</figure>
<tableCaption confidence="0.994597">
Table 1: The lambda DCS operations we use.
</tableCaption>
<sectionHeader confidence="0.991527" genericHeader="method">
5 Logical forms
</sectionHeader>
<bodyText confidence="0.999987333333333">
As our language for logical forms, we use
lambda dependency-based compositional seman-
tics (Liang, 2013), or lambda DCS, which we
briefly describe here. Each lambda DCS logical
form is either a unary (denoting a list of values) or
a binary (denoting a list of pairs). The most basic
unaries are singletons (e.g., China represents an
entity node, and 30 represents a single number),
while the most basic binaries are relations (e.g.,
City maps rows to city entities, Next maps rows
to rows, and &gt;= maps numbers to numbers). Log-
ical forms can be combined into larger ones via
various operations listed in Table 1. Each opera-
tion produces a unary except lambda abstraction:
λx[f(x)] is a binary mapping x to f(x).
</bodyText>
<sectionHeader confidence="0.913441" genericHeader="method">
6 Parsing and ranking
</sectionHeader>
<bodyText confidence="0.999288666666667">
Given the knowledge graph w, we now describe
how to parse the utterance x into a set of candidate
logical forms ix
</bodyText>
<subsectionHeader confidence="0.99984">
6.1 Parsing algorithm
</subsectionHeader>
<bodyText confidence="0.999860818181818">
We propose a new floating parser which is more
flexible than a standard chart parser. Both parsers
recursively build up derivations and corresponding
logical forms by repeatedly applying deduction
rules, but the floating parser allows logical form
predicates to be generated independently from the
utterance.
Chart parser. We briefly review the CKY al-
gorithm for chart parsing to introduce notation.
Given an utterance with tokens x1,... , xn, the
CKY algorithm applies deduction rules of the fol-
</bodyText>
<subsectionHeader confidence="0.970333">
Rule Semantics Example
</subsectionHeader>
<bodyText confidence="0.870719">
Anchored to the utterance
</bodyText>
<equation confidence="0.986811727272727">
TokenSpan --+ Entity match(zi) Greece
(match(s) =entity with name s) anchored to “Greece”
TokenSpan --+ Atomic val(zl) 2012-07-XX
(val(s) = interpreted value) anchored to “July 2012”
Unanchored (loating)
0 --+ Relation r Country
(r = row-entity relation)
0 --+ Relation Ax[r.p.x] Ax[Year.Date.x]
(p = normalization relation)
0 --+ Records Type.Row (list of all rows)
0 --+ RecordFn Index (row +— row index)
</equation>
<bodyText confidence="0.77055275">
Table 2: Base deduction rules. Entities and atomic
values (e.g., numbers, dates) are anchored to to-
ken spans, while other predicates are kept floating.
(a +— b represents a binary mapping b to a.)
</bodyText>
<equation confidence="0.93777775">
lowing two kinds:
(TokenSpan, i, j)[s] — (c, i,j)[f(s)], (4)
(c1, i, k)[z1] + (c2, k + 1, j)[z2] (5)
— (c, i,j)[f(z1,z2)].
</equation>
<bodyText confidence="0.999094333333334">
The first rule is a lexical rule that matches an utter-
ance token span xi · · · xj (e.g., s = “New York”)
and produces a logical form (e.g., f(s) =
NewYorkCity) with category c (e.g., Entity).
The second rule takes two adjacent spans giv-
ing rise to logical forms z1 and z2 and builds a
new logical form f(z1, z2). Algorithmically, CKY
stores derivations of category c covering the span
xi · · · xj in a cell (c, i, j). CKY fills in the cells of
increasing span lengths, and the logical forms in
the top cell (ROOT, 1, n) are returned.
Floating parser. Chart parsing uses lexical
rules (4) to generate relevant logical predicates,
but in our setting of semantic parsing on tables,
we do not have the luxury of starting with or
inducing a full-fledged lexicon. Moreover, there
is a mismatch between words in the utterance
and predicates in the logical form. For in-
stance, consider the question “Greece held its
last Summer Olympics in which year?” on the
table in Figure 1 and the correct logical form
R[Ax[Year.Date.x]].argmax(Country.Greece, Index).
While the entity Greece can be anchored to the
token “Greece”, some logical predicates (e.g.,
Country) cannot be clearly anchored to a token
span. We could potentially learn to anchor the
logical form Country.Greece to “Greece”, but if
the relation Country is not seen during training,
such a mapping is impossible to learn from the
training data. Similarly, some prominent tokens
</bodyText>
<page confidence="0.948251">
1473
</page>
<figure confidence="0.972630192307692">
Rule Semantics Example
Join + Aggregate
Entity or Atomic → Values z1 China
Atomic → Values c.z1 &gt;=.30 (at least 30)
(c ∈ {&lt;, &gt;, &lt;=, &gt;=})
Relation + Values → Records z1.z2 Country.China (events (rows) where the country is China)
Relation + Records → Values R[z1].z2 R[Year].Country.China (years of events in China)
Records → Records Next.z1 Next.Country.China (... before China)
Records → Records R[Next].z1 R[Next].Country.China (... after China)
Values → Atomic a(z1) count(Country.China) (How often did China ... )
(a ∈ {count, max, min, sum, avg})
Values → ROOT z1
Superlative
Relation → RecordFn z1 Ax[Nations.Number.x] (row ← value in Nations column)
Records + RecordFn → Records s(z1, z2) argmax(Type.Row, Ax[Nations.Number.x])
(s ∈ {argmax, argmin}) (events with the most participating nations)
argmin(City.Athens, Index) (first event in Athens)
Relation → ValueFn R[Ax[a(z1.x)]] R[Ax[count(City.x)]] (city ← num. of rows with that city)
Relation +Relation → ValueFn Ax[R[z1].z2.x] Ax[R[City].Nations.Number.x]
(city ← value in Nations column)
Values + ValueFn → Values s(z1, z2) argmax(... , R[Ax[count(City.x)]]) (most frequent city)
Other operations
ValueFn + Values + Values → Values o(R[z1].z2, R[z1].z3) sub(R[Number].R[Nations].City.London, ... )
(o ∈ {add, sub, mul, div}) (How many more participants were in London than ... )
Entity + Entity → Values z1 t z2 China t France (China or France)
Records + Records → Records z1 u z2 City.Beijing u Country.China (... in Beijing, China)
</figure>
<tableCaption confidence="0.8785775">
Table 3: Compositional deduction rules. Each rule c1, ... , ck —* c takes logical forms z1, ... , zk con-
structed over categories c1, ... , ck, respectively, and produces a logical form based on the semantics.
</tableCaption>
<bodyText confidence="0.99212525">
(e.g., “Olympics”) are irrelevant and have no
predicates anchored to them.
Therefore, instead of anchoring each predicate
in the logical form to tokens in the utterance via
lexical rules, we propose parsing more freely. We
replace the anchored cells (c, i, j) with floating
cells (c, s) of category c and logical form size s.
Then we apply rules of the following three kinds:
</bodyText>
<equation confidence="0.99963825">
(TokenSpan, i, j)[s] —* (c, 1)[f(s)], (6)
0 —* (c,1)[f()], (7)
(c1,s1)[z1] + (c2,s2)[z2] (8)
—* (c, s1 + s2 + 1)[f(z1, z2)].
</equation>
<bodyText confidence="0.9998592">
Note that rules (6) are similar to (4) in chart
parsing except that the floating cell (c, 1) only
keeps track of the category and its size 1, not
the span (i, j). Rules (7) allow us to construct
predicates out of thin air. For example, we can
construct a logical form representing a table rela-
tion Country in cell (Relation, 1) using the rule
0 —* Relation [Country] independent of the ut-
terance. Rules (8) perform composition, where
the induction is on the size s of the logical form
rather than the span length. The algorithm stops
when the specified maximum size is reached, after
which the logical forms in cells (ROOT, s) for any
s are included in ix. Figure 4 shows an example
derivation generated by our floating parser.
</bodyText>
<equation confidence="0.9951705">
(Values, 8)
R[Ax[Year.Date.x]].argmax(Country.Greece, Index)
(Relation, 1) (Records, 6)
Ax[Year.Date.x] argmax(Country.Greece,Index)
(Records, 4) (RecordFn, 1)
Country.Greece Index
(Relation, 1) (Values, 2)
Country Greece
(Entity, 1)
Greece
(TokenSpan, 1, 1)
“Greece”
</equation>
<figureCaption confidence="0.992371">
Figure 4: A derivation for the utterance “Greece
</figureCaption>
<bodyText confidence="0.952578090909091">
held its last Summer Olympics in which year?”
Only Greece is anchored to a phrase “Greece”;
Year and other predicates are floating.
The floating parser is very flexible: it can skip
tokens and combine logical forms in any order.
This flexibility might seem too unconstrained, but
we can use strong typing constraints to prevent
nonsensical derivations from being constructed.
Tables 2 and 3 show the full set of deduction
rules we use. We assume that all named entities
will explicitly appear in the question x, so we an-
</bodyText>
<page confidence="0.868027">
1474
</page>
<table confidence="0.183990444444444">
“Greece held its last Summer Olympics in which year?”
z = R[Ax[Year.Number.x]].argmax(Type.Row, Index)
y = {2012} (type: NUM, column: YEAR)
Feature Name Note
(“last”, predicate = argmax) lex
phrase = predicate unlex (. “year” = Year)
missing entity unlex (. missing Greece)
denotation type = NUM
denotation column = YEAR
</table>
<equation confidence="0.9193916">
(“which year”, type = NUM) lex
phrase = column unlex (. “year” = YEAR)
(Q = “which”, type = NUM) lex
(H = “year”, type = NUM) lex
H = column unlex (. “year” = YEAR)
</equation>
<bodyText confidence="0.946965692307692">
Table 4: Example features that fire for the (incor-
rect) logical form z. All features are binary. (lex =
lexicalized)
chor all entity predicates (e.g., Greece) to token
spans (e.g., “Greece”). We also anchor all numer-
ical values (numbers, dates, percentages, etc.) de-
tected by an NER system. In contrast, relations
(e.g., Country) and operations (e.g., argmax) are
kept floating since we want to learn how they
are expressed in language. Connections between
phrases in x and the generated relations and op-
erations in z are established in the ranking model
through features.
</bodyText>
<subsectionHeader confidence="0.973278">
6.2 Features
</subsectionHeader>
<bodyText confidence="0.996387275862069">
We define features O(x, w, z) for our log-linear
model to capture the relationship between the
question x and the candidate z. Table 4 shows
some example features from each feature type.
Most features are of the form (f(x), g(z)) or
(f(x), h(y)) where y = Qz�w is the denotation,
and f, g, and h extract some information (e.g.,
identity, POS tags) from x, z, or y, respectively.
phrase-predicate: Conjunctions between n-
grams f(x) from x and predicates g(z) from z.
We use both lexicalized features, where all possi-
ble pairs (f(x), g(z)) form distinct features, and
binary unlexicalized features indicating whether
f(x) and g(z) have a string match.
missing-predicate: Indicators on whether there
are entities or relations mentioned in x but not in
z. These features are unlexicalized.
denotation: Size and type of the denotation
y = Hw. The type can be either a primitive type
(e.g., NUM, DATE, ENTITY) or the name of the
column containing the entity in y (e.g., CITY).
phrase-denotation: Conjunctions between n-
grams from x and the types of y. Similar to the
phrase-predicate features, we use both lexicalized
and unlexicalized features.
headword-denotation: Conjunctions between
the question word Q (e.g., what, who, how many)
or the headword H (the first noun after the ques-
tion word) with the types of y.
</bodyText>
<subsectionHeader confidence="0.9976">
6.3 Generation and pruning
</subsectionHeader>
<bodyText confidence="0.999808636363636">
Due to their recursive nature, the rules allow us
to generate highly compositional logical forms.
However, the compositionality comes at the cost
of generating exponentially many logical forms,
most of which are redundant (e.g., logical forms
with an argmax operation on a set of size 1). We
employ several methods to deal with this combi-
natorial explosion:
Beam search. We compute the model proba-
bility of each partial logical form based on avail-
able features (i.e., features that do not depend on
the final denotation) and keep only the K = 200
highest-scoring logical forms in each cell.
Pruning. We prune partial logical forms that
lead to invalid or redundant final logical forms.
For example, we eliminate any logical form that
does not type check (e.g., Beijing U Greece),
executes to an empty list (e.g., Year.Number.24),
includes an aggregate or superlative on a singleton
set (e.g., argmax(Year.Number.2012, Index)), or
joins two relations that are the reverses of each
other (e.g., R[City].City.Beijing).
</bodyText>
<sectionHeader confidence="0.999174" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995565">
7.1 Main evaluation
</subsectionHeader>
<bodyText confidence="0.999917631578947">
We evaluate the system on the development sets
(three random 80:20 splits of the training data) and
the test data. In both settings, the tables we test on
do not appear during training.
Evaluation metrics. Our main metric is accu-
racy, which is the number of examples (x, t, y)
on which the system outputs the correct answer y.
We also report the oracle score, which counts the
number of examples where at least one generated
candidate z ∈ Zx executes to y.
Baselines. We compare the system to two base-
lines. The first baseline (IR), which simulates in-
formation retrieval, selects an answer y among the
entities in the table using a log-linear model over
entities (table cells) rather than logical forms. The
features are conjunctions between phrases in x and
properties of the answers y, which cover all fea-
tures in our main system that do not involve the
logical form. As an upper bound of this baseline,
</bodyText>
<page confidence="0.975386">
1475
</page>
<table confidence="0.9976442">
dev test
acc ora acc ora
IR baseline 13.4 69.1 12.7 70.6
WQ baseline 23.6 34.4 24.3 35.6
Our system 37.0 76.7 37.1 76.6
</table>
<tableCaption confidence="0.996392">
Table 5: Accuracy (acc) and oracle scores (ora)
</tableCaption>
<bodyText confidence="0.734119">
on the development sets (3 random splits of the
training data) and the test data.
</bodyText>
<figure confidence="0.959750111111111">
acc ora
Our system 37.0 76.7
(a) Rule Ablation
join only 10.6 15.7
join + count (= WQ baseline) 23.6 34.4
join + count + superlative 30.7 68.6
all − {n, u} 34.8 75.1
(b) Feature Ablation
all − features involving predicate 11.8 74.5
all − phrase-predicate 16.9 74.5
all − lex phrase-predicate 17.6 75.9
all − unlex phrase-predicate 34.3 76.7
all − missing-predicate 35.9 76.7
all − features involving denotation 33.5 76.8
all − denotation 34.3 76.6
all − phrase-denotation 35.7 76.8
all − headword-denotation 36.0 76.7
(c) Anchor operations to trigger words 37.1 59.4
</figure>
<tableCaption confidence="0.8714925">
Table 6: Average accuracy and oracle scores on
development data in various system settings.
</tableCaption>
<bodyText confidence="0.998915733333333">
69.1% of the development examples have the an-
swer appearing as an entity in the table.
In the second baseline (WQ), we only allow de-
duction rules that produce join and count logical
forms. This rule subset has the same logical cov-
erage as Berant and Liang (2014), which is de-
signed to handle the WEBQUESTIONS (Berant et
al., 2013) and FREE917 (Cai and Yates, 2013)
datasets.
Results. Table 5 shows the results compared
to the baselines. Our system gets an accuracy
of 37.1% on the test data, which is significantly
higher than both baselines, while the oracle is
76.6%. The next subsections analyze the system
components in more detail.
</bodyText>
<subsectionHeader confidence="0.993201">
7.2 Dataset statistics
</subsectionHeader>
<bodyText confidence="0.999815875">
In this section, we analyze the breadth and depth
of the WIKITABLEQUESTIONS dataset, and how
the system handles them.
Number of relations. With 3,929 unique col-
umn headers (relations) among 13,396 columns,
the tables in the WIKITABLEQUESTIONS dataset
contain many more relations than closed-domain
datasets such as Geoquery (Zelle and Mooney,
</bodyText>
<table confidence="0.999685714285714">
Operation Amount
join (table lookup) 13.5%
+ join with Next + 5.5%
+ aggregate (count, sum, max, ... ) + 15.0%
+ superlative (argmax, argmin) + 24.5%
+ arithmetic, n, u + 20.5%
+ other phenomena + 21.0%
</table>
<tableCaption confidence="0.9204885">
Table 7: The logical operations required to answer
the questions in 200 random examples.
</tableCaption>
<bodyText confidence="0.996164142857143">
1996) and ATIS (Price, 1990). Additionally, the
logical forms that execute to the correct denota-
tions refer to a total of 2,056 unique column head-
ers, which is greater than the number of relations
in the FREE917 dataset (635 Freebase relations).
Knowledge coverage. We sampled 50 exam-
ples from the dataset and tried to answer them
manually using Freebase. Even though Free-
base contains some information extracted from
Wikipedia, we can answer only 20% of the ques-
tions, indicating that WIKITABLEQUESTIONS
contains a broad set of facts beyond Freebase.
Logical operation coverage. The dataset cov-
ers a wide range of question types and logical
operations. Table 6(a) shows the drop in oracle
scores when different subsets of rules are used to
generate candidates logical forms. The join only
subset corresponds to simple table lookup, while
join + count is the WQ baseline for Freebase ques-
tion answering on the WEBQUESTIONS dataset.
Finally, join + count + superlative roughly corre-
sponds to the coverage of the Geoquery dataset.
To better understand the distribution of log-
ical operations in the WIKITABLEQUESTIONS
dataset, we manually classified 200 examples
based on the types of operations required to an-
swer the question. The statistics in Table 7 shows
that while a few questions only require simple
operations such as table lookup, the majority of
the questions demands more advanced operations.
Additionally, 21% of the examples cannot be an-
swered using any logical form generated from the
current deduction rules; these examples are dis-
cussed in Section 7.4.
Compositionality. From each example, we
compute the logical form size (number of rules
applied) of the highest-scoring candidate that exe-
cutes to the correct denotation. The histogram in
Figure 5 shows that a significant number of logical
forms are non-trivial.
Beam size and pruning. Figure 6 shows the
results with and without pruning on various beam
</bodyText>
<page confidence="0.985962">
1476
</page>
<figure confidence="0.996372375">
2500
2000
1500
1000
500
0
2 3 4 5 6 7 8 9 10 11
formula size
</figure>
<figureCaption confidence="0.9902665">
Figure 5: Sizes of the highest-scoring correct can-
didate logical forms in development examples.
</figureCaption>
<figure confidence="0.943253666666667">
with pruning without pruning
0 25 50 75 100
beam size
</figure>
<figureCaption confidence="0.952104">
Figure 6: Accuracy (solid red) and oracle (dashed
blue) scores with different beam sizes.
</figureCaption>
<bodyText confidence="0.996852">
sizes. Apart from saving time, pruning also pre-
vents bad logical forms from clogging up the beam
which hurts both oracle and accuracy metrics.
</bodyText>
<subsectionHeader confidence="0.955127">
7.3 Features
</subsectionHeader>
<bodyText confidence="0.999619826086957">
Effect of features. Table 6(b) shows the accu-
racy when some feature types are ablated. The
most influential features are lexicalized phrase-
predicate features, which capture the relationship
between phrases and logical operations (e.g., relat-
ing “last” to argmax) as well as between phrases
and relations (e.g., relating “before” to &lt; or Next,
and relating “who” to the relation Name).
Anchoring with trigger words. In our parsing
algorithm, relations and logical operations are not
anchored to the utterance. We consider an alter-
native approach where logical operations are an-
chored to “trigger” phrases, which are hand-coded
based on co-occurrence statistics (e.g., we trigger
a count logical form with how, many, and total).
Table 6(c) shows that the trigger words do not
significantly impact the accuracy, suggesting that
the original system is already able to learn the re-
lationship between phrases and operations even
without a manual lexicon. As an aside, the huge
drop in oracle is because fewer “semantically in-
correct” logical forms are generated; we discuss
this phenomenon in the next subsection.
</bodyText>
<subsectionHeader confidence="0.99702">
7.4 Semantically correct logical forms
</subsectionHeader>
<bodyText confidence="0.995415243243243">
In our setting, we face a new challenge that arises
from learning with denotations: with deeper com-
positionality, a larger number of nonsensical log-
ical forms can execute to the correct denotation.
For example, if the target answer is a small num-
ber (say, 2), it is possible to count the number of
rows with some random properties and arrive at
the correct answer. However, as the system en-
counters more examples, it can potentially learn to
disfavor them by recognizing the characteristics of
semantically correct logical forms.
Generating semantically correct logical
forms. The system can learn the features of
semantically correct logical forms only if it can
generate them in the first place. To see how well
the system can generate correct logical forms,
looking at the oracle score is insufficient since
bad logical forms can execute to the correct
denotations. Instead, we randomly chose 200 ex-
amples and manually annotated them with logical
forms to see if a trained system can produce the
annotated logical form as a candidate.
Out of 200 examples, we find that 79% can
be manually annotated. The remaining ones in-
clude artifacts such as unhandled question types
(e.g., yes-no questions, or questions with phrases
“same” or “consecutive”), table cells that require
advanced normalization methods (e.g., cells with
comma-separated lists), and incorrect annotations.
The system generates the annotated logical
form among the candidates in 53.5% of the ex-
amples. The missing examples are mostly caused
by anchoring errors due to lexical mismatch (e.g.,
“Italian” → Italy, or “no zip code” → an empty
cell in the zip code column) or the need to generate
complex logical forms from a single phrase (e.g.,
“May 2010” → &gt;=.2010-05-01F1&lt;=.2010-05-31).
</bodyText>
<subsectionHeader confidence="0.994651">
7.5 Error analysis
</subsectionHeader>
<bodyText confidence="0.999923785714286">
The errors on the development data can be divided
into four groups. The first two groups are unhan-
dled question types (21%) and the failure to an-
chor entities (25%) as described in Section 7.4.
The third group is normalization and type errors
(29%): although we handle some forms of en-
tity normalization, we observe many unhandled
string formats such as times (e.g., 3:45.79) and
city-country pairs (e.g., Beijing, China), as well as
complex calculation such as computing time peri-
ods (e.g., 12pm–1am → 1 hour). Finally, we have
ranking errors (25%) which mostly occur when the
utterance phrase and the relation are obliquely re-
lated (e.g., “airplane” and Model).
</bodyText>
<figure confidence="0.997653">
frequency
80
score
60
40
20
0
80
0 25 50 75 100
beam size
score
60
40
20
0
</figure>
<page confidence="0.984241">
1477
</page>
<sectionHeader confidence="0.998325" genericHeader="conclusions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999975989583334">
Our work simultaneously increases the breadth of
knowledge source and the depth of compositional-
ity in semantic parsing. This section explores the
connections in both aspects to related work.
Logical coverage. Different semantic parsing
systems are designed to handle different sets of
logical operations and degrees of compositional-
ity. For example, form-filling systems (Wang et
al., 2011) usually cover a smaller scope of opera-
tions and compositionality, while early statistical
semantic parsers for question answering (Wong
and Mooney, 2007; Zettlemoyer and Collins,
2007) and high-accuracy natural language inter-
faces for databases (Androutsopoulos et al., 1995;
Popescu et al., 2003) target more compositional
utterances with a wide range of logical opera-
tions. This work aims to increase the logical
coverage even further. For example, compared
to the Geoquery dataset, the WIKITABLEQUES-
TIONS dataset includes a move diverse set of log-
ical operations, and while it does not have ex-
tremely compositional questions like in Geoquery
(e.g., “What states border states that border states
that border Florida?”), our dataset contains fairly
compositional questions on average.
To parse a compositional utterance, many works
rely on a lexicon that translates phrases to enti-
ties, relations, and logical operations. A lexicon
can be automatically generated (Unger and Cimi-
ano, 2011; Unger et al., 2012), learned from data
(Zettlemoyer and Collins, 2007; Kwiatkowski et
al., 2011), or extracted from external sources (Cai
and Yates, 2013; Berant et al., 2013), but requires
some techniques to generalize to unseen data. Our
work takes a different approach similar to the log-
ical form growing algorithm in Berant and Liang
(2014) by not anchoring relations and operations
to the utterance.
Knowledge domain. Recent works on seman-
tic parsing for question answering operate on more
open and diverse data domains. In particular,
large-scale knowledge bases have gained popular-
ity in the semantic parsing community (Cai and
Yates, 2013; Berant et al., 2013; Fader et al.,
2014). The increasing number of relations and en-
tities motivates new resources and techniques for
improving the accuracy, including the use of ontol-
ogy matching models (Kwiatkowski et al., 2013),
paraphrase models (Fader et al., 2013; Berant and
Liang, 2014), and unlabeled sentences (Krishna-
murthy and Kollar, 2013; Reddy et al., 2014).
Our work leverages open-ended data from the
Web through semi-structured tables. There have
been several studies on analyzing or inferring the
table schemas (Cafarella et al., 2008; Venetis et al.,
2011; Syed et al., 2010; Limaye et al., 2010) and
answering search queries by joining tables on sim-
ilar columns (Cafarella et al., 2008; Gonzalez et
al., 2010; Pimplikar and Sarawagi, 2012). While
the latter is similar to question answering, the
queries tend to be keyword lists instead of natural
language sentences. In parallel, open information
extraction (Wu and Weld, 2010; Masaum et al.,
2012) and knowledge base population (Ji and Gr-
ishman, 2011) extract information from web pages
and compile them into structured data. The result-
ing knowledge base is systematically organized,
but as a trade-off, some knowledge is inevitably
lost during extraction and the information is forced
to conform to a specific schema. To avoid these is-
sues, we choose to work on HTML tables directly.
In future work, we wish to draw informa-
tion from other semi-structured formats such as
colon-delimited pairs (Wong et al., 2009), bulleted
lists (Gupta and Sarawagi, 2009), and top-k lists
(Zhang et al., 2013). Pasupat and Liang (2014)
used a framework similar to ours to extract entities
from web pages, where the “logical forms” were
XPath expressions. A natural direction is to com-
bine the logical compositionality of this work with
the even broader knowledge source of general web
pages.
Acknowledgements. We gratefully acknowl-
edge the support of the Google Natural Language
Understanding Focused Program and the Defense
Advanced Research Projects Agency (DARPA)
Deep Exploration and Filtering of Text (DEFT)
Program under Air Force Research Laboratory
(AFRL) contract no. FA8750-13-2-0040.
Data and reproducibility. The WIKITABLE-
QUESTIONS dataset can be downloaded at http:
//nlp.stanford.edu/software/sempre/wikitable/.
Additionally, code, data, and experiments for
this paper are available on the CodaLab plat-
form at https://www.codalab.org/worksheets/
0xf26cd79d4d734287868923ad1067cf4c/.
</bodyText>
<sectionHeader confidence="0.996391" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9359375">
I. Androutsopoulos, G. D. Ritchie, and P. Thanisch.
1995. Natural language interfaces to databases –
</reference>
<page confidence="0.950872">
1478
</page>
<reference confidence="0.995779905660378">
an introduction. Journal of Natural Language En-
gineering, 1:29–81.
J. Berant and P. Liang. 2014. Semantic parsing via
paraphrasing. In Association for Computational
Linguistics (ACL).
J. Berant, A. Chou, R. Frostig, and P. Liang. 2013.
Semantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).
M. J. Cafarella, A. Halevy, D. Z. Wang, E. Wu, and
Y. Zhang. 2008. WebTables: exploring the power
of tables on the web. In Very Large Data Bases
(VLDB), pages 538–549.
Q. Cai and A. Yates. 2013. Large-scale semantic pars-
ing via schema matching and lexicon extension. In
Association for Computational Linguistics (ACL).
J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In Conference on Learning Theory
(COLT).
A. Fader, L. Zettlemoyer, and O. Etzioni. 2013.
Paraphrase-driven learning for open question an-
swering. In Association for Computational Linguis-
tics (ACL).
A. Fader, L. Zettlemoyer, and O. Etzioni. 2014.
Open question answering over curated and extracted
knowledge bases. In International Conference on
Knowledge Discovery and Data Mining (KDD),
pages 1156–1165.
H. Gonzalez, A. Y. Halevy, C. S. Jensen, A. Langen,
J. Madhavan, R. Shapley, W. Shen, and J. Goldberg-
Kidon. 2010. Google fusion tables: web-centered
data management and collaboration. In Proceedings
of the 2010 ACM SIGMOD International Confer-
ence on Management of data, pages 1061–1066.
R. Gupta and S. Sarawagi. 2009. Answering table
augmentation queries from unstructured lists on the
web. In Very Large Data Bases (VLDB), number 1,
pages 289–300.
H. Ji and R. Grishman. 2011. Knowledge base pop-
ulation: Successful approaches and challenges. In
Association for Computational Linguistics (ACL),
pages 1148–1158.
J. Krishnamurthy and T. Kollar. 2013. Jointly learning
to parse and perceive: Connecting natural language
to the physical world. Transactions of the Associa-
tion for Computational Linguistics (TACL), 1:193–
206.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical generalization in
CCG grammar induction for semantic parsing. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 1512–1523.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).
P. Liang. 2013. Lambda dependency-based composi-
tional semantics. arXiv.
G. Limaye, S. Sarawagi, and S. Chakrabarti. 2010.
Annotating and searching web tables using entities,
types and relationships. In Very Large Data Bases
(VLDB), volume 3, pages 1338–1347.
Masaum, M. Schmitz, R. Bart, S. Soderland, and O. Et-
zioni. 2012. Open language learning for informa-
tion extraction. In Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP/CoNLL), pages 523–
534.
P. Pasupat and P. Liang. 2014. Zero-shot entity extrac-
tion from web pages. In Association for Computa-
tional Linguistics (ACL).
R. Pimplikar and S. Sarawagi. 2012. Answering table
queries on the web using column keywords. In Very
Large Data Bases (VLDB), volume 5, pages 908–
919.
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards
a theory of natural language interfaces to databases.
In International Conference on Intelligent User In-
terfaces (IUI), pages 149–157.
P. Price. 1990. Evaluation of spoken language sys-
tems: The ATIS domain. In Proceedings of the
Third DARPA Speech and Natural Language Work-
shop, pages 91–95.
S. Reddy, M. Lapata, and M. Steedman. 2014. Large-
scale semantic parsing without question-answer
pairs. Transactions of the Association for Compu-
tational Linguistics (TACL), 2(10):377–392.
Z. Syed, T. Finin, V. Mulwad, and A. Joshi. 2010.
Exploiting a web of semantic data for interpreting
tables. In Proceedings of the Second Web Science
Conference.
C. Unger and P. Cimiano. 2011. Pythia: compositional
meaning construction for ontology-based question
answering on the semantic web. In Proceedings of
the 16th international conference on Natural lan-
guage processing and information systems, pages
153–160.
C. Unger, L. B¨uhmann, J. Lehmann, A. Ngonga,
D. Gerber, and P. Cimiano. 2012. Template-based
question answering over RDF data. In World Wide
Web (WWW), pages 639–648.
P. Venetis, A. Halevy, J. Madhavan, M. Pas¸ca, W. Shen,
F. Wu, G. Miao, and C. Wu. 2011. Recovering se-
mantics of tables on the web. In Very Large Data
Bases (VLDB), volume 4, pages 528–538.
</reference>
<page confidence="0.895056">
1479
</page>
<reference confidence="0.999778333333334">
Y. Wang, L. Deng, and A. Acero. 2011. Semantic
frame-based spoken language understanding. Spo-
ken Language Understanding: Systems for Extract-
ing Semantic Information from Speech, pages 41–91.
Y. W. Wong and R. J. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Association for Computational
Linguistics (ACL), pages 960–967.
Y. W. Wong, D. Widdows, T. Lokovic, and K. Nigam.
2009. Scalable attribute-value extraction from semi-
structured text. In IEEE International Conference
on Data Mining Workshops, pages 302–307.
F. Wu and D. S. Weld. 2010. Open information extrac-
tion using Wikipedia. In Association for Computa-
tional Linguistics (ACL), pages 118–127.
M. Zelle and R. J. Mooney. 1996. Learning to
parse database queries using inductive logic pro-
gramming. In Association for the Advancement of
Artificial Intelligence (AAAI), pages 1050–1055.
L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to log-
ical form. In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL), pages 678–687.
Z. Zhang, K. Q. Zhu, H. Wang, and H. Li. 2013. Au-
tomatic extraction of top-k lists from the web. In
International Conference on Data Engineering.
</reference>
<page confidence="0.99059">
1480
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984879">
<title confidence="0.998905">Compositional Semantic Parsing on Semi-Structured Tables</title>
<author confidence="0.997799">Panupong Pasupat Percy Liang</author>
<affiliation confidence="0.9998695">Computer Science Department Computer Science Department Stanford University Stanford University</affiliation>
<email confidence="0.998264">ppasupat@cs.stanford.edupliang@cs.stanford.edu</email>
<abstract confidence="0.999545217391304">Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Androutsopoulos</author>
<author>G D Ritchie</author>
<author>P Thanisch</author>
</authors>
<title>Natural language interfaces to databases – an introduction.</title>
<date>1995</date>
<journal>Journal of Natural Language Engineering,</journal>
<pages>1--29</pages>
<contexts>
<context position="33127" citStr="Androutsopoulos et al., 1995" startWordPosition="5471" endWordPosition="5474">breadth of knowledge source and the depth of compositionality in semantic parsing. This section explores the connections in both aspects to related work. Logical coverage. Different semantic parsing systems are designed to handle different sets of logical operations and degrees of compositionality. For example, form-filling systems (Wang et al., 2011) usually cover a smaller scope of operations and compositionality, while early statistical semantic parsers for question answering (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and high-accuracy natural language interfaces for databases (Androutsopoulos et al., 1995; Popescu et al., 2003) target more compositional utterances with a wide range of logical operations. This work aims to increase the logical coverage even further. For example, compared to the Geoquery dataset, the WIKITABLEQUESTIONS dataset includes a move diverse set of logical operations, and while it does not have extremely compositional questions like in Geoquery (e.g., “What states border states that border states that border Florida?”), our dataset contains fairly compositional questions on average. To parse a compositional utterance, many works rely on a lexicon that translates phrases</context>
</contexts>
<marker>Androutsopoulos, Ritchie, Thanisch, 1995</marker>
<rawString>I. Androutsopoulos, G. D. Ritchie, and P. Thanisch. 1995. Natural language interfaces to databases – an introduction. Journal of Natural Language Engineering, 1:29–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>P Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="25369" citStr="Berant and Liang (2014)" startWordPosition="4220" endWordPosition="4223">5.9 all − unlex phrase-predicate 34.3 76.7 all − missing-predicate 35.9 76.7 all − features involving denotation 33.5 76.8 all − denotation 34.3 76.6 all − phrase-denotation 35.7 76.8 all − headword-denotation 36.0 76.7 (c) Anchor operations to trigger words 37.1 59.4 Table 6: Average accuracy and oracle scores on development data in various system settings. 69.1% of the development examples have the answer appearing as an entity in the table. In the second baseline (WQ), we only allow deduction rules that produce join and count logical forms. This rule subset has the same logical coverage as Berant and Liang (2014), which is designed to handle the WEBQUESTIONS (Berant et al., 2013) and FREE917 (Cai and Yates, 2013) datasets. Results. Table 5 shows the results compared to the baselines. Our system gets an accuracy of 37.1% on the test data, which is significantly higher than both baselines, while the oracle is 76.6%. The next subsections analyze the system components in more detail. 7.2 Dataset statistics In this section, we analyze the breadth and depth of the WIKITABLEQUESTIONS dataset, and how the system handles them. Number of relations. With 3,929 unique column headers (relations) among 13,396 colum</context>
<context position="34187" citStr="Berant and Liang (2014)" startWordPosition="5637" endWordPosition="5640">lorida?”), our dataset contains fairly compositional questions on average. To parse a compositional utterance, many works rely on a lexicon that translates phrases to entities, relations, and logical operations. A lexicon can be automatically generated (Unger and Cimiano, 2011; Unger et al., 2012), learned from data (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al., 2013), but requires some techniques to generalize to unseen data. Our work takes a different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), a</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>J. Berant and P. Liang. 2014. Semantic parsing via paraphrasing. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>A Chou</author>
<author>R Frostig</author>
<author>P Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1733" citStr="Berant et al., 2013" startWordPosition="247" endWordPosition="250">ilable. 1 Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a Year City Country Nations 1896 Athens Greece 14 1900 Paris France 24 1904 St. Louis USA 12 . . . . . . . . . . . . 2004 Athens Greece 201 2008 Beijing China 204 2012 London UK 204 x1: “Greece held its last Summer Olympics in which year?” y1: {2004} x2: “In which city’s the first time with at least 20 nations?” y2: {Paris} x3: “Which years have the most participating countries?” y3: {2008, 2012} x4: “How many events were in Athens, Greece?” y4: {2} x5: “How many more participants were there i</context>
<context position="25437" citStr="Berant et al., 2013" startWordPosition="4232" endWordPosition="4235"> 76.7 all − features involving denotation 33.5 76.8 all − denotation 34.3 76.6 all − phrase-denotation 35.7 76.8 all − headword-denotation 36.0 76.7 (c) Anchor operations to trigger words 37.1 59.4 Table 6: Average accuracy and oracle scores on development data in various system settings. 69.1% of the development examples have the answer appearing as an entity in the table. In the second baseline (WQ), we only allow deduction rules that produce join and count logical forms. This rule subset has the same logical coverage as Berant and Liang (2014), which is designed to handle the WEBQUESTIONS (Berant et al., 2013) and FREE917 (Cai and Yates, 2013) datasets. Results. Table 5 shows the results compared to the baselines. Our system gets an accuracy of 37.1% on the test data, which is significantly higher than both baselines, while the oracle is 76.6%. The next subsections analyze the system components in more detail. 7.2 Dataset statistics In this section, we analyze the breadth and depth of the WIKITABLEQUESTIONS dataset, and how the system handles them. Number of relations. With 3,929 unique column headers (relations) among 13,396 columns, the tables in the WIKITABLEQUESTIONS dataset contain many more r</context>
<context position="34018" citStr="Berant et al., 2013" startWordPosition="5609" endWordPosition="5612">f logical operations, and while it does not have extremely compositional questions like in Geoquery (e.g., “What states border states that border states that border Florida?”), our dataset contains fairly compositional questions on average. To parse a compositional utterance, many works rely on a lexicon that translates phrases to entities, relations, and logical operations. A lexicon can be automatically generated (Unger and Cimiano, 2011; Unger et al., 2012), learned from data (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al., 2013), but requires some techniques to generalize to unseen data. Our work takes a different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and technique</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Cafarella</author>
<author>A Halevy</author>
<author>D Z Wang</author>
<author>E Wu</author>
<author>Y Zhang</author>
</authors>
<title>WebTables: exploring the power of tables on the web. In Very Large Data Bases (VLDB),</title>
<date>2008</date>
<pages>538--549</pages>
<contexts>
<context position="35043" citStr="Cafarella et al., 2008" startWordPosition="5769" endWordPosition="5772">ed popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organiz</context>
</contexts>
<marker>Cafarella, Halevy, Wang, Wu, Zhang, 2008</marker>
<rawString>M. J. Cafarella, A. Halevy, D. Z. Wang, E. Wu, and Y. Zhang. 2008. WebTables: exploring the power of tables on the web. In Very Large Data Bases (VLDB), pages 538–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Cai</author>
<author>A Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1712" citStr="Cai and Yates, 2013" startWordPosition="243" endWordPosition="246"> is made publicly available. 1 Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a Year City Country Nations 1896 Athens Greece 14 1900 Paris France 24 1904 St. Louis USA 12 . . . . . . . . . . . . 2004 Athens Greece 201 2008 Beijing China 204 2012 London UK 204 x1: “Greece held its last Summer Olympics in which year?” y1: {2004} x2: “In which city’s the first time with at least 20 nations?” y2: {Paris} x3: “Which years have the most participating countries?” y3: {2008, 2012} x4: “How many events were in Athens, Greece?” y4: {2} x5: “How many more part</context>
<context position="25471" citStr="Cai and Yates, 2013" startWordPosition="4238" endWordPosition="4241">otation 33.5 76.8 all − denotation 34.3 76.6 all − phrase-denotation 35.7 76.8 all − headword-denotation 36.0 76.7 (c) Anchor operations to trigger words 37.1 59.4 Table 6: Average accuracy and oracle scores on development data in various system settings. 69.1% of the development examples have the answer appearing as an entity in the table. In the second baseline (WQ), we only allow deduction rules that produce join and count logical forms. This rule subset has the same logical coverage as Berant and Liang (2014), which is designed to handle the WEBQUESTIONS (Berant et al., 2013) and FREE917 (Cai and Yates, 2013) datasets. Results. Table 5 shows the results compared to the baselines. Our system gets an accuracy of 37.1% on the test data, which is significantly higher than both baselines, while the oracle is 76.6%. The next subsections analyze the system components in more detail. 7.2 Dataset statistics In this section, we analyze the breadth and depth of the WIKITABLEQUESTIONS dataset, and how the system handles them. Number of relations. With 3,929 unique column headers (relations) among 13,396 columns, the tables in the WIKITABLEQUESTIONS dataset contain many more relations than closed-domain datase</context>
<context position="33996" citStr="Cai and Yates, 2013" startWordPosition="5605" endWordPosition="5608"> a move diverse set of logical operations, and while it does not have extremely compositional questions like in Geoquery (e.g., “What states border states that border states that border Florida?”), our dataset contains fairly compositional questions on average. To parse a compositional utterance, many works rely on a lexicon that translates phrases to entities, relations, and logical operations. A lexicon can be automatically generated (Unger and Cimiano, 2011; Unger et al., 2012), learned from data (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al., 2013), but requires some techniques to generalize to unseen data. Our work takes a different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new r</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Q. Cai and A. Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2010</date>
<booktitle>In Conference on Learning Theory (COLT).</booktitle>
<contexts>
<context position="9284" citStr="Duchi et al., 2010" startWordPosition="1542" endWordPosition="1545">x, w, z)}, (1) where θ is the parameter vector. Finally, we choose the logical form z with the highest model probability and execute it on w to get the answer denotation y = Hw. Training. Given training examples D = {(xi, ti, yi)}Ni=1, we seek a parameter vector θ that maximizes the regularized log-likelihood of the correct denotation yi marginalized over logical forms z. Formally, we maximize the objective function N log pθ(yi |xi, wi) − λ 11θ111 , (2) 1 J(θ) = N i=1 where wi is deterministically generated from ti, and pθ(y |x, w) = � pθ(z |x, w). (3) zEZx;y=Qzlw We optimize θ using AdaGrad (Duchi et al., 2010), running 3 passes over the data. We use L1 regularization with λ = 3 x 10−5 obtained from cross-validation. The following sections explain individual system components in more detail. 4 Knowledge graph Inspired by the graph representation of knowledge bases, we preprocess the table t by deterministically converting it into a knowledge graph w as illustrated in Figure 3. In the most basic form, table rows become row nodes, strings in table cells become entity nodes,1 and table columns become directed edges from the row nodes to the entity 1Two occurrences of the same string constitute one node</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2010</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive subgradient methods for online learning and stochastic optimization. In Conference on Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>L Zettlemoyer</author>
<author>O Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="34759" citStr="Fader et al., 2013" startWordPosition="5726" endWordPosition="5729">m growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural </context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>A. Fader, L. Zettlemoyer, and O. Etzioni. 2013. Paraphrase-driven learning for open question answering. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>L Zettlemoyer</author>
<author>O Etzioni</author>
</authors>
<title>Open question answering over curated and extracted knowledge bases.</title>
<date>2014</date>
<booktitle>In International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>1156--1165</pages>
<contexts>
<context position="1753" citStr="Fader et al., 2014" startWordPosition="251" endWordPosition="254">n In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a Year City Country Nations 1896 Athens Greece 14 1900 Paris France 24 1904 St. Louis USA 12 . . . . . . . . . . . . 2004 Athens Greece 201 2008 Beijing China 204 2012 London UK 204 x1: “Greece held its last Summer Olympics in which year?” y1: {2004} x2: “In which city’s the first time with at least 20 nations?” y2: {Paris} x3: “Which years have the most participating countries?” y3: {2008, 2012} x4: “How many events were in Athens, Greece?” y4: {2} x5: “How many more participants were there in 1900 than in the f</context>
<context position="34531" citStr="Fader et al., 2014" startWordPosition="5691" endWordPosition="5694">; Kwiatkowski et al., 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al., 2013), but requires some techniques to generalize to unseen data. Our work takes a different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search que</context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2014</marker>
<rawString>A. Fader, L. Zettlemoyer, and O. Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In International Conference on Knowledge Discovery and Data Mining (KDD), pages 1156–1165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gonzalez</author>
<author>A Y Halevy</author>
<author>C S Jensen</author>
<author>A Langen</author>
<author>J Madhavan</author>
<author>R Shapley</author>
<author>W Shen</author>
<author>J GoldbergKidon</author>
</authors>
<title>Google fusion tables: web-centered data management and collaboration.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,</booktitle>
<pages>1061--1066</pages>
<contexts>
<context position="35219" citStr="Gonzalez et al., 2010" startWordPosition="5800" endWordPosition="5803">ources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to w</context>
</contexts>
<marker>Gonzalez, Halevy, Jensen, Langen, Madhavan, Shapley, Shen, GoldbergKidon, 2010</marker>
<rawString>H. Gonzalez, A. Y. Halevy, C. S. Jensen, A. Langen, J. Madhavan, R. Shapley, W. Shen, and J. GoldbergKidon. 2010. Google fusion tables: web-centered data management and collaboration. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, pages 1061–1066.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gupta</author>
<author>S Sarawagi</author>
</authors>
<title>Answering table augmentation queries from unstructured lists on the web.</title>
<date>2009</date>
<booktitle>In Very Large Data Bases (VLDB), number 1,</booktitle>
<pages>289--300</pages>
<contexts>
<context position="36019" citStr="Gupta and Sarawagi, 2009" startWordPosition="5927" endWordPosition="5930">en information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al., 2013). Pasupat and Liang (2014) used a framework similar to ours to extract entities from web pages, where the “logical forms” were XPath expressions. A natural direction is to combine the logical compositionality of this work with the even broader knowledge source of general web pages. Acknowledgements. We gratefully acknowledge the support of the Google Natural Language Understanding Focused Program and the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contrac</context>
</contexts>
<marker>Gupta, Sarawagi, 2009</marker>
<rawString>R. Gupta and S. Sarawagi. 2009. Answering table augmentation queries from unstructured lists on the web. In Very Large Data Bases (VLDB), number 1, pages 289–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Knowledge base population: Successful approaches and challenges.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1148--1158</pages>
<contexts>
<context position="35514" citStr="Ji and Grishman, 2011" startWordPosition="5845" endWordPosition="5849">d data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al., 2013). Pasupat and Liang (2014) used a framework similar to ou</context>
</contexts>
<marker>Ji, Grishman, 2011</marker>
<rawString>H. Ji and R. Grishman. 2011. Knowledge base population: Successful approaches and challenges. In Association for Computational Linguistics (ACL), pages 1148–1158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Krishnamurthy</author>
<author>T Kollar</author>
</authors>
<title>Jointly learning to parse and perceive: Connecting natural language to the physical world.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<volume>1</volume>
<pages>206</pages>
<contexts>
<context position="34841" citStr="Krishnamurthy and Kollar, 2013" startWordPosition="5737" endWordPosition="5741">ons and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; M</context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>J. Krishnamurthy and T. Kollar. 2013. Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics (TACL), 1:193– 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1512--1523</pages>
<contexts>
<context position="1578" citStr="Kwiatkowski et al., 2011" startWordPosition="223" endWordPosition="226">ificant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. 1 Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a Year City Country Nations 1896 Athens Greece 14 1900 Paris France 24 1904 St. Louis USA 12 . . . . . . . . . . . . 2004 Athens Greece 201 2008 Beijing China 204 2012 London UK 204 x1: “Greece held its last Summer Olympics in which year?” y1: {2004} x2: “In which city’s the first time with at least 20 nations?” y2: {Paris} x3: “Which years </context>
<context position="33939" citStr="Kwiatkowski et al., 2011" startWordPosition="5596" endWordPosition="5599">o the Geoquery dataset, the WIKITABLEQUESTIONS dataset includes a move diverse set of logical operations, and while it does not have extremely compositional questions like in Geoquery (e.g., “What states border states that border states that border Florida?”), our dataset contains fairly compositional questions on average. To parse a compositional utterance, many works rely on a lexicon that translates phrases to entities, relations, and logical operations. A lexicon can be automatically generated (Unger and Cimiano, 2011; Unger et al., 2012), learned from data (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al., 2013), but requires some techniques to generalize to unseen data. Our work takes a different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The in</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In Empirical Methods in Natural Language Processing (EMNLP), pages 1512–1523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>E Choi</author>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="34720" citStr="Kwiatkowski et al., 2013" startWordPosition="5720" endWordPosition="5723"> different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend </context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<date>2013</date>
<note>Lambda dependency-based compositional semantics. arXiv.</note>
<contexts>
<context position="12580" citStr="Liang, 2013" startWordPosition="2095" endWordPosition="2096">thens (row nodes with a City edge to Athens) Union City.(Athens U Beijing) Intersection City.Athens fl Year.Number.&lt;.1990 Reverse R[Year].City.Athens (entities where a row in City.Athens has a Year edge to) Aggregation count(City.Athens) (the number of rows with city Athens) Superlative argmax(City.Athens, Index) (the last row with city Athens) Arithmetic sub(204, 201) (= 204 − 201) Lambda Ax[Year.Date.x] (a binary: composition of two relations) Table 1: The lambda DCS operations we use. 5 Logical forms As our language for logical forms, we use lambda dependency-based compositional semantics (Liang, 2013), or lambda DCS, which we briefly describe here. Each lambda DCS logical form is either a unary (denoting a list of values) or a binary (denoting a list of pairs). The most basic unaries are singletons (e.g., China represents an entity node, and 30 represents a single number), while the most basic binaries are relations (e.g., City maps rows to city entities, Next maps rows to rows, and &gt;= maps numbers to numbers). Logical forms can be combined into larger ones via various operations listed in Table 1. Each operation produces a unary except lambda abstraction: λx[f(x)] is a binary mapping x to</context>
</contexts>
<marker>Liang, 2013</marker>
<rawString>P. Liang. 2013. Lambda dependency-based compositional semantics. arXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Limaye</author>
<author>S Sarawagi</author>
<author>S Chakrabarti</author>
</authors>
<title>Annotating and searching web tables using entities, types and relationships.</title>
<date>2010</date>
<booktitle>In Very Large Data Bases (VLDB),</booktitle>
<volume>3</volume>
<pages>1338--1347</pages>
<contexts>
<context position="35106" citStr="Limaye et al., 2010" startWordPosition="5781" endWordPosition="5784">013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost durin</context>
</contexts>
<marker>Limaye, Sarawagi, Chakrabarti, 2010</marker>
<rawString>G. Limaye, S. Sarawagi, and S. Chakrabarti. 2010. Annotating and searching web tables using entities, types and relationships. In Very Large Data Bases (VLDB), volume 3, pages 1338–1347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schmitz Masaum</author>
<author>R Bart</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL),</booktitle>
<pages>523--534</pages>
<contexts>
<context position="35460" citStr="Masaum et al., 2012" startWordPosition="5837" endWordPosition="5840">3; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al., 2013). P</context>
</contexts>
<marker>Masaum, Bart, Soderland, Etzioni, 2012</marker>
<rawString>Masaum, M. Schmitz, R. Bart, S. Soderland, and O. Etzioni. 2012. Open language learning for information extraction. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 523– 534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pasupat</author>
<author>P Liang</author>
</authors>
<title>Zero-shot entity extraction from web pages.</title>
<date>2014</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="36083" citStr="Pasupat and Liang (2014)" startWordPosition="5938" endWordPosition="5941">) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al., 2013). Pasupat and Liang (2014) used a framework similar to ours to extract entities from web pages, where the “logical forms” were XPath expressions. A natural direction is to combine the logical compositionality of this work with the even broader knowledge source of general web pages. Acknowledgements. We gratefully acknowledge the support of the Google Natural Language Understanding Focused Program and the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040. Data and reproducibility. The WIKITABLEQ</context>
</contexts>
<marker>Pasupat, Liang, 2014</marker>
<rawString>P. Pasupat and P. Liang. 2014. Zero-shot entity extraction from web pages. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pimplikar</author>
<author>S Sarawagi</author>
</authors>
<title>Answering table queries on the web using column keywords.</title>
<date>2012</date>
<booktitle>In Very Large Data Bases (VLDB),</booktitle>
<volume>5</volume>
<pages>908--919</pages>
<contexts>
<context position="35250" citStr="Pimplikar and Sarawagi, 2012" startWordPosition="5804" endWordPosition="5807">or improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In</context>
</contexts>
<marker>Pimplikar, Sarawagi, 2012</marker>
<rawString>R. Pimplikar and S. Sarawagi. 2012. Answering table queries on the web using column keywords. In Very Large Data Bases (VLDB), volume 5, pages 908– 919.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Popescu</author>
<author>O Etzioni</author>
<author>H Kautz</author>
</authors>
<title>Towards a theory of natural language interfaces to databases.</title>
<date>2003</date>
<booktitle>In International Conference on Intelligent User Interfaces (IUI),</booktitle>
<pages>149--157</pages>
<contexts>
<context position="33150" citStr="Popescu et al., 2003" startWordPosition="5475" endWordPosition="5478">d the depth of compositionality in semantic parsing. This section explores the connections in both aspects to related work. Logical coverage. Different semantic parsing systems are designed to handle different sets of logical operations and degrees of compositionality. For example, form-filling systems (Wang et al., 2011) usually cover a smaller scope of operations and compositionality, while early statistical semantic parsers for question answering (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and high-accuracy natural language interfaces for databases (Androutsopoulos et al., 1995; Popescu et al., 2003) target more compositional utterances with a wide range of logical operations. This work aims to increase the logical coverage even further. For example, compared to the Geoquery dataset, the WIKITABLEQUESTIONS dataset includes a move diverse set of logical operations, and while it does not have extremely compositional questions like in Geoquery (e.g., “What states border states that border states that border Florida?”), our dataset contains fairly compositional questions on average. To parse a compositional utterance, many works rely on a lexicon that translates phrases to entities, relations</context>
</contexts>
<marker>Popescu, Etzioni, Kautz, 2003</marker>
<rawString>A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards a theory of natural language interfaces to databases. In International Conference on Intelligent User Interfaces (IUI), pages 149–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Price</author>
</authors>
<title>Evaluation of spoken language systems: The ATIS domain.</title>
<date>1990</date>
<booktitle>In Proceedings of the Third DARPA Speech and Natural Language Workshop,</booktitle>
<pages>91--95</pages>
<contexts>
<context position="26430" citStr="Price, 1990" startWordPosition="4395" endWordPosition="4396">f the WIKITABLEQUESTIONS dataset, and how the system handles them. Number of relations. With 3,929 unique column headers (relations) among 13,396 columns, the tables in the WIKITABLEQUESTIONS dataset contain many more relations than closed-domain datasets such as Geoquery (Zelle and Mooney, Operation Amount join (table lookup) 13.5% + join with Next + 5.5% + aggregate (count, sum, max, ... ) + 15.0% + superlative (argmax, argmin) + 24.5% + arithmetic, n, u + 20.5% + other phenomena + 21.0% Table 7: The logical operations required to answer the questions in 200 random examples. 1996) and ATIS (Price, 1990). Additionally, the logical forms that execute to the correct denotations refer to a total of 2,056 unique column headers, which is greater than the number of relations in the FREE917 dataset (635 Freebase relations). Knowledge coverage. We sampled 50 examples from the dataset and tried to answer them manually using Freebase. Even though Freebase contains some information extracted from Wikipedia, we can answer only 20% of the questions, indicating that WIKITABLEQUESTIONS contains a broad set of facts beyond Freebase. Logical operation coverage. The dataset covers a wide range of question type</context>
</contexts>
<marker>Price, 1990</marker>
<rawString>P. Price. 1990. Evaluation of spoken language systems: The ATIS domain. In Proceedings of the Third DARPA Speech and Natural Language Workshop, pages 91–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Reddy</author>
<author>M Lapata</author>
<author>M Steedman</author>
</authors>
<title>Largescale semantic parsing without question-answer pairs.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<volume>2</volume>
<issue>10</issue>
<contexts>
<context position="1774" citStr="Reddy et al., 2014" startWordPosition="255" endWordPosition="258">g for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a Year City Country Nations 1896 Athens Greece 14 1900 Paris France 24 1904 St. Louis USA 12 . . . . . . . . . . . . 2004 Athens Greece 201 2008 Beijing China 204 2012 London UK 204 x1: “Greece held its last Summer Olympics in which year?” y1: {2004} x2: “In which city’s the first time with at least 20 nations?” y2: {Paris} x3: “Which years have the most participating countries?” y3: {2008, 2012} x4: “How many events were in Athens, Greece?” y4: {2} x5: “How many more participants were there in 1900 than in the first year?” y5: {10} </context>
<context position="34862" citStr="Reddy et al., 2014" startWordPosition="5742" endWordPosition="5745">nce. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) a</context>
</contexts>
<marker>Reddy, Lapata, Steedman, 2014</marker>
<rawString>S. Reddy, M. Lapata, and M. Steedman. 2014. Largescale semantic parsing without question-answer pairs. Transactions of the Association for Computational Linguistics (TACL), 2(10):377–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Syed</author>
<author>T Finin</author>
<author>V Mulwad</author>
<author>A Joshi</author>
</authors>
<title>Exploiting a web of semantic data for interpreting tables.</title>
<date>2010</date>
<booktitle>In Proceedings of the Second Web Science Conference.</booktitle>
<contexts>
<context position="35084" citStr="Syed et al., 2010" startWordPosition="5777" endWordPosition="5780">y (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is</context>
</contexts>
<marker>Syed, Finin, Mulwad, Joshi, 2010</marker>
<rawString>Z. Syed, T. Finin, V. Mulwad, and A. Joshi. 2010. Exploiting a web of semantic data for interpreting tables. In Proceedings of the Second Web Science Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Unger</author>
<author>P Cimiano</author>
</authors>
<title>Pythia: compositional meaning construction for ontology-based question answering on the semantic web.</title>
<date>2011</date>
<booktitle>In Proceedings of the 16th international conference on Natural language</booktitle>
<pages>153--160</pages>
<contexts>
<context position="33841" citStr="Unger and Cimiano, 2011" startWordPosition="5580" endWordPosition="5584">perations. This work aims to increase the logical coverage even further. For example, compared to the Geoquery dataset, the WIKITABLEQUESTIONS dataset includes a move diverse set of logical operations, and while it does not have extremely compositional questions like in Geoquery (e.g., “What states border states that border states that border Florida?”), our dataset contains fairly compositional questions on average. To parse a compositional utterance, many works rely on a lexicon that translates phrases to entities, relations, and logical operations. A lexicon can be automatically generated (Unger and Cimiano, 2011; Unger et al., 2012), learned from data (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al., 2013), but requires some techniques to generalize to unseen data. Our work takes a different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the</context>
</contexts>
<marker>Unger, Cimiano, 2011</marker>
<rawString>C. Unger and P. Cimiano. 2011. Pythia: compositional meaning construction for ontology-based question answering on the semantic web. In Proceedings of the 16th international conference on Natural language processing and information systems, pages 153–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Unger</author>
<author>L B¨uhmann</author>
<author>J Lehmann</author>
<author>A Ngonga</author>
<author>D Gerber</author>
<author>P Cimiano</author>
</authors>
<title>Template-based question answering over RDF data. In World Wide Web (WWW),</title>
<date>2012</date>
<pages>639--648</pages>
<marker>Unger, B¨uhmann, Lehmann, Ngonga, Gerber, Cimiano, 2012</marker>
<rawString>C. Unger, L. B¨uhmann, J. Lehmann, A. Ngonga, D. Gerber, and P. Cimiano. 2012. Template-based question answering over RDF data. In World Wide Web (WWW), pages 639–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Venetis</author>
<author>A Halevy</author>
<author>J Madhavan</author>
<author>M Pas¸ca</author>
<author>W Shen</author>
<author>F Wu</author>
<author>G Miao</author>
<author>C Wu</author>
</authors>
<title>Recovering semantics of tables on the web.</title>
<date>2011</date>
<booktitle>In Very Large Data Bases (VLDB),</booktitle>
<volume>4</volume>
<pages>528--538</pages>
<marker>Venetis, Halevy, Madhavan, Pas¸ca, Shen, Wu, Miao, Wu, 2011</marker>
<rawString>P. Venetis, A. Halevy, J. Madhavan, M. Pas¸ca, W. Shen, F. Wu, G. Miao, and C. Wu. 2011. Recovering semantics of tables on the web. In Very Large Data Bases (VLDB), volume 4, pages 528–538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>L Deng</author>
<author>A Acero</author>
</authors>
<title>Semantic frame-based spoken language understanding. Spoken Language Understanding: Systems for Extracting Semantic Information from Speech,</title>
<date>2011</date>
<pages>41--91</pages>
<contexts>
<context position="32852" citStr="Wang et al., 2011" startWordPosition="5433" endWordPosition="5436">anking errors (25%) which mostly occur when the utterance phrase and the relation are obliquely related (e.g., “airplane” and Model). frequency 80 score 60 40 20 0 80 0 25 50 75 100 beam size score 60 40 20 0 1477 8 Discussion Our work simultaneously increases the breadth of knowledge source and the depth of compositionality in semantic parsing. This section explores the connections in both aspects to related work. Logical coverage. Different semantic parsing systems are designed to handle different sets of logical operations and degrees of compositionality. For example, form-filling systems (Wang et al., 2011) usually cover a smaller scope of operations and compositionality, while early statistical semantic parsers for question answering (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and high-accuracy natural language interfaces for databases (Androutsopoulos et al., 1995; Popescu et al., 2003) target more compositional utterances with a wide range of logical operations. This work aims to increase the logical coverage even further. For example, compared to the Geoquery dataset, the WIKITABLEQUESTIONS dataset includes a move diverse set of logical operations, and while it does not have extre</context>
</contexts>
<marker>Wang, Deng, Acero, 2011</marker>
<rawString>Y. Wang, L. Deng, and A. Acero. 2011. Semantic frame-based spoken language understanding. Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, pages 41–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>960--967</pages>
<contexts>
<context position="1520" citStr="Wong and Mooney, 2007" startWordPosition="215" endWordPosition="218">trong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. 1 Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a Year City Country Nations 1896 Athens Greece 14 1900 Paris France 24 1904 St. Louis USA 12 . . . . . . . . . . . . 2004 Athens Greece 201 2008 Beijing China 204 2012 London UK 204 x1: “Greece held its last Summer Olympics in which year?” y1: {2004} x2: “In which city’s the first tim</context>
<context position="33005" citStr="Wong and Mooney, 2007" startWordPosition="5455" endWordPosition="5458">re 60 40 20 0 80 0 25 50 75 100 beam size score 60 40 20 0 1477 8 Discussion Our work simultaneously increases the breadth of knowledge source and the depth of compositionality in semantic parsing. This section explores the connections in both aspects to related work. Logical coverage. Different semantic parsing systems are designed to handle different sets of logical operations and degrees of compositionality. For example, form-filling systems (Wang et al., 2011) usually cover a smaller scope of operations and compositionality, while early statistical semantic parsers for question answering (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and high-accuracy natural language interfaces for databases (Androutsopoulos et al., 1995; Popescu et al., 2003) target more compositional utterances with a wide range of logical operations. This work aims to increase the logical coverage even further. For example, compared to the Geoquery dataset, the WIKITABLEQUESTIONS dataset includes a move diverse set of logical operations, and while it does not have extremely compositional questions like in Geoquery (e.g., “What states border states that border states that border Florida?”), our dataset contains fairly co</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Association for Computational Linguistics (ACL), pages 960–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>D Widdows</author>
<author>T Lokovic</author>
<author>K Nigam</author>
</authors>
<title>Scalable attribute-value extraction from semistructured text.</title>
<date>2009</date>
<booktitle>In IEEE International Conference on Data Mining Workshops,</booktitle>
<pages>302--307</pages>
<contexts>
<context position="35976" citStr="Wong et al., 2009" startWordPosition="5921" endWordPosition="5924"> language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al., 2013). Pasupat and Liang (2014) used a framework similar to ours to extract entities from web pages, where the “logical forms” were XPath expressions. A natural direction is to combine the logical compositionality of this work with the even broader knowledge source of general web pages. Acknowledgements. We gratefully acknowledge the support of the Google Natural Language Understanding Focused Program and the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under A</context>
</contexts>
<marker>Wong, Widdows, Lokovic, Nigam, 2009</marker>
<rawString>Y. W. Wong, D. Widdows, T. Lokovic, and K. Nigam. 2009. Scalable attribute-value extraction from semistructured text. In IEEE International Conference on Data Mining Workshops, pages 302–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wu</author>
<author>D S Weld</author>
</authors>
<title>Open information extraction using Wikipedia.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>118--127</pages>
<contexts>
<context position="35438" citStr="Wu and Weld, 2010" startWordPosition="5833" endWordPosition="5836">thy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>F. Wu and D. S. Weld. 2010. Open information extraction using Wikipedia. In Association for Computational Linguistics (ACL), pages 118–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1497" citStr="Zelle and Mooney, 1996" startWordPosition="211" endWordPosition="214">ng algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. 1 Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a Year City Country Nations 1896 Athens Greece 14 1900 Paris France 24 1904 St. Louis USA 12 . . . . . . . . . . . . 2004 Athens Greece 201 2008 Beijing China 204 2012 London UK 204 x1: “Greece held its last Summer Olympics in which year?” y1: {2004} x2: “In whi</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL),</booktitle>
<pages>678--687</pages>
<contexts>
<context position="1551" citStr="Zettlemoyer and Collins, 2007" startWordPosition="219" endWordPosition="222">s and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. 1 Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a Year City Country Nations 1896 Athens Greece 14 1900 Paris France 24 1904 St. Louis USA 12 . . . . . . . . . . . . 2004 Athens Greece 201 2008 Beijing China 204 2012 London UK 204 x1: “Greece held its last Summer Olympics in which year?” y1: {2004} x2: “In which city’s the first time with at least 20 nations?” y2</context>
<context position="33037" citStr="Zettlemoyer and Collins, 2007" startWordPosition="5459" endWordPosition="5462">0 75 100 beam size score 60 40 20 0 1477 8 Discussion Our work simultaneously increases the breadth of knowledge source and the depth of compositionality in semantic parsing. This section explores the connections in both aspects to related work. Logical coverage. Different semantic parsing systems are designed to handle different sets of logical operations and degrees of compositionality. For example, form-filling systems (Wang et al., 2011) usually cover a smaller scope of operations and compositionality, while early statistical semantic parsers for question answering (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and high-accuracy natural language interfaces for databases (Androutsopoulos et al., 1995; Popescu et al., 2003) target more compositional utterances with a wide range of logical operations. This work aims to increase the logical coverage even further. For example, compared to the Geoquery dataset, the WIKITABLEQUESTIONS dataset includes a move diverse set of logical operations, and while it does not have extremely compositional questions like in Geoquery (e.g., “What states border states that border states that border Florida?”), our dataset contains fairly compositional questions on average</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 678–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhang</author>
<author>K Q Zhu</author>
<author>H Wang</author>
<author>H Li</author>
</authors>
<title>Automatic extraction of top-k lists from the web.</title>
<date>2013</date>
<booktitle>In International Conference on Data Engineering.</booktitle>
<contexts>
<context position="36057" citStr="Zhang et al., 2013" startWordPosition="5934" endWordPosition="5937">; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al., 2013). Pasupat and Liang (2014) used a framework similar to ours to extract entities from web pages, where the “logical forms” were XPath expressions. A natural direction is to combine the logical compositionality of this work with the even broader knowledge source of general web pages. Acknowledgements. We gratefully acknowledge the support of the Google Natural Language Understanding Focused Program and the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040. Data and repro</context>
</contexts>
<marker>Zhang, Zhu, Wang, Li, 2013</marker>
<rawString>Z. Zhang, K. Q. Zhu, H. Wang, and H. Li. 2013. Automatic extraction of top-k lists from the web. In International Conference on Data Engineering.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>