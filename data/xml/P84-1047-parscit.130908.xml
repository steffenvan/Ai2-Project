<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000313">
<title confidence="0.830076">
Entity-Oriented Parsing
</title>
<author confidence="0.907619">
Philip J. Hayes
</author>
<affiliation confidence="0.9628145">
Computer Science Department, Carnegie-Mellon University
Pittsburgh, PA 13213, USA
</affiliation>
<sectionHeader confidence="0.949202" genericHeader="abstract">
Abstract&apos;
</sectionHeader>
<bodyText confidence="0.999829142857143">
An entity-oriented approach to restricted-domain parsing is
proposed. In this approach, the definitions of the structure and
surface representation of domain entities are grouped. together.
Like semantic grammar, this allows easy exploitation of limited
domain semantics. In addition, it facilitates fragmentary
recognition and the use of multiple parsing strategies, and so is
particularly useful for robust recognition of extragrammatical
input. Several advantages from the point of view of language
definition are also noted. Representative samples from an
entity-oriented language definition are presented, along with a
control structure for an entity-oriented parser, some parsing
strategies that use the control structure, and worked examples
of parses. A parser incorporating the control structure and the
parsing strategies is currently under implementation.
</bodyText>
<sectionHeader confidence="0.99877" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999692">
The task of lypical natural language interface systems is much
simpler than the general problem of natural language
understanding: The simplifications arise because:
</bodyText>
<listItem confidence="0.966825888888889">
1. the systems operate within a highly restricted domain of
discourse, so that a precise set of object types can be
established, and marry of the ambiguities that come up in
more general natural language processing can be ignored or
constrained away;
2. even within the restricted domain of discourse, a natural
language interface system only needs to recognize a limited
subset of all the things that could be said -- the subset that
its back-end can respond to.
</listItem>
<bodyText confidence="0.999977714285714">
The most commonly used technique to exploit these limited
domain constraints is semantic grammar [1, 2, 9] in which
semantically defined categories (such as &lt;ship&gt; or &lt;ship-
attribute&gt;) are used in a grammar (usually ATN based) in place of
syntactic categories (such as &lt;noun&gt; or &lt;adjective&gt;). While
semantic grammar has been very successful in exploiting limited
domain coestrainte to reduce ambiguities and eliminate spurious
parses of grammatical input, it still suffers from the fragility in the
face of extragrammatical input characteristic of parsing based on
transition nets [41. Also. thn task of restricted-domain language
definition is typically difficult in interfaces based on semantic
grammar, in part because Um grammar definition formalism is not
well integrated with the method of defining the object and actions
of the domain of discourse (though see [6]).
</bodyText>
<footnote confidence="0.581202666666667">
1
This research wns sponsored by the Air !Wee Office of Scient,fic flesearch
under Contract /11-0:311-82-0219
</footnote>
<bodyText confidence="0.996808441176471">
This paper proposes an alternative approach to restricted
domain language recognition called entity-odor:tad parsing.
Entity-oriented parsing uses the same notion of semantically-
defined categories mm, snmantic grammar, but does net embed
these categories in a grammatical structure designed for syntactic
recognition. Instead, a scheme more reminiscent of conceptual or
case-frame parsers [3, 10, H] is employed. An entity-oriented
parser operates from a collection of definitions of the various
entities (objects, events. cernmands, states, etc.) that a particular
interface system needs to recognize. These definitions contain
information about the internal structure of the entities, about the
way the entitlea will be manifested in the natural language input,
and about the correspondence between the internal structure and
surface representation. This arrangement provides a good
framework for exploiting the simplifications possible in restricted
domain natural language recognition because:
1, the entitios form a natural set of types through which to
constrain the recognition semantically. The types -also form a
natural basis for the structural definitions of entities.
2. the set of things that the back-end can respond to
corresponds to a subeet ef the domaie entities (remember
that entities can be events or commands as well as objects).
So the goal of an entity.orionted system will normally be to
rec.ognize one of a &amp;quot;top-revel&amp;quot; class of entities. This is
analogous to the set at basic nieseage patterns that the
machine translation system of Wilks [11] aimed to recognize
in an&apos;/ input.
In addition to providing a good general basis for restricted
domain natural language recognition, we claim that the entity,
oriented approach also facilitates roteretness in the face of
extragrammatical input and ease of language definition for
reetrietee domain lengueges. Entity- snot (cc parsing has the
potential to provide better parsing robustness than more
traditional semantic grammar techniques for two major reasons:
</bodyText>
<listItem confidence="0.7272785">
• The individual definition of all domain entities facilitates their
independent recoenition. Asetnesig there is appropriate
indexing of entities through lexical items that might appear in
a surface description of them, this recognition can be done
</listItem>
<bodyText confidence="0.8690945">
hottormup, thus making possible recognition of elliptical,
tragmemary, or partially incomprehr_i..sible input. The same
dntinitions can also tie used in a I/lore efficient top-down
inutun-3 when the input conforms to the system&apos;s
expectations.
Tiecent work [5, 8] has suggested the usefulness of multiple
constructon.specific recognition stratcgies for restricted
domain parsing, par ticularly for dealing with
extraqraramatica! input. lie individual entity riefinitions form
an ideai framework amend which to organize the multiple
</bodyText>
<page confidence="0.996967">
212
</page>
<bodyText confidence="0.996682142857143">
strategies. In particular, each definitical can specify which
strategies are applicable to recognizing it. Of course, this
only provides a framework for robust recognitior3, the
robustness achieved still depends on the quality of the actual
recognition strategies used.
The advantages of entity-oriented parsing for language
definition include:
</bodyText>
<listItem confidence="0.9260669375">
• All information relating to an entity is grouped in one place,
so that a language definer will be able to sec more clearly
whether a definition is complete and what would be the
consequences of any addition or change to the definition.
• Since surface (syntactic) and structural information about an
entity is grouped together, the surface information can refer
to the structure in a clear and coherent way. In particular,
this allows hierarchical surface information to use the natural
hierarchy defined by the structural information, leading to
greater consistency of coverage in the surface language.
• Since entity definitions are independent, the information
necessary to drive recognition by the multiple construction-
specific strategies mentioned above can be represented
directly in the form most useful to each strategy, thus
removing the need for any kind of &amp;quot;grammar compilation&amp;quot;
step and allowing more rapid grammar development.
</listItem>
<bodyText confidence="0.997768090909091">
In the remainder of the paper, we make these arguments more
concrete by looking at some fragments of an entity-oriented
language definition, by outlining the control structure of a robust
restr ictecl-domain parser driven by such definitions, and by tracing
through some worked examples of the parser in operation. These
examples also shown describe some specific parsing strategies
that exploit the control structures. A parser incorporating the
control structure and the parsing strategies is currently under
implementation. Its design embodies our experience with a pilot
entity-oriented parser that has already been implemented, but is
not described here.
</bodyText>
<sectionHeader confidence="0.982213" genericHeader="method">
2. Example Entity Definitions
</sectionHeader>
<bodyText confidence="0.87757525">
This section presents some example entity and language
defioitions suitable for use in entity-oriented parsing. The
examples are drawn from the domain of an interface to a database
of college courses. Here is the (partial) definition of a course.
</bodyText>
<figure confidence="0.989357411764705">
[ •
EntityName: CollegeCourse
lype: Structured
Components: (
[ComponentName: CourseNumber
type: Integer
Greaterihan: 99
Lesslhan: 1000
(ComponentName: CourseDepartment
lype: CollegeDepartment
]
[ComponentName: CourseClass
Eypo: CellegeClass
]
[ComponentName: CourseInstructor
1ype: CollegeProressor
)
SurfaceRepresent.ation:
[Syntaxiype: NounPhrise
Head, (course 1 seminar 1
SCourseDepartment. SCourseNumber 1 ...)
AdjectivalComponents: (CourseDepartment ...)
Adjectives: (
[AdjectivalPhrase: (new 1 most recent)
Component: CourseSemoster
Value: Curient.Somester
]
)
PostNominalCases: (
[Preposition: (?intended for 1 directed to 1
Component: CourseClass
[Preposition: (naught by 1 ...)
Component: Courseinstructor
)
</figure>
<bodyText confidence="0.99809504">
For reasons of space, we cannot explain all the details of this
language. In essence, a course is defined as a structured object
with components: number, department, instructor, etc. (square
brackets denote attribute/value lists, and round brackets ordinary
lists). This definition is kept separate from the surface
representation of a course which is defined to be a noun phrase
with adjectives, postnomiria! cases, etc.. At a more detailed level,
note the special purpose way of specifying a course by its
department juxtaposed with its number (e.g. Computer Science
101) is handled by an alternate pattern for the head of the noun
phrase (dollar signs refer back to the components). Timis allows
the user to say (redundantly) phrases like &amp;quot;CS 101 taught by
Smith&amp;quot;. Note also that the way the department of a course can
appear in the surface representation of a course is specified in
terms of the CourseDepartment component (and hence in terms of
its type, CollegeDeparlinent) rather than directly as an explicit
surface representation. This ensures consistency throughout the
language in what will be recognized as a description of a
department. Coupled with the ability to use general syntactic
descriptors (like NounPhrase in the description of a
SurfaceRepresentation), this can prevent the kind of patchy
coverage prevalent with standard semantic grammar language
definitions.
Subsidiary objects like CollegeDepartment are defined in similar
fashion.
</bodyText>
<figure confidence="0.993711538461538">
fritityName: CollegeDepartment
lype: Enumeration
EnumeratedValues: (
ComputeiScienceDepartment
MathematicsDepartment
NistoryDepartment
)
SurfaceRepresentation:
1Syntaxlype: PatternSet
Patterns: (
[Pattern: (CS 1 Computer Science 1 Comp Sci 1
Value: CompulerSciencenepartment
)
</figure>
<page confidence="0.998086">
213
</page>
<bodyText confidence="0.997617666666667">
CollegeCourse will also be inyulved in higher•Iovel entities of our
restricted domain such as a commancl to the data base syst;:m to
enrol a student in a course.
</bodyText>
<figure confidence="0.990094466666667">
IntityMame: InrelCoimeand
!you: St.&apos;ectured
Components: (
1.ComponentHamn: rnrollee
type: CollogeStndent.
[ComponentName: Unrolln
Typo: Collogi:Course
SurfoceRepresentatinn:
1Syntaxtype: imporativeC4serrame
HeaJ: (enrol 1 register 1 include I • •
Hirect.Clbject: (iEnrollee)
Cases: (
[Preposition: (in 1 into 1 ...)
Component: tnrolln
1
</figure>
<bodyText confidence="0.9870184">
These examples also show how all information about an entity,
concerning both fundamental structure and surface
representation, is grouped together and integrated. This supports
Lie claim that entity-oriented language definition makes it easier to
determine whether a language definition is complete.
</bodyText>
<sectionHeader confidence="0.764085" genericHeader="method">
3. Control Structure for a Robust Entity-
Oriented Parser
</sectionHeader>
<bodyText confidence="0.996544125">
The potential advantages of an entity-oriented approach from
the point of view of robustness in the face of ungrammatical input
were outlined in the introduction. To exploit this potential while
maintaining efficiency in parsing grammatical input, special
attention must be paid to the control structure of the parser used.
Desirable characteristics fur the control structure of any parser
capable of handling ungrammatical as well as grammatical input
include:
</bodyText>
<listItem confidence="0.9929495">
• the control structure allows grammatical input to be parsed
straightforwardly without considering any of the possible
grammatical deviations that could occur;
• the control structure enables progressively higher degrees of
grammatical deviation to be conaidel ed when the input does
not satisfy grammatical expectations;
• the control structure allows simpler deviations to be
considered before more complex deviations.
</listItem>
<bodyText confidence="0.999981956521739">
The first two points are self-evident, but the ttiird may require
sores explanation. The problem it addresses arises particularly
when there are several alternative parses under consideration. In
such cases, it is important to prevent the parser from consclering
drastic deviations in one branch of the parse before considering
simple ones in the other. For im-.:ance, the parser should not start
hypothesizing missing words in one branch when a simple spelling
correction in another branch would allow the parse to go through.
We have designed a parser control structure for use in entity-
oriented posing which ilas all the characteristics listed above.
This control structure operates through an agenda mechanism.
Each item of the agenda represents a difiermit continuation of the
parse, i.e. a partial parse plus a specification of what to do next to
continue that partial parse. With each continuation is associated
an integer flexibility level that represents the degree of
grammatical deviation implied by the continuation. That is, the
flexibility level represents the degree of grammatical deviation in
the input it the continuation were to produce a complete parse&apos;
without finding any more deviation. Continuations with a lower
flexibility are run before continuations with a higher flexibility level.
Once a complete parse has been obtained, continuations with a
flexibility level higher than that of the continuation which resulted
in the parse are abandoned. This means that the agenda
mechanism never activates any continuations with a flexibility
level higher than the level representing the lowest level of
grammatical deviation necessary to account for the input. Thus
effort is not wasted exploring more exotic grammatical deviations
when the input can be accounted for by simpler ones. This shows
that the parser has the first two of the characteristics listed above.
In addition to taking care of alternatives at different flexibility
levels, this control structure also handles the more usual kind of
alternatives faced by parsers — those representing alternative
parses due to local ambiguity in the input. Whenever such an
ambiguity arises, the control structure duplicates the relevant
continuation as many times as there are ambiguous alternatives,
giving each of the duplicated continuations the same flexibility
level. From there on, the same agenda mechanism used for the
various flexibility levels will keep each of the ambiguous
alternatives separate and ensure that all are investigated (as long
as their flexibility level is not too high). Integrating the treatment of
the normal kind of ambiguities with the treatment of alternative
ways of handling grammatical deviations ensures that the level of
grammatical deviation under consideration can be kept the same
in locally ambiguous branches of a parse. This fulfills the third
characteristic listed above.
Flexibility levels are additive, i.e. if some grammatical deviation
has already been found in the input, then finding a new one will
raise the flexibility level of the continuation concerned to the sum
of the flexibility levels involved. This ensures a relatively high
flexibility level and thus a relatively low likelihood of activation for
continuations in which combinations of deviations are being
postulated to account for the input.
Since space is limited, we cannot go into the implementation of
this control structure. However, it is possible to give a brief
description of the control structure primitives used in
programming the parser. Recall first that the kind of entity.
oriented parser we have been discussing consists of a collection
of recognition strategies. The more specific strategies exploit the
idiosyncratic features of the entities/construction types they are
specific to, while the more general strategies apply to wider
classes of entities and depend on more universal characteristics.
In either case, the strategies are pieces of (Lisp) program rather
than more abstract rules or networks. Integration of such
strategies with the general scheme of flexibility levels described
above is made straightforward through a special split function
which the control structure supports as a primitive. This split
function allows the programmer of a strategy to specify one or
more alternative continuations from any point in the strategy and
to associate a different flexibility increment with each of them.
</bodyText>
<page confidence="0.997106">
214
</page>
<bodyText confidence="0.9997357">
The implementation of this statement takes care of restarting each
of the alternative continuations at the appropriate time and with
the appropriate local context.
Some examples should make this account of the control
structure much clearer. The examples will also present some
specific parsing strategies and show how they use the split
function described above. These strategies are designed to effect
robust recognition of extragrammatical input and efficient
recognition of grammatical input by exploiting entity-oriented
language definitions like those in the previous section.
</bodyText>
<sectionHeader confidence="0.98948" genericHeader="method">
4. Example Parses
</sectionHeader>
<bodyText confidence="0.973572666666667">
Let us examine first how a simple data base command like:
Enrol Susan Smith in CS 101
might be parsed with the control structure and language
definitions presented in the two previous sections. We start off
with the top-level parsing strategy, RecognizeAnyEntity. This
strategy first tries to identify a top-level domain entity (in this case
a data base command) that might account for the entire input. It
does this in a bottom-up manner by indexing from words in the
input to those entities that they could appear in. In this case, the
best indexer is the first word, &apos;enrol&apos;, which indexes
EnrolCommand. In general. however, the best indexer need not
be the first word of the input and we need to consider all words,
thus raising the potential of indexing more than one entity. In our
example, we would also index CollegeStudent, CollegeCourse,
and CollegeDepartment. However, these are not top-level domain
entities and are subsumed by EnrolCommand, and so can be
ignored in favour of it.
Once EnrolCommand has been identified as an entity that might
account for the input, RecognizeAnyEntity initiates an attempt to
recognize it. Since EnrolCommand is listed as an imperative case
frame, this task is handled by the ImperativeCaseFrame
recognizer strategy. In contrast to the bottom-up approach of
RecognizeAnyEntity, this strategy tackles its more specific task in
a top-down manner using the case frame recognition algorithm
developed for the CASPAR parser 181. In particular, the strategy
will match the case frame header and the preposition &apos;in&apos;, and
initiate recognitions of fillers of its direct object case and its case
marked by &apos;in&apos;. These subgoals are to recognize a CollegeStudent
to fill the Enrollee case on the input segment &amp;quot;Susan Smith&amp;quot; and
a CollegeCourse to fill the EnrolIn case on the segment &amp;quot;CS 101&amp;quot;.
Both of these recognitions will be successful, hence causing the
ImperativeCaseFrame recognizer to succeed and hence the entire
recognition. The resulting parse would be:
</bodyText>
<figure confidence="0.729269571428571">
[Instance0f: EnrolCommand
Enrollee: [Instance0f: CollegeStudent
FirstNames: (Susan)
Surname: Smith
EnrolIn: [Instance0f: CollegeCourse
CourseDepartment: ComputerScienceDepartment
CourseNumber: 101
</figure>
<bodyText confidence="0.9490785">
Note how this parse result is expressed in terms of the underlying
structural representation used in the entity definitions without the
need for a separate semantic interpretation step.
The last example was completely grammatical and so did not
require any flexibility. After an initial bottom-up step to find a
dominant entity, that entity was recognized in a highly efficient
top-down manner. For an example involving input that is
ungrammatical (as far as the parser is concerned), consider:
</bodyText>
<subsubsectionHeader confidence="0.625791">
Place Susan Smith in computer science for freshmen
</subsubsectionHeader>
<bodyText confidence="0.998057820000001">
There are two problems here: we assume that the user intended
&apos;place&apos; as a synonym for &apos;enrol&apos;, but that it happens not to be in the
system&apos;s vocabulary; the user has also shortened the
grammatically acceptable phrase, &apos;the computer science course
for freshmen&apos;, to an equivalent phrase not covered by the surface
representation for CollegeCourse as defined earlier. Since &apos;place&apos;
is not a synonym for &apos;enrol in the language as presently defined,
the RecognizeAnyEntity strategy cannot index EnrolCommand
from it and hence cannot (as it did in the previous example) initiate
a top-down recognition of the entire input.
To deal with such eventualities, RecognizeAnyEntity executes a
split statement specifying two continuations immediately after it
has found all the entities indexed by the input. The first
continuation has a zero flexibility level increment. It looks at the
indexed entities to see if one subsumes all the others. If it finds
one, it attempts a top-down recognition as described in the
previous example. If it cannot find one, or if it does and the top-
down recognition fails, then the continuation itself fails. The
second continuation has a positive flexibility increment and
follows a more robust bottom-up approach described below. This
second continuation was established in the previous example too,
but was never activated since a complete parse was found at the
zero flexibility level. So we did not mention it. In the present
example, the first continuation fails since there is no subsuming
entity, and so the second continuation gets a chance to run.
Instead of insisting on identifying a single top-level entity, this
second continuation attempts to recognize all of the entities that
are indexed in the hope of later being able to piece together the
various fragmentary recognitions that result. The entities directly
indexed are CollegeStudent by &amp;quot;Susan&amp;quot; and &amp;quot;Smith&amp;quot;,2
CollegeDepartment by &amp;quot;computer&amp;quot; and &amp;quot;science&amp;quot;, and
CollegeClass by &amp;quot;freshmen&amp;quot;. So a top-down attempt is made to
recognize each of these entities. We can assume these goals are
fulfilled by simple top-down strategies, appropriate to the
SurfaceRepresentation of the corresponding entities, and
operating with no flexibility level increment.
Having recognized the low-level fragments, the second
continuation of RecognizeAnyEntity now attempts to unify them
into larger fragments, with the ultimate goal of unifying them into a
description of a single entity that spans the whole input. To do
this, it takes adjacent fragments pairwise and looks for entities of
which they are both components, and then tries to recognize the
subsuming entity in the spanning segment. The two pairs here are
CollegeStudent and CollegeDepartment (subsumed by
CollegeStudent) and CollegeDepartment and CollegeClass
(subsumed by CollegeCourse).
To investigate the second of these pairings, RecognizeAnyEntity
would try to recognize a CollegeCourse in the spanning segment
&apos;computer science for freshmen&apos; using an elevated level of
flexibility. This goal would be handled, just like all recognitions of
</bodyText>
<page confidence="0.997455">
215
</page>
<bodyText confidence="0.999179025">
CollegeCourse, by the NominalCaseFrame recognizer. With no
flexibility increment, this strategy fails because the head noun is
missing. However. with another flexibility increment, the
recognition can go through with the CollegeDepartment being
treated as an adjective and the C&apos;ollegeClass being treated as a
postnominal case — it has the right case marker, &amp;quot;for&amp;quot;, and the
adjective and post-nominal are in the right order. This successful
fragment unification leaves two fragments to unify — the old
CollegeStudent and the newly derived CollegeCourse.
There are several ways of unifying a CollegeStudent and a
CollegeCourse — either could subsume the other, or they could
form the parameters to one of three database modification
commands: EnrolCommand, WithdrawCommand, and
TransferCommand (with the obvious interpretations). Since the
commands are higher level entities than CollegeStudent and
CollegeCourse, they would be preferred as top-level fragment
unifiers. We can also rule out TransferCommand in favour of the
first two because it requires two courses and we only have one. In
addition, a recognition of EnrolCommand would succeed at a
lower flexibility increment than WithdrawCommand,3 since the
preposition &apos;in&apos; that marks the CollegeCourse in the input is the
correct marker of the EnrolIn case of EnrolCommand, but is not
the appropriate marker for WithdrawFrorn, the course-containing
case of WithdrawCommand. Thus a fragment unification based
on EnrolCommand would be preferred. Also, the alternate path of
fragment amalgamation — combining CollegeStudent and
CollegeDepartment into CollegeStudent and then combining
CoilegeStudent and CollegeCourse — that we left pending above
cannot lead to a complete instantiation of a top-level database
command. So RecognizeAnyEntity will be in a position to assume
that the user really Intended the EnrolCommand.
Since this recognition involved several significant assumptions,
we would need to use focused interaction techniques [7] to
present the interpretation to the user for approval before acting on
it. Note that if the user does approve it, it should be possible (with
further approval) to add &apos;place&apos; to the vocabulary as a synonym for
&apos;enrol&apos; since &apos;place&apos; was an unrecognized word in the surface
position where &apos;enrol&apos; should have been.
For a final example. let us examine an extragrammatical input
that involves continuations at several different flexibility levels:
</bodyText>
<subsectionHeader confidence="0.586461">
Transfer Smith from Compter Science 101 Economics 203
</subsectionHeader>
<bodyText confidence="0.992433933333333">
The problems here are that &apos;Computer&apos; has been misspelt and the
preposition &apos;to&apos; is missing from before &apos;Economics&apos;. The example
is similar to the first one in that RecognizeAnyEntity is able to
identify a top-level entity to be recognized top-down, in this case,
TransferCommand. Like EnrolCommand, TransferCommand is an
imperative case frame, and so the task of recognizing it is handled
by the ImperativeCaseFrarne strategy. This strategy can find the
preposition &apos;from&apos;, and so can Initiate the appropriate recognitions
for fillers of the Oi ttOf Course and Student cases. The recognition
for the student case succeeds without trouble, but the recognition
for the OutOlCourse case requires a spelling correction.
2We assume we have a complete listing of students and so can index from their
names.
Whenever a top-down parsing strategy fails to verify that an
input word is in a specific lexical class, there is the possibility that
the word that failed is a misspelling of a word that would have
succeeded. In such cases, the lexical lookup mechanism
executes a split statement.4 A zero increment branch fails
immediately, but a second branch with a small positive increment
tries spelling correction against the words in the predicted lexical
class. If the correction fails, this second branch fails, but if the
correction succeeds, the branch succeeds also. hi our example,
the continuation involving the second branch of the lexical lookup
is highest on the agenda after the primary branch has failed. In
particular, it is higher than the second branch of
RecognizeAnyEntity described in the previous example, since the
flexibility level increment for spelling correction is small. This
means that the lexical lookup is continued with a spelling
correction, thus resolving the problem. Note also that since the
spelling correction is only attempted within the context of
recognizing a CollegeCourse — the filler of OutOfCourse — the
target words are limited to course names. This means spelling
correction is much more accurate and efficient than if correction
were attempted against the whole dictionary.
After the OutOfCourse and Student cases have been
successfully filled, the ImperativeCaseFrame strategy can do no
more without a flexibility level increment. But it has not filled all
the required cases of TransferCommand, and it has not used up
all the input it was given, so it splits. and fails at the zero-level
flexibility increment. However, in a continuation with a positive
flexibility level increment, it is able to attempt recognition of cases
without their marking prepositions. Assuming the sum of this
increment and the spelling correction increment are still less than
the increment associated with the second branch of
RecognizeAnyEntity. this continuation would be the next one run.
In this continuation, the ImperativeCaseFrameRecognizer
attempts to match unparsed segments of the input against unfilled
cases. There is only one of each, and the resulting attempt to
recognize &apos;Economics 203&apos; as the filler of IntoCourse succeeds
Straightforwardly. Now all required cases are filled and all input is
accounted for, so the ImperativeCaseFrame strategy and hence
the whole parse succeeds with the correct result.
For the example just presented, obtaining the ideal behaviour
depends on careful choice of the flexibility level increments.
There is a danger here that the performance of the parser as a
whole will be dependent on iterative tuning of these increments,
and may become unstable with even small changes in the
increments. It is too early yet to say how easy it will be to manage
this problem, but we plan to pay close attention to it as the parser
comes into operation.
</bodyText>
<footnote confidence="0.977833625">
3This relatively fine distinction between EnrolCommand and
Withdi awComniand, based on the appropriateness of the preposition &apos;in&apos;, is
problematical in that it assumes that the flexibility level would be incremented in
very fine grained steps. If that was impractical, the final outcome of the parse
would be ambiguous between an EnrolCommand and a WithdrawCommand and
the user would have to be asked to make the discnmination.
41f this causes too many splits, an alternative is only to do the split when the
input word in question is not in the system&apos;s lexicon at all.
</footnote>
<page confidence="0.998818">
216
</page>
<sectionHeader confidence="0.992136" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.999955">
Entity-oriented parsing has several advantages as a basis •for
language recognition in restricted domain natural language
interfaces. Like techniques based on semantic grammar, it
exploits limited domain semantics through a series of domain-
specific entity types. However, because of its suitability for
fragmentary recognition and its ability to accommodate multiple
construction-specific parsing strategies, it has the potential for
greater robuetness in the face of extragrammatical input than the
usual semantic grammar techniques. In this way, it more closely
resembles conceptual or case-frame parsing techniques.
Moreover, entity-oriented parsing offers advantages tor language
definition because of the integration of structure! anJ surfaoe
representation information and the ability to repo sent surface
information in the form most convenient to dove construction-
specific recognition strategies directly.
A pilot implementation of an entity-oriented parser has been
completed and provides preliminary support for our claims.
However, a more rigorous test of the entity-oriented approach
roust wait for the more complete implementation currently being
undertaken. The agenda-style control structure we plan to uso in
this implementation is described above, along with some parsing
strategies it will employ and some worked examples of the
strategies and control structure in action.
</bodyText>
<sectionHeader confidence="0.987496" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999593142857143">
[he ideas in this paper benefited considerably from discussions
with other members of the Multipar group at Carnegie-Mellon
Computer Science Department, particularly Jaime Carbonell, Jill
Fain, and Steve Minton. Steve Minton was a co-designer of tho
control str ucti Ire presented above, and also found an efficient way
to implement the split function def.-cribed in connection with that
control structure.
</bodyText>
<sectionHeader confidence="0.941334" genericHeader="references">
Fiferences
</sectionHeader>
<reference confidence="0.999948694444444">
1. Brown, J. S. and Burton. R. R. Multiple Representations of
Knowledge for F utoriai Reasoning. In Ropresontation and
LIndl-irstanding, Be-brow, D. G. and Collins, A., Ed., Academic
Press, New York, 1975, pp. 311-349.
2. Burton, R. R. Semantic Grammar: An Engineering Technique
for Constructing Natural Language Understanding Systems. BM
Report :3453, Dolt, Beranek, arid Newman, Inc., Cambridge, Mass.,
December, 1976.
3. Carbonell, J. (3., Boggs, W. M., Mauldin, M. L., and Anick, P. G.
The X.CALIBt IR Project: A Natural Language Interface to Expert
Systems. Proc. Eighth Int. Jt. Conf. on Artificial Intelligence,
Karlsruhe, August, 1083.
4. Carbonell, J. G. and Hayes, P. J. &amp;quot;Recovery Strategies for
Parsing Extragrarnmatical Language.&amp;quot; Computational Linguistics
10 (1984).
5. Carbonell, J. G. and Hayes, P. J. Robust Parsing Using
Multiple Construction-Specific Strategies. In Natural Language
Parsing Systems, L. Bole, Ed.,Springer-Verlag, 1984.
6. Grosz, B. J. TEAM: A Transportable Natural Language
Interface System. Proc. Conf. on Applied Natural Language
Processing, Santa Monica, February, 1983.
7. Hayes P. J. A Construction Specific Approach to Focused
Ii teraction in Flexible Parsing. Proc. of 19th Annual Meeting of
the Assoc. for Commit. Tiny., Stanford University, June, 1981, pp.
149-152.
8. Hayes, P. J. and Carbonell. J. G. Multi-Strategy Parsing and its
Role in Robust Man- Machine Communicatioe. Carnegie-Mellon
1.1iiiversity Computer Sou:lice Department. May, 1981.
9. Hendrix, 0. G. Human Engineering for Applied Natural
Language Processing. Proc.. Fifth Int. Jt. Conf. on Artificial
Intelligence, T.1i T. 1077, pp. 183-191.
TO. Ries1)eck. C. K. and Scheok. R. C. Comprehension by
COMpULor: Fapectation-Dase,lAnaly:&apos;,is of Sentences in Context.
fech. Rept. /8, Computer Science Dept., Yale University, 1076.
1 1. Mks, Y. A. Prefereace Semantics. In Formal Semantics of
Natural Language, Keenan, Ecl..Cambridge University Press, 1975.
</reference>
<page confidence="0.998404">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989890">
<title confidence="0.999649">Entity-Oriented Parsing</title>
<author confidence="0.999994">Philip J Hayes</author>
<affiliation confidence="0.999997">Computer Science Department, Carnegie-Mellon University</affiliation>
<address confidence="0.999751">Pittsburgh, PA 13213, USA</address>
<abstract confidence="0.999305733333333">An entity-oriented approach to restricted-domain parsing is proposed. In this approach, the definitions of the structure and surface representation of domain entities are grouped. together. Like semantic grammar, this allows easy exploitation of limited domain semantics. In addition, it facilitates fragmentary recognition and the use of multiple parsing strategies, and so is particularly useful for robust recognition of extragrammatical input. Several advantages from the point of view of language definition are also noted. Representative samples from an entity-oriented language definition are presented, along with a control structure for an entity-oriented parser, some parsing strategies that use the control structure, and worked examples of parses. A parser incorporating the control structure and the parsing strategies is currently under implementation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R R</author>
</authors>
<title>Multiple Representations of Knowledge for F utoriai Reasoning. In Ropresontation</title>
<date>1975</date>
<pages>311--349</pages>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="1751" citStr="[1, 2, 9]" startWordPosition="251" endWordPosition="253">ng: The simplifications arise because: 1. the systems operate within a highly restricted domain of discourse, so that a precise set of object types can be established, and marry of the ambiguities that come up in more general natural language processing can be ignored or constrained away; 2. even within the restricted domain of discourse, a natural language interface system only needs to recognize a limited subset of all the things that could be said -- the subset that its back-end can respond to. The most commonly used technique to exploit these limited domain constraints is semantic grammar [1, 2, 9] in which semantically defined categories (such as &lt;ship&gt; or &lt;shipattribute&gt;) are used in a grammar (usually ATN based) in place of syntactic categories (such as &lt;noun&gt; or &lt;adjective&gt;). While semantic grammar has been very successful in exploiting limited domain coestrainte to reduce ambiguities and eliminate spurious parses of grammatical input, it still suffers from the fragility in the face of extragrammatical input characteristic of parsing based on transition nets [41. Also. thn task of restricted-domain language definition is typically difficult in interfaces based on semantic grammar, i</context>
</contexts>
<marker>1.</marker>
<rawString>Brown, J. S. and Burton. R. R. Multiple Representations of Knowledge for F utoriai Reasoning. In Ropresontation and LIndl-irstanding, Be-brow, D. G. and Collins, A., Ed., Academic Press, New York, 1975, pp. 311-349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R R Burton</author>
</authors>
<title>Semantic Grammar: An Engineering Technique for Constructing Natural Language Understanding Systems.</title>
<date>1976</date>
<tech>BM Report :3453,</tech>
<location>Dolt, Beranek, arid Newman, Inc., Cambridge, Mass.,</location>
<contexts>
<context position="1751" citStr="[1, 2, 9]" startWordPosition="251" endWordPosition="253">ng: The simplifications arise because: 1. the systems operate within a highly restricted domain of discourse, so that a precise set of object types can be established, and marry of the ambiguities that come up in more general natural language processing can be ignored or constrained away; 2. even within the restricted domain of discourse, a natural language interface system only needs to recognize a limited subset of all the things that could be said -- the subset that its back-end can respond to. The most commonly used technique to exploit these limited domain constraints is semantic grammar [1, 2, 9] in which semantically defined categories (such as &lt;ship&gt; or &lt;shipattribute&gt;) are used in a grammar (usually ATN based) in place of syntactic categories (such as &lt;noun&gt; or &lt;adjective&gt;). While semantic grammar has been very successful in exploiting limited domain coestrainte to reduce ambiguities and eliminate spurious parses of grammatical input, it still suffers from the fragility in the face of extragrammatical input characteristic of parsing based on transition nets [41. Also. thn task of restricted-domain language definition is typically difficult in interfaces based on semantic grammar, i</context>
</contexts>
<marker>2.</marker>
<rawString>Burton, R. R. Semantic Grammar: An Engineering Technique for Constructing Natural Language Understanding Systems. BM Report :3453, Dolt, Beranek, arid Newman, Inc., Cambridge, Mass., December, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
</authors>
<title>The X.CALIBt IR Project: A Natural Language Interface to Expert Systems.</title>
<date></date>
<booktitle>Proc. Eighth Int. Jt. Conf. on Artificial Intelligence,</booktitle>
<pages>1083</pages>
<location>Karlsruhe,</location>
<marker>3.</marker>
<rawString>Carbonell, J. (3., Boggs, W. M., Mauldin, M. L., and Anick, P. G. The X.CALIBt IR Project: A Natural Language Interface to Expert Systems. Proc. Eighth Int. Jt. Conf. on Artificial Intelligence, Karlsruhe, August, 1083.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
<author>P J Hayes</author>
</authors>
<title>Recovery Strategies for Parsing Extragrarnmatical Language.&amp;quot;</title>
<date>1984</date>
<journal>Computational Linguistics</journal>
<volume>10</volume>
<marker>4.</marker>
<rawString>Carbonell, J. G. and Hayes, P. J. &amp;quot;Recovery Strategies for Parsing Extragrarnmatical Language.&amp;quot; Computational Linguistics 10 (1984).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
<author>P J Hayes</author>
</authors>
<title>Robust Parsing Using Multiple Construction-Specific Strategies.</title>
<date>1984</date>
<booktitle>In Natural Language Parsing Systems,</booktitle>
<location>L. Bole, Ed.,Springer-Verlag,</location>
<contexts>
<context position="5170" citStr="[5, 8]" startWordPosition="755" endWordPosition="756">r parsing robustness than more traditional semantic grammar techniques for two major reasons: • The individual definition of all domain entities facilitates their independent recoenition. Asetnesig there is appropriate indexing of entities through lexical items that might appear in a surface description of them, this recognition can be done hottormup, thus making possible recognition of elliptical, tragmemary, or partially incomprehr_i..sible input. The same dntinitions can also tie used in a I/lore efficient top-down inutun-3 when the input conforms to the system&apos;s expectations. Tiecent work [5, 8] has suggested the usefulness of multiple constructon.specific recognition stratcgies for restricted domain parsing, par ticularly for dealing with extraqraramatica! input. lie individual entity riefinitions form an ideai framework amend which to organize the multiple 212 strategies. In particular, each definitical can specify which strategies are applicable to recognizing it. Of course, this only provides a framework for robust recognitior3, the robustness achieved still depends on the quality of the actual recognition strategies used. The advantages of entity-oriented parsing for language de</context>
</contexts>
<marker>5.</marker>
<rawString>Carbonell, J. G. and Hayes, P. J. Robust Parsing Using Multiple Construction-Specific Strategies. In Natural Language Parsing Systems, L. Bole, Ed.,Springer-Verlag, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
</authors>
<title>TEAM: A Transportable Natural Language Interface System.</title>
<date>1983</date>
<booktitle>Proc. Conf. on Applied Natural Language Processing,</booktitle>
<location>Santa Monica,</location>
<contexts>
<context position="2514" citStr="[6]" startWordPosition="368" endWordPosition="368">s &lt;noun&gt; or &lt;adjective&gt;). While semantic grammar has been very successful in exploiting limited domain coestrainte to reduce ambiguities and eliminate spurious parses of grammatical input, it still suffers from the fragility in the face of extragrammatical input characteristic of parsing based on transition nets [41. Also. thn task of restricted-domain language definition is typically difficult in interfaces based on semantic grammar, in part because Um grammar definition formalism is not well integrated with the method of defining the object and actions of the domain of discourse (though see [6]). 1 This research wns sponsored by the Air !Wee Office of Scient,fic flesearch under Contract /11-0:311-82-0219 This paper proposes an alternative approach to restricted domain language recognition called entity-odor:tad parsing. Entity-oriented parsing uses the same notion of semanticallydefined categories mm, snmantic grammar, but does net embed these categories in a grammatical structure designed for syntactic recognition. Instead, a scheme more reminiscent of conceptual or case-frame parsers [3, 10, H] is employed. An entity-oriented parser operates from a collection of definitions of the</context>
</contexts>
<marker>6.</marker>
<rawString>Grosz, B. J. TEAM: A Transportable Natural Language Interface System. Proc. Conf. on Applied Natural Language Processing, Santa Monica, February, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
</authors>
<title>A Construction Specific Approach to Focused Ii teraction in Flexible Parsing.</title>
<date>1981</date>
<booktitle>Proc. of 19th Annual Meeting of the Assoc. for Commit. Tiny.,</booktitle>
<pages>149--152</pages>
<institution>Stanford University,</institution>
<contexts>
<context position="24704" citStr="[7]" startWordPosition="3642" endWordPosition="3642">e-containing case of WithdrawCommand. Thus a fragment unification based on EnrolCommand would be preferred. Also, the alternate path of fragment amalgamation — combining CollegeStudent and CollegeDepartment into CollegeStudent and then combining CoilegeStudent and CollegeCourse — that we left pending above cannot lead to a complete instantiation of a top-level database command. So RecognizeAnyEntity will be in a position to assume that the user really Intended the EnrolCommand. Since this recognition involved several significant assumptions, we would need to use focused interaction techniques [7] to present the interpretation to the user for approval before acting on it. Note that if the user does approve it, it should be possible (with further approval) to add &apos;place&apos; to the vocabulary as a synonym for &apos;enrol&apos; since &apos;place&apos; was an unrecognized word in the surface position where &apos;enrol&apos; should have been. For a final example. let us examine an extragrammatical input that involves continuations at several different flexibility levels: Transfer Smith from Compter Science 101 Economics 203 The problems here are that &apos;Computer&apos; has been misspelt and the preposition &apos;to&apos; is missing from bef</context>
</contexts>
<marker>7.</marker>
<rawString>Hayes P. J. A Construction Specific Approach to Focused Ii teraction in Flexible Parsing. Proc. of 19th Annual Meeting of the Assoc. for Commit. Tiny., Stanford University, June, 1981, pp. 149-152.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J G</author>
</authors>
<title>Multi-Strategy Parsing and its Role in Robust Man- Machine Communicatioe. Carnegie-Mellon</title>
<contexts>
<context position="5170" citStr="[5, 8]" startWordPosition="755" endWordPosition="756">r parsing robustness than more traditional semantic grammar techniques for two major reasons: • The individual definition of all domain entities facilitates their independent recoenition. Asetnesig there is appropriate indexing of entities through lexical items that might appear in a surface description of them, this recognition can be done hottormup, thus making possible recognition of elliptical, tragmemary, or partially incomprehr_i..sible input. The same dntinitions can also tie used in a I/lore efficient top-down inutun-3 when the input conforms to the system&apos;s expectations. Tiecent work [5, 8] has suggested the usefulness of multiple constructon.specific recognition stratcgies for restricted domain parsing, par ticularly for dealing with extraqraramatica! input. lie individual entity riefinitions form an ideai framework amend which to organize the multiple 212 strategies. In particular, each definitical can specify which strategies are applicable to recognizing it. Of course, this only provides a framework for robust recognitior3, the robustness achieved still depends on the quality of the actual recognition strategies used. The advantages of entity-oriented parsing for language de</context>
</contexts>
<marker>8.</marker>
<rawString>Hayes, P. J. and Carbonell. J. G. Multi-Strategy Parsing and its Role in Robust Man- Machine Communicatioe. Carnegie-Mellon</rawString>
</citation>
<citation valid="false">
<date>1981</date>
<institution>1iiiversity Computer Sou:lice Department.</institution>
<contexts>
<context position="1751" citStr="[1, 2, 9]" startWordPosition="251" endWordPosition="253">ng: The simplifications arise because: 1. the systems operate within a highly restricted domain of discourse, so that a precise set of object types can be established, and marry of the ambiguities that come up in more general natural language processing can be ignored or constrained away; 2. even within the restricted domain of discourse, a natural language interface system only needs to recognize a limited subset of all the things that could be said -- the subset that its back-end can respond to. The most commonly used technique to exploit these limited domain constraints is semantic grammar [1, 2, 9] in which semantically defined categories (such as &lt;ship&gt; or &lt;shipattribute&gt;) are used in a grammar (usually ATN based) in place of syntactic categories (such as &lt;noun&gt; or &lt;adjective&gt;). While semantic grammar has been very successful in exploiting limited domain coestrainte to reduce ambiguities and eliminate spurious parses of grammatical input, it still suffers from the fragility in the face of extragrammatical input characteristic of parsing based on transition nets [41. Also. thn task of restricted-domain language definition is typically difficult in interfaces based on semantic grammar, i</context>
</contexts>
<marker>1.</marker>
<rawString>1iiiversity Computer Sou:lice Department. May, 1981.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hendrix</author>
</authors>
<title>Human Engineering for Applied Natural Language Processing.</title>
<date>1975</date>
<journal>Fapectation-Dase,lAnaly:&apos;,is of Sentences in Context. fech. Rept.</journal>
<booktitle>Proc.. Fifth Int. Jt. Conf. on Artificial Intelligence, T.1i T.</booktitle>
<volume>1077</volume>
<pages>183--191</pages>
<publisher>Ecl..Cambridge University Press,</publisher>
<institution>Computer Science Dept., Yale University,</institution>
<contexts>
<context position="1751" citStr="[1, 2, 9]" startWordPosition="251" endWordPosition="253">ng: The simplifications arise because: 1. the systems operate within a highly restricted domain of discourse, so that a precise set of object types can be established, and marry of the ambiguities that come up in more general natural language processing can be ignored or constrained away; 2. even within the restricted domain of discourse, a natural language interface system only needs to recognize a limited subset of all the things that could be said -- the subset that its back-end can respond to. The most commonly used technique to exploit these limited domain constraints is semantic grammar [1, 2, 9] in which semantically defined categories (such as &lt;ship&gt; or &lt;shipattribute&gt;) are used in a grammar (usually ATN based) in place of syntactic categories (such as &lt;noun&gt; or &lt;adjective&gt;). While semantic grammar has been very successful in exploiting limited domain coestrainte to reduce ambiguities and eliminate spurious parses of grammatical input, it still suffers from the fragility in the face of extragrammatical input characteristic of parsing based on transition nets [41. Also. thn task of restricted-domain language definition is typically difficult in interfaces based on semantic grammar, i</context>
</contexts>
<marker>9.</marker>
<rawString>Hendrix, 0. G. Human Engineering for Applied Natural Language Processing. Proc.. Fifth Int. Jt. Conf. on Artificial Intelligence, T.1i T. 1077, pp. 183-191. TO. Ries1)eck. C. K. and Scheok. R. C. Comprehension by COMpULor: Fapectation-Dase,lAnaly:&apos;,is of Sentences in Context. fech. Rept. /8, Computer Science Dept., Yale University, 1076. 1 1. Mks, Y. A. Prefereace Semantics. In Formal Semantics of Natural Language, Keenan, Ecl..Cambridge University Press, 1975.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>