<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018657">
<title confidence="0.997319">
Head-Driven Generation with HPSG
</title>
<author confidence="0.999394">
Graham Wilcock*
</author>
<affiliation confidence="0.989452">
Centre for Computational Linguistics
University of Manchester Institute
of Science and Technology
</affiliation>
<address confidence="0.9445605">
PO Box 88, Manchester M60 1QD
United Kingdom
</address>
<email confidence="0.927114">
grahamOccl.umist.ac.uk
</email>
<author confidence="0.990389">
Yuji Matsumoto
</author>
<affiliation confidence="0.9947935">
Graduate School of Information Science
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.647941">
8916-5 Takayama, Ikoma, Nara 630-01
Japan
</address>
<email confidence="0.940395">
matsueis.aist—nara.ac.jp
</email>
<sectionHeader confidence="0.992216" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972444444445">
As HPSG is head-driven, with clear semantic heads,
semantic head-driven generation should be simple.
We adapt van Noord&apos;s Prolog generator for use with
an HPSG grammar in ProFIT. However, quantifiers
and context factors are difficult to include in head-
driven generation. We must adopt recent theoretical
proposals for lexicalized scoping and context. With
these revisions, head-driven generation with HPSG
is not so simple, but it is possible.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977892857143">
A natural approach to generation with Head-driven
Phrase Structure Grammar (Pollard and Sag, 1994)
is to use a head-driven algorithm. HPSG is head-
driven not only syntactically, but also semantically.
While the Head Feature Principle requires identity
of major syntactic features between a phrase and
its syntactic head daughter, the Semantics Principle
(in various formulations) requires identity of major
semantic features between a phrase and its seman-
tic head daughter. Since the semantic head is very
clearly defined in HPSG, semantic head-driven gen-
eration should be easy to implement.
Efficient head-driven generation algorithms, such
as BUG, SHD and CSHD, have been presented as
Prolog algorithms for use with DCG grammars. In
Section 2 we briefly describe how an HPSG grammar
can be implemented as a PSG with typed feature
structures, which can be compiled into a DCG by
the ProFIT system. In this way, HPSG grammars
can be used with the existing Prolog algorithms.
Such a combination of head-driven grammar and
head-driven generator works well if the semantics is
strictly head-driven. However, in Section 3 we show
that if we implement the HPSG textbook semantics,
with quantifier storage and contextual background
conditions, the notion of semantic head becomes un-
clear and this approach no longer works. In fact,
head-driven generation of even simple phrases such
</bodyText>
<listItem confidence="0.890359">
• Visiting researcher of Sharp Corporation, Japan.
</listItem>
<bodyText confidence="0.9785436">
as &amp;quot;Kim walks&amp;quot; (Chapter 1 of the HPSG textbook)
raises fundamental difficulties.
To use a semantic head-driven algorithm, we must
adopt recent HPSG proposals to put quantifier store
and contextual background inside semantic heads.
We summarize these proposals in Section 4, and
show how they can be implemented in the ProFIT
HPSG grammar. We conclude that head-driven gen-
eration with HPSG is possible, but there are some
difficulties in implementing this approach.
</bodyText>
<sectionHeader confidence="0.986436" genericHeader="method">
2 Head-Driven Generation
</sectionHeader>
<bodyText confidence="0.99998775">
We assume that generation starts from logical forms,
which may be represented for HPSG as typed feature
structures. Logical form is not a separate linguistic
level in HPSG, but is equated with semantic content.
In this section, we take the starting logical form for
generation to be a semantic feature structure which
will be identical to the CONTENT feature of the
top-level HPSG sign to be generated.
</bodyText>
<subsectionHeader confidence="0.992275">
2.1 Semantic heads
</subsectionHeader>
<bodyText confidence="0.994186470588235">
Head-driven generation algorithms are based on the
idea that most grammar rules have a semantic head
daughter whose logical form is identical to the logi-
cal form of the mother. The bottom-up generation
(BUG) algorithm of van Noord (1990) requires every
rule to have such a head (except lexical entries). The
semantic head-driven (SHD) algorithm of Shieber et
at. (1990) relaxes this, dividing rules into chain rules
with such a head (processed bottom-up), and non-
chain rules (processed top-down). The chart-based
semantic head-driven (CSHD) algorithm&apos; of Haruno
et al. (1996) increases efficiency by using a chart to
eliminate recomputation of partial results.
Head-driven bottom-up generation is efficient as
it is geared both to the input logical form (head-
driven) and to lexical information (bottom-up). It
is good for HPSG, which is highly lexicalist and has
</bodyText>
<footnote confidence="0.999316666666667">
1For simplicity we illustrate the approach with BUG. A
ProFIT/HPSG framework using the CSHD algorithm is de-
scribed by Wilcock and Matsumoto (1996).
</footnote>
<page confidence="0.986378">
1393
</page>
<figure confidence="0.994345038461538">
&apos;HFP&apos; := synsem!loc!cat!head!HF k
hd_dtr!synsem!loc!cat!head!HF.
&apos;SemP&apos; := synsem!loc!cont!Cont k
hd_dtr!synsem!loc!cont!Cont.
&apos;SemP&apos; (adjunct) := synsem!loc!cont!Cont &amp;
adj_dtr!synsem!loc!cont!Cont.
hd_ph := &lt;hd_ph k C&apos;HFP&apos;
synsem!loc!cat!val!comps![].
hd_nexus_ph := &lt;hd_nexus_ph k Chd_ph &amp; C&apos;SemP&apos;.
hd_subj_ph := &lt;hd_subj_ph &amp; Chd_nexus_ph &amp;
O&apos;VALP&apos;(spr) k CVALP&apos;(comps) &amp;
synsem!loc!cat!val!subj![].
hd_comp_ph := &lt;hd_comp_ph k Chd_nexus_ph k
O&apos;VALP&apos;(subj) k C&apos;VALP&apos;(spr).
Chd_subj_ph &amp; phon!PO-PN &amp;
hd_dtr!(Head &amp;
synsem!loc!cat!val!subj![S]) k
subj_dtr!(Subj &amp; synsem!S)
---&gt; [Head &amp; &lt;phrase &amp; phon!Pl-PN,
Subj &amp; &lt;phrase &amp; phon!PO-P1].
Chd_comp_ph &amp; phon!PO-PN
hd_dtr!(Head &amp;
synsem!loc!cat!val!comps![C]) &amp;
comp_dtrs![Comp &amp; synsem!C]
- [Head &amp; &lt;word &amp; phon!PO-P1,
Comp &amp; &lt;phrase k phon!Pl-PN].
</figure>
<figureCaption confidence="0.999981">
Figure 1: Principles, Phrase Types, Schemata
</figureCaption>
<bodyText confidence="0.9997143">
a clear definition of semantic head: in head-adjunct
phrases, the adjunct daughter is the semantic head;
in other headed phrases, the syntactic head daughter
is the semantic head. In both cases, the Semantics
Principle basically requires the content of the seman-
tic head to be identical to the content of the mother.
If we ignore coordinate structures, and if we equate
logical form with semantic content for now, then all
HPSG grammar rules are SHD chain rules, meeting
the requirement of the BUG algorithm.
</bodyText>
<subsectionHeader confidence="0.997761">
2.2 HPSG in ProFIT
</subsectionHeader>
<bodyText confidence="0.999115483870968">
ProFIT: Prolog with Features, Inheritance and Tem-
plates (Erbach, 1995) is an extension of Prolog which
supports inheritance-based typed feature structures.
The type hierarchy is declared in a signature, which
defines subtypes and appropriate features of every
type. Terms with typed feature structures can then
be used alongside normal terms. Using the signature
declarations, the ProFIT system compiles the typed
feature structures into normal Prolog terms, which
can be compiled by the Prolog system.
Figure 1 shows some implementation details. We
use ProFIT templates (defined by &apos;:=&apos;) for princi-
ples such as the Head Feature Principle (&apos;HFP&apos;)
and Semantics Principle (&apos;SemP&apos;). Templates are
expanded where they are invoked (by @&apos;HFP&apos; or
©&apos;SemP&apos;). The type hierarchy includes the phrase
type hierarchy of Sag (1997). As ProFIT does not
support dynamic constraints, we use templates to
specify phrasal constraints. For example, for head-
nexus phrases, the hd_nexus_ph template specifies
the &lt;hd_nexus_ph type, invokes general constraints
on headed phrases (such as HFP) by @hd_ph, and
invokes the Semantics Principle by @&apos;SemP&apos;.
Immediate dominance schemata are implemented
as PSG rules, using schematic categories word and
phrase, not traditional categories (NP, VP etc). To
simplify the generator, the semantic head is first in
the list of daughters. Linear precedence is speci-
fied by the PHON strings, implemented as Prolog
difference lists. Example rules for Head-Subject and
Head-Complements Schemata are shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.990841">
2.3 HPSG Interface for BUG1
</subsectionHeader>
<bodyText confidence="0.99977475">
van Noord (1990) implements the BUG algorithm
as BUG1 in Prolog. For HPSG, we add the ProFIT
interface in Figure 2. Templates identify the head
features (HF) and logical form (LF), and keep the
algorithm independent from HPSG internal details.
Note that link, used by van Noord (1990) to im-
prove the efficiency of the algorithm, is replaced by
the HPSG Head Feature Principle.
</bodyText>
<equation confidence="0.809138333333333">
hf(HF) := synsem!loc!cat!head!HF.
lf(LF) := synsem!loc!cont!LF.
predict_vord( lf(LF) &amp; Chf(HF), Word ) :-
</equation>
<table confidence="0.800055388888889">
lex( Word &amp; Clf(LF) &amp; Chf(HF) ).
predict_rule(Head,Mother,Others,Chf(HF)) :-
( Mother &amp; hf(HF) [HeadlOthers] ).
generate(LF, Sign, String) :-
bugl( Sign &amp; phon!String-D &amp; lf(LF) ).
/* BUG1: van Noord 1990 */
bugl(Node) :-
predict_word(Node, Small),
connect(Small, Node).
connect(Node, Node).
connect(Small, Big) :-
predict_rule(Small,Middle,Others,Big),
gen_ds(Others),
connect(Middle, Big).
gen_ds([]).
gen_ds([NodelNodes]) :-
bugl(Node),
gen_ds(Nodes).
</table>
<figureCaption confidence="0.960571">
Figure 2: ProFIT/HPSG Interface for BUG1
</figureCaption>
<page confidence="0.741258">
1394
</page>
<figure confidence="0.992256">
PHON (she, saw, Kim)
see-rel
CONT U SEER
[
SEEN
El
El
BACKGR /
[female-rel
INST El
[name-re] 11
BEARER
NAME Kim
El
El
NP )1 VP
[PHON (she) [PHON (saw, Kim)]
CONT I INDEX CONT
BACKGR BACKGR {LI}
El
V
[PHON (saw)
CONT
BACKGR {)
NP
[HON (Kim)
CONT I INDEX
BACKGR {U)
</figure>
<figureCaption confidence="0.999275">
Figure 3: Contextual Background (Phrasal Amalgamation)
</figureCaption>
<sectionHeader confidence="0.965271" genericHeader="method">
3 Quantifiers and Context
</sectionHeader>
<bodyText confidence="0.9999445">
Head-driven generation as in Section 2 works fine if
the semantics is strictly head-driven. All semantic
information must be inside the CONTENT feature,
and cannot be distributed in other features such as
QSTORE or BACKGR. When an NP is assigned to
the semantic role of a verb, the whole of the NP&apos;s
CONTENT must be assigned, not only its INDEX.
This differs significantly from HPSG theory.
</bodyText>
<subsectionHeader confidence="0.999878">
3.1 Quantifier Storage and Retrieval
</subsectionHeader>
<bodyText confidence="0.99998595">
There is a complication in Pollard and Sag (1994)
caused by the use of Cooper storage to handle scope
ambiguities. While scoped quantifiers are included
in the QUANTS list within CONTENT, unscoped
quantifiers are stored in the QSTORE set outside
CONTENT. So logical form for generation needs to
include QSTORE as well as CONTENT.
In this approach, a quantifier may be retrieved at
any suitable syntactic node. A quantifier retrieved
at a particular node is a member of the QSTORE
set (but not the QUANTS list) of some daughter of
that node. Due to the retrieval it is a member of
the QUANTS list (but not the QSTORE set) of the
mother node. Pollard and Sag (1994) define a mod-
ified Semantics Principle to cater for this, but the
effect of retrieval on QSTORE and QUANTS means
that the mother and the semantic head daughter
must have different logical forms. The daughter is
the semantic head by the HPSG definition, but not
as required by the generation algorithm.
</bodyText>
<subsectionHeader confidence="0.999738">
3.2 Contextual Background
</subsectionHeader>
<bodyText confidence="0.9968664">
In addition to semantic content, natural language
generation requires presuppositions and other prag-
matic and discourse factors. In HPSG, such factors
are part of CONTEXT. To specify these factors for
generation, the usual approach is to include them in
the logical form. So logical form needs to include
CONTEXT as well as CONTENT and QSTORE.
This extended logical form is defined for BUG1 by
replacing the ProFIT template for &apos;1f(LF)&apos; shown in
Figure 2 with the new template in Figure 4.
</bodyText>
<equation confidence="0.938845">
lf(ct!CT &amp; qs!QS k cx!CX) :=
synsem!loc!(cont!CT &amp; qstore!QS &amp; conx!CX).
</equation>
<figureCaption confidence="0.993469">
Figure 4: Extending the Logical Form
</figureCaption>
<bodyText confidence="0.999170555555556">
However, head-driven generation does not work
with this inclusive logical form, given the theory of
Pollard and Sag (1994). Even if we ignore quantifier
retrieval and look at a very simple sentence, there is
a fundamental difficulty with CONTEXT.
Figure 3, from Wilcock (1997), shows the HPSG
analysis of she saw Kim. Note that she has a non-
empty BACKGR set (shown by tag El ) , stating a
pragmatic requirement that the referent is female.
</bodyText>
<page confidence="0.981325">
1395
</page>
<bodyText confidence="0.999926">
This background condition is part of CONTEXT,
and is passed up from NP to S by the Principle of
Contextual Consistency. Similarly, Kim has a back-
ground condition (shown by tag 0) that the referent
bears this name. This is also passed from NP to VP,
and from VP to S.
S, VP and V share the same CONTENT (shown
by tag LI). If logical form is restricted to seman-
tic content as in Figure 2, then V is the semantic
head of VP and VP is the semantic head of S, not
only in terms of the HPSG definition but also in
terms of the BUG algorithm. In this case, saw can
be found immediately by predict_word in BUG1.
But if we extend logical form as in Figure 4, to in-
clude the context factors required for adequate re-
alization, it is clear from Figure 3 that S does not
have the same logical form as VP, and VP does not
have the same logical form as V, as their BACKGR
sets differ. Therefore, although V is still the seman-
tic head of VP according to the HPSG definition,
it is not the semantic head according to the BUG
algorithm. Similarly, VP is still the semantic head
of S for HPSG, but it is not the semantic head for
BUG. In this case, predict_word cannot find any se-
mantic head word in the lexicon, and BUG1 cannot
generate the sentence.
</bodyText>
<sectionHeader confidence="0.977458" genericHeader="method">
4 Revising the Grammar
</sectionHeader>
<bodyText confidence="0.999975555555556">
If we include unscoped quantifiers and contextual
background in logical form, we see that there are two
different definitions of &amp;quot;semantic head&amp;quot;: the HPSG
definition based on adjunct daughter or syntactic
head daughter, and the BUG algorithm definition
based on identity of logical forms. However, recent
proposals for changes in HPSG theory suggest that
the two notions of semantic head can be brought
back together.
</bodyText>
<subsectionHeader confidence="0.999759">
4.1 Lexical amalgamation in HP SG
</subsectionHeader>
<bodyText confidence="0.99976803030303">
In Pollard and Sag (1994), QSTORE and BACKGR
sets are phrasally amalgamated. The Quantifier In-
heritance Principle requires a phrase&apos;s QSTORE to
be the set union of the QSTOREs of all daughters,
minus any quantifiers in the phrase&apos;s RETRIEVED
list. The Principle of Contextual Consistency re-
quires a phrase&apos;s BACKGR to be the set union of
the BACKGR sets of all the daughters.
It has recently been proposed that these sets
should be lexically amalgamated. A syntactic head
word&apos;s arguments are now lexically specified in its
ARGUMENT-STRUCTURE list. The word&apos;s set-
valued features can therefore be defined in terms of
the amalgamation of the set-valued features of its
arguments.
Lexical amalgamation of quantifier storage was
proposed by Pollard and Yoo (1995). They change
QSTORE into a local feature which can be included
in the features subcategorized for by a lexical head,
and can therefore be lexically amalgamated in the
head. A phrase no longer inherits unscoped quan-
tifiers directly from all daughters, instead they are
inherited indirectly via the semantic head daughter.
Lexical amalgamation of CONTEXT, proposed
by Wilcock (1997), follows the same approach. As
CONTEXT is a local feature, it can be subcatego-
rized for by a head word and lexically amalgamated
in the head by means of a BACKGR amalgamation
constraint. Instead of a phrase inheriting BACKGR
conditions directly from all daughters by the Prin-
ciple of Contextual Consistency, they are inherited
indirectly via the &amp;quot;contextual head&amp;quot; daughter which
is the same as the semantic head daughter.
</bodyText>
<subsectionHeader confidence="0.999049">
4.2 Lexical amalgamation in ProFIT
</subsectionHeader>
<bodyText confidence="0.996264444444445">
In the ProFIT implementation, QSTORE sets and
BACKGR sets are Prolog difference lists. Lexical
amalgamation of both sets is shown in Figure 5,
the lexical entry for the verb &amp;quot;saw&amp;quot;. The subject&apos;s
BACKGR set BO-Bl and the object&apos;s BACKGR set
Bl-BN are amalgamated in the verb&apos;s BACKGR set
BO-BN. The subject and object QSTORE sets, Q0-
Q1 and Q1-QN, are similarly amalgamated in the
verb&apos;s QSTORE QO-QN.
</bodyText>
<equation confidence="0.9590659375">
lex( phonqsawlX)-X &amp; @verb &amp;
synsem!loc!(
cat!(head!&lt;verb &amp;
val!(subp[Onp &amp;
loc!(cat!head!case!&lt;nom &amp;
cont!index!Subj &amp;
conx!backgr!BO-B1 &amp;
cistore!Q0-Q1)] &amp;
comps!DOnp &amp;
loc!(cat!head!caseRacc &amp;
cont!index!Obj &amp;
conx!backgr!Bl-BN &amp;
cistore!Q1-ON)))) &amp;
cont!nucqseer!Subj &amp; seen!Obj) &amp;
conx!backgr!BO-BN Xc
cistore!Q0-QN) ).
</equation>
<figureCaption confidence="0.992658">
Figure 5: Lexical amalgamation
</figureCaption>
<bodyText confidence="0.999830857142857">
The basic Semantics Principle, for semantic con-
tent only, was implemented by the ProFIT templates
&apos;SemP&apos; and &apos;SemP&apos;(adjunct) as shown in Figure 1.
In order to include unscoped quantifiers and back-
ground conditions in logical form, as in Figure 4,
and still make it possible for the logical form of
a phrase to be identical to the logical form of its
</bodyText>
<page confidence="0.974654">
1396
</page>
<bodyText confidence="0.9995927">
semantic head, the Semantics Principle is replaced
and extended. As proposed by Wilcock (1997), we
need three principles: Semantic Head Inheritance
Principle (SHIP), Quantifier Inheritance Principle
(QUIP), and Contextual Head Inheritance Princi-
ple (CHIP). These are implemented by templates as
shown in Figure 6 (only the non-adjunct forms are
shown). To include the three principles in the gram-
mar, the template for hd_nexus_ph in Figure 1 is
extended as shown in Figure 6.
</bodyText>
<figure confidence="0.994032125">
&apos;SHIP&apos; := synsem!loc!cont!Cont &amp;
hd_dtr!synsem!loc!cont!Cont.
&apos;QUIP&apos; := synsem!lockistoreNS &amp;
hd_dtr!synsem!loc!cistore!QS.
&apos;CHIP&apos; := synsem!loc!conx!Conx k
hd_dtr!synsem!loc!conx!Conx.
hd_nexus_ph := &lt;hd_nexus_ph k Chd_ph k
C&apos;SHIP&apos; k C&apos;QUIP&apos; &amp;
</figure>
<figureCaption confidence="0.999979">
Figure 6: Inheritance of Logical Form
</figureCaption>
<bodyText confidence="0.999994">
With these revisions, it is possible to include
unscoped quantifiers and background conditions in
the starting logical form, and perform head-driven
generation successfully using the BUG1 generator.
However, there remain various technical difficulties
in this implementation. The ProFIT system does
not support either dynamic constraint checking or
set-valued features. The methods shown (template
expansion and difference lists) are only partial sub-
stitutes for the required facilities.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999988291666667">
The combination of a head-driven HPSG grammar
with a head-driven generation algorithm is a natu-
ral approach to surface realization. We showed how
van Noord&apos;s BUG1 generator can easily be adapted
for use with an HPSG grammar implemented in
ProFIT, and that this works well if the semantics is
strictly head-driven. However, while the apparently
clear definition of semantic head in HPSG should
make semantic head-driven generation easy to imple-
ment, we found that if we implement the full HPSG
textbook semantics, with quantifier storage and con-
textual background conditions, the notion of seman-
tic head becomes unclear. Surprisingly, this natural
approach does not work, even for simple examples.
In order to use semantic head-driven generation
algorithms with HPSG, we must adopt recent pro-
posals to include quantifier storage and contextual
background inside semantic heads by means of lex-
ical amalgamation. We showed how the grammar
in ProFIT can be extended with these proposals.
We therefore conclude that head-driven generation
with HPSG is indeed a feasible approach to surface
realization, although there are some technical diffi-
culties.
</bodyText>
<sectionHeader confidence="0.997275" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999803">
We are grateful to Mr Yoshikazu Nakagawa of Sharp
Corporation for making our collaboration possible.
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998195">
Gregor Erbach. 1995. ProFIT: Prolog with Fea-
tures, Inheritance, and Templates. In Seventh
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 180-
187, Dublin.
Masahiko Haruno, Yasuharu Den, and Yuji Matsu-
moto. 1996. A chart-based semantic head driven
generation algorithm. In G. Adorni and M. Zock,
editors, Trends in Natural Language Generation:
An Artificial Intelligence Perspective, pages 300-
313. Springer.
Carl Pollard and Ivan A. Sag. 1994. Head-driven
Phrase Structure Grammar. CSLI Publications
and University of Chicago Press.
Carl Pollard and Eun Jung Yoo. 1995. Quantifiers,
wh-phrases and a theory of argument selection.
Tübingen HPSG workshop.
Ivan A. Sag. 1997. English relative clause construc-
tions. Journal of Linguistics, 33(2):431-484.
Stuart M. Shieber, Gertjan van Noord, Fer-
nando C.N. Pereira, and Robert C. Moore. 1990.
Semantic head-driven generation. Computational
Linguistics, 16(1):30-42.
Gertjan van Noord. 1990. An overview of head-
driven bottom-up generation. In R. Dale, C. Mel-
lish, and M. Zock, editors, Current Research
in Natural Language Generation, pages 141-165.
Academic Press.
Graham Wilcock and Yuji Matsumoto. 1996. Re-
versible delayed lexical choice in a bidirectional
framework. In 16th International Conference on
Computational Linguistics (COLING-96), pages
758-763, Copenhagen.
Graham Wilcock. 1997. Lexicalization of Context.
4th International Conference on HPSG, Ithaca. To
appear in G. Webelhuth, J.-P. Koenig and A. Kat-
hol, editors, Lexical and Constructional Aspects of
Linguistic Explanation. CSLI Publications.
</reference>
<page confidence="0.993767">
1397
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.242633">
<title confidence="0.999779">Head-Driven Generation with HPSG</title>
<author confidence="0.999939">Graham Wilcock</author>
<affiliation confidence="0.998753333333333">Centre for Computational Linguistics University of Manchester Institute of Science and Technology</affiliation>
<address confidence="0.9278335">88, Manchester M60 1QD United Kingdom</address>
<email confidence="0.995372">grahamOccl.umist.ac.uk</email>
<author confidence="0.84317">Yuji Matsumoto</author>
<affiliation confidence="0.9994535">Graduate School of Information Science Nara Institute of Science and Technology</affiliation>
<address confidence="0.997357">8916-5 Takayama, Ikoma, Nara 630-01</address>
<author confidence="0.364147">Japan</author>
<email confidence="0.950751">matsueis.aist—nara.ac.jp</email>
<abstract confidence="0.995178">As HPSG is head-driven, with clear semantic heads, semantic head-driven generation should be simple. We adapt van Noord&apos;s Prolog generator for use with an HPSG grammar in ProFIT. However, quantifiers and context factors are difficult to include in headdriven generation. We must adopt recent theoretical proposals for lexicalized scoping and context. With these revisions, head-driven generation with HPSG is not so simple, but it is possible.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gregor Erbach</author>
</authors>
<title>ProFIT: Prolog with Features, Inheritance, and Templates.</title>
<date>1995</date>
<booktitle>In Seventh Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>180--187</pages>
<location>Dublin.</location>
<contexts>
<context position="5645" citStr="Erbach, 1995" startWordPosition="837" endWordPosition="838">ypes, Schemata a clear definition of semantic head: in head-adjunct phrases, the adjunct daughter is the semantic head; in other headed phrases, the syntactic head daughter is the semantic head. In both cases, the Semantics Principle basically requires the content of the semantic head to be identical to the content of the mother. If we ignore coordinate structures, and if we equate logical form with semantic content for now, then all HPSG grammar rules are SHD chain rules, meeting the requirement of the BUG algorithm. 2.2 HPSG in ProFIT ProFIT: Prolog with Features, Inheritance and Templates (Erbach, 1995) is an extension of Prolog which supports inheritance-based typed feature structures. The type hierarchy is declared in a signature, which defines subtypes and appropriate features of every type. Terms with typed feature structures can then be used alongside normal terms. Using the signature declarations, the ProFIT system compiles the typed feature structures into normal Prolog terms, which can be compiled by the Prolog system. Figure 1 shows some implementation details. We use ProFIT templates (defined by &apos;:=&apos;) for principles such as the Head Feature Principle (&apos;HFP&apos;) and Semantics Principle</context>
</contexts>
<marker>Erbach, 1995</marker>
<rawString>Gregor Erbach. 1995. ProFIT: Prolog with Features, Inheritance, and Templates. In Seventh Conference of the European Chapter of the Association for Computational Linguistics, pages 180-187, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahiko Haruno</author>
<author>Yasuharu Den</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A chart-based semantic head driven generation algorithm.</title>
<date>1996</date>
<booktitle>Trends in Natural Language Generation: An Artificial Intelligence Perspective,</booktitle>
<pages>300--313</pages>
<editor>In G. Adorni and M. Zock, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3759" citStr="Haruno et al. (1996)" startWordPosition="570" endWordPosition="573">PSG sign to be generated. 2.1 Semantic heads Head-driven generation algorithms are based on the idea that most grammar rules have a semantic head daughter whose logical form is identical to the logical form of the mother. The bottom-up generation (BUG) algorithm of van Noord (1990) requires every rule to have such a head (except lexical entries). The semantic head-driven (SHD) algorithm of Shieber et at. (1990) relaxes this, dividing rules into chain rules with such a head (processed bottom-up), and nonchain rules (processed top-down). The chart-based semantic head-driven (CSHD) algorithm&apos; of Haruno et al. (1996) increases efficiency by using a chart to eliminate recomputation of partial results. Head-driven bottom-up generation is efficient as it is geared both to the input logical form (headdriven) and to lexical information (bottom-up). It is good for HPSG, which is highly lexicalist and has 1For simplicity we illustrate the approach with BUG. A ProFIT/HPSG framework using the CSHD algorithm is described by Wilcock and Matsumoto (1996). 1393 &apos;HFP&apos; := synsem!loc!cat!head!HF k hd_dtr!synsem!loc!cat!head!HF. &apos;SemP&apos; := synsem!loc!cont!Cont k hd_dtr!synsem!loc!cont!Cont. &apos;SemP&apos; (adjunct) := synsem!loc!c</context>
</contexts>
<marker>Haruno, Den, Matsumoto, 1996</marker>
<rawString>Masahiko Haruno, Yasuharu Den, and Yuji Matsumoto. 1996. A chart-based semantic head driven generation algorithm. In G. Adorni and M. Zock, editors, Trends in Natural Language Generation: An Artificial Intelligence Perspective, pages 300-313. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<date>1994</date>
<booktitle>Head-driven Phrase Structure Grammar. CSLI Publications and University of</booktitle>
<publisher>Chicago Press.</publisher>
<contexts>
<context position="943" citStr="Pollard and Sag, 1994" startWordPosition="127" endWordPosition="130">yama, Ikoma, Nara 630-01 Japan matsueis.aist—nara.ac.jp Abstract As HPSG is head-driven, with clear semantic heads, semantic head-driven generation should be simple. We adapt van Noord&apos;s Prolog generator for use with an HPSG grammar in ProFIT. However, quantifiers and context factors are difficult to include in headdriven generation. We must adopt recent theoretical proposals for lexicalized scoping and context. With these revisions, head-driven generation with HPSG is not so simple, but it is possible. 1 Introduction A natural approach to generation with Head-driven Phrase Structure Grammar (Pollard and Sag, 1994) is to use a head-driven algorithm. HPSG is headdriven not only syntactically, but also semantically. While the Head Feature Principle requires identity of major syntactic features between a phrase and its syntactic head daughter, the Semantics Principle (in various formulations) requires identity of major semantic features between a phrase and its semantic head daughter. Since the semantic head is very clearly defined in HPSG, semantic head-driven generation should be easy to implement. Efficient head-driven generation algorithms, such as BUG, SHD and CSHD, have been presented as Prolog algor</context>
<context position="8939" citStr="Pollard and Sag (1994)" startWordPosition="1338" endWordPosition="1341">PHON (saw) CONT BACKGR {) NP [HON (Kim) CONT I INDEX BACKGR {U) Figure 3: Contextual Background (Phrasal Amalgamation) 3 Quantifiers and Context Head-driven generation as in Section 2 works fine if the semantics is strictly head-driven. All semantic information must be inside the CONTENT feature, and cannot be distributed in other features such as QSTORE or BACKGR. When an NP is assigned to the semantic role of a verb, the whole of the NP&apos;s CONTENT must be assigned, not only its INDEX. This differs significantly from HPSG theory. 3.1 Quantifier Storage and Retrieval There is a complication in Pollard and Sag (1994) caused by the use of Cooper storage to handle scope ambiguities. While scoped quantifiers are included in the QUANTS list within CONTENT, unscoped quantifiers are stored in the QSTORE set outside CONTENT. So logical form for generation needs to include QSTORE as well as CONTENT. In this approach, a quantifier may be retrieved at any suitable syntactic node. A quantifier retrieved at a particular node is a member of the QSTORE set (but not the QUANTS list) of some daughter of that node. Due to the retrieval it is a member of the QUANTS list (but not the QSTORE set) of the mother node. Pollard </context>
<context position="10603" citStr="Pollard and Sag (1994)" startWordPosition="1617" endWordPosition="1620">matic and discourse factors. In HPSG, such factors are part of CONTEXT. To specify these factors for generation, the usual approach is to include them in the logical form. So logical form needs to include CONTEXT as well as CONTENT and QSTORE. This extended logical form is defined for BUG1 by replacing the ProFIT template for &apos;1f(LF)&apos; shown in Figure 2 with the new template in Figure 4. lf(ct!CT &amp; qs!QS k cx!CX) := synsem!loc!(cont!CT &amp; qstore!QS &amp; conx!CX). Figure 4: Extending the Logical Form However, head-driven generation does not work with this inclusive logical form, given the theory of Pollard and Sag (1994). Even if we ignore quantifier retrieval and look at a very simple sentence, there is a fundamental difficulty with CONTEXT. Figure 3, from Wilcock (1997), shows the HPSG analysis of she saw Kim. Note that she has a nonempty BACKGR set (shown by tag El ) , stating a pragmatic requirement that the referent is female. 1395 This background condition is part of CONTEXT, and is passed up from NP to S by the Principle of Contextual Consistency. Similarly, Kim has a background condition (shown by tag 0) that the referent bears this name. This is also passed from NP to VP, and from VP to S. S, VP and </context>
<context position="12659" citStr="Pollard and Sag (1994)" startWordPosition="1996" endWordPosition="1999">BUG. In this case, predict_word cannot find any semantic head word in the lexicon, and BUG1 cannot generate the sentence. 4 Revising the Grammar If we include unscoped quantifiers and contextual background in logical form, we see that there are two different definitions of &amp;quot;semantic head&amp;quot;: the HPSG definition based on adjunct daughter or syntactic head daughter, and the BUG algorithm definition based on identity of logical forms. However, recent proposals for changes in HPSG theory suggest that the two notions of semantic head can be brought back together. 4.1 Lexical amalgamation in HP SG In Pollard and Sag (1994), QSTORE and BACKGR sets are phrasally amalgamated. The Quantifier Inheritance Principle requires a phrase&apos;s QSTORE to be the set union of the QSTOREs of all daughters, minus any quantifiers in the phrase&apos;s RETRIEVED list. The Principle of Contextual Consistency requires a phrase&apos;s BACKGR to be the set union of the BACKGR sets of all the daughters. It has recently been proposed that these sets should be lexically amalgamated. A syntactic head word&apos;s arguments are now lexically specified in its ARGUMENT-STRUCTURE list. The word&apos;s setvalued features can therefore be defined in terms of the amalg</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase Structure Grammar. CSLI Publications and University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Eun Jung Yoo</author>
</authors>
<title>Quantifiers, wh-phrases and a theory of argument selection. Tübingen HPSG workshop.</title>
<date>1995</date>
<contexts>
<context position="13393" citStr="Pollard and Yoo (1995)" startWordPosition="2112" endWordPosition="2115">ORE to be the set union of the QSTOREs of all daughters, minus any quantifiers in the phrase&apos;s RETRIEVED list. The Principle of Contextual Consistency requires a phrase&apos;s BACKGR to be the set union of the BACKGR sets of all the daughters. It has recently been proposed that these sets should be lexically amalgamated. A syntactic head word&apos;s arguments are now lexically specified in its ARGUMENT-STRUCTURE list. The word&apos;s setvalued features can therefore be defined in terms of the amalgamation of the set-valued features of its arguments. Lexical amalgamation of quantifier storage was proposed by Pollard and Yoo (1995). They change QSTORE into a local feature which can be included in the features subcategorized for by a lexical head, and can therefore be lexically amalgamated in the head. A phrase no longer inherits unscoped quantifiers directly from all daughters, instead they are inherited indirectly via the semantic head daughter. Lexical amalgamation of CONTEXT, proposed by Wilcock (1997), follows the same approach. As CONTEXT is a local feature, it can be subcategorized for by a head word and lexically amalgamated in the head by means of a BACKGR amalgamation constraint. Instead of a phrase inheriting </context>
</contexts>
<marker>Pollard, Yoo, 1995</marker>
<rawString>Carl Pollard and Eun Jung Yoo. 1995. Quantifiers, wh-phrases and a theory of argument selection. Tübingen HPSG workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
</authors>
<title>English relative clause constructions.</title>
<date>1997</date>
<journal>Journal of Linguistics,</journal>
<pages>33--2</pages>
<contexts>
<context position="6393" citStr="Sag (1997)" startWordPosition="950" endWordPosition="951"> defines subtypes and appropriate features of every type. Terms with typed feature structures can then be used alongside normal terms. Using the signature declarations, the ProFIT system compiles the typed feature structures into normal Prolog terms, which can be compiled by the Prolog system. Figure 1 shows some implementation details. We use ProFIT templates (defined by &apos;:=&apos;) for principles such as the Head Feature Principle (&apos;HFP&apos;) and Semantics Principle (&apos;SemP&apos;). Templates are expanded where they are invoked (by @&apos;HFP&apos; or ©&apos;SemP&apos;). The type hierarchy includes the phrase type hierarchy of Sag (1997). As ProFIT does not support dynamic constraints, we use templates to specify phrasal constraints. For example, for headnexus phrases, the hd_nexus_ph template specifies the &lt;hd_nexus_ph type, invokes general constraints on headed phrases (such as HFP) by @hd_ph, and invokes the Semantics Principle by @&apos;SemP&apos;. Immediate dominance schemata are implemented as PSG rules, using schematic categories word and phrase, not traditional categories (NP, VP etc). To simplify the generator, the semantic head is first in the list of daughters. Linear precedence is specified by the PHON strings, implemented </context>
</contexts>
<marker>Sag, 1997</marker>
<rawString>Ivan A. Sag. 1997. English relative clause constructions. Journal of Linguistics, 33(2):431-484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Gertjan van Noord</author>
<author>Fernando C N Pereira</author>
<author>Robert C Moore</author>
</authors>
<date>1990</date>
<booktitle>Semantic head-driven generation. Computational Linguistics,</booktitle>
<pages>16--1</pages>
<marker>Shieber, van Noord, Pereira, Moore, 1990</marker>
<rawString>Stuart M. Shieber, Gertjan van Noord, Fernando C.N. Pereira, and Robert C. Moore. 1990. Semantic head-driven generation. Computational Linguistics, 16(1):30-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>An overview of headdriven bottom-up generation. In</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation,</booktitle>
<pages>141--165</pages>
<editor>R. Dale, C. Mellish, and M. Zock, editors,</editor>
<publisher>Academic Press.</publisher>
<marker>van Noord, 1990</marker>
<rawString>Gertjan van Noord. 1990. An overview of headdriven bottom-up generation. In R. Dale, C. Mellish, and M. Zock, editors, Current Research in Natural Language Generation, pages 141-165. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Wilcock</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Reversible delayed lexical choice in a bidirectional framework.</title>
<date>1996</date>
<booktitle>In 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>758--763</pages>
<location>Copenhagen.</location>
<contexts>
<context position="4193" citStr="Wilcock and Matsumoto (1996)" startWordPosition="638" endWordPosition="641">s, dividing rules into chain rules with such a head (processed bottom-up), and nonchain rules (processed top-down). The chart-based semantic head-driven (CSHD) algorithm&apos; of Haruno et al. (1996) increases efficiency by using a chart to eliminate recomputation of partial results. Head-driven bottom-up generation is efficient as it is geared both to the input logical form (headdriven) and to lexical information (bottom-up). It is good for HPSG, which is highly lexicalist and has 1For simplicity we illustrate the approach with BUG. A ProFIT/HPSG framework using the CSHD algorithm is described by Wilcock and Matsumoto (1996). 1393 &apos;HFP&apos; := synsem!loc!cat!head!HF k hd_dtr!synsem!loc!cat!head!HF. &apos;SemP&apos; := synsem!loc!cont!Cont k hd_dtr!synsem!loc!cont!Cont. &apos;SemP&apos; (adjunct) := synsem!loc!cont!Cont &amp; adj_dtr!synsem!loc!cont!Cont. hd_ph := &lt;hd_ph k C&apos;HFP&apos; synsem!loc!cat!val!comps![]. hd_nexus_ph := &lt;hd_nexus_ph k Chd_ph &amp; C&apos;SemP&apos;. hd_subj_ph := &lt;hd_subj_ph &amp; Chd_nexus_ph &amp; O&apos;VALP&apos;(spr) k CVALP&apos;(comps) &amp; synsem!loc!cat!val!subj![]. hd_comp_ph := &lt;hd_comp_ph k Chd_nexus_ph k O&apos;VALP&apos;(subj) k C&apos;VALP&apos;(spr). Chd_subj_ph &amp; phon!PO-PN &amp; hd_dtr!(Head &amp; synsem!loc!cat!val!subj![S]) k subj_dtr!(Subj &amp; synsem!S) ---&gt; [Head &amp; &lt;ph</context>
</contexts>
<marker>Wilcock, Matsumoto, 1996</marker>
<rawString>Graham Wilcock and Yuji Matsumoto. 1996. Reversible delayed lexical choice in a bidirectional framework. In 16th International Conference on Computational Linguistics (COLING-96), pages 758-763, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Wilcock</author>
</authors>
<date>1997</date>
<booktitle>Lexicalization of Context. 4th International Conference on HPSG,</booktitle>
<editor>in G. Webelhuth, J.-P. Koenig and A. Kathol, editors,</editor>
<publisher>CSLI Publications.</publisher>
<location>Ithaca.</location>
<note>To appear</note>
<contexts>
<context position="10757" citStr="Wilcock (1997)" startWordPosition="1644" endWordPosition="1645">cal form. So logical form needs to include CONTEXT as well as CONTENT and QSTORE. This extended logical form is defined for BUG1 by replacing the ProFIT template for &apos;1f(LF)&apos; shown in Figure 2 with the new template in Figure 4. lf(ct!CT &amp; qs!QS k cx!CX) := synsem!loc!(cont!CT &amp; qstore!QS &amp; conx!CX). Figure 4: Extending the Logical Form However, head-driven generation does not work with this inclusive logical form, given the theory of Pollard and Sag (1994). Even if we ignore quantifier retrieval and look at a very simple sentence, there is a fundamental difficulty with CONTEXT. Figure 3, from Wilcock (1997), shows the HPSG analysis of she saw Kim. Note that she has a nonempty BACKGR set (shown by tag El ) , stating a pragmatic requirement that the referent is female. 1395 This background condition is part of CONTEXT, and is passed up from NP to S by the Principle of Contextual Consistency. Similarly, Kim has a background condition (shown by tag 0) that the referent bears this name. This is also passed from NP to VP, and from VP to S. S, VP and V share the same CONTENT (shown by tag LI). If logical form is restricted to semantic content as in Figure 2, then V is the semantic head of VP and VP is </context>
<context position="13774" citStr="Wilcock (1997)" startWordPosition="2173" endWordPosition="2174">GUMENT-STRUCTURE list. The word&apos;s setvalued features can therefore be defined in terms of the amalgamation of the set-valued features of its arguments. Lexical amalgamation of quantifier storage was proposed by Pollard and Yoo (1995). They change QSTORE into a local feature which can be included in the features subcategorized for by a lexical head, and can therefore be lexically amalgamated in the head. A phrase no longer inherits unscoped quantifiers directly from all daughters, instead they are inherited indirectly via the semantic head daughter. Lexical amalgamation of CONTEXT, proposed by Wilcock (1997), follows the same approach. As CONTEXT is a local feature, it can be subcategorized for by a head word and lexically amalgamated in the head by means of a BACKGR amalgamation constraint. Instead of a phrase inheriting BACKGR conditions directly from all daughters by the Principle of Contextual Consistency, they are inherited indirectly via the &amp;quot;contextual head&amp;quot; daughter which is the same as the semantic head daughter. 4.2 Lexical amalgamation in ProFIT In the ProFIT implementation, QSTORE sets and BACKGR sets are Prolog difference lists. Lexical amalgamation of both sets is shown in Figure 5,</context>
<context position="15439" citStr="Wilcock (1997)" startWordPosition="2430" endWordPosition="2431">cc &amp; cont!index!Obj &amp; conx!backgr!Bl-BN &amp; cistore!Q1-ON)))) &amp; cont!nucqseer!Subj &amp; seen!Obj) &amp; conx!backgr!BO-BN Xc cistore!Q0-QN) ). Figure 5: Lexical amalgamation The basic Semantics Principle, for semantic content only, was implemented by the ProFIT templates &apos;SemP&apos; and &apos;SemP&apos;(adjunct) as shown in Figure 1. In order to include unscoped quantifiers and background conditions in logical form, as in Figure 4, and still make it possible for the logical form of a phrase to be identical to the logical form of its 1396 semantic head, the Semantics Principle is replaced and extended. As proposed by Wilcock (1997), we need three principles: Semantic Head Inheritance Principle (SHIP), Quantifier Inheritance Principle (QUIP), and Contextual Head Inheritance Principle (CHIP). These are implemented by templates as shown in Figure 6 (only the non-adjunct forms are shown). To include the three principles in the grammar, the template for hd_nexus_ph in Figure 1 is extended as shown in Figure 6. &apos;SHIP&apos; := synsem!loc!cont!Cont &amp; hd_dtr!synsem!loc!cont!Cont. &apos;QUIP&apos; := synsem!lockistoreNS &amp; hd_dtr!synsem!loc!cistore!QS. &apos;CHIP&apos; := synsem!loc!conx!Conx k hd_dtr!synsem!loc!conx!Conx. hd_nexus_ph := &lt;hd_nexus_ph k Ch</context>
</contexts>
<marker>Wilcock, 1997</marker>
<rawString>Graham Wilcock. 1997. Lexicalization of Context. 4th International Conference on HPSG, Ithaca. To appear in G. Webelhuth, J.-P. Koenig and A. Kathol, editors, Lexical and Constructional Aspects of Linguistic Explanation. CSLI Publications.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>