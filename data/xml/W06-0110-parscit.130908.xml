<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9944795">
Hybrid Models
for Chinese Named Entity Recognition
</title>
<author confidence="0.999615">
Lishuang Li, Tingting Mao, Degen Huang, Yuansheng Yang
</author>
<affiliation confidence="0.934752666666667">
Department of Computer Science and Engineering
Dalian University of Technology
116023 Dalian, China
</affiliation>
<email confidence="0.984101">
{computer, huangdg, yangys}@dlut.edu.cn
maotingting1007@sohu.com
</email>
<sectionHeader confidence="0.993816" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983277777778">
This paper describes a hybrid model and
the corresponding algorithm combining
support vector machines (SVMs) with
statistical methods to improve the per-
formance of SVMs for the task of Chi-
nese Named Entity Recognition (NER).
In this algorithm, a threshold of the dis-
tance from the test sample to the hyper-
plane of SVMs in feature space is used to
separate SVMs region and statistical
method region. If the distance is greater
than the given threshold, the test sample
is classified using SVMs; otherwise, the
statistical model is used. By integrating
the advantages of two methods, the hy-
brid model achieves 93.18% F-measure
for Chinese person names and 91.49% F-
measure for Chinese location names.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999775755555556">
Named entity (NE) recognition is a fundamental
step to many language processing tasks such as
information extraction (IE), question answering
(QA) and machine translation (MT). On its own,
NE recognition can also provide users who are
looking for person or location names with quick
information. Palma and Day (1997) reported that
person (PER), location (LOC) and organization
(ORG) names are the most difficult sub-tasks as
compared to other entities as defined in Message
Understanding Conference (MUC). So we focus
on the recognition of PER, LOC and ORG enti-
ties.
Recently, machine learning approaches are
widely used in NER, including the hidden
Markov model (Zhou and Su, 2000; Miller and
Crystal, 1998), maximum entropy model
(Borthwick, 1999), decision tree (Qin and Yuan,
2004), transformation-based learning (Black and
Vasilakopoulos, 2002), boosting (Collins, 2002;
Carreras et al., 2002), support vector machine
(Takeuchi and Collier, 2002; Yu et al., 2004;
Goh et al., 2003), memory-based learning (Sang,
2002). SVM has given high performance in vari-
ous classification tasks (Joachims, 1998; Kudo
and Matsumoto, 2001). Goh et al. (2003) pre-
sented a SVM-based chunker to extract Chinese
unknown words. It obtained higher F-measure
for person names and organization names.
Like other classifiers, the misclassified testing
samples by SVM are mostly near the decision
plane (i.e., the hyperplane of SVM in feature
space). In order to increase the accuracy of SVM,
we propose a hybrid model combining SVM
with a statistical approach for Chinese NER, that
is, in the region near the decision plane, statisti-
cal method is used to classify the samples instead
of SVM, and in the region far away from the de-
cision plane, SVM is used. In this way, the mis-
classification by SVM near the decision plane
can be decreased significantly. A higher F-
measure for Chinese NE recognition can be
achieved.
In the following sections, we shall describe
our approach in details.
</bodyText>
<sectionHeader confidence="0.70918" genericHeader="method">
2 Recognition of Chinese Named Entity
Using SVM
</sectionHeader>
<bodyText confidence="0.999801">
Firstly, we segment and assign part-of-speech
(POS) tags to words in the texts using a Chinese
lexical analyzer. Secondly, we break segmented
words into characters and assign each character
its features. Lastly, a model based on SVM to
identify Chinese named entities is set up by
choosing a proper kernel function.
In the following, we will exemplify the person
names and location names to illustrate the identi-
fication process.
</bodyText>
<page confidence="0.980063">
72
</page>
<note confidence="0.7925355">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 72–78,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.970535">
2.1 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.999150357142857">
Support Vector Machines first introduced by
Vapnik (1996) are learning systems that use a
hypothesis space of linear functions in a high
dimensional feature space, trained with a learn-
ing algorithm from optimization theory that im-
plements a learning bias derived from statistical
theory. SVMs are based on the principle of struc-
tural risk minimization. Viewing the data as
points in a high-dimensional feature space, the
goal is to fit a hyperplane between the positive
and negative examples so as to maximize the
distance between the data points and the hyper-
plane.
Given training examples:
</bodyText>
<equation confidence="0.997511428571428">
n
= {( 1, 1 ), ( 2, 2),..., ( , )},
x y x R
∈ , { 1 , 1} ,
∈ − + (1)
S x y x y yi
l l i
</equation>
<bodyText confidence="0.9328675">
xi is a feature vector (n dimension) of the i-th
sample. is the class (positive(+1) or nega-
</bodyText>
<equation confidence="0.796902">
yi
</equation>
<bodyText confidence="0.995339428571429">
tive(-1) class) label of the i-th sample. l is the
number of the given training samples. SVMs find
an “optimal” hyperplane: (wx + b) = 0 to sepa-
rate the training data into two classes. The opti-
mal hyperplane can be found by solving the fol-
lowing quadratic programming problem (we
leave the details to Vapnik (1998)):
</bodyText>
<equation confidence="0.982864166666667">
l
subject to ∑ yiαi = 0, i l.
1
The function K(xi, xj) = ϕ(xi) ⋅ ϕ(xj) is called
kernel function, is the mapping from pri-
ϕ(x)
</equation>
<bodyText confidence="0.992125">
mary input space to feature space. Given a test
example, its label y is decided by the following
function:
</bodyText>
<equation confidence="0.999329">
f ( ) sgn[ ∑
x = α iy iK ( , ) ] .
x i x + b (3)
</equation>
<bodyText confidence="0.9720859">
Basically, SVMs are binary classifiers, and
can be extended to multi-class classifiers in order
to solve multi-class discrimination problems.
There are two popular methods to extend a bi-
nary classification task to that of K classes: one
class vs. all others and pairwise. Here, we em-
ploy the simple pairwise method. This idea is to
build K × (K −1) / 2 classifiers considering all
pairs of classes, and final decision is given by
their voting.
</bodyText>
<subsectionHeader confidence="0.996174">
2.2 Recognition of Chinese Person Names
Based on SVM
</subsectionHeader>
<bodyText confidence="0.999361">
We use a SVM-based chunker, YamCha (Kudo
and Masumoto, 2001), to extract Chinese person
names from the Chinese lexical analyzer.
</bodyText>
<listItem confidence="0.862655">
1) Chinese Person Names Chunk Tags
</listItem>
<bodyText confidence="0.8631975">
We use the Inside/Outside representation for
proper chunks:
</bodyText>
<figure confidence="0.631148333333333">
I Current token is inside of a chunk.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
</figure>
<bodyText confidence="0.9592575">
A chunk is considered as a Chinese person
name in this case. Every character in the training
set is given a tag classification of B, I or O, that
is, yi ∈ {B, I, O} . Here, the multi-class decision
</bodyText>
<listItem confidence="0.768110666666667">
method pairwise is selected.
2) Features Extraction for Chinese Person
Names
</listItem>
<bodyText confidence="0.992762571428572">
Since Chinese person names are identified
from the segmented texts, the mistakes of word
segmentation can result in error identification of
person names. So we must break words into
characters and extract features for every charac-
ter. Table 1 summarizes types of features and
their values.
</bodyText>
<table confidence="0.997951727272727">
Type of feature Value
POS tag n-B, v-I, p-S
Whether a character Y or N
is a surname
Character surface form of the
character itself
The frequency of a Y or N
character in person
names table
Previous BIO tag B-character, I-character,
O-character
</table>
<tableCaption confidence="0.999847">
Table 1. Summary of Features and Their Values
</tableCaption>
<bodyText confidence="0.9991505">
The POS tag from the output of lexical analy-
sis is subcategorized to include the position of
the character in the word. The list of POS tags is
shown in Table 2.
</bodyText>
<table confidence="0.999382222222222">
POS tag Description of the position of the
character in a word
&lt;POS&gt;-S One-character word
&lt;POS&gt;-B first character in a multi-character
word
&lt;POS&gt;-I intermediate character in a multi-
character word
&lt;POS&gt;-E last character in a multi-character
word
</table>
<tableCaption confidence="0.999625">
Table 2. POS Tags in A Word
</tableCaption>
<bodyText confidence="0.999907571428572">
If the character is a surname, the value is as-
signed to Y, otherwise assigned to N.
The “character” is surface form of the charac-
ter in the word.
We extract all person names in January 1998
of the People’s Daily to set up person names ta-
ble and calculate the frequency of every charac-
</bodyText>
<equation confidence="0.9927604375">
1
∑ α − ∑ ∑
i
2
i=1 i=1 j=1
( xi xj (2)
, )
l
l l
max
αiyiαjyj K
i
∈ sv
x
i
=
</equation>
<page confidence="0.97833">
73
</page>
<bodyText confidence="0.989007">
ter (F) of person names table in the training cor-
pus. The frequency of F is defined as
P(F) = the number of F as a character of person names, (4)
the total number of F
if P(F) is greater than the given threshold, the
value is assigned to Y, otherwise assigned to N.
We also use previous BIO-tags as features.
Whether a character is inside a person name or
not, it depends on the context of the character.
Therefore, we use contextual information of two
previous and two successive characters of the
current character as features.
Figure 1 shows an example of features extrac-
tion for the i-th character. When training, the fea-
tures of the character “Min” contains all the fea-
tures surrounded in the frames. If the same sen-
tence is used as testing, the same features are
used.
</bodyText>
<figure confidence="0.98832125">
Position
Character
POS tags
Whether the character
is a surname
The frequency of a character in
the person names table
Previous BIO tags
</figure>
<figureCaption confidence="0.694292">
Figure 1. An example of features extraction
3) Choosing Kernel Functions
</figureCaption>
<bodyText confidence="0.979901666666667">
Here, we choose polynomial kernel functions:
K(x,xi) = [(x⋅ xi)+1 ] d to build an optimal
separating hyperplane.
</bodyText>
<subsectionHeader confidence="0.9933445">
2.3 Recognition of Chinese Location Names
Based on SVM
</subsectionHeader>
<bodyText confidence="0.9994822">
The identification process of location names is
the same as that of person names except for the
features extraction. Table 3 summarizes types of
features and their values of location names ex-
traction.
</bodyText>
<table confidence="0.994124">
Type of feature Value
POS tag n-B, v-I, p-S
Whether a character Y or N
appears in location names
characteristic table
Character surface form of the
character itself
Previous BIO tag B-character, I-
character, O-
character
</table>
<tableCaption confidence="0.997324">
Table 3. Summary of Features and Their Values
</tableCaption>
<bodyText confidence="0.986427285714286">
The location names characteristic table is set
up in advance, and it includes the characters or
words expressing the characteristics of location
names such as “sheng (province)”, “shi (city)”,
“xian (county)”etc. If the character is in the loca-
tion names characteristic table, the value is as-
signed to Y, otherwise assigned to N.
</bodyText>
<sectionHeader confidence="0.992127" genericHeader="method">
3 Statistical Models
</sectionHeader>
<bodyText confidence="0.9966514">
Many statistical models for NER have been pre-
sented (Zhang et al., 1992; Huang et al., 2003
etc). In this section, we proposed our statistical
models for Chinese person names recognition
and Chinese location names recognition.
</bodyText>
<subsectionHeader confidence="0.998662">
3.1 Chinese Person Names
</subsectionHeader>
<bodyText confidence="0.9993756">
We define a function to evaluate the person name
candidate PN. The evaluated function Total-
Probability(PN) is composed of two parts: the
lexical probability LP(PN) and contextual prob-
ability CP(PN) based on POS tags.
</bodyText>
<equation confidence="0.964240666666667">
TotalProbability PN LP PN
( ) = α ( ) (1 ) ( ),
+ − α CP PN (5)
</equation>
<bodyText confidence="0.973902">
where PN is the evaluated person name and α is
the balance cofficient.
</bodyText>
<equation confidence="0.576719">
1) lexical probability LP(PN)
</equation>
<bodyText confidence="0.999965333333333">
We establish the surname table (SurName) and
the first name table (FirstName) from the
students of year 1999 in a university (containing
9986 person names).
Suppose PN=LF1F2, where L is the surname
of the evaluated person name PN, Fi (i=1,2) is the
i-th first name of the evaluated person name PN.
The probability of the surname Pl(L) is defined
as
</bodyText>
<equation confidence="0.993194">
P L
( )
P L l0
( ) = ,
l (6)
∑ P y
0 ( )
l
y SurName
∈
</equation>
<bodyText confidence="0.698678">
where
</bodyText>
<equation confidence="0.925198333333333">
(L)
(N(L) + 2)
N(L) is the
</equation>
<bodyText confidence="0.990447333333333">
number of L as the single or multiple surname of
person names in the SurName.
The probability of the first name
</bodyText>
<equation confidence="0.976525666666667">
Pl0
=log2
,
Pf(F) is
defined as
Pf (F)Pf0 (y
Pf0 (F)
y FirstName
∈
</equation>
<bodyText confidence="0.991353">
where Pf0 (F) =log2 (N(F) + 2) , N(F) is the
number of F in the FirstName.
The lexical probability of the person name PN
is defined as
</bodyText>
<equation confidence="0.945664823529412">
LP PN P L P F if(PN LF)
LP PN P L C P F P F if(PN LFF )
( ) ( ) ( )
( ) ( ) ( ( ) ( ))
= × =
= × × + =
l f 1 1 (8)
l b f 1 f 2 1 2 ,
i
-2 -1 0 +1 +2
Jiang Ze Min zhu xi
n-S n-B n-E n-B n-E
Y N N N Y
Y Y Y N N
I O O
B I
) , (7)
</equation>
<page confidence="0.9814">
74
</page>
<bodyText confidence="0.966735">
where Cb is the balance cofficient between the
single name and the double name. Here,
Cb=0.844 (Huang et al., 2001).
2) contextual probability based on POS tags
CP(PN)
Chinese person names have characteristic
contexual POS tags in real Chinese texts, for
example, in the phrase “dui Zhangshuai shuo
(say to Zhangshuai)”, the POS tag before the
person name “Zhangshuai” is prepnoun and verb
occurs after the person name. We define the
bigram contextual probability CP(PN) of the
person name PN as the following equation:
TotalPOS is the total number of the contexual
POS tags of every person name in the whole
training corpus.
</bodyText>
<equation confidence="0.588995">
(10)
where LN is the evaluated location name
is
</equation>
<bodyText confidence="0.655541909090909">
the balance cofficient.
1) lexical probability LP (LN)
Suppose
where
is the first character of the evaluated
location name LN,
is the middle characters of
the evaluated location name LN, S is the last
character of the evaluated location name LN.
The probability of the first character of the
evaluated location name
</bodyText>
<equation confidence="0.9868115">
TotalProbability LN LP LN
( ) = α ( )(1 ) ( ),
+ − α CP LN
andα
LN=F0F+S,F+=F1...Fn, (i=1,...,n),
F0
F+
Ph (F0) is defined as
</equation>
<bodyText confidence="0.622868222222222">
where
=log 2
+ 2)
is the
number of
as the first character of location
names in the Chinese Location Names Record.
is the total
number of
</bodyText>
<equation confidence="0.996309">
Ph0(F0)
(C(F0)
, C(F0)
F0
Ph′0(F0)=log2(C′(F0)+2), C′(F0)
</equation>
<bodyText confidence="0.98789825">
F0 in the Chinese Location Names
Record.
The probability of the middle character of the
evaluated location name ( +) is defined as
</bodyText>
<equation confidence="0.992304928571429">
Pf F
P F
( )
+ =
f ∑=
i f i
P F
′ ( )
1
Pf
=log 2
+ 2)
C
is the
</equation>
<bodyText confidence="0.6808176">
number of
as the i-th middle character of loca-
tion names in the Chinese Location Names Re-
cord.
where
</bodyText>
<equation confidence="0.997387333333333">
(Fi)
(C(Fi)
,
(Fi)
Fi
Pf′(Fi)=log2(C′(Fi) + 2) , C′ (Fi) is the total
</equation>
<bodyText confidence="0.946339">
number of Fi in the Chinese Location Names
Record.
The probability of the last character of the
evaluated location name Pl (S) is defined as
</bodyText>
<equation confidence="0.9987935">
P S
( )
= l
P S ,
l ( )
P S
′
l ( )
</equation>
<subsectionHeader confidence="0.994723">
3.2 Chinese Location Names
</subsectionHeader>
<bodyText confidence="0.978035">
We also define a function to evaluate the location
name candidate LN. The evaluated function To-
</bodyText>
<equation confidence="0.830504722222222">
talProbability(LN) is composed of two parts: the
lexical probability LP (LN) and contextual prob-
ability CP (LN) based on POS tags.
Ph (F
)
P F
0
= P F
′ 0 0
( )
h
(12)
(13)
where
(S)
(S) + 2)
C ( S ) is the
number of
</equation>
<bodyText confidence="0.8447335">
the last character of location
names in the Chinese Location Names Record.
</bodyText>
<equation confidence="0.755631">
is the total number
of S in the Chinese Location Names Record.
The lexical probability of the location name
LN is defined as
LN =
</equation>
<bodyText confidence="0.638484428571429">
where Len(LN) is the length of the evaluated lo-
cation name LN.
2) contextual probability based on POS tags CP
(LN)
Location names also have characteristic
contexual POS tags in real Chinese texts, for
example, in the phrase
</bodyText>
<subsectionHeader confidence="0.327116">
Chongqing shi
</subsectionHeader>
<bodyText confidence="0.45293">
junxing (to be held in
the POS tag
before the location name
prepnoun an
</bodyText>
<equation confidence="0.939980125">
Pl
=log2(C′
,
S as
Pl′(S)=log2(C′(S)+2),C ′( S)
Ph F + Pf F+ + P l S Len LN (14)
( ( 0) ( ) ( )) / ( ),
“zai
</equation>
<bodyText confidence="0.998459428571429">
Chongqing)”,
“Chongqing”is
d verb occurs after the location name.
We define the bigram contextual probability
CP(LN) of the location name LN similar to that
of the person name PN in equation (9), where PN
is replaced with LN.
</bodyText>
<subsectionHeader confidence="0.7041395">
4Recognition of Chinese Named Entity
Using Hybrid Model
</subsectionHeader>
<bodyText confidence="0.9975635625">
Analyzing the classification results (obtained by
sole SVMs described in section 2) between B
and I, B and O, I and O respectively, we find that
the error is mainly caused by the second classifi-
cation. The samples which attribute to B class
are misclassified to O class, which leads to B
class
diminishing and the corresponding
named entities are lost. Therefore the Recall is
lower. In the meantime, the number of the mis-
classified samples whose function distances to
the hyperplane of SVM in feature space are less
than 1 can
vote’s
reach over 83% of the number of total
misclassified samples. That means the misclassi-
</bodyText>
<equation confidence="0.89076">
CP(PN)= PersonPOS(&lt; lpos, PN, rpos &gt;)
, (9)
TotalPOS
</equation>
<bodyText confidence="0.981031">
where lpos is the POS tag of the character before
PN (called POS forward), rpos is the POS tag of
the character after PN (called POS backward),
and PersonPOS(&lt; lpos, PN, rpos &gt;) is the number
of PN as a pereson name whose POS forward is
lpos and POS backward is rpos in training corpus.
</bodyText>
<figure confidence="0.5008085">
n Pf (Fi)
,
</figure>
<page confidence="0.994029">
75
</page>
<bodyText confidence="0.805000363636364">
fication of a classifier is occurred in the region of
two overlapping classes. Considering this fact,
we can expect to improve SVM using the follow-
ing hybrid model.
The hybrid model includes the following
procedure:
1) compute the distance from the test sample
to the hyperplane of SVM in feature space.
2) compare the distance with given threshold.
The algorithm of hybrid model can be de-
scribed as follows:
</bodyText>
<equation confidence="0.946016">
Suppose T is the testing set,
(1) if T * (D , select xe T , else stop;
l
(2) compute g(x) = αyiK(xi,x) + b
i 1
models and output the returned results.
(3) T &lt;- T −{ x), repeat(1)
</equation>
<sectionHeader confidence="0.997557" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.994071">
Our experimental results are all based on the
corpus of Peking University.
</bodyText>
<subsectionHeader confidence="0.999457">
5.1 Extracting Chinese Person Names
</subsectionHeader>
<bodyText confidence="0.999155714285714">
We use 180 thousand characters corpus of year
1998 from the People’s Daily as the training cor-
pus and extract other sentences (containing 1526
Chinese person names) as testing corpus to con-
duct an open test experiment. The results are ob-
tained as follows based on different models.
1) Based on Sole SVM
An experiment is carried out to recognize Chi-
nese person names based on sole SVM by the
method as described in Section 2. The Recall,
Precision and F-measure using different number
of degree of polynomial kernel function are
given in Table 4. The best result is obtained
when d=2.
</bodyText>
<table confidence="0.9991105">
Recall Precision F-measure
d=1 87.22% 94.26% 90.61%
d=2 87.16% 96.10% 91.41%
d=3 84.67% 95.14% 89.60%
</table>
<tableCaption confidence="0.9733985">
Table 4. Results for Person Names Extraction
Based on Sole SVM
</tableCaption>
<bodyText confidence="0.975577947368421">
2) Using Hybrid Model
As mentioned in section 4, the test samples
which attribute to B class are misclassified to O
class and therefore the Recall for person names
extraction from sole SVM is lower. So we only
deal with the test samples (B class and O class)
whose function distances to the hyperplane of
SVM in feature space (i.e. g(x)) is between 0 and
E . We move class-boundary learned by SVM
towards the O class, that is, the O class samples
are considered as B class in that area. 93.64% of
the Chinese person names in testing corpus are
recalled when E =0.9 (Here, E also represents
how much the boundary is moved). However, a
number of non-person names are also identified
as person names wrongly and the Precision is
decreased correspondingly. Table 5 shows the
Recall and Precision of person names extraction
with different E .
</bodyText>
<table confidence="0.999704454545455">
Recall Precision F-measure
E =1 93.05% 75.17% 83.16%
E =0.9 93.64% 81.75% 87.29%
E =0.8 93.51% 85.91% 89.55%
E =0.7 93.05% 88.31% 90.62%
E =0.6 92.39% 90.21% 91.29%
E =0.5 91.81% 91.87% 91.84%
E =0.4 91.02% 93.28% 92.13%
E =0.3 90.56% 95.05% 92.75%
E =0.2 90.03% 95.48% 92.68%
E =0.1 88.66% 95.82% 92.10%
</table>
<tableCaption confidence="0.9783205">
Table 5. Results for Person Names Extraction
with Different E
</tableCaption>
<bodyText confidence="0.991325823529412">
We use the evaluated function TotalProbabil-
ity(PN) as described in section 3 to filter the
wrongly recalled person names using SVM. We
tune α in equation (5) to obtain the best results.
The results based on the hybrid model with dif-
ferent α are listed in Table 6 (when d=2). We
can observe that the result is best when α =0.4.
Table 7 shows the results based on the hybrid
model with different E when =0.4. We can
α
observe that the Recall rises and the Precision
drops on the whole when E increases. The syn-
thetic index F-measures are improved when E is
between 0.1 and 0.8 compared with sole SVM.
The best result is obtained when E =0.3. The Re-
call and the F-measure increases 3.27% and
1.77% respectively.
</bodyText>
<table confidence="0.950033153846154">
Recall Precision F-measure
α =0.1 90.37% 95.76% 92.99%
α =0.2 90.37% 96.03% 93.11%
α =0.3 90.43% 96.03% 93.15%
α =0.4 90.43% 96.10% 93.18%
α =0.5 90.63% 95.76% 93.13%
α =0.6 90.43% 95.97% 93.12%
(4) if g(x) &gt; E , E e [0,1] output
f (x) = sgn(g(x)) , else use the statistic
76
α =0.7 90.43% 95.90% 93.09%
α =0.8 90.43% 95.90% 93.09%
α =0.9 90.37% 95.90% 93.05%
Table 6. Results for Person Names Extraction
Based on The Hybrid Model with Different α
Recall Precision F-measure
ε =1 92.53% 84.96% 88.58%
ε =0.9 93.05% 88.81% 90.88%
ε =0.8 92.86% 90.95% 91.89%
ε =0.7 92.46% 92.04% 92.25%
ε =0.6 91.93% 93.22% 92.58%
ε =0.5 91.48% 94.26% 92.85%
ε =0.4 90.76% 95.25% 92.95%
ε =0.3 90.43% 96.10% 93.18%
ε =0.2 90.04% 96.15% 92.99%
ε =0.1 88.73% 96.23% 92.32%
</table>
<tableCaption confidence="0.993566">
Table 7. Results for Person Names Extraction
Based on The Hybrid Model (α =0.4)
</tableCaption>
<subsectionHeader confidence="0.99851">
5.2 Extracting Chinese Location Names
</subsectionHeader>
<bodyText confidence="0.972782333333333">
We use 1.5M characters corpus of year 1998
from the People’s Daily as the training corpus
and extract sentences of year 2000 from the Peo-
ple’s Daily (containing 2919 Chinese location
names) as testing corpus to conduct an open test
experiment. The results are obtained as follows
based on different models.
1) Based on Sole SVM
The Recall, Precision and F-measure using
different number of degree of polynomial kernel
function are given in Table 8. The best result is
obtained when d=2.
</bodyText>
<table confidence="0.9986845">
Recall Precision F-measure
d=1 84.66% 91.95% 88.16%
d=2 86.69% 93.82% 90.12%
d=3 86.27% 94.23% 90.07%
</table>
<tableCaption confidence="0.9559285">
Table 8. Results for Location Names Extraction
Based on Sole SVM
</tableCaption>
<sectionHeader confidence="0.474113" genericHeader="method">
2) Using Hybrid Model
</sectionHeader>
<bodyText confidence="0.997522">
The results for Chinese location names extrac-
tion based on the hybrid model are listed in Ta-
ble 9 (when d=2; α =0.2 in equation (10)). We
can observe that the Recall rises and the Preci-
sion drops on the whole when ε increases. The
synthetic index F-measures are improved when
ε is between 0.1 and 0.7 compared with sole
SVM. The best result is obtained when ε =0.3.
The Recall increases 3.55%, the Precision de-
creases 1.05% and the F-measure increases
1.37%.
</bodyText>
<table confidence="0.999750909090909">
Recall Precision F-measure
ε =1 90.75% 83.00% 86.71%
ε =0.9 90.85% 85.33% 88.01%
ε =0.8 91.42% 87.42% 89.37%
ε =0.7 91.65% 89.05% 90.33%
ε =0.6 91.75% 90.38% 91.06%
ε =0.5 91.32% 90.98% 91.15%
ε =0.4 90.66% 91.87% 91.26%
ε =0.3 90.24% 92.77% 91.49%
ε =0.2 89.10% 93.28% 91.15%
ε =0.1 87.83% 93.38% 90.52%
</table>
<tableCaption confidence="0.992728">
Table 9. Results for Location Names Extraction
Based on The Hybrid Model (α =0.2)
</tableCaption>
<sectionHeader confidence="0.963399" genericHeader="method">
6 Comparison with other work
</sectionHeader>
<bodyText confidence="0.9997135">
The same corpus was also tested using statistics-
based approach to identify Chinese person names
(Huang et al, 2001) and location names (Huang
and Yue, 2003). In their systems, lexical reliabil-
ity and contextual reliability were used to iden-
tify person names and location names calculated
from statistical information drawn from a train-
ing corpus. The results of our models and the
statistics-based methods (Huang 2001; Huang
2003) are shown in Table 10 for comparison. We
can see that the Recall and F-measure in our
method all increase a lot.
</bodyText>
<table confidence="0.9992141">
Recall Precision F-
measure
Person Our 90.10% 96.15% 93.03%
names models
Huang 88.62% 92.37% 90.46%
(2001)
Location Our 90.24% 92.77% 91.49%
names models
Huang 86.86% 91.48% 89.11%
(2003)
</table>
<tableCaption confidence="0.928431">
Table 10. Results of Our Method and Huang
(2001; 2003) for Comparison
</tableCaption>
<sectionHeader confidence="0.994475" genericHeader="conclusions">
7 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.999954818181818">
We recognize Chinese named entities using a
hybrid model combining support vector ma-
chines with statistical methods. The model inte-
grates the advantages of two methods and the
experimental results show that it can achieve
higher F-measure than the sole SVM and indi-
vidual statistical approach.
Future work includes optimizing statistical
models, for example, we can add the probability
information of Chinese named entities in real
texts to compute lexical probability, and we can
</bodyText>
<page confidence="0.993958">
77
</page>
<bodyText confidence="0.9998315">
also use trigram models to compute contextual
probability.
The hybrid model is expected to extend to for-
eign names in transliteration to obtain improved
results by sole SVMs. The identification of trans-
literated names by SVMs has been completed (Li
et al., 2004). The future work includes: set up
statistical models for transliterated names and
combine statistical models with SVMs to identify
transliterated names.
</bodyText>
<sectionHeader confidence="0.99916" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99969767948718">
William J. Black and Argyrios Vasilakopoulos. 2002.
Language Independent Named Entity Classifica-
tion by Modified Transformation-based Learning
and by Decision Tree Induction. The 6th Confer-
ence on Natural Language Learning, Taipei.
Andrew Eliot Borthwick. 1999. A Maximum Entropy
Approach to Named Entity Recognition. PhD Dis-
sertation. New York University.
Xavier Carreras, Lluis Marquez, and Lluis Padro.
2002. Named Entity Extraction Using AdaBoost.
The 6th Conference on Natural Language Learning,
Taipei.
Michael Collins. 2002. Ranking Algorithms for
Named-entity Extraction: Boosting and the Voted
Perceptron. Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-2002), Philadelphia, 489-496.
Chooi-Ling Goh, Masayuki Asahara and Yuji Ma-
tsumoto. 2003. Chinese Unknown Word Identifica-
tion Based on Morphological Analysis and Chunk-
ing. The Companion Volume to the Proceedings of
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2003), Sapporo, 197-200.
De-Gen Huang, Yuan-Sheng Yang, and Xing Wang.
2001. Identification of Chinese Names Based on
Statistics. Journal of Chinese Information Process-
ing, 15(2): 31-37.
De-Gen Huang and Guang-Ling Yue. 2003. Identifi-
cation of Chinese Place Names Based on Statistics.
Journal of Chinese Information Processing, 17(2):
46-52.
Thorsten Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with Many
Relevant Features. In Proceedings of the European
Conference on Machine Learning, 1398:137-142.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with Support Vector Machines. In Proceedings of
NAACL 2001.
Li-Shuang Li, Chun-Rong Chen, De-Gen Huang and
Yuan-Sheng Yang. 2004. Identifying Pronuncia-
tion-Translated Names from Chinese Texts Based
on Support Vector Machines. Advances in Neural
Networks-ISNN 2004, Lecture Notes in Computer
Science, Berlin Heidelberg, 3173: 983-988.
Scott Miller and Michael Crystal. 1998. BBN: De-
scription of the SIFT System as Used for MUC-7.
Proceedings of 7th Message Understanding Con-
ference, Washington.
David D. Palmer. 1997. A Trainable Rule-Based Al-
gorithm for Word Segmentation. In Proc of 35th of
ACL &amp; 8th conf. of EACL, 321-328.
Wen Qin and Chun-Fa Yuan. 2004. Identification of
Chinese Unknown Word Based on Decision Tree.
Journal of Chinese Information Processing, 18(1):
14-19.
Erik Tjong Kim Sang. 2002. Memory-based Named
Entity Recognition. The 6th Conference on Natural
Language Learning, Taipei.
Koichi Takeuchi and Nigel Collier. 2002. Use of
Support Vector Machines in Extended Named En-
tity Recognition. The 6th Conference on Natural
Language Learning, Taipei.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, Berlin.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. John Wiley &amp; Sons, New York.
Ying Yu, Xiao-Long Wang, Bing-Quan Liu, and Hui
Wang. 2004. Efficient SVM-based Recognition of
Chinese Personal Names. High Technology Letters,
10(3): 15-18.
Jun-Sheng Zhang, Shun-De Chen, Ying Zheng, Xian-
Zhong Liu and Shu-Jin Ke. 1992. Large-Corpus-
Based Methods for Chinese Personal Name. Jour-
nal of Chinese Information Processing, 6(3): 7-15.
Guo-Dong Zhou and Jian Su. 2002. Named Entity
Recognition Using an HMM-based Chunk Tagger.
Proceedings of the 40th Annual Meeting of the
ACL, Philadelphia, 473-480.
</reference>
<page confidence="0.998824">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933292">
<title confidence="0.999094">Hybrid Models for Chinese Named Entity Recognition</title>
<author confidence="0.99711">Lishuang Li</author>
<author confidence="0.99711">Tingting Mao</author>
<author confidence="0.99711">Degen Huang</author>
<author confidence="0.99711">Yuansheng</author>
<affiliation confidence="0.999796">Department of Computer Science and Dalian University of</affiliation>
<address confidence="0.978596">116023 Dalian,</address>
<email confidence="0.987802">{computer,huangdg,maotingting1007@sohu.com</email>
<abstract confidence="0.998419578947369">This paper describes a hybrid model and the corresponding algorithm combining support vector machines (SVMs) with statistical methods to improve the performance of SVMs for the task of Chinese Named Entity Recognition (NER). In this algorithm, a threshold of the distance from the test sample to the hyperplane of SVMs in feature space is used to separate SVMs region and statistical method region. If the distance is greater than the given threshold, the test sample is classified using SVMs; otherwise, the statistical model is used. By integrating the advantages of two methods, the hybrid model achieves 93.18% F-measure for Chinese person names and 91.49% Fmeasure for Chinese location names.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William J Black</author>
<author>Argyrios Vasilakopoulos</author>
</authors>
<title>Language Independent Named Entity Classification by Modified Transformation-based Learning and by Decision Tree Induction.</title>
<date>2002</date>
<booktitle>The 6th Conference on Natural Language Learning,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="1842" citStr="Black and Vasilakopoulos, 2002" startWordPosition="274" endWordPosition="277">de users who are looking for person or location names with quick information. Palma and Day (1997) reported that person (PER), location (LOC) and organization (ORG) names are the most difficult sub-tasks as compared to other entities as defined in Message Understanding Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM are mostly near the decision plane (i.e., the hyperplane of SVM in feature space). In order to increa</context>
</contexts>
<marker>Black, Vasilakopoulos, 2002</marker>
<rawString>William J. Black and Argyrios Vasilakopoulos. 2002. Language Independent Named Entity Classification by Modified Transformation-based Learning and by Decision Tree Induction. The 6th Conference on Natural Language Learning, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Eliot Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition. PhD Dissertation.</title>
<date>1999</date>
<location>New York University.</location>
<contexts>
<context position="1742" citStr="Borthwick, 1999" startWordPosition="264" endWordPosition="265">nswering (QA) and machine translation (MT). On its own, NE recognition can also provide users who are looking for person or location names with quick information. Palma and Day (1997) reported that person (PER), location (LOC) and organization (ORG) names are the most difficult sub-tasks as compared to other entities as defined in Message Understanding Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM a</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Eliot Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. PhD Dissertation. New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Marquez</author>
<author>Lluis Padro</author>
</authors>
<title>Named Entity Extraction Using AdaBoost.</title>
<date>2002</date>
<booktitle>The 6th Conference on Natural Language Learning,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="1891" citStr="Carreras et al., 2002" startWordPosition="281" endWordPosition="284"> quick information. Palma and Day (1997) reported that person (PER), location (LOC) and organization (ORG) names are the most difficult sub-tasks as compared to other entities as defined in Message Understanding Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM are mostly near the decision plane (i.e., the hyperplane of SVM in feature space). In order to increase the accuracy of SVM, we propose a hybrid model</context>
</contexts>
<marker>Carreras, Marquez, Padro, 2002</marker>
<rawString>Xavier Carreras, Lluis Marquez, and Lluis Padro. 2002. Named Entity Extraction Using AdaBoost. The 6th Conference on Natural Language Learning, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking Algorithms for Named-entity Extraction: Boosting and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<pages>489--496</pages>
<location>Philadelphia,</location>
<contexts>
<context position="1867" citStr="Collins, 2002" startWordPosition="279" endWordPosition="280">tion names with quick information. Palma and Day (1997) reported that person (PER), location (LOC) and organization (ORG) names are the most difficult sub-tasks as compared to other entities as defined in Message Understanding Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM are mostly near the decision plane (i.e., the hyperplane of SVM in feature space). In order to increase the accuracy of SVM, w</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking Algorithms for Named-entity Extraction: Boosting and the Voted Perceptron. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), Philadelphia, 489-496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chooi-Ling Goh</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chinese Unknown Word Identification Based on Morphological Analysis and Chunking.</title>
<date>2003</date>
<booktitle>The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),</booktitle>
<location>Sapporo,</location>
<contexts>
<context position="1979" citStr="Goh et al., 2003" startWordPosition="296" endWordPosition="299">ization (ORG) names are the most difficult sub-tasks as compared to other entities as defined in Message Understanding Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM are mostly near the decision plane (i.e., the hyperplane of SVM in feature space). In order to increase the accuracy of SVM, we propose a hybrid model combining SVM with a statistical approach for Chinese NER, that is, in the region near </context>
</contexts>
<marker>Goh, Asahara, Matsumoto, 2003</marker>
<rawString>Chooi-Ling Goh, Masayuki Asahara and Yuji Matsumoto. 2003. Chinese Unknown Word Identification Based on Morphological Analysis and Chunking. The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), Sapporo, 197-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>De-Gen Huang</author>
<author>Yuan-Sheng Yang</author>
<author>Xing Wang</author>
</authors>
<title>Identification of Chinese Names Based on Statistics.</title>
<date>2001</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>15</volume>
<issue>2</issue>
<pages>31--37</pages>
<contexts>
<context position="11195" citStr="Huang et al., 2001" startWordPosition="2001" endWordPosition="2004">rname of person names in the SurName. The probability of the first name Pl0 =log2 , Pf(F) is defined as Pf (F)Pf0 (y Pf0 (F) y FirstName ∈ where Pf0 (F) =log2 (N(F) + 2) , N(F) is the number of F in the FirstName. The lexical probability of the person name PN is defined as LP PN P L P F if(PN LF) LP PN P L C P F P F if(PN LFF ) ( ) ( ) ( ) ( ) ( ) ( ( ) ( )) = × = = × × + = l f 1 1 (8) l b f 1 f 2 1 2 , i -2 -1 0 +1 +2 Jiang Ze Min zhu xi n-S n-B n-E n-B n-E Y N N N Y Y Y Y N N I O O B I ) , (7) 74 where Cb is the balance cofficient between the single name and the double name. Here, Cb=0.844 (Huang et al., 2001). 2) contextual probability based on POS tags CP(PN) Chinese person names have characteristic contexual POS tags in real Chinese texts, for example, in the phrase “dui Zhangshuai shuo (say to Zhangshuai)”, the POS tag before the person name “Zhangshuai” is prepnoun and verb occurs after the person name. We define the bigram contextual probability CP(PN) of the person name PN as the following equation: TotalPOS is the total number of the contexual POS tags of every person name in the whole training corpus. (10) where LN is the evaluated location name is the balance cofficient. 1) lexical probab</context>
<context position="21067" citStr="Huang et al, 2001" startWordPosition="3789" endWordPosition="3792">call increases 3.55%, the Precision decreases 1.05% and the F-measure increases 1.37%. Recall Precision F-measure ε =1 90.75% 83.00% 86.71% ε =0.9 90.85% 85.33% 88.01% ε =0.8 91.42% 87.42% 89.37% ε =0.7 91.65% 89.05% 90.33% ε =0.6 91.75% 90.38% 91.06% ε =0.5 91.32% 90.98% 91.15% ε =0.4 90.66% 91.87% 91.26% ε =0.3 90.24% 92.77% 91.49% ε =0.2 89.10% 93.28% 91.15% ε =0.1 87.83% 93.38% 90.52% Table 9. Results for Location Names Extraction Based on The Hybrid Model (α =0.2) 6 Comparison with other work The same corpus was also tested using statisticsbased approach to identify Chinese person names (Huang et al, 2001) and location names (Huang and Yue, 2003). In their systems, lexical reliability and contextual reliability were used to identify person names and location names calculated from statistical information drawn from a training corpus. The results of our models and the statistics-based methods (Huang 2001; Huang 2003) are shown in Table 10 for comparison. We can see that the Recall and F-measure in our method all increase a lot. Recall Precision Fmeasure Person Our 90.10% 96.15% 93.03% names models Huang 88.62% 92.37% 90.46% (2001) Location Our 90.24% 92.77% 91.49% names models Huang 86.86% 91.48%</context>
</contexts>
<marker>Huang, Yang, Wang, 2001</marker>
<rawString>De-Gen Huang, Yuan-Sheng Yang, and Xing Wang. 2001. Identification of Chinese Names Based on Statistics. Journal of Chinese Information Processing, 15(2): 31-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>De-Gen Huang</author>
<author>Guang-Ling Yue</author>
</authors>
<title>Identification of Chinese Place Names Based on Statistics.</title>
<date>2003</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>17</volume>
<issue>2</issue>
<pages>46--52</pages>
<contexts>
<context position="21108" citStr="Huang and Yue, 2003" startWordPosition="3796" endWordPosition="3799">reases 1.05% and the F-measure increases 1.37%. Recall Precision F-measure ε =1 90.75% 83.00% 86.71% ε =0.9 90.85% 85.33% 88.01% ε =0.8 91.42% 87.42% 89.37% ε =0.7 91.65% 89.05% 90.33% ε =0.6 91.75% 90.38% 91.06% ε =0.5 91.32% 90.98% 91.15% ε =0.4 90.66% 91.87% 91.26% ε =0.3 90.24% 92.77% 91.49% ε =0.2 89.10% 93.28% 91.15% ε =0.1 87.83% 93.38% 90.52% Table 9. Results for Location Names Extraction Based on The Hybrid Model (α =0.2) 6 Comparison with other work The same corpus was also tested using statisticsbased approach to identify Chinese person names (Huang et al, 2001) and location names (Huang and Yue, 2003). In their systems, lexical reliability and contextual reliability were used to identify person names and location names calculated from statistical information drawn from a training corpus. The results of our models and the statistics-based methods (Huang 2001; Huang 2003) are shown in Table 10 for comparison. We can see that the Recall and F-measure in our method all increase a lot. Recall Precision Fmeasure Person Our 90.10% 96.15% 93.03% names models Huang 88.62% 92.37% 90.46% (2001) Location Our 90.24% 92.77% 91.49% names models Huang 86.86% 91.48% 89.11% (2003) Table 10. Results of Our M</context>
</contexts>
<marker>Huang, Yue, 2003</marker>
<rawString>De-Gen Huang and Guang-Ling Yue. 2003. Identification of Chinese Place Names Based on Statistics. Journal of Chinese Information Processing, 17(2): 46-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>In Proceedings of the European Conference on Machine Learning,</booktitle>
<pages>1398--137</pages>
<contexts>
<context position="2095" citStr="Joachims, 1998" startWordPosition="314" endWordPosition="315">Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM are mostly near the decision plane (i.e., the hyperplane of SVM in feature space). In order to increase the accuracy of SVM, we propose a hybrid model combining SVM with a statistical approach for Chinese NER, that is, in the region near the decision plane, statistical method is used to classify the samples instead of SVM, and in the region far away fr</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In Proceedings of the European Conference on Machine Learning, 1398:137-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with Support Vector Machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="2122" citStr="Kudo and Matsumoto, 2001" startWordPosition="316" endWordPosition="319">. So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM are mostly near the decision plane (i.e., the hyperplane of SVM in feature space). In order to increase the accuracy of SVM, we propose a hybrid model combining SVM with a statistical approach for Chinese NER, that is, in the region near the decision plane, statistical method is used to classify the samples instead of SVM, and in the region far away from the decision plane, SVM </context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with Support Vector Machines. In Proceedings of NAACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li-Shuang Li</author>
<author>Chun-Rong Chen</author>
<author>De-Gen Huang</author>
<author>Yuan-Sheng Yang</author>
</authors>
<title>Identifying Pronunciation-Translated Names from Chinese Texts Based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>Advances in Neural Networks-ISNN 2004, Lecture Notes in Computer Science,</booktitle>
<volume>3173</volume>
<pages>983--988</pages>
<location>Berlin Heidelberg,</location>
<marker>Li, Chen, Huang, Yang, 2004</marker>
<rawString>Li-Shuang Li, Chun-Rong Chen, De-Gen Huang and Yuan-Sheng Yang. 2004. Identifying Pronunciation-Translated Names from Chinese Texts Based on Support Vector Machines. Advances in Neural Networks-ISNN 2004, Lecture Notes in Computer Science, Berlin Heidelberg, 3173: 983-988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Michael Crystal</author>
</authors>
<date>1998</date>
<booktitle>BBN: Description of the SIFT System as Used for MUC-7. Proceedings of 7th Message Understanding Conference,</booktitle>
<location>Washington.</location>
<contexts>
<context position="1701" citStr="Miller and Crystal, 1998" startWordPosition="257" endWordPosition="260">ks such as information extraction (IE), question answering (QA) and machine translation (MT). On its own, NE recognition can also provide users who are looking for person or location names with quick information. Palma and Day (1997) reported that person (PER), location (LOC) and organization (ORG) names are the most difficult sub-tasks as compared to other entities as defined in Message Understanding Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, t</context>
</contexts>
<marker>Miller, Crystal, 1998</marker>
<rawString>Scott Miller and Michael Crystal. 1998. BBN: Description of the SIFT System as Used for MUC-7. Proceedings of 7th Message Understanding Conference, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
</authors>
<title>A Trainable Rule-Based Algorithm for Word Segmentation. In</title>
<date>1997</date>
<booktitle>Proc of 35th of ACL &amp; 8th conf. of EACL,</booktitle>
<pages>321--328</pages>
<marker>Palmer, 1997</marker>
<rawString>David D. Palmer. 1997. A Trainable Rule-Based Algorithm for Word Segmentation. In Proc of 35th of ACL &amp; 8th conf. of EACL, 321-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Qin</author>
<author>Chun-Fa Yuan</author>
</authors>
<title>Identification of Chinese Unknown Word Based on Decision Tree.</title>
<date>2004</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>18</volume>
<issue>1</issue>
<pages>14--19</pages>
<contexts>
<context position="1778" citStr="Qin and Yuan, 2004" startWordPosition="268" endWordPosition="271">tion (MT). On its own, NE recognition can also provide users who are looking for person or location names with quick information. Palma and Day (1997) reported that person (PER), location (LOC) and organization (ORG) names are the most difficult sub-tasks as compared to other entities as defined in Message Understanding Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM are mostly near the decision plane (i</context>
</contexts>
<marker>Qin, Yuan, 2004</marker>
<rawString>Wen Qin and Chun-Fa Yuan. 2004. Identification of Chinese Unknown Word Based on Decision Tree. Journal of Chinese Information Processing, 18(1): 14-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
</authors>
<title>Memory-based Named Entity Recognition.</title>
<date>2002</date>
<booktitle>The 6th Conference on Natural Language Learning,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="2015" citStr="Sang, 2002" startWordPosition="302" endWordPosition="303"> sub-tasks as compared to other entities as defined in Message Understanding Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM are mostly near the decision plane (i.e., the hyperplane of SVM in feature space). In order to increase the accuracy of SVM, we propose a hybrid model combining SVM with a statistical approach for Chinese NER, that is, in the region near the decision plane, statistical meth</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik Tjong Kim Sang. 2002. Memory-based Named Entity Recognition. The 6th Conference on Natural Language Learning, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichi Takeuchi</author>
<author>Nigel Collier</author>
</authors>
<title>Use of Support Vector Machines in Extended Named Entity Recognition.</title>
<date>2002</date>
<booktitle>The 6th Conference on Natural Language Learning,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="1943" citStr="Takeuchi and Collier, 2002" startWordPosition="288" endWordPosition="291">d that person (PER), location (LOC) and organization (ORG) names are the most difficult sub-tasks as compared to other entities as defined in Message Understanding Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM are mostly near the decision plane (i.e., the hyperplane of SVM in feature space). In order to increase the accuracy of SVM, we propose a hybrid model combining SVM with a statistical approach for Chine</context>
</contexts>
<marker>Takeuchi, Collier, 2002</marker>
<rawString>Koichi Takeuchi and Nigel Collier. 2002. Use of Support Vector Machines in Extended Named Entity Recognition. The 6th Conference on Natural Language Learning, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="4712" citStr="Vapnik (1998)" startWordPosition="768" endWordPosition="769">egative examples so as to maximize the distance between the data points and the hyperplane. Given training examples: n = {( 1, 1 ), ( 2, 2),..., ( , )}, x y x R ∈ , { 1 , 1} , ∈ − + (1) S x y x y yi l l i xi is a feature vector (n dimension) of the i-th sample. is the class (positive(+1) or negayi tive(-1) class) label of the i-th sample. l is the number of the given training samples. SVMs find an “optimal” hyperplane: (wx + b) = 0 to separate the training data into two classes. The optimal hyperplane can be found by solving the following quadratic programming problem (we leave the details to Vapnik (1998)): l subject to ∑ yiαi = 0, i l. 1 The function K(xi, xj) = ϕ(xi) ⋅ ϕ(xj) is called kernel function, is the mapping from priϕ(x) mary input space to feature space. Given a test example, its label y is decided by the following function: f ( ) sgn[ ∑ x = α iy iK ( , ) ] . x i x + b (3) Basically, SVMs are binary classifiers, and can be extended to multi-class classifiers in order to solve multi-class discrimination problems. There are two popular methods to extend a binary classification task to that of K classes: one class vs. all others and pairwise. Here, we employ the simple pairwise method.</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Yu</author>
<author>Xiao-Long Wang</author>
<author>Bing-Quan Liu</author>
<author>Hui Wang</author>
</authors>
<title>Efficient SVM-based Recognition of Chinese Personal Names.</title>
<date>2004</date>
<journal>High Technology Letters,</journal>
<volume>10</volume>
<issue>3</issue>
<pages>15--18</pages>
<contexts>
<context position="1960" citStr="Yu et al., 2004" startWordPosition="292" endWordPosition="295">n (LOC) and organization (ORG) names are the most difficult sub-tasks as compared to other entities as defined in Message Understanding Conference (MUC). So we focus on the recognition of PER, LOC and ORG entities. Recently, machine learning approaches are widely used in NER, including the hidden Markov model (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). Goh et al. (2003) presented a SVM-based chunker to extract Chinese unknown words. It obtained higher F-measure for person names and organization names. Like other classifiers, the misclassified testing samples by SVM are mostly near the decision plane (i.e., the hyperplane of SVM in feature space). In order to increase the accuracy of SVM, we propose a hybrid model combining SVM with a statistical approach for Chinese NER, that is, </context>
</contexts>
<marker>Yu, Wang, Liu, Wang, 2004</marker>
<rawString>Ying Yu, Xiao-Long Wang, Bing-Quan Liu, and Hui Wang. 2004. Efficient SVM-based Recognition of Chinese Personal Names. High Technology Letters, 10(3): 15-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Sheng Zhang</author>
<author>Shun-De Chen</author>
</authors>
<title>Ying Zheng, XianZhong Liu and Shu-Jin Ke.</title>
<date>1992</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>6</volume>
<issue>3</issue>
<pages>7--15</pages>
<marker>Zhang, Shun-De Chen, 1992</marker>
<rawString>Jun-Sheng Zhang, Shun-De Chen, Ying Zheng, XianZhong Liu and Shu-Jin Ke. 1992. Large-CorpusBased Methods for Chinese Personal Name. Journal of Chinese Information Processing, 6(3): 7-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guo-Dong Zhou</author>
<author>Jian Su</author>
</authors>
<title>Named Entity Recognition Using an HMM-based Chunk Tagger.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>473--480</pages>
<location>Philadelphia,</location>
<marker>Zhou, Su, 2002</marker>
<rawString>Guo-Dong Zhou and Jian Su. 2002. Named Entity Recognition Using an HMM-based Chunk Tagger. Proceedings of the 40th Annual Meeting of the ACL, Philadelphia, 473-480.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>