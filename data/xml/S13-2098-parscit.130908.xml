<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016306">
<title confidence="0.748641">
CNGL: Grading Student Answers by Acts of Translation
</title>
<author confidence="0.977727">
Ergun Bic¸ici
</author>
<affiliation confidence="0.9944495">
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
</affiliation>
<email confidence="0.977642">
ebicici@computing.dcu.ie
</email>
<author confidence="0.835447">
Josef van Genabith
</author>
<affiliation confidence="0.980758">
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
</affiliation>
<email confidence="0.987834">
josef@computing.dcu.ie
</email>
<sectionHeader confidence="0.99855" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923620689655">
We invent referential translation machines
(RTMs), a computational model for identify-
ing the translation acts between any two data
sets with respect to a reference corpus se-
lected in the same domain, which can be used
for automatically grading student answers.
RTMs make quality and semantic similarity
judgments possible by using retrieved rele-
vant training data as interpretants for reach-
ing shared semantics. An MTPP (machine
translation performance predictor) model de-
rives features measuring the closeness of the
test sentences to the training data, the diffi-
culty of translating them, and the presence of
acts of translation involved. We view question
answering as translation from the question to
the answer, from the question to the reference
answer, from the answer to the reference an-
swer, or from the question and the answer to
the reference answer. Each view is modeled
by an RTM model, giving us a new perspective
on the ternary relationship between the ques-
tion, the answer, and the reference answer. We
show that all RTM models contribute and a
prediction model based on all four perspec-
tives performs the best. Our prediction model
is the 2nd best system on some tasks according
to the official results of the Student Response
Analysis (SRA 2013) challenge.
</bodyText>
<sectionHeader confidence="0.978731" genericHeader="method">
1 Automatically Grading Student Answers
</sectionHeader>
<bodyText confidence="0.999940735294118">
We introduce a fully automated student answer
grader that performs well in the student response
analysis (SRA) task (Dzikovska et al., 2013) and es-
pecially well in tasks with unseen answers. Auto-
matic grading can be used for assessing the level of
competency for students and estimating the required
tutoring effort in e-learning platforms. It can also
be used to adapt questions according to the average
student performance. Low scored topics can be dis-
cussed further in classrooms, enhancing the overall
coverage of the course material.
The quality estimation task (QET) (Callison-
Burch et al., 2012) aims to develop quality indica-
tors for translations at the sentence-level and pre-
dictors without access to the reference. Bicici et
al. (2013) develop a top performing machine transla-
tion performance predictor (MTPP), which uses ma-
chine learning models over features measuring how
well the test set matches the training set relying on
extrinsic and language independent features.
The student response analysis (SRA)
task (Dzikovska et al., 2013) addresses the fol-
lowing problem. Given a question, a known correct
reference answer, and a student answer, assess the
correctness of the student’s answer. The student
answers are categorized as correct, partially correct
incomplete, contradictory, irrelevant, or non do-
main, in the 5-way task; as correct, contradictory,
or incorrect in the 3-way task; and as correct or
incorrect in the 2-way task.
The student answer correctness prediction prob-
lem involves finding a function f approximating the
student answer correctness given the question (Q),
the answer (A), and the reference answer (R):
</bodyText>
<equation confidence="0.855311">
f(Q, A, R) ^ q(A, R). (1)
</equation>
<bodyText confidence="0.9684335">
We approach f as a supervised learning problem
with (Q, A, R, q(A, R)) tuples being the training
</bodyText>
<page confidence="0.97539">
585
</page>
<bodyText confidence="0.988837071428571">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 585–591, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
data and q(A, R) being the target correctness score.
We model the problem as a translation task where
one possible interpretation is translating Q (source
to translate, S) to R (target translation, T) and evalu-
ating with A (as reference target, RT) (QRA). Since
the information appearing in the question may be re-
peated in the reference answer or may be omitted in
the student answer, it also makes sense to concate-
nate Q and A when translating to R (QARQA). We
obtain 4 different perspectives on the ternary rela-
tionship between Q, A, and R depending on how we
model their relationship as an instance of translation:
</bodyText>
<construct confidence="0.8333725">
QAR: S = Q, T = A, RT = R.
QRA: S = Q, T = R, RT = A.
ARA: S = A, T = R, RT = A.
QARQA : S = Q + A, T = R, RT = Q + A.
</construct>
<sectionHeader confidence="0.916157" genericHeader="method">
2 The Machine Translation Performance
Predictor (MTPP)
</sectionHeader>
<bodyText confidence="0.999956941176471">
In machine translation (MT), pairs of source and tar-
get sentences are used for training statistical MT
(SMT) models. SMT system performance is af-
fected by the amount of training data used as well
as the closeness of the test set to the training set.
MTPP (Bic¸ici et al., 2013) is a top performing ma-
chine translation performance predictor, which uses
machine learning models over features measuring
how well the test set matches the training set to pre-
dict the quality of a translation without using a ref-
erence translation. MTPP measures the coverage of
individual test sentence features and syntactic struc-
tures found in the training set and derives feature
functions measuring the closeness of test sentences
to the available training data, the difficulty of trans-
lating the sentence, and the presence of acts of trans-
lation involved.
</bodyText>
<subsectionHeader confidence="0.989846">
Features for Translation Acts
</subsectionHeader>
<bodyText confidence="0.999811">
MTPP uses n-gram features defined over text or
common cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over which
similarity calculations are made. Unsupervised
parsing with CCL extracts links from base words
to head words, which allow us to obtain structures
representing the grammatical information instanti-
ated in the training and test data. Feature functions
use statistics involving the training set and the test
sentences to determine their closeness. Since they
are language independent, MTPP allows quality es-
timation to be performed extrinsically. Categories
for the 283 features used are listed below and their
detailed descriptions are presented in (Bic¸ici et al.,
2013) where the number of features are given in {#}.
</bodyText>
<listItem confidence="0.995691272727273">
• Coverage {110}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
• Synthetic Translation Performance {6}: Calcu-
lates translation scores achievable according to
the n-gram coverage.
• Length {4}: Calculates the number of words
and characters for S and T and their ratios.
• Feature Vector Similarity {16}: Calculates the
similarities between vector representations.
• Perplexity {90}: Measures the fluency of the
sentences according to language models (LM).
We use both forward ({30}) and backward
({15}) LM based features for S and T.
• Entropy {4}: Calculates the distributional sim-
ilarity of test sentences to the training set.
• Retrieval Closeness {24}: Measures the de-
gree to which sentences close to the test set are
found in the training set.
• Diversity {6}: Measures the diversity of co-
occurring features in the training set.
• IBM1 Translation Probability {16}: Calculates
the translation probability of test sentences us-
ing the training set (Brown et al., 1993).
• Minimum Bayes Retrieval Risk {4}: Calculates
the translation probability for the translation
having the minimum Bayes risk among the re-
trieved training instances.
• Sentence Translation Performance {3}: Calcu-
lates translation scores obtained according to
q(T, R) using BLEU (Papineni et al., 2002),
NIST (Doddington, 2002), or Fi (Bic¸ici and
Yuret, 2011b) for q.
</listItem>
<sectionHeader confidence="0.97284" genericHeader="method">
3 Referential Translation Machine (RTM)
</sectionHeader>
<bodyText confidence="0.998369333333333">
Referential translation machines (RTMs) we de-
velop provide a computational model for quality and
semantic similarity judgments using retrieval of rel-
evant training data (Bic¸ici and Yuret, 2011a; Bic¸ici,
2011) as interpretants for reaching shared seman-
tics (Bic¸ici, 2008). We show that RTM achieves
</bodyText>
<page confidence="0.991207">
586
</page>
<bodyText confidence="0.998811531914893">
very good performance in judging the semantic sim-
ilarity of sentences (Bic¸ici and van Genabith, 2013)
and we can also use RTM to automatically assess
the correctness of student answers to obtain better
results than the baselines proposed by (Dzikovska et
al., 2012), which achieve the best performance on
some tasks (Dzikovska et al., 2013).
RTM is a computational model for identifying the
acts of translation for translating between any given
two data sets with respect to a reference corpus se-
lected in the same domain. RTM can be used for
automatically grading student answers. An RTM
model is based on the selection of common train-
ing data relevant and close to both the training set
and the test set where the selected relevant set of
instances are called the interpretants. Interpretants
allow shared semantics to be possible by behaving
as a reference point for similarity judgments and
providing the context. In semiotics, an interpretant
I interprets the signs used to refer to the real ob-
jects (Bic¸ici, 2008). RTMs provide a model for com-
putational semantics using interpretants as a refer-
ence according to which semantic judgments with
translation acts are made. Each RTM model is a data
translation model between the instances in the train-
ing set and the test set. We use the FDA (Feature De-
cay Algorithms) instance selection model for select-
ing the interpretants (Bic¸ici and Yuret, 2011a) from a
given corpus, which can be monolingual when mod-
eling paraphrasing acts, in which case the MTPP
model is built using the interpretants themselves as
both the source and the target side of the parallel cor-
pus. RTMs map the training and test data to a space
where translation acts can be identified. We view
that acts of translation are ubiquitously used during
communication:
Every act of communication is an act of
translation (Bliss, 2012).
Translation need not be between different languages
and paraphrasing or communication also contain
acts of translation. When creating sentences, we use
our background knowledge and translate informa-
tion content according to the current context.
Given a training set train, a test set test, and
some monolingual corpus C, preferably in the same
domain as the training and test sets, the RTM steps
are:
</bodyText>
<listItem confidence="0.9976435">
1. T = train ∪ test.
2. select(T, C) → I
3. MTPP(I, train) → Ftrain
4. MTPP(I, test) → Ftest
</listItem>
<bodyText confidence="0.999978727272727">
Step 2 selects the interpretants, I, relevant to the
instances in the combined training and test data.
Steps 3 and 4 use I to map train and test to
a new space where similarities between the transla-
tion acts can be derived more easily. RTM relies on
the representativeness of I as a medium for building
translation models for translating between train
and test.
Our encouraging results in the SRA task provides
a greater understanding of the acts of translation we
ubiquitously use when communicating and how they
can be used to predict the performance of trans-
lation, judging the semantic similarity of text, and
evaluating the quality of student answers. RTM and
MTPP models are not data or language specific and
their modeling power and good performance are ap-
plicable across different domains and tasks. RTM
expands the applicability of MTPP by making it fea-
sible when making monolingual quality and simi-
larity judgments and it enhances the computational
scalability by building models over smaller but more
relevant training data as interpretants.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9556135">
SRA involves the prediction on Beetle (student
interactions when learning conceptual knowledge
in the basic electricity and electronics domain)
and SciEntsBank (science assessment questions)
datasets. SciEntsBank is harder due to contain-
ing questions from multiple domains (Dzikovska
et al., 2012). SRA challenge results are eval-
uated with the weighted average F1, F1w =
�
1 N cEC NcF1 (c) and the macro average F1, F1m =
�c, EcEC F1 (c) (Dzikovska et al., 2012).
The lexical baseline system is based on measures
of lexical overlap using 4 features: the number of
overlapping words, F1, Lesk (Lesk, 1986), and co-
sine scores over the words when comparing A and
R ({4}) and Q and R ({4}). Lesk score is calculated
as: L(A, R) = EpEM |p|2/(|A||R|), where M con-
tains the maximal overlapping phrases that match in
</bodyText>
<page confidence="0.987455">
587
</page>
<bodyText confidence="0.99920975">
A and R and IpI is the length of a phrase 1. This lex-
ical baseline is highly competitive: no submission
performed better in the 2-way Beetle unseen ques-
tions task.
</bodyText>
<subsectionHeader confidence="0.916204">
4.1 RTM Models
</subsectionHeader>
<bodyText confidence="0.999984696969697">
We obtain CNGL results for the SRA task as fol-
lows. For each perspective described in Section 1,
we build an RTM model. Each RTM model views
the SRA task from a different perspective using the
283 features extracted dependent on the interpre-
tants using MTPP. We extract the features both on
the training set of 4155 and the test set of 1258 (Q,
A, R) sentence triples for the Beetle task and the
training set of 5251 and the test set of 5835 (Q, A,
R) sentence triples for the SciEntsBank task. The
addition of lexical overlap baseline features slightly
helps. We use the best reference answer if the refer-
ence answer is not identified in the training set.
The training corpus used is the English side of
an out-of-domain corpus on European parliamen-
tary discussions, Europarl (Callison-Burch et al.,
2012) 2, to which we also add the unique sentences
from R. In-domain corpora are likely to improve the
performance. We do not perform any linguistic pro-
cessing or use other external resources. We use only
extrinsic features, or features that are ignorant of any
information intrinsic to, and dependent on, a given
language or domain. We use the training corpus to
build a 5-gram target LM. We use ridge regression
(RR) and support vector regression (SVR) with RBF
kernel (Smola and Sch¨olkopf, 2004). Both of these
models learn a regression function using the features
to estimate a numerical target value. The parameters
that govern the behavior of RR and SVR are the reg-
ularization A for RR and the C, c, and -y parameters
for SVR. At testing time, the predictions are bound
so as to have scores in the range [0, 1], [0, 2], or [0, 4]
and rounded for finding the predicted category.
</bodyText>
<subsectionHeader confidence="0.996135">
4.2 Training Results
</subsectionHeader>
<bodyText confidence="0.9999216">
Table 1 lists the 10-fold cross-validation (CV) re-
sults on the training set for RR and SVR for dif-
ferent RTM systems without the parameter op-
timization. As we combine different perspec-
tives, the performance improves and we use the
</bodyText>
<footnote confidence="0.998698">
1http://search.cpan.org/dist/Text-Similarity/
2We use WMT’13 corpora from www.statmt.org/wmt13/.
</footnote>
<bodyText confidence="0.9919618">
QAR+QRA+ARA+QARQA system for our submis-
sions using RR for run 1, SVR for run 2. ARA per-
forms the best among individual perspectives. Each
additional perspective adds another 283 features to
the representation.
</bodyText>
<table confidence="0.999616">
Fl&amp;quot; / Fi Beetle SciEntsBank
Model RR SVR RR SVR
QAR .38/.49 .45/.57 .21/.30 .28/.36
QRA .33/.50 .33/.53 .22/.31 .29/.42
ARA .45/.54 .50/.60 .21/.30 .30/.38
QARQA .35/.50 .40/.58 .20/.27 .27/.40
QAR+ARA .47/.55 .49/.61 .26/.36 .32/.39
QAR+ARA+QARQA .48/.57 .49/.62 .31/.38 .29/.40
QAR+QRA+ARA+QARQA .48/.56 .48/.61 .31/.38 .29/.40
</table>
<tableCaption confidence="0.999969">
Table 1: Performance on the training set without tuning.
</tableCaption>
<bodyText confidence="0.999467555555556">
We perform tuning on a subset of the Beetle
and SciEntsBank datasets separately after including
the baseline lexical overlap features and optimize
against the performance evaluated with R2, the coef-
ficient of determination. SVR performance is given
in Table 2. The CNGL system significantly outper-
forms the lexical overlap baseline in all tasks for
Beetle and in the 2-way task for SciEntsBank. For
3-way and 5-way, CNGL performs slightly better.
</bodyText>
<table confidence="0.9993298">
Fm / F� Beetle SciEntsBank
1
System 2 3 5 2 3 5
Lexical .74/.75 .53/.56 .46/.53 .61/.64 .43/.55 .29/.41
CNGL .84/.84 .61/.63 .55/.63 .74/.75 .47/.56 .30/.41
</table>
<tableCaption confidence="0.99216">
Table 2: Optimized SVR results vs. lexical overlap base-
line on the training set for 2-way, 3-way, or 5-way tasks.
</tableCaption>
<subsectionHeader confidence="0.996615">
4.3 SRA Challenge Results
</subsectionHeader>
<bodyText confidence="0.9990655">
The SRA task test set also contains instances that be-
long to unseen questions (uQ) and unseen domains
(uD), which make it harder to predict. The train-
ing data provided for the task correspond to learning
with unseen answers (uA). Table 3 presents the SRA
challenge results containing the lexical overlap, our
CNGL SVR submission (RR is slightly worse), and
the maximum and mean results 3.
According to the official results, CNGL SVR is
the 2nd best system based on 5-way evaluation (4th
</bodyText>
<footnote confidence="0.983669">
3Max is not the performance of the best performing system
but the maximum result obtained for each metric and subtask.
</footnote>
<page confidence="0.990054">
588
</page>
<table confidence="0.999908588235294">
Fm/Fi Beetle SciEntsBank
System uA uQ uA uQ uD
Lexical .80/.79 .74/.72 .64/.62 .65/.63 .66/.65
CNGL .80/.81 .67/.68 .55/.57 .56/.58 .56/.57
2 .71/.72 .61/.62 .64/.66 .60/.62 .61/.63
Mean .84/.84 .72/.73 .77/.77 .74/.74 .70/.71
Max
Lexical .55/.58 .48/.50 .40/.52 .39/.52 .42/.55
CNGL .57/.59 .45/.47 .33/.38 .31/.37 .31/.36
3 .54/.55 .41/.42 .48/.56 .39/.51 .39/.51
Mean .72/.73 .58/.60 .65/.71 .47/.63 .49/.62
Max
Lexical .42/.48 .41/.46 .30/.44 .26/.40 .25/.40
CNGL .43/.55 .38/.47 .20/.27 .21/.30 .22/.29
5 .44/.51 .34/.40 .34/.46 .24/.38 .26/.37
Mean .62/.70 .55/.61 .48/.64 .31/.49 .38/.47
Max
</table>
<tableCaption confidence="0.99457825">
Table 3: SRA challenge results: CNGL SVR submission,
the lexical overlap baseline, and the maximum and mean
results for 2-way, 3-way, or 5-way tasks. uA, uQ, and uD
correspond to unseen answers, questions, and domains.
</tableCaption>
<bodyText confidence="0.999710181818182">
result overall) and the 3rd best system based on 2-
way and 3-way evaluation (5th result overall) on the
uQ Beetle task. The SVR model performs better
than the lexical baseline and the mean result in the
Beetle task but performs worse in the SciEntsBank.
The lower performance is likely to be due to using an
out-of-domain training corpus for building the RTM
models and on the uQ and uD tasks, it may also be
due to optimizing on the uA task only. The lower
performance in SciEntsBank is also due to multiple
question domains (Dzikovska et al., 2012).
</bodyText>
<table confidence="0.993212">
SVR Beetle SciEntsBank
F� 2 3 5 2 3 5
1
(a) QAR+ARA .86 .66 .64 .77 .56 .42
(b) QAR+ARA+QARQA .86 .66 .65 .77 .57 .45
(c) QAR+QRA+ARA+QARQA .85 .64 .63 .77 .58 .45
F � 2 3 5 2 3 5
1
(a) QAR+ARA .86 .64 .55 .76 .47 .34
(b) QAR+ARA+QARQA .85 .64 .55 .76 .48 .36
(c) QAR+QRA+ARA+QARQA .85 .62 .54 .76 .49 .35
</table>
<tableCaption confidence="0.99531">
Table 4: Improved SVR performance on the training set
with tuning for 2-way, 3-way, or 5-way tasks.
</tableCaption>
<subsectionHeader confidence="0.991022">
4.4 Improved RTM Models
</subsectionHeader>
<bodyText confidence="0.9998135">
We improve the RTM model with the expansion of
our representation by adding the following features:
</bodyText>
<listItem confidence="0.977801333333333">
• Character n-grams {4}: Calculates the cosine
between the character n-grams (for n=2,3,4,5)
obtained for S and T (B¨ar et al., 2012).
• LIX {2}: Calculates the LIX readability
score (Wikipedia, 2013; Bj¨ornsson, 1968) for
S and T. 4
</listItem>
<bodyText confidence="0.58254">
Table 4 lists the improved results on the training set
after tuning, which shows about 0.04 increase in all
scores when compared with Table 1 and Table 2.
</bodyText>
<figure confidence="0.935702272727273">
Fm/Fi Beetle SciEntsBank
Model uA uQ uA uQ uD
(a) .81/.82 .70/.71 .55/.57 .58/.58 .56/.57
2 (b) .80/.81 .71/.72 .69/.70 .54/.56 .56/.58
(c) .79/.79 .70/.71 .60/.59 .57/.58 .55/.57
(a) .59/.61 .48/.49 .26/.34 .34/.40 .26/.32
3 (b) .60/.62 .47/.48 .36/.43 .31/.38 .29/.34
(c) .58/.60 .46/.48 .41/.48 .30/.39 .29/.34
(a) .47/.56 .37/.45 .19/.22 .22/.33 .22/.29
5 (b) .43/.56 .36/.45 .26/.37 .23/.33 .21/.30
(c) .42/.52 .40/.48 .27/.39 .24/.33 .20/.30
</figure>
<tableCaption confidence="0.714933">
Table 5: Improved SVR results on the SRA task test set.
</tableCaption>
<figure confidence="0.792208636363636">
Fm/Fi SciEntsBank
Model uA uQ uD
(a) .56/.57 .54/.55 .53/.55
2 (b) .57/.58 .53/.54 .56/.57
(c) .57/.58 .55/.57 .57/.59
(a) .36/.45 .33/.44 .39/.49
3 (b) .35/.40 .36/.44 .39/.48
(c) .37/.46 .36/.48 .40/.50
(a) .24/.34 .23/.33 .26/.39
5 (b) .24/.36 .25/.38 .26/.38
(c) .24/.36 .21/.32 .28/.39
</figure>
<tableCaption confidence="0.980161">
Table 6: Improved TREE results on the SRA task test set.
</tableCaption>
<bodyText confidence="0.993948857142857">
Table 5 presents the improved SVR results on the
SRA task test set, which shows about 0.03 increase
in all scores when compared with Table 3. SVR be-
comes the 2nd best system and 2nd best result in
2-way evaluation and the 3rd best system from the
top based on 2-way and 3-way evaluation (5th result
overall) on the uQ Beetle task.
</bodyText>
<footnote confidence="0.893778">
4LIX0 B + C 100A , where A is the number of words, C is
words longer than 6 characters, B is words that start or end with
any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012).
</footnote>
<page confidence="0.997723">
589
</page>
<bodyText confidence="0.999965142857143">
We observe that decision tree regression (Hastie
et al., 2009) (TREE) generalizes to uQ and uD do-
mains better than the RR or SVR models especially
in the SciEntsBank corpus. Table 6 presents TREE
results on the SRA SciEntsBank test set, which
shows significant increase in uQ and uD tasks when
compared with Table 5.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999916625">
Referential translation machines provide a clean
and intuitive computational model for automatically
grading student answers by measuring the acts of
translation involved and achieve to be the 2nd best
system on some tasks in the SRA challenge. RTMs
make quality and semantic similarity judgments
possible based on the retrieval of relevant training
data as interpretants for reaching shared semantics.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999869875">
This work is supported in part by SFI (07/CE/I1142)
as part of the Centre for Next Generation Locali-
sation (www.cngl.ie) at Dublin City University and
in part by the European Commission through the
QTLaunchPad FP7 project (No: 296347). We also
thank the SFI/HEA Irish Centre for High-End Com-
puting (ICHEC) for the provision of computational
facilities and support.
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719191780822">
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics – Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435–440, Montr´eal, Canada, 7-8 June.
Association for Computational Linguistics.
Ergun Bic¸ici and Josef van Genabith. 2013. CNGL-
CORE: Referential translation machines for measur-
ing semantic similarity. In *SEM 2013: The First Joint
Conference on Lexical and Computational Semantics,
Atlanta, Georgia, USA, 13-14 June. Association for
Computational Linguistics.
Ergun Bic¸ici and Deniz Yuret. 2011a. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272–283, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic¸ici and Deniz Yuret. 2011b. RegMT system for
machine translation, system combination, and evalua-
tion. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 323–329, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic¸ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using ex-
trinsic and language independent features. Machine
Translation.
Ergun Bic¸ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc¸ University. Supervisor:
Deniz Yuret.
Ergun Bic¸ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multiagent
and Grid Systems.
Carl Hugo Bj¨ornsson. 1968. L¨asbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311,
June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10–
51, Montr´eal, Canada, June. Association for Compu-
tational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138–145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200–210, Montr´eal, Canada, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
</reference>
<page confidence="0.969304">
590
</page>
<reference confidence="0.999824133333333">
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Kenth Hagstr¨om. 2012. Swedish readability calcula-
tor. https://github.com/keha76/Swedish-Readability-
Calculator.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009. The Elements of Statistical Learning: Data
Mining, Inference and Prediction. Springer-Verlag,
2nd edition.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ’86, pages 24–26, New York, NY,
USA. ACM.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Yoav Seginer. 2007. Learning Syntactic Structure. Ph.D.
thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199–222, August.
Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX.
</reference>
<page confidence="0.998105">
591
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<affiliation confidence="0.775932666666667">CNGL: Grading Student Answers by Acts of Translation Centre for Next Generation Dublin City University, Dublin,</affiliation>
<email confidence="0.979902">ebicici@computing.dcu.ie</email>
<author confidence="0.998107">Josef van</author>
<affiliation confidence="0.987202">Centre for Next Generation Dublin City University, Dublin,</affiliation>
<email confidence="0.992457">josef@computing.dcu.ie</email>
<abstract confidence="0.963885748571428">We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for automatically grading student answers. RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics. An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of acts of translation involved. We view question answering as translation from the question to the answer, from the question to the reference answer, from the answer to the reference answer, or from the question and the answer to the reference answer. Each view is modeled by an RTM model, giving us a new perspective on the ternary relationship between the question, the answer, and the reference answer. We show that all RTM models contribute and a prediction model based on all four perspectives performs the best. Our prediction model the best system on some tasks according to the official results of the Student Response Analysis (SRA 2013) challenge. 1 Automatically Grading Student Answers We introduce a fully automated student answer grader that performs well in the student response analysis (SRA) task (Dzikovska et al., 2013) and eswell in tasks with unseen answers. Automatic grading can be used for assessing the level of competency for students and estimating the required tutoring effort in e-learning platforms. It can also be used to adapt questions according to the average student performance. Low scored topics can be discussed further in classrooms, enhancing the overall coverage of the course material. The quality estimation task (QET) (Callison- Burch et al., 2012) aims to develop quality indicators for translations at the sentence-level and predictors without access to the reference. Bicici et al. (2013) develop a top performing machine translation performance predictor (MTPP), which uses machine learning models over features measuring how well the test set matches the training set relying on extrinsic and language independent features. The student response analysis (SRA) task (Dzikovska et al., 2013) addresses the following problem. Given a question, a known correct reference answer, and a student answer, assess the correctness of the student’s answer. The student answers are categorized as correct, partially correct incomplete, contradictory, irrelevant, or non domain, in the 5-way task; as correct, contradictory, or incorrect in the 3-way task; and as correct or incorrect in the 2-way task. answer correctness prediction probfinding a function the student answer correctness given the question (Q), the answer (A), and the reference answer (R): A, approach a supervised learning problem (Q, A, R, tuples being the training 585 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic (SemEval pages 585–591, Atlanta, Georgia, June 14-15, 2013. Association for Computational Linguistics and the target correctness score. We model the problem as a translation task where one possible interpretation is translating Q (source to translate, S) to R (target translation, T) and evaluating with A (as reference target, RT) (QRA). Since the information appearing in the question may be repeated in the reference answer or may be omitted in the student answer, it also makes sense to concatenate Q and A when translating to R (QARQA). We obtain 4 different perspectives on the ternary relationship between Q, A, and R depending on how we model their relationship as an instance of translation: T RT T RT T RT T RT 2 The Machine Translation Performance Predictor (MTPP) In machine translation (MT), pairs of source and target sentences are used for training statistical MT (SMT) models. SMT system performance is affected by the amount of training data used as well the the test set to the training set. et al., 2013) is a top performing machine translation performance predictor, which uses machine learning models over features measuring how well the test set matches the training set to predict the quality of a translation without using a reference translation. MTPP measures the coverage of individual test sentence features and syntactic structures found in the training set and derives feature functions measuring the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation involved. Features for Translation Acts uses features defined over text or common cover link (CCL) (Seginer, 2007) structures as the basic units of information over which similarity calculations are made. Unsupervised parsing with CCL extracts links from base words to head words, which allow us to obtain structures representing the grammatical information instantiated in the training and test data. Feature functions use statistics involving the training set and the test sentences to determine their closeness. Since they are language independent, MTPP allows quality estimation to be performed extrinsically. Categories the used are listed below and their descriptions are presented in et al., where the number of features are given in Coverage the degree to which the test features are found in the trainset for both S and T Synthetic Translation Performance Calculates translation scores achievable according to coverage. Length the number of words and characters for S and T and their ratios. Feature Vector Similarity the similarities between vector representations. Perplexity the fluency of the sentences according to language models (LM). use both forward and backward LM based features for S and T. Entropy the distributional similarity of test sentences to the training set. Retrieval Closeness the degree to which sentences close to the test set are found in the training set. Diversity the diversity of cooccurring features in the training set. IBM1 Translation Probability the translation probability of test sentences using the training set (Brown et al., 1993). Minimum Bayes Retrieval Risk the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. Sentence Translation Performance Calculates translation scores obtained according to BLEU (Papineni et al., 2002), (Doddington, 2002), or and 2011b) for 3 Referential Translation Machine (RTM) Referential translation machines (RTMs) we develop provide a computational model for quality and semantic similarity judgments using retrieval of reltraining data and Yuret, 2011a; 2011) as interpretants for reaching shared seman- 2008). We show that RTM achieves 586 very good performance in judging the semantic simof sentences and van Genabith, 2013) and we can also use RTM to automatically assess the correctness of student answers to obtain better results than the baselines proposed by (Dzikovska et al., 2012), which achieve the best performance on some tasks (Dzikovska et al., 2013). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. RTM can be used for automatically grading student answers. An RTM model is based on the selection of common training data relevant and close to both the training set and the test set where the selected relevant set of instances are called the interpretants. Interpretants allow shared semantics to be possible by behaving as a reference point for similarity judgments and providing the context. In semiotics, an interpretant the signs used to refer to the real ob- 2008). RTMs provide a model for computational semantics using interpretants as a reference according to which semantic judgments with translation acts are made. Each RTM model is a data translation model between the instances in the training set and the test set. We use the FDA (Feature Decay Algorithms) instance selection model for selectthe interpretants and Yuret, 2011a) from a given corpus, which can be monolingual when modeling paraphrasing acts, in which case the MTPP model is built using the interpretants themselves as both the source and the target side of the parallel corpus. RTMs map the training and test data to a space where translation acts can be identified. We view that acts of translation are ubiquitously used during communication: Every act of communication is an act 2012). Translation need not be between different languages and paraphrasing or communication also contain acts of translation. When creating sentences, we use our background knowledge and translate information content according to the current context. a training set a test set and monolingual corpus preferably in the same domain as the training and test sets, the RTM steps are: T 2. I 3. 4. 2 selects the interpretants, relevant to the instances in the combined training and test data. 3 and 4 use map a new space where similarities between the translation acts can be derived more easily. RTM relies on representativeness of a medium for building models for translating between Our encouraging results in the SRA task provides a greater understanding of the acts of translation we ubiquitously use when communicating and how they can be used to predict the performance of translation, judging the semantic similarity of text, and evaluating the quality of student answers. RTM and MTPP models are not data or language specific and their modeling power and good performance are applicable across different domains and tasks. RTM expands the applicability of MTPP by making it feasible when making monolingual quality and similarity judgments and it enhances the computational scalability by building models over smaller but more relevant training data as interpretants. 4 Experiments SRA involves the prediction on Beetle (student interactions when learning conceptual knowledge in the basic electricity and electronics domain) and SciEntsBank (science assessment questions) datasets. SciEntsBank is harder due to containing questions from multiple domains (Dzikovska et al., 2012). SRA challenge results are evalwith the weighted average � and the macro average (Dzikovska et al., 2012). The lexical baseline system is based on measures of lexical overlap using 4 features: the number of words, Lesk (Lesk, 1986), and cosine scores over the words when comparing A and and Q and R Lesk score is calculated = where M contains the maximal overlapping phrases that match in 587 and R and is the length of a phrase This lexical baseline is highly competitive: no submission performed better in the 2-way Beetle unseen questions task. 4.1 RTM Models We obtain CNGL results for the SRA task as follows. For each perspective described in Section 1, we build an RTM model. Each RTM model views the SRA task from a different perspective using the 283 features extracted dependent on the interpretants using MTPP. We extract the features both on the training set of 4155 and the test set of 1258 (Q, A, R) sentence triples for the Beetle task and the training set of 5251 and the test set of 5835 (Q, A, R) sentence triples for the SciEntsBank task. The addition of lexical overlap baseline features slightly helps. We use the best reference answer if the reference answer is not identified in the training set. The training corpus used is the English side of an out-of-domain corpus on European parliamentary discussions, Europarl (Callison-Burch et al., to which we also add the unique sentences from R. In-domain corpora are likely to improve the performance. We do not perform any linguistic processing or use other external resources. We use only extrinsic features, or features that are ignorant of any information intrinsic to, and dependent on, a given language or domain. We use the training corpus to build a 5-gram target LM. We use ridge regression (RR) and support vector regression (SVR) with RBF kernel (Smola and Sch¨olkopf, 2004). Both of these models learn a regression function using the features to estimate a numerical target value. The parameters that govern the behavior of RR and SVR are the reg- RR and the and for SVR. At testing time, the predictions are bound as to have scores in the range or and rounded for finding the predicted category. 4.2 Training Results Table 1 lists the 10-fold cross-validation (CV) results on the training set for RR and SVR for different RTM systems without the parameter optimization. As we combine different perspectives, the performance improves and we use the use WMT’13 corpora from www.statmt.org/wmt13/. QAR+QRA+ARA+QARQA system for our submissions using RR for run 1, SVR for run 2. ARA performs the best among individual perspectives. Each additional perspective adds another 283 features to the representation. Model Beetle SciEntsBank RR SVR RR SVR QAR .38/.49 .45/.57 .21/.30 .28/.36 QRA .33/.50 .33/.53 .22/.31 .29/.42 ARA .45/.54 .50/.60 .21/.30 .30/.38 QARQA .35/.50 .40/.58 .20/.27 .27/.40 QAR+ARA .47/.55 .49/.61 .26/.36 .32/.39 QAR+ARA+QARQA .48/.57 .49/.62 .31/.38 .29/.40 QAR+QRA+ARA+QARQA .48/.56 .48/.61 .31/.38 .29/.40 Table 1: Performance on the training set without tuning. We perform tuning on a subset of the Beetle and SciEntsBank datasets separately after including the baseline lexical overlap features and optimize the performance evaluated with the coefficient of determination. SVR performance is given in Table 2. The CNGL system significantly outperforms the lexical overlap baseline in all tasks for Beetle and in the 2-way task for SciEntsBank. For 3-way and 5-way, CNGL performs slightly better. Beetle SciEntsBank 1 System 2 3 5 2 3 5 Lexical .74/.75 .53/.56 .46/.53 .61/.64 .43/.55 .29/.41 CNGL .84/.84 .61/.63 .55/.63 .74/.75 .47/.56 .30/.41 Table 2: Optimized SVR results vs. lexical overlap baseline on the training set for 2-way, 3-way, or 5-way tasks. 4.3 SRA Challenge Results The SRA task test set also contains instances that belong to unseen questions (uQ) and unseen domains (uD), which make it harder to predict. The training data provided for the task correspond to learning with unseen answers (uA). Table 3 presents the SRA challenge results containing the lexical overlap, our CNGL SVR submission (RR is slightly worse), and maximum and mean results According to the official results, CNGL SVR is the 2nd best system based on 5-way evaluation (4th is not the performance of the best performing system but the maximum result obtained for each metric and subtask. 588 System Beetle SciEntsBank uA uQ uA uQ uD Lexical .80/.79 .80/.81 .71/.72 .84/.84 .74/.72 .67/.68 .61/.62 .72/.73 .64/.62 .55/.57 .64/.66 .77/.77 .65/.63 .56/.58 .60/.62 .74/.74 .66/.65 .56/.57 .61/.63 .70/.71 CNGL 2</abstract>
<title confidence="0.565126">Mean</title>
<author confidence="0.715025">Max</author>
<note confidence="0.677541625">Lexical .55/.58 .57/.59 .54/.55 .72/.73 .48/.50 .45/.47 .41/.42 .58/.60 .40/.52 .33/.38 .48/.56 .65/.71 .39/.52 .31/.37 .39/.51 .47/.63 .42/.55 .31/.36 .39/.51 .49/.62 CNGL 3 Mean Max Lexical .42/.48 .43/.55 .44/.51 .62/.70 .41/.46 .38/.47 .34/.40 .55/.61 .30/.44 .20/.27 .34/.46 .48/.64 .26/.40 .21/.30 .24/.38 .31/.49 .25/.40 .22/.29 .26/.37 .38/.47 CNGL 5</note>
<title confidence="0.675587">Mean</title>
<author confidence="0.772848">Max</author>
<abstract confidence="0.894174747474748">Table 3: SRA challenge results: CNGL SVR submission, the lexical overlap baseline, and the maximum and mean results for 2-way, 3-way, or 5-way tasks. uA, uQ, and uD correspond to unseen answers, questions, and domains. result overall) and the 3rd best system based on 2way and 3-way evaluation (5th result overall) on the uQ Beetle task. The SVR model performs better than the lexical baseline and the mean result in the Beetle task but performs worse in the SciEntsBank. The lower performance is likely to be due to using an out-of-domain training corpus for building the RTM models and on the uQ and uD tasks, it may also be due to optimizing on the uA task only. The lower performance in SciEntsBank is also due to multiple question domains (Dzikovska et al., 2012). SVR Beetle SciEntsBank 1 2 3 5 2 3 5 (a) QAR+ARA .86 .66 .64 .77 .56 .42 (b) QAR+ARA+QARQA .86 .66 .65 .77 .57 .45 (c) QAR+QRA+ARA+QARQA .85 .64 .63 .77 .58 .45 2 3 5 2 3 5 1 (a) QAR+ARA .86 .64 .55 .76 .47 .34 (b) QAR+ARA+QARQA .85 .64 .55 .76 .48 .36 (c) QAR+QRA+ARA+QARQA .85 .62 .54 .76 .49 .35 Table 4: Improved SVR performance on the training set with tuning for 2-way, 3-way, or 5-way tasks. 4.4 Improved RTM Models We improve the RTM model with the expansion of our representation by adding the following features: Character the cosine the character (for obtained for S and T (B¨ar et al., 2012). LIX the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for and T. Table 4 lists the improved results on the training set tuning, which shows about increase in all scores when compared with Table 1 and Table 2. Model Beetle SciEntsBank uA uQ uA uQ uD (a) .81/.82 .70/.71 .55/.57 .58/.58 .56/.57 2 (b) .80/.81 .71/.72 .69/.70 .54/.56 .56/.58 (c) .79/.79 .70/.71 .60/.59 .57/.58 .55/.57 (a) .59/.61 .48/.49 .26/.34 .34/.40 .26/.32 3 (b) .60/.62 .47/.48 .36/.43 .31/.38 .29/.34 (c) .58/.60 .46/.48 .41/.48 .30/.39 .29/.34 (a) .47/.56 .37/.45 .19/.22 .22/.33 .22/.29 5 (b) .43/.56 .36/.45 .26/.37 .23/.33 .21/.30 (c) .42/.52 .40/.48 .27/.39 .24/.33 .20/.30 Table 5: Improved SVR results on the SRA task test set. Model SciEntsBank uA uQ uD (a) .56/.57 .54/.55 .53/.55 2 (b) .57/.58 .53/.54 .56/.57 (c) .57/.58 .55/.57 .57/.59 (a) .36/.45 .33/.44 .39/.49 3 (b) .35/.40 .36/.44 .39/.48 (c) .37/.46 .36/.48 .40/.50 (a) .24/.34 .23/.33 .26/.39 5 (b) .24/.36 .25/.38 .26/.38 (c) .24/.36 .21/.32 .28/.39 Table 6: Improved TREE results on the SRA task test set. Table 5 presents the improved SVR results on the task test set, which shows about increase in all scores when compared with Table 3. SVR becomes the 2nd best system and 2nd best result in 2-way evaluation and the 3rd best system from the top based on 2-way and 3-way evaluation (5th result overall) on the uQ Beetle task. C , where A is the number of words, C is words longer than 6 characters, B is words that start or end with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012). 589 We observe that decision tree regression (Hastie et al., 2009) (TREE) generalizes to uQ and uD domains better than the RR or SVR models especially in the SciEntsBank corpus. Table 6 presents TREE results on the SRA SciEntsBank test set, which shows significant increase in uQ and uD tasks when compared with Table 5. 5 Conclusion Referential translation machines provide a clean and intuitive computational model for automatically grading student answers by measuring the acts of involved and achieve to be the best system on some tasks in the SRA challenge. RTMs make quality and semantic similarity judgments possible based on the retrieval of relevant training data as interpretants for reaching shared semantics. Acknowledgments This work is supported in part by SFI (07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie) at Dublin City University and in part by the European Commission through the QTLaunchPad FP7 project (No: 296347). We also thank the SFI/HEA Irish Centre for High-End Computing (ICHEC) for the provision of computational facilities and support.</abstract>
<note confidence="0.773716590909091">References Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity mea- In 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval pages 435–440, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics. and Josef van Genabith. 2013. CNGL- CORE: Referential translation machines for measursemantic similarity. In 2013: The First Joint on Lexical and Computational Atlanta, Georgia, USA, 13-14 June. Association for Computational Linguistics. and Deniz Yuret. 2011a. Instance selection for machine translation using feature decay al- In of the Sixth Workshop on Machine pages 272–283, Edinburgh, Scotland, July. Association for Computational</note>
<abstract confidence="0.79932195">Linguistics. and Deniz Yuret. 2011b. RegMT system for machine translation, system combination, and evalua- In of the Sixth Workshop on Sta- Machine pages 323–329, Edinburgh, Scotland, July. Association for Computational Linguistics. Declan Groves, and Josef van Genabith. 2013. Predicting sentence translation quality using exand language independent features. 2011. Regression Model of Machine Ph.D. thesis, University. Supervisor: Deniz Yuret. 2008. Consensus ontologies in socially multiagent systems. of Multiagent Grid Hugo Bj¨ornsson. 1968. Liber. Bliss. 2012. Comedy is translation, February. chris bliss comedy is translation.html.</abstract>
<author confidence="0.778981">Peter F Brown</author>
<author confidence="0.778981">Stephen A Della Pietra</author>
<author confidence="0.778981">Vincent J Della</author>
<note confidence="0.81421556">Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter esti- 19(2):263–311, June. Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine In of the Seventh Workon Statistical Machine pages 10– 51, Montr´eal, Canada, June. Association for Computational Linguistics. George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence In of the second international conference on Human Language Technology pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris Brew. 2012. Towards effective tutorial feedback for explanation questions: A dataset and baselines. of the 2012 Conference of the North American Chapter of the Association for Computa- Linguistics: Human Language pages 200–210, Montr´eal, Canada, June. Association for Computational Linguistics.</note>
<author confidence="0.867232">Myroslava O Dzikovska</author>
<author confidence="0.867232">Rodney Nielsen</author>
<author confidence="0.867232">Chris Brew</author>
<author confidence="0.867232">Claudia Leacock</author>
<author confidence="0.867232">Danilo Giampiccolo</author>
<author confidence="0.867232">Luisa Ben-</author>
<abstract confidence="0.458793">tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang. 2013. Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment</abstract>
<note confidence="0.855912111111111">590 In 2013: The First Joint Conference Lexical and Computational Atlanta, Georgia, USA, 13-14 June. Association for Computational Linguistics. Kenth Hagstr¨om. 2012. Swedish readability calculator. https://github.com/keha76/Swedish-Readability- Calculator. Trevor Hastie, Robert Tibshirani, and Jerome Friedman.</note>
<affiliation confidence="0.483518">Elements of Statistical Learning: Data Inference and Springer-Verlag,</affiliation>
<abstract confidence="0.85506">2nd edition. Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine from an ice cream cone. In of the</abstract>
<note confidence="0.822117529411765">5th annual international conference on Systems docu- SIGDOC ’86, pages 24–26, New York, NY, USA. ACM. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a method for automatic evalof machine translation. In of 40th Annual Meeting of the Association for Computational pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics. Seginer. 2007. Syntactic Ph.D. thesis, Universiteit van Amsterdam. Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial support vector regression. and Comput- 14(3):199–222, August. Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX. 591</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>435--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 435–440, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Josef van Genabith</author>
</authors>
<title>CNGLCORE: Referential translation machines for measuring semantic similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<marker>Bic¸ici, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici and Josef van Genabith. 2013. CNGLCORE: Referential translation machines for measuring semantic similarity. In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics, Atlanta, Georgia, USA, 13-14 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>Instance selection for machine translation using feature decay algorithms.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>272--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Bic¸ici, Yuret, 2011</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2011a. Instance selection for machine translation using feature decay algorithms. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 272–283, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>RegMT system for machine translation, system combination, and evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>323--329</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Bic¸ici, Yuret, 2011</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2011b. RegMT system for machine translation, system combination, and evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 323–329, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Declan Groves</author>
<author>Josef van Genabith</author>
</authors>
<title>Predicting sentence translation quality using extrinsic and language independent features. Machine Translation.</title>
<date>2013</date>
<marker>Bic¸ici, Groves, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici, Declan Groves, and Josef van Genabith. 2013. Predicting sentence translation quality using extrinsic and language independent features. Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>The Regression Model of Machine Translation.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Koc¸ University. Supervisor: Deniz Yuret.</institution>
<marker>Bic¸ici, 2011</marker>
<rawString>Ergun Bic¸ici. 2011. The Regression Model of Machine Translation. Ph.D. thesis, Koc¸ University. Supervisor: Deniz Yuret.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>Consensus ontologies in socially interacting multiagent systems.</title>
<date>2008</date>
<journal>Journal of Multiagent and Grid Systems.</journal>
<marker>Bic¸ici, 2008</marker>
<rawString>Ergun Bic¸ici. 2008. Consensus ontologies in socially interacting multiagent systems. Journal of Multiagent and Grid Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Hugo Bj¨ornsson</author>
</authors>
<date>1968</date>
<note>L¨asbarhet. Liber.</note>
<marker>Bj¨ornsson, 1968</marker>
<rawString>Carl Hugo Bj¨ornsson. 1968. L¨asbarhet. Liber.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Bliss</author>
</authors>
<title>Comedy is translation, February. http://www.ted.com/talks/ chris bliss comedy is translation.html.</title>
<date>2012</date>
<contexts>
<context position="9661" citStr="Bliss, 2012" startWordPosition="1561" endWordPosition="1562">s in the training set and the test set. We use the FDA (Feature Decay Algorithms) instance selection model for selecting the interpretants (Bic¸ici and Yuret, 2011a) from a given corpus, which can be monolingual when modeling paraphrasing acts, in which case the MTPP model is built using the interpretants themselves as both the source and the target side of the parallel corpus. RTMs map the training and test data to a space where translation acts can be identified. We view that acts of translation are ubiquitously used during communication: Every act of communication is an act of translation (Bliss, 2012). Translation need not be between different languages and paraphrasing or communication also contain acts of translation. When creating sentences, we use our background knowledge and translate information content according to the current context. Given a training set train, a test set test, and some monolingual corpus C, preferably in the same domain as the training and test sets, the RTM steps are: 1. T = train ∪ test. 2. select(T, C) → I 3. MTPP(I, train) → Ftrain 4. MTPP(I, test) → Ftest Step 2 selects the interpretants, I, relevant to the instances in the combined training and test data. S</context>
</contexts>
<marker>Bliss, 2012</marker>
<rawString>Chris Bliss. 2012. Comedy is translation, February. http://www.ted.com/talks/ chris bliss comedy is translation.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7096" citStr="Brown et al., 1993" startWordPosition="1145" endWordPosition="1148">sentations. • Perplexity {90}: Measures the fluency of the sentences according to language models (LM). We use both forward ({30}) and backward ({15}) LM based features for S and T. • Entropy {4}: Calculates the distributional similarity of test sentences to the training set. • Retrieval Closeness {24}: Measures the degree to which sentences close to the test set are found in the training set. • Diversity {6}: Measures the diversity of cooccurring features in the training set. • IBM1 Translation Probability {16}: Calculates the translation probability of test sentences using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or Fi (Bic¸ici and Yuret, 2011b) for q. 3 Referential Translation Machine (RTM) Referential translation machines (RTMs) we develop provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data (Bic¸ici and Yuret, 201</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="13022" citStr="Callison-Burch et al., 2012" startWordPosition="2131" endWordPosition="2134">tive using the 283 features extracted dependent on the interpretants using MTPP. We extract the features both on the training set of 4155 and the test set of 1258 (Q, A, R) sentence triples for the Beetle task and the training set of 5251 and the test set of 5835 (Q, A, R) sentence triples for the SciEntsBank task. The addition of lexical overlap baseline features slightly helps. We use the best reference answer if the reference answer is not identified in the training set. The training corpus used is the English side of an out-of-domain corpus on European parliamentary discussions, Europarl (Callison-Burch et al., 2012) 2, to which we also add the unique sentences from R. In-domain corpora are likely to improve the performance. We do not perform any linguistic processing or use other external resources. We use only extrinsic features, or features that are ignorant of any information intrinsic to, and dependent on, a given language or domain. We use the training corpus to build a 5-gram target LM. We use ridge regression (RR) and support vector regression (SVR) with RBF kernel (Smola and Sch¨olkopf, 2004). Both of these models learn a regression function using the features to estimate a numerical target value</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10– 51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="7422" citStr="Doddington, 2002" startWordPosition="1194" endWordPosition="1195">which sentences close to the test set are found in the training set. • Diversity {6}: Measures the diversity of cooccurring features in the training set. • IBM1 Translation Probability {16}: Calculates the translation probability of test sentences using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or Fi (Bic¸ici and Yuret, 2011b) for q. 3 Referential Translation Machine (RTM) Referential translation machines (RTMs) we develop provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data (Bic¸ici and Yuret, 2011a; Bic¸ici, 2011) as interpretants for reaching shared semantics (Bic¸ici, 2008). We show that RTM achieves 586 very good performance in judging the semantic similarity of sentences (Bic¸ici and van Genabith, 2013) and we can also use RTM to automatically assess the correctness of student answers to obtain better results th</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Rodney D Nielsen</author>
<author>Chris Brew</author>
</authors>
<title>Towards effective tutorial feedback for explanation questions: A dataset and baselines.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>200--210</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="8075" citStr="Dzikovska et al., 2012" startWordPosition="1293" endWordPosition="1296">11b) for q. 3 Referential Translation Machine (RTM) Referential translation machines (RTMs) we develop provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data (Bic¸ici and Yuret, 2011a; Bic¸ici, 2011) as interpretants for reaching shared semantics (Bic¸ici, 2008). We show that RTM achieves 586 very good performance in judging the semantic similarity of sentences (Bic¸ici and van Genabith, 2013) and we can also use RTM to automatically assess the correctness of student answers to obtain better results than the baselines proposed by (Dzikovska et al., 2012), which achieve the best performance on some tasks (Dzikovska et al., 2013). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. RTM can be used for automatically grading student answers. An RTM model is based on the selection of common training data relevant and close to both the training set and the test set where the selected relevant set of instances are called the interpretants. Interpretants allow shared semantics to be possible by behaving as a reference point</context>
<context position="11521" citStr="Dzikovska et al., 2012" startWordPosition="1858" endWordPosition="1861">ood performance are applicable across different domains and tasks. RTM expands the applicability of MTPP by making it feasible when making monolingual quality and similarity judgments and it enhances the computational scalability by building models over smaller but more relevant training data as interpretants. 4 Experiments SRA involves the prediction on Beetle (student interactions when learning conceptual knowledge in the basic electricity and electronics domain) and SciEntsBank (science assessment questions) datasets. SciEntsBank is harder due to containing questions from multiple domains (Dzikovska et al., 2012). SRA challenge results are evaluated with the weighted average F1, F1w = � 1 N cEC NcF1 (c) and the macro average F1, F1m = �c, EcEC F1 (c) (Dzikovska et al., 2012). The lexical baseline system is based on measures of lexical overlap using 4 features: the number of overlapping words, F1, Lesk (Lesk, 1986), and cosine scores over the words when comparing A and R ({4}) and Q and R ({4}). Lesk score is calculated as: L(A, R) = EpEM |p|2/(|A||R|), where M contains the maximal overlapping phrases that match in 587 A and R and IpI is the length of a phrase 1. This lexical baseline is highly competi</context>
<context position="17571" citStr="Dzikovska et al., 2012" startWordPosition="2866" endWordPosition="2869">s. uA, uQ, and uD correspond to unseen answers, questions, and domains. result overall) and the 3rd best system based on 2- way and 3-way evaluation (5th result overall) on the uQ Beetle task. The SVR model performs better than the lexical baseline and the mean result in the Beetle task but performs worse in the SciEntsBank. The lower performance is likely to be due to using an out-of-domain training corpus for building the RTM models and on the uQ and uD tasks, it may also be due to optimizing on the uA task only. The lower performance in SciEntsBank is also due to multiple question domains (Dzikovska et al., 2012). SVR Beetle SciEntsBank F� 2 3 5 2 3 5 1 (a) QAR+ARA .86 .66 .64 .77 .56 .42 (b) QAR+ARA+QARQA .86 .66 .65 .77 .57 .45 (c) QAR+QRA+ARA+QARQA .85 .64 .63 .77 .58 .45 F � 2 3 5 2 3 5 1 (a) QAR+ARA .86 .64 .55 .76 .47 .34 (b) QAR+ARA+QARQA .85 .64 .55 .76 .48 .36 (c) QAR+QRA+ARA+QARQA .85 .62 .54 .76 .49 .35 Table 4: Improved SVR performance on the training set with tuning for 2-way, 3-way, or 5-way tasks. 4.4 Improved RTM Models We improve the RTM model with the expansion of our representation by adding the following features: • Character n-grams {4}: Calculates the cosine between the character</context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, 2012</marker>
<rawString>Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris Brew. 2012. Towards effective tutorial feedback for explanation questions: A dataset and baselines. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200–210, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Rodney Nielsen</author>
<author>Chris Brew</author>
<author>Claudia Leacock</author>
<author>Danilo Giampiccolo</author>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="1761" citStr="Dzikovska et al., 2013" startWordPosition="266" endWordPosition="269">swer to the reference answer. Each view is modeled by an RTM model, giving us a new perspective on the ternary relationship between the question, the answer, and the reference answer. We show that all RTM models contribute and a prediction model based on all four perspectives performs the best. Our prediction model is the 2nd best system on some tasks according to the official results of the Student Response Analysis (SRA 2013) challenge. 1 Automatically Grading Student Answers We introduce a fully automated student answer grader that performs well in the student response analysis (SRA) task (Dzikovska et al., 2013) and especially well in tasks with unseen answers. Automatic grading can be used for assessing the level of competency for students and estimating the required tutoring effort in e-learning platforms. It can also be used to adapt questions according to the average student performance. Low scored topics can be discussed further in classrooms, enhancing the overall coverage of the course material. The quality estimation task (QET) (CallisonBurch et al., 2012) aims to develop quality indicators for translations at the sentence-level and predictors without access to the reference. Bicici et al. (2</context>
<context position="8150" citStr="Dzikovska et al., 2013" startWordPosition="1305" endWordPosition="1308"> machines (RTMs) we develop provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data (Bic¸ici and Yuret, 2011a; Bic¸ici, 2011) as interpretants for reaching shared semantics (Bic¸ici, 2008). We show that RTM achieves 586 very good performance in judging the semantic similarity of sentences (Bic¸ici and van Genabith, 2013) and we can also use RTM to automatically assess the correctness of student answers to obtain better results than the baselines proposed by (Dzikovska et al., 2012), which achieve the best performance on some tasks (Dzikovska et al., 2013). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. RTM can be used for automatically grading student answers. An RTM model is based on the selection of common training data relevant and close to both the training set and the test set where the selected relevant set of instances are called the interpretants. Interpretants allow shared semantics to be possible by behaving as a reference point for similarity judgments and providing the context. In semiotics, an inter</context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, Leacock, Giampiccolo, Bentivogli, Clark, Dagan, Dang, 2013</marker>
<rawString>Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew, Claudia Leacock, Danilo Giampiccolo, Luisa Bentivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang. 2013. Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge. In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics, Atlanta, Georgia, USA, 13-14 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenth Hagstr¨om</author>
</authors>
<date>2012</date>
<note>Swedish readability calculator. https://github.com/keha76/Swedish-ReadabilityCalculator.</note>
<marker>Hagstr¨om, 2012</marker>
<rawString>Kenth Hagstr¨om. 2012. Swedish readability calculator. https://github.com/keha76/Swedish-ReadabilityCalculator.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome Friedman</author>
</authors>
<date>2009</date>
<booktitle>The Elements of Statistical Learning: Data Mining, Inference and Prediction.</booktitle>
<publisher>Springer-Verlag,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="19919" citStr="Hastie et al., 2009" startWordPosition="3282" endWordPosition="3285">roved TREE results on the SRA task test set. Table 5 presents the improved SVR results on the SRA task test set, which shows about 0.03 increase in all scores when compared with Table 3. SVR becomes the 2nd best system and 2nd best result in 2-way evaluation and the 3rd best system from the top based on 2-way and 3-way evaluation (5th result overall) on the uQ Beetle task. 4LIX0 B + C 100A , where A is the number of words, C is words longer than 6 characters, B is words that start or end with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012). 589 We observe that decision tree regression (Hastie et al., 2009) (TREE) generalizes to uQ and uD domains better than the RR or SVR models especially in the SciEntsBank corpus. Table 6 presents TREE results on the SRA SciEntsBank test set, which shows significant increase in uQ and uD tasks when compared with Table 5. 5 Conclusion Referential translation machines provide a clean and intuitive computational model for automatically grading student answers by measuring the acts of translation involved and achieve to be the 2nd best system on some tasks in the SRA challenge. RTMs make quality and semantic similarity judgments possible based on the retrieval of </context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2009</marker>
<rawString>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer-Verlag, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems documentation, SIGDOC ’86,</booktitle>
<pages>24--26</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11828" citStr="Lesk, 1986" startWordPosition="1917" endWordPosition="1918">iments SRA involves the prediction on Beetle (student interactions when learning conceptual knowledge in the basic electricity and electronics domain) and SciEntsBank (science assessment questions) datasets. SciEntsBank is harder due to containing questions from multiple domains (Dzikovska et al., 2012). SRA challenge results are evaluated with the weighted average F1, F1w = � 1 N cEC NcF1 (c) and the macro average F1, F1m = �c, EcEC F1 (c) (Dzikovska et al., 2012). The lexical baseline system is based on measures of lexical overlap using 4 features: the number of overlapping words, F1, Lesk (Lesk, 1986), and cosine scores over the words when comparing A and R ({4}) and Q and R ({4}). Lesk score is calculated as: L(A, R) = EpEM |p|2/(|A||R|), where M contains the maximal overlapping phrases that match in 587 A and R and IpI is the length of a phrase 1. This lexical baseline is highly competitive: no submission performed better in the 2-way Beetle unseen questions task. 4.1 RTM Models We obtain CNGL results for the SRA task as follows. For each perspective described in Section 1, we build an RTM model. Each RTM model views the SRA task from a different perspective using the 283 features extrac</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation, SIGDOC ’86, pages 24–26, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="7397" citStr="Papineni et al., 2002" startWordPosition="1189" endWordPosition="1192"> {24}: Measures the degree to which sentences close to the test set are found in the training set. • Diversity {6}: Measures the diversity of cooccurring features in the training set. • IBM1 Translation Probability {16}: Calculates the translation probability of test sentences using the training set (Brown et al., 1993). • Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or Fi (Bic¸ici and Yuret, 2011b) for q. 3 Referential Translation Machine (RTM) Referential translation machines (RTMs) we develop provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data (Bic¸ici and Yuret, 2011a; Bic¸ici, 2011) as interpretants for reaching shared semantics (Bic¸ici, 2008). We show that RTM achieves 586 very good performance in judging the semantic similarity of sentences (Bic¸ici and van Genabith, 2013) and we can also use RTM to automatically assess the correctness of student answers to</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Learning Syntactic Structure.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Universiteit van Amsterdam.</institution>
<contexts>
<context position="5389" citStr="Seginer, 2007" startWordPosition="875" endWordPosition="876">which uses machine learning models over features measuring how well the test set matches the training set to predict the quality of a translation without using a reference translation. MTPP measures the coverage of individual test sentence features and syntactic structures found in the training set and derives feature functions measuring the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation involved. Features for Translation Acts MTPP uses n-gram features defined over text or common cover link (CCL) (Seginer, 2007) structures as the basic units of information over which similarity calculations are made. Unsupervised parsing with CCL extracts links from base words to head words, which allow us to obtain structures representing the grammatical information instantiated in the training and test data. Feature functions use statistics involving the training set and the test sentences to determine their closeness. Since they are language independent, MTPP allows quality estimation to be performed extrinsically. Categories for the 283 features used are listed below and their detailed descriptions are presented </context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Learning Syntactic Structure. Ph.D. thesis, Universiteit van Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>2004</date>
<journal>Statistics and Computing,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>Smola, Sch¨olkopf, 2004</marker>
<rawString>Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial on support vector regression. Statistics and Computing, 14(3):199–222, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<date>2013</date>
<note>Lix. http://en.wikipedia.org/wiki/LIX.</note>
<contexts>
<context position="18302" citStr="Wikipedia, 2013" startWordPosition="3005" endWordPosition="3006">.45 (c) QAR+QRA+ARA+QARQA .85 .64 .63 .77 .58 .45 F � 2 3 5 2 3 5 1 (a) QAR+ARA .86 .64 .55 .76 .47 .34 (b) QAR+ARA+QARQA .85 .64 .55 .76 .48 .36 (c) QAR+QRA+ARA+QARQA .85 .62 .54 .76 .49 .35 Table 4: Improved SVR performance on the training set with tuning for 2-way, 3-way, or 5-way tasks. 4.4 Improved RTM Models We improve the RTM model with the expansion of our representation by adding the following features: • Character n-grams {4}: Calculates the cosine between the character n-grams (for n=2,3,4,5) obtained for S and T (B¨ar et al., 2012). • LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for S and T. 4 Table 4 lists the improved results on the training set after tuning, which shows about 0.04 increase in all scores when compared with Table 1 and Table 2. Fm/Fi Beetle SciEntsBank Model uA uQ uA uQ uD (a) .81/.82 .70/.71 .55/.57 .58/.58 .56/.57 2 (b) .80/.81 .71/.72 .69/.70 .54/.56 .56/.58 (c) .79/.79 .70/.71 .60/.59 .57/.58 .55/.57 (a) .59/.61 .48/.49 .26/.34 .34/.40 .26/.32 3 (b) .60/.62 .47/.48 .36/.43 .31/.38 .29/.34 (c) .58/.60 .46/.48 .41/.48 .30/.39 .29/.34 (a) .47/.56 .37/.45 .19/.22 .22/.33 .22/.29 5 (b) .43/.56 .36/.45 .26/.37 .23/.33 .21/.30 (c) .4</context>
</contexts>
<marker>Wikipedia, 2013</marker>
<rawString>Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>