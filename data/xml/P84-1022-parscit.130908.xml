<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.922938">
A PARSING ARCHITECTURE BASED ON DISTRIBUTED MEMORY MACHINES
</title>
<author confidence="0.97275">
Jon M. Slack
</author>
<affiliation confidence="0.9637945">
Department of Psychology
Open University
</affiliation>
<address confidence="0.658351">
Milton Keynes MK7 6AA
</address>
<email confidence="0.333674">
ENGLAND
</email>
<sectionHeader confidence="0.763014" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9999882">
The paper begins by defining a class of
distributed memory machines which have useful
properties as retrieval and filtering devices.
These memory mechanisms store large numbers of
associations on a single composite vector. They
provide a natural format for encoding the
syntactic and semantic constraints associated
with linguistic elements. A computational
architecture for parsing natural language is
proposed which utilises the retrieval and
associative features of these devices. The
parsing mechanism is based on the principles of
Lexical Functional Grammar and the paper
demonstrates how these principles can be derived
from the properties of the memory mechanisms.
</bodyText>
<sectionHeader confidence="0.960899" genericHeader="keywords">
I INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999040944444445">
Recently, interest has focussed on
computational architectures employing massively
parallel processing [1,2]. Some of these
systems have used a distributed form of
knowledge representation [3]. This type of
representation encodes an item of knowledge in
terms of the relationships among a collection of
elementary processing units, and such
assemblages can encode large numbers of items.
Representational similarity and the ability to
generalize are the principal features of such
memory systems. The next section defines a
distributed memory machine which incorporates
some of the computational advantages of
distributed representations within a traditional
von Neumann architecture. The rest of the paper
explores the properties of such machines as the
basis for natural language parsing.
</bodyText>
<sectionHeader confidence="0.998197" genericHeader="introduction">
II DISTRIBUTED MEMORY MACHINES
</sectionHeader>
<bodyText confidence="0.964040263157895">
Distributed memory machines (DMM) can be
represented formally by the septuple
DMM=(V,X,Y,Q,q0,p,A), where
V is a finite set denoting the total vocabulary;
X is a finite set of inputs, and XmV;
Y is a finite set of acceptable outputs and YcV;
Q is a set of internal states;
q0 is a distinguished initial state;
OlQxX--&gt;Q, the retrieval function;
A:Q--&gt;QxY, the output function.
Further, where Y&apos; denotes the set of all finite
concatenations of the elements of the set Y,
QcY&apos;, and therefore QcV&apos;. This statement
represents the notion that internal states of
DMMs can encode multiple outputs or hypotheses.
The vocabulary, V, can be represented by the
space Ik, where I is some interval range defined
within a chosen number system, N; ImN. The
elements of X, Y and Q are encoded as k-element
vectors, referred to as memory vectors.
A. Holographic Associative Memory
One form of DMM is the holographic associative
memory [4,5,6] which encodes large numbers of
associations on a single composite vector.
Items of information are encoded as k-element
zero-centred vectors over an interval such as
[-1,+1]; &lt;X&gt;=(...x1,x0,x.0...). Two items, &lt;A&gt;
and &lt;B&gt; (angular brackets denote memory
vectors), are associated in memory through the
operation of convolution. This method of
association formation is fundamental to the
concept of holographic memory and the resulting
associative trace is denoted &lt;A&gt;*&lt;B&gt;. The
operation of convolution is define by the
equation has the
following properties&apos;
[7]:
Commutative: &lt;A&gt;*&lt;B&gt; = &lt;B&gt;*&lt;A&gt;,
Associative: &lt;A&gt;*(&lt;B&gt;*&lt;C&gt;)
Further, where a delta vector, denoted &amp;, is
defined as a vector that has values of zero on
all features except the central feature, which
has a value of one, then &lt;A&gt;*G= &lt;A&gt;. Moreover,
&lt;A&gt;*0 = 0, where 0 is a zero vector in which all
feature values are zero. Convolving an item
with an attenuated delta vector (i.e., a vector
with values of zero on all features except the
central one, which has a value between 0 and 1)
produces the original item with a strength that
is equal to the value of the central feature of
the attenuated delta vector.
The initial state, (10, encodes all the
associations stored in the machine. In this
model, associative traces are concatenated (+)
through the operations of vector addition and
normalization to produce a single vector.
Overlapping associative items produce composite
</bodyText>
<page confidence="0.992436">
92
</page>
<bodyText confidence="0.999837695652174">
vectors which represent both the range of items
stored and the central tendency of the those
items. This form of prototype generation is a
basic property of distributed memories.
The retrieval function, Ft , is simulated by the
operation of correlation. If the state,
encodes the association &lt;A&gt;*&lt;B&gt;, then presenting
say &lt;A&gt; as an input, or retrieval key, produces
a new state, $0.11 which encodes the item &lt;B&gt;&apos;, a
noisy version of &lt;B&gt;, under the operation of
correlation. This operation is defined by the
equation (&lt;A&gt;#&lt;B&gt;).=i.A0mgiand has the
following properties: An item correlated with
itself, autocorrelation, produces an
approximation to a delta vector. If two similar
memory vectors are correlated, the central
feature of the resulting vector will be equal to
their similarity, or dot product, producing an
attenuated delta vector. If the two items are
completely independent, correlation produces a
zero vector.
The relation between convolution and
correlation is given by
</bodyText>
<equation confidence="0.9308205">
&lt;A&gt;C&lt;A&gt;*&lt;B&gt;) = (&lt;A&gt;W&lt;A&gt;)*&lt;B&gt; +
(&lt;A)V&lt;B&gt;)*&lt;A&gt; + noise ...(1)
</equation>
<bodyText confidence="0.99078875">
where the noise component results from some of
the less significant cross products. Assuming
that &lt;A&gt; and &lt;B&gt; are unrelated, Equation (1)
becomes:
</bodyText>
<equation confidence="0.958462">
&lt;A&gt;W&lt;A&gt;*&lt;B&gt;) =S*&lt;B&gt; + 0*&lt;A&gt; + noise
= &lt;B&gt; + 0 + noise
</equation>
<bodyText confidence="0.999966333333333">
Extending these results to a composite trace,
suppose that q encodes two associated pairs of
four unrelated items forming the vector (&lt;A&gt;*&lt;B&gt;
+ &lt;C&gt;*&lt;D&gt;). When &lt;A&gt; is given as the retrieval
cue, the reconstruction can be characterized as
follows:
</bodyText>
<equation confidence="0.869283333333333">
&lt;A&gt;K&lt;A&gt;*&lt;B&gt; + &lt;C&gt;*&lt;D&gt;)
(&lt;A&gt;#&lt;A&gt;)*&lt;B&gt; + (&lt;A)W&lt;B&gt;)*&lt;A&gt; + noise
+ (&lt;A)#&lt;C&gt;)*&lt;D&gt; + ((A&gt;#&lt;D&gt;)*&lt;C&gt; + noise
</equation>
<bodyText confidence="0.97316184375">
•S*&lt;B&gt;+0*&lt;A&gt;+noise+0*&lt;D&gt;+0*&lt;C&gt;+noise
= &lt;B&gt; + noise + noise
When the additional unrelated items are added to
the memory trace their affect on retrieval is to
add noise to the reconstructed item &lt;B&gt;, which
was associated with the retrieval cue. In a
situation in which the encoded items are related
to each other, the composite trace causes all of
the related items to contribute to the
reconstructed pattern, in addition to producing
noise. The amount of noise added to a retrieved
item is a function of both the amount of
information held on the composite memory vector
and the size of the vector.
III BUILDING NATURAL LANGUAGE PARSERS
A. Case-Frame Parsing
The computational properties of distributed
memory machines (OHM) make them natural
mechanisms for case-frame parsing. Consider a
DMM which encodes case-frame structures of the
following form:
&lt;Pred&gt;*(&lt;C1&gt;*&lt;F1&gt; + &lt;C2&gt;*&lt;P2&gt; + ...+ &lt;Cn&gt;*&lt;Pn&gt;)
where &lt;Pred&gt; is the vector representing the
predicate associated with the verb of an input
clause; &lt;C1&gt; to &lt;Cn&gt; are the case vectors such
as &lt;agent&gt;, &lt;instrument&gt;, etc., and &lt;P1&gt; to &lt;Pn&gt;
are vectors representing prototype concepts
which can fill the associated cases. These
structures can be made more complex by including
tagging vectors which indicate such features as
obligatory case, as shown in the case-frame
vector for the predicate BREAK:
</bodyText>
<equation confidence="0.692851">
(&lt;agent&gt;*&lt;aniobj+natforce&gt; + &lt;object&gt;*&lt;physobj&gt;
*&lt;oblig&gt; + &lt;instrument&gt;*&lt;physobj&gt;)
</equation>
<bodyText confidence="0.999972421052632">
In this example, the object case has a prototype
covering the category of physical objects, and
is tagged as obligatory.
The initial state of the OHM, go, encodes the
concatenation of the set of case-frame vectors
stored by the parser. The system receives two
types of inputs, noun concept vectors
representing noun phrases, and predicate vectors
representing the verb components. If the system
is in state go only a predicate vector input
produces a significant new state representing
the case-frame structure associated with it.
Once in this state, noun vector inputs identify
the case slots they can potentially fill as
illustrated in the following example:
In parsing the sentence Fred broke the window
with a stone, the input vector encoding broke
will retrieve the case-frame structure for break
given above. The input of &lt;Fred&gt; now gives
</bodyText>
<equation confidence="0.9993966">
&lt;Fred&gt;41:(&lt;agent&gt;*&lt;Pa&gt;+&lt;obj&gt;*&lt;Po&gt;+&lt;instr&gt;*&lt;P0) =
&lt;FredWagent&gt;*&lt;Pa&gt;+&lt;Fred&gt;O&lt;Pa&gt;*&lt;agent&gt; + =
0*&lt;Pa&gt;+ea*&lt;agent&gt; + 0*&lt;Po&gt;+e0*&lt;obj&gt; +
0*&lt;Pi&gt;+es*&lt;instr&gt; ■
%Cogent&gt; + eiobj&gt; + giinstr&gt;
</equation>
<bodyText confidence="0.998431461538462">
wheree-is a measure of the similarity between
the vectors, and underlying concepts, &lt;Fred&gt; and
the case prototype &lt;Pj&gt;. In this example,
&lt;Fred&gt; would be identified as the agent because
e0 and ex. would be low relative to ea. The
vector is &amp;quot;cleaned-up&amp;quot; by a threshold function
which is a component of the output function,A.
This process is repeated for the other noun
concepts in the sentence, linking &lt;window&gt; and
&lt;stone&gt; with the object and instrument cases,
respectively. However, the parser requires
additional machinery to handle the large set of
sentences in which the case assignment is
ambiguous using semantic knowledge alone.
B. Encoding Syntactic Knowledge
Unambiguous case assignment can only be
achieved through the integration of syntactic
and semantic processing. Moreover, an adequate
parser should generate an encoding of the
grammatical relations between sentential elements
in addition to a semantic representation. The rest
of the paper demonstrates how the properties of
DMMa can be combined with the ideas embodied in
the theory of Lexical-functional Grammar (LFG) [8]
in a parser which builds both types of relational
structure.
</bodyText>
<page confidence="0.994561">
93
</page>
<bodyText confidence="0.9987176">
In LFG the mapping between grammatical and
semantic relations is represented directly in
the semantic form of the lexical entries for
verbs. For example, the lexical entry for the
verb hands is given by
</bodyText>
<equation confidence="0.81485075">
hands: V, eparticiple) = NONE
etense) = PRESENT
esubj num) = SG
bred) = HAND(esubj)eobj2)eobj)1
</equation>
<bodyText confidence="0.875550216216216">
where the arguments of the predicate HAND are
ordered such that they map directly onto the
arguments of the semantic predicate-argument
structure. The order and value of the arguments
in a. lexical entry are transformed by lexical
rules, such as the passive, to produce new
lexical entries, e.g., HAND[ebyobj)esubj)etoobj)].
The direct mapping between lexical predicates and
case-frame structures is encoded on the case-frame
DMM by augmenting the vectors as follows:
Hands:- &lt;HAND&gt;*(&lt;agent&gt;*&lt;Pa&gt;*&lt;subj&gt; +
&lt;object&gt;*&lt;Po&gt;*&lt;obj2)+&lt;goal&gt;*&lt;Pg&gt;*&lt;obJ&gt;)
When the SUBJ component has been identified
through syntactic processing the resulting
association vector, for example &lt;subj&gt;*&lt;John&gt;
for the sentence John handed Mary the book, will
retrieve &lt;agent&gt; on input to the CF-DMM,
according to the principles specified above.
The multiple lexical entries produced by lexical
rules have corresponding multiple case-frame
vectors which are tagged by the appropriate
grammatical vector. The CF-DMM encodes multiple
case-frame entries for verbs, and the grammatical
vector tags, such as &lt;PASSIVE&gt;, generated by the
syntactic component, are input to the CF-DMM to
retrieve the appropriate case-frame for the verb.
The grammatical relations between the
sentential elements are represented in the form
of functional structure (f-structures) as in
LFG. These structures correspond to embedded
lista of attribute-value pairs, and because of
the Uniqueness criterion which governs their
format they are efficiently encoded as memory
vectors. As an example, the grammatical
relations for the sentence John handed Mary a
book are encoded in the f-structure below:
MEM&amp;quot;
</bodyText>
<sectionHeader confidence="0.8883595" genericHeader="method">
SUBJ [NUM SG
PRED &apos;JOHN]
TENSE PAST
OBJ2 &apos;SPEC A
NUM SG
PRED &apos;BOOK&apos;
</sectionHeader>
<bodyText confidence="0.9761722">
The lists of grammatical functions and features
are encoded as single vectors under the +
operator, and the embedded structure is
preserved by the associative operator, *. The
f-structure is encoded by the vector
</bodyText>
<figure confidence="0.54413375">
(&lt;SUBJ&gt;*(&lt;NUM&gt;*&lt;SG&gt;+&lt;PRED&gt;*&lt;JOHN&gt;) + &lt;TENSE&gt;
*&lt;PAST&gt; + &lt;PRED&gt;*(&lt;HAND&gt;*(&lt;tSUBJ&gt;*&lt;TOBJ2&gt;*
‹tOBJ&gt;)) + &lt;OBJ&gt;*(&lt;NUM&gt;*&lt;SG&gt;*&lt;PRED&gt;*&lt;MARY&gt;)+
&lt;OBJ2&gt;*(&lt;SPEC&gt;*&lt;A&gt;+&lt;NUM&gt;*&lt;SG)+&lt;PRED&gt;*&lt;BOOK&gt;))
</figure>
<bodyText confidence="0.995253736842105">
This compatibility between f-structures and
memory vectors is the basis for an efficient
procedure for deriving f-structures from input
strings. In LFG f-structures are generated in
three steps. First, a context-free grammar
(CPG) is used to derive an input string&apos;s
constituent structure (C-structure). The grammar
is augmented so that it generates a phrase
structure tree which includes statements about
the properties of the string&apos;s f-structure. In
the next step, this structure is condensed to
derive a series of equations, called the functional
description of the string. Finally, the f-structure
is derived from the f-description. The properties
of DMMs enable a simple procedure to be written
which derives f-structures from augmented phrase
structure trees, obviating the need for an
f-description. Consider the tree in figure 1
generated for our example sentence:
</bodyText>
<figure confidence="0.300227">
esusJ)=4 t=4
</figure>
<figureCaption confidence="0.838595333333333">
John handed Mary a book
Figure 1. Augmented Phrase Structure Tree
OPRED)=HAND[..]
</figureCaption>
<bodyText confidence="0.964553466666667">
The f-structure, encoded as a memory vector, can
be derived from this tree by the following
procedure. First, all the grammatical
functions, features and semantic forms must be
encoded as vectors. The41.-variables, f,-f4,
have no values at this point; they are derived
by the procedure. All the vectors dominated by
a node are concatenated to produce a single
vector at that node. The symbol &apos;=&apos; is
interpreted as the association operator ,*.
Applying this interpretation to the tree from
the bottom up produces a memory vector for the
value of fl which encodes the f-structure for
the string, as given above. Accordingly, f2
takes the value (&lt;1&apos;N1J4&gt;*&lt;SG)+&lt;TPRED&gt;*&lt;JOHN&gt;);
</bodyText>
<subsectionHeader confidence="0.77663">
applying the rule specified at the node, (fo SUBJ)=fa
</subsectionHeader>
<bodyText confidence="0.9696905">
gives &lt;,SUBJ&gt;*(&lt;tNUM&gt;*&lt;SG&gt;+&lt;tPRED&gt;*&lt;JOHN&gt;) as a
component of fo. The other components of fo are
derived in the same way. The front-end CFG can
be veiwed as generating the control structure
for the derivation of a memory vector which
represents the input string&apos;s f-structure.
</bodyText>
<figure confidence="0.993664647058823">
PRED &apos;HAND[( SUBJ)( OBJ2)( OBJ)]&apos;
OBJ
[
1UM Sc
PRED &apos;MARY]
V N
\NPf4
eNUM)-SG
OPRED)=JOHN)
eSPEC)=A ONUM)=SG
ONUM)-SG OPRED)=BOOK
DET
fs
TENSE )-PAST
OBJ)-4.
(INUM)=SG
ePRED)=MARY eOBJ2)=4.
</figure>
<page confidence="0.996904">
94
</page>
<bodyText confidence="0.992518588235294">
The properties of memory vectors also enable
the procedure to automatically determine the
consistency pf the structure. For example, in
deriving the value of 4 the concatenation
operator merges the (itNUM)=SG features for A and
book to form a single component of the f‘vector,
(&lt;SPEC&gt;*&lt;A&gt;+&lt;NUM&gt;*&lt;SG)+&lt;PRED&gt;*&lt;MARY&gt;). Rowever,
if the two features had not matched, producing
the vector component &lt;NUM&gt;*(&lt;SG&gt;+&lt;PL&gt;) for
example, the vectors encoding the incompatible
feature values are set such that their
concatenation produces a special control vector
which signals the mismatch.
C. A Parsing Architecture
The ideas outlined above are combined in the
design of a tentative parsing architecture shown
in figure 2. The diamonds denote DMMs, and the
</bodyText>
<figureCaption confidence="0.996189">
Figure 2. Parsing Architecture
</figureCaption>
<bodyText confidence="0.999906">
ellipse denotes a form of DMM functioning as a
working memory for encoding temporary f-structures.
As elements of the input string enter the
lexicon their associated entries are retrieved.
The syntactic category of the element is passed
onto the CFG, and the lexical schemata {e.g.,
(tPRED)=&apos;JOHN&apos;}, encoded as memory vectors, are
passed to the f-structure working memory. The
lexical entry associated with the verb is passed
to the case-frame memory to retrieve the
appropriate set of structures. The partial
results of the CFG control the formation of
memory vectors in the f-structure memory, as
indicated by the broad arrow. The CFG also
generates grammatical vectors as inputs for
case-frame memory to select the appropriate
structure from the multiple encodings associated
with each verb. The partial f-structure
encoding can then be used as input to the
case-frame memory to assign the semantic forms
of grammatical functions to case slots. When
the end of the string is reached both the
case-frame instantiation and the f-structure
should be complete.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="method">
IV CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999875458333333">
This paper attempts to demonstrate the value of
distributed memory machines as components of a
parsing system which generates both semantic and
grammatical relational structures. The ideas
presented are similar to those being developed
within the connectionist paradigm [1]. Small,
and his colleagues [9], have proposed a parsing
model based directly on connectionist principles.
The computational architecture consists of a large
number of appropriately connected computing units
communicating through weighted levels of excitation
and inhibition. The ideas presented here differ
from those embodied in the connectionist parser
in that they emphasise distributed information
storage and retrieval, rather than distributed
parallel processing. Retrieval and filtering
are achieved through simple computable functions
operating on k-element arrays, in contrast to
the complex interactions of the independent
units in connectionist models. In figure 2,
although the network of machines requires
heterarchical control, the architecture can be
considered to be at the lower end of the family
of parallel processing machines [10].
</bodyText>
<sectionHeader confidence="0.962517" genericHeader="method">
V REFERENCES
</sectionHeader>
<reference confidence="0.999799692307692">
[1] Feldman, J.A. and Ballard, D.H. Connection-
1st models and their properties. Cognitive
Science, 1982, 6, 205-254.
[2] Hinton, G.E. and Anderson, J.A. (Eds)
Parallel Models of Associative Memory.
Hillsdale, NJ: Lawrence Erlbaum Associates,
1981.
[3] Hinton, G.E. Shape representation in parallel
systems. In Proceedings of the Seventh
International Joint Conference on Artificial
Intelligence, Vol. 2, Vancouver BC, Canada,
August, 1981.
[4] Longuet-Higgins, B.C., Willshaw, D.J., and
Bunemann, 0.P. Theories of associative recall.
Quarterly Reviews of Biophysics, 1970, 3,
223-244.
[5] Murdock, B.B. A theory for the storage and
retrieval of item and associative information.
Psychological Review, 1982, 89, 609-627.
[6] Kohonen, T. Associative memory _-_ A system-
theoretical approach. Berlin: Springer-
Verlag, 1977.
[7] Borsellino, A., and Poggio, T. Convolution
and Correlation algebras. Kybernetik,
1973, 13, 113-122.
[8] Kaplan, R., and Bresnan, J. Lexical-Functional
Grammar: A formal system for grammatical
representation. In J. Bresnan (ed.), The
Mental Representation of Grammatical Relations.
Cambridge,&apos;Mass.:MIT Press, 1982.
[9] Small, S.L., Cottrell, G.W., and Shastri, L.
Toward connectionist parsing. In Proceedings
of the National Conference on Artificial
Intelligence, Pittsburgh, Pennsylvania, 1982.
[10] Fahlman, S.E., Hinton, G.E., and Sejnowski, T.
Massively parallel architectures for Al: NETL,
THISTLE, and BOLTZMANN machines. In Proceed-
ings of the National Conference on Artificial
Intelligence, Washington D.C., 1983.
</reference>
<figure confidence="0.7987615">
Context-Free
Grammar
</figure>
<page confidence="0.970683">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.011105">
<title confidence="0.999787">A PARSING ARCHITECTURE BASED ON DISTRIBUTED MEMORY MACHINES</title>
<author confidence="1">Jon M Slack</author>
<affiliation confidence="0.999859">Department of Psychology Open University</affiliation>
<address confidence="0.743919">Milton Keynes MK7 6AA ENGLAND</address>
<abstract confidence="0.995378604562738">The paper begins by defining a class of distributed memory machines which have useful properties as retrieval and filtering devices. These memory mechanisms store large numbers of associations on a single composite vector. They provide a natural format for encoding the syntactic and semantic constraints associated with linguistic elements. A computational architecture for parsing natural language is proposed which utilises the retrieval and associative features of these devices. The parsing mechanism is based on the principles of Lexical Functional Grammar and the paper demonstrates how these principles can be derived from the properties of the memory mechanisms. Recently, interest has focussed on computational architectures employing massively parallel processing [1,2]. Some of these systems have used a distributed form of knowledge representation [3]. This type of representation encodes an item of knowledge in terms of the relationships among a collection of elementary processing units, and such assemblages can encode large numbers of items. Representational similarity and the ability to generalize are the principal features of such memory systems. The next section defines a distributed memory machine which incorporates some of the computational advantages of distributed representations within a traditional von Neumann architecture. The rest of the paper explores the properties of such machines as the basis for natural language parsing. MEMORY MACHINES Distributed memory machines (DMM) can be represented formally by the septuple where V is a finite set denoting the total vocabulary; X is a finite set of inputs, and XmV; Y is a finite set of acceptable outputs and YcV; Q is a set of internal states; q0 is a distinguished initial state; OlQxX--&gt;Q, the retrieval function; A:Q--&gt;QxY, the output function. Further, where Y&apos; denotes the set of all finite concatenations of the elements of the set Y, QcY&apos;, and therefore QcV&apos;. This statement represents the notion that internal states of DMMs can encode multiple outputs or hypotheses. The vocabulary, V, can be represented by the where I is some interval range defined within a chosen number system, N; ImN. The elements of X, Y and Q are encoded as k-element referred to as vectors. Associative Memory One form of DMM is the holographic associative memory [4,5,6] which encodes large numbers of associations on a single composite vector. Items of information are encoded as k-element zero-centred vectors over an interval such as Two items, &lt;A&gt; and &lt;B&gt; (angular brackets denote memory vectors), are associated in memory through the of method of association formation is fundamental to the concept of holographic memory and the resulting associative trace is denoted &lt;A&gt;*&lt;B&gt;. The operation of convolution is define by the equation has the following properties&apos; [7]: Commutative: &lt;A&gt;*&lt;B&gt; = &lt;B&gt;*&lt;A&gt;, Associative: &lt;A&gt;*(&lt;B&gt;*&lt;C&gt;) where a delta vector, denoted defined as a vector that has values of zero on all features except the central feature, which has a value of one, then &lt;A&gt;*G= &lt;A&gt;. Moreover, &lt;A&gt;*0 = 0, where 0 is a zero vector in which all feature values are zero. Convolving an item with an attenuated delta vector (i.e., a vector with values of zero on all features except the central one, which has a value between 0 and 1) produces the original item with a strength that is equal to the value of the central feature of the attenuated delta vector. initial state, encodes all the associations stored in the machine. In this model, associative traces are concatenated (+) through the operations of vector addition and normalization to produce a single vector. Overlapping associative items produce composite 92 vectors which represent both the range of items stored and the central tendency of the those items. This form of prototype generation is a basic property of distributed memories. The retrieval function, Ft , is simulated by the of the state, encodes the association &lt;A&gt;*&lt;B&gt;, then presenting say &lt;A&gt; as an input, or retrieval key, produces new state, which encodes the item &lt;B&gt;&apos;, a noisy version of &lt;B&gt;, under the operation of correlation. This operation is defined by the has the following properties: An item correlated with itself, autocorrelation, produces an approximation to a delta vector. If two similar memory vectors are correlated, the central feature of the resulting vector will be equal to their similarity, or dot product, producing an attenuated delta vector. If the two items are completely independent, correlation produces a zero vector. The relation between convolution and correlation is given by &lt;A&gt;C&lt;A&gt;*&lt;B&gt;) = (&lt;A&gt;W&lt;A&gt;)*&lt;B&gt; + (&lt;A)V&lt;B&gt;)*&lt;A&gt; + noise ...(1) where the noise component results from some of the less significant cross products. Assuming that &lt;A&gt; and &lt;B&gt; are unrelated, Equation (1) becomes: =S*&lt;B&gt; + noise = &lt;B&gt; + 0 + noise Extending these results to a composite trace, suppose that q encodes two associated pairs of four unrelated items forming the vector (&lt;A&gt;*&lt;B&gt; + &lt;C&gt;*&lt;D&gt;). When &lt;A&gt; is given as the retrieval cue, the reconstruction can be characterized as follows: &lt;A&gt;K&lt;A&gt;*&lt;B&gt; + &lt;C&gt;*&lt;D&gt;) (&lt;A&gt;#&lt;A&gt;)*&lt;B&gt; + (&lt;A)W&lt;B&gt;)*&lt;A&gt; + noise + (&lt;A)#&lt;C&gt;)*&lt;D&gt; + ((A&gt;#&lt;D&gt;)*&lt;C&gt; + noise •S*&lt;B&gt;+0*&lt;A&gt;+noise+0*&lt;D&gt;+0*&lt;C&gt;+noise = &lt;B&gt; + noise + noise When the additional unrelated items are added to the memory trace their affect on retrieval is to add noise to the reconstructed item &lt;B&gt;, which was associated with the retrieval cue. In a situation in which the encoded items are related to each other, the composite trace causes all of the related items to contribute to the reconstructed pattern, in addition to producing noise. The amount of noise added to a retrieved item is a function of both the amount of information held on the composite memory vector and the size of the vector. NATURAL LANGUAGE PARSERS Parsing The computational properties of distributed memory machines (OHM) make them natural mechanisms for case-frame parsing. Consider a DMM which encodes case-frame structures of the following form: &lt;Pred&gt;*(&lt;C1&gt;*&lt;F1&gt; + &lt;C2&gt;*&lt;P2&gt; + ...+ &lt;Cn&gt;*&lt;Pn&gt;) where &lt;Pred&gt; is the vector representing the predicate associated with the verb of an input clause; &lt;C1&gt; to &lt;Cn&gt; are the case vectors such as &lt;agent&gt;, &lt;instrument&gt;, etc., and &lt;P1&gt; to &lt;Pn&gt; are vectors representing prototype concepts which can fill the associated cases. These structures can be made more complex by including tagging vectors which indicate such features as case, shown in the case-frame vector for the predicate BREAK: (&lt;agent&gt;*&lt;aniobj+natforce&gt; + &lt;object&gt;*&lt;physobj&gt; *&lt;oblig&gt; + &lt;instrument&gt;*&lt;physobj&gt;) In this example, the object case has a prototype covering the category of physical objects, and is tagged as obligatory. initial state of the OHM, the concatenation of the set of case-frame vectors stored by the parser. The system receives two types of inputs, noun concept vectors representing noun phrases, and predicate vectors representing the verb components. If the system in state only a predicate vector input produces a significant new state representing the case-frame structure associated with it. Once in this state, noun vector inputs identify the case slots they can potentially fill as illustrated in the following example: parsing the sentence broke the window a stone, input vector encoding retrieve the case-frame structure for given above. The input of &lt;Fred&gt; now gives &lt;Fred&gt;41:(&lt;agent&gt;*&lt;Pa&gt;+&lt;obj&gt;*&lt;Po&gt;+&lt;instr&gt;*&lt;P0) = &lt;FredWagent&gt;*&lt;Pa&gt;+&lt;Fred&gt;O&lt;Pa&gt;*&lt;agent&gt; + = + + ■ %Cogent&gt; + eiobj&gt; + giinstr&gt; wheree-is a measure of the similarity between the vectors, and underlying concepts, &lt;Fred&gt; and the case prototype &lt;Pj&gt;. In this example, &lt;Fred&gt; would be identified as the agent because and would be low relative to The vector is &amp;quot;cleaned-up&amp;quot; by a threshold function which is a component of the output function,A. This process is repeated for the other noun concepts in the sentence, linking &lt;window&gt; and &lt;stone&gt; with the object and instrument cases, respectively. However, the parser requires additional machinery to handle the large set of sentences in which the case assignment is ambiguous using semantic knowledge alone. Syntactic Knowledge Unambiguous case assignment can only be achieved through the integration of syntactic and semantic processing. Moreover, an adequate parser should generate an encoding of the grammatical relations between sentential elements in addition to a semantic representation. The rest of the paper demonstrates how the properties of DMMa can be combined with the ideas embodied in theory of Grammar [8] in a parser which builds both types of relational structure. 93 In LFG the mapping between grammatical and semantic relations is represented directly in the semantic form of the lexical entries for verbs. For example, the lexical entry for the given by hands: V, eparticiple) = NONE etense) = PRESENT esubj num) = SG bred) = HAND(esubj)eobj2)eobj)1 where the arguments of the predicate HAND are ordered such that they map directly onto the arguments of the semantic predicate-argument structure. The order and value of the arguments in a. lexical entry are transformed by lexical rules, such as the passive, to produce new lexical entries, e.g., HAND[ebyobj)esubj)etoobj)]. The direct mapping between lexical predicates and case-frame structures is encoded on the case-frame DMM by augmenting the vectors as follows: Hands:- &lt;HAND&gt;*(&lt;agent&gt;*&lt;Pa&gt;*&lt;subj&gt; + &lt;object&gt;*&lt;Po&gt;*&lt;obj2)+&lt;goal&gt;*&lt;Pg&gt;*&lt;obJ&gt;) When the SUBJ component has been identified through syntactic processing the resulting association vector, for example &lt;subj&gt;*&lt;John&gt; the sentence handed Mary the book, retrieve &lt;agent&gt; on input to the CF-DMM, according to the principles specified above. The multiple lexical entries produced by lexical rules have corresponding multiple case-frame vectors which are tagged by the appropriate grammatical vector. The CF-DMM encodes multiple case-frame entries for verbs, and the grammatical vector tags, such as &lt;PASSIVE&gt;, generated by the syntactic component, are input to the CF-DMM to retrieve the appropriate case-frame for the verb. The grammatical relations between the sentential elements are represented in the form structure as in LFG. These structures correspond to embedded lista of attribute-value pairs, and because of which governs their format they are efficiently encoded as memory vectors. As an example, the grammatical for the sentence handed Mary a encoded in the f-structure below:</abstract>
<title confidence="0.881877714285714">MEM&amp;quot; SUBJ [NUM SG TENSE PAST A NUM SG PRED &apos;BOOK&apos; The lists of grammatical functions and features</title>
<abstract confidence="0.982274664233577">are encoded as single vectors under the + operator, and the embedded structure is preserved by the associative operator, *. The f-structure is encoded by the vector (&lt;SUBJ&gt;*(&lt;NUM&gt;*&lt;SG&gt;+&lt;PRED&gt;*&lt;JOHN&gt;) + &lt;TENSE&gt; *&lt;PAST&gt; + &lt;PRED&gt;*(&lt;HAND&gt;*(&lt;tSUBJ&gt;*&lt;TOBJ2&gt;* ‹tOBJ&gt;)) + &lt;OBJ&gt;*(&lt;NUM&gt;*&lt;SG&gt;*&lt;PRED&gt;*&lt;MARY&gt;)+ &lt;OBJ2&gt;*(&lt;SPEC&gt;*&lt;A&gt;+&lt;NUM&gt;*&lt;SG)+&lt;PRED&gt;*&lt;BOOK&gt;)) This compatibility between f-structures and memory vectors is the basis for an efficient procedure for deriving f-structures from input strings. In LFG f-structures are generated in three steps. First, a context-free grammar (CPG) is used to derive an input string&apos;s constituent structure (C-structure). The grammar is augmented so that it generates a phrase structure tree which includes statements about the properties of the string&apos;s f-structure. In the next step, this structure is condensed to derive a series of equations, called the functional description of the string. Finally, the f-structure is derived from the f-description. The properties of DMMs enable a simple procedure to be written which derives f-structures from augmented phrase structure trees, obviating the need for an f-description. Consider the tree in figure 1 generated for our example sentence: esusJ)=4 t=4 John handed Mary a book Figure 1. Augmented Phrase Structure Tree OPRED)=HAND[..] The f-structure, encoded as a memory vector, can be derived from this tree by the following procedure. First, all the grammatical functions, features and semantic forms must be as vectors. The41.-variables, have no values at this point; they are derived by the procedure. All the vectors dominated by a node are concatenated to produce a single vector at that node. The symbol &apos;=&apos; is interpreted as the association operator ,*. Applying this interpretation to the tree from the bottom up produces a memory vector for the value of fl which encodes the f-structure for the string, as given above. Accordingly, f2 the value the rule specified at the node, gives &lt;,SUBJ&gt;*(&lt;tNUM&gt;*&lt;SG&gt;+&lt;tPRED&gt;*&lt;JOHN&gt;) as a of The other components of fo are derived in the same way. The front-end CFG can be veiwed as generating the control structure for the derivation of a memory vector which represents the input string&apos;s f-structure. PRED &apos;HAND[( SUBJ)( OBJ2)( OBJ)]&apos; OBJ [ PRED &apos;MARY] eNUM)-SG OPRED)=JOHN) eSPEC)=A ONUM)=SG ONUM)-SG OPRED)=BOOK DET TENSE )-PAST OBJ)-4. (INUM)=SG ePRED)=MARY eOBJ2)=4. 94 The properties of memory vectors also enable the procedure to automatically determine the consistency pf the structure. For example, in the value of concatenation merges the (itNUM)=SG features for to a single component of the (&lt;SPEC&gt;*&lt;A&gt;+&lt;NUM&gt;*&lt;SG)+&lt;PRED&gt;*&lt;MARY&gt;). Rowever, if the two features had not matched, producing the vector component &lt;NUM&gt;*(&lt;SG&gt;+&lt;PL&gt;) for example, the vectors encoding the incompatible feature values are set such that their concatenation produces a special control vector which signals the mismatch. A Architecture The ideas outlined above are combined in the design of a tentative parsing architecture shown in figure 2. The diamonds denote DMMs, and the Figure 2. Parsing Architecture denotes a form of DMM as a working memory for encoding temporary f-structures. As elements of the input string enter the lexicon their associated entries are retrieved. The syntactic category of the element is passed onto the CFG, and the lexical schemata {e.g., (tPRED)=&apos;JOHN&apos;}, encoded as memory vectors, are passed to the f-structure working memory. The lexical entry associated with the verb is passed to the case-frame memory to retrieve the appropriate set of structures. The partial results of the CFG control the formation of memory vectors in the f-structure memory, as indicated by the broad arrow. The CFG also generates grammatical vectors as inputs for case-frame memory to select the appropriate structure from the multiple encodings associated with each verb. The partial f-structure encoding can then be used as input to the case-frame memory to assign the semantic forms of grammatical functions to case slots. When the end of the string is reached both the case-frame instantiation and the f-structure should be complete. IV CONCLUSIONS This paper attempts to demonstrate the value of distributed memory machines as components of a parsing system which generates both semantic and grammatical relational structures. The ideas presented are similar to those being developed within the connectionist paradigm [1]. Small, and his colleagues [9], have proposed a parsing model based directly on connectionist principles. The computational architecture consists of a large number of appropriately connected computing units communicating through weighted levels of excitation and inhibition. The ideas presented here differ from those embodied in the connectionist parser in that they emphasise distributed information storage and retrieval, rather than distributed parallel processing. Retrieval and filtering are achieved through simple computable functions operating on k-element arrays, in contrast to the complex interactions of the independent units in connectionist models. In figure 2, although the network of machines requires control, the architecture can considered to be at the lower end of the family of parallel processing machines [10]. V REFERENCES [1] Feldman, J.A. and Ballard, D.H. Connectionmodels and their properties.</abstract>
<note confidence="0.948478575">Science,1982, 6, 205-254. [2] Hinton, G.E. and Anderson, J.A. (Eds) Modelsof AssociativeMemory. Hillsdale, NJ: Lawrence Erlbaum Associates, 1981. Hinton, G.E. representation in parallel In Proceedingsof the InternationalJoint Conferenceon Intelligence,Vol. 2, Vancouver BC, Canada, August, 1981. [4] Longuet-Higgins, B.C., Willshaw, D.J., and Bunemann, 0.P. Theories of associative recall. Reviewsof Biophysics,1970, 3, 223-244. [5] Murdock, B.B. A theory for the storage and retrieval of item and associative information. Review,1982, 89, 609-627. Kohonen, T. memory_-_ A systemapproach.Berlin: Springer- Verlag, 1977. [7] Borsellino, A., and Poggio, T. Convolution Correlation algebras. Kybernetik, 1973, 13, 113-122. [8] Kaplan, R., and Bresnan, J. Lexical-Functional Grammar: A formal system for grammatical representation. In J. Bresnan (ed.), The Representationof Relations. Cambridge,&apos;Mass.:MIT Press, 1982. [9] Small, S.L., Cottrell, G.W., and Shastri, L. connectionist parsing. In the Conferenceon Intelligence,Pittsburgh, Pennsylvania, 1982. [10] Fahlman, S.E., Hinton, G.E., and Sejnowski, T. Massively parallel architectures for Al: NETL, and BOLTZMANN machines. In Proceedingsof the Conferenceon Intelligence,Washington D.C., 1983. Context-Free Grammar 95</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J A Feldman</author>
<author>D H Ballard</author>
</authors>
<title>Connection1st models and their properties.</title>
<date>1982</date>
<journal>Cognitive Science,</journal>
<volume>6</volume>
<pages>205--254</pages>
<contexts>
<context position="948" citStr="[1,2]" startWordPosition="132" endWordPosition="132">ns on a single composite vector. They provide a natural format for encoding the syntactic and semantic constraints associated with linguistic elements. A computational architecture for parsing natural language is proposed which utilises the retrieval and associative features of these devices. The parsing mechanism is based on the principles of Lexical Functional Grammar and the paper demonstrates how these principles can be derived from the properties of the memory mechanisms. I INTRODUCTION Recently, interest has focussed on computational architectures employing massively parallel processing [1,2]. Some of these systems have used a distributed form of knowledge representation [3]. This type of representation encodes an item of knowledge in terms of the relationships among a collection of elementary processing units, and such assemblages can encode large numbers of items. Representational similarity and the ability to generalize are the principal features of such memory systems. The next section defines a distributed memory machine which incorporates some of the computational advantages of distributed representations within a traditional von Neumann architecture. The rest of the paper e</context>
<context position="15914" citStr="[1]" startWordPosition="2407" endWordPosition="2407">the multiple encodings associated with each verb. The partial f-structure encoding can then be used as input to the case-frame memory to assign the semantic forms of grammatical functions to case slots. When the end of the string is reached both the case-frame instantiation and the f-structure should be complete. IV CONCLUSIONS This paper attempts to demonstrate the value of distributed memory machines as components of a parsing system which generates both semantic and grammatical relational structures. The ideas presented are similar to those being developed within the connectionist paradigm [1]. Small, and his colleagues [9], have proposed a parsing model based directly on connectionist principles. The computational architecture consists of a large number of appropriately connected computing units communicating through weighted levels of excitation and inhibition. The ideas presented here differ from those embodied in the connectionist parser in that they emphasise distributed information storage and retrieval, rather than distributed parallel processing. Retrieval and filtering are achieved through simple computable functions operating on k-element arrays, in contrast to the comple</context>
</contexts>
<marker>[1]</marker>
<rawString>Feldman, J.A. and Ballard, D.H. Connection1st models and their properties. Cognitive Science, 1982, 6, 205-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>J A Anderson</author>
</authors>
<title>Parallel Models of Associative Memory. Hillsdale, NJ: Lawrence Erlbaum Associates,</title>
<date>1981</date>
<contexts>
<context position="948" citStr="[1,2]" startWordPosition="132" endWordPosition="132">ns on a single composite vector. They provide a natural format for encoding the syntactic and semantic constraints associated with linguistic elements. A computational architecture for parsing natural language is proposed which utilises the retrieval and associative features of these devices. The parsing mechanism is based on the principles of Lexical Functional Grammar and the paper demonstrates how these principles can be derived from the properties of the memory mechanisms. I INTRODUCTION Recently, interest has focussed on computational architectures employing massively parallel processing [1,2]. Some of these systems have used a distributed form of knowledge representation [3]. This type of representation encodes an item of knowledge in terms of the relationships among a collection of elementary processing units, and such assemblages can encode large numbers of items. Representational similarity and the ability to generalize are the principal features of such memory systems. The next section defines a distributed memory machine which incorporates some of the computational advantages of distributed representations within a traditional von Neumann architecture. The rest of the paper e</context>
</contexts>
<marker>[2]</marker>
<rawString>Hinton, G.E. and Anderson, J.A. (Eds) Parallel Models of Associative Memory. Hillsdale, NJ: Lawrence Erlbaum Associates, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
</authors>
<title>Shape representation in parallel systems.</title>
<date>1981</date>
<booktitle>In Proceedings of the Seventh International Joint Conference on Artificial Intelligence,</booktitle>
<volume>2</volume>
<location>Vancouver BC, Canada,</location>
<contexts>
<context position="1032" citStr="[3]" startWordPosition="145" endWordPosition="145">tic and semantic constraints associated with linguistic elements. A computational architecture for parsing natural language is proposed which utilises the retrieval and associative features of these devices. The parsing mechanism is based on the principles of Lexical Functional Grammar and the paper demonstrates how these principles can be derived from the properties of the memory mechanisms. I INTRODUCTION Recently, interest has focussed on computational architectures employing massively parallel processing [1,2]. Some of these systems have used a distributed form of knowledge representation [3]. This type of representation encodes an item of knowledge in terms of the relationships among a collection of elementary processing units, and such assemblages can encode large numbers of items. Representational similarity and the ability to generalize are the principal features of such memory systems. The next section defines a distributed memory machine which incorporates some of the computational advantages of distributed representations within a traditional von Neumann architecture. The rest of the paper explores the properties of such machines as the basis for natural language parsing. I</context>
</contexts>
<marker>[3]</marker>
<rawString>Hinton, G.E. Shape representation in parallel systems. In Proceedings of the Seventh International Joint Conference on Artificial Intelligence, Vol. 2, Vancouver BC, Canada, August, 1981.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B C Longuet-Higgins</author>
<author>D J Willshaw</author>
<author>0 P Bunemann</author>
</authors>
<title>Theories of associative recall.</title>
<journal>Quarterly Reviews of Biophysics,</journal>
<volume>1970</volume>
<pages>223--244</pages>
<contexts>
<context position="2587" citStr="[4,5,6]" startWordPosition="390" endWordPosition="390">Q, the retrieval function; A:Q--&gt;QxY, the output function. Further, where Y&apos; denotes the set of all finite concatenations of the elements of the set Y, QcY&apos;, and therefore QcV&apos;. This statement represents the notion that internal states of DMMs can encode multiple outputs or hypotheses. The vocabulary, V, can be represented by the space Ik, where I is some interval range defined within a chosen number system, N; ImN. The elements of X, Y and Q are encoded as k-element vectors, referred to as memory vectors. A. Holographic Associative Memory One form of DMM is the holographic associative memory [4,5,6] which encodes large numbers of associations on a single composite vector. Items of information are encoded as k-element zero-centred vectors over an interval such as [-1,+1]; &lt;X&gt;=(...x1,x0,x.0...). Two items, &lt;A&gt; and &lt;B&gt; (angular brackets denote memory vectors), are associated in memory through the operation of convolution. This method of association formation is fundamental to the concept of holographic memory and the resulting associative trace is denoted &lt;A&gt;*&lt;B&gt;. The operation of convolution is define by the equation has the following properties&apos; [7]: Commutative: &lt;A&gt;*&lt;B&gt; = &lt;B&gt;*&lt;A&gt;, Associ</context>
</contexts>
<marker>[4]</marker>
<rawString>Longuet-Higgins, B.C., Willshaw, D.J., and Bunemann, 0.P. Theories of associative recall. Quarterly Reviews of Biophysics, 1970, 3, 223-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B B Murdock</author>
</authors>
<title>A theory for the storage and retrieval of item and associative information. Psychological Review,</title>
<date>1982</date>
<volume>89</volume>
<pages>609--627</pages>
<contexts>
<context position="2587" citStr="[4,5,6]" startWordPosition="390" endWordPosition="390">Q, the retrieval function; A:Q--&gt;QxY, the output function. Further, where Y&apos; denotes the set of all finite concatenations of the elements of the set Y, QcY&apos;, and therefore QcV&apos;. This statement represents the notion that internal states of DMMs can encode multiple outputs or hypotheses. The vocabulary, V, can be represented by the space Ik, where I is some interval range defined within a chosen number system, N; ImN. The elements of X, Y and Q are encoded as k-element vectors, referred to as memory vectors. A. Holographic Associative Memory One form of DMM is the holographic associative memory [4,5,6] which encodes large numbers of associations on a single composite vector. Items of information are encoded as k-element zero-centred vectors over an interval such as [-1,+1]; &lt;X&gt;=(...x1,x0,x.0...). Two items, &lt;A&gt; and &lt;B&gt; (angular brackets denote memory vectors), are associated in memory through the operation of convolution. This method of association formation is fundamental to the concept of holographic memory and the resulting associative trace is denoted &lt;A&gt;*&lt;B&gt;. The operation of convolution is define by the equation has the following properties&apos; [7]: Commutative: &lt;A&gt;*&lt;B&gt; = &lt;B&gt;*&lt;A&gt;, Associ</context>
</contexts>
<marker>[5]</marker>
<rawString>Murdock, B.B. A theory for the storage and retrieval of item and associative information. Psychological Review, 1982, 89, 609-627.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
</authors>
<title>Associative memory _-_ A systemtheoretical approach.</title>
<date>1977</date>
<publisher>SpringerVerlag,</publisher>
<location>Berlin:</location>
<contexts>
<context position="2587" citStr="[4,5,6]" startWordPosition="390" endWordPosition="390">Q, the retrieval function; A:Q--&gt;QxY, the output function. Further, where Y&apos; denotes the set of all finite concatenations of the elements of the set Y, QcY&apos;, and therefore QcV&apos;. This statement represents the notion that internal states of DMMs can encode multiple outputs or hypotheses. The vocabulary, V, can be represented by the space Ik, where I is some interval range defined within a chosen number system, N; ImN. The elements of X, Y and Q are encoded as k-element vectors, referred to as memory vectors. A. Holographic Associative Memory One form of DMM is the holographic associative memory [4,5,6] which encodes large numbers of associations on a single composite vector. Items of information are encoded as k-element zero-centred vectors over an interval such as [-1,+1]; &lt;X&gt;=(...x1,x0,x.0...). Two items, &lt;A&gt; and &lt;B&gt; (angular brackets denote memory vectors), are associated in memory through the operation of convolution. This method of association formation is fundamental to the concept of holographic memory and the resulting associative trace is denoted &lt;A&gt;*&lt;B&gt;. The operation of convolution is define by the equation has the following properties&apos; [7]: Commutative: &lt;A&gt;*&lt;B&gt; = &lt;B&gt;*&lt;A&gt;, Associ</context>
</contexts>
<marker>[6]</marker>
<rawString>Kohonen, T. Associative memory _-_ A systemtheoretical approach. Berlin: SpringerVerlag, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borsellino</author>
<author>T Poggio</author>
</authors>
<title>Convolution and Correlation algebras.</title>
<date>1973</date>
<volume>13</volume>
<pages>113--122</pages>
<location>Kybernetik,</location>
<contexts>
<context position="3147" citStr="[7]" startWordPosition="471" endWordPosition="471"> the holographic associative memory [4,5,6] which encodes large numbers of associations on a single composite vector. Items of information are encoded as k-element zero-centred vectors over an interval such as [-1,+1]; &lt;X&gt;=(...x1,x0,x.0...). Two items, &lt;A&gt; and &lt;B&gt; (angular brackets denote memory vectors), are associated in memory through the operation of convolution. This method of association formation is fundamental to the concept of holographic memory and the resulting associative trace is denoted &lt;A&gt;*&lt;B&gt;. The operation of convolution is define by the equation has the following properties&apos; [7]: Commutative: &lt;A&gt;*&lt;B&gt; = &lt;B&gt;*&lt;A&gt;, Associative: &lt;A&gt;*(&lt;B&gt;*&lt;C&gt;) Further, where a delta vector, denoted &amp;, is defined as a vector that has values of zero on all features except the central feature, which has a value of one, then &lt;A&gt;*G= &lt;A&gt;. Moreover, &lt;A&gt;*0 = 0, where 0 is a zero vector in which all feature values are zero. Convolving an item with an attenuated delta vector (i.e., a vector with values of zero on all features except the central one, which has a value between 0 and 1) produces the original item with a strength that is equal to the value of the central feature of the attenuated delta </context>
</contexts>
<marker>[7]</marker>
<rawString>Borsellino, A., and Poggio, T. Convolution and Correlation algebras. Kybernetik, 1973, 13, 113-122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>Lexical-Functional Grammar: A formal system for grammatical representation.</title>
<date>1982</date>
<editor>In J. Bresnan (ed.),</editor>
<publisher>Cambridge,&apos;Mass.:MIT Press,</publisher>
<contexts>
<context position="9156" citStr="[8]" startWordPosition="1403" endWordPosition="1403"> parser requires additional machinery to handle the large set of sentences in which the case assignment is ambiguous using semantic knowledge alone. B. Encoding Syntactic Knowledge Unambiguous case assignment can only be achieved through the integration of syntactic and semantic processing. Moreover, an adequate parser should generate an encoding of the grammatical relations between sentential elements in addition to a semantic representation. The rest of the paper demonstrates how the properties of DMMa can be combined with the ideas embodied in the theory of Lexical-functional Grammar (LFG) [8] in a parser which builds both types of relational structure. 93 In LFG the mapping between grammatical and semantic relations is represented directly in the semantic form of the lexical entries for verbs. For example, the lexical entry for the verb hands is given by hands: V, eparticiple) = NONE etense) = PRESENT esubj num) = SG bred) = HAND(esubj)eobj2)eobj)1 where the arguments of the predicate HAND are ordered such that they map directly onto the arguments of the semantic predicate-argument structure. The order and value of the arguments in a. lexical entry are transformed by lexical rules</context>
</contexts>
<marker>[8]</marker>
<rawString>Kaplan, R., and Bresnan, J. Lexical-Functional Grammar: A formal system for grammatical representation. In J. Bresnan (ed.), The Mental Representation of Grammatical Relations. Cambridge,&apos;Mass.:MIT Press, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Small</author>
<author>G W Cottrell</author>
<author>L Shastri</author>
</authors>
<title>Toward connectionist parsing.</title>
<date>1982</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<location>Pittsburgh, Pennsylvania,</location>
<contexts>
<context position="15945" citStr="[9]" startWordPosition="2412" endWordPosition="2412">ed with each verb. The partial f-structure encoding can then be used as input to the case-frame memory to assign the semantic forms of grammatical functions to case slots. When the end of the string is reached both the case-frame instantiation and the f-structure should be complete. IV CONCLUSIONS This paper attempts to demonstrate the value of distributed memory machines as components of a parsing system which generates both semantic and grammatical relational structures. The ideas presented are similar to those being developed within the connectionist paradigm [1]. Small, and his colleagues [9], have proposed a parsing model based directly on connectionist principles. The computational architecture consists of a large number of appropriately connected computing units communicating through weighted levels of excitation and inhibition. The ideas presented here differ from those embodied in the connectionist parser in that they emphasise distributed information storage and retrieval, rather than distributed parallel processing. Retrieval and filtering are achieved through simple computable functions operating on k-element arrays, in contrast to the complex interactions of the independe</context>
</contexts>
<marker>[9]</marker>
<rawString>Small, S.L., Cottrell, G.W., and Shastri, L. Toward connectionist parsing. In Proceedings of the National Conference on Artificial Intelligence, Pittsburgh, Pennsylvania, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Fahlman</author>
<author>G E Hinton</author>
<author>T Sejnowski</author>
</authors>
<title>Massively parallel architectures for Al: NETL, THISTLE, and BOLTZMANN machines.</title>
<date>1983</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<location>Washington D.C.,</location>
<marker>[10]</marker>
<rawString>Fahlman, S.E., Hinton, G.E., and Sejnowski, T. Massively parallel architectures for Al: NETL, THISTLE, and BOLTZMANN machines. In Proceedings of the National Conference on Artificial Intelligence, Washington D.C., 1983.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>