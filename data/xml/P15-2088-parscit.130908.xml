<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000074">
<title confidence="0.9978455">
Context-Dependent Translation Selection Using Convolutional Neural
Network
</title>
<author confidence="0.998746">
Baotian Hu$ Zhaopeng Tu†* Zhengdong Lu† Hang Li† Qingcai Chen$
</author>
<affiliation confidence="0.798218">
$Intelligent Computing Research †Noah’s Ark Lab
Center, Harbin Institute of Technology Huawei Technologies Co. Ltd.
Shenzhen Graduate School tu.zhaopeng@huawei.com
</affiliation>
<email confidence="0.9712065">
baotianchina@gmail.com lu.zhengdong@huawei.com
qingcai.chen@hitsz.edu.cn hangli.hl@huawei.com
</email>
<sectionHeader confidence="0.997286" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993194136363637">
We propose a novel method for translation
selection in statistical machine translation,
in which a convolutional neural network is
employed to judge the similarity between
a phrase pair in two languages. The specif-
ically designed convolutional architecture
encodes not only the semantic similarity
of the translation pair, but also the con-
text containing the phrase in the source
language. Therefore, our approach is
able to capture context-dependent seman-
tic similarities of translation pairs. We
adopt a curriculum learning strategy to
train the model: we classify the training
examples into easy, medium, and difficult
categories, and gradually build the abil-
ity of representing phrases and sentence-
level contexts by using training examples
from easy to difficult. Experimental re-
sults show that our approach significantly
outperforms the baseline system by up to
1.4 BLEU points.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999812923076923">
Conventional statistical machine translation
(SMT) systems extract and estimate translation
pairs based on their surface forms (Koehn et al.,
2003), which often fail to capture translation
pairs which are grammatically and semantically
similar. To alleviate the above problems, several
researchers have proposed learning and utilizing
semantically similar translation pairs in a contin-
uous space (Gao et al., 2014; Zhang et al., 2014;
Cho et al., 2014). The core idea is that the two
phrases in a translation pair should share the same
semantic meaning and have similar (close) feature
vectors in the continuous space.
</bodyText>
<subsectionHeader confidence="0.523225">
∗* Corresponding author
</subsectionHeader>
<bodyText confidence="0.990870942857143">
The above methods, however, neglect the infor-
mation of local contexts, which has been proven
to be useful for disambiguating translation candi-
dates during decoding (He et al., 2008; Marton and
Resnik, 2008). The matching scores of translation
pairs are treated the same, even they are in dif-
ferent contexts. Accordingly, the methods fail to
adapt to local contexts and lead to precision issues
for specific sentences in different contexts.
To capture useful context information, we pro-
pose a convolutional neural network architecture
to measure context-dependent semantic similari-
ties between phrase pairs in two languages. For
each phrase pair, we use the sentence contain-
ing the phrase in source language as the context.
With the convolutional neural network, we sum-
marize the information of a phrase pair and its con-
text, and further compute the pair’s matching score
with a multi-layer perceptron. We discriminately
train the model using a curriculum learning strat-
egy. We classify the training examples according
to the difficulty level of distinguishing the positive
candidate from the negative candidate. Then we
train the model to learn the semantic information
from easy (basic semantic similarities) to difficult
(context-dependent semantic similarities).
Experimental results on a large-scale transla-
tion task show that the context-dependent convo-
lutional matching (CDCM) model improves the
performance by up to 1.4 BLEU points over a
strong phrase-based SMT system. Moreover,
the CDCM model significantly outperforms its
context-independent counterpart, proving that it is
necessary to incorporate local contexts into SMT.
Contributions. Our key contributions include:
</bodyText>
<listItem confidence="0.969673833333333">
• we introduce a novel CDCM model to cap-
ture context-dependent semantic similarities
between phrase pairs (Section 2);
• we develop a novel learning algorithm to
train the CDCM model using a curriculum
learning strategy (Section 3).
</listItem>
<page confidence="0.859755">
536
</page>
<note confidence="0.366032333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 536–541,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.974462666666667">
Figure 1: Architecture of the CDCM model. The convolutional sentence model (bottom) summarizes the
meaning of the tagged sentence and target phrase, and the matching model (top) compares the represen-
tations using a multi-layer perceptron. “P” indicates all-zero padding turned off by the gating function.
</figureCaption>
<figure confidence="0.985943733333333">
É
É
/ / At I lfx A 44- &amp;if
/ / the key point is
pooling
convolution
Layer-2
Layer-1
: tagged words : untagged words
Matching Score
matching model
convolutional
sentence model
representation
representation
</figure>
<sectionHeader confidence="0.952548" genericHeader="method">
2 Context-Dependent Convolutional
</sectionHeader>
<subsectionHeader confidence="0.759709">
Matching Model
</subsectionHeader>
<bodyText confidence="0.943074">
The model architecture, shown in Figure 1, is a
variant of the convolutional architecture of Hu et
al. (2014). It consists of two components:
</bodyText>
<listItem confidence="0.990616833333334">
• convolutional sentence model that summa-
rizes the meaning of the source sentence and
the target phrase;
• matching model that compares the two
representations with a multi-layer percep-
tron (Bengio, 2009).
</listItem>
<bodyText confidence="0.980724416666667">
Let eˆ be a target phrase and f be the source sen-
tence that contains the source phrase aligning to ˆe.
We first project f and eˆ into feature vectors x and
y via the convolutional sentence model, and then
compute the matching score s(x, y) by the match-
ing model. Finally, the score is introduced into a
conventional SMT system as an additional feature.
Convolutional sentence model. As shown in Fig-
ure 1, the model takes as input the embeddings of
words (trained beforehand elsewhere) in f and ˆe.
It then iteratively summarizes the meaning of the
input through layers of convolution and pooling,
until reaching a fixed length vectorial representa-
tion in the final layer.
In Layer-1, the convolution layer takes sliding
windows on f and eˆ respectively, and models all
the possible compositions of neighbouring words.
The convolution involves a filter to produce a new
feature for each possible composition. Given a
k-sized sliding window i on f or ˆe, for example,
the jth convolution unit of the composition of the
words is generated by:
ci(1,j) = g(ˆci(0)) · 0(w(1,j) · ˆci(0) + b(1,j)) (1)
where
</bodyText>
<listItem confidence="0.978379">
• g(·) is the gate function that determines
whether to activate 0(·);
• 0(·) is a non-linear activation function. In
this work, we use ReLu (Dahl et al., 2013)
as the activation function;
• w(1,j) is the parameters for the jth convolu-
tion unit on Layer-1, with matrix W(1) =
[w(1,1),... , w(1,J)];
• ˆci(0) is a vector constructed by concatenating
word vectors in the k-sized sliding widow i;
• b(1,j) is a bias term, with vector B(1) =
[b(1,1),..., b(1,J)].
</listItem>
<bodyText confidence="0.9993295">
To distinguish the phrase pair from its con-
text, we use one additional dimension in word
embeddings: 1 for words in the phrase pair and
0 for the others. After transforming words to
</bodyText>
<page confidence="0.981681">
537
</page>
<bodyText confidence="0.999359666666667">
their tagged embeddings, the convolutional sen-
tence model takes multiple choices of composition
using sliding windows in the convolution layer.
Note that sliding windows are allowed to cross
the boundary of the source phrase to exploit both
phrasal and contextual information.
In Layer-2, we apply a local max-pooling in
non-overlapping 1 x 2 windows for every convo-
lution unit
</bodyText>
<equation confidence="0.8821978">
(2,j) = maxffc(1,j) c(1,j) (2)
ci l 2z,2i+1
In Layer-3, we perform convolution on output
from Layer-2:
ci(3,j) = g(ˆci(2)) · O(W(3,j) · ˆci(2) + b(3,j)) (3)
</equation>
<bodyText confidence="0.999694818181818">
After more convolution and max-pooling opera-
tions, we obtain two feature vectors for the source
sentence and the target phrase, respectively.
Matching model. The matching score of a source
sentence and a target phrase can be measured
as the similarity between their feature vectors.
Specifically, we use the multi-layer perceptron
(MLP), a nonlinear function for similarity, to com-
pute their matching score. First we use one layer
to combine their feature vectors to get a hidden
state hc:
</bodyText>
<equation confidence="0.846323">
hc = O(wc · [x¯fi : y¯ej] + bc) (4)
Then we get the matching score from the MLP:
s(x, y) = MLP(hc) (5)
</equation>
<sectionHeader confidence="0.989834" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.999873166666667">
We employ a discriminative training strategy with
a max-margin objective. Suppose we are given
the following triples (x, y+, y−) from the ora-
cle, where x, y+, y− are the feature vectors for
f, ˆe+, ˆe− respectively. We have the ranking-based
loss as objective:
</bodyText>
<equation confidence="0.882618">
Le(x, y+, y−) = max(0,1+s(x, y−)−s(x, y+))
</equation>
<bodyText confidence="0.978458">
(6)
where s(x, y) is the matching score function de-
fined in Eq. 5, Θ consists of parameters for both
the convolutional sentence model and MLP. The
model is trained by minimizing the above ob-
jective, to encourage the model to assign higher
matching scores to positive examples and to as-
sign lower scores to negative examples. We use
stochastic gradient descent (SGD) to optimize the
model parameters Θ. We train the CDCM model
with a curriculum strategy to learn the context-
dependent semantic similarity at the phrase level
from easy (basic semantic similarities between
the source and target phrase pair) to difficult
(context-dependent semantic similarities for the
same source phrase in varying contexts).
</bodyText>
<subsectionHeader confidence="0.998899">
3.1 Curriculum Training
</subsectionHeader>
<bodyText confidence="0.9850659375">
Curriculum learning, first proposed by Bengio et
al. (2009) in machine learning, refers to a se-
quence of training strategies that start small, learn
easier aspects of the task, and then gradually in-
crease the difficulty level. It has been shown
that the curriculum learning can benefit the non-
convex training by giving rise to improved gener-
alization and faster convergence. The key point is
that the training examples are not randomly pre-
sented but organized in a meaningful order which
illustrates gradually more concepts, and gradually
more complex ones.
For each positive example (f, ˆe+), we have three
types of negative examples according to the diffi-
culty level of distinguishing the positive example
from them:
</bodyText>
<listItem confidence="0.993524428571429">
• Easy: target phrases randomly chosen from
the phrase table;
• Medium: target phrases extracted from the
aligned target sentence for other non-overlap
source phrases in the source sentence;
• Difficult: target phrases extracted from other
candidates for the same source phrase.
</listItem>
<bodyText confidence="0.850026">
We want the CDCM model to learn the following
semantic information from easy to difficult:
</bodyText>
<listItem confidence="0.931248666666667">
• the basic semantic similarity between the
source sentence and target phrase from the
easy negative examples;
• the general semantic equivalent between
the source and target phrase pair from the
medium negative examples;
• the context-dependent semantic similarities
for the same source phrase in varying con-
texts from the difficult negative examples.
</listItem>
<bodyText confidence="0.999648">
Alg. 1 shows the curriculum training algorithm
for the CDCM model. We use different portions of
the overall training instances for different curricu-
lums (lines 2-11). For example, we only use the
</bodyText>
<page confidence="0.989796">
538
</page>
<bodyText confidence="0.9978278">
Algorithm 1 Curriculum training algorithm. Here
T denotes the training examples, W the initial
word embeddings, q the learning rate in SGD, n
the pre-defined number, and t the number of train-
ing examples.
</bodyText>
<listItem confidence="0.829657666666667">
1: procedure CURRICULUM-TRAINING(T, W)
2: N1 ← easy negative(T)
3: N2 ← medium negative(T)
4: N3 ← difficult negative(T)
5: T ← N1
6: CURRICULUM(T, n · t) &gt; CUR. easy
7: T ← MIX([N1, N2])
8: CURRICULUM(T, n · t) &gt; CUR. medium
9: for step ← 1 ... n do
</listItem>
<figure confidence="0.918661">
10: T ← MIX([N1, N2, N3], step)
11: CURRICULUM(T, t) &gt; CUR. difficult
12: procedure CURRICULUM(T, K)
13: iterate until reaching a local minima or K iterations
14: calculate LΘ for a random instance in T
15: O = O − rl · ∂LΘ &gt; update parameters
∂Θ
16: W = W − rl · 0.01 · ∂LΘ &gt; update embeddings
∂W
17: procedure MIX(N, s = 0)
18: len ← length of N
19: if len &lt; 3 then
20: T ← sampling with [0.5, 0.5] from N
21: else
22: T ← sampling with [ 1
s+2 , 1
s+2, s
s+2 ] from N
</figure>
<bodyText confidence="0.988682307692308">
training instances that consist of positive examples
and easy negative examples in the easy curriculum
(lines 5-6). For the latter curriculums, we gradu-
ally increase the difficulty level of the training in-
stances (lines 7-12).
For each curriculum (lines 12-16), we compute
the gradient of the loss objective LΘ and learn Θ
using the SGD algorithm. Note that we mean-
while update the word embeddings to better cap-
ture the semantic equivalence across languages
during training. If the loss function LΘ reaches
a local minima or the iterations reach the pre-
defined number, we terminate this curriculum.
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999972536585366">
Our research builds on previous work in the field
of context-dependent rule matching and bilingual
phrase representations.
There is a line of work that employs local con-
texts over discrete representations of words or
phrases. For example, He et al. (2008), Liu et
al. (2008) and Marton and Resnik (2008) em-
ployed within-sentence contexts that consist of
discrete words to guide rule matching. Wu et
al. (2014) exploited discrete contextual features in
the source sentence (e.g. words and part-of-speech
tags) to learn better bilingual word embeddings for
SMT. In this study, we take into account all the
phrase pairs and directly compute phrasal similari-
ties with convolutional representations of the local
contexts, integrating the strengths associated with
the convolutional neural networks (Collobert and
Weston, 2008).
In recent years, there has also been growing
interest in bilingual phrase representations that
group phrases with a similar meaning across dif-
ferent languages. Based on that translation equiv-
alents share the same semantic meaning, they can
supervise each other to learn their semantic phrase
embeddings in a continuous space (Gao et al.,
2014; Zhang et al., 2014). However, these mod-
els focused on capturing semantic similarities be-
tween phrase pairs in the global contexts, and ne-
glected the local contexts, thus ignored the use-
ful discriminative information. Alternatively, we
integrate the local contexts into our convolutional
matching architecture to obtain context-dependent
semantic similarities.
Meng et al. (2015) and Zhang (2015) have
proposed independently to summary source sen-
tences with convolutional neural networks. How-
ever, they both extend the neural network joint
model (NNJM) of Devlin et al. (2014) to include
the whole source sentence, while we focus on cap-
turing context-dependent semantic similarities of
translation pairs.
</bodyText>
<sectionHeader confidence="0.999567" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.960913">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.9999445">
We carry out our experiments on the NIST
Chinese-English translation tasks. Our training
data contains 1.5M sentence pairs coming from
LDC dataset.1 We train a 4-gram language model
on the Xinhua portion of the GIGAWORD corpus
using the SRI Language Toolkit (Stolcke, 2002)
with modified Kneser-Ney Smoothing (Kneser
and Ney, 1995). We use the 2002 NIST MT
evaluation test data as the development data, and
the 2004, 2005 NIST MT evaluation test data as
the test data. We use minimum error rate train-
ing (Och, 2003) to optimize the feature weights.
For evaluation, case-insensitive NIST BLEU (Pa-
pineni et al., 2002) is used to measure translation
performance. We perform a significance test using
the sign-test approach (Collins et al., 2005).
</bodyText>
<footnote confidence="0.992992">
1The corpus includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06.
</footnote>
<page confidence="0.989807">
539
</page>
<table confidence="0.999549">
Models MT04 MT05 All
Baseline 34.86 33.18 34.40
CICM 35.82α 33.51α 34.95α
CDCM1 35.87α 33.58 35.01α
CDCM2 35.97α 33.80α 35.21α
CDCM3 36.26α,3 33.94α ,3 35.40α,3
</table>
<tableCaption confidence="0.999572">
Table 1: Evaluation of translation quality.
</tableCaption>
<bodyText confidence="0.976607523809524">
CDCMk denotes the CDCM model trained in the
kth curriculum in Alg. 1 (i.e., three levels of
curriculum training), CICM denotes its context-
independent counterpart, and “All” is the com-
bined test sets. The superscripts α and Q indicate
statistically significant difference (p &lt; 0.05) from
Baseline and CICM, respectively.
For training the neural networks, we use 4 con-
volution layers for source sentences and 3 convo-
lution layers for target phrases. For both of them, 4
pooling layers (pooling size is 2) are used, and all
the feature maps are 100. We set the sliding win-
dow k = 3, and the learning rate q = 0.02. All
the parameters are selected based on the develop-
ment data. We train the word embeddings using a
bilingual strategy similar to Yang et al. (2013), and
set the dimension of the word embeddings be 50.
To produce high-quality bilingual phrase pairs to
train the CDCM model, we perform forced decod-
ing on the bilingual training sentences and collect
the used phrase pairs.
</bodyText>
<subsectionHeader confidence="0.99856">
5.2 Evaluation of Translation Quality
</subsectionHeader>
<bodyText confidence="0.998769">
We have two baseline systems:
</bodyText>
<listItem confidence="0.9905738">
• Baseline: The baseline system is an open-
source system of the phrase-based model –
Moses (Koehn et al., 2007) with a set of com-
mon features, including translation models,
word and phrase penalties, a linear distortion
model, a lexicalized reordering model, and a
language model.
• CICM (context-independent convolutional
matching) model: Following the previous
works (Gao et al., 2014; Zhang et al., 2014;
</listItem>
<bodyText confidence="0.972406133333334">
Cho et al., 2014), we calculate the match-
ing degree of a phrase pair without consider-
ing any contextual information. Each unique
phrase pair serves as a positive example and
a randomly selected target phrase from the
phrase table is the corresponding negative ex-
ample. The matching score is also introduced
into Baseline as an additional feature.
Table 1 summaries the results of CDCMs
trained from different curriculums. No matter
from which curriculum it is trained, the CDCM
model significantly improves the translation qual-
ity on the overall test data (with gains of 1.0
BLEU points). The best improvement can be up to
1.4 BLEU points on MT04 with the fully trained
CDCM. As expected, the translation performance
is consistently increased with curriculum grow-
ing. This indicates that the CDCM model indeed
captures the desirable semantic information by the
curriculum learning from easy to difficult.
Comparing with its context-independent coun-
terpart (CICM, Row 2), the CDCM model shows
significant improvement on all the test data con-
sistently. We contribute this to the incorporation
of useful discriminative information embedded in
the local context. In addition, the performance of
CICM is comparable with that of CDCM1. This is
intuitive, because both of them try to capture the
basic semantic similarity between the source and
target phrase pair.
One of the hypotheses we tested in the course of
this research was disproved. We thought it likely
that the difficult curriculum (CDCM3 that distin-
guishs the correct translation from other candi-
dates for a given context) would contribute most to
the improvement, since this circumstance is more
consistent with the real decoding procedure. This
turned out to be false, as shown in Table 1. One
possible reason is that the “negative” examples
(other candidates for the same source phrase) may
share the same semantic meaning with the posi-
tive one, thus give a wrong guide in the supervised
training. Constructing a reasonable set of nega-
tive examples that are more semantically different
from the positive one is left for our future work.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999255">
In this paper, we propose a context-dependent con-
volutional matching model to capture semantic
similarities between phrase pairs that are sensitive
to contexts. Experimental results show that our ap-
proach significantly improves the translation per-
formance and obtains improvement of 1.0 BLEU
scores on the overall test data.
Integrating deep architecture into context-
dependent translation selection is a promising way
to improve machine translation. In the future, we
will try to exploit contextual information at the tar-
get side (e.g., partial translations).
</bodyText>
<page confidence="0.993307">
540
</page>
<sectionHeader confidence="0.999205" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998292666666666">
This work is supported by China National 973
project 2014CB340301. Baotian Hu and Qinghai
Chen are supported by National Natural Science
Foundation of China 61173075 and 61473101. We
thank Junhui Li, and the anonymous reviewers for
their insightful comments.
</bodyText>
<sectionHeader confidence="0.998914" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880963855422">
Yoshua Bengio, J´erˆome Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
ICML 2009.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and Trends ® in Machine Learning,
2(1):1–127.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. In EMNLP 2014.
M. Collins, P. Koehn, and I. Kuˇcerov´a. 2005. Clause
restructuring for statistical machine translation. In
ACL 2005.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In ICML
2008.
George E Dahl, Tara N Sainath, and Geoffrey E Hinton.
2013. Improving deep neural networks for lvcsr us-
ing rectified linear units and dropout. In ICASSP
2013.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In ACL 2014.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase rep-
resentations for translation modeling. In ACL 2014.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In COLING 2008.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences. In
NIPS 2014.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
ICASSP 1995.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
ACL 2007.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
EMNLP 2008.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In ACL 2008.
Fandong Meng, Zhengdong Lu, Mingxuan Wang,
Hang Li, Wenbin Jiang, and Qun Liu. 2015. Encod-
ing source language with convolutional neural net-
work for machine translation. In ACL 2015.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL 2002.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of Seventh Inter-
national Conference on Spoken Language Process-
ing, volume 3, pages 901–904. Citeseer.
Haiyang Wu, Daxiang Dong, Xiaoguang Hu, Dian-
hai Yu, Wei He, Hua Wu, Haifeng Wang, and Ting
Liu. 2014. Improve statistical machine transla-
tion with context-sensitive bilingual semantic em-
bedding model. In EMNLP 2014.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai
Yu. 2013. Word Alignment Modeling with Context
Dependent Deep Neural Network. In ACL 2013.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In ACL
2014.
Jiajun Zhang. 2015. Local translation prediction with
global sentence representation. In IJCAI 2015.
</reference>
<page confidence="0.997851">
541
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.570146">
<title confidence="0.9892415">Context-Dependent Translation Selection Using Convolutional Neural Network</title>
<author confidence="0.816779">Zhengdong</author>
<affiliation confidence="0.984985333333333">Computing Research Ark Lab Center, Harbin Institute of Technology Huawei Technologies Co. Ltd. Graduate School</affiliation>
<email confidence="0.9252985">baotianchina@gmail.comlu.zhengdong@huawei.comqingcai.chen@hitsz.edu.cnhangli.hl@huawei.com</email>
<abstract confidence="0.992807304347826">We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is to capture semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrases and sentencelevel contexts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>J´erˆome Louradour</author>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>Curriculum learning.</title>
<date>2009</date>
<booktitle>In ICML</booktitle>
<contexts>
<context position="9027" citStr="Bengio et al. (2009)" startWordPosition="1404" endWordPosition="1407">mizing the above objective, to encourage the model to assign higher matching scores to positive examples and to assign lower scores to negative examples. We use stochastic gradient descent (SGD) to optimize the model parameters Θ. We train the CDCM model with a curriculum strategy to learn the contextdependent semantic similarity at the phrase level from easy (basic semantic similarities between the source and target phrase pair) to difficult (context-dependent semantic similarities for the same source phrase in varying contexts). 3.1 Curriculum Training Curriculum learning, first proposed by Bengio et al. (2009) in machine learning, refers to a sequence of training strategies that start small, learn easier aspects of the task, and then gradually increase the difficulty level. It has been shown that the curriculum learning can benefit the nonconvex training by giving rise to improved generalization and faster convergence. The key point is that the training examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. For each positive example (f, ˆe+), we have three types of negative examples according to the difficu</context>
</contexts>
<marker>Bengio, Louradour, Collobert, Weston, 2009</marker>
<rawString>Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In ICML 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning deep architectures for ai. Foundations and Trends ®</title>
<date>2009</date>
<booktitle>in Machine Learning,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="5046" citStr="Bengio, 2009" startWordPosition="735" endWordPosition="736">ng function. É É / / At I lfx A 44- &amp;if / / the key point is pooling convolution Layer-2 Layer-1 : tagged words : untagged words Matching Score matching model convolutional sentence model representation representation 2 Context-Dependent Convolutional Matching Model The model architecture, shown in Figure 1, is a variant of the convolutional architecture of Hu et al. (2014). It consists of two components: • convolutional sentence model that summarizes the meaning of the source sentence and the target phrase; • matching model that compares the two representations with a multi-layer perceptron (Bengio, 2009). Let eˆ be a target phrase and f be the source sentence that contains the source phrase aligning to ˆe. We first project f and eˆ into feature vectors x and y via the convolutional sentence model, and then compute the matching score s(x, y) by the matching model. Finally, the score is introduced into a conventional SMT system as an additional feature. Convolutional sentence model. As shown in Figure 1, the model takes as input the embeddings of words (trained beforehand elsewhere) in f and ˆe. It then iteratively summarizes the meaning of the input through layers of convolution and pooling, u</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Yoshua Bengio. 2009. Learning deep architectures for ai. Foundations and Trends ® in Machine Learning, 2(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<booktitle>In EMNLP</booktitle>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL</booktitle>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In ICML</booktitle>
<contexts>
<context position="12972" citStr="Collobert and Weston, 2008" startWordPosition="2072" endWordPosition="2075">sentations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks (Collobert and Weston, 2008). In recent years, there has also been growing interest in bilingual phrase representations that group phrases with a similar meaning across different languages. Based on that translation equivalents share the same semantic meaning, they can supervise each other to learn their semantic phrase embeddings in a continuous space (Gao et al., 2014; Zhang et al., 2014). However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Tara N Sainath</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Improving deep neural networks for lvcsr using rectified linear units and dropout.</title>
<date>2013</date>
<booktitle>In ICASSP</booktitle>
<contexts>
<context position="6307" citStr="Dahl et al., 2013" startWordPosition="951" endWordPosition="954">esentation in the final layer. In Layer-1, the convolution layer takes sliding windows on f and eˆ respectively, and models all the possible compositions of neighbouring words. The convolution involves a filter to produce a new feature for each possible composition. Given a k-sized sliding window i on f or ˆe, for example, the jth convolution unit of the composition of the words is generated by: ci(1,j) = g(ˆci(0)) · 0(w(1,j) · ˆci(0) + b(1,j)) (1) where • g(·) is the gate function that determines whether to activate 0(·); • 0(·) is a non-linear activation function. In this work, we use ReLu (Dahl et al., 2013) as the activation function; • w(1,j) is the parameters for the jth convolution unit on Layer-1, with matrix W(1) = [w(1,1),... , w(1,J)]; • ˆci(0) is a vector constructed by concatenating word vectors in the k-sized sliding widow i; • b(1,j) is a bias term, with vector B(1) = [b(1,1),..., b(1,J)]. To distinguish the phrase pair from its context, we use one additional dimension in word embeddings: 1 for words in the phrase pair and 0 for the others. After transforming words to 537 their tagged embeddings, the convolutional sentence model takes multiple choices of composition using sliding wind</context>
</contexts>
<marker>Dahl, Sainath, Hinton, 2013</marker>
<rawString>George E Dahl, Tara N Sainath, and Geoffrey E Hinton. 2013. Improving deep neural networks for lvcsr using rectified linear units and dropout. In ICASSP 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="13893" citStr="Devlin et al. (2014)" startWordPosition="2211" endWordPosition="2214">ngs in a continuous space (Gao et al., 2014; Zhang et al., 2014). However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs. 5 Experiments 5.1 Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test d</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In ACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning continuous phrase representations for translation modeling.</title>
<date>2014</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="1717" citStr="Gao et al., 2014" startWordPosition="230" endWordPosition="233">ntencelevel contexts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points. 1 Introduction Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms (Koehn et al., 2003), which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗* Corresponding author The above methods, however, neglect the information of local contexts, which has been proven to be useful for disambiguating translation candidates during decoding (He et al., 2008; Marton and Resnik, 2008). The matching scores of translation pairs are treated the same, even they are in different contexts. Accordingly, the methods fail to adapt to local contexts and l</context>
<context position="13316" citStr="Gao et al., 2014" startWordPosition="2126" endWordPosition="2129">s for SMT. In this study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks (Collobert and Weston, 2008). In recent years, there has also been growing interest in bilingual phrase representations that group phrases with a similar meaning across different languages. Based on that translation equivalents share the same semantic meaning, they can supervise each other to learn their semantic phrase embeddings in a continuous space (Gao et al., 2014; Zhang et al., 2014). However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole s</context>
<context position="16552" citStr="Gao et al., 2014" startWordPosition="2639" endWordPosition="2642">uce high-quality bilingual phrase pairs to train the CDCM model, we perform forced decoding on the bilingual training sentences and collect the used phrase pairs. 5.2 Evaluation of Translation Quality We have two baseline systems: • Baseline: The baseline system is an opensource system of the phrase-based model – Moses (Koehn et al., 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model. • CICM (context-independent convolutional matching) model: Following the previous works (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014), we calculate the matching degree of a phrase pair without considering any contextual information. Each unique phrase pair serves as a positive example and a randomly selected target phrase from the phrase table is the corresponding negative example. The matching score is also introduced into Baseline as an additional feature. Table 1 summaries the results of CDCMs trained from different curriculums. No matter from which curriculum it is trained, the CDCM model significantly improves the translation quality on the overall test data (with gains of 1.0 BLE</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In ACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving statistical machine translation using lexicalized rule selection.</title>
<date>2008</date>
<booktitle>In COLING</booktitle>
<contexts>
<context position="2127" citStr="He et al., 2008" startWordPosition="298" endWordPosition="301">mmatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗* Corresponding author The above methods, however, neglect the information of local contexts, which has been proven to be useful for disambiguating translation candidates during decoding (He et al., 2008; Marton and Resnik, 2008). The matching scores of translation pairs are treated the same, even they are in different contexts. Accordingly, the methods fail to adapt to local contexts and lead to precision issues for specific sentences in different contexts. To capture useful context information, we propose a convolutional neural network architecture to measure context-dependent semantic similarities between phrase pairs in two languages. For each phrase pair, we use the sentence containing the phrase in source language as the context. With the convolutional neural network, we summarize the i</context>
<context position="12406" citStr="He et al. (2008)" startWordPosition="1988" endWordPosition="1991"> 12-16), we compute the gradient of the loss objective LΘ and learn Θ using the SGD algorithm. Note that we meanwhile update the word embeddings to better capture the semantic equivalence across languages during training. If the loss function LΘ reaches a local minima or the iterations reach the predefined number, we terminate this curriculum. 4 Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks (Collobert and Weston, 2008). In recent years, there has also </context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improving statistical machine translation using lexicalized rule selection. In COLING 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In NIPS</booktitle>
<contexts>
<context position="4809" citStr="Hu et al. (2014)" startWordPosition="696" endWordPosition="699">utional sentence model (bottom) summarizes the meaning of the tagged sentence and target phrase, and the matching model (top) compares the representations using a multi-layer perceptron. “P” indicates all-zero padding turned off by the gating function. É É / / At I lfx A 44- &amp;if / / the key point is pooling convolution Layer-2 Layer-1 : tagged words : untagged words Matching Score matching model convolutional sentence model representation representation 2 Context-Dependent Convolutional Matching Model The model architecture, shown in Figure 1, is a variant of the convolutional architecture of Hu et al. (2014). It consists of two components: • convolutional sentence model that summarizes the meaning of the source sentence and the target phrase; • matching model that compares the two representations with a multi-layer perceptron (Bengio, 2009). Let eˆ be a target phrase and f be the source sentence that contains the source phrase aligning to ˆe. We first project f and eˆ into feature vectors x and y via the convolutional sentence model, and then compute the matching score s(x, y) by the matching model. Finally, the score is introduced into a conventional SMT system as an additional feature. Convolut</context>
</contexts>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In NIPS 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In ICASSP</booktitle>
<contexts>
<context position="14377" citStr="Kneser and Ney, 1995" startWordPosition="2284" endWordPosition="2287">rce sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs. 5 Experiments 5.1 Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). 1The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models MT04 MT05 All Baseline 34.86 33.18 34.40 CICM 35.82α 33.51α </context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In ICASSP 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL</booktitle>
<contexts>
<context position="1451" citStr="Koehn et al., 2003" startWordPosition="192" endWordPosition="195">pture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrases and sentencelevel contexts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points. 1 Introduction Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms (Koehn et al., 2003), which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗* Corresponding author The above methods, however, neglect the information of local contexts, which has been proven to be usefu</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="16277" citStr="Koehn et al., 2007" startWordPosition="2599" endWordPosition="2602">00. We set the sliding window k = 3, and the learning rate q = 0.02. All the parameters are selected based on the development data. We train the word embeddings using a bilingual strategy similar to Yang et al. (2013), and set the dimension of the word embeddings be 50. To produce high-quality bilingual phrase pairs to train the CDCM model, we perform forced decoding on the bilingual training sentences and collect the used phrase pairs. 5.2 Evaluation of Translation Quality We have two baseline systems: • Baseline: The baseline system is an opensource system of the phrase-based model – Moses (Koehn et al., 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model. • CICM (context-independent convolutional matching) model: Following the previous works (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014), we calculate the matching degree of a phrase pair without considering any contextual information. Each unique phrase pair serves as a positive example and a randomly selected target phrase from the phrase table is the corresponding negative example. The matching score is also introdu</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qun Liu</author>
<author>Zhongjun He</author>
<author>Yang Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based rule selection model for syntax-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="12425" citStr="Liu et al. (2008)" startWordPosition="1992" endWordPosition="1995">e the gradient of the loss objective LΘ and learn Θ using the SGD algorithm. Note that we meanwhile update the word embeddings to better capture the semantic equivalence across languages during training. If the loss function LΘ reaches a local minima or the iterations reach the predefined number, we terminate this curriculum. 4 Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks (Collobert and Weston, 2008). In recent years, there has also been growing intere</context>
</contexts>
<marker>Liu, He, Liu, Lin, 2008</marker>
<rawString>Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin. 2008. Maximum entropy based rule selection model for syntax-based statistical machine translation. In EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="2153" citStr="Marton and Resnik, 2008" startWordPosition="302" endWordPosition="305">mantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗* Corresponding author The above methods, however, neglect the information of local contexts, which has been proven to be useful for disambiguating translation candidates during decoding (He et al., 2008; Marton and Resnik, 2008). The matching scores of translation pairs are treated the same, even they are in different contexts. Accordingly, the methods fail to adapt to local contexts and lead to precision issues for specific sentences in different contexts. To capture useful context information, we propose a convolutional neural network architecture to measure context-dependent semantic similarities between phrase pairs in two languages. For each phrase pair, we use the sentence containing the phrase in source language as the context. With the convolutional neural network, we summarize the information of a phrase pai</context>
<context position="12454" citStr="Marton and Resnik (2008)" startWordPosition="1997" endWordPosition="2000">loss objective LΘ and learn Θ using the SGD algorithm. Note that we meanwhile update the word embeddings to better capture the semantic equivalence across languages during training. If the loss function LΘ reaches a local minima or the iterations reach the predefined number, we terminate this curriculum. 4 Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks (Collobert and Weston, 2008). In recent years, there has also been growing interest in bilingual phrase repres</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fandong Meng</author>
<author>Zhengdong Lu</author>
<author>Mingxuan Wang</author>
<author>Hang Li</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Encoding source language with convolutional neural network for machine translation.</title>
<date>2015</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="13696" citStr="Meng et al. (2015)" startWordPosition="2180" endWordPosition="2183">ases with a similar meaning across different languages. Based on that translation equivalents share the same semantic meaning, they can supervise each other to learn their semantic phrase embeddings in a continuous space (Gao et al., 2014; Zhang et al., 2014). However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs. 5 Experiments 5.1 Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language </context>
</contexts>
<marker>Meng, Lu, Wang, Li, Jiang, Liu, 2015</marker>
<rawString>Fandong Meng, Zhengdong Lu, Mingxuan Wang, Hang Li, Wenbin Jiang, and Qun Liu. 2015. Encoding source language with convolutional neural network for machine translation. In ACL 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="14561" citStr="Och, 2003" startWordPosition="2321" endWordPosition="2322">apturing context-dependent semantic similarities of translation pairs. 5 Experiments 5.1 Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). 1The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models MT04 MT05 All Baseline 34.86 33.18 34.40 CICM 35.82α 33.51α 34.95α CDCM1 35.87α 33.58 35.01α CDCM2 35.97α 33.80α 35.21α CDCM3 36.26α,3 33.94α ,3 35.40α,3 Table 1: Evaluation of translation quality. CDCMk denotes the CDCM model trained in the kt</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="14661" citStr="Papineni et al., 2002" startWordPosition="2333" endWordPosition="2337"> Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). 1The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models MT04 MT05 All Baseline 34.86 33.18 34.40 CICM 35.82α 33.51α 34.95α CDCM1 35.87α 33.58 35.01α CDCM2 35.97α 33.80α 35.21α CDCM3 36.26α,3 33.94α ,3 35.40α,3 Table 1: Evaluation of translation quality. CDCMk denotes the CDCM model trained in the kth curriculum in Alg. 1 (i.e., three levels of curriculum training), CICM denotes its contextindepend</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of Seventh International Conference on Spoken Language Processing,</booktitle>
<volume>3</volume>
<pages>901--904</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="14319" citStr="Stolcke, 2002" startWordPosition="2278" endWordPosition="2279">g (2015) have proposed independently to summary source sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs. 5 Experiments 5.1 Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). 1The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models MT</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of Seventh International Conference on Spoken Language Processing, volume 3, pages 901–904. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haiyang Wu</author>
<author>Daxiang Dong</author>
<author>Xiaoguang Hu</author>
<author>Dianhai Yu</author>
<author>Wei He</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Improve statistical machine translation with context-sensitive bilingual semantic embedding model.</title>
<date>2014</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="12560" citStr="Wu et al. (2014)" startWordPosition="2014" endWordPosition="2017"> capture the semantic equivalence across languages during training. If the loss function LΘ reaches a local minima or the iterations reach the predefined number, we terminate this curriculum. 4 Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks (Collobert and Weston, 2008). In recent years, there has also been growing interest in bilingual phrase representations that group phrases with a similar meaning across different languages. Based on that translation </context>
</contexts>
<marker>Wu, Dong, Hu, Yu, He, Wu, Wang, Liu, 2014</marker>
<rawString>Haiyang Wu, Daxiang Dong, Xiaoguang Hu, Dianhai Yu, Wei He, Hua Wu, Haifeng Wang, and Ting Liu. 2014. Improve statistical machine translation with context-sensitive bilingual semantic embedding model. In EMNLP 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Nenghai Yu</author>
</authors>
<title>Word Alignment Modeling with Context Dependent Deep Neural Network. In</title>
<date>2013</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="15875" citStr="Yang et al. (2013)" startWordPosition="2532" endWordPosition="2535">pendent counterpart, and “All” is the combined test sets. The superscripts α and Q indicate statistically significant difference (p &lt; 0.05) from Baseline and CICM, respectively. For training the neural networks, we use 4 convolution layers for source sentences and 3 convolution layers for target phrases. For both of them, 4 pooling layers (pooling size is 2) are used, and all the feature maps are 100. We set the sliding window k = 3, and the learning rate q = 0.02. All the parameters are selected based on the development data. We train the word embeddings using a bilingual strategy similar to Yang et al. (2013), and set the dimension of the word embeddings be 50. To produce high-quality bilingual phrase pairs to train the CDCM model, we perform forced decoding on the bilingual training sentences and collect the used phrase pairs. 5.2 Evaluation of Translation Quality We have two baseline systems: • Baseline: The baseline system is an opensource system of the phrase-based model – Moses (Koehn et al., 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model. • CICM (context-independent </context>
</contexts>
<marker>Yang, Liu, Li, Zhou, Yu, 2013</marker>
<rawString>Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai Yu. 2013. Word Alignment Modeling with Context Dependent Deep Neural Network. In ACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Bilingually-constrained phrase embeddings for machine translation.</title>
<date>2014</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="1737" citStr="Zhang et al., 2014" startWordPosition="234" endWordPosition="237">ts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points. 1 Introduction Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms (Koehn et al., 2003), which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗* Corresponding author The above methods, however, neglect the information of local contexts, which has been proven to be useful for disambiguating translation candidates during decoding (He et al., 2008; Marton and Resnik, 2008). The matching scores of translation pairs are treated the same, even they are in different contexts. Accordingly, the methods fail to adapt to local contexts and lead to precision iss</context>
<context position="13337" citStr="Zhang et al., 2014" startWordPosition="2130" endWordPosition="2133"> study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks (Collobert and Weston, 2008). In recent years, there has also been growing interest in bilingual phrase representations that group phrases with a similar meaning across different languages. Based on that translation equivalents share the same semantic meaning, they can supervise each other to learn their semantic phrase embeddings in a continuous space (Gao et al., 2014; Zhang et al., 2014). However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while</context>
<context position="16572" citStr="Zhang et al., 2014" startWordPosition="2643" endWordPosition="2646">ilingual phrase pairs to train the CDCM model, we perform forced decoding on the bilingual training sentences and collect the used phrase pairs. 5.2 Evaluation of Translation Quality We have two baseline systems: • Baseline: The baseline system is an opensource system of the phrase-based model – Moses (Koehn et al., 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model. • CICM (context-independent convolutional matching) model: Following the previous works (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014), we calculate the matching degree of a phrase pair without considering any contextual information. Each unique phrase pair serves as a positive example and a randomly selected target phrase from the phrase table is the corresponding negative example. The matching score is also introduced into Baseline as an additional feature. Table 1 summaries the results of CDCMs trained from different curriculums. No matter from which curriculum it is trained, the CDCM model significantly improves the translation quality on the overall test data (with gains of 1.0 BLEU points). The best </context>
</contexts>
<marker>Zhang, Liu, Li, Zhou, Zong, 2014</marker>
<rawString>Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and Chengqing Zong. 2014. Bilingually-constrained phrase embeddings for machine translation. In ACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
</authors>
<title>Local translation prediction with global sentence representation.</title>
<date>2015</date>
<booktitle>In IJCAI</booktitle>
<contexts>
<context position="13713" citStr="Zhang (2015)" startWordPosition="2185" endWordPosition="2186">ning across different languages. Based on that translation equivalents share the same semantic meaning, they can supervise each other to learn their semantic phrase embeddings in a continuous space (Gao et al., 2014; Zhang et al., 2014). However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs. 5 Experiments 5.1 Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke,</context>
</contexts>
<marker>Zhang, 2015</marker>
<rawString>Jiajun Zhang. 2015. Local translation prediction with global sentence representation. In IJCAI 2015.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>