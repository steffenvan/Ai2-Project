<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000444">
<title confidence="0.989484666666667">
SemEval-2010 Task 13:
Evaluating Events, Time Expressions, and Temporal Relations
(TempEval-2)
</title>
<author confidence="0.969458">
James Pustejovsky
</author>
<affiliation confidence="0.898311666666667">
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
</affiliation>
<email confidence="0.99723">
jamesp@cs.brandeis.edu
</email>
<sectionHeader confidence="0.997375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989666666667">
We describe the TempEval-2 task which is
currently in preparation for the SemEval-2010
evaluation exercise. This task involves iden-
tifying the temporal relations between events
and temporal expressions in text. Six distinct
subtasks are defined, ranging from identifying
temporal and event expressions, to anchoring
events to temporal expressions, and ordering
events relative to each other.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999790565217391">
Newspaper texts, narratives and other such texts de-
scribe events which occur in time and specify the
temporal location and order of these events. Text
comprehension, even at the most general level, in-
volves the capability to identify the events described
in a text and locate these in time. This capability is
crucial to a wide range of NLP applications, from
document summarization and question answering to
machine translation. As in many areas of NLP, an
open evaluation challenge in the area of temporal an-
notation will serve to drive research forward.
The automatic identification of all temporal re-
ferring expressions, events, and temporal relations
within a text is the ultimate aim of research in this
area. However, addressing this aim in a first evalua-
tion challenge was deemed too difficult and a staged
approach was suggested. The 2007 SemEval task,
TempEval (henceforth TempEval-1), was an initial
evaluation exercise based on three limited tasks that
were considered realistic both from the perspective
of assembling resources for development and test-
ing and from the perspective of developing systems
capable of addressing the tasks.
</bodyText>
<author confidence="0.782134">
Marc Verhagen
</author>
<affiliation confidence="0.797139333333333">
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
</affiliation>
<email confidence="0.987118">
marc@cs.brandeis.edu
</email>
<bodyText confidence="0.9991452">
We are now preparing TempEval-2, a temporal
evaluation task based on TempEval-1. TempEval-2
is more elaborate in two respects: (i) it is a multilin-
gual task, and (ii) it consists of six subtasks rather
than three.
</bodyText>
<sectionHeader confidence="0.972673" genericHeader="introduction">
2 TempEval-1
</sectionHeader>
<subsectionHeader confidence="0.520106">
TempEval-1 consisted of three tasks:
</subsectionHeader>
<bodyText confidence="0.99530795">
A. determine the relation between an event and a
timex in the same sentence;
B. determine the relation between an event and the
document creation time;
C. determine the relation between the main events
of two consecutive sentences.
The data sets were based on TimeBank (Puste-
jovsky et al., 2003; Boguraev et al., 2007), a hand-
built gold standard of annotated texts using the
TimeML markup scheme.1 The data sets included
sentence boundaries, TIMEX3 tags (including the
special document creation time tag), and EVENT
tags. For tasks A and B, a restricted set of events
was used, namely those events that occur more than
5 times in TimeBank. For all three tasks, the re-
lation labels used were BEFORE, AFTER, OVER-
LAP, BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER
and VAGUE.2 For a more elaborate description of
TempEval-1, see (Verhagen et al., 2007; Verhagen
et al., 2009).
</bodyText>
<footnote confidence="0.990541714285714">
1See www.timeml.org for details on TimeML, Time-
Bank is distributed free of charge by the Linguistic
Data Consortium (www.ldc.upenn.edu), catalog number
LDC2006T08.
2Which is different from the set of 13 labels from TimeML.
The set of labels for TempEval-1 was simplified to aid data
preparation and to reduce the complexity of the task.
</footnote>
<page confidence="0.92123">
112
</page>
<note confidence="0.8147015">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 112–116,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.869952962962963">
There were six systems competing in TempEval-
1: University of Colorado at Boulder (CU-TMP);
Language Computer Corporation (LCC-TE); Nara
Institute of Science and Technology (NAIST); Uni-
versity of Sheffield (USFD); Universities of Wolver-
hampton and Allicante (WVALI); and XEROX Re-
search Centre Europe (XRCE-T).
The difference between these systems was not
large, and details of system performance, along with
comparisons and evaluation, are presented in (Ver-
hagen et al., 2009). The scores for WVALI’s hybrid
approach were noticeably higher than those of the
other systems in task B and, using relaxed scoring,
in task C as well. But for task A, the highest scoring
systems are barely ahead of the rest of the field. Sim-
ilarly, for task C using strict scoring, there is no sys-
tem that clearly separates itself from the field. Inter-
estingly, the baseline is close to the average system
performance on task A, but for other tasks the sys-
tem scores noticeably exceed the baseline. Note that
the XRCE-T system is somewhat conservative in as-
signing TLINKS for tasks A and B, producing lower
recall scores than other systems, which in turn yield
lower f-measure scores. For task A, this is mostly
due to a decision only to assign a temporal relation
between elements that can also be linked by the syn-
tactic analyzer.
</bodyText>
<sectionHeader confidence="0.958987" genericHeader="method">
3 TempEval-2
</sectionHeader>
<bodyText confidence="0.997609878787879">
The set of tasks chosen for TempEval-1 was by no
means complete, but was a first step towards a fuller
set of tasks for temporal parsing of texts. While the
main goal of the division in subtasks was to aid eval-
uation, the larger goal of temporal annotation in or-
der to create a complete temporal characterization of
a document was not accomplished. Results from the
first competition indicate that task A was defined too
generally. As originally defined, it asks to tempo-
rally link all events in a sentence to all time expres-
sions in the same sentence. A clearer task would
have been to solicit local anchorings and to sepa-
rate these from the less well-defined temporal rela-
tions between arbitrary events and times in the same
sentence. We expect both inter-annotator agree-
ment and system performance to be higher with a
more precise subtask. Thus, the set of tasks used
in TempEval-1 is far from complete and the tasks
could have been made more restrictive. As a re-
sult, inter-annotator agreement scores lag, making
precise evaluation more challenging.
The overall goal of temporal tagging of a text is to
provide a temporal characterization of a set of events
that is as complete as possible. If the annotation
graph of a document is not completely connected
then it is impossible to determine temporal relations
between two arbitrary events because these events
could be in separate subgraphs. Hence, for the cur-
rent competition, TempEval-2, we have enriched the
task description to bring us closer to creating such
a temporal characterization for a text. We have en-
riched the TempEval-2 task definition to include six
distinct subtasks:
</bodyText>
<listItem confidence="0.911286259259259">
A. Determine the extent of the time expressions
in a text as defined by the TimeML TIMEX3
tag. In addition, determine value of the fea-
tures TYPE and VAL. The possible values of
TYPE are TIME, DATE, DURATION, and SET;
the value of VAL is a normalized value as de-
fined by the TIMEX2 and TIMEX3 standards.
B. Determine the extent of the events in a text as
defined by the TimeML EVENT tag. In addi-
tion, determine the value of the features TENSE,
ASPECT, POLARITY, and MODALITY.
C. Determine the temporal relation between an
event and a time expression in the same sen-
tence. For TempEval-2, this task is further re-
stricted by requiring that either the event syn-
tactically dominates the time expression or the
event and time expression occur in the same
noun phrase.
D. Determine the temporal relation between an
event and the document creation time.
E. Determine the temporal relation between two
main events in consecutive sentences.
F. Determine the temporal relation between two
events where one event syntactically dominates
the other event. This refers to examples like
”she heard an explosion” and ”he said they
postponed the meeting”.
</listItem>
<bodyText confidence="0.812431333333333">
The complete TimeML specification assumes the
temporal interval relations as defined by Allen
(Allen, 1983) in Figure 1.
</bodyText>
<page confidence="0.989072">
113
</page>
<figure confidence="0.999871090909091">
A A EQUALS B
B
A B A is BEFORE B; B is AFTER A
A B A MEETS B; B is MET BY A
A B A OVERLAPS B;
B is OVERLAPPED BY A
A A STARTS B;
B B is STARTED BY A
B A A FINISHES B;
B is FINISHED BY A
B A A is DURING B; B CONTAINS A
</figure>
<figureCaption confidence="0.999984">
Figure 1: Allen Relations
</figureCaption>
<bodyText confidence="0.999809230769231">
For this task, however, we assume a reduced sub-
set, as introduced in TempEval-1: BEFORE, AFTER,
OVERLAP, BEFORE-OR-OVERLAP, OVERLAP-OR-
AFTER and VAGUE. However, we are investigat-
ing whether for some tasks the more precise set of
TimeML relations could be used.
Task participants may choose to either do all
tasks, focus on the time expression task, focus on
the event task, or focus on the four temporal rela-
tion tasks. In addition, participants may choose one
or more of the five languages for which we provide
data: English, Italian, Chinese, Spanish, and Ko-
rean.
</bodyText>
<subsectionHeader confidence="0.999853">
3.1 Extent of Time Expression
</subsectionHeader>
<bodyText confidence="0.99994625">
This task involves identification of the EXTENT,
TYPE, and VAL of temporal expressions in the text.
Times can be expressed syntactically by adverbial or
prepositional phrases, as shown in the following:
</bodyText>
<listItem confidence="0.988173166666667">
(1) a. on Thursday
b. November 15, 2004
c. Thursday evening
d. in the late 80’s
e. Later this afternoon
f. yesterday
</listItem>
<bodyText confidence="0.997699333333333">
The TYPE of the temporal extent must be identified.
There are four temporal types that will be distin-
guished for this task;
</bodyText>
<listItem confidence="0.99739575">
(2) a. Time: at 2:45 p.m.
b. Date: January 27, 1920, yesterday
c. Duration two weeks
d. Set: every Monday morning
</listItem>
<bodyText confidence="0.987382666666667">
The VAL attribute will assume values according to
an extension of the ISO 8601 standard, as enhanced
by TIMEX2.
</bodyText>
<figure confidence="0.751296333333333">
(3) November 22, 2004
&lt;TIMEX3 tid=&amp;quot;t1&amp;quot; type=&amp;quot;DATE&amp;quot;
value=&amp;quot;2004-11-22&amp;quot;/&gt;
</figure>
<subsectionHeader confidence="0.999218">
3.2 Extent of Event Expression
</subsectionHeader>
<bodyText confidence="0.881523476190476">
The EVENT tag is used to annotate those elements in
a text that describe what is conventionally referred to
as an eventuality. Syntactically, events are typically
expressed as inflected verbs, although event nomi-
nals, such as ”crash” in killed by the crash, should
also be annotated as EVENTs.
In this task, event extents must be identified and
tagged with EVENT, along with values for the fea-
tures TENSE, ASPECT, POLARITY, and MODALITY.
Examples of these features are shown below:
(4) should have bought
&lt;EVENT id=&amp;quot;e1&amp;quot; pred=&amp;quot;BUY&amp;quot; pos=&amp;quot;VERB&amp;quot;
tense=&amp;quot;PAST&amp;quot; aspect=&amp;quot;PERFECTIVE&amp;quot;
modality=&amp;quot;SHOULD&amp;quot; polarity=&amp;quot;POS&amp;quot;/&gt;
(5) did not teach
&lt;EVENT id=&amp;quot;e2&amp;quot; pred=&amp;quot;TEACH&amp;quot; pos=&amp;quot;VERB&amp;quot;
tense=&amp;quot;PAST&amp;quot; aspect=&amp;quot;NONE&amp;quot;
modality=&amp;quot;NONE&amp;quot; polarity=&amp;quot;NEG&amp;quot;/&gt;
The specifics on the definition of event extent
will follow the published TimeML guideline (cf.
timeml.org).
</bodyText>
<subsectionHeader confidence="0.999447">
3.3 Within-sentence Event-Time Anchoring
</subsectionHeader>
<bodyText confidence="0.999492111111111">
This task involves determining the temporal relation
between an event and a time expression in the same
sentence. This was present in TempEval-1, but here,
in TempEval-2, this problem is further restricted by
requiring that the event either syntactically domi-
nates the time expression or the event and time ex-
pression occur in the same noun phrase. For exam-
ple, the following constructions will be targeted for
temporal labeling:
</bodyText>
<page confidence="0.994963">
114
</page>
<listItem confidence="0.6435925">
(6) Mary taughte, on Tuesday morningt,
OVERLAP(e1,t1)
(7) They cancelled the eveningt2 classe2
OVERLAP(e2,t2)
</listItem>
<subsectionHeader confidence="0.998293">
3.4 Neighboring Sentence Event-Event
Ordering
</subsectionHeader>
<bodyText confidence="0.998934111111111">
In this task, the goal is to identify the temporal re-
lation between two main events in consecutive sen-
tences. This task was covered in the previous com-
petition, and includes pairs such as that shown be-
low:
(8) The President spokee, to the nation on Tuesday
on the financial crisis. He had conferrede2 with
his cabinet regarding policy the day before.
AFTER(e1,e2)
</bodyText>
<subsectionHeader confidence="0.996655">
3.5 Sentence Event-DCT Ordering
</subsectionHeader>
<bodyText confidence="0.999695142857143">
This task was also included in TempEval-1 and re-
quires the identification of the temporal order be-
tween the matrix event of the sentence and the Docu-
ment Creation Time (DCT) of the article or text. For
example, the text fragment below specifies a fixed
DCT, relative to which matrix events from the two
sentences are ordered:
</bodyText>
<listItem confidence="0.940782333333333">
(9) DCT: MARCH 5, 2009
a. Most troops will leavee, Iraq by August of
2010. AFTER(e1,dct)
</listItem>
<bodyText confidence="0.682398">
b. The country defaultede2 on debts for that
entire year. BEFORE(e2,dct)
</bodyText>
<subsectionHeader confidence="0.996632">
3.6 Within-sentence Event-Event Ordering
</subsectionHeader>
<bodyText confidence="0.99962975">
The final task involves identifying the temporal re-
lation between two events, where one event syntac-
tically dominates the other event. This includes ex-
amples such as those illustrated below.
</bodyText>
<listItem confidence="0.9688205">
(10) The students hearde, afire alarme2.
OVERLAP(e1,e2)
(11) He saide, they had postponede2 the meeting.
AFTER(e1,e2)
</listItem>
<sectionHeader confidence="0.929083" genericHeader="evaluation">
4 Resources and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.992013">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.998725">
The development corpus will contain the following
data:
</bodyText>
<listItem confidence="0.9983238">
1. Sentence boundaries;
2. The document creation time (DCT) for each
document;
3. All temporal expressions in accordance with
the TimeML TIMEX3 tag;
4. All events in accordance with the TimeML
EVENT tag;
5. Main event markers for each sentence;
6. All temporal relations defined by tasks C
through F.
</listItem>
<bodyText confidence="0.99992175">
The data for the five languages are being prepared
independently of each other. We do not provide a
parallel corpus. However, annotation specifications
and guidelines for the five languages will be devel-
oped in conjunction with one other. For some lan-
guages, we may not use all four temporal linking
tasks. Data preparation is currently underway for
English and will start soon for the other languages.
Obviously, data preparation is a large task. For En-
glish and Chinese, the data are being developed at
Brandeis University under three existing grants.
For evaluation data, we will provide two data sets,
each consisting of different documents. DataSet1 is
for tasks A and B and will contain data item 1 and 2
from the list above. DataSet2 is for tasks C though
F and will contain data items 1 through 5.
</bodyText>
<subsectionHeader confidence="0.984647">
4.2 Data Preparation
</subsectionHeader>
<bodyText confidence="0.9914468">
For all languages, annotation guidelines are defined
for all tasks, based on version 1.2.1 of the TimeML
annotation guidelines for English3. The most no-
table changes relative to the previous TimeML
guidelines are the following:
• The guidelines are not all presented in one doc-
ument, but are split up according to the seven
TempEval-2 tasks. Full temporal annotation
has proven to be a very complex task, split-
ting it into subtasks with separate guidelines for
</bodyText>
<footnote confidence="0.650846">
3Seehttp://www.timeml.org.
</footnote>
<page confidence="0.997975">
115
</page>
<bodyText confidence="0.984143411764706">
each task has proven to make temporal annota-
tion more manageable.
• It is not required that all tasks for temporal link-
ing (tasks C through F) use the same relation
set. One of the goals during the data prepara-
tion phase is to determine what kind of relation
set makes sense for each individual task.
• The guidelines can be different depending on
the language. This is obviously required be-
cause time expressions, events, and relations
are expressed differently across languages.
Annotation proceeds in two phases: a dual
annotation phase where two annotators annotate
each document and an adjudication phase where a
judge resolves disagreements between the annota-
tors. We are expanding the annotation tool used for
TempEval-1, making sure that we can quickly an-
notate data for all tasks while making it easy for a
language to define an annotation task in a slightly
different way from another language. The Brandeis
Annotation Tool (BAT) is a generic web-based anno-
tation tool that is centered around the notion of an-
notation tasks. With the task decomposition allowed
by BAT, it is possible to flexibly structure the com-
plex task of temporal annotation by splitting it up in
as many sub tasks as seems useful. As such, BAT is
well-suited for TempEval-2 annotation. Comparison
of annotation speed with tools that do not allow task
decomposition showed that annotation with BAT is
up to ten times faster. Annotation has started for
Italian and English.
For all tasks, precision and recall are used as eval-
uation metrics. A scoring program will be supplied
for participants.
</bodyText>
<sectionHeader confidence="0.999597" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999969222222222">
In this paper, we described the TempEval-2 task
within the SemEval 2010 competition. This task
involves identifying the temporal relations between
events and temporal expressions in text. Using
a subset of TimeML temporal relations, we show
how temporal relations and anchorings can be an-
notated and identified in five different languages.
The markup language adopted presents a descrip-
tive framework with which to examine the tempo-
ral aspects of natural language information, demon-
strating in particular, how tense and temporal infor-
mation is encoded in specific sentences, and how
temporal relations are encoded between events and
temporal expressions. This work paves the way to-
wards establishing a broad and open standard meta-
data markup language for natural language texts, ex-
amining events, temporal expressions, and their or-
derings.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999767958333333">
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832–843.
Bran Boguraev, James Pustejovsky, Rie Ando, and Marc
Verhagen. 2007. Timebank evolution as a community
resource for timeml parsing. Language Resource and
Evaluation, 41(1):91–115.
James Pustejovsky, David Day, Lisa Ferro, Robert
Gaizauskas, Patrick Hanks, Marcia Lazo, Roser Saur´ı,
Andrew See, Andrea Setzer, and Beth Sundheim.
2003. The TimeBank Corpus. Corpus Linguistics,
March.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proc. of the Fourth Int. Work-
shop on Semantic Evaluations (SemEval-2007), pages
75–80, Prague, Czech Republic, June. Association for
Computational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The tempeval challenge: identifying
temporal relations in text. Language Resources and
Evaluation.
</reference>
<page confidence="0.999019">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.468484">
<title confidence="0.758837">SemEval-2010 Task 13: Evaluating Events, Time Expressions, and Temporal (TempEval-2)</title>
<author confidence="0.996809">James Pustejovsky</author>
<affiliation confidence="0.999879">Computer Science Department Brandeis University</affiliation>
<address confidence="0.998383">Waltham, Massachusetts, USA</address>
<email confidence="0.99986">jamesp@cs.brandeis.edu</email>
<abstract confidence="0.9991881">We describe the TempEval-2 task which is currently in preparation for the SemEval-2010 evaluation exercise. This task involves identifying the temporal relations between events and temporal expressions in text. Six distinct subtasks are defined, ranging from identifying temporal and event expressions, to anchoring events to temporal expressions, and ordering events relative to each other.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Maintaining knowledge about temporal intervals.</title>
<date>1983</date>
<journal>Communications of the ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="7765" citStr="Allen, 1983" startWordPosition="1242" endWordPosition="1243">hat either the event syntactically dominates the time expression or the event and time expression occur in the same noun phrase. D. Determine the temporal relation between an event and the document creation time. E. Determine the temporal relation between two main events in consecutive sentences. F. Determine the temporal relation between two events where one event syntactically dominates the other event. This refers to examples like ”she heard an explosion” and ”he said they postponed the meeting”. The complete TimeML specification assumes the temporal interval relations as defined by Allen (Allen, 1983) in Figure 1. 113 A A EQUALS B B A B A is BEFORE B; B is AFTER A A B A MEETS B; B is MET BY A A B A OVERLAPS B; B is OVERLAPPED BY A A A STARTS B; B B is STARTED BY A B A A FINISHES B; B is FINISHED BY A B A A is DURING B; B CONTAINS A Figure 1: Allen Relations For this task, however, we assume a reduced subset, as introduced in TempEval-1: BEFORE, AFTER, OVERLAP, BEFORE-OR-OVERLAP, OVERLAP-ORAFTER and VAGUE. However, we are investigating whether for some tasks the more precise set of TimeML relations could be used. Task participants may choose to either do all tasks, focus on the time express</context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>James Allen. 1983. Maintaining knowledge about temporal intervals. Communications of the ACM, 26(11):832–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bran Boguraev</author>
<author>James Pustejovsky</author>
<author>Rie Ando</author>
<author>Marc Verhagen</author>
</authors>
<title>Timebank evolution as a community resource for timeml parsing. Language Resource and Evaluation,</title>
<date>2007</date>
<pages>41--1</pages>
<contexts>
<context position="2470" citStr="Boguraev et al., 2007" startWordPosition="366" endWordPosition="369">Massachusetts, USA marc@cs.brandeis.edu We are now preparing TempEval-2, a temporal evaluation task based on TempEval-1. TempEval-2 is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three. 2 TempEval-1 TempEval-1 consisted of three tasks: A. determine the relation between an event and a timex in the same sentence; B. determine the relation between an event and the document creation time; C. determine the relation between the main events of two consecutive sentences. The data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a handbuilt gold standard of annotated texts using the TimeML markup scheme.1 The data sets included sentence boundaries, TIMEX3 tags (including the special document creation time tag), and EVENT tags. For tasks A and B, a restricted set of events was used, namely those events that occur more than 5 times in TimeBank. For all three tasks, the relation labels used were BEFORE, AFTER, OVERLAP, BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER and VAGUE.2 For a more elaborate description of TempEval-1, see (Verhagen et al., 2007; Verhagen et al., 2009). 1See www.timeml.org for details on TimeML, TimeBank is </context>
</contexts>
<marker>Boguraev, Pustejovsky, Ando, Verhagen, 2007</marker>
<rawString>Bran Boguraev, James Pustejovsky, Rie Ando, and Marc Verhagen. 2007. Timebank evolution as a community resource for timeml parsing. Language Resource and Evaluation, 41(1):91–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>David Day</author>
<author>Lisa Ferro</author>
<author>Robert Gaizauskas</author>
<author>Patrick Hanks</author>
<author>Marcia Lazo</author>
<author>Roser Saur´ı</author>
<author>Andrew See</author>
<author>Andrea Setzer</author>
<author>Beth Sundheim</author>
</authors>
<title>The TimeBank Corpus. Corpus Linguistics,</title>
<date>2003</date>
<marker>Pustejovsky, Day, Ferro, Gaizauskas, Hanks, Lazo, Saur´ı, See, Setzer, Sundheim, 2003</marker>
<rawString>James Pustejovsky, David Day, Lisa Ferro, Robert Gaizauskas, Patrick Hanks, Marcia Lazo, Roser Saur´ı, Andrew See, Andrea Setzer, and Beth Sundheim. 2003. The TimeBank Corpus. Corpus Linguistics, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2007 task 15: Tempeval temporal relation identification.</title>
<date>2007</date>
<booktitle>In Proc. of the Fourth Int. Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>75--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2989" citStr="Verhagen et al., 2007" startWordPosition="450" endWordPosition="453">tive sentences. The data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a handbuilt gold standard of annotated texts using the TimeML markup scheme.1 The data sets included sentence boundaries, TIMEX3 tags (including the special document creation time tag), and EVENT tags. For tasks A and B, a restricted set of events was used, namely those events that occur more than 5 times in TimeBank. For all three tasks, the relation labels used were BEFORE, AFTER, OVERLAP, BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER and VAGUE.2 For a more elaborate description of TempEval-1, see (Verhagen et al., 2007; Verhagen et al., 2009). 1See www.timeml.org for details on TimeML, TimeBank is distributed free of charge by the Linguistic Data Consortium (www.ldc.upenn.edu), catalog number LDC2006T08. 2Which is different from the set of 13 labels from TimeML. The set of labels for TempEval-1 was simplified to aid data preparation and to reduce the complexity of the task. 112 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 112–116, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics There were six systems competing </context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In Proc. of the Fourth Int. Workshop on Semantic Evaluations (SemEval-2007), pages 75–80, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Jessica Moszkowicz</author>
<author>James Pustejovsky</author>
</authors>
<title>The tempeval challenge: identifying temporal relations in text. Language Resources and Evaluation.</title>
<date>2009</date>
<contexts>
<context position="3013" citStr="Verhagen et al., 2009" startWordPosition="454" endWordPosition="457">a sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a handbuilt gold standard of annotated texts using the TimeML markup scheme.1 The data sets included sentence boundaries, TIMEX3 tags (including the special document creation time tag), and EVENT tags. For tasks A and B, a restricted set of events was used, namely those events that occur more than 5 times in TimeBank. For all three tasks, the relation labels used were BEFORE, AFTER, OVERLAP, BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER and VAGUE.2 For a more elaborate description of TempEval-1, see (Verhagen et al., 2007; Verhagen et al., 2009). 1See www.timeml.org for details on TimeML, TimeBank is distributed free of charge by the Linguistic Data Consortium (www.ldc.upenn.edu), catalog number LDC2006T08. 2Which is different from the set of 13 labels from TimeML. The set of labels for TempEval-1 was simplified to aid data preparation and to reduce the complexity of the task. 112 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 112–116, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics There were six systems competing in TempEval1: University</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Moszkowicz, Pustejovsky, 2009</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Jessica Moszkowicz, and James Pustejovsky. 2009. The tempeval challenge: identifying temporal relations in text. Language Resources and Evaluation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>