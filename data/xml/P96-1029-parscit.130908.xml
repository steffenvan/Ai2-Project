<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.997609">
Compilation of Weighted Finite-State Transducers from
Decision Trees
</title>
<author confidence="0.946119">
Richard Sproat
</author>
<affiliation confidence="0.741294">
Bell Laboratories
</affiliation>
<address confidence="0.6817395">
700 Mountain Avenue
Murray Hill, NJ, USA
</address>
<email confidence="0.917311">
rwsObell—labs.com
</email>
<sectionHeader confidence="0.991765" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999739375">
We report on a method for compiling
decision trees into weighted finite-state
transducers. The key assumptions are
that the tree predictions specify how to
rewrite symbols from an input string,
and the decision at each tree node is
stateable in terms of regular expressions
on the input string. Each leaf node
can then be treated as a separate rule
where the left and right contexts are
constructable from the decisions made
traversing the tree from the root to the
leaf. These rules are compiled into trans-
ducers using the weighted rewrite-rule
rule-compilation algorithm described in
(Mohri and Sproat, 1996).
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.966421782608696">
Much attention has been devoted recently to
methods for inferring linguistic models from data.
One powerful inference method that has been
used in various applications are decision trees,
and in particular classification and regression trees
(Breiman et al., 1984).
An increasing amount of attention has also
been focussed on finite-state methods for imple-
menting linguistic models, in particular finite-
state transducers and weighted finite-state trans-
ducers; see (Kaplan and Kay, 1994; Pereira et al.,
1994, inter alia). The reason for the renewed in-
terest in finite-state mechanisms is clear. Finite-
state machines provide a mathematically well-
understood computational framework for repre-
senting a wide variety of information, both in NLP
and speech processing. Lexicons, phonological
rules, Hidden Markov Models, and (regular) gram-
mars are all representable as finite-state machines,
and finite-state operations such as union, intersec-
tion and composition mean that information from
these various sources can be combined in useful
Michael Riley
</bodyText>
<sectionHeader confidence="0.898485" genericHeader="introduction">
AT&amp;T Research
</sectionHeader>
<address confidence="0.679643">
600 Mountain Avenue
Murray Hill, NJ, USA
</address>
<email confidence="0.660293">
rileyOresearch.att.com
</email>
<bodyText confidence="0.999739363636364">
and computationally attractive ways. The reader
is referred to the above-cited papers (among oth-
ers) for more extensive justification.
This paper reports on a marriage of these two
strands of research in the form of an algorithm for
compiling the information in decision trees into
weighted finite-state transducers.&apos; Given this al-
gorithm, information inferred from data and rep-
resented in a tree can be used directly in a system
that represents other information, such as lexicons
or grammars, in the form of finite-state machines.
</bodyText>
<sectionHeader confidence="0.942796" genericHeader="method">
2 Quick Review of Tree-Based
Modeling
</sectionHeader>
<bodyText confidence="0.999928916666667">
A general introduction to classification and regres-
sion trees (&apos;CART&apos;) including the algorithm for
growing trees from data can be found in (Breiman
et al., 1984). Applications of tree-based modeling
to problems in speech and NLP are discussed in
(Riley, 1989; Riley, 1991; Wang and Hirschberg,
1992; Magerman, 1995, inter alia). In this section
we presume that one has already trained a tree
or set of trees, and we merely remind the reader
of the salient points in the interpretation of those
trees.
Consider the tree depicted in Figure 1, which
was trained on the TIMIT database (Fisher et al.,
1987), and which models the phonetic realization
of the English phoneme /aa/ (/a/) in various en-
vironments (Riley, 1991). When this tree is used
in predicting the allophonic form of a particular
instance of /aa/, one starts at the root of the
tree, and asks questions about the environment
in which the /aa/ is found. Each non-leaf node n,
dominates two daughter nodes conventionally la-
beled as 2n and 2n + 1; the decision on whether to
go left to 2n or right to 2n + 1 depends on the an-
swer to the question that is being asked at node n.
</bodyText>
<footnote confidence="0.87777">
1The work reported here can thus be seen as com-
plementary to recent reports on methods for directly
inferring transducers from data (Oncina et al., 1993;
Gildea and Jurafsky, 1995).
</footnote>
<page confidence="0.998986">
215
</page>
<bodyText confidence="0.998655104166667">
A concrete example will serve to illustrate. Con-
sider that we have /aa/ in some environment. The
first question that is asked concerns the number of
segments, including the /aa/ itself, that occur to
the left of the /aa/ in the word in which /aa/ oc-
curs. (See Table 1 for an explanation of the sym-
bols used in Figure 1.) In this case, if the /aa/
is initial — i.e., lseg is 1, one goes left; if there
is one or more segments to the left in the word,
go right. Let us assume that this /aa/ is initial
in the word, in which case we go left. The next
question concerns the consonantal &apos;place&apos; of artic-
ulation of the segment to the right of /aa/; if it
is alveolar go left; otherwise, if it is of some other
quality, or if the segment to the right of /aa/ is not
a consonant, then go right. Let us assume that the
segment to the right is /z/, which is alveolar, so we
go left. This lands us at terminal node 4. The tree
in Figure 1 shows us that in the training data 119
out of 308 occurrences of /aa/ in this environment
were realized as [ao], or in other words that we can
estimate the probability of /aa/ being realized as
[ao] in this environment as .385. The full set of
realizations at this node with estimated non-zero
probabilities is as follows (see Table 2 for a rele-
vant set of ARPABET-IPA correspondences):
phone probability — log prob. (weight)
ao 0.385 0.95
aa 0.289 1.24
q+aa 0.103 2.27
q+ao 0.096 2.34
ah 0.069 2.68
ax 0.058 2.84
An important point to bear in mind is that a
decision tree in general is a complete description,
in the sense that for any new data point, there
will be some leaf node that corresponds to it. So
for the tree in Figure 1, each new novel instance
of /aa/ will be handled by (exactly) one leaf node
in the tree, depending upon the environment in
which the /aa/ finds itself.
Another important point is that each deci-
sion tree considered here has the property that
its predictions specify how to rewrite a symbol (in
context) in an input string. In particular, they
specify a two-level mapping from a set of input
symbols (phonemes) to a set of output symbols
(allophones).
</bodyText>
<sectionHeader confidence="0.9933095" genericHeader="method">
3 Quick Review of Rule
Compilation
</sectionHeader>
<bodyText confidence="0.99972196">
Work on finite-state phonology (Johnson, 1972;
Koskenniemi, 1983; Kaplan and Kay, 1994) has
shown that systems of rewrite rules of the famil-
iar form 0 —+ O/A p, where 0, 0, A and p are
regular expressions, can be represented computa-
tionally as finite-state transducers (FSTs): note
that 0 represents the rule&apos;s input rule, V) the out-
put, and A and p, respectively, the left and right
contexts.
Kaplan and Kay (1994) have presented a con-
crete algorithm for compiling systems of such
rules into FSTs. These methods can be ex-
tended slightly to include the compilation of prob-
abilistic or weighted rules into weighted finite-
state-transducers (WFSTs — see (Pereira et al.,
1994)); Mohri and Sproat (1996) describe a rule-
compilation algorithm which is more efficient than
the Kaplan-Kay algorithm, and which has been
extended to handle weighted rules. For present
purposes it is sufficient to observe that given this
extended algorithm, we can allow in the expres-
sion 0 —+ O/A. p, to represent a weighted reg-
ular expression. The compiled transducer corre-
sponding to that rule will replace 0 with b with
the appropriate weights in the context A p.
</bodyText>
<sectionHeader confidence="0.976953" genericHeader="method">
4 The Tree Compilation Algorithm
</sectionHeader>
<bodyText confidence="0.999992764705882">
The key requirements on the kind of decision trees
that we can compile into WFSTs are (1) the pre-
dictions at the leaf nodes specify how to rewrite
a particular symbol in an input string, and (2)
the decisions at each node are stateable as regu-
lar expressions over the input string. Each leaf
node represents a single rule. The regular expres-
sions for each branch describe one aspect of the
left context A, right context p, or both. The left
and right contexts for the rule consist of the inter-
sections of the partial descriptions of these con-
texts defined for each branch traversed between
the root and leaf node. The input 0 is prede-
fined for the entire tree, whereas the output 7,b is
defined as the union of the set of outputs, along
with their weights, that are associated with the
leaf node. The weighted rule belonging to the leaf
node can then be compiled into a transducer us-
ing the weighted-rule-compilation algorithm refer-
enced in the preceding section. The transducer for
the entire tree can be derived by the intersection
of the entire set of transducers associated with the
leaf nodes. Note that while regular relations are
not generally closed under intersection, the subset
of same-length (or more strictly speaking length-
preserving) relations is closed; see below.
To see how this works, let us return to the ex-
ample in Figure 1. To start with, we know that
this tree models the phonetic realization of /aa/,
so we can immediately set 0 to be aa for the whole
tree. Next, consider again the traversal of the tree
from the root node to leaf node 4. The first deci-
sion concerns the number of segments to the left
of the /aa/ in the word, either none for the left
</bodyText>
<page confidence="0.994455">
216
</page>
<figure confidence="0.9702462">
megi , party str:n/a,no,sec
aa I aa aa
q+aa
110/349 2080/2080 415/439
10 69/11128 14 15
</figure>
<figureCaption confidence="0.847797333333333">
Figure 1: Tree modeling the phonetic realization of /aaf. All phones are given in ARPABET. Table 2 gives
ARPABET-IPA conversions for symbols relevant to this example. See Table 1 for an explanation of other
symbols
</figureCaption>
<bodyText confidence="0.991023266666667">
cpn place of articulation of consonant n segments to the right
cp-n place of articulation of consonant n segments to the left
values: alveolar; bilabial; labiodental; dental; palatal; velar; pharyngeal;
n/a if is a vowel, or there is no such segment
vpn place of articulation of vowel n segments to the right
vp-n place of articulation of vowel n segments to the left
values: central-mid-high; back-low; back-mid-low; back-high; front-low;
front-mid-low; front-mid-high; front-high; central-mid-low; back-mid-high
n/a if is a consonant, or there is no such segment
lseg number of preceding segments including the segment of interest within the word
rseg number of following segments including the segment of interest within the word
values: 1, 2, 3, many
sir stress assigned to this vowel
values: primary, secondary, no (zero) stress
n/a if there is no stress mark
</bodyText>
<figure confidence="0.998652285714286">
Iseg:2,3,many
119/308
ao
cp1:alv vp-1:cmh,b1,bml,bh
i
p1:blab,labd,den,pal,vel,pha,n/a
f
4
rseg:2
172/477
258/385
aa
6
vp-1:fl,fml,fmh,fh,cml,bmh,n/a
</figure>
<tableCaption confidence="0.9946">
Table 1: Explanation of symbols in Figure 1.
</tableCaption>
<page confidence="0.974997">
217
</page>
<figure confidence="0.876115333333333">
aa a
ao
ax a
ah A
q+aa 7a
q+ao 7o
</figure>
<tableCaption confidence="0.978571">
Table 2: ARPABET-IPA conversion for symbols relevant for Figure 1.
</tableCaption>
<bodyText confidence="0.999153157894737">
branch, or one or more for the right branch. As-
suming that we have a symbol a representing a
single segment, the symbol # representing a word
boundary, and allowing for the possibility of in-
tervening optional stress marks — &apos; — which do
not count as segments, these two possibilities can
be represented by the regular expressions for A in
(a) of Table 3.2 At this node there is no deci-
sion based on the righthand context, so the right-
hand context is free. We can represent this by
setting p at this node to be E*, where E (con-
ventionally) represents the entire alphabet: note
that the alphabet is defined to be an alphabet of
all 00 correspondence pairs that were determined
empirically to be possible.
The decision at the left daughter of the root
node concerns whether or not the segment to the
right is an alveolar. Assuming we have defined
classes of segments alv, blab, and so forth (repre-
sented as unions of segments) we can represent the
regular expression for p as in (b) of Table 3. In
this case it is A which is unrestricted, so we can
set that at E*.
We can derive the A and p expressions for
the rule at leaf node 4 by intersecting together
the expressions for these contexts defined for each
branch traversed on the way to the leaf. For
leaf node 4, A = #Opt(&apos;) n E* = #Opt(&apos;), and
p = *fl Opt(&apos;)(alv) = Opt(1)(alv).3 The rule
input 0 has already been given as aa. The output
1,1) is defined as the union of all of the possible ex-
pressions — at the leaf node in question — that aa
could become, with their associated weights (neg-
ative log probabilities), which we represent here as
subscripted floating-point numbers:
= ao0.95 U aa1.24 U q+aa2.27 U q+a02.34U
ah2.68 U aX2.84
Thus the entire weighted rule can be written as
</bodyText>
<footnote confidence="0.99098475">
2As far as possible, we use the notation of Kaplan
and Kay (1994).
3Strictly speaking, since the As and ps at each
branch may define expressions of different lengths, it
is necessary to left-pad each A with E*, and right-pad
each p with E. We gloss over this point here in order
to make the regular expressions somewhat simpler to
understand
</footnote>
<bodyText confidence="0.699385">
follows:
</bodyText>
<equation confidence="0.9116685">
aa (ao0.9sUaai.24Uq-4-aa2.27Uq±a02.34Uah2.68U
ax2.84)1#0pt0 Opt(&apos;)(alv)
</equation>
<bodyText confidence="0.941368435897436">
By a similar construction, the rule at node 6, for
example, would be represented as:
aa (aa0,40 U aoLii)/ (#(0pt(1)a)+Opt(&apos;)) fl
(E*((crnh) U (14) U (bml) U (bh))) E*
Each node thus represents a rule which states
that a mapping occurs between the input symbol
0 and the weighted expression 0 in the condition
described by A p. Now, in cases where 0 finds
itself in a context that is not subsumed by A p,
the rule behaves exactly as a two-level surface co-
ercion rule (Koskenniemi, 1983): it freely allows
0 to correspond to any 0 as specified by the al-
phabet of pairs. These 0:0 correspondences are,
however, constrained by other rules derived from
the tree, as we shall see directly.
The interpretation of the full tree is that it
represents the conjunction of all such mappings:
for rules 1, 2 ... n, cb corresponds to 01 given con-
dition Ai pi and 0 corresponds to 02 given
condition A2 p2 ...and 0 corresponds to On
given condition An_ pn. But this conjunction is
simply the intersection of the entire set of trans-
ducers defined for the leaves of the tree. Observe
now that the 0:0 correspondences that were left
free by the rule of one leaf node, are constrained
by intersection with the other leaf nodes: since, as
noted above, the tree is a complete description, it
follows that for any leaf node i, and for any context
A p not subsumed by Ai pi, there is some
leaf node j such that Ai pj subsumes A_ p.
Thus, the transducers compiled for the rules at
nodes 4 and 6, are intersected together, along with
the rules for all the other leaf nodes. Now, as
noted above, and as discussed by Kaplan and Kay
(1994) regular relations — the algebraic counter-
part of FSTs — are not in general closed under
intersection; however, the subset of same-length
regular relations is closed under intersection, since
they can be thought of as finite-state acceptors ex-
</bodyText>
<page confidence="0.991599">
218
</page>
<figure confidence="0.776015727272727">
(a) left branch A = #Opt(&apos;)
P = E*
right branch A = (#0ptnaOpt0)U (#0ptnaOptnaOpt(9)U
(#0 ptnaOptnceOpt(&apos;)(a0pt(9)+)
=
P = E*
(b) left branch A = E*
p = Opt(1)(alv)
right branch A = E*
p = (0 pt(&apos;)(blab)) U (Opt(1)(labd)) U (Opt(&apos;)(den)) U (Opt(&apos;)(pal))U
(Opt(1)(vel)) U (Opt(&apos;)(pha)) U (Opt(1)(n/a))
</figure>
<tableCaption confidence="0.748787666666667">
Table 3: Regular-expression interpretation of the decisions involved in going from the root node to leaf node
4 in the tree in Figure 1. Note that, as per convention, superscript `+&apos; denotes one or more instances of an
expression.
</tableCaption>
<bodyText confidence="0.924082894736842">
pressed over pairs of symbols.4 This point can
be extended somewhat to include relations that
involve bounded deletions or insertions: this is pre-
cisely the interpretation necessary for systems of
two-level rules (Koskenniemi, 1983), where a sin-
gle transducer expressing the entire system may
be constructed via intersection of the transduc-
ers expressing the individual rules (Kaplan and
Kay, 1994, pages 367-376). Indeed, our decision
tree represents neither more nor less than a set of
weighted two-level rules. Each of the symbols in
the expressions for A and p actually represent (sets
of) pairs of symbols: thus (zit), for example, rep-
resents all lexical alveolars paired with all of their
possible surface realizations. And just as each tree
represents a system of weighted two-level rules, so
a set of trees — e.g., where each tree deals with
the realization of a particular phone — represents
a system of weighted two-level rules, where each
two-level rule is compiled from each of the indi-
vidual trees.
We can summarize this discussion more for-
mally as follows. We presume a function Compile
which given a rule returns the WFST computing
that rule. The WFST for a single leaf L is thus
defined as follows, where OT is the input symbol
for the entire tree, OL is the output expression de-
fined at L, PL represents the path traversed from
the root node to L, p is an individual branch on
4 One can thus define intersection for transducers
analogously with intersection for acceptors. Given
two machines G1 and G2, with transition functions
61 and 62, one can define the transition function
of G, 6, as follows: for an input-output pair (i, o),
6((qi , q2), (i, o)) = (q, q) if and only if bi (qi , (i, o)) =
q; and 62 (q2 , (i, o)) = q;.
that path, and Ap and pp are the expressions for
A and p defined at p:
</bodyText>
<equation confidence="0.803740428571429">
PP) A
Rule&apos;, = Compile(OT -- Oa n P— fl
pEPL pePL.
The transducer for an entire tree T is defined as:
&apos;,
RuleT n Rule&apos;,
LET LET
</equation>
<bodyText confidence="0.598441">
Finally, the transducer for a forest F of trees is
just:
</bodyText>
<equation confidence="0.925695666666667">
T
Rule F = n Rule
TEF
</equation>
<sectionHeader confidence="0.9719605" genericHeader="method">
5 Empirical Verification of the
Method.
</sectionHeader>
<bodyText confidence="0.999966">
The algorithm just described has been empiri-
cally verified on the Resource Management (RM)
continuous speech recognition task (Price et al.,
1988). Following somewhat the discussion in
(Pereira et al., 1994; Pereira and Riley, 1996),
we can represent the speech recognition task as
the problem of finding the best path in the com-
position of a grammar (language model) G, the
transitive-closure of a dictionary D mapping be,
tween words and their phonemic representation,
a model of phone realization (I., and a weighted
lattice representing the acoustic observations A.
</bodyText>
<page confidence="0.991778">
219
</page>
<equation confidence="0.590925666666667">
Thus:
BestPath(G o D* o 4o A) (1)
The transducer (I) = nTE..
</equation>
<bodyText confidence="0.999700382352942">
Rule T can be con-
structed out of the forest F of 40 trees, one for
each phoneme, trained on the TIMIT database.
The size of the trees range from 1 to 23 leaf nodes,
with a total of 291 leaves for the entire forest.
The model was tested on 300 sentences from
the RM task containing 2560 word tokens, and
approximately 10,500 phonemes. A version of the
model of recognition given in expression (1), where
(I) is a transducer computed from the trees, was
compared with a version where the trees were used
directly following a method described in (Ljolje
and Riley, 1992). The phonetic realizations and
their weights were identical for both methods, thus
verifying the correctness of the compilation algo-
rithm described here.
The sizes of the compiled transducers can be
quite large; in fact they were sufficiently large that
instead of constructing (I) beforehand, we inter-
sected the 40 individual transducers with the lat-
tice D* at runtime. Table 4 gives sizes for the
entire set of phone trees: tree sizes are listed in
terms of number of rules (terminal nodes) and raw
size in bytes; transducer sizes are listed in terms
of number of states and arcs. Note that the entire
alphabet comprises 215 symbol pairs. Also given
in Table 4 are the compilation times for the indi-
vidual trees on a Silicon Graphics R4400 machine
running at 150 MHz with 1024 Mbytes of memory.
The times are somewhat slow for the larger trees,
but still acceptable for off-line compilation.
While the sizes of the resulting transducers
seem at first glance to be unfavorable, it is im-
portant to bear in mind that size is not the only
consideration in deciding upon a particular repre-
sentation. WFSTs possess several nice properties
that are not shared by trees, or handwritten rule-
sets for that matter. In particular, once compiled
into a WFST, a tree can be used in the same way
as a WFST derived from any other source, such as
a lexicon or a language model; a compiled WFST
can be used directly in a speech recognition model
such as that of (Pereira and Riley, 1996) or in a
speech synthesis text-analysis model such as that
of (Sproat, 1996). Use of a tree directly requires
a special-purpose interpreter, which is much less
flexible.
It should also be borne in mind that the size
explosion evident in Table 4 also characterizes
rules that are compiled from hand-built rewrite
rules (Kaplan and Kay, 1994; Mohri and Sproat,
1996). For example, the text-analysis ruleset for
the Bell Labs German text-to-speech (TTS) sys-
tem (see (Sproat, 1996; Mohri and Sproat, 1996))
contains sets of rules for the pronunciation of var-
ious orthographic symbols. The ruleset for &lt;a&gt;,
for example, contains 25 ordered rewrite rules.
Over an alphabet of 194 symbols, this compiles,
using the algorithm of (Mohri and Sproat, 1996),
into a transducer containing 213,408 arcs and
1,927 states. This is 72% as many arcs and 48%
as many states as the transducer for /ah/ in Ta-
ble 4. The size explosion is not quite as great here,
but the resulting transducer is still large compared
to the original rule file, which only requires 1428
bytes of storage. Again, the advantages of rep-
resenting the rules as a transducer outweigh the
problems of size.&apos;
</bodyText>
<sectionHeader confidence="0.995746" genericHeader="discussions">
6 Future Applications
</sectionHeader>
<bodyText confidence="0.999925684210526">
We have presented a practical algorithm for con-
verting decision trees inferred from data into
weighted finite-state transducers that directly im-
plement the models implicit in the trees, and we
have empirically verified that the algorithm is cor-
rect.
Several interesting areas of application come
to mind. In addition to speech recognition, where
we hope to apply the phonetic realization models
described above to the much larger North Amer-
ican Business task (Paul and Baker, 1992), there
are also applications to TTS where, for example,
the decision trees for prosodic phrase-boundary
prediction discussed in (Wang and Hirschberg,
1992) can be compiled into transducers and used
directly in the WFST-based model of text analysis
used in the multi-lingual version of the Bell Lab-
oratories TTS system, described in (Sproat, 1995;
Sproat, 1996).
</bodyText>
<sectionHeader confidence="0.998962" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999839">
The authors wish to thank Fernando Pereira,
Mehryar Mohri and two anonymous referees for
useful comments.
</bodyText>
<sectionHeader confidence="0.984238" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.882416111111111">
Leo Breiman, Jerome H. Friedman, Richard A.
Olshen, and Charles J. Stone. 1984. Clas-
5 Having said this, we note that obviously one would
like to decrease the size of the resulting transducers if
that is possible. We are currently investigating ways
to avoid precompiling the transducers beforehand, but
rather to construct &apos;on the fly&apos;, only those portions
of the transducers that are necessary for a particular
intersection.
</bodyText>
<page confidence="0.991005">
220
</page>
<table confidence="0.990137512195122">
ARPABET phone # nodes size of tree (bytes) # arcs # states time (sec)
zh 1 47 215 1 0.3
jh 2 146 675 6 0.8
aw 2 149 1,720 8 1
f 2 119 669 6 0.9
ng 2 150 645 3 0.8
oy 2 159 1,720 8 1
uh 2 126 645 3 0.9
P 3 252 6,426 90 4
ay 3 228 4,467 38 2
m 3 257 2,711 27 1
ow 3 236 3,010 14 3
sh 3 230 694 8 1
v 3 230 685 8 1
b 4 354 3,978 33 2
eh 4 353 3,010 25 4
th 4 373 1,351 11 2
dh 5 496 1,290 6 3
ey 5 480 11,510 96 27
g 6 427 372,339 3,000 21
k 6 500 6,013 85 9
aa 6 693 18,441 106 15
ah 7 855 40,135 273 110
Y 7 712 9,245 43 12
ao 8 1,099 85,439 841 21
eh 8 960 16,731 167 13
er 8 894 101,765 821 31
w 8 680 118,154 1,147 51
hh 9 968 17,459 160 10
1 9 947 320,266 3,152 31
uw 9 1,318 44,868 552 28
z 9 1,045 1,987 33 5
s 10 1,060 175,901 2,032 25
ae 11 1,598 582,445 4,152 231
iy 11 1,196 695,255 9,625 103
d 12 1,414 36,067 389 38
n 16 1,899 518,066 3,827 256
r 16 1,901 131,903 680 69
ih 17 2,748 108,970 669 71
t 22 2,990 1,542,612 8,382 628
ax 23 4,281 295,954 3,966 77
</table>
<tableCaption confidence="0.999399">
Table 4: Sizes of transducers corresponding to each of the individual phone trees.
</tableCaption>
<page confidence="0.996595">
221
</page>
<reference confidence="0.998764876404494">
sification and Regression Trees. Wadsworth
&amp; Brooks, Pacific Grove CA.
William Fisher, Victor Zue, D. Bernstein, and
David Pallet. 1987. An acoustic-phonetic
data base. Journal of the Acoustical Society
of America, 91, December. Supplement 1.
Daniel Gildea and Daniel Jurafsky. 1995. Au-
tomatic induction of finite state transducers
for simple phonological rules. In 33rd Annual
Meeting of the Association for Computational
Linguistics, pages 9-15, Morristown, NJ. As-
sociation for Computational Linguistics.
C. Douglas Johnson. 1972. Formal Aspects of
Phonological Description. Mouton, Mouton,
The Hague.
Ronald Kaplan and Martin Kay. 1994. Regular
models of phonological rule systems. Compu-
tational Linguistics, 20:331-378.
Kimmo Koskenniemi. 1983. Two-Level Mor-
phology: a General Computational Model
for Word-Form Recognition and Production.
Ph.D. thesis, University of Helsinki, Helsinki.
Andrej Ljolje and Michael D. Riley. 1992. Op-
timal speech recognition using phone recogni-
tion and lexical access. In Proceedings of IC-
SLP, pages 313-316, Banff, Canada, October.
David Magerman. 1995. Statistical decision-tree
models for parsing. In 33rd Annual Meeting
of the Association for Computational Linguis-
tics, pages 276-283, Morristown, NJ. Associ-
ation for Computational Linguistics.
Mehryar Mohri and Richard Sproat. 1996. An ef-
ficient compiler for weighted rewrite rules. In
34rd Annual Meeting of the Association for
Computational Linguistics, Morristown, NJ.
Association for Computational Linguistics.
Jose Oncina, Pedro Garcia, and Enrique Vidal.
1993. Learning subsequential transducers for
pattern recognition tasks. IEEE Transactions
on Pattern Analysis and Machine Intelligence,
15:448-458.
Douglas Paul and Janet Baker. 1992. The design
for the Wall Street Journal-based CSR corpus.
In Proceedings of International Conference on
Spoken Language Processing, Banff, Alberta.
ICSLP.
Fernando Pereira and Michael Riley. 1996. Speech
recognition by composition of weighted finite
automata. CMP-LG archive paper 9603001.
Fernando Pereira, Michael Riley, and Richard
Sproat. 1994. Weighted rational transduc-
tions and their application to human lan-
guage processing. In ARPA Workshop on
Human Language Technology, pages 249-254.
Advanced Research Projects Agency, March
8-11.
Patty Price, William Fisher, Jared Bernstein, and
David Pallett. 1988. The DARPA 1000-word
Resource Management Database for contin-
uous speech recognition. In Proceedings of
ICASSP88, volume 1, pages 651-654, New
York. ICASSP.
Michael Riley. 1989. Some applications of tree-
based modelling to speech and language. In
Proceedings of the Speech and Natural Lan-
guage Workshop, pages 339-352, Cape Cod
MA, October. DARPA, Morgan Kaufmann.
Michael Riley. 1991. A statistical model for gener-
ating pronunciation networks. In Proceedings
of the International Conference on Acoustics,
Speech, and Signal Processing, pages S11.1.—
S11.4. ICASSP91, October.
Richard Sproat. 1995. A finite-state architecture
for tokenization and grapheme-to-phoneme
conversion for multilingual text analysis. In
Susan Armstrong and Evelyne Tzoukermann,
editors, Proceedings of the EACL SIGDAT
Workshop, pages 65-72, Dublin, Ireland. As-
sociation for Computational Linguistics.
Richard Sproat. 1996. Multilingual text analy-
sis for text-to-speech synthesis. In Proceed-
ings of the ECAI-96 Workshop on Extended
Finite State Models of Language, Budapest,
Hungary. European Conference on Artificial
Intelligence.
Michelle Wang and Julia Hirschberg. 1992. Au-
tomatic classification of intonational phrase
boundaries. Computer Speech and Language,
6:175-196.
</reference>
<page confidence="0.997985">
222
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.907092">
<title confidence="0.9957245">Compilation of Weighted Finite-State Transducers from Decision Trees</title>
<author confidence="0.999993">Richard Sproat</author>
<affiliation confidence="0.999741">Bell Laboratories</affiliation>
<address confidence="0.999751">700 Mountain Avenue Murray Hill, NJ, USA</address>
<email confidence="0.999897">rwsObell—labs.com</email>
<abstract confidence="0.99776625">We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in</abstract>
<note confidence="0.950208">(Mohri and Sproat, 1996).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>sification and Regression Trees.</title>
<publisher>Wadsworth &amp; Brooks,</publisher>
<location>Pacific Grove CA.</location>
<marker></marker>
<rawString>sification and Regression Trees. Wadsworth &amp; Brooks, Pacific Grove CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Fisher</author>
<author>Victor Zue</author>
<author>D Bernstein</author>
<author>David Pallet</author>
</authors>
<title>An acoustic-phonetic data base.</title>
<date>1987</date>
<journal>Journal of the Acoustical Society of America,</journal>
<volume>91</volume>
<contexts>
<context position="3095" citStr="Fisher et al., 1987" startWordPosition="472" endWordPosition="475">ee-Based Modeling A general introduction to classification and regression trees (&apos;CART&apos;) including the algorithm for growing trees from data can be found in (Breiman et al., 1984). Applications of tree-based modeling to problems in speech and NLP are discussed in (Riley, 1989; Riley, 1991; Wang and Hirschberg, 1992; Magerman, 1995, inter alia). In this section we presume that one has already trained a tree or set of trees, and we merely remind the reader of the salient points in the interpretation of those trees. Consider the tree depicted in Figure 1, which was trained on the TIMIT database (Fisher et al., 1987), and which models the phonetic realization of the English phoneme /aa/ (/a/) in various environments (Riley, 1991). When this tree is used in predicting the allophonic form of a particular instance of /aa/, one starts at the root of the tree, and asks questions about the environment in which the /aa/ is found. Each non-leaf node n, dominates two daughter nodes conventionally labeled as 2n and 2n + 1; the decision on whether to go left to 2n or right to 2n + 1 depends on the answer to the question that is being asked at node n. 1The work reported here can thus be seen as complementary to recen</context>
</contexts>
<marker>Fisher, Zue, Bernstein, Pallet, 1987</marker>
<rawString>William Fisher, Victor Zue, D. Bernstein, and David Pallet. 1987. An acoustic-phonetic data base. Journal of the Acoustical Society of America, 91, December. Supplement 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic induction of finite state transducers for simple phonological rules.</title>
<date>1995</date>
<booktitle>In 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>9--15</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ.</location>
<contexts>
<context position="3809" citStr="Gildea and Jurafsky, 1995" startWordPosition="603" endWordPosition="606"> environments (Riley, 1991). When this tree is used in predicting the allophonic form of a particular instance of /aa/, one starts at the root of the tree, and asks questions about the environment in which the /aa/ is found. Each non-leaf node n, dominates two daughter nodes conventionally labeled as 2n and 2n + 1; the decision on whether to go left to 2n or right to 2n + 1 depends on the answer to the question that is being asked at node n. 1The work reported here can thus be seen as complementary to recent reports on methods for directly inferring transducers from data (Oncina et al., 1993; Gildea and Jurafsky, 1995). 215 A concrete example will serve to illustrate. Consider that we have /aa/ in some environment. The first question that is asked concerns the number of segments, including the /aa/ itself, that occur to the left of the /aa/ in the word in which /aa/ occurs. (See Table 1 for an explanation of the symbols used in Figure 1.) In this case, if the /aa/ is initial — i.e., lseg is 1, one goes left; if there is one or more segments to the left in the word, go right. Let us assume that this /aa/ is initial in the word, in which case we go left. The next question concerns the consonantal &apos;place&apos; of a</context>
</contexts>
<marker>Gildea, Jurafsky, 1995</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 1995. Automatic induction of finite state transducers for simple phonological rules. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 9-15, Morristown, NJ. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Douglas Johnson</author>
</authors>
<title>Formal Aspects of Phonological Description.</title>
<date>1972</date>
<location>Mouton, Mouton, The Hague.</location>
<contexts>
<context position="6007" citStr="Johnson, 1972" startWordPosition="1018" endWordPosition="1019">, there will be some leaf node that corresponds to it. So for the tree in Figure 1, each new novel instance of /aa/ will be handled by (exactly) one leaf node in the tree, depending upon the environment in which the /aa/ finds itself. Another important point is that each decision tree considered here has the property that its predictions specify how to rewrite a symbol (in context) in an input string. In particular, they specify a two-level mapping from a set of input symbols (phonemes) to a set of output symbols (allophones). 3 Quick Review of Rule Compilation Work on finite-state phonology (Johnson, 1972; Koskenniemi, 1983; Kaplan and Kay, 1994) has shown that systems of rewrite rules of the familiar form 0 —+ O/A p, where 0, 0, A and p are regular expressions, can be represented computationally as finite-state transducers (FSTs): note that 0 represents the rule&apos;s input rule, V) the output, and A and p, respectively, the left and right contexts. Kaplan and Kay (1994) have presented a concrete algorithm for compiling systems of such rules into FSTs. These methods can be extended slightly to include the compilation of probabilistic or weighted rules into weighted finitestate-transducers (WFSTs </context>
</contexts>
<marker>Johnson, 1972</marker>
<rawString>C. Douglas Johnson. 1972. Formal Aspects of Phonological Description. Mouton, Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--331</pages>
<contexts>
<context position="1282" citStr="Kaplan and Kay, 1994" startWordPosition="189" endWordPosition="192">iled into transducers using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996). 1 Introduction Much attention has been devoted recently to methods for inferring linguistic models from data. One powerful inference method that has been used in various applications are decision trees, and in particular classification and regression trees (Breiman et al., 1984). An increasing amount of attention has also been focussed on finite-state methods for implementing linguistic models, in particular finitestate transducers and weighted finite-state transducers; see (Kaplan and Kay, 1994; Pereira et al., 1994, inter alia). The reason for the renewed interest in finite-state mechanisms is clear. Finitestate machines provide a mathematically wellunderstood computational framework for representing a wide variety of information, both in NLP and speech processing. Lexicons, phonological rules, Hidden Markov Models, and (regular) grammars are all representable as finite-state machines, and finite-state operations such as union, intersection and composition mean that information from these various sources can be combined in useful Michael Riley AT&amp;T Research 600 Mountain Avenue Murr</context>
<context position="6049" citStr="Kaplan and Kay, 1994" startWordPosition="1022" endWordPosition="1025">t corresponds to it. So for the tree in Figure 1, each new novel instance of /aa/ will be handled by (exactly) one leaf node in the tree, depending upon the environment in which the /aa/ finds itself. Another important point is that each decision tree considered here has the property that its predictions specify how to rewrite a symbol (in context) in an input string. In particular, they specify a two-level mapping from a set of input symbols (phonemes) to a set of output symbols (allophones). 3 Quick Review of Rule Compilation Work on finite-state phonology (Johnson, 1972; Koskenniemi, 1983; Kaplan and Kay, 1994) has shown that systems of rewrite rules of the familiar form 0 —+ O/A p, where 0, 0, A and p are regular expressions, can be represented computationally as finite-state transducers (FSTs): note that 0 represents the rule&apos;s input rule, V) the output, and A and p, respectively, the left and right contexts. Kaplan and Kay (1994) have presented a concrete algorithm for compiling systems of such rules into FSTs. These methods can be extended slightly to include the compilation of probabilistic or weighted rules into weighted finitestate-transducers (WFSTs — see (Pereira et al., 1994)); Mohri and S</context>
<context position="12086" citStr="Kaplan and Kay (1994)" startWordPosition="2078" endWordPosition="2081">ontexts defined for each branch traversed on the way to the leaf. For leaf node 4, A = #Opt(&apos;) n E* = #Opt(&apos;), and p = *fl Opt(&apos;)(alv) = Opt(1)(alv).3 The rule input 0 has already been given as aa. The output 1,1) is defined as the union of all of the possible expressions — at the leaf node in question — that aa could become, with their associated weights (negative log probabilities), which we represent here as subscripted floating-point numbers: = ao0.95 U aa1.24 U q+aa2.27 U q+a02.34U ah2.68 U aX2.84 Thus the entire weighted rule can be written as 2As far as possible, we use the notation of Kaplan and Kay (1994). 3Strictly speaking, since the As and ps at each branch may define expressions of different lengths, it is necessary to left-pad each A with E*, and right-pad each p with E. We gloss over this point here in order to make the regular expressions somewhat simpler to understand follows: aa (ao0.9sUaai.24Uq-4-aa2.27Uq±a02.34Uah2.68U ax2.84)1#0pt0 Opt(&apos;)(alv) By a similar construction, the rule at node 6, for example, would be represented as: aa (aa0,40 U aoLii)/ (#(0pt(1)a)+Opt(&apos;)) fl (E*((crnh) U (14) U (bml) U (bh))) E* Each node thus represents a rule which states that a mapping occurs between</context>
<context position="14055" citStr="Kaplan and Kay (1994)" startWordPosition="2429" endWordPosition="2432">of the entire set of transducers defined for the leaves of the tree. Observe now that the 0:0 correspondences that were left free by the rule of one leaf node, are constrained by intersection with the other leaf nodes: since, as noted above, the tree is a complete description, it follows that for any leaf node i, and for any context A p not subsumed by Ai pi, there is some leaf node j such that Ai pj subsumes A_ p. Thus, the transducers compiled for the rules at nodes 4 and 6, are intersected together, along with the rules for all the other leaf nodes. Now, as noted above, and as discussed by Kaplan and Kay (1994) regular relations — the algebraic counterpart of FSTs — are not in general closed under intersection; however, the subset of same-length regular relations is closed under intersection, since they can be thought of as finite-state acceptors ex218 (a) left branch A = #Opt(&apos;) P = E* right branch A = (#0ptnaOpt0)U (#0ptnaOptnaOpt(9)U (#0 ptnaOptnceOpt(&apos;)(a0pt(9)+) = P = E* (b) left branch A = E* p = Opt(1)(alv) right branch A = E* p = (0 pt(&apos;)(blab)) U (Opt(1)(labd)) U (Opt(&apos;)(den)) U (Opt(&apos;)(pal))U (Opt(1)(vel)) U (Opt(&apos;)(pha)) U (Opt(1)(n/a)) Table 3: Regular-expression interpretation of the de</context>
<context position="19909" citStr="Kaplan and Kay, 1994" startWordPosition="3451" endWordPosition="3454">or that matter. In particular, once compiled into a WFST, a tree can be used in the same way as a WFST derived from any other source, such as a lexicon or a language model; a compiled WFST can be used directly in a speech recognition model such as that of (Pereira and Riley, 1996) or in a speech synthesis text-analysis model such as that of (Sproat, 1996). Use of a tree directly requires a special-purpose interpreter, which is much less flexible. It should also be borne in mind that the size explosion evident in Table 4 also characterizes rules that are compiled from hand-built rewrite rules (Kaplan and Kay, 1994; Mohri and Sproat, 1996). For example, the text-analysis ruleset for the Bell Labs German text-to-speech (TTS) system (see (Sproat, 1996; Mohri and Sproat, 1996)) contains sets of rules for the pronunciation of various orthographic symbols. The ruleset for &lt;a&gt;, for example, contains 25 ordered rewrite rules. Over an alphabet of 194 symbols, this compiles, using the algorithm of (Mohri and Sproat, 1996), into a transducer containing 213,408 arcs and 1,927 states. This is 72% as many arcs and 48% as many states as the transducer for /ah/ in Table 4. The size explosion is not quite as great here</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Ronald Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20:331-378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-Level Morphology: a General Computational Model for Word-Form Recognition and Production.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki,</institution>
<location>Helsinki.</location>
<contexts>
<context position="6026" citStr="Koskenniemi, 1983" startWordPosition="1020" endWordPosition="1021"> some leaf node that corresponds to it. So for the tree in Figure 1, each new novel instance of /aa/ will be handled by (exactly) one leaf node in the tree, depending upon the environment in which the /aa/ finds itself. Another important point is that each decision tree considered here has the property that its predictions specify how to rewrite a symbol (in context) in an input string. In particular, they specify a two-level mapping from a set of input symbols (phonemes) to a set of output symbols (allophones). 3 Quick Review of Rule Compilation Work on finite-state phonology (Johnson, 1972; Koskenniemi, 1983; Kaplan and Kay, 1994) has shown that systems of rewrite rules of the familiar form 0 —+ O/A p, where 0, 0, A and p are regular expressions, can be represented computationally as finite-state transducers (FSTs): note that 0 represents the rule&apos;s input rule, V) the output, and A and p, respectively, the left and right contexts. Kaplan and Kay (1994) have presented a concrete algorithm for compiling systems of such rules into FSTs. These methods can be extended slightly to include the compilation of probabilistic or weighted rules into weighted finitestate-transducers (WFSTs — see (Pereira et a</context>
<context position="12929" citStr="Koskenniemi, 1983" startWordPosition="2221" endWordPosition="2222"> regular expressions somewhat simpler to understand follows: aa (ao0.9sUaai.24Uq-4-aa2.27Uq±a02.34Uah2.68U ax2.84)1#0pt0 Opt(&apos;)(alv) By a similar construction, the rule at node 6, for example, would be represented as: aa (aa0,40 U aoLii)/ (#(0pt(1)a)+Opt(&apos;)) fl (E*((crnh) U (14) U (bml) U (bh))) E* Each node thus represents a rule which states that a mapping occurs between the input symbol 0 and the weighted expression 0 in the condition described by A p. Now, in cases where 0 finds itself in a context that is not subsumed by A p, the rule behaves exactly as a two-level surface coercion rule (Koskenniemi, 1983): it freely allows 0 to correspond to any 0 as specified by the alphabet of pairs. These 0:0 correspondences are, however, constrained by other rules derived from the tree, as we shall see directly. The interpretation of the full tree is that it represents the conjunction of all such mappings: for rules 1, 2 ... n, cb corresponds to 01 given condition Ai pi and 0 corresponds to 02 given condition A2 p2 ...and 0 corresponds to On given condition An_ pn. But this conjunction is simply the intersection of the entire set of transducers defined for the leaves of the tree. Observe now that the 0:0 c</context>
<context position="15066" citStr="Koskenniemi, 1983" startWordPosition="2593" endWordPosition="2594">E* p = Opt(1)(alv) right branch A = E* p = (0 pt(&apos;)(blab)) U (Opt(1)(labd)) U (Opt(&apos;)(den)) U (Opt(&apos;)(pal))U (Opt(1)(vel)) U (Opt(&apos;)(pha)) U (Opt(1)(n/a)) Table 3: Regular-expression interpretation of the decisions involved in going from the root node to leaf node 4 in the tree in Figure 1. Note that, as per convention, superscript `+&apos; denotes one or more instances of an expression. pressed over pairs of symbols.4 This point can be extended somewhat to include relations that involve bounded deletions or insertions: this is precisely the interpretation necessary for systems of two-level rules (Koskenniemi, 1983), where a single transducer expressing the entire system may be constructed via intersection of the transducers expressing the individual rules (Kaplan and Kay, 1994, pages 367-376). Indeed, our decision tree represents neither more nor less than a set of weighted two-level rules. Each of the symbols in the expressions for A and p actually represent (sets of) pairs of symbols: thus (zit), for example, represents all lexical alveolars paired with all of their possible surface realizations. And just as each tree represents a system of weighted two-level rules, so a set of trees — e.g., where eac</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. Two-Level Morphology: a General Computational Model for Word-Form Recognition and Production. Ph.D. thesis, University of Helsinki, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Ljolje</author>
<author>Michael D Riley</author>
</authors>
<title>Optimal speech recognition using phone recognition and lexical access.</title>
<date>1992</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<pages>313--316</pages>
<location>Banff, Canada,</location>
<contexts>
<context position="18098" citStr="Ljolje and Riley, 1992" startWordPosition="3138" endWordPosition="3141"> BestPath(G o D* o 4o A) (1) The transducer (I) = nTE.. Rule T can be constructed out of the forest F of 40 trees, one for each phoneme, trained on the TIMIT database. The size of the trees range from 1 to 23 leaf nodes, with a total of 291 leaves for the entire forest. The model was tested on 300 sentences from the RM task containing 2560 word tokens, and approximately 10,500 phonemes. A version of the model of recognition given in expression (1), where (I) is a transducer computed from the trees, was compared with a version where the trees were used directly following a method described in (Ljolje and Riley, 1992). The phonetic realizations and their weights were identical for both methods, thus verifying the correctness of the compilation algorithm described here. The sizes of the compiled transducers can be quite large; in fact they were sufficiently large that instead of constructing (I) beforehand, we intersected the 40 individual transducers with the lattice D* at runtime. Table 4 gives sizes for the entire set of phone trees: tree sizes are listed in terms of number of rules (terminal nodes) and raw size in bytes; transducer sizes are listed in terms of number of states and arcs. Note that the en</context>
</contexts>
<marker>Ljolje, Riley, 1992</marker>
<rawString>Andrej Ljolje and Michael D. Riley. 1992. Optimal speech recognition using phone recognition and lexical access. In Proceedings of ICSLP, pages 313-316, Banff, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ.</location>
<contexts>
<context position="2807" citStr="Magerman, 1995" startWordPosition="422" endWordPosition="423">trees into weighted finite-state transducers.&apos; Given this algorithm, information inferred from data and represented in a tree can be used directly in a system that represents other information, such as lexicons or grammars, in the form of finite-state machines. 2 Quick Review of Tree-Based Modeling A general introduction to classification and regression trees (&apos;CART&apos;) including the algorithm for growing trees from data can be found in (Breiman et al., 1984). Applications of tree-based modeling to problems in speech and NLP are discussed in (Riley, 1989; Riley, 1991; Wang and Hirschberg, 1992; Magerman, 1995, inter alia). In this section we presume that one has already trained a tree or set of trees, and we merely remind the reader of the salient points in the interpretation of those trees. Consider the tree depicted in Figure 1, which was trained on the TIMIT database (Fisher et al., 1987), and which models the phonetic realization of the English phoneme /aa/ (/a/) in various environments (Riley, 1991). When this tree is used in predicting the allophonic form of a particular instance of /aa/, one starts at the root of the tree, and asks questions about the environment in which the /aa/ is found.</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David Magerman. 1995. Statistical decision-tree models for parsing. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 276-283, Morristown, NJ. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Richard Sproat</author>
</authors>
<title>An efficient compiler for weighted rewrite rules.</title>
<date>1996</date>
<booktitle>In 34rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ.</location>
<contexts>
<context position="780" citStr="Mohri and Sproat, 1996" startWordPosition="116" endWordPosition="119">com Abstract We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996). 1 Introduction Much attention has been devoted recently to methods for inferring linguistic models from data. One powerful inference method that has been used in various applications are decision trees, and in particular classification and regression trees (Breiman et al., 1984). An increasing amount of attention has also been focussed on finite-state methods for implementing linguistic models, in particular finitestate transducers and weighted finite-state transducers; see (Kaplan and Kay, 1994; Pereira et al., 1994, inter alia). The reason for the renewed interest in finite-state mechanism</context>
<context position="6661" citStr="Mohri and Sproat (1996)" startWordPosition="1128" endWordPosition="1131"> Kay, 1994) has shown that systems of rewrite rules of the familiar form 0 —+ O/A p, where 0, 0, A and p are regular expressions, can be represented computationally as finite-state transducers (FSTs): note that 0 represents the rule&apos;s input rule, V) the output, and A and p, respectively, the left and right contexts. Kaplan and Kay (1994) have presented a concrete algorithm for compiling systems of such rules into FSTs. These methods can be extended slightly to include the compilation of probabilistic or weighted rules into weighted finitestate-transducers (WFSTs — see (Pereira et al., 1994)); Mohri and Sproat (1996) describe a rulecompilation algorithm which is more efficient than the Kaplan-Kay algorithm, and which has been extended to handle weighted rules. For present purposes it is sufficient to observe that given this extended algorithm, we can allow in the expression 0 —+ O/A. p, to represent a weighted regular expression. The compiled transducer corresponding to that rule will replace 0 with b with the appropriate weights in the context A p. 4 The Tree Compilation Algorithm The key requirements on the kind of decision trees that we can compile into WFSTs are (1) the predictions at the leaf nodes s</context>
<context position="19934" citStr="Mohri and Sproat, 1996" startWordPosition="3455" endWordPosition="3458">ticular, once compiled into a WFST, a tree can be used in the same way as a WFST derived from any other source, such as a lexicon or a language model; a compiled WFST can be used directly in a speech recognition model such as that of (Pereira and Riley, 1996) or in a speech synthesis text-analysis model such as that of (Sproat, 1996). Use of a tree directly requires a special-purpose interpreter, which is much less flexible. It should also be borne in mind that the size explosion evident in Table 4 also characterizes rules that are compiled from hand-built rewrite rules (Kaplan and Kay, 1994; Mohri and Sproat, 1996). For example, the text-analysis ruleset for the Bell Labs German text-to-speech (TTS) system (see (Sproat, 1996; Mohri and Sproat, 1996)) contains sets of rules for the pronunciation of various orthographic symbols. The ruleset for &lt;a&gt;, for example, contains 25 ordered rewrite rules. Over an alphabet of 194 symbols, this compiles, using the algorithm of (Mohri and Sproat, 1996), into a transducer containing 213,408 arcs and 1,927 states. This is 72% as many arcs and 48% as many states as the transducer for /ah/ in Table 4. The size explosion is not quite as great here, but the resulting trans</context>
</contexts>
<marker>Mohri, Sproat, 1996</marker>
<rawString>Mehryar Mohri and Richard Sproat. 1996. An efficient compiler for weighted rewrite rules. In 34rd Annual Meeting of the Association for Computational Linguistics, Morristown, NJ. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jose Oncina</author>
<author>Pedro Garcia</author>
<author>Enrique Vidal</author>
</authors>
<title>Learning subsequential transducers for pattern recognition tasks.</title>
<date>1993</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>15--448</pages>
<contexts>
<context position="3781" citStr="Oncina et al., 1993" startWordPosition="599" endWordPosition="602">/aa/ (/a/) in various environments (Riley, 1991). When this tree is used in predicting the allophonic form of a particular instance of /aa/, one starts at the root of the tree, and asks questions about the environment in which the /aa/ is found. Each non-leaf node n, dominates two daughter nodes conventionally labeled as 2n and 2n + 1; the decision on whether to go left to 2n or right to 2n + 1 depends on the answer to the question that is being asked at node n. 1The work reported here can thus be seen as complementary to recent reports on methods for directly inferring transducers from data (Oncina et al., 1993; Gildea and Jurafsky, 1995). 215 A concrete example will serve to illustrate. Consider that we have /aa/ in some environment. The first question that is asked concerns the number of segments, including the /aa/ itself, that occur to the left of the /aa/ in the word in which /aa/ occurs. (See Table 1 for an explanation of the symbols used in Figure 1.) In this case, if the /aa/ is initial — i.e., lseg is 1, one goes left; if there is one or more segments to the left in the word, go right. Let us assume that this /aa/ is initial in the word, in which case we go left. The next question concerns </context>
</contexts>
<marker>Oncina, Garcia, Vidal, 1993</marker>
<rawString>Jose Oncina, Pedro Garcia, and Enrique Vidal. 1993. Learning subsequential transducers for pattern recognition tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15:448-458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Paul</author>
<author>Janet Baker</author>
</authors>
<title>The design for the Wall Street Journal-based CSR corpus.</title>
<date>1992</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<location>Banff, Alberta. ICSLP.</location>
<contexts>
<context position="21233" citStr="Paul and Baker, 1992" startWordPosition="3668" endWordPosition="3671">428 bytes of storage. Again, the advantages of representing the rules as a transducer outweigh the problems of size.&apos; 6 Future Applications We have presented a practical algorithm for converting decision trees inferred from data into weighted finite-state transducers that directly implement the models implicit in the trees, and we have empirically verified that the algorithm is correct. Several interesting areas of application come to mind. In addition to speech recognition, where we hope to apply the phonetic realization models described above to the much larger North American Business task (Paul and Baker, 1992), there are also applications to TTS where, for example, the decision trees for prosodic phrase-boundary prediction discussed in (Wang and Hirschberg, 1992) can be compiled into transducers and used directly in the WFST-based model of text analysis used in the multi-lingual version of the Bell Laboratories TTS system, described in (Sproat, 1995; Sproat, 1996). 7 Acknowledgments The authors wish to thank Fernando Pereira, Mehryar Mohri and two anonymous referees for useful comments. References Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. 1984. Clas5 Having said this</context>
</contexts>
<marker>Paul, Baker, 1992</marker>
<rawString>Douglas Paul and Janet Baker. 1992. The design for the Wall Street Journal-based CSR corpus. In Proceedings of International Conference on Spoken Language Processing, Banff, Alberta. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Speech recognition by composition of weighted finite automata. CMP-LG archive paper 9603001.</title>
<date>1996</date>
<contexts>
<context position="17128" citStr="Pereira and Riley, 1996" startWordPosition="2966" endWordPosition="2969"> = (q, q) if and only if bi (qi , (i, o)) = q; and 62 (q2 , (i, o)) = q;. that path, and Ap and pp are the expressions for A and p defined at p: PP) A Rule&apos;, = Compile(OT -- Oa n P— fl pEPL pePL. The transducer for an entire tree T is defined as: &apos;, RuleT n Rule&apos;, LET LET Finally, the transducer for a forest F of trees is just: T Rule F = n Rule TEF 5 Empirical Verification of the Method. The algorithm just described has been empirically verified on the Resource Management (RM) continuous speech recognition task (Price et al., 1988). Following somewhat the discussion in (Pereira et al., 1994; Pereira and Riley, 1996), we can represent the speech recognition task as the problem of finding the best path in the composition of a grammar (language model) G, the transitive-closure of a dictionary D mapping be, tween words and their phonemic representation, a model of phone realization (I., and a weighted lattice representing the acoustic observations A. 219 Thus: BestPath(G o D* o 4o A) (1) The transducer (I) = nTE.. Rule T can be constructed out of the forest F of 40 trees, one for each phoneme, trained on the TIMIT database. The size of the trees range from 1 to 23 leaf nodes, with a total of 291 leaves for t</context>
<context position="19570" citStr="Pereira and Riley, 1996" startWordPosition="3395" endWordPosition="3398">but still acceptable for off-line compilation. While the sizes of the resulting transducers seem at first glance to be unfavorable, it is important to bear in mind that size is not the only consideration in deciding upon a particular representation. WFSTs possess several nice properties that are not shared by trees, or handwritten rulesets for that matter. In particular, once compiled into a WFST, a tree can be used in the same way as a WFST derived from any other source, such as a lexicon or a language model; a compiled WFST can be used directly in a speech recognition model such as that of (Pereira and Riley, 1996) or in a speech synthesis text-analysis model such as that of (Sproat, 1996). Use of a tree directly requires a special-purpose interpreter, which is much less flexible. It should also be borne in mind that the size explosion evident in Table 4 also characterizes rules that are compiled from hand-built rewrite rules (Kaplan and Kay, 1994; Mohri and Sproat, 1996). For example, the text-analysis ruleset for the Bell Labs German text-to-speech (TTS) system (see (Sproat, 1996; Mohri and Sproat, 1996)) contains sets of rules for the pronunciation of various orthographic symbols. The ruleset for &lt;a&gt;</context>
</contexts>
<marker>Pereira, Riley, 1996</marker>
<rawString>Fernando Pereira and Michael Riley. 1996. Speech recognition by composition of weighted finite automata. CMP-LG archive paper 9603001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
<author>Richard Sproat</author>
</authors>
<title>Weighted rational transductions and their application to human language processing.</title>
<date>1994</date>
<booktitle>In ARPA Workshop on Human Language Technology,</booktitle>
<pages>249--254</pages>
<contexts>
<context position="1304" citStr="Pereira et al., 1994" startWordPosition="193" endWordPosition="196">using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996). 1 Introduction Much attention has been devoted recently to methods for inferring linguistic models from data. One powerful inference method that has been used in various applications are decision trees, and in particular classification and regression trees (Breiman et al., 1984). An increasing amount of attention has also been focussed on finite-state methods for implementing linguistic models, in particular finitestate transducers and weighted finite-state transducers; see (Kaplan and Kay, 1994; Pereira et al., 1994, inter alia). The reason for the renewed interest in finite-state mechanisms is clear. Finitestate machines provide a mathematically wellunderstood computational framework for representing a wide variety of information, both in NLP and speech processing. Lexicons, phonological rules, Hidden Markov Models, and (regular) grammars are all representable as finite-state machines, and finite-state operations such as union, intersection and composition mean that information from these various sources can be combined in useful Michael Riley AT&amp;T Research 600 Mountain Avenue Murray Hill, NJ, USA riley</context>
<context position="6635" citStr="Pereira et al., 1994" startWordPosition="1124" endWordPosition="1127">nniemi, 1983; Kaplan and Kay, 1994) has shown that systems of rewrite rules of the familiar form 0 —+ O/A p, where 0, 0, A and p are regular expressions, can be represented computationally as finite-state transducers (FSTs): note that 0 represents the rule&apos;s input rule, V) the output, and A and p, respectively, the left and right contexts. Kaplan and Kay (1994) have presented a concrete algorithm for compiling systems of such rules into FSTs. These methods can be extended slightly to include the compilation of probabilistic or weighted rules into weighted finitestate-transducers (WFSTs — see (Pereira et al., 1994)); Mohri and Sproat (1996) describe a rulecompilation algorithm which is more efficient than the Kaplan-Kay algorithm, and which has been extended to handle weighted rules. For present purposes it is sufficient to observe that given this extended algorithm, we can allow in the expression 0 —+ O/A. p, to represent a weighted regular expression. The compiled transducer corresponding to that rule will replace 0 with b with the appropriate weights in the context A p. 4 The Tree Compilation Algorithm The key requirements on the kind of decision trees that we can compile into WFSTs are (1) the predi</context>
<context position="17102" citStr="Pereira et al., 1994" startWordPosition="2962" endWordPosition="2965">, 6((qi , q2), (i, o)) = (q, q) if and only if bi (qi , (i, o)) = q; and 62 (q2 , (i, o)) = q;. that path, and Ap and pp are the expressions for A and p defined at p: PP) A Rule&apos;, = Compile(OT -- Oa n P— fl pEPL pePL. The transducer for an entire tree T is defined as: &apos;, RuleT n Rule&apos;, LET LET Finally, the transducer for a forest F of trees is just: T Rule F = n Rule TEF 5 Empirical Verification of the Method. The algorithm just described has been empirically verified on the Resource Management (RM) continuous speech recognition task (Price et al., 1988). Following somewhat the discussion in (Pereira et al., 1994; Pereira and Riley, 1996), we can represent the speech recognition task as the problem of finding the best path in the composition of a grammar (language model) G, the transitive-closure of a dictionary D mapping be, tween words and their phonemic representation, a model of phone realization (I., and a weighted lattice representing the acoustic observations A. 219 Thus: BestPath(G o D* o 4o A) (1) The transducer (I) = nTE.. Rule T can be constructed out of the forest F of 40 trees, one for each phoneme, trained on the TIMIT database. The size of the trees range from 1 to 23 leaf nodes, with a</context>
</contexts>
<marker>Pereira, Riley, Sproat, 1994</marker>
<rawString>Fernando Pereira, Michael Riley, and Richard Sproat. 1994. Weighted rational transductions and their application to human language processing. In ARPA Workshop on Human Language Technology, pages 249-254.</rawString>
</citation>
<citation valid="true">
<title>Advanced Research Projects Agency,</title>
<date></date>
<marker></marker>
<rawString>Advanced Research Projects Agency, March 8-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patty Price</author>
<author>William Fisher</author>
<author>Jared Bernstein</author>
<author>David Pallett</author>
</authors>
<title>The DARPA 1000-word Resource Management Database for continuous speech recognition.</title>
<date>1988</date>
<booktitle>In Proceedings of ICASSP88,</booktitle>
<volume>1</volume>
<pages>651--654</pages>
<location>New York. ICASSP.</location>
<contexts>
<context position="17042" citStr="Price et al., 1988" startWordPosition="2953" endWordPosition="2956">nction of G, 6, as follows: for an input-output pair (i, o), 6((qi , q2), (i, o)) = (q, q) if and only if bi (qi , (i, o)) = q; and 62 (q2 , (i, o)) = q;. that path, and Ap and pp are the expressions for A and p defined at p: PP) A Rule&apos;, = Compile(OT -- Oa n P— fl pEPL pePL. The transducer for an entire tree T is defined as: &apos;, RuleT n Rule&apos;, LET LET Finally, the transducer for a forest F of trees is just: T Rule F = n Rule TEF 5 Empirical Verification of the Method. The algorithm just described has been empirically verified on the Resource Management (RM) continuous speech recognition task (Price et al., 1988). Following somewhat the discussion in (Pereira et al., 1994; Pereira and Riley, 1996), we can represent the speech recognition task as the problem of finding the best path in the composition of a grammar (language model) G, the transitive-closure of a dictionary D mapping be, tween words and their phonemic representation, a model of phone realization (I., and a weighted lattice representing the acoustic observations A. 219 Thus: BestPath(G o D* o 4o A) (1) The transducer (I) = nTE.. Rule T can be constructed out of the forest F of 40 trees, one for each phoneme, trained on the TIMIT database.</context>
</contexts>
<marker>Price, Fisher, Bernstein, Pallett, 1988</marker>
<rawString>Patty Price, William Fisher, Jared Bernstein, and David Pallett. 1988. The DARPA 1000-word Resource Management Database for continuous speech recognition. In Proceedings of ICASSP88, volume 1, pages 651-654, New York. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Riley</author>
</authors>
<title>Some applications of treebased modelling to speech and language.</title>
<date>1989</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>339--352</pages>
<publisher>DARPA, Morgan Kaufmann.</publisher>
<location>Cape Cod MA,</location>
<contexts>
<context position="2751" citStr="Riley, 1989" startWordPosition="414" endWordPosition="415"> algorithm for compiling the information in decision trees into weighted finite-state transducers.&apos; Given this algorithm, information inferred from data and represented in a tree can be used directly in a system that represents other information, such as lexicons or grammars, in the form of finite-state machines. 2 Quick Review of Tree-Based Modeling A general introduction to classification and regression trees (&apos;CART&apos;) including the algorithm for growing trees from data can be found in (Breiman et al., 1984). Applications of tree-based modeling to problems in speech and NLP are discussed in (Riley, 1989; Riley, 1991; Wang and Hirschberg, 1992; Magerman, 1995, inter alia). In this section we presume that one has already trained a tree or set of trees, and we merely remind the reader of the salient points in the interpretation of those trees. Consider the tree depicted in Figure 1, which was trained on the TIMIT database (Fisher et al., 1987), and which models the phonetic realization of the English phoneme /aa/ (/a/) in various environments (Riley, 1991). When this tree is used in predicting the allophonic form of a particular instance of /aa/, one starts at the root of the tree, and asks que</context>
</contexts>
<marker>Riley, 1989</marker>
<rawString>Michael Riley. 1989. Some applications of treebased modelling to speech and language. In Proceedings of the Speech and Natural Language Workshop, pages 339-352, Cape Cod MA, October. DARPA, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Riley</author>
</authors>
<title>A statistical model for generating pronunciation networks.</title>
<date>1991</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, pages S11.1.— S11.4. ICASSP91,</booktitle>
<contexts>
<context position="2764" citStr="Riley, 1991" startWordPosition="416" endWordPosition="417">r compiling the information in decision trees into weighted finite-state transducers.&apos; Given this algorithm, information inferred from data and represented in a tree can be used directly in a system that represents other information, such as lexicons or grammars, in the form of finite-state machines. 2 Quick Review of Tree-Based Modeling A general introduction to classification and regression trees (&apos;CART&apos;) including the algorithm for growing trees from data can be found in (Breiman et al., 1984). Applications of tree-based modeling to problems in speech and NLP are discussed in (Riley, 1989; Riley, 1991; Wang and Hirschberg, 1992; Magerman, 1995, inter alia). In this section we presume that one has already trained a tree or set of trees, and we merely remind the reader of the salient points in the interpretation of those trees. Consider the tree depicted in Figure 1, which was trained on the TIMIT database (Fisher et al., 1987), and which models the phonetic realization of the English phoneme /aa/ (/a/) in various environments (Riley, 1991). When this tree is used in predicting the allophonic form of a particular instance of /aa/, one starts at the root of the tree, and asks questions about </context>
</contexts>
<marker>Riley, 1991</marker>
<rawString>Michael Riley. 1991. A statistical model for generating pronunciation networks. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, pages S11.1.— S11.4. ICASSP91, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
</authors>
<title>A finite-state architecture for tokenization and grapheme-to-phoneme conversion for multilingual text analysis.</title>
<date>1995</date>
<booktitle>In Susan Armstrong and Evelyne Tzoukermann, editors, Proceedings of the EACL SIGDAT Workshop,</booktitle>
<pages>65--72</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Dublin, Ireland.</location>
<contexts>
<context position="21579" citStr="Sproat, 1995" startWordPosition="3723" endWordPosition="3724">ied that the algorithm is correct. Several interesting areas of application come to mind. In addition to speech recognition, where we hope to apply the phonetic realization models described above to the much larger North American Business task (Paul and Baker, 1992), there are also applications to TTS where, for example, the decision trees for prosodic phrase-boundary prediction discussed in (Wang and Hirschberg, 1992) can be compiled into transducers and used directly in the WFST-based model of text analysis used in the multi-lingual version of the Bell Laboratories TTS system, described in (Sproat, 1995; Sproat, 1996). 7 Acknowledgments The authors wish to thank Fernando Pereira, Mehryar Mohri and two anonymous referees for useful comments. References Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. 1984. Clas5 Having said this, we note that obviously one would like to decrease the size of the resulting transducers if that is possible. We are currently investigating ways to avoid precompiling the transducers beforehand, but rather to construct &apos;on the fly&apos;, only those portions of the transducers that are necessary for a particular intersection. 220 ARPABET phone # no</context>
</contexts>
<marker>Sproat, 1995</marker>
<rawString>Richard Sproat. 1995. A finite-state architecture for tokenization and grapheme-to-phoneme conversion for multilingual text analysis. In Susan Armstrong and Evelyne Tzoukermann, editors, Proceedings of the EACL SIGDAT Workshop, pages 65-72, Dublin, Ireland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
</authors>
<title>Multilingual text analysis for text-to-speech synthesis.</title>
<date>1996</date>
<booktitle>In Proceedings of the ECAI-96 Workshop on Extended Finite State Models of Language, Budapest, Hungary. European Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="780" citStr="Sproat, 1996" startWordPosition="118" endWordPosition="119">ct We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996). 1 Introduction Much attention has been devoted recently to methods for inferring linguistic models from data. One powerful inference method that has been used in various applications are decision trees, and in particular classification and regression trees (Breiman et al., 1984). An increasing amount of attention has also been focussed on finite-state methods for implementing linguistic models, in particular finitestate transducers and weighted finite-state transducers; see (Kaplan and Kay, 1994; Pereira et al., 1994, inter alia). The reason for the renewed interest in finite-state mechanism</context>
<context position="6661" citStr="Sproat (1996)" startWordPosition="1130" endWordPosition="1131">) has shown that systems of rewrite rules of the familiar form 0 —+ O/A p, where 0, 0, A and p are regular expressions, can be represented computationally as finite-state transducers (FSTs): note that 0 represents the rule&apos;s input rule, V) the output, and A and p, respectively, the left and right contexts. Kaplan and Kay (1994) have presented a concrete algorithm for compiling systems of such rules into FSTs. These methods can be extended slightly to include the compilation of probabilistic or weighted rules into weighted finitestate-transducers (WFSTs — see (Pereira et al., 1994)); Mohri and Sproat (1996) describe a rulecompilation algorithm which is more efficient than the Kaplan-Kay algorithm, and which has been extended to handle weighted rules. For present purposes it is sufficient to observe that given this extended algorithm, we can allow in the expression 0 —+ O/A. p, to represent a weighted regular expression. The compiled transducer corresponding to that rule will replace 0 with b with the appropriate weights in the context A p. 4 The Tree Compilation Algorithm The key requirements on the kind of decision trees that we can compile into WFSTs are (1) the predictions at the leaf nodes s</context>
<context position="19646" citStr="Sproat, 1996" startWordPosition="3410" endWordPosition="3411">cers seem at first glance to be unfavorable, it is important to bear in mind that size is not the only consideration in deciding upon a particular representation. WFSTs possess several nice properties that are not shared by trees, or handwritten rulesets for that matter. In particular, once compiled into a WFST, a tree can be used in the same way as a WFST derived from any other source, such as a lexicon or a language model; a compiled WFST can be used directly in a speech recognition model such as that of (Pereira and Riley, 1996) or in a speech synthesis text-analysis model such as that of (Sproat, 1996). Use of a tree directly requires a special-purpose interpreter, which is much less flexible. It should also be borne in mind that the size explosion evident in Table 4 also characterizes rules that are compiled from hand-built rewrite rules (Kaplan and Kay, 1994; Mohri and Sproat, 1996). For example, the text-analysis ruleset for the Bell Labs German text-to-speech (TTS) system (see (Sproat, 1996; Mohri and Sproat, 1996)) contains sets of rules for the pronunciation of various orthographic symbols. The ruleset for &lt;a&gt;, for example, contains 25 ordered rewrite rules. Over an alphabet of 194 sy</context>
<context position="21594" citStr="Sproat, 1996" startWordPosition="3725" endWordPosition="3726">lgorithm is correct. Several interesting areas of application come to mind. In addition to speech recognition, where we hope to apply the phonetic realization models described above to the much larger North American Business task (Paul and Baker, 1992), there are also applications to TTS where, for example, the decision trees for prosodic phrase-boundary prediction discussed in (Wang and Hirschberg, 1992) can be compiled into transducers and used directly in the WFST-based model of text analysis used in the multi-lingual version of the Bell Laboratories TTS system, described in (Sproat, 1995; Sproat, 1996). 7 Acknowledgments The authors wish to thank Fernando Pereira, Mehryar Mohri and two anonymous referees for useful comments. References Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. 1984. Clas5 Having said this, we note that obviously one would like to decrease the size of the resulting transducers if that is possible. We are currently investigating ways to avoid precompiling the transducers beforehand, but rather to construct &apos;on the fly&apos;, only those portions of the transducers that are necessary for a particular intersection. 220 ARPABET phone # nodes size of tre</context>
</contexts>
<marker>Sproat, 1996</marker>
<rawString>Richard Sproat. 1996. Multilingual text analysis for text-to-speech synthesis. In Proceedings of the ECAI-96 Workshop on Extended Finite State Models of Language, Budapest, Hungary. European Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Wang</author>
<author>Julia Hirschberg</author>
</authors>
<title>Automatic classification of intonational phrase boundaries.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--175</pages>
<contexts>
<context position="2791" citStr="Wang and Hirschberg, 1992" startWordPosition="418" endWordPosition="421">he information in decision trees into weighted finite-state transducers.&apos; Given this algorithm, information inferred from data and represented in a tree can be used directly in a system that represents other information, such as lexicons or grammars, in the form of finite-state machines. 2 Quick Review of Tree-Based Modeling A general introduction to classification and regression trees (&apos;CART&apos;) including the algorithm for growing trees from data can be found in (Breiman et al., 1984). Applications of tree-based modeling to problems in speech and NLP are discussed in (Riley, 1989; Riley, 1991; Wang and Hirschberg, 1992; Magerman, 1995, inter alia). In this section we presume that one has already trained a tree or set of trees, and we merely remind the reader of the salient points in the interpretation of those trees. Consider the tree depicted in Figure 1, which was trained on the TIMIT database (Fisher et al., 1987), and which models the phonetic realization of the English phoneme /aa/ (/a/) in various environments (Riley, 1991). When this tree is used in predicting the allophonic form of a particular instance of /aa/, one starts at the root of the tree, and asks questions about the environment in which th</context>
<context position="21389" citStr="Wang and Hirschberg, 1992" startWordPosition="3690" endWordPosition="3693">sented a practical algorithm for converting decision trees inferred from data into weighted finite-state transducers that directly implement the models implicit in the trees, and we have empirically verified that the algorithm is correct. Several interesting areas of application come to mind. In addition to speech recognition, where we hope to apply the phonetic realization models described above to the much larger North American Business task (Paul and Baker, 1992), there are also applications to TTS where, for example, the decision trees for prosodic phrase-boundary prediction discussed in (Wang and Hirschberg, 1992) can be compiled into transducers and used directly in the WFST-based model of text analysis used in the multi-lingual version of the Bell Laboratories TTS system, described in (Sproat, 1995; Sproat, 1996). 7 Acknowledgments The authors wish to thank Fernando Pereira, Mehryar Mohri and two anonymous referees for useful comments. References Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. 1984. Clas5 Having said this, we note that obviously one would like to decrease the size of the resulting transducers if that is possible. We are currently investigating ways to avoid </context>
</contexts>
<marker>Wang, Hirschberg, 1992</marker>
<rawString>Michelle Wang and Julia Hirschberg. 1992. Automatic classification of intonational phrase boundaries. Computer Speech and Language, 6:175-196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>