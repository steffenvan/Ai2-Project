<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000525">
<title confidence="0.998405">
A High-Precision Approach to Detecting Hedges and Their Scopes
</title>
<author confidence="0.9944">
Halil Kilicoglu and Sabine Bergler
</author>
<affiliation confidence="0.9991425">
Department of Computer Science and Software Engineering
Concordia University
</affiliation>
<address confidence="0.496091">
1455 de Maisonneuve Blvd. West
Montr´eal, Canada
</address>
<email confidence="0.992582">
{h kilico,bergler}@cse.concordia.ca
</email>
<sectionHeader confidence="0.993546" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976043478261">
We extend our prior work on specula-
tive sentence recognition and speculation
scope detection in biomedical text to the
CoNLL-2010 Shared Task on Hedge De-
tection. In our participation, we sought
to assess the extensibility and portability
of our prior work, which relies on linguis-
tic categorization and weighting of hedg-
ing cues and on syntactic patterns in which
these cues play a role. For Task 1B,
we tuned our categorization and weight-
ing scheme to recognize hedging in bio-
logical text. By accommodating a small
number of vagueness quantifiers, we were
able to extend our methodology to de-
tecting vague sentences in Wikipedia arti-
cles. We exploited constituent parse trees
in addition to syntactic dependency rela-
tions in resolving hedging scope. Our re-
sults are competitive with those of closed-
domain trained systems and demonstrate
that our high-precision oriented methodol-
ogy is extensible and portable.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932018181819">
Natural language is imbued with uncertainty,
vagueness, and subjectivity. However, informa-
tion extraction systems generally focus on ex-
tracting factual information, ignoring the wealth
of information expressed through such phenom-
ena. In recent years, the need for information ex-
traction and text mining systems to identify and
model such extra-factual information has increas-
ingly become clear. For example, online product
and movie reviews have provided a rich context
for analyzing sentiments and opinions in text (see
Pang and Lee (2008) for a recent survey), while
tentative, speculative nature of scientific writing,
particularly in biomedical literature, has provided
impetus for recent research in speculation detec-
tion (Light et al., 2004). The term hedging is often
used as an umbrella term to refer to an array of
extra-factual phenomena in natural language and
is the focus of the CoNLL-2010 Shared Task on
Hedge Detection.
The CoNLL-2010 Shared Task on Hedge De-
tection (Farkas et al., 2010) follows in the steps
of the recent BioNLP’09 Shared Task on Event
Extraction (Kim et al., 2009), in which one task
(speculation and negation detection) was con-
cerned with notions related to hedging in biomed-
ical abstracts. However, the CoNLL-2010 Shared
Task differs in several aspects. It sheds light on
the pervasiveness of hedging across genres and do-
mains: in addition to biomedical abstracts, it is
concerned with biomedical full text articles as well
as with Wikipedia articles. Both shared tasks have
been concerned with scope resolution; however,
their definitions of scope are fundamentally differ-
ent: the BioNLP’09 Shared Task takes the scope
of a speculation instance to be an abstract seman-
tic object (an event), thus a normalized logical
form. The CoNLL-2010 Shared Task, on the other
hand, defines it as a textual unit based on syntac-
tic considerations. It is also important to note that
hedging in scientific writing is a core aspect of the
genre (Hyland, 1998), while it is judged to be a
flaw which has to be eradicated in Wikipedia ar-
ticles. Therefore, hedge detection in these genres
serves different purposes: explicitly encoding the
factuality of a scientific claim (doubtful, probable,
etc.) versus flagging unreliable text.
We participated in both tasks of the CoNLL-
2010 Shared Task: namely, detection of sentences
with uncertainty (Task 1) and resolution of uncer-
tainty scope (Task 2). Since we pursued both of
these directions in prior work, one of our goals in
participating in the shared task was to assess how
our approach generalized to previously unseen
texts, even genres. Towards this goal, we adopted
</bodyText>
<page confidence="0.978877">
70
</page>
<note confidence="0.9551065">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 70–77,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999881631578947">
an open-domain approach, where we aimed to use
previously developed techniques to the extent pos-
sible. Among all participating groups, we distin-
guished ourselves as the one that fully worked in
an open-domain setting. This approach worked
reasonably well for uncertainty detection (Task 1);
however, for the scope resolution task, we needed
to extend our work more substantially, since the
notion of scope was fundamentally different than
what we adopted previously. The performance
of our system was competitive; in terms of F-
measure, we were ranked near the middle in Task
1, while a more significant focus on scope reso-
lution resulted in fourth place ranking among fif-
teen systems. We obtained the highest precision
in tasks focusing on biological text. Considering
that we chose not to exploit the training data pro-
vided to the full extent, we believe that our system
is viable in terms of extensibility and portability.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99991638">
Several notions related to hedging have been pre-
viously explored in natural language processing.
In the news article genre, these have included
certainty, modality, and subjectivity. For ex-
ample, Rubin et al. (2005) proposed a four di-
mensional model to categorize certainty in news
text: certainty level, focus, perspective and time.
In the context of TimeML (Pustejovsky et al.,
2005), which focuses on temporal expressions
in news articles, event modality is encoded us-
ing subordination links (SLINKs), some of which
(MODAL,EVIDENTIAL) indicate hedging (Sauriet
al., 2006). Saur´ı (2008) exploits modality and
polarity to assess the factuality degree of events
(whether they correspond to facts, counter-facts or
possibilities), and reports on FactBank, a corpus
annotated for event factuality (Saur´ı and Puste-
jovsky, 2009). Wiebe et al. (2005) consider
subjectivity in news articles, and focus on the
notion of private states, encompassing specula-
tions, opinions, and evaluations in their subjectiv-
ity frames.
The importance of speculative language in
biomedical articles was first acknowledged by
Light et al. (2004). Following work in this area
focused on detecting speculative sentences (Med-
lock and Briscoe, 2007; Szarvas, 2008; Kilicoglu
and Bergler, 2008). Similar to Rubin et al.’s
(2005) work, Thompson et al. (2008) proposed
a categorization scheme for epistemic modality in
biomedical text according to the type of infor-
mation expressed (e.g., certainty level, point of
view, knowledge type). With the availability of the
BioScope corpus (Vincze et al., 2008), in which
negation, hedging and their scopes are annotated,
studies in detecting speculation scope have also
been reported (Morante and Daelemans, 2009;
¨Ozg¨ur and Radev, 2009). Negation and uncer-
tainty of bio-events are also annotated to some ex-
tent in the GENIA event corpus (Kim et al., 2008).
The BioNLP’09 Shared Task on Event Extraction
(Kim et al., 2009) dedicated a task to detecting
negation and speculation in biomedical abstracts,
based on the GENIA event corpus annotations.
Ganter and Strube (2009) elaborated on the link
between vagueness in Wikipedia articles indicated
by weasel words and hedging. They exploited
word frequency measures and shallow syntactic
patterns to detect weasel words in Wikipedia ar-
ticles.
</bodyText>
<sectionHeader confidence="0.997988" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999882">
Our methodology for hedge detection is essen-
tially rule-based and relies on a combination of
lexical and syntactic information. Lexical infor-
mation is encoded in a simple dictionary, and rel-
evant syntactic information is identified using the
Stanford Lexicalized Parser (Klein and Manning,
2003). We exploit constituent parse trees as well
as corresponding collapsed dependency represen-
tations (deMarneffe et al., 2006), provided by the
parser.
</bodyText>
<subsectionHeader confidence="0.999923">
3.1 Detecting Uncertainty in Biological Text
</subsectionHeader>
<bodyText confidence="0.999936470588235">
For detecting uncertain sentences in biological text
(Task 1B), we built on the linguistically-inspired
system previously described in detail in Kilicoglu
and Bergler (2008). In summary, this system relies
on a dictionary of lexical speculation cues, derived
from a set of core surface realizations of hedging
identified by Hyland (1998) and expanded through
WordNet (Fellbaum, 1998) synsets and UMLS
SPECIALIST Lexicon (McCray et al., 1994) nom-
inalizations. A set of lexical certainty markers (un-
hedgers) are also included, as they indicate hedg-
ing when they are negated (e.g., know). These
hedging cues are categorized by their type (modal
auxiliaries, epistemic verbs, approximative adjec-
tives, etc.) and are weighted to reflect their cen-
tral/peripheral contribution to hedging, inspired by
the fuzzy model of Hyland (1998). We use a scale
</bodyText>
<page confidence="0.997625">
71
</page>
<bodyText confidence="0.999997760869565">
of 1-5, where 5 is assigned to cues most central
to hedging and 1 to those that are most periph-
eral. For example, the modal auxiliary may has
a weight of 5, while a relatively weak hedging
cue, the epistemic adverb apparently, has a weight
of 2. The weight sum of cues in a sentence in
combination with a predetermined threshold de-
termines whether the sentence in question is un-
certain. Syntax, generally ignored in other stud-
ies on hedging, plays a prominent role in our ap-
proach. Certain syntactic constructions act as cues
(e.g., whether- and if-complements), while others
strengthen or weaken the effect of the cue associ-
ated with them. For example, a that-complement
taken by an epistemic verb increases the hedging
score contributed by the verb by 2, while lack of
any complement decreases the score by 1.
For the shared task, we tuned this categoriza-
tion and weighting scheme, based on an analy-
sis of the biomedical full text articles in training
data. We also adjusted the threshold. We elim-
inated some hedging cue categories completely
and adjusted the weights of a small number of
the remaining cues. The eliminated cue categories
included approximative adverbs (e.g., generally,
largely, partially) and approximative adjectives
(e.g., partial), often used to “manipulate preci-
sion in quantification” (Hyland, 1998). The other
eliminated category included verbs of effort (e.g.,
try, attempt, seek), also referred to as rationalising
narrators (Hyland, 1998). The motivation behind
eliminating these categories was that cues belong-
ing to these categories were never annotated as
hedging cues in the training data. The elimination
process resulted in a total of 147 remaining hedg-
ing cues. Additionally, we adjusted the weights of
several other cues that were not consistently anno-
tated as cues in the training data, despite our view
that they were strong hedging cues. One example
is the epistemic verb predict, previously assigned a
weight of 4 based on Hyland’s analysis. We found
its annotation in the training data somewhat incon-
sistent, and lowered its weight to 3, thus requiring
a syntactic strengthening effect (an infinitival com-
plement, for example) for it to qualify as a hedging
cue in the current setting (threshold of 4).
</bodyText>
<subsectionHeader confidence="0.993828">
3.2 Detecting Uncertainty in Wikipedia
Articles
</subsectionHeader>
<bodyText confidence="0.981991714285715">
Task 1W was concerned with detecting uncer-
tainty in Wikipedia articles. Uncertainty in this
context refers more or less to vagueness indicated
by weasel words, an undesirable feature accord-
ing to Wikipedia policy. Analysis of Wikipedia
training data provided by the organizers revealed
that there is overlap between weasel words and
hedging cues described in previous section. We,
therefore, sought to adapt our dictionary of hedg-
ing cues to the task of detecting vagueness in
Wikipedia articles. Similar to Task 1B, changes
involved eliminating cue categories and adjusting
cue weights. In addition, however, we also added
a previously unconsidered category of cues, due
to their prominence in Wikipedia data as weasel
words. This category (vagueness quantifiers (Lap-
pin, 2000)) includes words, such as some, several,
many and various, which introduce imprecision
when in modifier position. For instance, in the ex-
ample below, both some and certain contribute to
vagueness of the sentence.
</bodyText>
<listItem confidence="0.916218">
(1) Even today, some cultures have certain in-
stances of their music intending to imitate
natural sounds.
</listItem>
<bodyText confidence="0.99961625">
For Wikipedia uncertainty detection, eliminated
categories included verbs and nouns concerning
tendencies (e.g., tend, inclination) in addition to
verbs of effort. The only modal auxiliary consis-
tently considered a weasel word was might; there-
fore, we only kept might in this category and elim-
inated the rest (e.g., may, would). Approxima-
tive adverbs, eliminated in detecting uncertainty
in biological text, not only were revived for this
task, but also their weights were increased as they
were more central to vagueness expressions. Be-
sides these changes in weighting and categoriza-
tion, the methodology for uncertainty detection in
Wikipedia articles was essentially the same as that
for biological text. The threshold we used in our
submission was, similarly, 4.
</bodyText>
<subsectionHeader confidence="0.9995545">
3.3 Scope Resolution for Uncertainty in
Biological Text
</subsectionHeader>
<bodyText confidence="0.999939111111111">
Task 2 of the shared task involved hedging scope
resolution in biological text. We previously tack-
led this problem within the context of biological
text in the BioNLP’09 Shared Task (Kilicoglu and
Bergler, 2009). That task defined the scope of
speculation instances as abstract, previously ex-
tracted bio-events. Our approach relied on find-
ing an appropriate syntactic dependency relation
between the bio-event trigger word identified in
</bodyText>
<page confidence="0.990076">
72
</page>
<bodyText confidence="0.999687785714286">
earlier steps and the speculation cue. The cate-
gory of the hedging cue constrained the depen-
dency relations that are deemed appropriate. For
example, consider the sentence in (2a), where in-
volves is a bio-event trigger for a Regulation
event and suggest is a speculation cue of epis-
temic verb type. The first dependency relation
in (2b) indicates that the epistemic verb takes a
clausal complement headed by the bio-event trig-
ger. The second indicates that that is the comple-
mentizer. This cue category/dependency combi-
nation licenses the generation of a speculation in-
stance where the event indicated by the event trig-
ger represents the scope.
</bodyText>
<listItem confidence="0.7961912">
(2) (a) The results suggest that M-CSF induc-
tion of M-CSF involves G proteins, PKC
and NF kappa B.
(b) ccomp(suggest,involves)
complm(involves,that)
</listItem>
<bodyText confidence="0.8172816">
Several other cue category/dependency combi-
nations sought for speculation scope resolution are
given in Table 1. X represents a token that is nei-
ther a cue nor a trigger (aux: auxiliary, dobj: direct
object, neg: negation modifier).
</bodyText>
<table confidence="0.998953666666667">
Cue Category Dependency
Modal auxiliary (may) aux(Trigger,Cue)
Conditional (if) complm(Trigger,Cue)
Unhedging noun dobj(X,Cue)
(evidence) ccomp(X,Trigger)
neg(Cue,no)
</table>
<tableCaption confidence="0.9874925">
Table 1: Cue categories with examples and the de-
pendency relations to search
</tableCaption>
<bodyText confidence="0.999809857142857">
In contrast to this notion of scope being an ab-
stract semantic object, Task 2 (BioScope corpus,
in general) conceptualizes hedge scope as a con-
tinuous textual unit, including the hedging cue it-
self and the biggest syntactic unit the cue is in-
volved in (Vincze et al., 2008). This fundamen-
tal difference in conceptualization limits the di-
rect applicability of our prior approach to this
task. Nevertheless, we were able to use our work
as a building block in extending scope resolution
heuristics. We further augmented it by exploiting
constituent parse trees provided by Stanford Lex-
icalized Parser. These extensions are summarized
below.
</bodyText>
<subsectionHeader confidence="0.956392">
3.3.1 Exploiting parse trees
</subsectionHeader>
<bodyText confidence="0.998595">
The constituent parse trees contribute to scope
resolution uniformly across all hedging cue cate-
gories. We simply determine the phrasal node that
dominates the hedging cue and consider the tokens
within that phrase as being in the scope of the cue,
unless they meet one of the following exclusion
criteria:
</bodyText>
<listItem confidence="0.952403888888889">
1. Exclude tokens within post-cue sentential
complements (indicated by S and SBAR
nodes) introduced by a small number of
discourse markers (thus, whereas, because,
since, if, and despite).
2. Exclude punctuation marks at the right
boundary of the phrase
3. Exclude pre-cue determiners and adverbs at
the left boundary of the phrase
</listItem>
<bodyText confidence="0.9972644">
For example, in the sentence below, the verb
phrase that included the modal auxiliary may also
included the complement introduced by thereby.
Using the exclusion criteria 1 and 2, we excluded
the tokens following SPACER from the scope:
</bodyText>
<listItem confidence="0.998049333333333">
(3) (a) ... motifs may be easily compared with
the results from BEAM, PRISM and
SPACER, thereby extending the SCOPE
ensemble to include a fourth class of
motifs.
(b) CUE: may
</listItem>
<bodyText confidence="0.960902333333333">
SCOPE: motifs may be easily compared
with the results from BEAM, PRISM
and SPACER
</bodyText>
<subsectionHeader confidence="0.936656">
3.3.2 Extending dependency-based heuristics
</subsectionHeader>
<bodyText confidence="0.9127436875">
The new scope definition was also accommodated
by extending the basic dependency-based heuris-
tics summarized earlier in this section. In addition
to finding the trigger word that satisfies the ap-
propriate dependency constraint with the hedging
cue (we refer to this trigger word as scope head,
henceforth), we also considered the other depen-
dency relations that the scope head was involved
in. These relations, then, were used in right ex-
pansion and left expansion of the scope. Right ex-
pansion involves finding the rightmost token that
is in a dependency relation with the scope head.
Consider the sentence below:
(4) The surprisingly low correlations between
Sig and accuracy may indicate that the ob-
jective functions employed by motif finding
</bodyText>
<page confidence="0.994902">
73
</page>
<bodyText confidence="0.999628538461538">
programs are only a first approximation to bi-
ological significance.
The epistemic verb indicate has as its scope
head the token approximation, due to the existence
of a clausal complement dependency (ccomp) be-
tween them. On the other hand, the rightmost to-
ken of the sentence, significance, has a preposi-
tional modifier dependency (prep to) with approx-
imation. It is, therefore, included in the scope of
indicate. Two dependency types, adverbial clause
modifier (advcl) and conjunct (conj), were ex-
cluded from consideration when the rightmost to-
ken is sought, since they are likely to signal new
discourse units outside the scope.
In contrast to right expansion, which applies
to all hedging cue categories, left expansion ap-
plies only to a subset. Left expansion involves
searching for a subject dependency governed by
the scope head. The dependency types descend-
ing from the subject (subj) type in the Stanford de-
pendency hierarchy are considered: nsubj (nom-
inal subject), nsubjpass (passive nominal sub-
ject), csubj (clausal subject) and csubjpass (pas-
sive clausal subject). In the following example,
the first token, This, is added to the scope of likely
through left expansion (cop: copula).
</bodyText>
<listItem confidence="0.961743">
(5) (a) This is most likely a conservative esti-
mate since a certain proportion of inter-
actions remain unknown ...
</listItem>
<equation confidence="0.5975165">
(b) nsubj(likely,This)
cop(likely,is)
</equation>
<bodyText confidence="0.988765">
Left expansion was limited to the following cue
categories, with the additional constraints given:
</bodyText>
<listItem confidence="0.841391166666667">
1. Modal auxiliaries, only when their scope
head takes a passive subject (e.g., they is
added to the scope of may in they may be an-
notated as pseudogenes).
2. Cues in adjectival categories, when they are
in copular constructions (e.g., Example (5)).
3. Cues in several adjectival ad verbal cate-
gories, when they take infinitival comple-
ments (e.g., this is added to the scope of ap-
pears in However, this appears to add more
noise to the prediction without increasing the
accuracy).
</listItem>
<bodyText confidence="0.9994122">
After scope tokens are identified using the parse
tree as well as via left and right expansion, the al-
gorithm simply sets as scope the continuous tex-
tual unit that includes all the scope tokens and the
hedging cue. Since, likely is the hedging cue and
This and estimate are identified as scope tokens in
Example (5), the scope associated with likely be-
comes This is most likely a conservative estimate.
We found that citations, numbers and punc-
tuation marks occurring at the end of sentences
caused problems in scope resolution, specifically
in biomedical full text articles. Since they are
rarely within any scope, we implemented a simple
stripping algorithm to eliminate them from scopes
in such documents.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.99720575">
The official evaluation results regarding our sub-
mission are given in Table 2. These results were
achieved with the threshold 4, which was the opti-
mal threshold on the training data.
</bodyText>
<table confidence="0.99851125">
Prec. Recall F-score Rank
Task 1B 92.07 74.94 82.62 12/24
Task 1W 67.90 46.02 54.86 10/17
Task 2 62.47 49.47 55.21 4/15
</table>
<tableCaption confidence="0.99885">
Table 2: Evaluation results
</tableCaption>
<bodyText confidence="0.995949230769231">
In Task 1B, we achieved the highest precision.
However, our relatively low recall led to the place-
ment of our system in the middle. Our system al-
lows adjusting precision versus recall by setting
the threshold. In fact, setting the threshold to 3 af-
ter the shared task, we were able to obtain overall
better results (Precision=83.43, Recall=84.81, F-
score=84.12, Rank=8/24). However, we explicitly
targeted precision, and in that respect, our submis-
sion results were not surprising. In fact, we iden-
tified a new type of hedging signalled by coordi-
nation (either ... or ... as well as just or) in the
training data. An example is given below:
</bodyText>
<listItem confidence="0.85762">
(6) (a) It will be either a sequencing error or a
pseudogene.
(b) CUE: either-or
SCOPE: either a seqeuncing error or a
pseudogene
</listItem>
<bodyText confidence="0.999749571428571">
By handling this class to some extent, we could
have increased our recall, and therefore, F-score
(65 out of 1,044 cues in the evaluation data for
biological text involved this class). However, we
decided against treating this class, as we believe
it requires a slightly different treatment due to its
special semantics.
</bodyText>
<page confidence="0.996967">
74
</page>
<bodyText confidence="0.99991474074074">
In participating in Task 1W, our goal was to
test the ease of extensibility of our system. In
that regard, our results show that we were able
to exploit the overlap between our hedging cues
and the weasel words. The major difference we
noted between hedging in two genres was the
class of vagueness quantifiers, and, with little ef-
fort, we extended our system to consider them.
We also note that setting the threshold to 3 after
the shared task, our recall and F-score improved
significantly (Precision=63.21, Recall=53.67, F-
score=58.05, Rank=3/17).
Our more substantial effort for Task 2 resulted
in a better overall ranking, as well as the highest
precision in this task. In contrast to Task 1, chang-
ing the threshold in this task did not have a pos-
itive effect on the outcome. We also measured
the relative contribution of the enhancements to
scope resolution. The results are presented in Ta-
ble 3. Baseline is taken as the scope resolution al-
gorithm we developed in prior work. These results
show that: a) scope definition we adopted earlier
is essentially incompatible with the BioScope def-
inition b) simply taking the phrase that the hedg-
ing cue belongs to as the scope provides relatively
good results c) left and right expansion heuristics
are needed for increased precision and recall.
</bodyText>
<table confidence="0.999626">
Prec. Recall F-score
Baseline 3.29 2.61 2.91
Baseline+ Left/ 25.18 20.03 22.31
right expansion
Parse tree 49.20 39.10 43.58
Baseline+ 50.66 40.27 44.87
Parse tree
All 62.47 49.47 55.21
</table>
<tableCaption confidence="0.999742">
Table 3: Effect of scope resolution enhancements
</tableCaption>
<subsectionHeader confidence="0.984135">
4.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999796473684211">
In this section, we provide a short analysis of the
errors our system generated, focusing on biologi-
cal text.
Since our dictionary of hedging cues is incom-
plete and we did not attempt to expand it for Task
1B, we had a fair number of recall errors. As
we mentioned above, either-or constructions oc-
cur frequently in the training and evaluation data,
and we did not attempt to handle them. Addition-
ally, some lexical cues, such as feasible and im-
plicate, do not appear in our dictionary, causing
further recall errors. The weighting scheme also
affects recall. For example, the adjective appar-
ent has a weight of 2, which is not itself sufficient
to qualify a sentence as uncertain (with a thresh-
old of 4) (7a). On the other hand, when it takes
a clausal complement, the sentence is considered
uncertain (7b). The first sentence (7a) causes a re-
call error.
</bodyText>
<listItem confidence="0.852415666666667">
(7) (a) An apparent contradiction between the
previously reported number of cycling
genes ...
(b) ... it is apparent that the axonal termini
contain a significantly reduced number
of varicosities ...
</listItem>
<bodyText confidence="0.993878125">
In some cases, syntactic constructions that play
a role in determining the certainty status of a sen-
tence cannot be correctly identified by the parser,
often leading to recall errors. For example, in the
sentence below, the clausal complement construc-
tion is missed by the parser. Since the verb indi-
cate has weight 3, this leads to a recall error in the
current setting.
</bodyText>
<listItem confidence="0.8186465">
(8) ... indicating that dMyc overexpression can
substitute for PI3K activation ...
</listItem>
<bodyText confidence="0.999852545454545">
Adjusting the weights of cues worked well gen-
erally, but also caused unexpected problems, due
to what seem like inconsistencies in annotation.
The examples below highlight the effect of low-
ering the weight of predict from 4 to 3. Exam-
ples (9a) and (9b) are almost identical on surface
and our system predicted both to be uncertain, due
to the fact that predicted took infinitival comple-
ments in both cases. However, only (9a) was an-
notated as uncertain, leading to a precision error in
(9b).
</bodyText>
<listItem confidence="0.864034">
(9) (a) ... include all protein pairs predicted to
have posterior odds ratio ...
(b) Protein pairs predicted to have a poste-
rior odds ratio ...
</listItem>
<bodyText confidence="0.999885777777778">
The error cases in scope resolution are more
varied. Syntax has a larger role in this task, and
therefore, parsing errors tend to affect the results
more directly. In the following example, dur-
ing left-expanding the scope of the modal auxil-
iary could, RNAi screens, rather than the full noun
phrase fruit fly RNAi screens, is identified as the
passive subject of the scope head (associated), be-
cause an appropriate modifier dependency cannot
</bodyText>
<page confidence="0.997233">
75
</page>
<bodyText confidence="0.9906084375">
be found between the noun phrase head screens
and either of the modifiers, fruit and fly.
(10) ... was to investigate whether fruit fly RNAi
screens of conserved genes could be asso-
ciated with similar tick phenotypes and tick
gene function.
In general, the simple mechanism to exploit
constituent parse trees was useful in resolving
scope. However, it appears that a nuanced ap-
proach based on cue categories could enhance the
results further. In particular, the current mecha-
nism does not contribute much to resolving scopes
of adverbial cues. In the following example, parse
tree mechanism does not have any effect, leading
to both a precision and a recall error in scope res-
olution.
</bodyText>
<listItem confidence="0.9786045">
(11) (a) ... we will consider tightening the defi-
nitions and possibly splitting them into
different roles.
(b) FP: possibly
</listItem>
<bodyText confidence="0.9967977">
FN: possibly splitting them into differ-
ent roles
Left/right expansion strategies were based on
the analysis of training data. However, we en-
countered errors caused by these strategies where
we found the annotations contradictory. In Exam-
ple (12a), the entire fragment is in the scope of
thought, while in (12b), the scope of suggested
does not include it was, even though on surface
both fragments are very similar.
</bodyText>
<listItem confidence="0.969255">
(12) (a) ... the kinesin-5 motor is thought to play
a key role.
(b) ... it was suggested to enhance the nu-
</listItem>
<bodyText confidence="0.9721955">
clear translocation of NF-κB.
Post-processing in the form of citation stripping
was simplistic, and, therefore, was unable to han-
dle complex cases, as the one shown in the exam-
ple below. The algorithm is only able to remove
one reference at the end.
</bodyText>
<listItem confidence="0.8565158">
(13) (a) ...it is possible that some other sig-
nalling system may operate with Semas
to confine dorsally projecting neurons to
dorsal neuropile [3],[40],[41].
(b) FP: may operate with Semas to con-
</listItem>
<bodyText confidence="0.9824254">
fine dorsally projecting neurons to dor-
sal neuropile [3],[40],
FN: may operate with Semas to con-
fine dorsally projecting neurons to dor-
sal neuropile
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999992296296296">
Rather than developing a dedicated methodology
that exclusively relies on the data provided by or-
ganizers, we chose to extend and refine our prior
work in hedge detection and used the training
data only in a limited manner: to tune our sys-
tem in a principled way. With little tuning, we
achieved the highest precision in Task 1B. We
were able to capitalize on the overlap between
hedging cues and weasel words for Task 1W and
achieved competitive results. Adapting our pre-
vious work in scope resolution to Task 2, how-
ever, was less straightforward, due to the incom-
patible definitions of scope. Nevertheless, by re-
fining the prior dependency-based heuristics with
left and right expansion strategies and utilizing a
simple mechanism for parse tree information, we
were able to accommodate the new definition of
scope to a large extent. With these results, we con-
clude that our methodology is portable and easily
extensible.
While the results show that using the parse tree
information for scope resolution benefited our per-
formance greatly, error analysis presented in the
previous sections also suggests that a finer-grained
approach based on cue categories could further
improve results, and we aim to explore this exten-
sion further.
</bodyText>
<sectionHeader confidence="0.998548" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997623142857143">
Marie-Catherine deMarneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449–
454.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1–12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ing Wikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173–176.
</reference>
<page confidence="0.929828">
76
</page>
<reference confidence="0.999748715789473">
Ken Hyland. 1998. Hedging in scientific research ar-
ticles. John Benjamins B.V., Amsterdam, Nether-
lands.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9 Suppl 11:s10.
Halil Kilicoglu and Sabine Bergler. 2009. Syntac-
tic dependency based heuristics for biological event
extraction. In Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL 2009
Workshop, pages 119–127.
Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of BioNLP’09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing
in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 1–9.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41th Meeting of the Association for Computational
Linguistics, pages 423–430.
Shalom Lappin. 2000. An intensional parametric se-
mantics for vague quantifiers. Linguistics and Phi-
losophy, 23(6):599–620.
Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: facts, speculations, and
statements in between. In BioLINK 2004: Linking
Biological Literature, Ontologies and Databases,
pages 17–24.
Alexa T. McCray, Suresh Srinivasan, and Allen C.
Browne. 1994. Lexical methods for managing vari-
ation in biomedical terminologies. In Proceedings
of the 18th Annual Symposium on Computer Appli-
cations in Medical Care, pages 235–239.
Ben Medlock and Ted Briscoe. 2007. Weakly su-
pervised learning for hedge classification in scien-
tific literature. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguis-
tics, pages 992–999.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28–36.
Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detect-
ing speculations and their scopes in scientific text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1398–1407.
Bo Pang and Lillian Lee. 2008. Sentiment Analysis
and Opinion Mining. Now Publishers Inc, Boston,
MA.
James Pustejovsky, Robert Knippen, Jessica Littman,
and Roser Saur´ı. 2005. Temporal and event in-
formation in natural language text. Language Re-
sources and Evaluation, 39(2):123–164.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko
Kando. 2005. Certainty identification in texts: Cat-
egorization model and manual tagging results. In
James G. Shanahan, Yan Qu, and Janyce Wiebe, ed-
itors, Computing Attitude and Affect in Text: The-
ories and Applications, volume 20, pages 61–76.
Springer Netherlands, Dordrecht.
Roser Saur´ı and James Pustejovsky. 2009. FactBank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227–268.
Roser Saur´ı, Marc Verhagen, and James Pustejovsky.
2006. Annotating and recognizing event modality in
text. In Proceedings of 19th International FLAIRS
Conference.
Roser Saur´ı. 2008. A Factuality Profiler for Eventual-
ities in Text. Ph.D. thesis, Brandeis University.
Gy¨orgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of the 46th Meeting
of the Association for Computational Linguistics,
pages 281–289.
Paul Thompson, Giulia Venturi, John McNaught,
Simonetta Montemagni, and Sophia Ananiadou.
2008. Categorising modality in biomedical texts. In
Proceedings of LREC 2008 Workshop on Building
and Evaluating Resources for Biomedical Text Min-
ing.
Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas,
Gy¨orgy M´ora, and J´anos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 Suppl 11:S9.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2):165–210.
</reference>
<page confidence="0.999122">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.427583">
<title confidence="0.999919">A High-Precision Approach to Detecting Hedges and Their Scopes</title>
<author confidence="0.989686">Halil Kilicoglu</author>
<author confidence="0.989686">Sabine</author>
<affiliation confidence="0.760383">Department of Computer Science and Software Concordia</affiliation>
<address confidence="0.938192">1455 de Maisonneuve Blvd. Montr´eal,</address>
<abstract confidence="0.997378916666667">We extend our prior work on speculative sentence recognition and speculation scope detection in biomedical text to the CoNLL-2010 Shared Task on Hedge Detection. In our participation, we sought to assess the extensibility and portability of our prior work, which relies on linguistic categorization and weighting of hedging cues and on syntactic patterns in which these cues play a role. For Task 1B, we tuned our categorization and weighting scheme to recognize hedging in biological text. By accommodating a small number of vagueness quantifiers, we were able to extend our methodology to detecting vague sentences in Wikipedia articles. We exploited constituent parse trees in addition to syntactic dependency relations in resolving hedging scope. Our results are competitive with those of closeddomain trained systems and demonstrate that our high-precision oriented methodology is extensible and portable.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marie-Catherine deMarneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--454</pages>
<contexts>
<context position="7703" citStr="deMarneffe et al., 2006" startWordPosition="1186" endWordPosition="1189">between vagueness in Wikipedia articles indicated by weasel words and hedging. They exploited word frequency measures and shallow syntactic patterns to detect weasel words in Wikipedia articles. 3 Methods Our methodology for hedge detection is essentially rule-based and relies on a combination of lexical and syntactic information. Lexical information is encoded in a simple dictionary, and relevant syntactic information is identified using the Stanford Lexicalized Parser (Klein and Manning, 2003). We exploit constituent parse trees as well as corresponding collapsed dependency representations (deMarneffe et al., 2006), provided by the parser. 3.1 Detecting Uncertainty in Biological Text For detecting uncertain sentences in biological text (Task 1B), we built on the linguistically-inspired system previously described in detail in Kilicoglu and Bergler (2008). In summary, this system relies on a dictionary of lexical speculation cues, derived from a set of core surface realizations of hedging identified by Hyland (1998) and expanded through WordNet (Fellbaum, 1998) synsets and UMLS SPECIALIST Lexicon (McCray et al., 1994) nominalizations. A set of lexical certainty markers (unhedgers) are also included, as t</context>
</contexts>
<marker>deMarneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine deMarneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 449– 454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 1–12, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8157" citStr="Fellbaum, 1998" startWordPosition="1254" endWordPosition="1255">ized Parser (Klein and Manning, 2003). We exploit constituent parse trees as well as corresponding collapsed dependency representations (deMarneffe et al., 2006), provided by the parser. 3.1 Detecting Uncertainty in Biological Text For detecting uncertain sentences in biological text (Task 1B), we built on the linguistically-inspired system previously described in detail in Kilicoglu and Bergler (2008). In summary, this system relies on a dictionary of lexical speculation cues, derived from a set of core surface realizations of hedging identified by Hyland (1998) and expanded through WordNet (Fellbaum, 1998) synsets and UMLS SPECIALIST Lexicon (McCray et al., 1994) nominalizations. A set of lexical certainty markers (unhedgers) are also included, as they indicate hedging when they are negated (e.g., know). These hedging cues are categorized by their type (modal auxiliaries, epistemic verbs, approximative adjectives, etc.) and are weighted to reflect their central/peripheral contribution to hedging, inspired by the fuzzy model of Hyland (1998). We use a scale 71 of 1-5, where 5 is assigned to cues most central to hedging and 1 to those that are most peripheral. For example, the modal auxiliary may</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viola Ganter</author>
<author>Michael Strube</author>
</authors>
<title>Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia Tags and Shallow Linguistic Features.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>173--176</pages>
<contexts>
<context position="7055" citStr="Ganter and Strube (2009)" startWordPosition="1091" endWordPosition="1094">ainty level, point of view, knowledge type). With the availability of the BioScope corpus (Vincze et al., 2008), in which negation, hedging and their scopes are annotated, studies in detecting speculation scope have also been reported (Morante and Daelemans, 2009; ¨Ozg¨ur and Radev, 2009). Negation and uncertainty of bio-events are also annotated to some extent in the GENIA event corpus (Kim et al., 2008). The BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009) dedicated a task to detecting negation and speculation in biomedical abstracts, based on the GENIA event corpus annotations. Ganter and Strube (2009) elaborated on the link between vagueness in Wikipedia articles indicated by weasel words and hedging. They exploited word frequency measures and shallow syntactic patterns to detect weasel words in Wikipedia articles. 3 Methods Our methodology for hedge detection is essentially rule-based and relies on a combination of lexical and syntactic information. Lexical information is encoded in a simple dictionary, and relevant syntactic information is identified using the Stanford Lexicalized Parser (Klein and Manning, 2003). We exploit constituent parse trees as well as corresponding collapsed depe</context>
</contexts>
<marker>Ganter, Strube, 2009</marker>
<rawString>Viola Ganter and Michael Strube. 2009. Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia Tags and Shallow Linguistic Features. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Hyland</author>
</authors>
<title>Hedging in scientific research articles. John Benjamins B.V.,</title>
<date>1998</date>
<location>Amsterdam, Netherlands.</location>
<contexts>
<context position="3174" citStr="Hyland, 1998" startWordPosition="493" endWordPosition="494"> addition to biomedical abstracts, it is concerned with biomedical full text articles as well as with Wikipedia articles. Both shared tasks have been concerned with scope resolution; however, their definitions of scope are fundamentally different: the BioNLP’09 Shared Task takes the scope of a speculation instance to be an abstract semantic object (an event), thus a normalized logical form. The CoNLL-2010 Shared Task, on the other hand, defines it as a textual unit based on syntactic considerations. It is also important to note that hedging in scientific writing is a core aspect of the genre (Hyland, 1998), while it is judged to be a flaw which has to be eradicated in Wikipedia articles. Therefore, hedge detection in these genres serves different purposes: explicitly encoding the factuality of a scientific claim (doubtful, probable, etc.) versus flagging unreliable text. We participated in both tasks of the CoNLL2010 Shared Task: namely, detection of sentences with uncertainty (Task 1) and resolution of uncertainty scope (Task 2). Since we pursued both of these directions in prior work, one of our goals in participating in the shared task was to assess how our approach generalized to previously</context>
<context position="8111" citStr="Hyland (1998)" startWordPosition="1248" endWordPosition="1249">ion is identified using the Stanford Lexicalized Parser (Klein and Manning, 2003). We exploit constituent parse trees as well as corresponding collapsed dependency representations (deMarneffe et al., 2006), provided by the parser. 3.1 Detecting Uncertainty in Biological Text For detecting uncertain sentences in biological text (Task 1B), we built on the linguistically-inspired system previously described in detail in Kilicoglu and Bergler (2008). In summary, this system relies on a dictionary of lexical speculation cues, derived from a set of core surface realizations of hedging identified by Hyland (1998) and expanded through WordNet (Fellbaum, 1998) synsets and UMLS SPECIALIST Lexicon (McCray et al., 1994) nominalizations. A set of lexical certainty markers (unhedgers) are also included, as they indicate hedging when they are negated (e.g., know). These hedging cues are categorized by their type (modal auxiliaries, epistemic verbs, approximative adjectives, etc.) and are weighted to reflect their central/peripheral contribution to hedging, inspired by the fuzzy model of Hyland (1998). We use a scale 71 of 1-5, where 5 is assigned to cues most central to hedging and 1 to those that are most pe</context>
<context position="9945" citStr="Hyland, 1998" startWordPosition="1545" endWordPosition="1546"> hedging score contributed by the verb by 2, while lack of any complement decreases the score by 1. For the shared task, we tuned this categorization and weighting scheme, based on an analysis of the biomedical full text articles in training data. We also adjusted the threshold. We eliminated some hedging cue categories completely and adjusted the weights of a small number of the remaining cues. The eliminated cue categories included approximative adverbs (e.g., generally, largely, partially) and approximative adjectives (e.g., partial), often used to “manipulate precision in quantification” (Hyland, 1998). The other eliminated category included verbs of effort (e.g., try, attempt, seek), also referred to as rationalising narrators (Hyland, 1998). The motivation behind eliminating these categories was that cues belonging to these categories were never annotated as hedging cues in the training data. The elimination process resulted in a total of 147 remaining hedging cues. Additionally, we adjusted the weights of several other cues that were not consistently annotated as cues in the training data, despite our view that they were strong hedging cues. One example is the epistemic verb predict, pre</context>
</contexts>
<marker>Hyland, 1998</marker>
<rawString>Ken Hyland. 1998. Hedging in scientific research articles. John Benjamins B.V., Amsterdam, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Recognizing speculative language in biomedical research articles: a linguistically motivated perspective.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<note>Suppl 11:s10.</note>
<contexts>
<context position="6234" citStr="Kilicoglu and Bergler, 2008" startWordPosition="962" endWordPosition="965">ctuality degree of events (whether they correspond to facts, counter-facts or possibilities), and reports on FactBank, a corpus annotated for event factuality (Saur´ı and Pustejovsky, 2009). Wiebe et al. (2005) consider subjectivity in news articles, and focus on the notion of private states, encompassing speculations, opinions, and evaluations in their subjectivity frames. The importance of speculative language in biomedical articles was first acknowledged by Light et al. (2004). Following work in this area focused on detecting speculative sentences (Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). Similar to Rubin et al.’s (2005) work, Thompson et al. (2008) proposed a categorization scheme for epistemic modality in biomedical text according to the type of information expressed (e.g., certainty level, point of view, knowledge type). With the availability of the BioScope corpus (Vincze et al., 2008), in which negation, hedging and their scopes are annotated, studies in detecting speculation scope have also been reported (Morante and Daelemans, 2009; ¨Ozg¨ur and Radev, 2009). Negation and uncertainty of bio-events are also annotated to some extent in the GENIA event corpus (Kim et al., </context>
<context position="7947" citStr="Kilicoglu and Bergler (2008)" startWordPosition="1220" endWordPosition="1223"> is essentially rule-based and relies on a combination of lexical and syntactic information. Lexical information is encoded in a simple dictionary, and relevant syntactic information is identified using the Stanford Lexicalized Parser (Klein and Manning, 2003). We exploit constituent parse trees as well as corresponding collapsed dependency representations (deMarneffe et al., 2006), provided by the parser. 3.1 Detecting Uncertainty in Biological Text For detecting uncertain sentences in biological text (Task 1B), we built on the linguistically-inspired system previously described in detail in Kilicoglu and Bergler (2008). In summary, this system relies on a dictionary of lexical speculation cues, derived from a set of core surface realizations of hedging identified by Hyland (1998) and expanded through WordNet (Fellbaum, 1998) synsets and UMLS SPECIALIST Lexicon (McCray et al., 1994) nominalizations. A set of lexical certainty markers (unhedgers) are also included, as they indicate hedging when they are negated (e.g., know). These hedging cues are categorized by their type (modal auxiliaries, epistemic verbs, approximative adjectives, etc.) and are weighted to reflect their central/peripheral contribution to </context>
</contexts>
<marker>Kilicoglu, Bergler, 2008</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2008. Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC Bioinformatics, 9 Suppl 11:s10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Syntactic dependency based heuristics for biological event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of Natural Language Processing in Biomedicine (BioNLP) NAACL 2009 Workshop,</booktitle>
<pages>119--127</pages>
<contexts>
<context position="13044" citStr="Kilicoglu and Bergler, 2009" startWordPosition="2025" endWordPosition="2028">, not only were revived for this task, but also their weights were increased as they were more central to vagueness expressions. Besides these changes in weighting and categorization, the methodology for uncertainty detection in Wikipedia articles was essentially the same as that for biological text. The threshold we used in our submission was, similarly, 4. 3.3 Scope Resolution for Uncertainty in Biological Text Task 2 of the shared task involved hedging scope resolution in biological text. We previously tackled this problem within the context of biological text in the BioNLP’09 Shared Task (Kilicoglu and Bergler, 2009). That task defined the scope of speculation instances as abstract, previously extracted bio-events. Our approach relied on finding an appropriate syntactic dependency relation between the bio-event trigger word identified in 72 earlier steps and the speculation cue. The category of the hedging cue constrained the dependency relations that are deemed appropriate. For example, consider the sentence in (2a), where involves is a bio-event trigger for a Regulation event and suggest is a speculation cue of epistemic verb type. The first dependency relation in (2b) indicates that the epistemic verb </context>
</contexts>
<marker>Kilicoglu, Bergler, 2009</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2009. Syntactic dependency based heuristics for biological event extraction. In Proceedings of Natural Language Processing in Biomedicine (BioNLP) NAACL 2009 Workshop, pages 119–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus annotation for mining biomedical events from literature.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<pages>9--10</pages>
<contexts>
<context position="6839" citStr="Kim et al., 2008" startWordPosition="1058" endWordPosition="1061">rgler, 2008). Similar to Rubin et al.’s (2005) work, Thompson et al. (2008) proposed a categorization scheme for epistemic modality in biomedical text according to the type of information expressed (e.g., certainty level, point of view, knowledge type). With the availability of the BioScope corpus (Vincze et al., 2008), in which negation, hedging and their scopes are annotated, studies in detecting speculation scope have also been reported (Morante and Daelemans, 2009; ¨Ozg¨ur and Radev, 2009). Negation and uncertainty of bio-events are also annotated to some extent in the GENIA event corpus (Kim et al., 2008). The BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009) dedicated a task to detecting negation and speculation in biomedical abstracts, based on the GENIA event corpus annotations. Ganter and Strube (2009) elaborated on the link between vagueness in Wikipedia articles indicated by weasel words and hedging. They exploited word frequency measures and shallow syntactic patterns to detect weasel words in Wikipedia articles. 3 Methods Our methodology for hedge detection is essentially rule-based and relies on a combination of lexical and syntactic information. Lexical information is enco</context>
</contexts>
<marker>Kim, Ohta, Tsujii, 2008</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii. 2008. Corpus annotation for mining biomedical events from literature. BMC Bioinformatics, 9:10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of BioNLP’09 Shared Task on Event Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of Natural Language Processing in Biomedicine (BioNLP) NAACL 2009 Workshop,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="2293" citStr="Kim et al., 2009" startWordPosition="348" endWordPosition="351">ng sentiments and opinions in text (see Pang and Lee (2008) for a recent survey), while tentative, speculative nature of scientific writing, particularly in biomedical literature, has provided impetus for recent research in speculation detection (Light et al., 2004). The term hedging is often used as an umbrella term to refer to an array of extra-factual phenomena in natural language and is the focus of the CoNLL-2010 Shared Task on Hedge Detection. The CoNLL-2010 Shared Task on Hedge Detection (Farkas et al., 2010) follows in the steps of the recent BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009), in which one task (speculation and negation detection) was concerned with notions related to hedging in biomedical abstracts. However, the CoNLL-2010 Shared Task differs in several aspects. It sheds light on the pervasiveness of hedging across genres and domains: in addition to biomedical abstracts, it is concerned with biomedical full text articles as well as with Wikipedia articles. Both shared tasks have been concerned with scope resolution; however, their definitions of scope are fundamentally different: the BioNLP’09 Shared Task takes the scope of a speculation instance to be an abstrac</context>
<context position="6905" citStr="Kim et al., 2009" startWordPosition="1069" endWordPosition="1072">l. (2008) proposed a categorization scheme for epistemic modality in biomedical text according to the type of information expressed (e.g., certainty level, point of view, knowledge type). With the availability of the BioScope corpus (Vincze et al., 2008), in which negation, hedging and their scopes are annotated, studies in detecting speculation scope have also been reported (Morante and Daelemans, 2009; ¨Ozg¨ur and Radev, 2009). Negation and uncertainty of bio-events are also annotated to some extent in the GENIA event corpus (Kim et al., 2008). The BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009) dedicated a task to detecting negation and speculation in biomedical abstracts, based on the GENIA event corpus annotations. Ganter and Strube (2009) elaborated on the link between vagueness in Wikipedia articles indicated by weasel words and hedging. They exploited word frequency measures and shallow syntactic patterns to detect weasel words in Wikipedia articles. 3 Methods Our methodology for hedge detection is essentially rule-based and relies on a combination of lexical and syntactic information. Lexical information is encoded in a simple dictionary, and relevant syntactic information is </context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of BioNLP’09 Shared Task on Event Extraction. In Proceedings of Natural Language Processing in Biomedicine (BioNLP) NAACL 2009 Workshop, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="7579" citStr="Klein and Manning, 2003" startWordPosition="1169" endWordPosition="1172">ation in biomedical abstracts, based on the GENIA event corpus annotations. Ganter and Strube (2009) elaborated on the link between vagueness in Wikipedia articles indicated by weasel words and hedging. They exploited word frequency measures and shallow syntactic patterns to detect weasel words in Wikipedia articles. 3 Methods Our methodology for hedge detection is essentially rule-based and relies on a combination of lexical and syntactic information. Lexical information is encoded in a simple dictionary, and relevant syntactic information is identified using the Stanford Lexicalized Parser (Klein and Manning, 2003). We exploit constituent parse trees as well as corresponding collapsed dependency representations (deMarneffe et al., 2006), provided by the parser. 3.1 Detecting Uncertainty in Biological Text For detecting uncertain sentences in biological text (Task 1B), we built on the linguistically-inspired system previously described in detail in Kilicoglu and Bergler (2008). In summary, this system relies on a dictionary of lexical speculation cues, derived from a set of core surface realizations of hedging identified by Hyland (1998) and expanded through WordNet (Fellbaum, 1998) synsets and UMLS SPEC</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41th Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
</authors>
<title>An intensional parametric semantics for vague quantifiers.</title>
<date>2000</date>
<journal>Linguistics and Philosophy,</journal>
<volume>23</volume>
<issue>6</issue>
<contexts>
<context position="11692" citStr="Lappin, 2000" startWordPosition="1817" endWordPosition="1819">desirable feature according to Wikipedia policy. Analysis of Wikipedia training data provided by the organizers revealed that there is overlap between weasel words and hedging cues described in previous section. We, therefore, sought to adapt our dictionary of hedging cues to the task of detecting vagueness in Wikipedia articles. Similar to Task 1B, changes involved eliminating cue categories and adjusting cue weights. In addition, however, we also added a previously unconsidered category of cues, due to their prominence in Wikipedia data as weasel words. This category (vagueness quantifiers (Lappin, 2000)) includes words, such as some, several, many and various, which introduce imprecision when in modifier position. For instance, in the example below, both some and certain contribute to vagueness of the sentence. (1) Even today, some cultures have certain instances of their music intending to imitate natural sounds. For Wikipedia uncertainty detection, eliminated categories included verbs and nouns concerning tendencies (e.g., tend, inclination) in addition to verbs of effort. The only modal auxiliary consistently considered a weasel word was might; therefore, we only kept might in this catego</context>
</contexts>
<marker>Lappin, 2000</marker>
<rawString>Shalom Lappin. 2000. An intensional parametric semantics for vague quantifiers. Linguistics and Philosophy, 23(6):599–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Xin Y Qiu</author>
<author>Padmini Srinivasan</author>
</authors>
<title>The language of bioscience: facts, speculations, and statements in between.</title>
<date>2004</date>
<booktitle>In BioLINK 2004: Linking Biological Literature, Ontologies and Databases,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="1942" citStr="Light et al., 2004" startWordPosition="286" endWordPosition="289">s on extracting factual information, ignoring the wealth of information expressed through such phenomena. In recent years, the need for information extraction and text mining systems to identify and model such extra-factual information has increasingly become clear. For example, online product and movie reviews have provided a rich context for analyzing sentiments and opinions in text (see Pang and Lee (2008) for a recent survey), while tentative, speculative nature of scientific writing, particularly in biomedical literature, has provided impetus for recent research in speculation detection (Light et al., 2004). The term hedging is often used as an umbrella term to refer to an array of extra-factual phenomena in natural language and is the focus of the CoNLL-2010 Shared Task on Hedge Detection. The CoNLL-2010 Shared Task on Hedge Detection (Farkas et al., 2010) follows in the steps of the recent BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009), in which one task (speculation and negation detection) was concerned with notions related to hedging in biomedical abstracts. However, the CoNLL-2010 Shared Task differs in several aspects. It sheds light on the pervasiveness of hedging across gen</context>
<context position="6090" citStr="Light et al. (2004)" startWordPosition="941" endWordPosition="944">), some of which (MODAL,EVIDENTIAL) indicate hedging (Sauriet al., 2006). Saur´ı (2008) exploits modality and polarity to assess the factuality degree of events (whether they correspond to facts, counter-facts or possibilities), and reports on FactBank, a corpus annotated for event factuality (Saur´ı and Pustejovsky, 2009). Wiebe et al. (2005) consider subjectivity in news articles, and focus on the notion of private states, encompassing speculations, opinions, and evaluations in their subjectivity frames. The importance of speculative language in biomedical articles was first acknowledged by Light et al. (2004). Following work in this area focused on detecting speculative sentences (Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). Similar to Rubin et al.’s (2005) work, Thompson et al. (2008) proposed a categorization scheme for epistemic modality in biomedical text according to the type of information expressed (e.g., certainty level, point of view, knowledge type). With the availability of the BioScope corpus (Vincze et al., 2008), in which negation, hedging and their scopes are annotated, studies in detecting speculation scope have also been reported (Morante and Daelemans, </context>
</contexts>
<marker>Light, Qiu, Srinivasan, 2004</marker>
<rawString>Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 2004. The language of bioscience: facts, speculations, and statements in between. In BioLINK 2004: Linking Biological Literature, Ontologies and Databases, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexa T McCray</author>
<author>Suresh Srinivasan</author>
<author>Allen C Browne</author>
</authors>
<title>Lexical methods for managing variation in biomedical terminologies.</title>
<date>1994</date>
<booktitle>In Proceedings of the 18th Annual Symposium on Computer Applications in Medical Care,</booktitle>
<pages>235--239</pages>
<contexts>
<context position="8215" citStr="McCray et al., 1994" startWordPosition="1261" endWordPosition="1264">stituent parse trees as well as corresponding collapsed dependency representations (deMarneffe et al., 2006), provided by the parser. 3.1 Detecting Uncertainty in Biological Text For detecting uncertain sentences in biological text (Task 1B), we built on the linguistically-inspired system previously described in detail in Kilicoglu and Bergler (2008). In summary, this system relies on a dictionary of lexical speculation cues, derived from a set of core surface realizations of hedging identified by Hyland (1998) and expanded through WordNet (Fellbaum, 1998) synsets and UMLS SPECIALIST Lexicon (McCray et al., 1994) nominalizations. A set of lexical certainty markers (unhedgers) are also included, as they indicate hedging when they are negated (e.g., know). These hedging cues are categorized by their type (modal auxiliaries, epistemic verbs, approximative adjectives, etc.) and are weighted to reflect their central/peripheral contribution to hedging, inspired by the fuzzy model of Hyland (1998). We use a scale 71 of 1-5, where 5 is assigned to cues most central to hedging and 1 to those that are most peripheral. For example, the modal auxiliary may has a weight of 5, while a relatively weak hedging cue, t</context>
</contexts>
<marker>McCray, Srinivasan, Browne, 1994</marker>
<rawString>Alexa T. McCray, Suresh Srinivasan, and Allen C. Browne. 1994. Lexical methods for managing variation in biomedical terminologies. In Proceedings of the 18th Annual Symposium on Computer Applications in Medical Care, pages 235–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>992--999</pages>
<contexts>
<context position="6189" citStr="Medlock and Briscoe, 2007" startWordPosition="955" endWordPosition="959">its modality and polarity to assess the factuality degree of events (whether they correspond to facts, counter-facts or possibilities), and reports on FactBank, a corpus annotated for event factuality (Saur´ı and Pustejovsky, 2009). Wiebe et al. (2005) consider subjectivity in news articles, and focus on the notion of private states, encompassing speculations, opinions, and evaluations in their subjectivity frames. The importance of speculative language in biomedical articles was first acknowledged by Light et al. (2004). Following work in this area focused on detecting speculative sentences (Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). Similar to Rubin et al.’s (2005) work, Thompson et al. (2008) proposed a categorization scheme for epistemic modality in biomedical text according to the type of information expressed (e.g., certainty level, point of view, knowledge type). With the availability of the BioScope corpus (Vincze et al., 2008), in which negation, hedging and their scopes are annotated, studies in detecting speculation scope have also been reported (Morante and Daelemans, 2009; ¨Ozg¨ur and Radev, 2009). Negation and uncertainty of bio-events are also annotated to some e</context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock and Ted Briscoe. 2007. Weakly supervised learning for hedge classification in scientific literature. In Proceedings of the 45th Meeting of the Association for Computational Linguistics, pages 992–999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the scope of hedge cues in biomedical texts.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="6694" citStr="Morante and Daelemans, 2009" startWordPosition="1032" endWordPosition="1035"> by Light et al. (2004). Following work in this area focused on detecting speculative sentences (Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). Similar to Rubin et al.’s (2005) work, Thompson et al. (2008) proposed a categorization scheme for epistemic modality in biomedical text according to the type of information expressed (e.g., certainty level, point of view, knowledge type). With the availability of the BioScope corpus (Vincze et al., 2008), in which negation, hedging and their scopes are annotated, studies in detecting speculation scope have also been reported (Morante and Daelemans, 2009; ¨Ozg¨ur and Radev, 2009). Negation and uncertainty of bio-events are also annotated to some extent in the GENIA event corpus (Kim et al., 2008). The BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009) dedicated a task to detecting negation and speculation in biomedical abstracts, based on the GENIA event corpus annotations. Ganter and Strube (2009) elaborated on the link between vagueness in Wikipedia articles indicated by weasel words and hedging. They exploited word frequency measures and shallow syntactic patterns to detect weasel words in Wikipedia articles. 3 Methods Our method</context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. Learning the scope of hedge cues in biomedical texts. In Proceedings of the BioNLP 2009 Workshop, pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arzucan ¨Ozg¨ur</author>
<author>Dragomir R Radev</author>
</authors>
<title>Detecting speculations and their scopes in scientific text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1398--1407</pages>
<marker>¨Ozg¨ur, Radev, 2009</marker>
<rawString>Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detecting speculations and their scopes in scientific text. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1398–1407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Sentiment Analysis and Opinion Mining.</title>
<date>2008</date>
<publisher>Now Publishers Inc,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="1735" citStr="Pang and Lee (2008)" startWordPosition="257" endWordPosition="260">high-precision oriented methodology is extensible and portable. 1 Introduction Natural language is imbued with uncertainty, vagueness, and subjectivity. However, information extraction systems generally focus on extracting factual information, ignoring the wealth of information expressed through such phenomena. In recent years, the need for information extraction and text mining systems to identify and model such extra-factual information has increasingly become clear. For example, online product and movie reviews have provided a rich context for analyzing sentiments and opinions in text (see Pang and Lee (2008) for a recent survey), while tentative, speculative nature of scientific writing, particularly in biomedical literature, has provided impetus for recent research in speculation detection (Light et al., 2004). The term hedging is often used as an umbrella term to refer to an array of extra-factual phenomena in natural language and is the focus of the CoNLL-2010 Shared Task on Hedge Detection. The CoNLL-2010 Shared Task on Hedge Detection (Farkas et al., 2010) follows in the steps of the recent BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009), in which one task (speculation and negat</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Sentiment Analysis and Opinion Mining. Now Publishers Inc, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Robert Knippen</author>
<author>Jessica Littman</author>
<author>Roser Saur´ı</author>
</authors>
<title>Temporal and event information in natural language text.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>2</issue>
<marker>Pustejovsky, Knippen, Littman, Saur´ı, 2005</marker>
<rawString>James Pustejovsky, Robert Knippen, Jessica Littman, and Roser Saur´ı. 2005. Temporal and event information in natural language text. Language Resources and Evaluation, 39(2):123–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria L Rubin</author>
<author>Elizabeth D Liddy</author>
<author>Noriko Kando</author>
</authors>
<title>Certainty identification in texts: Categorization model and manual tagging results.</title>
<date>2005</date>
<booktitle>Computing Attitude and Affect in Text: Theories and Applications,</booktitle>
<volume>20</volume>
<pages>61--76</pages>
<editor>In James G. Shanahan, Yan Qu, and Janyce Wiebe, editors,</editor>
<publisher>Springer</publisher>
<location>Netherlands, Dordrecht.</location>
<contexts>
<context position="5184" citStr="Rubin et al. (2005)" startWordPosition="808" endWordPosition="811">we were ranked near the middle in Task 1, while a more significant focus on scope resolution resulted in fourth place ranking among fifteen systems. We obtained the highest precision in tasks focusing on biological text. Considering that we chose not to exploit the training data provided to the full extent, we believe that our system is viable in terms of extensibility and portability. 2 Related Work Several notions related to hedging have been previously explored in natural language processing. In the news article genre, these have included certainty, modality, and subjectivity. For example, Rubin et al. (2005) proposed a four dimensional model to categorize certainty in news text: certainty level, focus, perspective and time. In the context of TimeML (Pustejovsky et al., 2005), which focuses on temporal expressions in news articles, event modality is encoded using subordination links (SLINKs), some of which (MODAL,EVIDENTIAL) indicate hedging (Sauriet al., 2006). Saur´ı (2008) exploits modality and polarity to assess the factuality degree of events (whether they correspond to facts, counter-facts or possibilities), and reports on FactBank, a corpus annotated for event factuality (Saur´ı and Pustejo</context>
</contexts>
<marker>Rubin, Liddy, Kando, 2005</marker>
<rawString>Victoria L. Rubin, Elizabeth D. Liddy, and Noriko Kando. 2005. Certainty identification in texts: Categorization model and manual tagging results. In James G. Shanahan, Yan Qu, and Janyce Wiebe, editors, Computing Attitude and Affect in Text: Theories and Applications, volume 20, pages 61–76. Springer Netherlands, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
<author>James Pustejovsky</author>
</authors>
<title>FactBank: a corpus annotated with event factuality.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>3</issue>
<marker>Saur´ı, Pustejovsky, 2009</marker>
<rawString>Roser Saur´ı and James Pustejovsky. 2009. FactBank: a corpus annotated with event factuality. Language Resources and Evaluation, 43(3):227–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
<author>Marc Verhagen</author>
<author>James Pustejovsky</author>
</authors>
<title>Annotating and recognizing event modality in text.</title>
<date>2006</date>
<booktitle>In Proceedings of 19th International FLAIRS Conference.</booktitle>
<marker>Saur´ı, Verhagen, Pustejovsky, 2006</marker>
<rawString>Roser Saur´ı, Marc Verhagen, and James Pustejovsky. 2006. Annotating and recognizing event modality in text. In Proceedings of 19th International FLAIRS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
</authors>
<title>A Factuality Profiler for Eventualities in Text.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Brandeis University.</institution>
<marker>Saur´ı, 2008</marker>
<rawString>Roser Saur´ı. 2008. A Factuality Profiler for Eventualities in Text. Ph.D. thesis, Brandeis University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>Hedge classification in biomedical texts with a weakly supervised selection of keywords.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>281--289</pages>
<contexts>
<context position="6204" citStr="Szarvas, 2008" startWordPosition="960" endWordPosition="961">o assess the factuality degree of events (whether they correspond to facts, counter-facts or possibilities), and reports on FactBank, a corpus annotated for event factuality (Saur´ı and Pustejovsky, 2009). Wiebe et al. (2005) consider subjectivity in news articles, and focus on the notion of private states, encompassing speculations, opinions, and evaluations in their subjectivity frames. The importance of speculative language in biomedical articles was first acknowledged by Light et al. (2004). Following work in this area focused on detecting speculative sentences (Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). Similar to Rubin et al.’s (2005) work, Thompson et al. (2008) proposed a categorization scheme for epistemic modality in biomedical text according to the type of information expressed (e.g., certainty level, point of view, knowledge type). With the availability of the BioScope corpus (Vincze et al., 2008), in which negation, hedging and their scopes are annotated, studies in detecting speculation scope have also been reported (Morante and Daelemans, 2009; ¨Ozg¨ur and Radev, 2009). Negation and uncertainty of bio-events are also annotated to some extent in the GE</context>
</contexts>
<marker>Szarvas, 2008</marker>
<rawString>Gy¨orgy Szarvas. 2008. Hedge classification in biomedical texts with a weakly supervised selection of keywords. In Proceedings of the 46th Meeting of the Association for Computational Linguistics, pages 281–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Thompson</author>
<author>Giulia Venturi</author>
<author>John McNaught</author>
<author>Simonetta Montemagni</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Categorising modality in biomedical texts.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC 2008 Workshop on Building and Evaluating Resources for Biomedical Text Mining.</booktitle>
<contexts>
<context position="6297" citStr="Thompson et al. (2008)" startWordPosition="973" endWordPosition="976">facts or possibilities), and reports on FactBank, a corpus annotated for event factuality (Saur´ı and Pustejovsky, 2009). Wiebe et al. (2005) consider subjectivity in news articles, and focus on the notion of private states, encompassing speculations, opinions, and evaluations in their subjectivity frames. The importance of speculative language in biomedical articles was first acknowledged by Light et al. (2004). Following work in this area focused on detecting speculative sentences (Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). Similar to Rubin et al.’s (2005) work, Thompson et al. (2008) proposed a categorization scheme for epistemic modality in biomedical text according to the type of information expressed (e.g., certainty level, point of view, knowledge type). With the availability of the BioScope corpus (Vincze et al., 2008), in which negation, hedging and their scopes are annotated, studies in detecting speculation scope have also been reported (Morante and Daelemans, 2009; ¨Ozg¨ur and Radev, 2009). Negation and uncertainty of bio-events are also annotated to some extent in the GENIA event corpus (Kim et al., 2008). The BioNLP’09 Shared Task on Event Extraction (Kim et al</context>
</contexts>
<marker>Thompson, Venturi, McNaught, Montemagni, Ananiadou, 2008</marker>
<rawString>Paul Thompson, Giulia Venturi, John McNaught, Simonetta Montemagni, and Sophia Ananiadou. 2008. Categorising modality in biomedical texts. In Proceedings of LREC 2008 Workshop on Building and Evaluating Resources for Biomedical Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Gy¨orgy Szarvas</author>
<author>Rich´ard Farkas</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
</authors>
<title>The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<note>Suppl 11:S9.</note>
<marker>Vincze, Szarvas, Farkas, M´ora, Csirik, 2008</marker>
<rawString>Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas, Gy¨orgy M´ora, and J´anos Csirik. 2008. The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC Bioinformatics, 9 Suppl 11:S9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="5816" citStr="Wiebe et al. (2005)" startWordPosition="901" endWordPosition="904">our dimensional model to categorize certainty in news text: certainty level, focus, perspective and time. In the context of TimeML (Pustejovsky et al., 2005), which focuses on temporal expressions in news articles, event modality is encoded using subordination links (SLINKs), some of which (MODAL,EVIDENTIAL) indicate hedging (Sauriet al., 2006). Saur´ı (2008) exploits modality and polarity to assess the factuality degree of events (whether they correspond to facts, counter-facts or possibilities), and reports on FactBank, a corpus annotated for event factuality (Saur´ı and Pustejovsky, 2009). Wiebe et al. (2005) consider subjectivity in news articles, and focus on the notion of private states, encompassing speculations, opinions, and evaluations in their subjectivity frames. The importance of speculative language in biomedical articles was first acknowledged by Light et al. (2004). Following work in this area focused on detecting speculative sentences (Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). Similar to Rubin et al.’s (2005) work, Thompson et al. (2008) proposed a categorization scheme for epistemic modality in biomedical text according to the type of information expres</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2):165–210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>