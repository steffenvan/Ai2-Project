<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008305">
<title confidence="0.991633">
Annotating named entities in clinical text
by combining pre-annotation and active learning
</title>
<author confidence="0.994097">
Maria Skeppstedt
</author>
<affiliation confidence="0.994954">
Dept. of Computer and Systems Sciences (DSV)
</affiliation>
<address confidence="0.505077">
Stockholm University, Forum 100, 164 40 Kista, Sweden
</address>
<email confidence="0.997203">
mariask@dsv.su.se
</email>
<sectionHeader confidence="0.993852" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994605">
For expanding a corpus of clinical text, an-
notated for named entities, a method that
combines pre-tagging with a version of ac-
tive learning is proposed. In order to fa-
cilitate annotation and to avoid bias, two
alternative automatic pre-taggings are pre-
sented to the annotator, without reveal-
ing which of them is given a higher con-
fidence by the pre-tagging system. The
task of the annotator is to select the cor-
rect version among these two alternatives.
To minimise the instances in which none
of the presented pre-taggings is correct,
the texts presented to the annotator are ac-
tively selected from a pool of unlabelled
text, with the selection criterion that one
of the presented pre-taggings should have
a high probability of being correct, while
still being useful for improving the result
of an automatic classifier.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982356">
One of the key challenges for many NLP appli-
cations is to create the annotated corpus needed
for development and evaluation of the application.
Such a corpus is typically created through man-
ual annotation, which is a time-consuming task.
Therefore, there is a need to explore methods for
simplifying the annotation task and for reducing
the amount of data that must be annotated.
Annotation can be simplified by automatic pre-
annotation, in which the task of the annotator is
to improve or correct annotations provided by an
existing system. The amount of data needed to be
annotated can be reduced by active learning, i.e.
by actively selecting data to annotate that is useful
to a machine learning system. When using pre-
tagged data, the annotator might, however, be bi-
ased to choose the annotation provided by the pre-
tagger. Also, if the produced pre-taggings are not
good enough, it is still a time-consuming task to
correct them or select the correct tagging among
many suggestions.
Consequently, there is a need to further explore
how an annotated corpus can be expanded with
less effort and using methods that will not bias the
annotators.
</bodyText>
<sectionHeader confidence="0.987664" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99991425">
The background discusses basic ideas of pre-
annotation and active learning, as well as the parti-
cular challenges associated with annotating clini-
cal text.
</bodyText>
<subsectionHeader confidence="0.995185">
2.1 Annotating clinical text
</subsectionHeader>
<bodyText confidence="0.999984269230769">
A number of text annotation projects have been
carried out in the clinical domain, some of them
including annotations of clinical named entities,
such as mentions of symptoms, diseases and med-
ication. Such studies have for example been
described by Ogren et al. (2008), Chapman et
al. (2008), Roberts et al. (2009), Wang (2009),
Uzuner et al. (2010), Koeling et al. (2011) and Al-
bright et al. (2013).
As in many specialised domains, expert annota-
tors are typically required to create a reliable an-
notated clinical corpus. These expert annotators
are often more expensive than annotators without
the required specialised knowledge. It is also diffi-
cult to use crowdsourcing approaches, such as us-
ing e.g. Amazon’s Mechanical Turk to hire on-
line annotators with the required knowledge (Xia
and Yetisgen-Yildiz, 2012). A further challenge
is posed by the content of the clinical data, which
is often sensitive and should therefore only be ac-
cessed by a limited number of people. Research
community annotation is consequently another op-
tion that is not always open to annotation projects
in the clinical domain, even if there are examples
of such community annotations also for clinical
text, e.g. described by Uzuner et al. (2010).
</bodyText>
<page confidence="0.986188">
74
</page>
<note confidence="0.512907">
Proceedings of the ACL Student Research Workshop, pages 74–80,
</note>
<page confidence="0.363028">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</page>
<bodyText confidence="0.99997075">
To simplify the annotation process, and to min-
imise the amount of annotated data is therefore
even more important for annotations in the clini-
cal domain than for annotation in general.
</bodyText>
<subsectionHeader confidence="0.999873">
2.2 Pre-annotation
</subsectionHeader>
<bodyText confidence="0.999992962962963">
A way to simplify annotation is automatic pre-
annotation (or pre-tagging), in which a text is auto-
matically annotated by an existing system, before
it is given to the annotator. Instead of annotating
unlabelled data, the annotator either corrects mis-
takes made by this existing system (Chou et al.,
2006), or chooses between different taggings pro-
vided by the system (Brants and Plaehn, 2000).
The system providing the pre-annotations could
be rule- or terminology based, not requiring an-
notated data (Mykowiecka and Marciniak, 2011),
as well as a machine learning/hybrid system that
uses the annotations provided by the annotator to
constantly improve the pre-annotation (Tomanek
et al., 2012). There exist several annotation tools
that facilitate the use of pre-annotation by allow-
ing the user to import pre-annotations or by pro-
viding pre-annotation included in the tools (Neves
and Leser, 2012).
A condition for pre-annotation to be useful is
that the produced annotations are good enough, or
the effect can be the opposite, slowing the annota-
tors down (Ogren et al., 2008). Another potential
problem with pre-annotation is that it might bias
towards the annotations given by the pre-tagging,
for instance if a good pre-tagger reduces the atten-
tion of the annotators (Fort and Sagot, 2010).
</bodyText>
<subsectionHeader confidence="0.999162">
2.3 Active learning
</subsectionHeader>
<bodyText confidence="0.999185966666667">
Active learning can be used to reduce the amount
of annotated data needed to successfully train a
machine learning model. Instead of randomly se-
lecting annotation data, instances in the data that
are highly informative, and thereby also highly
useful for the machine learning system, are then
actively selected. (Olsson, 2008, p. 27).
There are several methods for selecting the
most informative instances among the unlabelled
ones in the available pool of data. A frequently
used method is uncertainty sampling, in which in-
stances that the machine learner is least certain
how to classify are selected for annotation. For
a model learning to classify into two classes, in-
stances, for which the classifier has no clear pref-
erence for one of the two alternatives, are chosen
for annotation. If there are more than two classes,
the confidence for the most probable class can be
used as the measure of uncertainty. Only using the
certainty level for the most probable classification
means that not all available information is used,
i.e. the information of the certainty levels for the
less probable classes. (Settles, 2009)
An alternative for a multi-class classifier is
therefore to instead use the difference of the cer-
tainty levels for the two most probable classes. If
cp1 is the most probable class and cp2 is the sec-
ond most probable class for the observation xn,
the margin used for measuring uncertainty for that
instance is:
</bodyText>
<equation confidence="0.996743">
Mn = P(cp1|xn) − P(cp2|xn) (1)
</equation>
<bodyText confidence="0.999840083333333">
An instance with a large margin is easy to clas-
sify because the classifier is much more certain of
the most probable classification than on the second
most probable. Instances with a small margin, on
the other hand, are difficult to classify, and there-
fore instances with a small margin are selected for
annotation (Schein and Ungar, 2007). A common
alternative is to use entropy as an uncertainty mea-
sure, which takes the certainty levels of all possi-
ble classes into account (Settles, 2009).
There are also a number of other possible meth-
ods for selecting informative instances for anno-
tation, for instance to use a committee of learners
and select the instances for which the committee
disagrees the most, or to search for annotation in-
stances that would result in the largest expected
change to the current model (Settles, 2009).
There are also methods to ensure that the se-
lected data correctly reflects the distribution in the
pool of unlabelled data, avoiding a selection of
outliers that would not lead to a correct model of
the available data. Such methods for structured
prediction have been described by Symons et al.
(2006) and Settles and Craven (2008).
Many different machine learning methods have
been used together with active learning for solving
various NLP tasks. Support vector machines have
been used for text classification (Tong and Koller,
2002), using properties of the support vector ma-
chine algorithm for determining what unlabelled
data to select for classification. For structured out-
put tasks, such as named entity recognition, hid-
den markov models have been used by Scheffer
et al. (2001) and conditional random fields (CRF)
by Settles and Craven (2008) and Symons et al.
(2006).
</bodyText>
<page confidence="0.993211">
75
</page>
<bodyText confidence="0.999940125">
Olsson (2008) suggests combining active learn-
ing and pre-annotation for a named entity recogni-
tion task, that is providing the annotator with pre-
tagged data from an actively learned named entity
recogniser. It is proposed not to indiscriminately
pre-tagg the data, but to only provide those pre-
annotated labels to the human annotator, for which
the pre-tagger is relatively certain.
</bodyText>
<sectionHeader confidence="0.978262" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.99991225">
Previous research on pre-annotation shows two
seemingly incompatible desirable properties in a
pre-annotation system. A pre-annotation that is
not good enough might slow the human annota-
tor down, whereas a good pre-annotation might
make the annotator lose concentration, trusting the
pre-annotation too much, resulting in a biased an-
notation. One possibility suggested in previous
research, is to only provide pre-annotations for
which the pre-annotation system is certain of its
classification. For annotations of named entities in
text, this would mean to only provide pre-tagged
entities for which the pre-annotations system is
certain. Such a high precision pre-tagger might,
however, also bias the human annotator towards
not correcting the pre-annotation.
Even more incompatible seems a combination
between pre-annotation and active learning, that
is to provide the human annotator with pre-tagged
data that has been selected for active learning.
The data selected for annotation when using active
learning, is the data for which the pre-annotator is
most uncertain and therefore the data which would
be least suitable for pre-annotation.
The method proposed here aims at finding a
way of combining pre-annotation and active learn-
ing while reducing the risk of annotation bias.
Thereby decreasing the amount of data that needs
to be annotated as well as facilitating the annota-
tion, without introducing bias. A previous version
of this idea has been outlined by Skeppstedt and
Dalianis (2012).
The method is focused on the annotation of
named entities in clinical text, that is marking of
spans of text as well as classification of the spans
into an entity class.
</bodyText>
<subsectionHeader confidence="0.999834">
3.1 Pre-annotation
</subsectionHeader>
<bodyText confidence="0.999993888888889">
As in standard pre-annotation, the annotator will
be presented with pre-tagged data, and does not
have to annotate the data from scratch.
To reduce the bias problem that might be asso-
ciated with pre-tagging, the mode of presentation
will, however, be slightly different in the method
proposed here. Instead of presenting the best tag-
ging for the human annotator to correct, or to
present the n best taggings, the two best taggings
produced by a pre-tagger will be presented, with-
out informing the annotator which of them that the
pre-tagger considers most likely.
When being presented with two possible anno-
tations of the same text without knowing which of
them that the pre-annotation system considers as
most likely, the annotator always has to make an
active choice of which annotation to choose. This
reduces the bias to one particular pre-annotation,
thereby eliminating a drawback associated with
standard pre-annotation. Having to consider two
alternatives might add cognitive load to the anno-
tator compared to correcting one alternative, but
ought to be easier than annotating a text that is not
pre-tagged.
The reason for presenting two annotations, as
opposed to three or more, is that it is relatively
easy to compare two texts, letting your eyes wan-
der from one text to the other, when you have one
comparison to make. Having three optional an-
notations would result in three comparisons, and
having four would result in six comparisons, and
so on. Therefore, having two optional annotations
to choose from, reduces the bias problem while at
the same time still offering a method for speeding
up the annotation.
A simple Java program for choosing between
two alternative pre-annotated sentences has been
created (Figure 1). The program randomly
chooses in which of the two text boxes to place
which pre-annotation. The user can either choose
the left or the right annotation, or that none of them
is correct.
The data will be split into sentences, and one
sentence at time will be presented to the annotator
for annotation.
</bodyText>
<subsectionHeader confidence="0.998571">
3.2 Active learning
</subsectionHeader>
<bodyText confidence="0.999668571428571">
To choose from two presented annotations might
also potentially be faster than making corrections
to one presented annotation. For this to be the
case, however, one of the presented annotations
has to be a correct annotation. In order to achieve
that, the proposed method is to use a version of
active learning.
</bodyText>
<page confidence="0.965524">
76
</page>
<figureCaption confidence="0.991474">
Figure 1: A simple program for choosing between two alternative annotations, showing a constructed
example in English.
</figureCaption>
<bodyText confidence="0.999350303030303">
The standard use of active learning is to actively
select instances to annotate that are useful to a ma-
chine learner. Instances for which the machine
learning model can make a confident classifica-
tion are not presented to the annotator, as these
instances will be of little benefit for improving the
machine learning system.
The version of active learning proposed here is
retaining this general idea of active learning, but
is also adding an additional constraint to what in-
stances that are actively selected for annotation.
This constraint is to only select text passages for
which it is probable that one of the two best
pre-taggings is correct, i.e. the pre-tagger has to
be confident that one of the two presented pre-
annotations is correct, but it should be uncertain
as to which one of them is correct.
For ensuring that the sentences selected for an-
notation are informative enough, the previously
described difference of the certainty level of the
two most probable classes will be used. The same
standard for expressing margin as used in (1), can
be used here, except that in (1), cp1 and cp2 stand
for classification of one instance, whereas in this
case the output is a sequence of labels, labelling
each token in a sentence. Therefore, cp1 and cp2
stand for the classification of a sequence of labels.
Let cp1 be the most probable labelling sequence,
cp2 the second most probable labelling sequence
and cp3 the third most probable labelling sequence.
Moreover, let xn be the observations in sentence
n, then the following margins can be defined for
that sentence:
</bodyText>
<equation confidence="0.783954">
MtoSecond n = P (cp1|xn) − P(cp2|xn) (2)
MtoThird n = P(cp1|xn) − P(cp3|xn) (3)
</equation>
<bodyText confidence="0.999954666666667">
To make the probability high that one of the
two presented pre-annotations is correct, the same
method that is used for determining that an an-
notation instance is informative enough could be
used. However, instead of minimising the margin
between two classification instances, it is ensured
that the margin in high enough. That is, the differ-
ence in certainty level between the two most prob-
able annotations and the third most probable must
be high enough to make it probable that one of the
two best classification candidates is correct. This
can be achieved by forcing MtoThird to be above a
threshold, t.
The criteria for selecting the next candidate sen-
tence to annotate can then be described as:
</bodyText>
<equation confidence="0.994329">
x∗ = argmin P(cp1|x) − P(cp2|x) (4)
x
</equation>
<bodyText confidence="0.805302">
where
</bodyText>
<equation confidence="0.987776">
P(cp1|x) − P(cp3|x) &gt; t
</equation>
<bodyText confidence="0.9999718125">
As instances with the highest possible P(cp2|x)
in relation to P(cp1|x) are favoured, no threshold
for the margin between P(cp2|x) and P(cp3|x) is
needed.
It might be difficult to automatically determine
an appropriate value of the threshold t. Therefore,
the proposed method for finding a good threshold,
is to adapt it to the behaviour of the annotator. If
the annotator often rejects the two presented pre-
taggings, text passages for which the pre-tagger is
more certain ought to be selected, that is the value
of t ought to be increased. On the other hand,
if one of the presented pre-taggings often is se-
lected by the annotator as the correct annotation,
the value of t can be decreased, possibly allowing
for annotation instances with a smaller MtoSecond.
</bodyText>
<subsectionHeader confidence="0.993161">
3.3 Machine learning system
</subsectionHeader>
<bodyText confidence="0.996653">
As machine learning system, the conditional ran-
dom fields system CRF++ (Kudo, 2013) will be
</bodyText>
<page confidence="0.996811">
77
</page>
<bodyText confidence="0.99955425">
used. This system uses a combination of forward
Viterbi and backward A* search for finding the
best classification sequence for an input sentence,
given the trained model. It can also produce the
n-best classification sequences for each sentence,
which is necessary for the proposed pre-tagger that
presents the two best pre-taggings to the human
annotator.
CRF++ can also give the conditional probably
for the output, that is for the entire classification
sequence of a sentence, which is needed in the pro-
posed active learning algorithm.
</bodyText>
<subsectionHeader confidence="0.899253">
3.4 Materials
</subsectionHeader>
<bodyText confidence="0.999992703703704">
There is a corpus of Swedish clinical text, i.e.
the text in the narrative part of the health record,
that contains clinical text from the Stockholm area,
from the years 2006-2008 (Dalianis et al., 2009).
A subset of this corpus, containing texts from an
emergency unit of internal medicine, has been an-
notated for four types of named entities: disorder,
finding, pharmaceutical drug and body structure
(Skeppstedt et al., 2012). For approximately one
third of this annotated corpus, double annotation
has been performed, and the instances, for which
there were a disagreement, have been resolved by
one of the annotators.
The annotated corpus will form the main source
of materials for the study proposed here, and addi-
tional data to annotate will be selected from a pool
of unlabelled data from internal medicine emer-
gency notes.
The larger subset of the annotated data, only
annotated by one annotator, will be referred to
as Single (containing 45 482 tokens), and the
smaller subset, annotated by two annotators, will
be referred to as Double (containing 25 370 to-
kens). The Single subset will be the main source
for developing the pre-annotation/active learning
method, whereas the Double subset will be used
for a final evaluation.
</bodyText>
<subsectionHeader confidence="0.82347">
3.5 Step-by-step explanation
</subsectionHeader>
<bodyText confidence="0.941095789473684">
The proposed method can be divided into 8 steps:
1. Train a CRF model with a randomly selected
subset of the Single part of the annotated cor-
pus, the seed set. The size of this seed set, as
well as suitable features for the CRF model
will be evaluated using cross validation on
the seed set. The size should be as small as
possible, limiting the amount of initial anno-
tation needed, but large enough to have re-
sults in line with a baseline system using ter-
minology matching for named entity recog-
nition (Skeppstedt et al., 2012).
2. Apply the constructed CRF model on unla-
belled data from the pool of data from in-
ternal medicine emergency notes. Let the
model, which operates on a sentence level,
provide the three most probable label se-
quences for each sentence, together with its
level of certainty.
</bodyText>
<listItem confidence="0.9571276875">
3. Calculate the difference in certainty be-
tween the most probable and the third most
probable suggestion sequence for each sen-
tence, that is MtoThird. Start with a low
threshold t and place all sentences with
MtoThird above the threshold t in a list of
candidates for presenting to the annotator
(that is the sentences fulfilling the criterion
P(cp1|x) − P(cp3|x) &gt; t).
4. Order the sentences in the list of se-
lected candidates in increasing order of
MtoSecond. Present the sentence with the
lowest MtoSecond to the annotator. This is the
sentence, for which the pre-tagger is most un-
certain of which one of the two most probable
pre-taggings is correct.
</listItem>
<bodyText confidence="0.997436785714286">
Present the most probable pre-annotation
as well as the second most probable pre-
annotation, as shown in Figure 1.
5. If the annotator chooses that none of the pre-
sented pre-annotations is correct, discard the
previous candidate selection and make a new
one from the pool with a higher threshold
value t. Again, order the sentences in increas-
ing order of MtoSecond, and present the sen-
tence with the lowest MtoSecond to the anno-
tator.
Repeat step 3., 4. and 5., gradually increasing
the threshold until the annotator accepts one
of the presented pre-annotations.
</bodyText>
<listItem confidence="0.964694">
6. Continue presenting the annotator with the
two most probable pre-annotations for the
sentences in the list of selected candidate
sentences, and allow the human annotator to
choose one of the pre-annotations.
</listItem>
<page confidence="0.996518">
78
</page>
<bodyText confidence="0.945268133333333">
The threshold t could be further adjusted ac-
cording to how often the option ’None’ is
chosen.
7. Each selected annotation is added to a set
of annotated data. When a sufficiently large
amount of new sentences have been added to
this set, the model needs to be retrained with
the new data. The retraining of the model can
be carried out as a background process while
the human annotator is annotating. In or-
der to use the annotator time efficiently, there
should not be any waiting time while retrain-
ing.
8. When the model has been retrained, the pro-
cess starts over from step 2.
</bodyText>
<subsectionHeader confidence="0.974109">
3.6 Evaluation
</subsectionHeader>
<bodyText confidence="0.99998044">
The text passages chosen in the selection process
will, as explained above, be used to re-train the
machine learning model, and used when select-
ing new text passages for annotation. The effect
of adding additional annotations will also be con-
stantly measured, using cross validation on the
seed set. The additional data added by the active
learning experiments will, however, not be used
in the validation part of the cross validation, but
only be used as additional training data, in order to
make sure that the results are not improved due to
easily classified examples being added to the cor-
pus.
When an actively selected corpus of the same
size as the entire Single subset of the corpus has
been created, this actively selected corpus will be
used for training a machine learning model. The
performance of this model will then be compared
to a model trained on the single subset. Both mod-
els will be evaluated on the Double subset of the
corpus. The hypothesis is that the machine learn-
ing model trained on the corpus partly created by
pre-tagging and active learning will perform bet-
ter than the model created on the original Single
subset.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999829294117647">
A method that combines pre-annotation and active
learning, while reducing annotation bias, is pro-
posed. A program for presenting pre-annotated
data to the human annotator for selection has been
constructed, and a corpus of annotated data suit-
able as a seed set and as evaluation data has
been constructed. The active learning part of the
proposed method remains, however, to be imple-
mented.
Applying the proposed methods aims at creat-
ing a corpus suitable for training a machine learn-
ing system to recognise the four entities Disorder,
Finding, Pharmaceutical drug and Body struc-
ture. Moreover, methods for facilitating annotated
corpus construction will be explored, potentially
adding new knowledge to the science of annota-
tion.
</bodyText>
<sectionHeader confidence="0.99473" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999946714285714">
I am very grateful to the reviewers and the pre-
submission mentor for their many valuable com-
ments. I would also like to thank Hercules Dalia-
nis and Magnus Ahltorp as well as the participants
of the ’Southern California Workshop on Medical
Text Analysis and Visualization’ for fruitful dis-
cussions on the proposed method.
</bodyText>
<sectionHeader confidence="0.997918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995893548387097">
Daniel Albright, Arrick Lanfranchi, Anwen Fredrik-
sen, William F 4th Styler, Colin Warner, Jena D
Hwang, Jinho D Choi, Dmitriy Dligach, Rod-
ney D Nielsen, James Martin, Wayne Ward, Martha
Palmer, and Guergana K Savova. 2013. Towards
comprehensive syntactic and semantic annotations
of the clinical narrative. J Am Med Inform Assoc,
Jan.
Thorsten Brants and Oliver Plaehn. 2000. Interactive
corpus annotation. In LREC. European Language
Resources Association.
Wendy W Chapman, John N Dowling, and George
Hripcsak. 2008. Evaluation of training with an an-
notation schema for manual annotation of clinical
conditions from emergency department reports. Int
J Med Inform, Epub 2007 Feb 20, 77(2):107–113,
February.
Wen-chi Chou, Richard Tzong-han Tsai, and Ying-
shan Su. 2006. A semi-automatic method for anno-
tating a biomedical proposition bank. In FLAC’06.
ACL.
Hercules Dalianis, Martin Hassel, and Sumithra
Velupillai. 2009. The Stockholm EPR Corpus -
Characteristics and Some Initial Findings. In Pro-
ceedings of ISHIMR 2009, Evaluation and imple-
mentation of e-health and health information initia-
tives: international perspectives. 14th International
Symposium for Health Information Management Re-
search, Kalmar, Sweden, pages 243–249.
Kar¨en Fort and Benoit Sagot. 2010. Influence of
pre-annotation on pos-tagged corpus development.
</reference>
<page confidence="0.992569">
79
</page>
<reference confidence="0.992086777777778">
In Proceedings of the Fourth Linguistic Annotation
Workshop, LAW IV ’10, pages 56–63, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Rob Koeling, John Carroll, Rosemary Tate, and
Amanda Nicholson. 2011. Annotating a corpus of
clinical text records for learning to recognize symp-
toms automatically. In Proceedings of the LOUHI
2011, Third International Workshop on Health Doc-
ument Text Mining and Information Analysis.
Taku Kudo. 2013. CRF++: Yet Another CRF toolkit.
http://crfpp.sourceforge.net/. Accessed 2013-05-21.
Agnieszka Mykowiecka and Małgorzata Marciniak.
2011. Some remarks on automatic semantic an-
notation of a medical corpus. In Proceedings of
the LOUHI 2011, Third International Workshop
on Health Document Text Mining and Information
Analysis.
Mariana Neves and Ulf Leser. 2012. A survey on an-
notation tools for the biomedical literature. Brief-
ings in Bioinformatics.
Philip Ogren, Guergana Savova, and Christopher
Chute. 2008. Constructing evaluation corpora for
automated clinical named entity recognition. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC’08), pages 3143–
3149, Marrakech, Morocco, May. European Lan-
guage Resources Association (ELRA).
Fredrik Olsson. 2008. Bootstrapping Named Entity
Annotation by Means of Active Machine Learning.
Ph.D. thesis, University of Gothenburg. Faculty of
Arts.
Angus Roberts, Robert Gaizauskas, Mark Hepple,
George Demetriou, Yikun Guo, Ian Roberts, and
Andrea Setzer. 2009. Building a semantically an-
notated corpus of clinical texts. J. of Biomedical In-
formatics, 42:950–966, October.
Tobias Scheffer, Christian Decomain, and Stefan Wro-
bel. 2001. Active hidden markov models for in-
formation extraction. In Proceedings of the 4th In-
ternational Conference on Advances in Intelligent
Data Analysis, IDA ’01, pages 309–318, London,
UK, UK. Springer-Verlag.
Andrew I. Schein and Lyle H. Ungar. 2007. Ac-
tive learning for logistic regression: an evaluation.
Mach. Learn., 68(3):235–265, October.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’08, pages 1070–1079, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, Univer-
sity of Wisconsin–Madison.
Maria Skeppstedt and Hercules Dalianis. 2012. Using
active learning and pre-tagging for annotating clin-
ical findings in health record text. In Proceedings
of SMBM 2012 - The 5th International Symposium
on Semantic Mining in Biomedicine, pages 98–99,
Zurich, Switzerland, September 3-4.
Maria Skeppstedt, Maria Kvist, and Hercules Dalianis.
2012. Rule-based entity recognition and coverage of
SNOMED CT in Swedish clinical text. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC’12), pages
1250–1257, Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).
Christopher T. Symons, Nagiza F. Samatova, Ramya
Krishnamurthy, Byung H. Park, Tarik Umar, David
Buttler, Terence Critchlow, and David Hysom.
2006. Multi-criterion active learning in conditional
random fields. In Proceedings of the 18th IEEE In-
ternational Conference on Tools with Artificial In-
telligence, ICTAI ’06, pages 323–331, Washington,
DC, USA. IEEE Computer Society.
Katrin Tomanek, Philipp Daumke, Frank Enders, Jens
Huber, Katharina Theres, and Marcel M¨uller. 2012.
An interactive de-identification-system. In Proceed-
ings of SMBM 2012 - The 5th International Sympo-
sium on Semantic Mining in Biomedicine, pages 82–
86, Zurich, Switzerland, September 3-4.
Simon Tong and Daphne Koller. 2002. Support
vector machine active learning with applications to
text classification. J. Mach. Learn. Res., 2:45–66,
March.
¨Ozlem Uzuner, Imre Solti, Fei Xia, and Eithon
Cadag. 2010. Community annotation experiment
for ground truth generation for the i2b2 medication
challenge. JAm Med Inform Assoc, 17(5):519–523.
Yefeng Wang. 2009. Annotating and recognising
named entities in clinical notes. In Proceedings of
the ACL-IJCNLP Student Research Workshop, pages
18–26, Singapore.
Fei Xia and Meliha Yetisgen-Yildiz. 2012. Clinical
corpus annotation: Challenges and strategies. In
The Third Workshop on Building and Evaluating Re-
sources for Biomedical Text Mining (BioTxtM), an
LREC Workshop. Turkey.
</reference>
<page confidence="0.998238">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.725175">
<title confidence="0.9158455">Annotating named entities in clinical by combining pre-annotation and active learning</title>
<author confidence="0.957677">Maria</author>
<affiliation confidence="0.968115">Dept. of Computer and Systems Sciences Stockholm University, Forum 100, 164 40 Kista,</affiliation>
<email confidence="0.952582">mariask@dsv.su.se</email>
<abstract confidence="0.999121238095238">For expanding a corpus of clinical text, annotated for named entities, a method that combines pre-tagging with a version of active learning is proposed. In order to facilitate annotation and to avoid bias, two alternative automatic pre-taggings are presented to the annotator, without revealing which of them is given a higher confidence by the pre-tagging system. The task of the annotator is to select the correct version among these two alternatives. To minimise the instances in which none of the presented pre-taggings is correct, the texts presented to the annotator are actively selected from a pool of unlabelled text, with the selection criterion that one of the presented pre-taggings should have a high probability of being correct, while still being useful for improving the result of an automatic classifier.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Albright</author>
<author>Arrick Lanfranchi</author>
<author>Anwen Fredriksen</author>
<author>William F 4th Styler</author>
<author>Colin Warner</author>
<author>Jena D Hwang</author>
<author>Jinho D Choi</author>
<author>Dmitriy Dligach</author>
<author>Rodney D Nielsen</author>
<author>James Martin</author>
<author>Wayne Ward</author>
<author>Martha Palmer</author>
<author>Guergana K Savova</author>
</authors>
<title>Towards comprehensive syntactic and semantic annotations of the clinical narrative.</title>
<date>2013</date>
<journal>J Am Med Inform Assoc,</journal>
<contexts>
<context position="2811" citStr="Albright et al. (2013)" startWordPosition="455" endWordPosition="459">l not bias the annotators. 2 Background The background discusses basic ideas of preannotation and active learning, as well as the particular challenges associated with annotating clinical text. 2.1 Annotating clinical text A number of text annotation projects have been carried out in the clinical domain, some of them including annotations of clinical named entities, such as mentions of symptoms, diseases and medication. Such studies have for example been described by Ogren et al. (2008), Chapman et al. (2008), Roberts et al. (2009), Wang (2009), Uzuner et al. (2010), Koeling et al. (2011) and Albright et al. (2013). As in many specialised domains, expert annotators are typically required to create a reliable annotated clinical corpus. These expert annotators are often more expensive than annotators without the required specialised knowledge. It is also difficult to use crowdsourcing approaches, such as using e.g. Amazon’s Mechanical Turk to hire online annotators with the required knowledge (Xia and Yetisgen-Yildiz, 2012). A further challenge is posed by the content of the clinical data, which is often sensitive and should therefore only be accessed by a limited number of people. Research community anno</context>
</contexts>
<marker>Albright, Lanfranchi, Fredriksen, Styler, Warner, Hwang, Choi, Dligach, Nielsen, Martin, Ward, Palmer, Savova, 2013</marker>
<rawString>Daniel Albright, Arrick Lanfranchi, Anwen Fredriksen, William F 4th Styler, Colin Warner, Jena D Hwang, Jinho D Choi, Dmitriy Dligach, Rodney D Nielsen, James Martin, Wayne Ward, Martha Palmer, and Guergana K Savova. 2013. Towards comprehensive syntactic and semantic annotations of the clinical narrative. J Am Med Inform Assoc, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Oliver Plaehn</author>
</authors>
<title>Interactive corpus annotation. In</title>
<date>2000</date>
<booktitle>LREC. European Language Resources Association.</booktitle>
<contexts>
<context position="4385" citStr="Brants and Plaehn, 2000" startWordPosition="705" endWordPosition="708">or Computational Linguistics To simplify the annotation process, and to minimise the amount of annotated data is therefore even more important for annotations in the clinical domain than for annotation in general. 2.2 Pre-annotation A way to simplify annotation is automatic preannotation (or pre-tagging), in which a text is automatically annotated by an existing system, before it is given to the annotator. Instead of annotating unlabelled data, the annotator either corrects mistakes made by this existing system (Chou et al., 2006), or chooses between different taggings provided by the system (Brants and Plaehn, 2000). The system providing the pre-annotations could be rule- or terminology based, not requiring annotated data (Mykowiecka and Marciniak, 2011), as well as a machine learning/hybrid system that uses the annotations provided by the annotator to constantly improve the pre-annotation (Tomanek et al., 2012). There exist several annotation tools that facilitate the use of pre-annotation by allowing the user to import pre-annotations or by providing pre-annotation included in the tools (Neves and Leser, 2012). A condition for pre-annotation to be useful is that the produced annotations are good enough</context>
</contexts>
<marker>Brants, Plaehn, 2000</marker>
<rawString>Thorsten Brants and Oliver Plaehn. 2000. Interactive corpus annotation. In LREC. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy W Chapman</author>
<author>John N Dowling</author>
<author>George Hripcsak</author>
</authors>
<title>Evaluation of training with an annotation schema for manual annotation of clinical conditions from emergency department reports.</title>
<date>2008</date>
<journal>Int J Med Inform, Epub</journal>
<pages>77--2</pages>
<contexts>
<context position="2703" citStr="Chapman et al. (2008)" startWordPosition="436" endWordPosition="439">need to further explore how an annotated corpus can be expanded with less effort and using methods that will not bias the annotators. 2 Background The background discusses basic ideas of preannotation and active learning, as well as the particular challenges associated with annotating clinical text. 2.1 Annotating clinical text A number of text annotation projects have been carried out in the clinical domain, some of them including annotations of clinical named entities, such as mentions of symptoms, diseases and medication. Such studies have for example been described by Ogren et al. (2008), Chapman et al. (2008), Roberts et al. (2009), Wang (2009), Uzuner et al. (2010), Koeling et al. (2011) and Albright et al. (2013). As in many specialised domains, expert annotators are typically required to create a reliable annotated clinical corpus. These expert annotators are often more expensive than annotators without the required specialised knowledge. It is also difficult to use crowdsourcing approaches, such as using e.g. Amazon’s Mechanical Turk to hire online annotators with the required knowledge (Xia and Yetisgen-Yildiz, 2012). A further challenge is posed by the content of the clinical data, which is </context>
</contexts>
<marker>Chapman, Dowling, Hripcsak, 2008</marker>
<rawString>Wendy W Chapman, John N Dowling, and George Hripcsak. 2008. Evaluation of training with an annotation schema for manual annotation of clinical conditions from emergency department reports. Int J Med Inform, Epub 2007 Feb 20, 77(2):107–113, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-chi Chou</author>
<author>Richard Tzong-han Tsai</author>
<author>Yingshan Su</author>
</authors>
<title>A semi-automatic method for annotating a biomedical proposition bank.</title>
<date>2006</date>
<booktitle>In FLAC’06. ACL.</booktitle>
<contexts>
<context position="4297" citStr="Chou et al., 2006" startWordPosition="691" endWordPosition="694">arch Workshop, pages 74–80, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics To simplify the annotation process, and to minimise the amount of annotated data is therefore even more important for annotations in the clinical domain than for annotation in general. 2.2 Pre-annotation A way to simplify annotation is automatic preannotation (or pre-tagging), in which a text is automatically annotated by an existing system, before it is given to the annotator. Instead of annotating unlabelled data, the annotator either corrects mistakes made by this existing system (Chou et al., 2006), or chooses between different taggings provided by the system (Brants and Plaehn, 2000). The system providing the pre-annotations could be rule- or terminology based, not requiring annotated data (Mykowiecka and Marciniak, 2011), as well as a machine learning/hybrid system that uses the annotations provided by the annotator to constantly improve the pre-annotation (Tomanek et al., 2012). There exist several annotation tools that facilitate the use of pre-annotation by allowing the user to import pre-annotations or by providing pre-annotation included in the tools (Neves and Leser, 2012). A co</context>
</contexts>
<marker>Chou, Tsai, Su, 2006</marker>
<rawString>Wen-chi Chou, Richard Tzong-han Tsai, and Yingshan Su. 2006. A semi-automatic method for annotating a biomedical proposition bank. In FLAC’06. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hercules Dalianis</author>
<author>Martin Hassel</author>
<author>Sumithra Velupillai</author>
</authors>
<title>The Stockholm EPR Corpus -Characteristics and Some Initial Findings.</title>
<date>2009</date>
<booktitle>In Proceedings of ISHIMR 2009, Evaluation and implementation of e-health and health information initiatives: international perspectives. 14th International Symposium for Health Information Management Research,</booktitle>
<pages>243--249</pages>
<location>Kalmar, Sweden,</location>
<contexts>
<context position="17120" citStr="Dalianis et al., 2009" startWordPosition="2785" endWordPosition="2788">t sentence, given the trained model. It can also produce the n-best classification sequences for each sentence, which is necessary for the proposed pre-tagger that presents the two best pre-taggings to the human annotator. CRF++ can also give the conditional probably for the output, that is for the entire classification sequence of a sentence, which is needed in the proposed active learning algorithm. 3.4 Materials There is a corpus of Swedish clinical text, i.e. the text in the narrative part of the health record, that contains clinical text from the Stockholm area, from the years 2006-2008 (Dalianis et al., 2009). A subset of this corpus, containing texts from an emergency unit of internal medicine, has been annotated for four types of named entities: disorder, finding, pharmaceutical drug and body structure (Skeppstedt et al., 2012). For approximately one third of this annotated corpus, double annotation has been performed, and the instances, for which there were a disagreement, have been resolved by one of the annotators. The annotated corpus will form the main source of materials for the study proposed here, and additional data to annotate will be selected from a pool of unlabelled data from intern</context>
</contexts>
<marker>Dalianis, Hassel, Velupillai, 2009</marker>
<rawString>Hercules Dalianis, Martin Hassel, and Sumithra Velupillai. 2009. The Stockholm EPR Corpus -Characteristics and Some Initial Findings. In Proceedings of ISHIMR 2009, Evaluation and implementation of e-health and health information initiatives: international perspectives. 14th International Symposium for Health Information Management Research, Kalmar, Sweden, pages 243–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kar¨en Fort</author>
<author>Benoit Sagot</author>
</authors>
<title>Influence of pre-annotation on pos-tagged corpus development.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth Linguistic Annotation Workshop, LAW IV ’10,</booktitle>
<pages>56--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5286" citStr="Fort and Sagot, 2010" startWordPosition="846" endWordPosition="849">(Tomanek et al., 2012). There exist several annotation tools that facilitate the use of pre-annotation by allowing the user to import pre-annotations or by providing pre-annotation included in the tools (Neves and Leser, 2012). A condition for pre-annotation to be useful is that the produced annotations are good enough, or the effect can be the opposite, slowing the annotators down (Ogren et al., 2008). Another potential problem with pre-annotation is that it might bias towards the annotations given by the pre-tagging, for instance if a good pre-tagger reduces the attention of the annotators (Fort and Sagot, 2010). 2.3 Active learning Active learning can be used to reduce the amount of annotated data needed to successfully train a machine learning model. Instead of randomly selecting annotation data, instances in the data that are highly informative, and thereby also highly useful for the machine learning system, are then actively selected. (Olsson, 2008, p. 27). There are several methods for selecting the most informative instances among the unlabelled ones in the available pool of data. A frequently used method is uncertainty sampling, in which instances that the machine learner is least certain how </context>
</contexts>
<marker>Fort, Sagot, 2010</marker>
<rawString>Kar¨en Fort and Benoit Sagot. 2010. Influence of pre-annotation on pos-tagged corpus development. In Proceedings of the Fourth Linguistic Annotation Workshop, LAW IV ’10, pages 56–63, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>John Carroll</author>
<author>Rosemary Tate</author>
<author>Amanda Nicholson</author>
</authors>
<title>Annotating a corpus of clinical text records for learning to recognize symptoms automatically.</title>
<date>2011</date>
<booktitle>In Proceedings of the LOUHI 2011, Third International Workshop on Health Document Text Mining and Information Analysis.</booktitle>
<contexts>
<context position="2784" citStr="Koeling et al. (2011)" startWordPosition="450" endWordPosition="453">and using methods that will not bias the annotators. 2 Background The background discusses basic ideas of preannotation and active learning, as well as the particular challenges associated with annotating clinical text. 2.1 Annotating clinical text A number of text annotation projects have been carried out in the clinical domain, some of them including annotations of clinical named entities, such as mentions of symptoms, diseases and medication. Such studies have for example been described by Ogren et al. (2008), Chapman et al. (2008), Roberts et al. (2009), Wang (2009), Uzuner et al. (2010), Koeling et al. (2011) and Albright et al. (2013). As in many specialised domains, expert annotators are typically required to create a reliable annotated clinical corpus. These expert annotators are often more expensive than annotators without the required specialised knowledge. It is also difficult to use crowdsourcing approaches, such as using e.g. Amazon’s Mechanical Turk to hire online annotators with the required knowledge (Xia and Yetisgen-Yildiz, 2012). A further challenge is posed by the content of the clinical data, which is often sensitive and should therefore only be accessed by a limited number of peop</context>
</contexts>
<marker>Koeling, Carroll, Tate, Nicholson, 2011</marker>
<rawString>Rob Koeling, John Carroll, Rosemary Tate, and Amanda Nicholson. 2011. Annotating a corpus of clinical text records for learning to recognize symptoms automatically. In Proceedings of the LOUHI 2011, Third International Workshop on Health Document Text Mining and Information Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
</authors>
<date>2013</date>
<booktitle>CRF++: Yet Another CRF toolkit. http://crfpp.sourceforge.net/. Accessed</booktitle>
<pages>2013--05</pages>
<contexts>
<context position="16351" citStr="Kudo, 2013" startWordPosition="2662" endWordPosition="2663">roposed method for finding a good threshold, is to adapt it to the behaviour of the annotator. If the annotator often rejects the two presented pretaggings, text passages for which the pre-tagger is more certain ought to be selected, that is the value of t ought to be increased. On the other hand, if one of the presented pre-taggings often is selected by the annotator as the correct annotation, the value of t can be decreased, possibly allowing for annotation instances with a smaller MtoSecond. 3.3 Machine learning system As machine learning system, the conditional random fields system CRF++ (Kudo, 2013) will be 77 used. This system uses a combination of forward Viterbi and backward A* search for finding the best classification sequence for an input sentence, given the trained model. It can also produce the n-best classification sequences for each sentence, which is necessary for the proposed pre-tagger that presents the two best pre-taggings to the human annotator. CRF++ can also give the conditional probably for the output, that is for the entire classification sequence of a sentence, which is needed in the proposed active learning algorithm. 3.4 Materials There is a corpus of Swedish clini</context>
</contexts>
<marker>Kudo, 2013</marker>
<rawString>Taku Kudo. 2013. CRF++: Yet Another CRF toolkit. http://crfpp.sourceforge.net/. Accessed 2013-05-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agnieszka Mykowiecka</author>
<author>Małgorzata Marciniak</author>
</authors>
<title>Some remarks on automatic semantic annotation of a medical corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the LOUHI 2011, Third International Workshop on Health Document Text Mining and Information Analysis.</booktitle>
<contexts>
<context position="4526" citStr="Mykowiecka and Marciniak, 2011" startWordPosition="725" endWordPosition="728">mportant for annotations in the clinical domain than for annotation in general. 2.2 Pre-annotation A way to simplify annotation is automatic preannotation (or pre-tagging), in which a text is automatically annotated by an existing system, before it is given to the annotator. Instead of annotating unlabelled data, the annotator either corrects mistakes made by this existing system (Chou et al., 2006), or chooses between different taggings provided by the system (Brants and Plaehn, 2000). The system providing the pre-annotations could be rule- or terminology based, not requiring annotated data (Mykowiecka and Marciniak, 2011), as well as a machine learning/hybrid system that uses the annotations provided by the annotator to constantly improve the pre-annotation (Tomanek et al., 2012). There exist several annotation tools that facilitate the use of pre-annotation by allowing the user to import pre-annotations or by providing pre-annotation included in the tools (Neves and Leser, 2012). A condition for pre-annotation to be useful is that the produced annotations are good enough, or the effect can be the opposite, slowing the annotators down (Ogren et al., 2008). Another potential problem with pre-annotation is that </context>
</contexts>
<marker>Mykowiecka, Marciniak, 2011</marker>
<rawString>Agnieszka Mykowiecka and Małgorzata Marciniak. 2011. Some remarks on automatic semantic annotation of a medical corpus. In Proceedings of the LOUHI 2011, Third International Workshop on Health Document Text Mining and Information Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariana Neves</author>
<author>Ulf Leser</author>
</authors>
<title>A survey on annotation tools for the biomedical literature. Briefings in Bioinformatics.</title>
<date>2012</date>
<contexts>
<context position="4891" citStr="Neves and Leser, 2012" startWordPosition="781" endWordPosition="784">g system (Chou et al., 2006), or chooses between different taggings provided by the system (Brants and Plaehn, 2000). The system providing the pre-annotations could be rule- or terminology based, not requiring annotated data (Mykowiecka and Marciniak, 2011), as well as a machine learning/hybrid system that uses the annotations provided by the annotator to constantly improve the pre-annotation (Tomanek et al., 2012). There exist several annotation tools that facilitate the use of pre-annotation by allowing the user to import pre-annotations or by providing pre-annotation included in the tools (Neves and Leser, 2012). A condition for pre-annotation to be useful is that the produced annotations are good enough, or the effect can be the opposite, slowing the annotators down (Ogren et al., 2008). Another potential problem with pre-annotation is that it might bias towards the annotations given by the pre-tagging, for instance if a good pre-tagger reduces the attention of the annotators (Fort and Sagot, 2010). 2.3 Active learning Active learning can be used to reduce the amount of annotated data needed to successfully train a machine learning model. Instead of randomly selecting annotation data, instances in t</context>
</contexts>
<marker>Neves, Leser, 2012</marker>
<rawString>Mariana Neves and Ulf Leser. 2012. A survey on annotation tools for the biomedical literature. Briefings in Bioinformatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Ogren</author>
<author>Guergana Savova</author>
<author>Christopher Chute</author>
</authors>
<title>Constructing evaluation corpora for automated clinical named entity recognition.</title>
<date>2008</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<pages>3143--3149</pages>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="2680" citStr="Ogren et al. (2008)" startWordPosition="432" endWordPosition="435">equently, there is a need to further explore how an annotated corpus can be expanded with less effort and using methods that will not bias the annotators. 2 Background The background discusses basic ideas of preannotation and active learning, as well as the particular challenges associated with annotating clinical text. 2.1 Annotating clinical text A number of text annotation projects have been carried out in the clinical domain, some of them including annotations of clinical named entities, such as mentions of symptoms, diseases and medication. Such studies have for example been described by Ogren et al. (2008), Chapman et al. (2008), Roberts et al. (2009), Wang (2009), Uzuner et al. (2010), Koeling et al. (2011) and Albright et al. (2013). As in many specialised domains, expert annotators are typically required to create a reliable annotated clinical corpus. These expert annotators are often more expensive than annotators without the required specialised knowledge. It is also difficult to use crowdsourcing approaches, such as using e.g. Amazon’s Mechanical Turk to hire online annotators with the required knowledge (Xia and Yetisgen-Yildiz, 2012). A further challenge is posed by the content of the c</context>
<context position="5070" citStr="Ogren et al., 2008" startWordPosition="812" endWordPosition="815">nology based, not requiring annotated data (Mykowiecka and Marciniak, 2011), as well as a machine learning/hybrid system that uses the annotations provided by the annotator to constantly improve the pre-annotation (Tomanek et al., 2012). There exist several annotation tools that facilitate the use of pre-annotation by allowing the user to import pre-annotations or by providing pre-annotation included in the tools (Neves and Leser, 2012). A condition for pre-annotation to be useful is that the produced annotations are good enough, or the effect can be the opposite, slowing the annotators down (Ogren et al., 2008). Another potential problem with pre-annotation is that it might bias towards the annotations given by the pre-tagging, for instance if a good pre-tagger reduces the attention of the annotators (Fort and Sagot, 2010). 2.3 Active learning Active learning can be used to reduce the amount of annotated data needed to successfully train a machine learning model. Instead of randomly selecting annotation data, instances in the data that are highly informative, and thereby also highly useful for the machine learning system, are then actively selected. (Olsson, 2008, p. 27). There are several methods f</context>
</contexts>
<marker>Ogren, Savova, Chute, 2008</marker>
<rawString>Philip Ogren, Guergana Savova, and Christopher Chute. 2008. Constructing evaluation corpora for automated clinical named entity recognition. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), pages 3143– 3149, Marrakech, Morocco, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredrik Olsson</author>
</authors>
<title>Bootstrapping Named Entity Annotation by Means of Active Machine Learning.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Gothenburg. Faculty of Arts.</institution>
<contexts>
<context position="5633" citStr="Olsson, 2008" startWordPosition="902" endWordPosition="903"> slowing the annotators down (Ogren et al., 2008). Another potential problem with pre-annotation is that it might bias towards the annotations given by the pre-tagging, for instance if a good pre-tagger reduces the attention of the annotators (Fort and Sagot, 2010). 2.3 Active learning Active learning can be used to reduce the amount of annotated data needed to successfully train a machine learning model. Instead of randomly selecting annotation data, instances in the data that are highly informative, and thereby also highly useful for the machine learning system, are then actively selected. (Olsson, 2008, p. 27). There are several methods for selecting the most informative instances among the unlabelled ones in the available pool of data. A frequently used method is uncertainty sampling, in which instances that the machine learner is least certain how to classify are selected for annotation. For a model learning to classify into two classes, instances, for which the classifier has no clear preference for one of the two alternatives, are chosen for annotation. If there are more than two classes, the confidence for the most probable class can be used as the measure of uncertainty. Only using th</context>
<context position="8498" citStr="Olsson (2008)" startWordPosition="1378" endWordPosition="1379"> Symons et al. (2006) and Settles and Craven (2008). Many different machine learning methods have been used together with active learning for solving various NLP tasks. Support vector machines have been used for text classification (Tong and Koller, 2002), using properties of the support vector machine algorithm for determining what unlabelled data to select for classification. For structured output tasks, such as named entity recognition, hidden markov models have been used by Scheffer et al. (2001) and conditional random fields (CRF) by Settles and Craven (2008) and Symons et al. (2006). 75 Olsson (2008) suggests combining active learning and pre-annotation for a named entity recognition task, that is providing the annotator with pretagged data from an actively learned named entity recogniser. It is proposed not to indiscriminately pre-tagg the data, but to only provide those preannotated labels to the human annotator, for which the pre-tagger is relatively certain. 3 Method Previous research on pre-annotation shows two seemingly incompatible desirable properties in a pre-annotation system. A pre-annotation that is not good enough might slow the human annotator down, whereas a good pre-annota</context>
</contexts>
<marker>Olsson, 2008</marker>
<rawString>Fredrik Olsson. 2008. Bootstrapping Named Entity Annotation by Means of Active Machine Learning. Ph.D. thesis, University of Gothenburg. Faculty of Arts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angus Roberts</author>
<author>Robert Gaizauskas</author>
<author>Mark Hepple</author>
<author>George Demetriou</author>
<author>Yikun Guo</author>
<author>Ian Roberts</author>
<author>Andrea Setzer</author>
</authors>
<title>Building a semantically annotated corpus of clinical texts.</title>
<date>2009</date>
<journal>J. of Biomedical Informatics,</journal>
<pages>42--950</pages>
<contexts>
<context position="2726" citStr="Roberts et al. (2009)" startWordPosition="440" endWordPosition="443"> how an annotated corpus can be expanded with less effort and using methods that will not bias the annotators. 2 Background The background discusses basic ideas of preannotation and active learning, as well as the particular challenges associated with annotating clinical text. 2.1 Annotating clinical text A number of text annotation projects have been carried out in the clinical domain, some of them including annotations of clinical named entities, such as mentions of symptoms, diseases and medication. Such studies have for example been described by Ogren et al. (2008), Chapman et al. (2008), Roberts et al. (2009), Wang (2009), Uzuner et al. (2010), Koeling et al. (2011) and Albright et al. (2013). As in many specialised domains, expert annotators are typically required to create a reliable annotated clinical corpus. These expert annotators are often more expensive than annotators without the required specialised knowledge. It is also difficult to use crowdsourcing approaches, such as using e.g. Amazon’s Mechanical Turk to hire online annotators with the required knowledge (Xia and Yetisgen-Yildiz, 2012). A further challenge is posed by the content of the clinical data, which is often sensitive and sho</context>
</contexts>
<marker>Roberts, Gaizauskas, Hepple, Demetriou, Guo, Roberts, Setzer, 2009</marker>
<rawString>Angus Roberts, Robert Gaizauskas, Mark Hepple, George Demetriou, Yikun Guo, Ian Roberts, and Andrea Setzer. 2009. Building a semantically annotated corpus of clinical texts. J. of Biomedical Informatics, 42:950–966, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Scheffer</author>
<author>Christian Decomain</author>
<author>Stefan Wrobel</author>
</authors>
<title>Active hidden markov models for information extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the 4th International Conference on Advances in Intelligent Data Analysis, IDA ’01,</booktitle>
<pages>309--318</pages>
<publisher>UK. Springer-Verlag.</publisher>
<location>London, UK,</location>
<contexts>
<context position="8390" citStr="Scheffer et al. (2001)" startWordPosition="1358" endWordPosition="1361">ould not lead to a correct model of the available data. Such methods for structured prediction have been described by Symons et al. (2006) and Settles and Craven (2008). Many different machine learning methods have been used together with active learning for solving various NLP tasks. Support vector machines have been used for text classification (Tong and Koller, 2002), using properties of the support vector machine algorithm for determining what unlabelled data to select for classification. For structured output tasks, such as named entity recognition, hidden markov models have been used by Scheffer et al. (2001) and conditional random fields (CRF) by Settles and Craven (2008) and Symons et al. (2006). 75 Olsson (2008) suggests combining active learning and pre-annotation for a named entity recognition task, that is providing the annotator with pretagged data from an actively learned named entity recogniser. It is proposed not to indiscriminately pre-tagg the data, but to only provide those preannotated labels to the human annotator, for which the pre-tagger is relatively certain. 3 Method Previous research on pre-annotation shows two seemingly incompatible desirable properties in a pre-annotation sys</context>
</contexts>
<marker>Scheffer, Decomain, Wrobel, 2001</marker>
<rawString>Tobias Scheffer, Christian Decomain, and Stefan Wrobel. 2001. Active hidden markov models for information extraction. In Proceedings of the 4th International Conference on Advances in Intelligent Data Analysis, IDA ’01, pages 309–318, London, UK, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew I Schein</author>
<author>Lyle H Ungar</author>
</authors>
<title>Active learning for logistic regression: an evaluation.</title>
<date>2007</date>
<location>Mach. Learn., 68(3):235–265,</location>
<contexts>
<context position="7110" citStr="Schein and Ungar, 2007" startWordPosition="1149" endWordPosition="1152">to instead use the difference of the certainty levels for the two most probable classes. If cp1 is the most probable class and cp2 is the second most probable class for the observation xn, the margin used for measuring uncertainty for that instance is: Mn = P(cp1|xn) − P(cp2|xn) (1) An instance with a large margin is easy to classify because the classifier is much more certain of the most probable classification than on the second most probable. Instances with a small margin, on the other hand, are difficult to classify, and therefore instances with a small margin are selected for annotation (Schein and Ungar, 2007). A common alternative is to use entropy as an uncertainty measure, which takes the certainty levels of all possible classes into account (Settles, 2009). There are also a number of other possible methods for selecting informative instances for annotation, for instance to use a committee of learners and select the instances for which the committee disagrees the most, or to search for annotation instances that would result in the largest expected change to the current model (Settles, 2009). There are also methods to ensure that the selected data correctly reflects the distribution in the pool o</context>
</contexts>
<marker>Schein, Ungar, 2007</marker>
<rawString>Andrew I. Schein and Lyle H. Ungar. 2007. Active learning for logistic regression: an evaluation. Mach. Learn., 68(3):235–265, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
</authors>
<title>An analysis of active learning strategies for sequence labeling tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>1070--1079</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7936" citStr="Settles and Craven (2008)" startWordPosition="1288" endWordPosition="1291">ods for selecting informative instances for annotation, for instance to use a committee of learners and select the instances for which the committee disagrees the most, or to search for annotation instances that would result in the largest expected change to the current model (Settles, 2009). There are also methods to ensure that the selected data correctly reflects the distribution in the pool of unlabelled data, avoiding a selection of outliers that would not lead to a correct model of the available data. Such methods for structured prediction have been described by Symons et al. (2006) and Settles and Craven (2008). Many different machine learning methods have been used together with active learning for solving various NLP tasks. Support vector machines have been used for text classification (Tong and Koller, 2002), using properties of the support vector machine algorithm for determining what unlabelled data to select for classification. For structured output tasks, such as named entity recognition, hidden markov models have been used by Scheffer et al. (2001) and conditional random fields (CRF) by Settles and Craven (2008) and Symons et al. (2006). 75 Olsson (2008) suggests combining active learning an</context>
</contexts>
<marker>Settles, Craven, 2008</marker>
<rawString>Burr Settles and Mark Craven. 2008. An analysis of active learning strategies for sequence labeling tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 1070–1079, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey. Computer Sciences</title>
<date>2009</date>
<tech>Technical Report 1648,</tech>
<institution>University of Wisconsin–Madison.</institution>
<contexts>
<context position="6429" citStr="Settles, 2009" startWordPosition="1033" endWordPosition="1034">ling, in which instances that the machine learner is least certain how to classify are selected for annotation. For a model learning to classify into two classes, instances, for which the classifier has no clear preference for one of the two alternatives, are chosen for annotation. If there are more than two classes, the confidence for the most probable class can be used as the measure of uncertainty. Only using the certainty level for the most probable classification means that not all available information is used, i.e. the information of the certainty levels for the less probable classes. (Settles, 2009) An alternative for a multi-class classifier is therefore to instead use the difference of the certainty levels for the two most probable classes. If cp1 is the most probable class and cp2 is the second most probable class for the observation xn, the margin used for measuring uncertainty for that instance is: Mn = P(cp1|xn) − P(cp2|xn) (1) An instance with a large margin is easy to classify because the classifier is much more certain of the most probable classification than on the second most probable. Instances with a small margin, on the other hand, are difficult to classify, and therefore i</context>
</contexts>
<marker>Settles, 2009</marker>
<rawString>Burr Settles. 2009. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin–Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Skeppstedt</author>
<author>Hercules Dalianis</author>
</authors>
<title>Using active learning and pre-tagging for annotating clinical findings in health record text.</title>
<date>2012</date>
<booktitle>In Proceedings of SMBM 2012 - The 5th International Symposium on Semantic Mining in Biomedicine,</booktitle>
<pages>98--99</pages>
<location>Zurich, Switzerland,</location>
<contexts>
<context position="10375" citStr="Skeppstedt and Dalianis (2012)" startWordPosition="1661" endWordPosition="1664">the human annotator with pre-tagged data that has been selected for active learning. The data selected for annotation when using active learning, is the data for which the pre-annotator is most uncertain and therefore the data which would be least suitable for pre-annotation. The method proposed here aims at finding a way of combining pre-annotation and active learning while reducing the risk of annotation bias. Thereby decreasing the amount of data that needs to be annotated as well as facilitating the annotation, without introducing bias. A previous version of this idea has been outlined by Skeppstedt and Dalianis (2012). The method is focused on the annotation of named entities in clinical text, that is marking of spans of text as well as classification of the spans into an entity class. 3.1 Pre-annotation As in standard pre-annotation, the annotator will be presented with pre-tagged data, and does not have to annotate the data from scratch. To reduce the bias problem that might be associated with pre-tagging, the mode of presentation will, however, be slightly different in the method proposed here. Instead of presenting the best tagging for the human annotator to correct, or to present the n best taggings, </context>
</contexts>
<marker>Skeppstedt, Dalianis, 2012</marker>
<rawString>Maria Skeppstedt and Hercules Dalianis. 2012. Using active learning and pre-tagging for annotating clinical findings in health record text. In Proceedings of SMBM 2012 - The 5th International Symposium on Semantic Mining in Biomedicine, pages 98–99, Zurich, Switzerland, September 3-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Skeppstedt</author>
<author>Maria Kvist</author>
<author>Hercules Dalianis</author>
</authors>
<title>Rule-based entity recognition and coverage of SNOMED CT in Swedish clinical text.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>1250--1257</pages>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="17345" citStr="Skeppstedt et al., 2012" startWordPosition="2820" endWordPosition="2823">CRF++ can also give the conditional probably for the output, that is for the entire classification sequence of a sentence, which is needed in the proposed active learning algorithm. 3.4 Materials There is a corpus of Swedish clinical text, i.e. the text in the narrative part of the health record, that contains clinical text from the Stockholm area, from the years 2006-2008 (Dalianis et al., 2009). A subset of this corpus, containing texts from an emergency unit of internal medicine, has been annotated for four types of named entities: disorder, finding, pharmaceutical drug and body structure (Skeppstedt et al., 2012). For approximately one third of this annotated corpus, double annotation has been performed, and the instances, for which there were a disagreement, have been resolved by one of the annotators. The annotated corpus will form the main source of materials for the study proposed here, and additional data to annotate will be selected from a pool of unlabelled data from internal medicine emergency notes. The larger subset of the annotated data, only annotated by one annotator, will be referred to as Single (containing 45 482 tokens), and the smaller subset, annotated by two annotators, will be ref</context>
<context position="18712" citStr="Skeppstedt et al., 2012" startWordPosition="3053" endWordPosition="3056">od, whereas the Double subset will be used for a final evaluation. 3.5 Step-by-step explanation The proposed method can be divided into 8 steps: 1. Train a CRF model with a randomly selected subset of the Single part of the annotated corpus, the seed set. The size of this seed set, as well as suitable features for the CRF model will be evaluated using cross validation on the seed set. The size should be as small as possible, limiting the amount of initial annotation needed, but large enough to have results in line with a baseline system using terminology matching for named entity recognition (Skeppstedt et al., 2012). 2. Apply the constructed CRF model on unlabelled data from the pool of data from internal medicine emergency notes. Let the model, which operates on a sentence level, provide the three most probable label sequences for each sentence, together with its level of certainty. 3. Calculate the difference in certainty between the most probable and the third most probable suggestion sequence for each sentence, that is MtoThird. Start with a low threshold t and place all sentences with MtoThird above the threshold t in a list of candidates for presenting to the annotator (that is the sentences fulfil</context>
</contexts>
<marker>Skeppstedt, Kvist, Dalianis, 2012</marker>
<rawString>Maria Skeppstedt, Maria Kvist, and Hercules Dalianis. 2012. Rule-based entity recognition and coverage of SNOMED CT in Swedish clinical text. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), pages 1250–1257, Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher T Symons</author>
<author>Nagiza F Samatova</author>
<author>Ramya Krishnamurthy</author>
<author>Byung H Park</author>
<author>Tarik Umar</author>
<author>David Buttler</author>
<author>Terence Critchlow</author>
<author>David Hysom</author>
</authors>
<title>Multi-criterion active learning in conditional random fields.</title>
<date>2006</date>
<booktitle>In Proceedings of the 18th IEEE International Conference on Tools with Artificial Intelligence, ICTAI ’06,</booktitle>
<pages>323--331</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="7906" citStr="Symons et al. (2006)" startWordPosition="1283" endWordPosition="1286">er of other possible methods for selecting informative instances for annotation, for instance to use a committee of learners and select the instances for which the committee disagrees the most, or to search for annotation instances that would result in the largest expected change to the current model (Settles, 2009). There are also methods to ensure that the selected data correctly reflects the distribution in the pool of unlabelled data, avoiding a selection of outliers that would not lead to a correct model of the available data. Such methods for structured prediction have been described by Symons et al. (2006) and Settles and Craven (2008). Many different machine learning methods have been used together with active learning for solving various NLP tasks. Support vector machines have been used for text classification (Tong and Koller, 2002), using properties of the support vector machine algorithm for determining what unlabelled data to select for classification. For structured output tasks, such as named entity recognition, hidden markov models have been used by Scheffer et al. (2001) and conditional random fields (CRF) by Settles and Craven (2008) and Symons et al. (2006). 75 Olsson (2008) suggest</context>
</contexts>
<marker>Symons, Samatova, Krishnamurthy, Park, Umar, Buttler, Critchlow, Hysom, 2006</marker>
<rawString>Christopher T. Symons, Nagiza F. Samatova, Ramya Krishnamurthy, Byung H. Park, Tarik Umar, David Buttler, Terence Critchlow, and David Hysom. 2006. Multi-criterion active learning in conditional random fields. In Proceedings of the 18th IEEE International Conference on Tools with Artificial Intelligence, ICTAI ’06, pages 323–331, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Philipp Daumke</author>
<author>Frank Enders</author>
<author>Jens Huber</author>
<author>Katharina Theres</author>
<author>Marcel M¨uller</author>
</authors>
<title>An interactive de-identification-system.</title>
<date>2012</date>
<booktitle>In Proceedings of SMBM 2012 - The 5th International Symposium on Semantic Mining in Biomedicine, pages 82– 86,</booktitle>
<pages>3--4</pages>
<location>Zurich, Switzerland,</location>
<marker>Tomanek, Daumke, Enders, Huber, Theres, M¨uller, 2012</marker>
<rawString>Katrin Tomanek, Philipp Daumke, Frank Enders, Jens Huber, Katharina Theres, and Marcel M¨uller. 2012. An interactive de-identification-system. In Proceedings of SMBM 2012 - The 5th International Symposium on Semantic Mining in Biomedicine, pages 82– 86, Zurich, Switzerland, September 3-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>2--45</pages>
<contexts>
<context position="8140" citStr="Tong and Koller, 2002" startWordPosition="1318" endWordPosition="1321">that would result in the largest expected change to the current model (Settles, 2009). There are also methods to ensure that the selected data correctly reflects the distribution in the pool of unlabelled data, avoiding a selection of outliers that would not lead to a correct model of the available data. Such methods for structured prediction have been described by Symons et al. (2006) and Settles and Craven (2008). Many different machine learning methods have been used together with active learning for solving various NLP tasks. Support vector machines have been used for text classification (Tong and Koller, 2002), using properties of the support vector machine algorithm for determining what unlabelled data to select for classification. For structured output tasks, such as named entity recognition, hidden markov models have been used by Scheffer et al. (2001) and conditional random fields (CRF) by Settles and Craven (2008) and Symons et al. (2006). 75 Olsson (2008) suggests combining active learning and pre-annotation for a named entity recognition task, that is providing the annotator with pretagged data from an actively learned named entity recogniser. It is proposed not to indiscriminately pre-tagg </context>
</contexts>
<marker>Tong, Koller, 2002</marker>
<rawString>Simon Tong and Daphne Koller. 2002. Support vector machine active learning with applications to text classification. J. Mach. Learn. Res., 2:45–66, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨Ozlem Uzuner</author>
<author>Imre Solti</author>
<author>Fei Xia</author>
<author>Eithon Cadag</author>
</authors>
<title>Community annotation experiment for ground truth generation for the i2b2 medication challenge. JAm Med Inform Assoc,</title>
<date>2010</date>
<contexts>
<context position="2761" citStr="Uzuner et al. (2010)" startWordPosition="446" endWordPosition="449">nded with less effort and using methods that will not bias the annotators. 2 Background The background discusses basic ideas of preannotation and active learning, as well as the particular challenges associated with annotating clinical text. 2.1 Annotating clinical text A number of text annotation projects have been carried out in the clinical domain, some of them including annotations of clinical named entities, such as mentions of symptoms, diseases and medication. Such studies have for example been described by Ogren et al. (2008), Chapman et al. (2008), Roberts et al. (2009), Wang (2009), Uzuner et al. (2010), Koeling et al. (2011) and Albright et al. (2013). As in many specialised domains, expert annotators are typically required to create a reliable annotated clinical corpus. These expert annotators are often more expensive than annotators without the required specialised knowledge. It is also difficult to use crowdsourcing approaches, such as using e.g. Amazon’s Mechanical Turk to hire online annotators with the required knowledge (Xia and Yetisgen-Yildiz, 2012). A further challenge is posed by the content of the clinical data, which is often sensitive and should therefore only be accessed by a</context>
</contexts>
<marker>Uzuner, Solti, Xia, Cadag, 2010</marker>
<rawString>¨Ozlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag. 2010. Community annotation experiment for ground truth generation for the i2b2 medication challenge. JAm Med Inform Assoc, 17(5):519–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yefeng Wang</author>
</authors>
<title>Annotating and recognising named entities in clinical notes.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP Student Research Workshop,</booktitle>
<pages>18--26</pages>
<contexts>
<context position="2739" citStr="Wang (2009)" startWordPosition="444" endWordPosition="445">s can be expanded with less effort and using methods that will not bias the annotators. 2 Background The background discusses basic ideas of preannotation and active learning, as well as the particular challenges associated with annotating clinical text. 2.1 Annotating clinical text A number of text annotation projects have been carried out in the clinical domain, some of them including annotations of clinical named entities, such as mentions of symptoms, diseases and medication. Such studies have for example been described by Ogren et al. (2008), Chapman et al. (2008), Roberts et al. (2009), Wang (2009), Uzuner et al. (2010), Koeling et al. (2011) and Albright et al. (2013). As in many specialised domains, expert annotators are typically required to create a reliable annotated clinical corpus. These expert annotators are often more expensive than annotators without the required specialised knowledge. It is also difficult to use crowdsourcing approaches, such as using e.g. Amazon’s Mechanical Turk to hire online annotators with the required knowledge (Xia and Yetisgen-Yildiz, 2012). A further challenge is posed by the content of the clinical data, which is often sensitive and should therefore</context>
</contexts>
<marker>Wang, 2009</marker>
<rawString>Yefeng Wang. 2009. Annotating and recognising named entities in clinical notes. In Proceedings of the ACL-IJCNLP Student Research Workshop, pages 18–26, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Meliha Yetisgen-Yildiz</author>
</authors>
<title>Clinical corpus annotation: Challenges and strategies.</title>
<date>2012</date>
<booktitle>In The Third Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM), an LREC Workshop.</booktitle>
<contexts>
<context position="3226" citStr="Xia and Yetisgen-Yildiz, 2012" startWordPosition="519" endWordPosition="522"> and medication. Such studies have for example been described by Ogren et al. (2008), Chapman et al. (2008), Roberts et al. (2009), Wang (2009), Uzuner et al. (2010), Koeling et al. (2011) and Albright et al. (2013). As in many specialised domains, expert annotators are typically required to create a reliable annotated clinical corpus. These expert annotators are often more expensive than annotators without the required specialised knowledge. It is also difficult to use crowdsourcing approaches, such as using e.g. Amazon’s Mechanical Turk to hire online annotators with the required knowledge (Xia and Yetisgen-Yildiz, 2012). A further challenge is posed by the content of the clinical data, which is often sensitive and should therefore only be accessed by a limited number of people. Research community annotation is consequently another option that is not always open to annotation projects in the clinical domain, even if there are examples of such community annotations also for clinical text, e.g. described by Uzuner et al. (2010). 74 Proceedings of the ACL Student Research Workshop, pages 74–80, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics To simplify the annotation process, </context>
</contexts>
<marker>Xia, Yetisgen-Yildiz, 2012</marker>
<rawString>Fei Xia and Meliha Yetisgen-Yildiz. 2012. Clinical corpus annotation: Challenges and strategies. In The Third Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM), an LREC Workshop. Turkey.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>