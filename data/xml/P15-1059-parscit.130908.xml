<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022187">
<title confidence="0.984931">
Building a Scientific Concept Hierarchy Database (SCHBASE)
</title>
<author confidence="0.984175">
Eytan Adar Srayan Datta
</author>
<affiliation confidence="0.99959">
University of Michigan University of Michigan
</affiliation>
<address confidence="0.970161">
Ann Arbor, MI 48104 Ann Arbor, MI 48104
</address>
<email confidence="0.999573">
eadar@umich.edu srayand@umich.edu
</email>
<sectionHeader confidence="0.993913" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999650578947368">
Extracted keyphrases can enhance numer-
ous applications ranging from search to
tracking the evolution of scientific dis-
course. We present SCHBASE, a hier-
archical database of keyphrases extracted
from large collections of scientific liter-
ature. SCHBASE relies on a tendency
of scientists to generate new abbrevia-
tions that “extend” existing forms as a
form of signaling novelty. We demon-
strate how these keyphrases/concepts can
be extracted, and their viability as a
database in relation to existing collections.
We further show how keyphrases can
be placed into a semantically-meaningful
“phylogenetic” structure and describe key
features of this structure. The com-
plete SCHBASE dataset is available at:
http://cond.org/schbase.html.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974094339623">
Due to the immense practical value to Informa-
tion Retrieval and other text mining applications,
keyphrase extraction has become an extremely
popular topic of research. Extracted keyphrases,
specifically those derived from scientific literature,
support search tasks (Anick, 2003), classification
and tagging (Medelyan et al., 2009), informa-
tion extraction (Wu and Weld, 2008), and higher-
level analysis such as the tracking of influence and
dynamics of information propagation (Shi et al.,
2010; Ohniwa et al., 2010). In our own work
we use the extracted hierarchies to predict scien-
tific emergence based on how rapidly new vari-
ants emerge. Keyphrases themselves capture a
diverse set of scientific language (e.g., methods,
techniques, materials, phenomena, processes, dis-
eases, devices).
Keyphrases, and their uses, have been stud-
ied extensively (Gil-Leiva and Alonso-Arroyo,
2007). However, automated keyphrase extrac-
tion work has often focused on large-scale statis-
tical techniques and ignored the scientific com-
munication literature. This literature points to
the complex ways in which keyphrases are cre-
ated in light of competing demands: expressive-
ness, findability, succinct writing, signaling nov-
elty, signaling community membership, and so
on (Hartley and Kostoff, 2003; Ibrahim, 1989;
Grange and Bloom, 2000; Gil-Leiva and Alonso-
Arroyo, 2007). Furthermore, the tendency to ex-
tract keyphrases through statistical mechanisms
often leads to flat keyphrase spaces that make anal-
ysis of evolution and emergence difficult.
Our contention, and the main motivation be-
hind our work, is that we can do better by lever-
aging explicit mechanisms adopted by authors
in keyphrase generation. Specifically, we focus
on a tendency to expand keyphrases by adding
terms, coupled with a pressure to abbreviate to
retain succinctness. As we argue below, scien-
tific communication has evolved the use of ab-
breviations to deal with various constraints. Ab-
breviations, and acronyms specifically, are rela-
tively new in many scientific domains (Grange and
Bloom, 2000; Fandrych, 2008) but are now ubiq-
uitous (Ibrahim, 1989; Cheng, 2010).
Keyphrase selection is often motivated by
increasing article findability within a domain
(thereby increasing citation). This strategy leads
to keyphrase reuse. A competing pressure, how-
ever, is to signal novelty in an author’s work which
is often done by creating new terminology (e.g.,
creating a “brand” around a system or idea). For
</bodyText>
<page confidence="0.98083">
606
</page>
<note confidence="0.978071666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 606–615,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99905985106383">
example, a machine learning expert working on
a new type of Support Vector Machine will want
their article found when someone searches for
“Support Vector Machine,” but will also want to
add their own unique brand. In response, they will
often augment the original keyphrase (e.g., “Least-
Squares Support Vector Machine”) rather than in-
venting a completely new one. Unfortunately,
continuous expansion will soon render a paper un-
readable (e.g., one of many extensions to Poly-
merase Chain Reaction is Standard Curve Quan-
titative Competitive Reverse Transcription Poly-
merase Chain Reaction). Thus emerges a second
strategy: abbreviation.
Our assertion is that abbreviations are a key
mechanism for resolving competing demands.
Authors can simultaneously expand keyphrases,
thus maintaining both findability and novelty,
while at the same time addressing the need to be
succinct and non-repetitive. Of interest to us is
the phenomena that if a new keyphrase expands
an existing keyphrase that has an established ab-
breviation, the new keyphrase will also be ab-
breviated (e.g., LS-SVM and SVM). This ten-
dency allows us to construct hierarchies of evolved
keyphrases (rather than assuming a flat keyphrase
space) which can be leveraged to identify emer-
gence, keyphrase “mash-ups,” and perform other
high level analysis. As we demonstrate below,
edges represent the rough semantic of EXTENDS
or ISSUBTYPEOF. So if keyphrase A is connected
to B, we can say A is a subtype of B (e.g., A is
“Least-Squares Support Vector Machine” and B is
“Support Vector Machine”).
In this paper we introduce SCHBASE, a hi-
erarchical database of keyphrases. We demon-
strate how we can simply, but effectively, extract
keyphrases by mining abbreviations from scien-
tific literature and composing those keyphrases
into semantically-meaningful hierarchies. We fur-
ther show that abbreviations are a viable mech-
anism for building a domain-specific keyphrase
database by comparing our extracted keyphrases
to a number of author-defined and automatically-
created keyphrase corpora. Finally, we illustrate
how authors build upon each others’ terminology
over time to create new keyphrases.1
</bodyText>
<footnote confidence="0.973881">
1Full database available at: http://cond.org/schbase.html
</footnote>
<sectionHeader confidence="0.996784" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999986847826087">
Initial work in keyphrase extraction utilized
heuristics that were based on the understood struc-
ture of scientific documents (Edmundson, 1969).
As more data became available, it was possible
to move away from heuristic cues and to lever-
age statistical techniques (Paice and Jones, 1993;
Turney, 2000; Frank et al., 1999) that could iden-
tify keyphrases within, and between, documents.
The guiding model in this approach is that phrases
that appear as statistical “anomalies” (by some
measure) are effective for summarizing a docu-
ment or corpus. This style of keyphrase extrac-
tion represents much of the current state-of-the-
art (Kim et al., 2010). Specific extensions in this
space involve the use of network structures (Mi-
halcea and Tarau, 2004; Litvak and Last, 2008;
Das Gollapalli and Caragea, 2014), part-of-speech
features (Barker and Cornacchia, 2000; Hulth,
2003), or more sophisticated metrics (Tomokiyo
and Hurst, 2003).
However, as we note above, these statistical ap-
proaches largely ignore the underlying tensions in
scientific communication that lead to the creation
of new keyphrases and how they are signaled to
others. The result is that these techniques often
find statistically “anomalous” phrases which often
are not valid scientific concepts (but are simply un-
common phrasing), are unstructured and discon-
nected, and inflexible to size variance (as in the
case of fixed length n-grams), and fail to capture
extremely rare terminology.
The idea that abbreviations may be useful for
keyphrase extraction has been partially realized.
Nguyen et al., (2007) found that they could pro-
duce better keyphrases by extending existing mod-
els (Frank et al., 1999) to include an acronym in-
dicator as a feature. That is, if a candidate phrase
had an associated parenthetical acronym associ-
ated with it in the text a binary feature would be
set. This approach has been implemented by oth-
ers (Bordea and Buitelaar, 2010). We propose to
expand on this idea by implementing a simple, but
effective, solution by performing abbreviation ex-
traction to build a hierarchical keyphrase database
– a form of open-information extraction (Etzioni
et al., 2008) on large scientific corpora.
</bodyText>
<sectionHeader confidence="0.992316" genericHeader="method">
3 Keyphrases and Hierarchies
</sectionHeader>
<bodyText confidence="0.993904">
Our high level strategy for finding an initial set
of keyphrases is to mine a corpus for abbrevia-
</bodyText>
<page confidence="0.997141">
607
</page>
<bodyText confidence="0.999980761904762">
tion expansions. This is a simple strategy, but
as we show below, highly effective. Though the
idea that abbreviations and keyphrases are linked
fits within our understanding of scientific writing,
we confirmed our intuition through a small exper-
iment. Specifically, we looked at the 85 unique
keyphrases (in this case, article titles) listed in
the Wikipedia entry for List of Machine Learning
Concepts (Wikipedia, 2014). These ranged from
well known terms (e.g., Support Vector Machines
and Autoencoders) to less known (e.g., Informa-
tion fuzzy networks). In all 85 cases we were able
to find an abbreviation on the Web (using Google)
alongside the expansion (e.g., searching for the
phrases “Support Vector Machines (SVMs)” or
“Information Fuzzy Networks (IFN)”). Though
there may be bias in the use of abbreviations in
the Machine Learning literature, our experience
has been that this holds in other domains as well.
When a scientific keyphrase is used often enough,
someone, somewhere, will have abbreviated it.
</bodyText>
<subsectionHeader confidence="0.999839">
3.1 Abbreviation Extraction
</subsectionHeader>
<bodyText confidence="0.999928140350877">
To find all abbreviation expansions we use the un-
supervised SaRAD algorithm (Adar, 2004). This
algorithm is simple to implement, does not re-
quire extremely large amounts of data, works for
both acronyms and more general abbreviations,
and has been demonstrated as effective in various
contexts (Adar, 2004; Schwartz and Hearst, 2003).
However, our solution does not depend on a spe-
cific implementation, only that we are able to ac-
curately identify abbreviation expansions.
Adar (2004) presents the full details for the al-
gorithm, but for completeness we present the high
level details. The algorithm progresses by identi-
fying abbreviations inside of parentheses (defined
as single words with at least one capital letter).
The algorithm then extracts a “window” of text
preceding the parenthesis, up to n words long
(where n is the character length of the abbrevia-
tion plus padding). This window does not cross
sentence boundaries. Within the window all possi-
ble “explanations” of the abbreviation are derived.
An explanation consists of a continuous sub-
sequence of words that contain all the characters
of the original abbreviation in order. For example,
the window “determine the geographical distribu-
tion of ribonucleic acid” preceding the abbrevia-
tion “RNA” includes the explanations: “determine
the geographical,” “graphical distribution of ri-
bonucleic acid” and “ribonucleic acid” (matching
characters in italics). In the example above there
are ten explanations (five unique). Each explana-
tion is scored heuristically: 1 point for each ab-
breviation character at the start of a word; 1 point
subtracted for every word between the explanation
and the parenthesis; 1 point bonus if the explana-
tion is adjacent to the parenthesis; 1 point sub-
tracted for each extra word beyond the abbrevia-
tion length. For the explanations above, the scores
are −4, 0, and 3 respectively. The highest scor-
ing match (we require a minimum of 1 point) is
returned as the mostly likely expansion.
In practice, pairs of extracted abbrevia-
tions/expansions are pulled from a large textual
corpus. This both allows us to identify vari-
ants of expansions (e.g., different pluralization,
spelling, hyphenation, etc.) as well as finding
more plausible expansions (those that are repeated
multiple times in a corpus). Thus, each ex-
pansion/abbreviation pair has an associated count
which can be used to threshold and filter for in-
creased quality. To discard units of measurement,
single letter abbreviations and single word expan-
sions are removed. We return to this decision
later, but our experience is also that single word
keyphrases are rare. Additionally, expansions con-
taining brackets are not considered as they usually
represent mathematical formulae.
</bodyText>
<subsubsectionHeader confidence="0.728596">
3.1.1 The ABBREVCORPUS
</subsubsectionHeader>
<bodyText confidence="0.999995619047619">
In our experiments we utilize the ACM Digital Li-
brary (ACMDL) as our main corpus. Though the
ACMDL is more limited than other collections,
it has a number of desirable properties: spanning
nearly the entire history (1954-2011) of a domain
(Computer Science) with full-text and clean meta-
data. The corpus itself contains both journal and
conference articles (77k and 197k, respectively).
In addition to the filtering rules described
above, we manually constructed a set of fil-
ter terms to remove publication venues, agen-
cies, and other institutions: ‘university’, ‘confer-
ence’, ‘symposium’, ‘journal’, ‘foundation’, ‘con-
sortium’, ‘agency’, ‘institute’ and ‘school’ are dis-
carded. We further normalize our keyphrases by
lowercasing, removing hyphens, and using the
Snowball stemmer (Porter, 2001) to merge plu-
ral variants. After stemming and normalizing, we
found a total of 155,957 unique abbreviation ex-
pansions. Among these, 48,890 expansions occur
more than once, 25,107 expansions thrice or more
</bodyText>
<page confidence="0.993835">
608
</page>
<bodyText confidence="0.999936458333333">
and 16,916 expansions four or more times. We re-
fer to this collection as the ABBREVCORPUS.
For each keyphrase we search within the full-
text corpus to identify set of documents containing
the keyphrase. This allowed us to find both the
earliest mention of the keyphrase (the expansion,
not the abbreviation) as well as overall popularity
of keyphrases. We do not argue that abbreviations
are the norm in the introduction of new keyphrases
and may, in fact, only happen much later when the
domain is familiar enough with the phrase.
To find the expansions in the full-text we uti-
lize a modified suffix-tree that greedily finds
the longest-matching phrase and avoids “double-
counting”. For example, if the text contains
the phrase, “...we utilize a Least-Squares Sup-
port Vector Machine for ... ” it will match
against Least-Squares Support Vector Machine but
not Least Squares, Support Vector Machines, or
Support Vector (also keyphrases in our collec-
tion). The distribution of keyphrase frequency is a
power-law (many keyphrases appearing once with
a long tail) with exponent (α) of 2.17 (fit using
Clauset et al., (2009)).
</bodyText>
<subsectionHeader confidence="0.999936">
3.2 Building Keyphrase Hierarchies
</subsectionHeader>
<bodyText confidence="0.999947617647059">
We employ a very simple method of text con-
tainment to build keyphrase hierarchies from AB-
BREVCORPUS. If a keyphrase A is a substring
of keyphrase B, A is said to be contained by B
(B → A). If a third keyphrase, C, contains
B and is contained by A, the containment link
between A and B is dropped and two new ones
(A → C and C → B) are added. For example for
the keyphrases, circuit switching, optical circuit
switching and dynamic optical circuit switching,
there are links from optical circuit switching to cir-
cuit switching, and dynamic optical circuit switch-
ing to optical circuit switching, but there is no link
from dynamic optical circuit switching to circuit
switching. The hierarchies formed in this manner
are mostly trees, but in rare cases a keyphrase can
have links to multiple branches. Example hierar-
chies are displayed in Figure 1.
For efficiency we sort all keyphrases by length
(from largest to shortest) and iterate over each one,
testing for containment in all previously “seen”
keyphrases. This is computationally intensive,
O(n2), but can be parallelized.
A potential issue with string containment is
that negating prefixes can also appear (e.g., non-
monotonic reasoning and monotonic reasoning).
Our algorithm uses a dictionary of negations and
can annotate the results. However, in practice
we find that only .6% of our data has a leading
negating-prefix (“internal” negating prefixes can
also be caught in this way, but are similarly rare).
It is an application-specific question if we want to
consider such pairs as “siblings” or “parent-child”
(with both supported).
</bodyText>
<sectionHeader confidence="0.977915" genericHeader="method">
4 Overlap with Keyphrase Corpora
</sectionHeader>
<bodyText confidence="0.999974">
To test our newly-constructed keyphrase database
we generate a mixture of human- and machine-
built datasets to compare. Our goal is to char-
acterize both the intersection (keyphrases appear-
ing in our corpus as well as the external datasets)
as well as those keyphrases uniquely captured by
each dataset.
</bodyText>
<subsectionHeader confidence="0.991879">
4.1 ACM Author keyphrases (ACMCORPUS)
</subsectionHeader>
<bodyText confidence="0.8567445">
The metadata for articles in ACM corpus contain
author-provided keyphrases. In the corpus de-
scribed above, we found 145,373 unique author-
provided keyphrases after stemming and normal-
ization. We discard 16,418 single-word keywords
and those that do not appear in the full-text of any
document. We retain 116,246 keyphrases which
we refer to as the ACMCORPUS.
Figure 2: Keyphrase counts for the ACMCOR-
PUS (powerlaw α = 2.36), WIKICORPUS (2.49),
MSRACORPUS (2.55) and MESHCORPUS (2.7)
within the ACM full-text.
</bodyText>
<subsectionHeader confidence="0.974586">
4.2 Microsoft Academic (MSRACORPUS)
</subsectionHeader>
<bodyText confidence="0.999482">
Our second keyphrase dataset comes from the Mi-
crosoft Academic (MSRA) search corpus (Mi-
crosoft, 2015). While particularly focused on
</bodyText>
<sectionHeader confidence="0.8713785" genericHeader="method">
ACMCORPUS
WIKICORPUS
MSRACORPUS
MESHCORPUS
</sectionHeader>
<page confidence="0.976458">
609
</page>
<table confidence="0.866065375">
geographic information science and technology (2010)
geographic information systems and science (2003)
practical byzantine fault tolerance (2000)
volunteered geographic information (2008)
geographic information network (2011)
geographic information science (1996)
fault tolerance index (2006)
software fault tolerance (1973)
algorithm based fault tolerance (1984)
partial fault tolerance (1975)
byzantine fault tolerance (1991)
geographic information services (2000)
geographic information system (1975)
geographic information retrieval (1976)
fault tolerance (1969)
geographic information (1973)
</table>
<figureCaption confidence="0.9606835">
Figure 1: Keyphrase hierarchy for Fault Tolerance (top) and Geographic Information (Bottom). Colors
encode earliest appearance (brighter green is earlier)
</figureCaption>
<bodyText confidence="0.9986245">
Computer Science, this collection contains arti-
cles and keyphrases from over a dozen domains2.
MSRA provides a list of keyphrases with unique
IDs and different stemming variations of each
keyphrase. There are a total of 46,978 (without
counting stemming variations) of which 30,477
keyphrases occur in ACM full-text corpus after
stemming and normalization (64% coverage).
</bodyText>
<subsectionHeader confidence="0.947744">
4.3 MeSH (MESHCORPUS)
</subsectionHeader>
<bodyText confidence="0.999587125">
Medical Subject Headings (MeSH) (Lipscomb,
2000) is set of subject headings or descriptors in
the life sciences domain. For the purpose of our
work, we use the 27,149 keyphrases from the 2014
MeSH dataset. Similar to the other keyphrase lists
we only use stemmed and normalized multi-word
keywords that occur in in the ACM full-text cor-
pus, which is 4,363 in case of MeSH.
</bodyText>
<subsectionHeader confidence="0.955443">
4.4 Wikipedia (WIKICORPUS)
</subsectionHeader>
<bodyText confidence="0.99997575">
Scientific article headings in Wikipedia can often
be used as a proxy for keyphrases. To collect rele-
vant titles, we find Wikipedia articles that exactly
match (in title name) existing MeSH and MSRA
keyphrases. For these “seed” articles, we com-
pile their categories and mark all the articles in
these categories as potentially “relevant.” How-
ever, as this also captures scientist names (e.g., a
</bodyText>
<footnote confidence="0.9219325">
2We know these keyphrases are algorithmically derived,
but the details are not disclosed.
</footnote>
<bodyText confidence="0.998777818181818">
researcher’s page may be placed under the “Com-
puter Science” category), research institutes and
other non-keyphrase matches, we use the page’s
infobox as a further filter. Pages containing “per-
son,” “place,” infoboxes, in “book,” “video game,”
“TV show” or other related “media” category, and
those with geographical coordinates are removed.
After applying these filters, we obtain 110,102
unique article titles (after stemming) which we
treat as keyphrases. Of these, 39,974 occur in the
ACM full-text corpus.
</bodyText>
<sectionHeader confidence="0.781864" genericHeader="evaluation">
4.5 Results
</sectionHeader>
<bodyText confidence="0.9999095">
The total overlap for ACMCORPUS, MESH-
CORPUS, MSRACORPUS and WIKICORPUS are
14.12%, 12.28%, 32.33% and 17.41% respec-
tively. While these numbers seem low, it is worth
noting that many of these terms only appear once
in the ACM full-text corpus (see Figure 2).
Figure 3 illustrates the relationship between
the number of times a keyphrase appears in the
full-text and the probability that it will appear
in ABBREVCORPUS. In all cases, the more of-
ten a keyphrase appears in the corpus, the more
likely it is to have an abbreviation. If we quali-
tatively examine popular phrases that do not ap-
pear in ABBREVCORPUS we find mathematical
forms (e.g., of-the-form, well-defined or a priori),
and nouns/entities that are largely unrelated to sci-
entific keyphrases (e.g., New Jersey, Government
Agency, and Private Sector). More importantly,
</bodyText>
<page confidence="0.994172">
610
</page>
<bodyText confidence="0.99987">
the majority of phrases that are never abbreviated
are simply not Computer Science keyphrases (we
return to this in Section 4.6).
We were somewhat surprised by the poor over-
lap of the ACMCORPUS, even for terms that were
very common in the full-text. We found that the
cause was a large set of “bad” keyphrases. Specif-
ically, 69.3k (69.5%) of author-defined keyphrases
(occurring in ACMCORPUS but not in AB-
BREVCORPUS) are used as a keyword in only one
paper. However, they appear more than once in
the full-text – often many times. For example,
one author (and only one) used if and only if as
a keyphrase, which matched a great many articles.
The result is that there is little correlation between
the number of times a keyphrase appears in the
full-text and how many times it used explicitly as
a keyphrase in the document metadata. Because
these will never be found as an abbreviation, they
“pull” the mean probability down.
Instead of counting the number of times a
keyphrase occurs in the full-text we generate a fre-
quency count based on the number of times au-
thors explicitly use it in the metadata. This new
curve, labeled as ACMCORPUS (KEY) in Figure 3
displays a very different tendency, with a rapid
upward slope that peaks at 100% for frequently-
occurring keyphrases. Notably, only 16k (16%)
keyphrases appear once in full-text but are never
abbreviated (far fewer than the 69.5% above).
It is worth briefly considering those terms
that appear in ABBREVCORPUS and not in the
other keyphrases lists. We find roughly 17.6k,
24.7k, 19.4k, and 21.4k terms that appear in AB-
BREVCORPUS (with a threshold of 2 to elimi-
nate “noisy” expansions), but not in ACMCOR-
PUS, MESHCORPUS, MSRACORPUS, and WI-
KICORPUS respectively. As MeSH keyphrases
tend to be focused on the biological keyphrases
this is perhaps unsurprising but the high numbers
for the author-provided ACM keyphrases is unex-
pected. We find that some of the keyphrases that
are in ABBREVCORPUS but not in ACMCORPUS
are highly specific (e.g., Multi-object Evolutionary
Algorithm Based on Decomposition or Stochastic
Variable Graph Model). However, many are also
extremely generic terms that one would expect to
find in a computer science corpus: Run-Time Er-
ror Detection, Parallel Execution Tree, and Little
Endian. Our hypothesis is that these are often not
the focus of a paper and are unlikely to be selected
</bodyText>
<figureCaption confidence="0.986375125">
Figure 3: The probability of inclusion of
keyphrases in ABBREVCORPUS based on fre-
quency of appearance in full text or, in the case if
ACMCORPUS (KEY), frequency of use as a key-
word. At frequency x, the y value represents prob-
ability of appearence in ABBREVCORPUS if we
only consider terms that appear at least x times in
the other corpus.
</figureCaption>
<bodyText confidence="0.990919">
by the author. We believe this provides further evi-
dence of the viability of the abbreviation approach
to generating good keyphrase lists.
</bodyText>
<subsectionHeader confidence="0.986599">
4.6 Domain keyphrases
</subsectionHeader>
<bodyText confidence="0.999972583333333">
When looking at keyphrases that appear in MESH-
CORPUS but not in the ABBREVCORPUS we find
that many phrases do, in fact, appear in the full
text but are never abbreviated. For example, Color
Perception and Blood Cell both appear in ACM
articles but are not abbreviated. Our hypothesis—
which is motivated by the tendency of scientists to
abbreviate terms that are deeply familiar to their
community (Grange and Bloom, 2000)—is that
terms that are possibly distant from the core do-
main focus tend not to be abbreviated. This is sup-
ported by the fact that these terms are abbreviated
in other collections (e.g., one can find CP as an ab-
breviation for Color Perception in psychology and
cognition work and BC, for Blood Cell, in medi-
cal and biological journals). Additional evidence
is apparent in Figure 3 which shows that ACM-
CORPUS keyphrases are more likely to be abbre-
viated (with far fewer repeats necessary). MSRA-
CORPUS, which contains many Computer Science
articles, also has higher probabilities (though not
nearly matching the ACM).
To test this systematically, we calculated se-
mantic similarity between each keyphrase in
</bodyText>
<figure confidence="0.960173333333333">
Probabilty of Appearance in ABBRCORPUS
ACMCORPUS (TEXT)
ACMCORPUS (KEY)
WIKICORPUS
MSRACORPUS
MESHCORPUS
</figure>
<page confidence="0.99435">
611
</page>
<bodyText confidence="0.9989175">
the WikiCorpus dataset to “computer science.”
Specifically, we utilize Explicit Semantic Anal-
ysis (Gabrilovich and Markovitch, 2009) to cal-
culate similarity. In this method, every segment
of text is represented in a very high dimensional
space in terms of keyphrases (based on Wikipedia
categories). The similarity score for each term is
between 0 (unrelated) and 1 (very similar).
</bodyText>
<figureCaption confidence="0.915590357142857">
Figure 4 demonstrates that with increasing sim-
ilarity, the likelihood of abbreviation increases.
From this, one may infer that to generate a
domain-specific database that excludes unrelated
keyphrases, the abbreviation-derived corpus is
highly appropriate. Conversely, to get coverage of
keyphrases from all scientific domains it is insuffi-
cient to mine for abbreviations in one specific do-
main’s text. Even though a keyphrase may appear
in the full-text it will simply never be abbreviated.
Figure 4: Probability of a keyphrase appearing in
ABBREVCORPUS (y-axis) based on semantic sim-
ilarity of the keyphrase to “Computer Science” (x-
axis, binned exponentially for readability).
</figureCaption>
<subsectionHeader confidence="0.9956">
4.7 Keyphrase Hierarchies
</subsectionHeader>
<bodyText confidence="0.999973044117647">
Our hierarchy generation process (see Section 3.2)
generated 1716 hierarchies accounting for 8661
unique keyphrases. Most of the hierarchies (1002
or 58%) only contained two nodes (a root and one
child). The degree distribution, aggregated across
all hierarchies, is again power-law (α = 2.895).
Hierarchy sizes are power-law distributed (α =
2.807) and an average “diameter” (max height) of
1.135. The hierarchies contain a giant component
with 2302 nodes and 2436 edges.
While most of our hierarchies are trees,
keyphrases can connect to two independent
branches. For example, Least-Squares Support
Vector Machines (LS-SVMs) appears in both the
Least Squares and Support Vector hierarchies.
In total, 649 keyphrases appear in multiple hi-
erarchies, the majority appearing 2. Only 17
keyphrases appear in 3 hierarchies. For exam-
ple, the particularly long Single Instruction Mul-
tiple Thread Evolution Strategy Pattern Search
appears in the Evolution(ary) Strategy, Pattern
Search, and Single-Instruction-Multiple-Thread
hierarchies. These collisions are interesting in
that they reflect a mash-ups of different concepts,
and by extension, different sub-disciplines or tech-
niques. In some situations, where there is an
overlap in many sub-keyphrases, this may indicate
that two root keyphrases are in fact equivalent or
highly related (e.g., likelihood ratio and log likeli-
hood). We do not currently handle such ambiguity
in SCHBASE.
To test the semantic interpretation of edges as
EXTENDS/ISSUBTYPEOF we randomly sampled
200 edges and manually checked these. We found
that in 92% (184) this interpretation was cor-
rect. The remaining 16 were largely an artifact
of normalization errors rather than a wrong “type”
(e.g., “session identifier” and “session id” where
clearly a more accurate interpretation is ISEXPAN-
SIONOF). We believe it is fair to say that the hier-
archies we construct are the “skeleton” of a full
EXTENDS hierarchy but one that is nonetheless
fairly encompassing. Our qualitative analysis is
that most keyphrases that share a type also share a
root keyphrase (e.g., “classifier”).
It is interesting to consider if edges which are
derived by “containment” reflect a temporal pat-
tern. That is, if keyphrase A EXTENDS B, does
the first mention of A in the literature happen af-
ter B? We find that this is almost always the case.
Among the 7136 edges generated by our algorithm
only 165 (2.3%) are “reversed.” Qualitatively, we
find that these instances appear either due to miss-
ing data (the parent keyphrase first appeared out-
side the ACM) or publication ordering (in some
cases the difference in first-appearance is only a
year). In most situations the date is only 1-2 years
apart. This high degree of consistency lends fur-
ther support to the tendency of scientists to expand
upon keyphrases over time.
Figure 5 depicts the mean change in length of
“children” in keyphrase hierarchies. The numbers
depicted are relative change. Thus, at year “0”,
the year the root keyphrase is introduced, there is
no relative increase. Within 1 year, new children
of that root are 50% larger in character length and
after that children continue to “grow” as authors
add additional keyphrases. A particularly obvious
</bodyText>
<page confidence="0.996052">
612
</page>
<bodyText confidence="0.99805875">
example of this is the branch for Petri Net (PN)
which was extended as Queueing Petri Net (QPN)
and then Hierarchically Combined Queueing Petri
Nets (HCQPN) and finally Extended Hierarchi-
cally Combined Queueing Petri Nets (EHCQPN).
Notably, this may have implications to other ex-
tractors that assume fixed-sized entities over the
history of the collection.
</bodyText>
<figureCaption confidence="0.963215">
Figure 5: Average increase in character length of
sub-keyphrases over time
</figureCaption>
<sectionHeader confidence="0.972589" genericHeader="discussions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999990413793104">
Our decision to eliminate single-word keyphrases
from consideration is an explicit one. Of the
145k keyphrases in the original ACMCORPUS
(pre-filtering), 16,418 (11.29%) were single-word
keyphrases. Our experience with the ACM author-
defined keyphrases is that such terms are too
generic to be useful as “scientific” keyphrases. For
example, In all the ACM proceedings, the top-
5 most common single-word keyphrases are se-
curity, visualization, evaluation, design, and pri-
vacy. Even in specific sub-domains, such as rec-
ommender systems (Proceedings of Recsys), the
most popular single-word keyphrases are person-
alization, recommendation, evaluation, and trust.
Contrast these to the most popular multi-word
terms: recommender system(s), collaborative fil-
tering, matrix factorization, and social network(s).
Notably, in the MSRA corpus, which is algo-
rithmically filtered, only .46% (226 keyphrases)
were single word. MeSH, in contrast, has a full
37% of keyphrases as single-term. In most sit-
uations these reflect chemical names (e.g., 382
single-word enzymes) or biological structures. In
such a domain, and if these keyphrases are desir-
able, it may be advisable to retain single-word ab-
breviations. While it may seem surprising, even
single words are often abbreviated (e.g., Transal-
dolase is “T” and Ultrafiltration is “U” or “U/F”).
A second key observation is that while the
ACM full-text corpus is large, it is by no means
“big.” We selected to use it because it controlled
and “clean.” However, we have also run our al-
gorithms on the MSRA Corpus (which contains
only abstracts) and CiteSeer (which contains full-
text). Because the corpora contain more text we
find significantly higher overlap with the differ-
ent keyphrase corpora. However, this comes at
the cost of not being able to isolate the domain-
specific keyphrases. To put it differently, the
broader full-text collections enable to us gener-
ate a more fleshed out keyphrase hierarchies that
tracks keyphrases across all domains but which
may not be appropriate for certain workloads.
Finally, it is worth considering the possibility
of building hierarchies (and connecting them) by
relations other than “containment.” We have be-
gun to utilize metrics such as co-occurrence of
keyphrases (e.g., PMI) as well as higher level cita-
tion and co-citation structure in the corpora. Thus,
we are able to connect terms that are highly related
but are textually dissimilar. When experimenting
with PMI, for example, we have found a diverse
set of edge types including ISUSEDFOR (e.g., “n-
gram language model” and “machine translation”)
or ISUSEDIN (e.g., “Expectation Maximization”
and “Baum-Welch” or “euclidean algorithm” and
“k-means”). By necessity, edges generated by this
technique require an additional classification.
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.999939529411765">
We have introduced SCHBASE, a simple, robust,
and highly effective system and database of sci-
entific concepts/keyphrases. By leveraging the
incentive structure of scientists to expand exist-
ing ideas while simultaneously signaling novelty
we are able to construct semantically-meaningful
hierarchies of related keyphrases. The further
tendency by authors to succinctly describe new
keyphrases results in a general habit of utilizing
abbreviations. We have demonstrated a mecha-
nism to identify these keyphrases by extracting ab-
breviation expansions and have shown that these
keyphrases cover the bulk of “useful” keyphrases
within the domain of the corpus. We believe
that SCHBASE will enable a number of appli-
cations ranging from search, categorization, and
analysis of scientific communication patterns.
</bodyText>
<page confidence="0.998913">
613
</page>
<sectionHeader confidence="0.998332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999948666666667">
The authors thank the Microsoft Academic team,
Jaime Teevan, Susan Dumais, and Carl Lagoze for
providing us with data and advice. This work is
supported by the Intelligence Advanced Research
Projects Activity (IARPA) via Department of In-
terior National Business Center contract number
D11PC20155. The U.S. government is authorized
to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright
annotation thereon. Disclaimer: The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of IARPA, DoI/NBC,
or the U.S. Government.
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999568141176471">
Eytan Adar. 2004. SaRAD: a simple and robust abbre-
viation dictionary. Bioinformatics, 20(4):527–533.
Peter Anick. 2003. Using terminological feedback for
web search refinement: A log-based study. In Pro-
ceedings of the 26th Annual International ACM SI-
GIR Conference on Research and Development in
Informaion Retrieval, SIGIR ’03, pages 88–95, New
York, NY, USA. ACM.
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In
Howard J. Hamilton, editor, Advances in Artificial
Intelligence, volume 1822 of Lecture Notes in Com-
puter Science, pages 40–52. Springer Berlin Heidel-
berg.
Georgeta Bordea and Paul Buitelaar. 2010. Deriunlp:
A context based approach to automatic keyphrase
extraction. In Proceedings of the 5th international
workshop on semantic evaluation, pages 146–149.
Association for Computational Linguistics.
Tsung O. Cheng. 2010. What’s in a name? another un-
explained acronym! International Journal of Cardi-
ology, 144(2):291 – 292.
Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ
Newman. 2009. Power-law distributions in empiri-
cal data. SIAM Review, 51(4):661–703.
Sujatha Das Gollapalli and Cornelia Caragea. 2014.
Extracting keyphrases from research papers using
citation networks. In Twenty-Eighth AAAI Confer-
ence on Artificial Intelligence.
Harold P Edmundson. 1969. New methods in auto-
matic extracting. Journal of the ACM, 16(2):264–
285, April.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM,
51(12):68–74, December.
Ingrid Fandrych. 2008. Submorphemic elements in the
formation of acronyms, blends and clippings 147.
Lexis, page 105.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence - Volume 2, IJCAI’99, pages
668–673, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Evgeniy Gabrilovich and Shaul Markovitch. 2009.
Wikipedia-based semantic interpretation for natural
language processing. Journal of Artificial Intelli-
gence Research, 34(1):443–498, March.
Isidoro Gil-Leiva and Adolfo Alonso-Arroyo. 2007.
Keywords given by authors of scientific articles
in database descriptors. Journal of the American
Society for Information Science and Technology,
58(8):1175–1187.
Bob Grange and D.A. Bloom. 2000. Acronyms,
abbreviations and initialisms. BJU International,
86(1):1–6.
James Hartley and Ronald N. Kostoff. 2003. How use-
ful are ‘key words’ in scientific journals? Journal of
Information Science, 29(5):433–438.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’03,
pages 216–223, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A.M. Ibrahim. 1989. Acronyms observed. Pro-
fessional Communication, IEEE Transactions on,
32(1):27–28, Mar.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Semeval-2010 task 5: Au-
tomatic keyphrase extraction from scientific articles.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 21–26. Association for
Computational Linguistics.
Carolyn E. Lipscomb. 2000. Medical subject headings
(mesh). Bull Med Libr Assoc. 88(3): 265266.
Marina Litvak and Mark Last. 2008. Graph-based
keyword extraction for single-document summariza-
tion. In Proceedings of the Workshop on Multi-
source Multilingual Information Extraction and
Summarization, MMIES ’08, pages 17–24, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.983891">
614
</page>
<reference confidence="0.990534878787879">
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 3 - Volume 3, EMNLP
’09, pages 1318–1327, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Microsoft. 2015. Microsoft academic search.
http://academic.research.microsoft.com. Accessed:
2015-2-26.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages
404–411, Barcelona, Spain, July. Association for
Computational Linguistics.
ThuyDung Nguyen and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications.
In Dion Hoe-Lian Goh, Tru Hoang Cao, Inge-
borg Torvik Sølvberg, and Edie Rasmussen, edi-
tors, Asian Digital Libraries. Looking Back 10 Years
and Forging New Frontiers, volume 4822 of Lec-
ture Notes in Computer Science, pages 317–326.
Springer Berlin Heidelberg.
Ryosuke L. Ohniwa, Aiko Hibino, and Kunio
Takeyasu. 2010. Trends in research foci in life
science fields over the last 30 years monitored by
emerging topics. Scientometrics, 85(1):111–127.
Chris D. Paice and Paul A. Jones. 1993. The iden-
tification of important concepts in highly structured
technical papers. In Proceedings of the 16th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
’93, pages 69–78, New York, NY, USA. ACM.
Martin F. Porter. 2001. Snowball:
A language for stemming algorithms.
http://snowball.tartarus.org/texts/introduction.html.
Accessed: 2015-2-26.
Ariel S Schwartz and Marti A Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. Pacific Symposium on Biocom-
puting. Pacific Symposium on Biocomputing, page
451462.
Xiaolin Shi, Jure Leskovec, and Daniel A. McFarland.
2010. Citing for high impact. In Proceedings of
the 10th Annual Joint Conference on Digital Li-
braries, JCDL ’10, pages 49–58, New York, NY,
USA. ACM.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment - Volume 18, MWE ’03, pages 33–40, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Peter D. Turney. 2000. Learning algorithms
for keyphrase extraction. Information Retrieval,
2(4):303–336, May.
Wikipedia. 2014. Wikipedia: List of ma-
chine learning concepts. http://en.wiki-
pedia.org/wiki/List of machine learning concepts.
Accessed: 2015-2-26.
Fei Wu and Daniel S. Weld. 2008. Automatically re-
fining the wikipedia infobox ontology. In Proceed-
ings of the 17th International Conference on World
Wide Web, WWW ’08, pages 635–644, New York,
NY, USA. ACM.
</reference>
<page confidence="0.998505">
615
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.606253">
<title confidence="0.995809">a Scientific Concept Hierarchy Database</title>
<author confidence="0.966905">Eytan Adar Srayan Datta</author>
<affiliation confidence="0.999993">University of Michigan University of Michigan</affiliation>
<address confidence="0.999331">Ann Arbor, MI 48104 Ann Arbor, MI 48104</address>
<email confidence="0.999749">eadar@umich.edusrayand@umich.edu</email>
<abstract confidence="0.989628421052632">Extracted keyphrases can enhance numerous applications ranging from search to tracking the evolution of scientific dis- We present a hierarchical database of keyphrases extracted from large collections of scientific literon a tendency of scientists to generate new abbreviations that “extend” existing forms as a form of signaling novelty. We demonstrate how these keyphrases/concepts can be extracted, and their viability as a database in relation to existing collections. We further show how keyphrases can be placed into a semantically-meaningful “phylogenetic” structure and describe key features of this structure. The comis available at:</abstract>
<web confidence="0.803988">http://cond.org/schbase.html.</web>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eytan Adar</author>
</authors>
<title>SaRAD: a simple and robust abbreviation dictionary.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="9349" citStr="Adar, 2004" startWordPosition="1417" endWordPosition="1418">e.g., Information fuzzy networks). In all 85 cases we were able to find an abbreviation on the Web (using Google) alongside the expansion (e.g., searching for the phrases “Support Vector Machines (SVMs)” or “Information Fuzzy Networks (IFN)”). Though there may be bias in the use of abbreviations in the Machine Learning literature, our experience has been that this holds in other domains as well. When a scientific keyphrase is used often enough, someone, somewhere, will have abbreviated it. 3.1 Abbreviation Extraction To find all abbreviation expansions we use the unsupervised SaRAD algorithm (Adar, 2004). This algorithm is simple to implement, does not require extremely large amounts of data, works for both acronyms and more general abbreviations, and has been demonstrated as effective in various contexts (Adar, 2004; Schwartz and Hearst, 2003). However, our solution does not depend on a specific implementation, only that we are able to accurately identify abbreviation expansions. Adar (2004) presents the full details for the algorithm, but for completeness we present the high level details. The algorithm progresses by identifying abbreviations inside of parentheses (defined as single words w</context>
</contexts>
<marker>Adar, 2004</marker>
<rawString>Eytan Adar. 2004. SaRAD: a simple and robust abbreviation dictionary. Bioinformatics, 20(4):527–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Anick</author>
</authors>
<title>Using terminological feedback for web search refinement: A log-based study.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR ’03,</booktitle>
<pages>88--95</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1239" citStr="Anick, 2003" startWordPosition="172" endWordPosition="173">ncepts can be extracted, and their viability as a database in relation to existing collections. We further show how keyphrases can be placed into a semantically-meaningful “phylogenetic” structure and describe key features of this structure. The complete SCHBASE dataset is available at: http://cond.org/schbase.html. 1 Introduction Due to the immense practical value to Information Retrieval and other text mining applications, keyphrase extraction has become an extremely popular topic of research. Extracted keyphrases, specifically those derived from scientific literature, support search tasks (Anick, 2003), classification and tagging (Medelyan et al., 2009), information extraction (Wu and Weld, 2008), and higherlevel analysis such as the tracking of influence and dynamics of information propagation (Shi et al., 2010; Ohniwa et al., 2010). In our own work we use the extracted hierarchies to predict scientific emergence based on how rapidly new variants emerge. Keyphrases themselves capture a diverse set of scientific language (e.g., methods, techniques, materials, phenomena, processes, diseases, devices). Keyphrases, and their uses, have been studied extensively (Gil-Leiva and Alonso-Arroyo, 200</context>
</contexts>
<marker>Anick, 2003</marker>
<rawString>Peter Anick. 2003. Using terminological feedback for web search refinement: A log-based study. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR ’03, pages 88–95, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Barker</author>
<author>Nadia Cornacchia</author>
</authors>
<title>Using noun phrase heads to extract document keyphrases.</title>
<date>2000</date>
<booktitle>Advances in Artificial Intelligence,</booktitle>
<volume>1822</volume>
<pages>40--52</pages>
<editor>In Howard J. Hamilton, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="6761" citStr="Barker and Cornacchia, 2000" startWordPosition="1006" endWordPosition="1009">to leverage statistical techniques (Paice and Jones, 1993; Turney, 2000; Frank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 2004; Litvak and Last, 2008; Das Gollapalli and Caragea, 2014), part-of-speech features (Barker and Cornacchia, 2000; Hulth, 2003), or more sophisticated metrics (Tomokiyo and Hurst, 2003). However, as we note above, these statistical approaches largely ignore the underlying tensions in scientific communication that lead to the creation of new keyphrases and how they are signaled to others. The result is that these techniques often find statistically “anomalous” phrases which often are not valid scientific concepts (but are simply uncommon phrasing), are unstructured and disconnected, and inflexible to size variance (as in the case of fixed length n-grams), and fail to capture extremely rare terminology. Th</context>
</contexts>
<marker>Barker, Cornacchia, 2000</marker>
<rawString>Ken Barker and Nadia Cornacchia. 2000. Using noun phrase heads to extract document keyphrases. In Howard J. Hamilton, editor, Advances in Artificial Intelligence, volume 1822 of Lecture Notes in Computer Science, pages 40–52. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgeta Bordea</author>
<author>Paul Buitelaar</author>
</authors>
<title>Deriunlp: A context based approach to automatic keyphrase extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th international workshop on semantic evaluation,</booktitle>
<pages>146--149</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7827" citStr="Bordea and Buitelaar, 2010" startWordPosition="1177" endWordPosition="1180">, are unstructured and disconnected, and inflexible to size variance (as in the case of fixed length n-grams), and fail to capture extremely rare terminology. The idea that abbreviations may be useful for keyphrase extraction has been partially realized. Nguyen et al., (2007) found that they could produce better keyphrases by extending existing models (Frank et al., 1999) to include an acronym indicator as a feature. That is, if a candidate phrase had an associated parenthetical acronym associated with it in the text a binary feature would be set. This approach has been implemented by others (Bordea and Buitelaar, 2010). We propose to expand on this idea by implementing a simple, but effective, solution by performing abbreviation extraction to build a hierarchical keyphrase database – a form of open-information extraction (Etzioni et al., 2008) on large scientific corpora. 3 Keyphrases and Hierarchies Our high level strategy for finding an initial set of keyphrases is to mine a corpus for abbrevia607 tion expansions. This is a simple strategy, but as we show below, highly effective. Though the idea that abbreviations and keyphrases are linked fits within our understanding of scientific writing, we confirmed </context>
</contexts>
<marker>Bordea, Buitelaar, 2010</marker>
<rawString>Georgeta Bordea and Paul Buitelaar. 2010. Deriunlp: A context based approach to automatic keyphrase extraction. In Proceedings of the 5th international workshop on semantic evaluation, pages 146–149. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsung O Cheng</author>
</authors>
<title>What’s in a name? another unexplained acronym!</title>
<date>2010</date>
<journal>International Journal of Cardiology,</journal>
<volume>144</volume>
<issue>2</issue>
<pages>292</pages>
<contexts>
<context position="3078" citStr="Cheng, 2010" startWordPosition="450" endWordPosition="451">mergence difficult. Our contention, and the main motivation behind our work, is that we can do better by leveraging explicit mechanisms adopted by authors in keyphrase generation. Specifically, we focus on a tendency to expand keyphrases by adding terms, coupled with a pressure to abbreviate to retain succinctness. As we argue below, scientific communication has evolved the use of abbreviations to deal with various constraints. Abbreviations, and acronyms specifically, are relatively new in many scientific domains (Grange and Bloom, 2000; Fandrych, 2008) but are now ubiquitous (Ibrahim, 1989; Cheng, 2010). Keyphrase selection is often motivated by increasing article findability within a domain (thereby increasing citation). This strategy leads to keyphrase reuse. A competing pressure, however, is to signal novelty in an author’s work which is often done by creating new terminology (e.g., creating a “brand” around a system or idea). For 606 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 606–615, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguisti</context>
</contexts>
<marker>Cheng, 2010</marker>
<rawString>Tsung O. Cheng. 2010. What’s in a name? another unexplained acronym! International Journal of Cardiology, 144(2):291 – 292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>Cosma Rohilla Shalizi</author>
<author>Mark EJ Newman</author>
</authors>
<title>Power-law distributions in empirical data.</title>
<date>2009</date>
<journal>SIAM Review,</journal>
<volume>51</volume>
<issue>4</issue>
<contexts>
<context position="14136" citStr="Clauset et al., (2009)" startWordPosition="2165" endWordPosition="2168">s familiar enough with the phrase. To find the expansions in the full-text we utilize a modified suffix-tree that greedily finds the longest-matching phrase and avoids “doublecounting”. For example, if the text contains the phrase, “...we utilize a Least-Squares Support Vector Machine for ... ” it will match against Least-Squares Support Vector Machine but not Least Squares, Support Vector Machines, or Support Vector (also keyphrases in our collection). The distribution of keyphrase frequency is a power-law (many keyphrases appearing once with a long tail) with exponent (α) of 2.17 (fit using Clauset et al., (2009)). 3.2 Building Keyphrase Hierarchies We employ a very simple method of text containment to build keyphrase hierarchies from ABBREVCORPUS. If a keyphrase A is a substring of keyphrase B, A is said to be contained by B (B → A). If a third keyphrase, C, contains B and is contained by A, the containment link between A and B is dropped and two new ones (A → C and C → B) are added. For example for the keyphrases, circuit switching, optical circuit switching and dynamic optical circuit switching, there are links from optical circuit switching to circuit switching, and dynamic optical circuit switchi</context>
</contexts>
<marker>Clauset, Shalizi, Newman, 2009</marker>
<rawString>Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman. 2009. Power-law distributions in empirical data. SIAM Review, 51(4):661–703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujatha Das Gollapalli</author>
<author>Cornelia Caragea</author>
</authors>
<title>Extracting keyphrases from research papers using citation networks.</title>
<date>2014</date>
<booktitle>In Twenty-Eighth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6707" citStr="Gollapalli and Caragea, 2014" startWordPosition="1000" endWordPosition="1003">e, it was possible to move away from heuristic cues and to leverage statistical techniques (Paice and Jones, 1993; Turney, 2000; Frank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 2004; Litvak and Last, 2008; Das Gollapalli and Caragea, 2014), part-of-speech features (Barker and Cornacchia, 2000; Hulth, 2003), or more sophisticated metrics (Tomokiyo and Hurst, 2003). However, as we note above, these statistical approaches largely ignore the underlying tensions in scientific communication that lead to the creation of new keyphrases and how they are signaled to others. The result is that these techniques often find statistically “anomalous” phrases which often are not valid scientific concepts (but are simply uncommon phrasing), are unstructured and disconnected, and inflexible to size variance (as in the case of fixed length n-gram</context>
</contexts>
<marker>Gollapalli, Caragea, 2014</marker>
<rawString>Sujatha Das Gollapalli and Cornelia Caragea. 2014. Extracting keyphrases from research papers using citation networks. In Twenty-Eighth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>Journal of the ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<pages>285</pages>
<contexts>
<context position="6048" citStr="Edmundson, 1969" startWordPosition="895" endWordPosition="896">and composing those keyphrases into semantically-meaningful hierarchies. We further show that abbreviations are a viable mechanism for building a domain-specific keyphrase database by comparing our extracted keyphrases to a number of author-defined and automaticallycreated keyphrase corpora. Finally, we illustrate how authors build upon each others’ terminology over time to create new keyphrases.1 1Full database available at: http://cond.org/schbase.html 2 Related Work Initial work in keyphrase extraction utilized heuristics that were based on the understood structure of scientific documents (Edmundson, 1969). As more data became available, it was possible to move away from heuristic cues and to leverage statistical techniques (Paice and Jones, 1993; Turney, 2000; Frank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 200</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Harold P Edmundson. 1969. New methods in automatic extracting. Journal of the ACM, 16(2):264– 285, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<issue>12</issue>
<contexts>
<context position="8056" citStr="Etzioni et al., 2008" startWordPosition="1212" endWordPosition="1215">partially realized. Nguyen et al., (2007) found that they could produce better keyphrases by extending existing models (Frank et al., 1999) to include an acronym indicator as a feature. That is, if a candidate phrase had an associated parenthetical acronym associated with it in the text a binary feature would be set. This approach has been implemented by others (Bordea and Buitelaar, 2010). We propose to expand on this idea by implementing a simple, but effective, solution by performing abbreviation extraction to build a hierarchical keyphrase database – a form of open-information extraction (Etzioni et al., 2008) on large scientific corpora. 3 Keyphrases and Hierarchies Our high level strategy for finding an initial set of keyphrases is to mine a corpus for abbrevia607 tion expansions. This is a simple strategy, but as we show below, highly effective. Though the idea that abbreviations and keyphrases are linked fits within our understanding of scientific writing, we confirmed our intuition through a small experiment. Specifically, we looked at the 85 unique keyphrases (in this case, article titles) listed in the Wikipedia entry for List of Machine Learning Concepts (Wikipedia, 2014). These ranged from</context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open information extraction from the web. Communications of the ACM, 51(12):68–74, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Fandrych</author>
</authors>
<title>Submorphemic elements in the formation of acronyms, blends and clippings 147. Lexis,</title>
<date>2008</date>
<pages>105</pages>
<contexts>
<context position="3026" citStr="Fandrych, 2008" startWordPosition="441" endWordPosition="442"> keyphrase spaces that make analysis of evolution and emergence difficult. Our contention, and the main motivation behind our work, is that we can do better by leveraging explicit mechanisms adopted by authors in keyphrase generation. Specifically, we focus on a tendency to expand keyphrases by adding terms, coupled with a pressure to abbreviate to retain succinctness. As we argue below, scientific communication has evolved the use of abbreviations to deal with various constraints. Abbreviations, and acronyms specifically, are relatively new in many scientific domains (Grange and Bloom, 2000; Fandrych, 2008) but are now ubiquitous (Ibrahim, 1989; Cheng, 2010). Keyphrase selection is often motivated by increasing article findability within a domain (thereby increasing citation). This strategy leads to keyphrase reuse. A competing pressure, however, is to signal novelty in an author’s work which is often done by creating new terminology (e.g., creating a “brand” around a system or idea). For 606 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 606–615, Beijing, China, July 26-31, </context>
</contexts>
<marker>Fandrych, 2008</marker>
<rawString>Ingrid Fandrych. 2008. Submorphemic elements in the formation of acronyms, blends and clippings 147. Lexis, page 105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI’99,</booktitle>
<pages>668--673</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="6226" citStr="Frank et al., 1999" startWordPosition="923" endWordPosition="926">abase by comparing our extracted keyphrases to a number of author-defined and automaticallycreated keyphrase corpora. Finally, we illustrate how authors build upon each others’ terminology over time to create new keyphrases.1 1Full database available at: http://cond.org/schbase.html 2 Related Work Initial work in keyphrase extraction utilized heuristics that were based on the understood structure of scientific documents (Edmundson, 1969). As more data became available, it was possible to move away from heuristic cues and to leverage statistical techniques (Paice and Jones, 1993; Turney, 2000; Frank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 2004; Litvak and Last, 2008; Das Gollapalli and Caragea, 2014), part-of-speech features (Barker and Cornacchia, 2000; Hulth, 2003), or more sophisticated metrics (Tomokiyo and Hurst</context>
<context position="7574" citStr="Frank et al., 1999" startWordPosition="1132" endWordPosition="1135">on that lead to the creation of new keyphrases and how they are signaled to others. The result is that these techniques often find statistically “anomalous” phrases which often are not valid scientific concepts (but are simply uncommon phrasing), are unstructured and disconnected, and inflexible to size variance (as in the case of fixed length n-grams), and fail to capture extremely rare terminology. The idea that abbreviations may be useful for keyphrase extraction has been partially realized. Nguyen et al., (2007) found that they could produce better keyphrases by extending existing models (Frank et al., 1999) to include an acronym indicator as a feature. That is, if a candidate phrase had an associated parenthetical acronym associated with it in the text a binary feature would be set. This approach has been implemented by others (Bordea and Buitelaar, 2010). We propose to expand on this idea by implementing a simple, but effective, solution by performing abbreviation extraction to build a hierarchical keyphrase database – a form of open-information extraction (Etzioni et al., 2008) on large scientific corpora. 3 Keyphrases and Hierarchies Our high level strategy for finding an initial set of keyph</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI’99, pages 668–673, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Wikipedia-based semantic interpretation for natural language processing.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="24489" citStr="Gabrilovich and Markovitch, 2009" startWordPosition="3804" endWordPosition="3807">d biological journals). Additional evidence is apparent in Figure 3 which shows that ACMCORPUS keyphrases are more likely to be abbreviated (with far fewer repeats necessary). MSRACORPUS, which contains many Computer Science articles, also has higher probabilities (though not nearly matching the ACM). To test this systematically, we calculated semantic similarity between each keyphrase in Probabilty of Appearance in ABBRCORPUS ACMCORPUS (TEXT) ACMCORPUS (KEY) WIKICORPUS MSRACORPUS MESHCORPUS 611 the WikiCorpus dataset to “computer science.” Specifically, we utilize Explicit Semantic Analysis (Gabrilovich and Markovitch, 2009) to calculate similarity. In this method, every segment of text is represented in a very high dimensional space in terms of keyphrases (based on Wikipedia categories). The similarity score for each term is between 0 (unrelated) and 1 (very similar). Figure 4 demonstrates that with increasing similarity, the likelihood of abbreviation increases. From this, one may infer that to generate a domain-specific database that excludes unrelated keyphrases, the abbreviation-derived corpus is highly appropriate. Conversely, to get coverage of keyphrases from all scientific domains it is insufficient to m</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2009</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2009. Wikipedia-based semantic interpretation for natural language processing. Journal of Artificial Intelligence Research, 34(1):443–498, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isidoro Gil-Leiva</author>
<author>Adolfo Alonso-Arroyo</author>
</authors>
<title>Keywords given by authors of scientific articles in database descriptors.</title>
<date>2007</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>58</volume>
<issue>8</issue>
<contexts>
<context position="1841" citStr="Gil-Leiva and Alonso-Arroyo, 2007" startWordPosition="260" endWordPosition="263">pport search tasks (Anick, 2003), classification and tagging (Medelyan et al., 2009), information extraction (Wu and Weld, 2008), and higherlevel analysis such as the tracking of influence and dynamics of information propagation (Shi et al., 2010; Ohniwa et al., 2010). In our own work we use the extracted hierarchies to predict scientific emergence based on how rapidly new variants emerge. Keyphrases themselves capture a diverse set of scientific language (e.g., methods, techniques, materials, phenomena, processes, diseases, devices). Keyphrases, and their uses, have been studied extensively (Gil-Leiva and Alonso-Arroyo, 2007). However, automated keyphrase extraction work has often focused on large-scale statistical techniques and ignored the scientific communication literature. This literature points to the complex ways in which keyphrases are created in light of competing demands: expressiveness, findability, succinct writing, signaling novelty, signaling community membership, and so on (Hartley and Kostoff, 2003; Ibrahim, 1989; Grange and Bloom, 2000; Gil-Leiva and AlonsoArroyo, 2007). Furthermore, the tendency to extract keyphrases through statistical mechanisms often leads to flat keyphrase spaces that make an</context>
</contexts>
<marker>Gil-Leiva, Alonso-Arroyo, 2007</marker>
<rawString>Isidoro Gil-Leiva and Adolfo Alonso-Arroyo. 2007. Keywords given by authors of scientific articles in database descriptors. Journal of the American Society for Information Science and Technology, 58(8):1175–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Grange</author>
<author>D A Bloom</author>
</authors>
<title>Acronyms, abbreviations and initialisms.</title>
<date>2000</date>
<journal>BJU International,</journal>
<volume>86</volume>
<issue>1</issue>
<contexts>
<context position="2276" citStr="Grange and Bloom, 2000" startWordPosition="323" endWordPosition="326">fic language (e.g., methods, techniques, materials, phenomena, processes, diseases, devices). Keyphrases, and their uses, have been studied extensively (Gil-Leiva and Alonso-Arroyo, 2007). However, automated keyphrase extraction work has often focused on large-scale statistical techniques and ignored the scientific communication literature. This literature points to the complex ways in which keyphrases are created in light of competing demands: expressiveness, findability, succinct writing, signaling novelty, signaling community membership, and so on (Hartley and Kostoff, 2003; Ibrahim, 1989; Grange and Bloom, 2000; Gil-Leiva and AlonsoArroyo, 2007). Furthermore, the tendency to extract keyphrases through statistical mechanisms often leads to flat keyphrase spaces that make analysis of evolution and emergence difficult. Our contention, and the main motivation behind our work, is that we can do better by leveraging explicit mechanisms adopted by authors in keyphrase generation. Specifically, we focus on a tendency to expand keyphrases by adding terms, coupled with a pressure to abbreviate to retain succinctness. As we argue below, scientific communication has evolved the use of abbreviations to deal with</context>
<context position="23543" citStr="Grange and Bloom, 2000" startWordPosition="3658" endWordPosition="3661">r at least x times in the other corpus. by the author. We believe this provides further evidence of the viability of the abbreviation approach to generating good keyphrase lists. 4.6 Domain keyphrases When looking at keyphrases that appear in MESHCORPUS but not in the ABBREVCORPUS we find that many phrases do, in fact, appear in the full text but are never abbreviated. For example, Color Perception and Blood Cell both appear in ACM articles but are not abbreviated. Our hypothesis— which is motivated by the tendency of scientists to abbreviate terms that are deeply familiar to their community (Grange and Bloom, 2000)—is that terms that are possibly distant from the core domain focus tend not to be abbreviated. This is supported by the fact that these terms are abbreviated in other collections (e.g., one can find CP as an abbreviation for Color Perception in psychology and cognition work and BC, for Blood Cell, in medical and biological journals). Additional evidence is apparent in Figure 3 which shows that ACMCORPUS keyphrases are more likely to be abbreviated (with far fewer repeats necessary). MSRACORPUS, which contains many Computer Science articles, also has higher probabilities (though not nearly mat</context>
</contexts>
<marker>Grange, Bloom, 2000</marker>
<rawString>Bob Grange and D.A. Bloom. 2000. Acronyms, abbreviations and initialisms. BJU International, 86(1):1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Hartley</author>
<author>Ronald N Kostoff</author>
</authors>
<title>How useful are ‘key words’ in scientific journals?</title>
<date>2003</date>
<journal>Journal of Information Science,</journal>
<volume>29</volume>
<issue>5</issue>
<contexts>
<context position="2237" citStr="Hartley and Kostoff, 2003" startWordPosition="317" endWordPosition="320">hemselves capture a diverse set of scientific language (e.g., methods, techniques, materials, phenomena, processes, diseases, devices). Keyphrases, and their uses, have been studied extensively (Gil-Leiva and Alonso-Arroyo, 2007). However, automated keyphrase extraction work has often focused on large-scale statistical techniques and ignored the scientific communication literature. This literature points to the complex ways in which keyphrases are created in light of competing demands: expressiveness, findability, succinct writing, signaling novelty, signaling community membership, and so on (Hartley and Kostoff, 2003; Ibrahim, 1989; Grange and Bloom, 2000; Gil-Leiva and AlonsoArroyo, 2007). Furthermore, the tendency to extract keyphrases through statistical mechanisms often leads to flat keyphrase spaces that make analysis of evolution and emergence difficult. Our contention, and the main motivation behind our work, is that we can do better by leveraging explicit mechanisms adopted by authors in keyphrase generation. Specifically, we focus on a tendency to expand keyphrases by adding terms, coupled with a pressure to abbreviate to retain succinctness. As we argue below, scientific communication has evolve</context>
</contexts>
<marker>Hartley, Kostoff, 2003</marker>
<rawString>James Hartley and Ronald N. Kostoff. 2003. How useful are ‘key words’ in scientific journals? Journal of Information Science, 29(5):433–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP ’03,</booktitle>
<pages>216--223</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6775" citStr="Hulth, 2003" startWordPosition="1010" endWordPosition="1011">iques (Paice and Jones, 1993; Turney, 2000; Frank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 2004; Litvak and Last, 2008; Das Gollapalli and Caragea, 2014), part-of-speech features (Barker and Cornacchia, 2000; Hulth, 2003), or more sophisticated metrics (Tomokiyo and Hurst, 2003). However, as we note above, these statistical approaches largely ignore the underlying tensions in scientific communication that lead to the creation of new keyphrases and how they are signaled to others. The result is that these techniques often find statistically “anomalous” phrases which often are not valid scientific concepts (but are simply uncommon phrasing), are unstructured and disconnected, and inflexible to size variance (as in the case of fixed length n-grams), and fail to capture extremely rare terminology. The idea that ab</context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>Anette Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP ’03, pages 216–223, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Ibrahim</author>
</authors>
<title>Acronyms observed. Professional Communication,</title>
<date>1989</date>
<journal>IEEE Transactions on,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="2252" citStr="Ibrahim, 1989" startWordPosition="321" endWordPosition="322"> set of scientific language (e.g., methods, techniques, materials, phenomena, processes, diseases, devices). Keyphrases, and their uses, have been studied extensively (Gil-Leiva and Alonso-Arroyo, 2007). However, automated keyphrase extraction work has often focused on large-scale statistical techniques and ignored the scientific communication literature. This literature points to the complex ways in which keyphrases are created in light of competing demands: expressiveness, findability, succinct writing, signaling novelty, signaling community membership, and so on (Hartley and Kostoff, 2003; Ibrahim, 1989; Grange and Bloom, 2000; Gil-Leiva and AlonsoArroyo, 2007). Furthermore, the tendency to extract keyphrases through statistical mechanisms often leads to flat keyphrase spaces that make analysis of evolution and emergence difficult. Our contention, and the main motivation behind our work, is that we can do better by leveraging explicit mechanisms adopted by authors in keyphrase generation. Specifically, we focus on a tendency to expand keyphrases by adding terms, coupled with a pressure to abbreviate to retain succinctness. As we argue below, scientific communication has evolved the use of ab</context>
</contexts>
<marker>Ibrahim, 1989</marker>
<rawString>A.M. Ibrahim. 1989. Acronyms observed. Professional Communication, IEEE Transactions on, 32(1):27–28, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Olena Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>21--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6550" citStr="Kim et al., 2010" startWordPosition="975" endWordPosition="978">xtraction utilized heuristics that were based on the understood structure of scientific documents (Edmundson, 1969). As more data became available, it was possible to move away from heuristic cues and to leverage statistical techniques (Paice and Jones, 1993; Turney, 2000; Frank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 2004; Litvak and Last, 2008; Das Gollapalli and Caragea, 2014), part-of-speech features (Barker and Cornacchia, 2000; Hulth, 2003), or more sophisticated metrics (Tomokiyo and Hurst, 2003). However, as we note above, these statistical approaches largely ignore the underlying tensions in scientific communication that lead to the creation of new keyphrases and how they are signaled to others. The result is that these techniques often find statistically “anomalous” phrases which often are not valid scie</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010. Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 21–26. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn E Lipscomb</author>
</authors>
<title>Medical subject headings (mesh).</title>
<date>2000</date>
<journal>Bull Med Libr Assoc.</journal>
<volume>88</volume>
<issue>3</issue>
<pages>265266</pages>
<contexts>
<context position="18057" citStr="Lipscomb, 2000" startWordPosition="2754" endWordPosition="2755">nformation (1973) Figure 1: Keyphrase hierarchy for Fault Tolerance (top) and Geographic Information (Bottom). Colors encode earliest appearance (brighter green is earlier) Computer Science, this collection contains articles and keyphrases from over a dozen domains2. MSRA provides a list of keyphrases with unique IDs and different stemming variations of each keyphrase. There are a total of 46,978 (without counting stemming variations) of which 30,477 keyphrases occur in ACM full-text corpus after stemming and normalization (64% coverage). 4.3 MeSH (MESHCORPUS) Medical Subject Headings (MeSH) (Lipscomb, 2000) is set of subject headings or descriptors in the life sciences domain. For the purpose of our work, we use the 27,149 keyphrases from the 2014 MeSH dataset. Similar to the other keyphrase lists we only use stemmed and normalized multi-word keywords that occur in in the ACM full-text corpus, which is 4,363 in case of MeSH. 4.4 Wikipedia (WIKICORPUS) Scientific article headings in Wikipedia can often be used as a proxy for keyphrases. To collect relevant titles, we find Wikipedia articles that exactly match (in title name) existing MeSH and MSRA keyphrases. For these “seed” articles, we compile</context>
</contexts>
<marker>Lipscomb, 2000</marker>
<rawString>Carolyn E. Lipscomb. 2000. Medical subject headings (mesh). Bull Med Libr Assoc. 88(3): 265266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Litvak</author>
<author>Mark Last</author>
</authors>
<title>Graph-based keyword extraction for single-document summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Multisource Multilingual Information Extraction and Summarization, MMIES ’08,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6672" citStr="Litvak and Last, 2008" startWordPosition="995" endWordPosition="998">s more data became available, it was possible to move away from heuristic cues and to leverage statistical techniques (Paice and Jones, 1993; Turney, 2000; Frank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 2004; Litvak and Last, 2008; Das Gollapalli and Caragea, 2014), part-of-speech features (Barker and Cornacchia, 2000; Hulth, 2003), or more sophisticated metrics (Tomokiyo and Hurst, 2003). However, as we note above, these statistical approaches largely ignore the underlying tensions in scientific communication that lead to the creation of new keyphrases and how they are signaled to others. The result is that these techniques often find statistically “anomalous” phrases which often are not valid scientific concepts (but are simply uncommon phrasing), are unstructured and disconnected, and inflexible to size variance (as</context>
</contexts>
<marker>Litvak, Last, 2008</marker>
<rawString>Marina Litvak and Mark Last. 2008. Graph-based keyword extraction for single-document summarization. In Proceedings of the Workshop on Multisource Multilingual Information Extraction and Summarization, MMIES ’08, pages 17–24, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Olena Medelyan</author>
<author>Eibe Frank</author>
<author>Ian H Witten</author>
</authors>
<title>Human-competitive tagging using automatic keyphrase extraction.</title>
<date>2009</date>
<journal>http://academic.research.microsoft.com. Accessed:</journal>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1318--1327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics. Microsoft.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1291" citStr="Medelyan et al., 2009" startWordPosition="177" endWordPosition="180">ty as a database in relation to existing collections. We further show how keyphrases can be placed into a semantically-meaningful “phylogenetic” structure and describe key features of this structure. The complete SCHBASE dataset is available at: http://cond.org/schbase.html. 1 Introduction Due to the immense practical value to Information Retrieval and other text mining applications, keyphrase extraction has become an extremely popular topic of research. Extracted keyphrases, specifically those derived from scientific literature, support search tasks (Anick, 2003), classification and tagging (Medelyan et al., 2009), information extraction (Wu and Weld, 2008), and higherlevel analysis such as the tracking of influence and dynamics of information propagation (Shi et al., 2010; Ohniwa et al., 2010). In our own work we use the extracted hierarchies to predict scientific emergence based on how rapidly new variants emerge. Keyphrases themselves capture a diverse set of scientific language (e.g., methods, techniques, materials, phenomena, processes, diseases, devices). Keyphrases, and their uses, have been studied extensively (Gil-Leiva and Alonso-Arroyo, 2007). However, automated keyphrase extraction work has</context>
</contexts>
<marker>Medelyan, Frank, Witten, 2009</marker>
<rawString>Olena Medelyan, Eibe Frank, and Ian H. Witten. 2009. Human-competitive tagging using automatic keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09, pages 1318–1327, Stroudsburg, PA, USA. Association for Computational Linguistics. Microsoft. 2015. Microsoft academic search. http://academic.research.microsoft.com. Accessed: 2015-2-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>404--411</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="6649" citStr="Mihalcea and Tarau, 2004" startWordPosition="990" endWordPosition="994">ments (Edmundson, 1969). As more data became available, it was possible to move away from heuristic cues and to leverage statistical techniques (Paice and Jones, 1993; Turney, 2000; Frank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 2004; Litvak and Last, 2008; Das Gollapalli and Caragea, 2014), part-of-speech features (Barker and Cornacchia, 2000; Hulth, 2003), or more sophisticated metrics (Tomokiyo and Hurst, 2003). However, as we note above, these statistical approaches largely ignore the underlying tensions in scientific communication that lead to the creation of new keyphrases and how they are signaled to others. The result is that these techniques often find statistically “anomalous” phrases which often are not valid scientific concepts (but are simply uncommon phrasing), are unstructured and disconnected, and inflexib</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 404–411, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ThuyDung Nguyen</author>
<author>Min-Yen Kan</author>
</authors>
<title>Keyphrase extraction in scientific publications. In Dion Hoe-Lian Goh, Tru Hoang Cao, Ingeborg Torvik Sølvberg,</title>
<date>2007</date>
<booktitle>Asian Digital Libraries. Looking Back 10 Years and Forging New Frontiers,</booktitle>
<volume>4822</volume>
<pages>317--326</pages>
<editor>and Edie Rasmussen, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Nguyen, Kan, 2007</marker>
<rawString>ThuyDung Nguyen and Min-Yen Kan. 2007. Keyphrase extraction in scientific publications. In Dion Hoe-Lian Goh, Tru Hoang Cao, Ingeborg Torvik Sølvberg, and Edie Rasmussen, editors, Asian Digital Libraries. Looking Back 10 Years and Forging New Frontiers, volume 4822 of Lecture Notes in Computer Science, pages 317–326. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryosuke L Ohniwa</author>
</authors>
<title>Aiko Hibino, and Kunio Takeyasu.</title>
<date>2010</date>
<journal>Scientometrics,</journal>
<volume>85</volume>
<issue>1</issue>
<marker>Ohniwa, 2010</marker>
<rawString>Ryosuke L. Ohniwa, Aiko Hibino, and Kunio Takeyasu. 2010. Trends in research foci in life science fields over the last 30 years monitored by emerging topics. Scientometrics, 85(1):111–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
<author>Paul A Jones</author>
</authors>
<title>The identification of important concepts in highly structured technical papers.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’93,</booktitle>
<pages>69--78</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6191" citStr="Paice and Jones, 1993" startWordPosition="917" endWordPosition="920">lding a domain-specific keyphrase database by comparing our extracted keyphrases to a number of author-defined and automaticallycreated keyphrase corpora. Finally, we illustrate how authors build upon each others’ terminology over time to create new keyphrases.1 1Full database available at: http://cond.org/schbase.html 2 Related Work Initial work in keyphrase extraction utilized heuristics that were based on the understood structure of scientific documents (Edmundson, 1969). As more data became available, it was possible to move away from heuristic cues and to leverage statistical techniques (Paice and Jones, 1993; Turney, 2000; Frank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 2004; Litvak and Last, 2008; Das Gollapalli and Caragea, 2014), part-of-speech features (Barker and Cornacchia, 2000; Hulth, 2003), or more sophis</context>
</contexts>
<marker>Paice, Jones, 1993</marker>
<rawString>Chris D. Paice and Paul A. Jones. 1993. The identification of important concepts in highly structured technical papers. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’93, pages 69–78, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<journal>http://snowball.tartarus.org/texts/introduction.html. Accessed:</journal>
<pages>2015--2</pages>
<contexts>
<context position="12809" citStr="Porter, 2001" startWordPosition="1951" endWordPosition="1952">panning nearly the entire history (1954-2011) of a domain (Computer Science) with full-text and clean metadata. The corpus itself contains both journal and conference articles (77k and 197k, respectively). In addition to the filtering rules described above, we manually constructed a set of filter terms to remove publication venues, agencies, and other institutions: ‘university’, ‘conference’, ‘symposium’, ‘journal’, ‘foundation’, ‘consortium’, ‘agency’, ‘institute’ and ‘school’ are discarded. We further normalize our keyphrases by lowercasing, removing hyphens, and using the Snowball stemmer (Porter, 2001) to merge plural variants. After stemming and normalizing, we found a total of 155,957 unique abbreviation expansions. Among these, 48,890 expansions occur more than once, 25,107 expansions thrice or more 608 and 16,916 expansions four or more times. We refer to this collection as the ABBREVCORPUS. For each keyphrase we search within the fulltext corpus to identify set of documents containing the keyphrase. This allowed us to find both the earliest mention of the keyphrase (the expansion, not the abbreviation) as well as overall popularity of keyphrases. We do not argue that abbreviations are </context>
</contexts>
<marker>Porter, 2001</marker>
<rawString>Martin F. Porter. 2001. Snowball: A language for stemming algorithms. http://snowball.tartarus.org/texts/introduction.html. Accessed: 2015-2-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel S Schwartz</author>
<author>Marti A Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<booktitle>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing,</booktitle>
<pages>451462</pages>
<contexts>
<context position="9594" citStr="Schwartz and Hearst, 2003" startWordPosition="1453" endWordPosition="1456">orks (IFN)”). Though there may be bias in the use of abbreviations in the Machine Learning literature, our experience has been that this holds in other domains as well. When a scientific keyphrase is used often enough, someone, somewhere, will have abbreviated it. 3.1 Abbreviation Extraction To find all abbreviation expansions we use the unsupervised SaRAD algorithm (Adar, 2004). This algorithm is simple to implement, does not require extremely large amounts of data, works for both acronyms and more general abbreviations, and has been demonstrated as effective in various contexts (Adar, 2004; Schwartz and Hearst, 2003). However, our solution does not depend on a specific implementation, only that we are able to accurately identify abbreviation expansions. Adar (2004) presents the full details for the algorithm, but for completeness we present the high level details. The algorithm progresses by identifying abbreviations inside of parentheses (defined as single words with at least one capital letter). The algorithm then extracts a “window” of text preceding the parenthesis, up to n words long (where n is the character length of the abbreviation plus padding). This window does not cross sentence boundaries. Wi</context>
</contexts>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>Ariel S Schwartz and Marti A Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text. Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing, page 451462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaolin Shi</author>
<author>Jure Leskovec</author>
<author>Daniel A McFarland</author>
</authors>
<title>Citing for high impact.</title>
<date>2010</date>
<booktitle>In Proceedings of the 10th Annual Joint Conference on Digital Libraries, JCDL ’10,</booktitle>
<pages>49--58</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1453" citStr="Shi et al., 2010" startWordPosition="203" endWordPosition="206">key features of this structure. The complete SCHBASE dataset is available at: http://cond.org/schbase.html. 1 Introduction Due to the immense practical value to Information Retrieval and other text mining applications, keyphrase extraction has become an extremely popular topic of research. Extracted keyphrases, specifically those derived from scientific literature, support search tasks (Anick, 2003), classification and tagging (Medelyan et al., 2009), information extraction (Wu and Weld, 2008), and higherlevel analysis such as the tracking of influence and dynamics of information propagation (Shi et al., 2010; Ohniwa et al., 2010). In our own work we use the extracted hierarchies to predict scientific emergence based on how rapidly new variants emerge. Keyphrases themselves capture a diverse set of scientific language (e.g., methods, techniques, materials, phenomena, processes, diseases, devices). Keyphrases, and their uses, have been studied extensively (Gil-Leiva and Alonso-Arroyo, 2007). However, automated keyphrase extraction work has often focused on large-scale statistical techniques and ignored the scientific communication literature. This literature points to the complex ways in which keyp</context>
</contexts>
<marker>Shi, Leskovec, McFarland, 2010</marker>
<rawString>Xiaolin Shi, Jure Leskovec, and Daniel A. McFarland. 2010. Citing for high impact. In Proceedings of the 10th Annual Joint Conference on Digital Libraries, JCDL ’10, pages 49–58, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Tomokiyo</author>
<author>Matthew Hurst</author>
</authors>
<title>A language model approach to keyphrase extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment - Volume 18, MWE ’03,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6833" citStr="Tomokiyo and Hurst, 2003" startWordPosition="1016" endWordPosition="1019">rank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 2004; Litvak and Last, 2008; Das Gollapalli and Caragea, 2014), part-of-speech features (Barker and Cornacchia, 2000; Hulth, 2003), or more sophisticated metrics (Tomokiyo and Hurst, 2003). However, as we note above, these statistical approaches largely ignore the underlying tensions in scientific communication that lead to the creation of new keyphrases and how they are signaled to others. The result is that these techniques often find statistically “anomalous” phrases which often are not valid scientific concepts (but are simply uncommon phrasing), are unstructured and disconnected, and inflexible to size variance (as in the case of fixed length n-grams), and fail to capture extremely rare terminology. The idea that abbreviations may be useful for keyphrase extraction has bee</context>
</contexts>
<marker>Tomokiyo, Hurst, 2003</marker>
<rawString>Takashi Tomokiyo and Matthew Hurst. 2003. A language model approach to keyphrase extraction. In Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment - Volume 18, MWE ’03, pages 33–40, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction.</title>
<date>2000</date>
<journal>Information Retrieval,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="6205" citStr="Turney, 2000" startWordPosition="921" endWordPosition="922"> keyphrase database by comparing our extracted keyphrases to a number of author-defined and automaticallycreated keyphrase corpora. Finally, we illustrate how authors build upon each others’ terminology over time to create new keyphrases.1 1Full database available at: http://cond.org/schbase.html 2 Related Work Initial work in keyphrase extraction utilized heuristics that were based on the understood structure of scientific documents (Edmundson, 1969). As more data became available, it was possible to move away from heuristic cues and to leverage statistical techniques (Paice and Jones, 1993; Turney, 2000; Frank et al., 1999) that could identify keyphrases within, and between, documents. The guiding model in this approach is that phrases that appear as statistical “anomalies” (by some measure) are effective for summarizing a document or corpus. This style of keyphrase extraction represents much of the current state-of-theart (Kim et al., 2010). Specific extensions in this space involve the use of network structures (Mihalcea and Tarau, 2004; Litvak and Last, 2008; Das Gollapalli and Caragea, 2014), part-of-speech features (Barker and Cornacchia, 2000; Hulth, 2003), or more sophisticated metric</context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>Peter D. Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2(4):303–336, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<title>Wikipedia: List of machine learning concepts. http://en.wikipedia.org/wiki/List of machine learning concepts.</title>
<date>2014</date>
<journal>Accessed:</journal>
<pages>2015--2</pages>
<contexts>
<context position="8637" citStr="Wikipedia, 2014" startWordPosition="1306" endWordPosition="1307">n extraction (Etzioni et al., 2008) on large scientific corpora. 3 Keyphrases and Hierarchies Our high level strategy for finding an initial set of keyphrases is to mine a corpus for abbrevia607 tion expansions. This is a simple strategy, but as we show below, highly effective. Though the idea that abbreviations and keyphrases are linked fits within our understanding of scientific writing, we confirmed our intuition through a small experiment. Specifically, we looked at the 85 unique keyphrases (in this case, article titles) listed in the Wikipedia entry for List of Machine Learning Concepts (Wikipedia, 2014). These ranged from well known terms (e.g., Support Vector Machines and Autoencoders) to less known (e.g., Information fuzzy networks). In all 85 cases we were able to find an abbreviation on the Web (using Google) alongside the expansion (e.g., searching for the phrases “Support Vector Machines (SVMs)” or “Information Fuzzy Networks (IFN)”). Though there may be bias in the use of abbreviations in the Machine Learning literature, our experience has been that this holds in other domains as well. When a scientific keyphrase is used often enough, someone, somewhere, will have abbreviated it. 3.1 </context>
</contexts>
<marker>Wikipedia, 2014</marker>
<rawString>Wikipedia. 2014. Wikipedia: List of machine learning concepts. http://en.wikipedia.org/wiki/List of machine learning concepts. Accessed: 2015-2-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Automatically refining the wikipedia infobox ontology.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on World Wide Web, WWW ’08,</booktitle>
<pages>635--644</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1335" citStr="Wu and Weld, 2008" startWordPosition="184" endWordPosition="187">ions. We further show how keyphrases can be placed into a semantically-meaningful “phylogenetic” structure and describe key features of this structure. The complete SCHBASE dataset is available at: http://cond.org/schbase.html. 1 Introduction Due to the immense practical value to Information Retrieval and other text mining applications, keyphrase extraction has become an extremely popular topic of research. Extracted keyphrases, specifically those derived from scientific literature, support search tasks (Anick, 2003), classification and tagging (Medelyan et al., 2009), information extraction (Wu and Weld, 2008), and higherlevel analysis such as the tracking of influence and dynamics of information propagation (Shi et al., 2010; Ohniwa et al., 2010). In our own work we use the extracted hierarchies to predict scientific emergence based on how rapidly new variants emerge. Keyphrases themselves capture a diverse set of scientific language (e.g., methods, techniques, materials, phenomena, processes, diseases, devices). Keyphrases, and their uses, have been studied extensively (Gil-Leiva and Alonso-Arroyo, 2007). However, automated keyphrase extraction work has often focused on large-scale statistical te</context>
</contexts>
<marker>Wu, Weld, 2008</marker>
<rawString>Fei Wu and Daniel S. Weld. 2008. Automatically refining the wikipedia infobox ontology. In Proceedings of the 17th International Conference on World Wide Web, WWW ’08, pages 635–644, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>