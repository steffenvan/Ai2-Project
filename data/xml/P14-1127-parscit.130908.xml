<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.992418">
Unsupervised Morphology-Based Vocabulary Expansion
</title>
<author confidence="0.989703">
Mohammad Sadegh Rasooli, Thomas Lippincott, Nizar Habash and Owen Rambow
</author>
<affiliation confidence="0.979742">
Center for Computational Learning Systems
Columbia University, New York, NY, USA
</affiliation>
<email confidence="0.995071">
{rasooli,tom,habash,rambow}@ccls.columbia.edu
</email>
<sectionHeader confidence="0.997333" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975875">
We present a novel way of generating un-
seen words, which is useful for certain ap-
plications such as automatic speech recog-
nition or optical character recognition in
low-resource languages. We test our vo-
cabulary generator on seven low-resource
languages by measuring the decrease in
out-of-vocabulary word rate on a held-out
test set. The languages we study have
very different morphological properties;
we show how our results differ depend-
ing on the morphological complexity of
the language. In our best result (on As-
samese), our approach can predict 29% of
the token-based out-of-vocabulary with a
small amount of unlabeled training data.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937431034483">
In many applications in human language technolo-
gies (HLT), the goal is to generate text in a target
language, using its standard orthography. Typical
examples include automatic speech recognition
(ASR, also known as STT or speech-to-text), opti-
cal character recognition (OCR), or machine trans-
lation (MT) into a target language. We will call
such HLT applications “target-language genera-
tion technologies” (TLGT). The best-performing
systems for these applications today rely on train-
ing on large amounts of data: in the case of ASR,
the data is aligned audio and transcription, plus
large unannotated data for the language model-
ing; in the case of OCR, it is transcribed optical
data; in the case of MT, it is aligned bitexts. More
data provides for better results. For languages with
rich resources, such as English, more data is often
the best solution, since the required data is readily
available (including bitexts), and the cost of anno-
tating (e.g., transcribing) data is outweighed by the
potential significance of the systems that the data
will enable. Thus, in HLT, improvements in qual-
ity are often brought about by using larger data
sets (Banko and Brill, 2001).
When we move to low-resource languages, the
solution of simply using more data becomes less
appealing. Unannotated data is less readily avail-
able: for example, at the time of publishing this
paper, 55% of all websites are in English, the top
10 languages collectively account for 90% of web
presence, and the top 36 languages have a web
presence that covers at least 0.1% of web sites.1
All other languages (and all languages considered
in this paper except Persian) have a web presence
of less than 0.1%. Considering Wikipedia, another
resource often used in HLT, English has 4.4 mil-
lion articles, while only 48 other languages have
more than 100,000.2 As attention turns to de-
veloping HLT for more languages, including low-
resource languages, alternatives to “more-data”
approaches become important.
At the same time, it is often not possible to use
knowledge-rich approaches. For low-resource lan-
guages, resources such as morphological analyz-
ers are not usually available, and even good schol-
arly descriptions of the morphology (from which
a tool could be built) are often not available. The
challenge is therefore to use data, but to make do
with a small amount of data, and thus to use data
better. This paper is a contribution to this goal.
Specifically, we address TLGTs, i.e., the types
of HLT mentioned above that generate target lan-
guage text. We propose a new approach to gener-
ating unseen words of the target language which
have not been seen in the training data. Our ap-
proach is entirely unsupervised. It assumes that
word-units are specified, typically by whitespace
and punctuation.
</bodyText>
<footnote confidence="0.98645425">
1http://en.wikipedia.org/wiki/
Languages_used_on_the_Internet
2http://meta.wikimedia.org/wiki/List_
of_Wikipedias
</footnote>
<page confidence="0.913255">
1349
</page>
<note confidence="0.8335925">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1349–1359,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99986958974359">
Expanding the vocabulary of the target lan-
guage can be useful for TLGTs in different ways.
For ASR and OCR, which can compose words
from smaller units (phones or graphically recog-
nized letters), an expanded target language vocab-
ulary can be directly exploited without the need
for changing the technology at all: the new words
need to be inserted into the relevant resources (lex-
icon, language model) etc, with appropriately es-
timated probabilities. In the case of MT into mor-
phologically rich low-resource languages, mor-
phological segmentation is typically used in devel-
oping the translation models to reduce sparsity, but
this does not guarantee against generating wrong
word combinations. The expanded word combi-
nations can be used to extend the language models
used for MT to bias against incoherent hypothe-
sized new sequences of segmented words.
Our approach relies on unsupervised morpho-
logical segmentation. We do not in this paper con-
tribute to research in unsupervised morphological
segmentation; we only use it. The contribution
of this paper lies in proposing how to use the re-
sults of unsupervised morphological segmentation
in order to generate unseen words of the language.
We investigate several ways of doing so, and we
test them on seven low-resource languages. These
languages have very different morphological prop-
erties, and we show how our results differ depend-
ing on the morphological complexity of the lan-
guage. In our best result (on Assamese), we show
that our approach can predict 29% of the token-
based out-of-vocabulary with a small amount of
unlabeled training data.
The paper is structured as follows. We first dis-
cuss related work in Section 2. We then present
our method in Section 3, and present experimental
results in Section 4. We conclude with a discus-
sion of future work in Section 5.
</bodyText>
<sectionHeader confidence="0.999915" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99071768852459">
Approaches to Morphological Modeling
Computational morphology is a very active area
of research with a multitude of approaches that
vary in the degree of manual annotation needed,
and the amount of machine learning used. At one
extreme, we find systems that are painstakingly
and carefully designed by hand (Koskenniemi,
1983; Buckwalter, 2004; Habash and Rambow,
2006; D´etrez and Ranta, 2012). Next on the
continuum, we find work that focuses on defining
morphological models with limited lexica that
are then extended using raw text (Cl´ement et al.,
2004; Forsberg et al., 2006). In the middle of
this continuum, we find efforts to learn complete
paradigms using fully supervised methods relying
on completely annotated data points with rich
morphological information (Durrett and DeNero,
2013; Eskander et al., 2013). Next, there is
work on minimally supervised methods that use
available resources such as dictionaries, bitexts,
and other additional morphological annotations
(Yarowsky and Wicentowski, 2000; Cucerzan and
Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder
and Barzilay, 2008). At the other extreme, we
find unsupervised methods that learn morphology
models from unannotated data (Creutz and Lagus,
2007; Monson et al., 2008; Dreyer and Eisner,
2011; Sirts and Goldwater, 2013).
The work we present in this paper makes no
use of any morphological annotations whatsoever,
yet we are quite distinct from the approaches cited
above. We compare our work to two efforts specif-
ically. First, consider work in automatic mor-
phological segmentation learning from unanno-
tated data (Creutz and Lagus, 2007; Monson et
al., 2008). Unlike these approaches which provide
segmentations for training data and produce mod-
els that can be used to segment unseen words, our
approach can generate words that have not been
seen in the training data. The focus of efforts is
rather complementary: we actually use an off-the-
shelf unsupervised segmentation system (Creutz
and Lagus, 2007) as part of our approach. Second,
consider paradigm completion methods such as
the work of Dreyer and Eisner (2011). This effort
is closely related to our work although unlike it,
we make no assumptions about the data and do not
introduce any restrictions along the lines of deriva-
tion/inflectional morphology: Dreyer and Eisner
(2011) limited their work to verbal paradigms and
used annotated training data in addition to basic
assumptions about the problem such as the size
of the paradigms. In our approach, we have zero
annotated information and we do not distinguish
between inflectional and derivational morphology,
nor do we limit ourselves to a specific part-of-
speech (POS).
Vocabulary Expansion in HLT There have
been diverse approaches towards dealing with out-
of-vocabulary (OOV) words in ASR. In some
models, the approach is to expand the lexicon by
</bodyText>
<page confidence="0.98262">
1350
</page>
<bodyText confidence="0.999867709677419">
adding new words or pronunciations. Ohtsuki et
al. (2005) propose a two-run model where in the
first run, the input speech is recognized by the
reference vocabulary and relevant words are ex-
tracted from the vocabulary database and added
thereafter to the reference vocabulary to build an
expanded lexicon. Word recognition is done in the
second run based on the lexicon. Lei et al. (2009)
expanded the pronunciation lexicon via generat-
ing all possible pronunciations for a word be-
fore lattice generation and indexation. There are
also other methods for generating abbreviations in
voice search systems such as Yang et al. (2012).
While all of these approaches involve lexicon ex-
pansion, they do not employ any morphological
information.
In the context of MT, several researchers have
addressed the problem of OOV words by relating
them to known in-vocabulary (INV) words. Yang
and Kirchhoff (2006) anticipated OOV words
that are potentially morphologically related using
phrase-based backoff models. Habash (2008) con-
sidered different techniques for vocabulary expan-
sion online. One of their techniques learned mod-
els of morphological mapping between morpho-
logically rich source words in Arabic that pro-
duce the same English translation. This was used
to relate an OOV word to a morphologically re-
lated INV word. Another technique expanded
the MT phrase tables with possible transliterations
and spelling alternatives.
</bodyText>
<sectionHeader confidence="0.9994615" genericHeader="method">
3 Morphology-based Vocabulary
Expansion
</sectionHeader>
<subsectionHeader confidence="0.999626">
3.1 Approach
</subsectionHeader>
<bodyText confidence="0.999699">
Our approach to morphology-based vocabulary
expansion consists of three steps (Figure 1). We
start with a “training” corpus of (unannotated)
words and generate a list of new (unseen) words
that expands the vocabulary of the training corpus.
</bodyText>
<listItem confidence="0.977498714285714">
1. Unsupervised Morphology Segmentation
The first step is to segment each word in the
training corpus into sequences of prefixes,
stem and suffixes, where the prefixes and suf-
fixes are optional.3
2. FST-based Morphology Expansion We
then construct new word models using the
</listItem>
<bodyText confidence="0.7820295">
3In this paper, we use an off-the-shelf system for this step
but plan to explore new methods in the future, such as joint
segmentation and expansion.
segmented stems and affixes. We explore two
different techniques for morphology-based
vocabulary expansion that we discuss below.
The output of these models is represented as
a weighted finite state machine (WFST).
</bodyText>
<listItem confidence="0.736036857142857">
3. Reranking Models Given that the size of the
expanded vocabulary can be quite large and
it may include a lot of over-generation, we
rerank the expanded set of words before tak-
ing the top n words to use in downstream
processes. We consider four reranking con-
ditions which we describe below.
</listItem>
<figureCaption confidence="0.9734715">
Figure 1: The flowchart of the lexicon expansion
system.
</figureCaption>
<subsectionHeader confidence="0.999418">
3.2 Morphology Expansion Techniques
</subsectionHeader>
<bodyText confidence="0.9996875">
As stated above, the input to the morphology ex-
pansion step is a list of words segmented into mor-
phemes: zero or more prefixes, one stem, and zero
or more suffixes. Figure 2a presents an example of
such input using English words (for clarity).
We use two different models of morphology ex-
pansion in this paper: Fixed Affix model and Bi-
gram Affix model.
</bodyText>
<subsectionHeader confidence="0.652081">
3.2.1 Fixed Affix Expansion Model
</subsectionHeader>
<bodyText confidence="0.999501">
In the Fixed Affix model, we construct a set of
fused prefixes from all the unique prefix sequences
in the training data; and we similarly construct a
</bodyText>
<figure confidence="0.997199129032258">
Training Transcripts
Unsupervised
Morphology
Segmentation
Segmented Words
FST-based
Expansion Model
Reranked Expansion
Expanded List
Reranking
1351
re+ pro+ duc +e
func +tion +al
re+ duc +e
re+ duc +tion +s
in
pro+ duct
concept +u +al + ly
(a) Training data with morpheme boundaries. Prefixes end with and suffixes start with “+” signs.
(b) FST for the Fixed Affix expansion model
duc
duc
repro
e
&lt;epsilon&gt;
0 1
re
pro
func
in
concept
duct
tional
tions
2
utually
&lt;epsilon&gt;
3
re
0 4
&lt;epsilon&gt;
pro
&lt;epsilon&gt;
1
func
in
concept
duct
2
tion
tion
u
5
&lt;epsilon&gt;
al
7
e
ly
6 &lt;epsilon&gt;
s
3
(c) FST for the Bigram Affix expansion model
</figure>
<figureCaption confidence="0.705661">
Figure 2: Two models of word generation from morphologically annotated data. In our experiments, we
used weighted finite state machine. We use character-based WFST in the implementation to facilitate
analyzing inputs as well as word generation.
</figureCaption>
<bodyText confidence="0.947658368421053">
set of fused suffixes from all the unique suffix se-
quences in the training data. In other words, we
simply pick characters from beginning of the word
up to the first stem as the prefix and characters
from the first suffix to the end of the word as the
suffix. Everything in the middle is the stem. In
this model, each word has one single prefix and
one single suffix (each of which can be empty in-
dependently). The Fixed Affix model is simply
the concatenation of the disjunction of all prefixes
with the disjunction of all stems and the disjunc-
tion of all suffixes into one FST:
prefix —* stem —* suffix
The morpheme paths in the FST are weighted to
reflect their probability in the training corpus.4
Figure 2b exemplifies a Fixed Affix model derived
from the example training data in Figure 2a.
4We convert the probability into a cost by taking the neg-
ative of the log of the probability.
</bodyText>
<subsubsectionHeader confidence="0.503137">
3.2.2 Bigram Affix Expansion Model
</subsubsectionHeader>
<bodyText confidence="0.999967466666667">
In the Bigram Affix model, we do the same for the
stem as in the Fixed Affix model, but for prefixes
and suffixes, we create a bigram language model
in the finite state machine. The advantage of this
technique is that unseen compound affixes can be
generated by our model. For example, the Fixed
Affix model in Figure 2b cannot generate the word
func+tion+al+ly since the suffix +tionally is not
seen in the training data. However, this word can
be generated in the Bigram Affix model as shown
in Figure 2c: there is a path passing 0 —* 4 —* 1 —*
2 —* 5 —* 6 —* 3 in the FST that can produce this
word. We expect this model to have better recall
for generating new words in the language because
of its affixation flexibility.
</bodyText>
<subsectionHeader confidence="0.999125">
3.3 Reranking Techniques
</subsectionHeader>
<bodyText confidence="0.9999515">
The expanded models allow for a large number of
words to be generated. We limit the number of vo-
cabulary expansion using different thresholds af-
ter reranking or reweighing the WFSTs generated
</bodyText>
<page confidence="0.977141">
1352
</page>
<bodyText confidence="0.808743">
above. We consider four reranking conditions.
3.3.1 No Reranking (NRR)
The baseline reranking option is no reranking
(NRR). In this approach we use the weights in
the WFST, which are based on the independent
prefix/stem/suffix probabilities, to determine the
ranking of the expanded vocabulary.
</bodyText>
<subsubsectionHeader confidence="0.55263">
3.3.2 Trigraph-based Reweighting (W◦Tr)
</subsubsectionHeader>
<bodyText confidence="0.999965636363636">
We reweight the weights in the WFST model
(Fixed or Bigram) by composing it with a letter
trigraph language model (W ◦Tr). A letter tri-
graph LM is itself a WFST where each trigraph (a
sequence of three consequent letters) has an asso-
ciated weight equal to its negative log-likelihood
in the training data. This reweighting allows us
to model preferences of sequences of word letters
seen more in the training data. For example, in a
word like producttions, the trigraphs ctt and tti are
very rare and thus decrease its probability.
</bodyText>
<subsectionHeader confidence="0.602436">
3.3.3 Trigraph-based Reranking (TRR)
</subsectionHeader>
<bodyText confidence="0.999978">
When we compose our initial WFST with the tri-
graph FST, the probability of each generated word
from the new FST is equal to the product of the
probability of its morphemes and the probabilities
of each trigraph in that word. This basically makes
the model prefer shorter words and may degrade
the effect of morphology information. Instead of
reweighting the WFST, we get the n-best list of
generated words and rerank them using their tri-
graph probabilities. We will refer to this technique
as TRR.
</bodyText>
<sectionHeader confidence="0.8708555" genericHeader="method">
3.3.4 Reranking Morpheme Boundaries
(BRR)
</sectionHeader>
<bodyText confidence="0.99998975">
The last reranking technique reranks the n-best
generated word list with trigraphs that are incident
on the morpheme boundaries (in case of Bigram
Affix model, the last prefix and first suffix). The
intuition is that we already know that any mor-
pheme that is generated from the morphology FST
is already seen in the training data but the bound-
ary for different morphemes are not guaranteed to
be seen in the training data. For example, for the
word producttions, we only take into account the
trigraphs rod, odu, ctt and tti instead of all possible
trigraphs. We will refer to this technique as BRR.
</bodyText>
<sectionHeader confidence="0.999771" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999843">
4.1 Evaluation Data and Tools
</subsectionHeader>
<bodyText confidence="0.994119875">
Evaluation Data The IARPA Babel program is
a research program for developing rapid spoken
detection systems for under-resourced languages
(Harper, 2013). We use the IARPA Babel pro-
gram limited language pack data which consists
of 20 hours of telephone speech with transcrip-
tion. We use six languages which are known
to have rich morphology: Assamese (IARPA-
babel102b-v0.5a), Bengali (IARPA-babel103b-
v0.4b), Pashto (IARPA-babel104b-v0.4bY), Taga-
log (IARPA-babel106-v0.2g), Turkish (IARPA-
babel105b-v0.4) and Zulu (IARPA-babel206b-
v0.1e). Speech annotation such as silences and
hesitations are removed from transcription and all
words are turned into lower-case (for languages
using the Roman script – Tagalog, Turkish and
Zulu). Moreover, in order to be able to perform a
manual error analysis, we include a language that
has rich morphology and of which the first author
is a native speaker: Persian. We sampled data from
the training and development set of the Persian de-
pendency treebank (Rasooli et al., 2013) to create
a comparable seventh dataset in Persian. Statis-
tics about the datasets are shown in Table 1. We
also conduct further experiments on just verbs and
nouns in the data set for Persian (Persian-N and
Persian V). As shown in Table 1, the training data
is very small and the OOV rate is high especially
in terms of types. For some languages that have
richer morphology such as Turkish and Zulu, the
OOV rate is much higher than other languages.
Word Generation Tools and Settings For un-
supervised learning of morphology, we use Mor-
fessor CAT-MAP (v. 0.9.2) which was shown to be
a very accurate morphological analyzer for mor-
phologically rich languages (Creutz and Lagus,
2007). In order to be able to analyze Unicode-
based data, we convert each character in our
dataset to some conventional ASCII character and
then train Morfessor on the mapped dataset; after
finishing the training, we map the data back to the
original character set. We use the default setting
in Morfessor for unsupervised learning.
For preparing the WFST, we use OpenFST (Ri-
ley et al., 2009). We get the top one million short-
est paths (i.e., least costly paths of words) and ap-
ply our reranking models on them. It is worth
pointing out that our WFSTs are character-based
</bodyText>
<page confidence="0.941236">
1353
</page>
<table confidence="0.999796090909091">
Language Training Data Development Data
Type Token Type Token Type OOV% Token OOV%
Assamese 8694 73151 7253 66184 49.57 8.28
Bengali 9460 81476 7794 70633 50.65 8.47
Pashto 6968 115069 6135 108137 44.89 4.25
Persian 14047 71527 10479 42939 44.16 12.78
Tagalog 6213 69577 5480 64334 54.95 7.81
Turkish 11985 77128 9852 67042 56.84 12.34
Zulu 15868 65655 13756 57141 68.72 21.76
Persian-N 9204 31369 7502 18816 46.36 22.11
Persian-V 2653 11409 1332 7318 41.07 9.01
</table>
<tableCaption confidence="0.8949275">
Table 1: Statistics of training and development data for morphology-based unsupervised word generation
experiments.
</tableCaption>
<bodyText confidence="0.999383833333333">
and thus we also have a morphological analyzer
that can give all possible segmentations for a given
word. By running the morphological analyzer on
the OOVs, we can have the potential upper bound
of OOV reduction by the system (labeled “∞” in
Tables 2 and 3).
</bodyText>
<subsectionHeader confidence="0.987946">
4.2 Lexicon Expansion Results
</subsectionHeader>
<bodyText confidence="0.99996903125">
The results for lexicon expansion are shown in Ta-
ble 2 for types and Table 3 for tokens.
We use the trigraph WFST as our baseline
model. This model does not use any morphologi-
cal information. In this case, words are generated
according to the likelihood of their trigraphs, with-
out using any information from the morphologi-
cal segmentation. We call this model the trigraph
WFST (Tr. WFST). We consistently have better
numbers than this baseline in all of our models
except for Pashto when measured by tokens. ∞
is the upper-bound OOV reduction for our expan-
sion model: for each word in the development set,
we ask if our model, without any vocabulary size
restriction at all, could generate it.
The best results (again, except for Pashto) are
achieved using one of the three reranking methods
(reranking by trigraph probabilities or morpheme
boundaries) as opposed to doing no reranking. To
our surprise, the Fixed Affix model does a slightly
better job in reducing out of vocabulary than the
Bigram Affix model. We can also see from the
results that reranking in general is very effective.
We also compare our models with the case that
there is much more training data and we do not do
vocabulary expansion at all. In Table 2 and Ta-
ble 3, “FP” indicates the full language pack for
the Babel project data which is approximately six
to eight times larger than the limited pack training
data, and the full training data for Persian which
is approximately five times larger. We see that
the larger training data outperforms our methods
in all languages. However, from the results of ∞,
which is the upper-bound OOV reduction by our
expansion model, for some languages such as As-
samese, our numbers are close to the FP results
and for Zulu it is even better than FP.
We also study how OOV reduction is affected
by the size of the generated vocabulary. The
trends for different sizes of the lexicon expansion
by Fixed Affix model that is reranked by trigraph
probabilities is shown in Figure 3. As seen in the
results, for languages that have richer morphol-
ogy, it is harder to achieve results near to the up-
per bound. As an outlier, morphology does not
help for Pashto. One possible reason might be that
based on the results in Table 4, Morfessor does not
explore morphology in Pashto as well as other lan-
guages.
Morphological Complexity As for further anal-
ysis, we can study the correlation between mor-
phological complexity and hardness of reducing
OOVs. Much work has been done in linguis-
tics to classify languages (Sapir, 1921; Greenberg,
1960). The common wisdom is that languages
are not either agglutinative or fusional, but are
on a spectrum; however, no work to our knowl-
edge places all languages (or at least the ones we
worked on) on such a spectrum. We propose sev-
eral metrics. First, we can consider the number
of unique affixival morphemes in each language,
as determined by Morfessor. As shown in Table 4
(|pr |+ |sf |), Zulu has the most morphemes and
Pashto the fewest. A second possible metric of the
</bodyText>
<page confidence="0.982228">
1354
</page>
<table confidence="0.999557076923077">
Language Tr. Fixed Affix Model Bigram Affix Model FP
WFST
NRR WoTr TRR BRR oc NRR WoTr TRR BRR oc
Assamese 15.94 24.03 28.46 28.15 27.15 48.07 23.50 28.15 27.84 26.59 51.02 50.96
Bengali 15.68 20.09 24.75 24.49 22.54 40.98 21.78 24.65 24.67 23.51 42.55 48.83
Pashto 18.70 19.03 19.28 19.24 18.63 25.13 19.43 18.81 18.92 18.77 25.24 64.96
Persian 12.83 18.95 18.39 19.30 19.99 50.11 18.58 18.09 18.65 18.84 53.13 58.45
Tagalog 11.39 14.61 16.51 16.21 16.81 35.64 14.45 16.01 15.81 16.74 38.72 53.64
Turkish 07.75 09.11 14.79 14.79 14.71 55.48 09.04 13.63 14.34 13.52 66.54 53.54
Zulu 07.63 11.87 12.96 13.87 13.68 66.73 12.04 12.35 13.69 13.75 82.38 35.62
Average 12.85 16.81 19.31 19.31 19.07 46.02 17.02 18.81 19.13 18.81 51.37 52.29
Persian-N 14.86 24.67 22.74 22.83 24.15 37.32 23.78 21.68 22.51 23.32 38.38 -
Persian-V 54.84 68.19 72.39 73.49 71.12 80.44 67.28 71.48 72.58 70.02 80.62 -
</table>
<tableCaption confidence="0.995081">
Table 2: Type-based expansion results for the 50k-best list for different models. Tr. WFST stands for
</tableCaption>
<bodyText confidence="0.93741375">
trigraph WFST, NRR for no reranking, WoTr for trigraph reweighting, TRR for trigraph-based rereank-
ing, BRR for reranking morpheme boundary, and oc for the upper bound of OOV reduction via lexicon
expansion if we produce all words. FP (full-pack data) shows the effect of using bigger data with the size
of about seven times larger than our data set, instead of using our unsupervised approach.
</bodyText>
<table confidence="0.999706153846154">
Language Tr. Fixed Affix Model Bigram Affix Model FP
WFST
NRR WoTr TRR BRR oc NRR WoTr TRR BRR oc
Assamese 18.07 25.70 29.43 29.12 28.13 47.88 25.34 29.06 28.82 27.64 50.31 58.03
Bengali 17.79 20.91 25.61 25.27 23.65 40.60 22.58 25.20 25.41 24.77 42.22 55.92
Pashto 21.27 19.40 19.94 19.92 18.59 25.45 19.68 19.40 19.29 18.72 25.58 71.46
Persian 14.78 20.77 20.32 21.30 22.03 51.00 20.63 19.72 20.61 20.95 54.01 63.10
Tagalog 12.88 14.55 16.88 16.36 16.60 33.95 14.37 16.12 16.12 16.38 37.07 61.53
Turkish 09.97 11.42 17.82 17.67 17.23 56.54 11.05 16.82 17.41 15.98 66.54 59.68
Zulu 08.85 13.70 14.72 15.62 15.67 68.07 13.70 14.07 15.47 15.60 87.90 41.27
Average 14.80 18.06 20.67 20.75 20.27 44.78 18.19 20.48 20.45 20.01 51.95 58.71
Persian-N 16.82 26.46 24.42 24.56 25.71 38.40 25.69 23.50 24.20 25.04 39.41 –
Persian-V 60.09 71.47 75.57 76.48 73.60 82.55 70.56 74.81 75.72 72.53 82.70 –
</table>
<tableCaption confidence="0.998752">
Table 3: Token-based expansion results for the 50k-best list for different models. Abbreviations are the
</tableCaption>
<bodyText confidence="0.993979484848485">
same as Table 2.
complexity of the morphology is by calculating
the average number of unique prefix-suffix pairs
in the training data after morpheme segmentation
which is shown as |If  |in Table 4. Finally, a third
possible metric is the number of all possible words
that can be generated (|L|). These three metrics
correlate fairly well across the languages.
The metrics we propose also correlate with
commonly accepted classifications: e.g., Zulu and
Turkish (highly agglutinative) have higher scores
in terms of our |pr |+ |sf |, |If  |and |L |metrics in
Table 4 than other languages. The results from full
language packs in Table 3 also show that there is
a reverse interaction of morphological complexity
and the effect of blindly adding more data. Thus
for morphologically rich languages, adding more
data is less effective than for languages with poor
morphology.
The size of the languages (|L|) suggests that we
are suffering from vast overgeneration; we over-
generate because in our model any affix can at-
tach to any stem, which is not in general true.
Thus there is a lack of linguistic knowledge such
as paradigm information (Stump, 2001) for each
word category in our model. In other words, all
morphemes are treated the same in our model
which is not true in natural languages. One way
to tackle this problem is through an unsupervised
POS tagger. The challenge here is that fully unsu-
pervised POS taggers (without any tag dictionary)
are not very accurate (Christodoulopoulos et al.,
2010). Another way is through using joint mor-
</bodyText>
<page confidence="0.994163">
1355
</page>
<figureCaption confidence="0.9934565">
Figure 3: Trends for token-based OOV reduction with different sizes for the Fixed Affix model with
trigraph reranking.
</figureCaption>
<table confidence="0.9997992">
Language |pr ||stm ||sf ||L ||If|
Assamese 4 4791 564 10.8M 1.8
Bengali 3 6496 378 7.4M 1.5
Pashto 1 5395 271 1.5M 1.3
Persian 49 6998 538 184M 2.0
Tagalog 179 4259 299 228M 1.5
Turkish 45 5266 1801 427M 2.3
Zulu 2254 5680 427 5.5B 2.8
Persian-N 3 6121 268 4.9M 1.5
Persian-V 43 788 44 1.5M 3.4
</table>
<tableCaption confidence="0.995557">
Table 4: Information about the number of unique
</tableCaption>
<bodyText confidence="0.996679074074074">
morphemes in the Fixed Affix model for each
dataset including empty affixes. |L |shows the
upper bound of the number of possible unique
words that can be generated from the word gener-
ation model. |If |is the average number of unique
prefix-suffix pairs (including empty pairs) for each
stem.
phology and tagging models such as Frank et al.
(2013).
Error Analysis on Turkish Unfortunately for
most languages we could not find an available
rule-based or supervised morphological analyzer
to verify the words generated by our model. The
only available tool for us is a Turkish finite-state
morphological analyzer (Oflazer, 1996) imple-
mented with the Xerox FST toolkit (Beesley and
Karttunen, 2003). As we can see in Table 5, the
system with the largest proportion of correct gen-
erated words reranks the expansion with trigraph
probabilities using a Fixed Affix model. Results
also show that we are overgenerating many non-
sense words that we ought to be pruning from our
results. Another observation is that the recognition
percentage of the morphological analyzer on INV
words is much higher than on OOVs, which shows
that OOVs in Turkish dataset are much harder to
analyze.
</bodyText>
<page confidence="0.969835">
1356
</page>
<table confidence="0.999759538461538">
Model Precision
Tr. WFST 17.19
NRR 13.36
Fixed Affix Model W°Tr 25.66
TRR 26.30
BRR 25.14
NRR 12.94
Bigram Affix Model W°Tr 24.21
TRR 25.39
BRR 23.45
words 89.30
Development INVs 95.44
OOVs 84.64
</table>
<tableCaption confidence="0.7747535">
Table 5: Results from running a hand-crafted
Turkish morphological analyzer (Oflazer, 1996)
</tableCaption>
<bodyText confidence="0.999685085106383">
on different expansions and on the development
set. Precision refers to the percentage of the words
are recognized by the analyzer. The results on de-
velopment are also separated into INV and OOV.
Error Analysis on Persian From the best 50k
word result for Persian (Fixed Affix Model:BRR),
we randomly picked 200 words and manually an-
alyzed them. 89 words are correct (45.5%) where
55.0% of these words are from noun affixation,
23.6% from verb clitics, 9.0% from verb inflec-
tions, 5.6% from incorrect affixations that acci-
dentally resulted in possible words, 4.5% from un-
inflected stems, and a few from adjective affixa-
tion. Among incorrectly generated words, 65.8%
are from combining a stem of one POS with af-
fixes from another POS (e.g., attaching a noun af-
fix to a verb stem), 14.4% from combining a stem
with affixes which are compatible with POS but
not allowed for that particular stem (e.g., there is
a noun suffix that can only attach to a subset of
noun stems), 9.0% are from wrong affixes pro-
duced by Morfessor and others are from incorrect
vowel harmony or double affixation.
In order to study the effect of vocabulary ex-
pansion more deeply, we trained a subset of all
nouns and verbs in the same dataset (also shown
in Table 1). Verbs in Persian have rich but more
or less regular morphology, while nouns, which
have many irregular cases, have rich morphol-
ogy but not as rich as verbs. The results in Ta-
ble 4 show that Morfessor captures these phenom-
ena. Furthermore, our results in Table 2 and Ta-
ble 3 show that our performance on OOV reduc-
tion with verbs is far superior to our performance
with nouns. We also randomly picked 200 words
from each of the experiments (noun and verbs)
to study the degree of correctness of generated
forms. For nouns, 94 words are correct and for
verbs only 71 words are correct. Most verb errors
are due to incorrect morpheme extraction by Mor-
fessor. In contrast, most noun errors result from
affixes that are only compatible with a subset of
all possible noun stems. This suggests that if we
conduct experiments using more accurate unsu-
pervised morphology and also have a more fine-
grained paradigm completion model, we might
improve our performance.
</bodyText>
<sectionHeader confidence="0.998298" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999964117647059">
We have presented an approach to generating new
words. This approach is useful for low-resource,
morphologically rich languages. It provides words
that can be used in HLT applications that require
target-language generation in this language, such
as ASR, OCR, and MT. An implementation of our
approach, named BabelGUM (Babel General Un-
supervised Morphology), will be publicly avail-
able. Please contact the authors for more infor-
mation.
In future work we will explore the possibil-
ity of jointly performing unsupervised morpho-
logical segmentation with clustering of words
into classes with similar morphological behavior.
These classes will extend POS classes. We will
tune the system for our purposes, namely OOV re-
duction.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998850625">
We thank Anahita Bhiwandiwalla, Brian Kings-
bury, Lidia Mangu, Michael Picheny, Benoit
Sagot, Murat Saraclar, and G´eraldine Walther for
helpful discussions. The project is supported by
the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Department of Defense U.S.
Army Research Laboratory (DoD/ARL) contract
number W911NF-12-C-0012. The U.S. Govern-
ment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright annotation thereon. Disclaimer:
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, DoD/ARL, or the U.S. Government.
</bodyText>
<page confidence="0.993094">
1357
</page>
<sectionHeader confidence="0.996261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99961571559633">
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, ACL
’01, pages 26–33, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Kenneth R Beesley and Lauri Karttunen. 2003. Finite-
state morphology: Xerox tools and techniques.
CSLI, Stanford.
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsu-
pervised pos induction: How far have we come?
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
575–584. Association for Computational Linguis-
tics.
Lionel Cl´ement, Benoit Sagot, and Bernard Lang.
2004. Morphology based automatic acquisition of
large-coverage lexica. In Proceedings of the Fourth
International Conference on Language Resources
and Evaluation (LREC’04). European Language Re-
sources Association (ELRA).
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In The 6th Conference on Natural Lan-
guage Learning (CoNLL-2002), pages 1–7.
Gr´egoire D´etrez and Aarne Ranta. 2012. Smart
paradigms and the predictability and complexity of
inflectional morphology. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 645–653.
Association for Computational Linguistics.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using a
dirichlet process mixture model. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 616–627. Associ-
ation for Computational Linguistics.
Greg Durrett and John DeNero. 2013. Supervised
learning of complete morphological paradigms. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1185–1195. Association for Computa-
tional Linguistics.
Ramy Eskander, Nizar Habash, and Owen Rambow.
2013. Automatic extraction of morphological lex-
icons from morphologically annotated corpora. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1032–1043, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Markus Forsberg, Harald Hammarstr¨om, and Aarne
Ranta. 2006. Morphological lexicon extraction
from raw text data. Advances in Natural Language
Processing, pages 488–499.
Stella Frank, Frank Keller, and Sharon Goldwater.
2013. Exploring the utility of joint morphological
and syntactic learning from child-directed speech.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
30–41. Association for Computational Linguistics.
Joseph H Greenberg. 1960. A quantitative approach to
the morphological typology of language. Interna-
tional journal of American linguistics, pages 178–
194.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A morphological analyzer and generator for the Ara-
bic dialects. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 681–688, Sydney, Aus-
tralia.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in Arabic-English
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 57–60. Association for
Computational Linguistics.
Mary Harper. 2013. The babel program and low
resource speech technology. In Automatic Speech
Recognition and Understanding Workshop (ASRU)
Invited talk.
Kimmo Koskenniemi. 1983. Two-Level Model for
Morphological Analysis. In Proceedings of the 8th
International Joint Conference on Artificial Intelli-
gence, pages 683–685.
Xin Lei, Wen Wang, and Andreas Stolcke. 2009.
Data-driven lexicon expansion for Mandarin broad-
cast news and conversation speech recognition. In
International conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 4329–4332.
Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. Paramor: Finding paradigms
across morphology. Advances in Multilingual and
Multimodal Information Retrieval, pages 900–907.
Sylvain Neuvel and Sean A Fulop. 2002. Unsuper-
vised learning of morphology without morphemes.
In Proceedings of the ACL-02 workshop on Morpho-
logical and phonological learning-Volume 6, pages
31–40. Association for Computational Linguistics.
</reference>
<page confidence="0.828371">
1358
</page>
<reference confidence="0.9995258">
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22(1):73–89.
Katsutoshi Ohtsuki, Nobuaki Hiroshima, Masahiro
Oku, and Akihiro Imamura. 2005. Unsupervised
vocabulary expansion for automatic transcription of
broadcast news. In International conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 1021–1024.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of
a Persian syntactic dependency treebank. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
306–314. Association for Computational Linguis-
tics.
Michael Riley, Cyril Allauzen, and Martin Jansche.
2009. Openfst: An open-source, weighted finite-
state transducer library and its applications to speech
and language. In Human Language Technologies
Tutorials: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 9–10.
Edward Sapir. 1921. Language: An introduction to
the study of speech. Harcourt, Brace and company
(New York).
Kairit Sirts and Sharon Goldwater. 2013. Minimally-
supervised morphological segmentation using adap-
tor grammars. Transactions for the ACL, 1:255–266.
Benjamin Snyder and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphologi-
cal segmentation. In Proceedings of the 46th an-
nual meeting of the association for computational
linguistics: Human language Technologies (ACL-
HLT), pages 737–745. Association for Computa-
tional Linguistics.
Gregory T. Stump. 2001. A theory of paradigm struc-
ture. Cambridge.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 41–48, Trento,
Italy.
Dong Yang, Yi-Cheng Pan, and Sadaoki Furui. 2012.
Vocabulary expansion through automatic abbrevia-
tion generation for Chinese voice search. Computer
Speech &amp; Language, 26(5):321–335.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics, pages 207–216.
</reference>
<page confidence="0.995961">
1359
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.898764">
<title confidence="0.999423">Unsupervised Morphology-Based Vocabulary Expansion</title>
<author confidence="0.994074">Sadegh Rasooli</author>
<author confidence="0.994074">Thomas Lippincott</author>
<author confidence="0.994074">Nizar Habash</author>
<affiliation confidence="0.9541885">Center for Computational Learning Columbia University, New York, NY,</affiliation>
<abstract confidence="0.999733235294118">We present a novel way of generating unseen words, which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages. We test our vocabulary generator on seven low-resource languages by measuring the decrease in out-of-vocabulary word rate on a held-out test set. The languages we study have very different morphological properties; we show how our results differ depending on the morphological complexity of the language. In our best result (on Assamese), our approach can predict 29% of the token-based out-of-vocabulary with a small amount of unlabeled training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL ’01,</booktitle>
<pages>26--33</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2090" citStr="Banko and Brill, 2001" startWordPosition="316" endWordPosition="319">ed audio and transcription, plus large unannotated data for the language modeling; in the case of OCR, it is transcribed optical data; in the case of MT, it is aligned bitexts. More data provides for better results. For languages with rich resources, such as English, more data is often the best solution, since the required data is readily available (including bitexts), and the cost of annotating (e.g., transcribing) data is outweighed by the potential significance of the systems that the data will enable. Thus, in HLT, improvements in quality are often brought about by using larger data sets (Banko and Brill, 2001). When we move to low-resource languages, the solution of simply using more data becomes less appealing. Unannotated data is less readily available: for example, at the time of publishing this paper, 55% of all websites are in English, the top 10 languages collectively account for 90% of web presence, and the top 36 languages have a web presence that covers at least 0.1% of web sites.1 All other languages (and all languages considered in this paper except Persian) have a web presence of less than 0.1%. Considering Wikipedia, another resource often used in HLT, English has 4.4 million articles,</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL ’01, pages 26–33, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth R Beesley</author>
<author>Lauri Karttunen</author>
</authors>
<title>Finitestate morphology: Xerox tools and techniques.</title>
<date>2003</date>
<publisher>CSLI, Stanford.</publisher>
<contexts>
<context position="28029" citStr="Beesley and Karttunen, 2003" startWordPosition="4603" endWordPosition="4606">y affixes. |L |shows the upper bound of the number of possible unique words that can be generated from the word generation model. |If |is the average number of unique prefix-suffix pairs (including empty pairs) for each stem. phology and tagging models such as Frank et al. (2013). Error Analysis on Turkish Unfortunately for most languages we could not find an available rule-based or supervised morphological analyzer to verify the words generated by our model. The only available tool for us is a Turkish finite-state morphological analyzer (Oflazer, 1996) implemented with the Xerox FST toolkit (Beesley and Karttunen, 2003). As we can see in Table 5, the system with the largest proportion of correct generated words reranks the expansion with trigraph probabilities using a Fixed Affix model. Results also show that we are overgenerating many nonsense words that we ought to be pruning from our results. Another observation is that the recognition percentage of the morphological analyzer on INV words is much higher than on OOVs, which shows that OOVs in Turkish dataset are much harder to analyze. 1356 Model Precision Tr. WFST 17.19 NRR 13.36 Fixed Affix Model W°Tr 25.66 TRR 26.30 BRR 25.14 NRR 12.94 Bigram Affix Mode</context>
</contexts>
<marker>Beesley, Karttunen, 2003</marker>
<rawString>Kenneth R Beesley and Lauri Karttunen. 2003. Finitestate morphology: Xerox tools and techniques. CSLI, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Buckwalter</author>
</authors>
<date>2004</date>
<booktitle>Buckwalter Arabic Morphological Analyzer Version 2.0. LDC catalog number LDC2004L02, ISBN</booktitle>
<pages>1--58563</pages>
<contexts>
<context position="6187" citStr="Buckwalter, 2004" startWordPosition="966" endWordPosition="967">nt of unlabeled training data. The paper is structured as follows. We first discuss related work in Section 2. We then present our method in Section 3, and present experimental results in Section 4. We conclude with a discussion of future work in Section 5. 2 Related Work Approaches to Morphological Modeling Computational morphology is a very active area of research with a multitude of approaches that vary in the degree of manual annotation needed, and the amount of machine learning used. At one extreme, we find systems that are painstakingly and carefully designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012). Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and othe</context>
</contexts>
<marker>Buckwalter, 2004</marker>
<rawString>Tim Buckwalter. 2004. Buckwalter Arabic Morphological Analyzer Version 2.0. LDC catalog number LDC2004L02, ISBN 1-58563-324-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised pos induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>575--584</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26828" citStr="Christodoulopoulos et al., 2010" startWordPosition="4398" endWordPosition="4401"> The size of the languages (|L|) suggests that we are suffering from vast overgeneration; we overgenerate because in our model any affix can attach to any stem, which is not in general true. Thus there is a lack of linguistic knowledge such as paradigm information (Stump, 2001) for each word category in our model. In other words, all morphemes are treated the same in our model which is not true in natural languages. One way to tackle this problem is through an unsupervised POS tagger. The challenge here is that fully unsupervised POS taggers (without any tag dictionary) are not very accurate (Christodoulopoulos et al., 2010). Another way is through using joint mor1355 Figure 3: Trends for token-based OOV reduction with different sizes for the Fixed Affix model with trigraph reranking. Language |pr ||stm ||sf ||L ||If| Assamese 4 4791 564 10.8M 1.8 Bengali 3 6496 378 7.4M 1.5 Pashto 1 5395 271 1.5M 1.3 Persian 49 6998 538 184M 2.0 Tagalog 179 4259 299 228M 1.5 Turkish 45 5266 1801 427M 2.3 Zulu 2254 5680 427 5.5B 2.8 Persian-N 3 6121 268 4.9M 1.5 Persian-V 43 788 44 1.5M 3.4 Table 4: Information about the number of unique morphemes in the Fixed Affix model for each dataset including empty affixes. |L |shows the up</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised pos induction: How far have we come? In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 575–584. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lionel Cl´ement</author>
<author>Benoit Sagot</author>
<author>Bernard Lang</author>
</authors>
<title>Morphology based automatic acquisition of large-coverage lexica.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04). European Language Resources Association (ELRA).</booktitle>
<marker>Cl´ement, Sagot, Lang, 2004</marker>
<rawString>Lionel Cl´ement, Benoit Sagot, and Bernard Lang. 2004. Morphology based automatic acquisition of large-coverage lexica. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04). European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing (TSLP),</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="7065" citStr="Creutz and Lagus, 2007" startWordPosition="1094" endWordPosition="1097">f this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike these approaches which provide segmentations for training data and produce models that can be used to segment unseen words, our approach can generate words that have not been see</context>
<context position="18487" citStr="Creutz and Lagus, 2007" startWordPosition="2984" endWordPosition="2987">ics about the datasets are shown in Table 1. We also conduct further experiments on just verbs and nouns in the data set for Persian (Persian-N and Persian V). As shown in Table 1, the training data is very small and the OOV rate is high especially in terms of types. For some languages that have richer morphology such as Turkish and Zulu, the OOV rate is much higher than other languages. Word Generation Tools and Settings For unsupervised learning of morphology, we use Morfessor CAT-MAP (v. 0.9.2) which was shown to be a very accurate morphological analyzer for morphologically rich languages (Creutz and Lagus, 2007). In order to be able to analyze Unicodebased data, we convert each character in our dataset to some conventional ASCII character and then train Morfessor on the mapped dataset; after finishing the training, we map the data back to the original character set. We use the default setting in Morfessor for unsupervised learning. For preparing the WFST, we use OpenFST (Riley et al., 2009). We get the top one million shortest paths (i.e., least costly paths of words) and apply our reranking models on them. It is worth pointing out that our WFSTs are character-based 1353 Language Training Data Develo</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing (TSLP), 4(1):3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Bootstrapping a multilingual part-of-speech tagger in one person-day.</title>
<date>2002</date>
<booktitle>In The 6th Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<pages>1--7</pages>
<contexts>
<context position="6886" citStr="Cucerzan and Yarowsky, 2002" startWordPosition="1067" endWordPosition="1070">um, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike</context>
</contexts>
<marker>Cucerzan, Yarowsky, 2002</marker>
<rawString>Silviu Cucerzan and David Yarowsky. 2002. Bootstrapping a multilingual part-of-speech tagger in one person-day. In The 6th Conference on Natural Language Learning (CoNLL-2002), pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gr´egoire D´etrez</author>
<author>Aarne Ranta</author>
</authors>
<title>Smart paradigms and the predictability and complexity of inflectional morphology.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>645--653</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>D´etrez, Ranta, 2012</marker>
<rawString>Gr´egoire D´etrez and Aarne Ranta. 2012. Smart paradigms and the predictability and complexity of inflectional morphology. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 645–653. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Discovering morphological paradigms from plain text using a dirichlet process mixture model.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>616--627</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7111" citStr="Dreyer and Eisner, 2011" startWordPosition="1102" endWordPosition="1105">mplete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike these approaches which provide segmentations for training data and produce models that can be used to segment unseen words, our approach can generate words that have not been seen in the training data. The focus of efforts i</context>
</contexts>
<marker>Dreyer, Eisner, 2011</marker>
<rawString>Markus Dreyer and Jason Eisner. 2011. Discovering morphological paradigms from plain text using a dirichlet process mixture model. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 616–627. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>John DeNero</author>
</authors>
<title>Supervised learning of complete morphological paradigms.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1185--1195</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6641" citStr="Durrett and DeNero, 2013" startWordPosition="1034" endWordPosition="1037">eeded, and the amount of machine learning used. At one extreme, we find systems that are painstakingly and carefully designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012). Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we a</context>
</contexts>
<marker>Durrett, DeNero, 2013</marker>
<rawString>Greg Durrett and John DeNero. 2013. Supervised learning of complete morphological paradigms. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1185–1195. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramy Eskander</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Automatic extraction of morphological lexicons from morphologically annotated corpora.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1032--1043</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="6665" citStr="Eskander et al., 2013" startWordPosition="1038" endWordPosition="1041">achine learning used. At one extreme, we find systems that are painstakingly and carefully designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012). Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from t</context>
</contexts>
<marker>Eskander, Habash, Rambow, 2013</marker>
<rawString>Ramy Eskander, Nizar Habash, and Owen Rambow. 2013. Automatic extraction of morphological lexicons from morphologically annotated corpora. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Forsberg</author>
<author>Harald Hammarstr¨om</author>
<author>Aarne Ranta</author>
</authors>
<title>Morphological lexicon extraction from raw text data.</title>
<date>2006</date>
<booktitle>Advances in Natural Language Processing,</booktitle>
<pages>488--499</pages>
<marker>Forsberg, Hammarstr¨om, Ranta, 2006</marker>
<rawString>Markus Forsberg, Harald Hammarstr¨om, and Aarne Ranta. 2006. Morphological lexicon extraction from raw text data. Advances in Natural Language Processing, pages 488–499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stella Frank</author>
<author>Frank Keller</author>
<author>Sharon Goldwater</author>
</authors>
<title>Exploring the utility of joint morphological and syntactic learning from child-directed speech.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>30--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27681" citStr="Frank et al. (2013)" startWordPosition="4551" endWordPosition="4554">i 3 6496 378 7.4M 1.5 Pashto 1 5395 271 1.5M 1.3 Persian 49 6998 538 184M 2.0 Tagalog 179 4259 299 228M 1.5 Turkish 45 5266 1801 427M 2.3 Zulu 2254 5680 427 5.5B 2.8 Persian-N 3 6121 268 4.9M 1.5 Persian-V 43 788 44 1.5M 3.4 Table 4: Information about the number of unique morphemes in the Fixed Affix model for each dataset including empty affixes. |L |shows the upper bound of the number of possible unique words that can be generated from the word generation model. |If |is the average number of unique prefix-suffix pairs (including empty pairs) for each stem. phology and tagging models such as Frank et al. (2013). Error Analysis on Turkish Unfortunately for most languages we could not find an available rule-based or supervised morphological analyzer to verify the words generated by our model. The only available tool for us is a Turkish finite-state morphological analyzer (Oflazer, 1996) implemented with the Xerox FST toolkit (Beesley and Karttunen, 2003). As we can see in Table 5, the system with the largest proportion of correct generated words reranks the expansion with trigraph probabilities using a Fixed Affix model. Results also show that we are overgenerating many nonsense words that we ought to</context>
</contexts>
<marker>Frank, Keller, Goldwater, 2013</marker>
<rawString>Stella Frank, Frank Keller, and Sharon Goldwater. 2013. Exploring the utility of joint morphological and syntactic learning from child-directed speech. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 30–41. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph H Greenberg</author>
</authors>
<title>A quantitative approach to the morphological typology of language.</title>
<date>1960</date>
<journal>International journal of American linguistics,</journal>
<pages>178--194</pages>
<contexts>
<context position="22469" citStr="Greenberg, 1960" startWordPosition="3668" endWordPosition="3669">at is reranked by trigraph probabilities is shown in Figure 3. As seen in the results, for languages that have richer morphology, it is harder to achieve results near to the upper bound. As an outlier, morphology does not help for Pashto. One possible reason might be that based on the results in Table 4, Morfessor does not explore morphology in Pashto as well as other languages. Morphological Complexity As for further analysis, we can study the correlation between morphological complexity and hardness of reducing OOVs. Much work has been done in linguistics to classify languages (Sapir, 1921; Greenberg, 1960). The common wisdom is that languages are not either agglutinative or fusional, but are on a spectrum; however, no work to our knowledge places all languages (or at least the ones we worked on) on such a spectrum. We propose several metrics. First, we can consider the number of unique affixival morphemes in each language, as determined by Morfessor. As shown in Table 4 (|pr |+ |sf |), Zulu has the most morphemes and Pashto the fewest. A second possible metric of the 1354 Language Tr. Fixed Affix Model Bigram Affix Model FP WFST NRR WoTr TRR BRR oc NRR WoTr TRR BRR oc Assamese 15.94 24.03 28.46</context>
</contexts>
<marker>Greenberg, 1960</marker>
<rawString>Joseph H Greenberg. 1960. A quantitative approach to the morphological typology of language. International journal of American linguistics, pages 178– 194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>MAGEAD: A morphological analyzer and generator for the Arabic dialects.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>681--688</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="6212" citStr="Habash and Rambow, 2006" startWordPosition="968" endWordPosition="971">aining data. The paper is structured as follows. We first discuss related work in Section 2. We then present our method in Section 3, and present experimental results in Section 4. We conclude with a discussion of future work in Section 5. 2 Related Work Approaches to Morphological Modeling Computational morphology is a very active area of research with a multitude of approaches that vary in the degree of manual annotation needed, and the amount of machine learning used. At one extreme, we find systems that are painstakingly and carefully designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012). Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphologica</context>
</contexts>
<marker>Habash, Rambow, 2006</marker>
<rawString>Nizar Habash and Owen Rambow. 2006. MAGEAD: A morphological analyzer and generator for the Arabic dialects. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 681–688, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9703" citStr="Habash (2008)" startWordPosition="1515" endWordPosition="1516">e pronunciation lexicon via generating all possible pronunciations for a word before lattice generation and indexation. There are also other methods for generating abbreviations in voice search systems such as Yang et al. (2012). While all of these approaches involve lexicon expansion, they do not employ any morphological information. In the context of MT, several researchers have addressed the problem of OOV words by relating them to known in-vocabulary (INV) words. Yang and Kirchhoff (2006) anticipated OOV words that are potentially morphologically related using phrase-based backoff models. Habash (2008) considered different techniques for vocabulary expansion online. One of their techniques learned models of morphological mapping between morphologically rich source words in Arabic that produce the same English translation. This was used to relate an OOV word to a morphologically related INV word. Another technique expanded the MT phrase tables with possible transliterations and spelling alternatives. 3 Morphology-based Vocabulary Expansion 3.1 Approach Our approach to morphology-based vocabulary expansion consists of three steps (Figure 1). We start with a “training” corpus of (unannotated) </context>
</contexts>
<marker>Habash, 2008</marker>
<rawString>Nizar Habash. 2008. Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 57–60. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Harper</author>
</authors>
<title>The babel program and low resource speech technology.</title>
<date>2013</date>
<booktitle>In Automatic Speech Recognition and Understanding Workshop (ASRU) Invited talk.</booktitle>
<contexts>
<context position="16948" citStr="Harper, 2013" startWordPosition="2738" endWordPosition="2739">rst suffix). The intuition is that we already know that any morpheme that is generated from the morphology FST is already seen in the training data but the boundary for different morphemes are not guaranteed to be seen in the training data. For example, for the word producttions, we only take into account the trigraphs rod, odu, ctt and tti instead of all possible trigraphs. We will refer to this technique as BRR. 4 Evaluation 4.1 Evaluation Data and Tools Evaluation Data The IARPA Babel program is a research program for developing rapid spoken detection systems for under-resourced languages (Harper, 2013). We use the IARPA Babel program limited language pack data which consists of 20 hours of telephone speech with transcription. We use six languages which are known to have rich morphology: Assamese (IARPAbabel102b-v0.5a), Bengali (IARPA-babel103bv0.4b), Pashto (IARPA-babel104b-v0.4bY), Tagalog (IARPA-babel106-v0.2g), Turkish (IARPAbabel105b-v0.4) and Zulu (IARPA-babel206bv0.1e). Speech annotation such as silences and hesitations are removed from transcription and all words are turned into lower-case (for languages using the Roman script – Tagalog, Turkish and Zulu). Moreover, in order to be ab</context>
</contexts>
<marker>Harper, 2013</marker>
<rawString>Mary Harper. 2013. The babel program and low resource speech technology. In Automatic Speech Recognition and Understanding Workshop (ASRU) Invited talk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-Level Model for Morphological Analysis.</title>
<date>1983</date>
<booktitle>In Proceedings of the 8th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>683--685</pages>
<contexts>
<context position="6169" citStr="Koskenniemi, 1983" startWordPosition="964" endWordPosition="965">y with a small amount of unlabeled training data. The paper is structured as follows. We first discuss related work in Section 2. We then present our method in Section 3, and present experimental results in Section 4. We conclude with a discussion of future work in Section 5. 2 Related Work Approaches to Morphological Modeling Computational morphology is a very active area of research with a multitude of approaches that vary in the degree of manual annotation needed, and the amount of machine learning used. At one extreme, we find systems that are painstakingly and carefully designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012). Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries,</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. Two-Level Model for Morphological Analysis. In Proceedings of the 8th International Joint Conference on Artificial Intelligence, pages 683–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Lei</author>
<author>Wen Wang</author>
<author>Andreas Stolcke</author>
</authors>
<title>Data-driven lexicon expansion for Mandarin broadcast news and conversation speech recognition.</title>
<date>2009</date>
<booktitle>In International conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>4329--4332</pages>
<contexts>
<context position="9078" citStr="Lei et al. (2009)" startWordPosition="1420" endWordPosition="1423"> ourselves to a specific part-ofspeech (POS). Vocabulary Expansion in HLT There have been diverse approaches towards dealing with outof-vocabulary (OOV) words in ASR. In some models, the approach is to expand the lexicon by 1350 adding new words or pronunciations. Ohtsuki et al. (2005) propose a two-run model where in the first run, the input speech is recognized by the reference vocabulary and relevant words are extracted from the vocabulary database and added thereafter to the reference vocabulary to build an expanded lexicon. Word recognition is done in the second run based on the lexicon. Lei et al. (2009) expanded the pronunciation lexicon via generating all possible pronunciations for a word before lattice generation and indexation. There are also other methods for generating abbreviations in voice search systems such as Yang et al. (2012). While all of these approaches involve lexicon expansion, they do not employ any morphological information. In the context of MT, several researchers have addressed the problem of OOV words by relating them to known in-vocabulary (INV) words. Yang and Kirchhoff (2006) anticipated OOV words that are potentially morphologically related using phrase-based back</context>
</contexts>
<marker>Lei, Wang, Stolcke, 2009</marker>
<rawString>Xin Lei, Wen Wang, and Andreas Stolcke. 2009. Data-driven lexicon expansion for Mandarin broadcast news and conversation speech recognition. In International conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4329–4332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
<author>Jaime Carbonell</author>
<author>Alon Lavie</author>
<author>Lori Levin</author>
</authors>
<title>Paramor: Finding paradigms across morphology. Advances in Multilingual and Multimodal Information Retrieval,</title>
<date>2008</date>
<pages>900--907</pages>
<contexts>
<context position="7086" citStr="Monson et al., 2008" startWordPosition="1098" endWordPosition="1101">d efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike these approaches which provide segmentations for training data and produce models that can be used to segment unseen words, our approach can generate words that have not been seen in the training dat</context>
</contexts>
<marker>Monson, Carbonell, Lavie, Levin, 2008</marker>
<rawString>Christian Monson, Jaime Carbonell, Alon Lavie, and Lori Levin. 2008. Paramor: Finding paradigms across morphology. Advances in Multilingual and Multimodal Information Retrieval, pages 900–907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Neuvel</author>
<author>Sean A Fulop</author>
</authors>
<title>Unsupervised learning of morphology without morphemes.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Morphological and phonological learning-Volume 6,</booktitle>
<pages>31--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6910" citStr="Neuvel and Fulop, 2002" startWordPosition="1071" endWordPosition="1074"> on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike these approaches which </context>
</contexts>
<marker>Neuvel, Fulop, 2002</marker>
<rawString>Sylvain Neuvel and Sean A Fulop. 2002. Unsupervised learning of morphology without morphemes. In Proceedings of the ACL-02 workshop on Morphological and phonological learning-Volume 6, pages 31–40. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="27960" citStr="Oflazer, 1996" startWordPosition="4594" endWordPosition="4595">n the Fixed Affix model for each dataset including empty affixes. |L |shows the upper bound of the number of possible unique words that can be generated from the word generation model. |If |is the average number of unique prefix-suffix pairs (including empty pairs) for each stem. phology and tagging models such as Frank et al. (2013). Error Analysis on Turkish Unfortunately for most languages we could not find an available rule-based or supervised morphological analyzer to verify the words generated by our model. The only available tool for us is a Turkish finite-state morphological analyzer (Oflazer, 1996) implemented with the Xerox FST toolkit (Beesley and Karttunen, 2003). As we can see in Table 5, the system with the largest proportion of correct generated words reranks the expansion with trigraph probabilities using a Fixed Affix model. Results also show that we are overgenerating many nonsense words that we ought to be pruning from our results. Another observation is that the recognition percentage of the morphological analyzer on INV words is much higher than on OOVs, which shows that OOVs in Turkish dataset are much harder to analyze. 1356 Model Precision Tr. WFST 17.19 NRR 13.36 Fixed A</context>
</contexts>
<marker>Oflazer, 1996</marker>
<rawString>Kemal Oflazer. 1996. Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction. Computational Linguistics, 22(1):73–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsutoshi Ohtsuki</author>
<author>Nobuaki Hiroshima</author>
<author>Masahiro Oku</author>
<author>Akihiro Imamura</author>
</authors>
<title>Unsupervised vocabulary expansion for automatic transcription of broadcast news.</title>
<date>2005</date>
<booktitle>In International conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>1021--1024</pages>
<contexts>
<context position="8747" citStr="Ohtsuki et al. (2005)" startWordPosition="1364" endWordPosition="1367">phology: Dreyer and Eisner (2011) limited their work to verbal paradigms and used annotated training data in addition to basic assumptions about the problem such as the size of the paradigms. In our approach, we have zero annotated information and we do not distinguish between inflectional and derivational morphology, nor do we limit ourselves to a specific part-ofspeech (POS). Vocabulary Expansion in HLT There have been diverse approaches towards dealing with outof-vocabulary (OOV) words in ASR. In some models, the approach is to expand the lexicon by 1350 adding new words or pronunciations. Ohtsuki et al. (2005) propose a two-run model where in the first run, the input speech is recognized by the reference vocabulary and relevant words are extracted from the vocabulary database and added thereafter to the reference vocabulary to build an expanded lexicon. Word recognition is done in the second run based on the lexicon. Lei et al. (2009) expanded the pronunciation lexicon via generating all possible pronunciations for a word before lattice generation and indexation. There are also other methods for generating abbreviations in voice search systems such as Yang et al. (2012). While all of these approach</context>
</contexts>
<marker>Ohtsuki, Hiroshima, Oku, Imamura, 2005</marker>
<rawString>Katsutoshi Ohtsuki, Nobuaki Hiroshima, Masahiro Oku, and Akihiro Imamura. 2005. Unsupervised vocabulary expansion for automatic transcription of broadcast news. In International conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1021–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Sadegh Rasooli</author>
<author>Manouchehr Kouhestani</author>
<author>Amirsaeid Moloodi</author>
</authors>
<title>Development of a Persian syntactic dependency treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>306--314</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17805" citStr="Rasooli et al., 2013" startWordPosition="2866" endWordPosition="2869">-babel103bv0.4b), Pashto (IARPA-babel104b-v0.4bY), Tagalog (IARPA-babel106-v0.2g), Turkish (IARPAbabel105b-v0.4) and Zulu (IARPA-babel206bv0.1e). Speech annotation such as silences and hesitations are removed from transcription and all words are turned into lower-case (for languages using the Roman script – Tagalog, Turkish and Zulu). Moreover, in order to be able to perform a manual error analysis, we include a language that has rich morphology and of which the first author is a native speaker: Persian. We sampled data from the training and development set of the Persian dependency treebank (Rasooli et al., 2013) to create a comparable seventh dataset in Persian. Statistics about the datasets are shown in Table 1. We also conduct further experiments on just verbs and nouns in the data set for Persian (Persian-N and Persian V). As shown in Table 1, the training data is very small and the OOV rate is high especially in terms of types. For some languages that have richer morphology such as Turkish and Zulu, the OOV rate is much higher than other languages. Word Generation Tools and Settings For unsupervised learning of morphology, we use Morfessor CAT-MAP (v. 0.9.2) which was shown to be a very accurate </context>
</contexts>
<marker>Rasooli, Kouhestani, Moloodi, 2013</marker>
<rawString>Mohammad Sadegh Rasooli, Manouchehr Kouhestani, and Amirsaeid Moloodi. 2013. Development of a Persian syntactic dependency treebank. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 306–314. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Riley</author>
<author>Cyril Allauzen</author>
<author>Martin Jansche</author>
</authors>
<title>Openfst: An open-source, weighted finitestate transducer library and its applications to speech and language.</title>
<date>2009</date>
<booktitle>In Human Language Technologies Tutorials: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>9--10</pages>
<contexts>
<context position="18873" citStr="Riley et al., 2009" startWordPosition="3049" endWordPosition="3053"> Word Generation Tools and Settings For unsupervised learning of morphology, we use Morfessor CAT-MAP (v. 0.9.2) which was shown to be a very accurate morphological analyzer for morphologically rich languages (Creutz and Lagus, 2007). In order to be able to analyze Unicodebased data, we convert each character in our dataset to some conventional ASCII character and then train Morfessor on the mapped dataset; after finishing the training, we map the data back to the original character set. We use the default setting in Morfessor for unsupervised learning. For preparing the WFST, we use OpenFST (Riley et al., 2009). We get the top one million shortest paths (i.e., least costly paths of words) and apply our reranking models on them. It is worth pointing out that our WFSTs are character-based 1353 Language Training Data Development Data Type Token Type Token Type OOV% Token OOV% Assamese 8694 73151 7253 66184 49.57 8.28 Bengali 9460 81476 7794 70633 50.65 8.47 Pashto 6968 115069 6135 108137 44.89 4.25 Persian 14047 71527 10479 42939 44.16 12.78 Tagalog 6213 69577 5480 64334 54.95 7.81 Turkish 11985 77128 9852 67042 56.84 12.34 Zulu 15868 65655 13756 57141 68.72 21.76 Persian-N 9204 31369 7502 18816 46.36 </context>
</contexts>
<marker>Riley, Allauzen, Jansche, 2009</marker>
<rawString>Michael Riley, Cyril Allauzen, and Martin Jansche. 2009. Openfst: An open-source, weighted finitestate transducer library and its applications to speech and language. In Human Language Technologies Tutorials: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 9–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Sapir</author>
</authors>
<title>Language: An introduction to the study of speech. Harcourt, Brace and company</title>
<date>1921</date>
<location>(New York).</location>
<contexts>
<context position="22451" citStr="Sapir, 1921" startWordPosition="3666" endWordPosition="3667">ffix model that is reranked by trigraph probabilities is shown in Figure 3. As seen in the results, for languages that have richer morphology, it is harder to achieve results near to the upper bound. As an outlier, morphology does not help for Pashto. One possible reason might be that based on the results in Table 4, Morfessor does not explore morphology in Pashto as well as other languages. Morphological Complexity As for further analysis, we can study the correlation between morphological complexity and hardness of reducing OOVs. Much work has been done in linguistics to classify languages (Sapir, 1921; Greenberg, 1960). The common wisdom is that languages are not either agglutinative or fusional, but are on a spectrum; however, no work to our knowledge places all languages (or at least the ones we worked on) on such a spectrum. We propose several metrics. First, we can consider the number of unique affixival morphemes in each language, as determined by Morfessor. As shown in Table 4 (|pr |+ |sf |), Zulu has the most morphemes and Pashto the fewest. A second possible metric of the 1354 Language Tr. Fixed Affix Model Bigram Affix Model FP WFST NRR WoTr TRR BRR oc NRR WoTr TRR BRR oc Assamese</context>
</contexts>
<marker>Sapir, 1921</marker>
<rawString>Edward Sapir. 1921. Language: An introduction to the study of speech. Harcourt, Brace and company (New York).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kairit Sirts</author>
<author>Sharon Goldwater</author>
</authors>
<title>Minimallysupervised morphological segmentation using adaptor grammars. Transactions for the ACL,</title>
<date>2013</date>
<pages>1--255</pages>
<contexts>
<context position="7139" citStr="Sirts and Goldwater, 2013" startWordPosition="1106" endWordPosition="1109">lly supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike these approaches which provide segmentations for training data and produce models that can be used to segment unseen words, our approach can generate words that have not been seen in the training data. The focus of efforts is rather complementary: we a</context>
</contexts>
<marker>Sirts, Goldwater, 2013</marker>
<rawString>Kairit Sirts and Sharon Goldwater. 2013. Minimallysupervised morphological segmentation using adaptor grammars. Transactions for the ACL, 1:255–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th annual meeting of the association for computational linguistics: Human language Technologies (ACLHLT),</booktitle>
<pages>737--745</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6938" citStr="Snyder and Barzilay, 2008" startWordPosition="1075" endWordPosition="1078">al models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike these approaches which provide segmentations for tr</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In Proceedings of the 46th annual meeting of the association for computational linguistics: Human language Technologies (ACLHLT), pages 737–745. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory T Stump</author>
</authors>
<title>A theory of paradigm structure.</title>
<date>2001</date>
<location>Cambridge.</location>
<contexts>
<context position="26474" citStr="Stump, 2001" startWordPosition="4340" endWordPosition="4341">f |and |L |metrics in Table 4 than other languages. The results from full language packs in Table 3 also show that there is a reverse interaction of morphological complexity and the effect of blindly adding more data. Thus for morphologically rich languages, adding more data is less effective than for languages with poor morphology. The size of the languages (|L|) suggests that we are suffering from vast overgeneration; we overgenerate because in our model any affix can attach to any stem, which is not in general true. Thus there is a lack of linguistic knowledge such as paradigm information (Stump, 2001) for each word category in our model. In other words, all morphemes are treated the same in our model which is not true in natural languages. One way to tackle this problem is through an unsupervised POS tagger. The challenge here is that fully unsupervised POS taggers (without any tag dictionary) are not very accurate (Christodoulopoulos et al., 2010). Another way is through using joint mor1355 Figure 3: Trends for token-based OOV reduction with different sizes for the Fixed Affix model with trigraph reranking. Language |pr ||stm ||sf ||L ||If| Assamese 4 4791 564 10.8M 1.8 Bengali 3 6496 378</context>
</contexts>
<marker>Stump, 2001</marker>
<rawString>Gregory T. Stump. 2001. A theory of paradigm structure. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mei Yang</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Phrase-based backoff models for machine translation of highly inflected languages.</title>
<date>2006</date>
<booktitle>In Proceedings of Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>41--48</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="9587" citStr="Yang and Kirchhoff (2006)" startWordPosition="1499" endWordPosition="1502">ary to build an expanded lexicon. Word recognition is done in the second run based on the lexicon. Lei et al. (2009) expanded the pronunciation lexicon via generating all possible pronunciations for a word before lattice generation and indexation. There are also other methods for generating abbreviations in voice search systems such as Yang et al. (2012). While all of these approaches involve lexicon expansion, they do not employ any morphological information. In the context of MT, several researchers have addressed the problem of OOV words by relating them to known in-vocabulary (INV) words. Yang and Kirchhoff (2006) anticipated OOV words that are potentially morphologically related using phrase-based backoff models. Habash (2008) considered different techniques for vocabulary expansion online. One of their techniques learned models of morphological mapping between morphologically rich source words in Arabic that produce the same English translation. This was used to relate an OOV word to a morphologically related INV word. Another technique expanded the MT phrase tables with possible transliterations and spelling alternatives. 3 Morphology-based Vocabulary Expansion 3.1 Approach Our approach to morpholog</context>
</contexts>
<marker>Yang, Kirchhoff, 2006</marker>
<rawString>Mei Yang and Katrin Kirchhoff. 2006. Phrase-based backoff models for machine translation of highly inflected languages. In Proceedings of Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 41–48, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Yang</author>
<author>Yi-Cheng Pan</author>
<author>Sadaoki Furui</author>
</authors>
<title>Vocabulary expansion through automatic abbreviation generation for Chinese voice search.</title>
<date>2012</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>26</volume>
<issue>5</issue>
<contexts>
<context position="9318" citStr="Yang et al. (2012)" startWordPosition="1457" endWordPosition="1460"> words or pronunciations. Ohtsuki et al. (2005) propose a two-run model where in the first run, the input speech is recognized by the reference vocabulary and relevant words are extracted from the vocabulary database and added thereafter to the reference vocabulary to build an expanded lexicon. Word recognition is done in the second run based on the lexicon. Lei et al. (2009) expanded the pronunciation lexicon via generating all possible pronunciations for a word before lattice generation and indexation. There are also other methods for generating abbreviations in voice search systems such as Yang et al. (2012). While all of these approaches involve lexicon expansion, they do not employ any morphological information. In the context of MT, several researchers have addressed the problem of OOV words by relating them to known in-vocabulary (INV) words. Yang and Kirchhoff (2006) anticipated OOV words that are potentially morphologically related using phrase-based backoff models. Habash (2008) considered different techniques for vocabulary expansion online. One of their techniques learned models of morphological mapping between morphologically rich source words in Arabic that produce the same English tra</context>
</contexts>
<marker>Yang, Pan, Furui, 2012</marker>
<rawString>Dong Yang, Yi-Cheng Pan, and Sadaoki Furui. 2012. Vocabulary expansion through automatic abbreviation generation for Chinese voice search. Computer Speech &amp; Language, 26(5):321–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Richard Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="6857" citStr="Yarowsky and Wicentowski, 2000" startWordPosition="1063" endWordPosition="1066">anta, 2012). Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007;</context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>David Yarowsky and Richard Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 207–216.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>