<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017938">
<title confidence="0.99921">
A Connectionist Treatment of Grammar
for Generation: Relying on Emergents
</title>
<author confidence="0.996232">
Nigel Ward
</author>
<affiliation confidence="0.9982765">
Computer Science Division
University of California at Berkeley
</affiliation>
<sectionHeader confidence="0.980419" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994219352941177">
Parallel treatment of syntactic considerations in generation
promises quality and speed. Parallelism should be used not only
for simultaneous processing of several sub-parts of the output, but
even within single parts. If both types of parallelism are used with
incremental generation it becomes unnecessary to build up and ma-
nipulate representations of sentence structure — the syntactic form
of the output can be emergent.
FIG is a structured cormectionist generator built in this way.
Constructions and their constituents are represented in the same
network which encodes world knowledge and lexical knowledge.
Grammatical output results from synergy among many construc-
tions simultaneously active at run-time. FIG incorporates new
ways of handling constituency, word order and optional con-
stituents; and simple ways to avoid the problems of instantiation
and binding. Syntactic knowledge is expressed in a simple, read-
able form; this representation straightforwardly defines parts of the
network.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999601">
Generation research has not yet fully identified the advan-
tages offered by parallelism nor the techniques necessary to
take advantage of it. This is especially true for the syntactic
aspects of generation.
This paper presents a way to exploit parallelism for syn-
tax in generation. The key points are: Syntactic construc-
tions are encoded in the same knowledge network as words
and concepts. Many constructions are active in parallel;
there is synergy, and sometimes competition. The syntactic
form of the output emerges from interactions among con-
structions at run-time — explicit syntactic choice and build-
ing up of representations of syntactic structure are unneces-
sary.
To see that this approach works for syntactically non-
trivial examples, consider that FIG&apos; s outputs include: &amp;quot;once
</bodyText>
<footnote confidence="0.858823142857143">
&apos;Thanks to Daniel Jurafsky, Robert Wilensky, Dekai Wu, and Terry
Regier. This research was sponsored by the Defense Advanced Research
Projects Agency (DoD), monitored by the Space and Naval Warfare Sys-
tems Command under N00039-88-C-0292, and the Office of Naval Re-
search under contract N00014-89-J-3205. An early version of this paper
appears in the Proceedings of the 12th Cognitive Science Conference, Erl-
baum, 1990.
</footnote>
<bodyText confidence="0.998896318181818">
upon a time there lived an old man and an old woman,&amp;quot;
&amp;quot;one day the old man went into the hills to gather wood,&amp;quot;
&amp;quot;a big peach bobbed down towards an old woman from up-
stream,&amp;quot; &amp;quot;an old woman gave a peach to an old man,&amp;quot;
&amp;quot;John broke a dish,&amp;quot; &amp;quot;John made the cake vanish,&amp;quot;
and &amp;quot;Mary was killed;&amp;quot; and when producing Japanese:
&amp;quot;mukashi mukashi aru tokoro ni ojiisan to obaasan ga
sunde imashita,&amp;quot; &amp;quot;aru hi ojiisan wa yama e shibakari ni
ikimashita,&amp;quot; &amp;quot;kawakami kara ookii momo ga donburiko
donburako to obaasan e nagarete kimashita,&amp;quot; &amp;quot;ojiisan wa
meeri ni momo o agemashita,&amp;quot; and &amp;quot;meeri o koroshi-
mashita.&amp;quot;
Section 2 discusses parallelism in syntax and presents the
basic proposal. Section 3 presents a framework for connec-
tionist generation, and Section 4 elaborates the proposal in
this framework. Sections 5 through 8 discuss an implemen-
tation of these ideas: Section 5 presents a representation for
grammatical knowledge, Section 6 explains how the pro-
posal accounts for specific syntactic phenomena, Section 7
presents an example of the generator in action, and Section
8 discusses general implementation issues. Section 9 sum-
marizes.
</bodyText>
<sectionHeader confidence="0.975945" genericHeader="introduction">
2 Parallel Syntax
</sectionHeader>
<bodyText confidence="0.999160588235294">
This section discusses two types of parallelism for syn-
tax, proposes that a generator should have both of them, and
sketches out the advantages of such an approach.
Natural language generation research traditionally as-
sumed that syntactic choices are made in a fixed (and gen-
erally top-down) order. Yet, for incremental generation at
least, it is clear that a fixed order of decisions is not appro-
priate. This realization has led to generators which work on
several parts of the input in parallel, simultaneously build-
ing several sub-trees. Recent work in this area includes
(De Smedt 1990) and (Finlder &amp; Neumann 1989). I will
refer to this type of parallelism as &apos;part-wise&apos; parallelism.
A second kind of parallelism involves using several con-
structions to generate even one part of the output. As far
as I know, this &apos;within-part&apos; parallelism has not been pro-
posed in the generation literature. It has proven useful in lin-
guistics. In Fillmore&apos;s Construction Grammar the syntactic
</bodyText>
<page confidence="0.995932">
15
</page>
<bodyText confidence="0.999980303030303">
structure of sentences is accounted for in terms of &apos;superim-
position&apos; of constructions (Fillmore 1989b). It has also been
used in psycholinguistics, where analysis of speech errors
suggests that even normal speech is the result of competing
`plans&apos; (Baars 1980). More specifically, (S temberger 1985)
suggested that human speakers can be modeled as having
many `phrase structure units&apos; being `partially activated&apos; si-
multaneously. That is, many syntactic alternatives for ex-
pressing some piece of meaning are considered in parallel.
I propose that a generator should exploit both part-wise
and within-part parallelism.
Parallel generation is a good idea for several reasons. 1.
It has been observed that part-wise parallelism is a good
way to improve the speed of response, especially for incre-
mental generation. 2. Part-wise parallelism is also useful
for handling dependencies. It is not always the case that
one part can be processed without consideration of the way
the surrounding utterance will turn out. If the various parts
are generated in parallel then knowledge about the proba-
ble output for one part is available for consideration when
building another part. This can lead to better quality. 3.
Given the possibility of constraints among the various syn-
tactic choices involved in building an utterance, there is the
possibility that a `first choice&apos; will not work out when the
larger context is considered. This suggests within-part par-
allelism, so that a generator has available alternative ways
to realize some information. Given this it can find a set of
choices satisfies all the dependencies, resulting in consis-
tent and natural utterance. 4. If a generator is indeed to
consider all the possible dependencies among choices, then
parallelism becomes necessary to cope with the amount of
computation necessary. 5. Parallelism is the natural way to
generate if the input is very complex (Ward 1989a).
</bodyText>
<sectionHeader confidence="0.987754" genericHeader="method">
3 The FIG Approach to Generation
</sectionHeader>
<bodyText confidence="0.95265925">
Reduced to bare essentials, a generator&apos;s task is to get
from concepts (what the speaker wants to express) to words
(what he can say). On this view, the key problem in genera-
tion is computing the relevance (pertinence) of a particular
word, given the concepts to express. Syntactic and other
knowledge mediates this computation of relevance.
Accordingly FIG is based on word choice — every other
consideration is analyzed in terms of how it affects word
choice.
FIG is based on a large semantic network. Words are
nodes in the network, the activation they receive represents
evidence for their relevance. The basic FIG algorithm is:
</bodyText>
<listItem confidence="0.996175625">
1. each node of the input is a source of activation
2. activation flows through the network
3. when the network settles, the most highly activated
word is selected and emitted
4. activation levels are updated to represent the new cur-
rent state
5. steps 2 through 4 repeat until all of the input has been
conveyed
</listItem>
<bodyText confidence="0.9995908">
Thus FIG is an incremental generator. Its network must
be designed so that, when it settles, the node which is most
highly activated corresponds to the best next word. This
paper discusses only the network structures which encode
syntactic knowledge.
Elsewhere I argue that FIG points the way to accurate
and flexible word choice (Ward 1988), producing natural-
sounding output for machine translation (Ward 1989c), and
modeling the key aspects of the human language production
process (Ward 1989a).
</bodyText>
<sectionHeader confidence="0.969605" genericHeader="method">
4 Connectionist Syntax: Overview
</sectionHeader>
<bodyText confidence="0.999883212121212">
In FIG constructions and constituents also are represented
as nodes in the knowledge network. Their activation levels
represent their current relevance. They interact with other
nodes by means of activation flow. Any number of construc-
tions can be simultaneously active. This handles part-wise
parallelism, competition, and superimposition.
Syntactic considerations manifest themselves only
through their effects on the activation levels of words (di-
rectly or indirectly). An utterance is simply the result of
successive word choices. FIG does produce grammatical
sentences, most of the time, but their &apos;syntactic structure&apos; is
emergent, a side-effect of expressing the meaning. Thus we
can say that the syntactic form of utterances is emergent in
F1G2. This point will be illustrated repeatedly in Section 6.
Mechanisms developed by linguists (and often adopted
by generation researchers), such as unification, are not di-
rected to the task of generation (or parsing) so much as to
the goal of explaining sentence structure. Accounting for
the structure of sentences may be a worthwhile goal for lin-
guistics, but building syntactic structures is not necessary
for language generation, as subsequent sections will show.
The most common metaphor for generation is that of
making choices among alternatives. For example, a gen-
erator may choose among words for a concept, among ways
to syntactically realize a constituent, and among concepts
to bind to a slot. Given this metaphor, organizing choices
becomes the key problem in generator design. Attempts
to build parallel generators while retaining the notion of
explicit choice run up against problems of sequencing the
choices or of doing bookkeeping so that the order of choices
can vary. This appears to be difficult, judging by the gen-
eral paucity of published outputs in descriptions of parallel
generators. On the other hand, relying on emergents means
</bodyText>
<footnote confidence="0.996412571428571">
2Post hoc examination of FIG output might make one think, for exam-
ple, &apos;this exhibits the choice of the existential-there construction.&apos; In FIG
there is indeed an inhibit link between the nodes ex-there and subkpred,
and so when generating the network tends to reach a state where only one of
these is highly activated. The most highly activated construction can have
a strong effect on word choices, which is why the appearance of syntactic
choice arises.
</footnote>
<page confidence="0.980645">
16
</page>
<figure confidence="0.494748">
(defp noun-phr
(constituents (np-1 obl article ((article 1.2)))
(np-2 opt adjective((adjective .28)))
(np-3 obl noun ((cnoun .47))) ))
</figure>
<figureCaption confidence="0.99675">
Figure 1: Representation of the English Noun-Phrase Construction
</figureCaption>
<bodyText confidence="0.4375486">
(defp go-p obl go-w ((go-w .2)))
(constituents (qp-1 opt epart ((vparticle .6) (directionr .2)))
(gp-2 opt noun ((prep-phr .6) (destinationr .2)))
(gP-3 opt verb ((purpose-clause .7) (purposer .2))) ))
(gp-4
</bodyText>
<figureCaption confidence="0.997512">
Figure 2: Representation of the Valence of &amp;quot;Go&amp;quot;
</figureCaption>
<equation confidence="0.850974">
(defp ex-there
(inhibit subj-pred passive)
(constituents (et-1 obl therew
(et-2 obl verb
(et-3 obl noun
((therew .5)))
((verb .5)))
((noun .3))) ))
</equation>
<figureCaption confidence="0.999283">
Figure 3: Representation of the Existential &amp;quot;There&amp;quot; Construction
</figureCaption>
<bodyText confidence="0.999949166666667">
there are no explicit choices to worry about, and thus there
are no problems of ordering or bookkeeping at all(Ward
1989b).
In FIG all types of knowledge represented are uniformly
in the network, and interact freely at run time. FIG not only
allows this kind of interaction among various considerations
when generating, it relies on it. It relies on synergy among
constructions in the same way that Construction Grammar
does. It relies on synergy between semantic and syntactic
considerations, as seen below in Section 6.7. It also enables
interaction among lexical choices and syntactic considera-
tions.
</bodyText>
<sectionHeader confidence="0.996051" genericHeader="method">
5 KnowledgeofSyntax
</sectionHeader>
<bodyText confidence="0.999895333333333">
This section presents FIG&apos;s representation of knowledge,
first presenting it in a declarative form then showing how
that representation maps into network structures.
Starting with this section I will be largely describing FIG-
as-implemented, as of May 1990. This is for the sake of
concreteness. The theory, however, is intended to apply
to parallel generators in general. Moreover, the syntactic
knowledge presented in this section is purely illustrative. I
do not claim that these represent the facts of English, nor
the best way to describe them in a grammar. In particular,
many generalizations are not captured. The examples are
intended simply to illustrate the representational tools and
computational mechanisms available in FIG. Many details
are left unexplained for lack of space.
Figure 1 shows FIG&apos;s definition of noun-phr, represent-
ing the English noun-phrase construction. This construction
has three constituents: np-1, np-2, and np-3. np-1 and np-
3 are obligatory, np-2 is optional. Glossing over the details
for the moment, the list at the end of each constituent&apos;s defi-
nition specifies how to realize the constituent. For example,
np-1, np-2, and np-3, should be realized as an article, ad-
jective, and noun, respectively.
Figure 2 shows the construction for the case frame of the
word &amp;quot;go.&amp;quot; First comes go-w, for the word &amp;quot;go,&amp;quot; which is
obligatory. Next come (optionally): a verb-particle repre-
senting direction (as in &amp;quot;go away&amp;quot; or &amp;quot;go back home&amp;quot; or
&amp;quot;go down to the lake&amp;quot;), a prepositional phrase to express
the destination, and a purpose clause.
Figure 3 shows the representation of the existential
&amp;quot;there&amp;quot; construction, as in &amp;quot;there was a poor cobbler&amp;quot; The
&apos;inhibit&apos; field indicates that this construction is incompati-
ble with the passive construction and also with subj-pred,
the construction responsible for the basic SVO ordering of
English.
Figure 4 shows knowledge about when and where con-
structions are relevant. Briefly, constructions are associated
with words, with concepts, and with other constructions.
Constructions are associated with the meanings they can
express. For example, ex-there is listed under the concept
introductory, representing that this construction is appro-
priate for introducing some character into the story, and
purpose-clause is listed as a way to express the purposer
relation.
Constructions are associated with words. For example
go-p is the `valence&apos; (case frame) of go-w and noun-phr is
the &apos;maximal&apos; of cnoun.
Constructions are also associated with other construc-
tions. For example, the fourth constituent of go-p subcat-
egorizes for purpose-clause (Figure 2); and there are nega-
tive associations among incompatible constructions, for ex-
ample the &apos;inhibit&apos; link between ex-there and subj-pred
(Figure 3).
Figure 5 shows a fragment of FIG&apos;s network, where the
numbers on the links are their weights. This is partially
</bodyText>
<page confidence="0.985588">
17
</page>
<figure confidence="0.993434">
(defw peachw
(smallcat cnoun)(expresses momoc) (grapheme &amp;quot;peach&amp;quot;) (english (consnt-initial .5)) )
(defs cnoun (bigcat noun .4) (maximals (noun-phr .4))) ; common-noun
(defw go-w (cat verb) (expresses ikuc) (valence (go-p .2))
(grapheme (inf &amp;quot;go&amp;quot;) (past &amp;quot;went&amp;quot;) (pastp &amp;quot;gone&amp;quot;) (presp &amp;quot;going&amp;quot;)) )
(defc introductoryc (properties persistent) (english (ex-there .2) ))
(defr purposer (english (to2w .4) (purpose-clause .1)) (Japanese (ni-w .6)))
</figure>
<figureCaption confidence="0.995894">
Figure 4: Some Knowledge Related to Constructions
</figureCaption>
<figure confidence="0.999497272727273">
In-contextc
noun-phr
1
1.1
1.0•21e,
np-1 np-2 .28 np-3 x1:667,
article
r. 5 \ 5 adjective noun
4 )1
the-w a-w peachw consnt-initial
.5
</figure>
<figureCaption confidence="0.999956">
Figure 5: A Fragment of the Network
</figureCaption>
<bodyText confidence="0.999927142857143">
specified by the knowledge shown in the previous figures.
The mapping from s-expressions to network structures is not
quite trivial. For example, the link from noun to peachw
comes from the statements that peachw has `subcat&apos; cnoun
and that cnoun has `bigcat&apos; noun. Similarly, the link from
peachw to noun-phr is inherited by peachw from the &apos;max-
imals&apos; information on cnoun.
</bodyText>
<sectionHeader confidence="0.942996" genericHeader="method">
6 Various Syntactic Phenomena
</sectionHeader>
<subsectionHeader confidence="0.73653">
6.1 Constituency
</subsectionHeader>
<bodyText confidence="0.99985594117647">
The links described above suffice to handle constituency.
Consider for example the fact that common nouns must be
preceded by articles in FIG&apos;s subset of English. Suppose
that peachw is activated, perhaps because a peachc concept
is in the input. Activation flows from peachw via noun-
phr, np-1, and article to a-w and the-w.
In this way the relevance of a noun increases the rele-
vance rating of articles. Provided that other activation levels
are appropriate, this will cause some article to become the
most highly activated word, and thus be selected and emit-
ted. Note that FIG does not first choose to say a noun, then
decide to say an article; rather the these &apos;decisions&apos; emerge
as activation levels settle.
Any node can be mentioned by a constituent, thus con-
structions can specify: which semantic elements to include
(metonymies), what order to mention things in, what func-
tion words to choose, and what inflections to use.
</bodyText>
<subsectionHeader confidence="0.998182">
6.2 Subcategorization
</subsectionHeader>
<bodyText confidence="0.999996461538462">
Consider the problem of specifying where a given con-
cept should appear and what syntactic form it should take.
In FIG this is handled by simultaneously activating a con-
cept node and a syntactic construction or category node. For
example, the third constituent of go-p specifies that `the di-
rection of the going&apos; be expressed as a &apos;verbal particle.&apos; Ac-
tivation will thus flow to an appropriate word node, such as
downw, both via the concept filling the directionr slot and
via the syntactic category vparticle. Thanks to this sort of
activation flow FIG tends to select and emit an appropriate
word in an appropriate form (Ward 1988). Government, for
example, the way that some verbs govern case markers, is
handled in the same way.
</bodyText>
<subsectionHeader confidence="0.999563">
6.3 Word Order
</subsectionHeader>
<bodyText confidence="0.999950285714286">
In an incremental connectionist generator, at each time
the activation level of a word must represent its current rele-
vance. In particular, words which are currently syntactically
appropriate must be strongly activated. In FIG the represen-
tation of the current syntactic state is distributed across the
constructions. There is no central process which plans or
manipulates word order; each construction simply operates
</bodyText>
<page confidence="0.995999">
18
</page>
<bodyText confidence="0.9999661875">
independently. More highly activated constructions send
out more activation, and so have a greater effect. But in the
end, FIG just follows the simple rule, &apos;select and emit the
most highly activated word.&apos; Thus word order is emergent.
In FIG the current syntactic state is encoded in construc-
tions&apos; activation levels and &apos;cursors.&apos; The cursor of a con-
struction points to the currently appropriate constituent and
ensures that it is relatively highly activated. To be spe-
cific, the cursor gives the location of a `mask&apos; specifying the
weights of the links from the construction to constituents.
The mask specifies a weight of 1.0 for the constituent un-
der the cursor, and for subsequent constituents a weight pro-
portional to their closeness to the cursor. (Subsequent con-
stituents must receive some activation so that there is part-
wise parallelism.) (For unordered constructions the weights
on all construction-constituent links are the same.)
For example, when the cursor of noun-phr points to up-
1, articles receive a large proportion of the activation of
noun-phr. Thus, an article is likely to be the most highly
activated word and therefore selected and emitted. After an
article is emitted the cursor is advanced to np-2, and so on.
Advancing cursors is described in Section 6.5.
In accordance with the intuition that a word is not truly
appropriate unless it is both syntactically and semantically
appropriate, the activation level for words is given by the
product (not the sum) of incoming syntactic and seman-
tic activation, where `syntactic activation&apos; is activation re-
ceived from constituents and syntactic categories. The
problem with simply summing is that it results in the the
network often being in a state where many word-nodes have
nearly equal activation, which makes the behavior is over-
sensitive to minor changes in link weights.
</bodyText>
<subsectionHeader confidence="0.989661">
6.4 Optional Constituents
</subsectionHeader>
<bodyText confidence="0.999794217391304">
When building a noun-phrase a generator should emit an
adjective if semantically appropriate, otherwise it should ig-
nore that option and emit a noun next. FIG does this without
additional mechanism.
To see this, suppose &amp;quot;the&amp;quot; has been emitted and the cursor
of noun-phr is on its second constituent, np-2. As a result
adjectives get activation, via np-2, and so to a lesser extent
do nouns via np-3. There are two cases: If the input includes
a concept linked (indirectly perhaps) to some adjective, that
adjective will receive activation from it. In this case the ad-
jective will receive more syntactic activation than any noun
does, and hence have more total activation, so it will be se-
lected next. If the input does not include any concept linked
to an adjective, then a noun will have more activation than
any adjective (since only the noun receives semantic activa-
tion also), and so a noun will be selected next.
Most generators use some syntax-driven procedure to in-
spect semantics and decide explicitly whether or not to real-
ize an optional constituent. In FIG, the decision to include
or to omit an optional constituent (or adjunct) is emergent
— if an adjective becomes highly activated it will be cho-
sen, in the usual fashion, otherwise some other word, most
likely a noun, will be.
</bodyText>
<subsectionHeader confidence="0.998302">
6.5 Updating Constructions
</subsectionHeader>
<bodyText confidence="0.99994575">
Recall that FIG, after selecting and emitting a word, up-
dates activation levels to represent the new state. There are
are several aspects to this.
The cursors of constructions must advance as constituents
are completed. The update mechanism can &apos;skip over&apos; opt
constituents, since, for example, if there are no adjectives,
the cursor of noun-phr should not remain stuck forever at
the second constituent. More than one construction may be
updated after a word is output, for example, emitting a noun
may cause updates to both the prep-phr construction and
the noun-phr construction.
Constructions which are `guiding&apos; the output should be
scored as more relevant. Therefore the update process
adds activation to those constructions whose cursors have
changed and sets temporary lower bounds on their activa-
tion levels. Thus, even though FIG does not make any syn-
tactic plans, it tends to form a grammatical continuation of
whatever it has already output. After the last constituent of
a construction has been completed, the cursor is reset and
the lower bound is removed.
Why is a separate update mechanism necessary? Most
generators simply choose a construction and `execute&apos; it
straightforwardly. However, in FIG no construction is ever
&apos;in control.&apos; For example, one construction may be strongly
activating a verb, but activation from other constructions
may `interfere,&apos; causing an adverbial, for example, to be in-
terpolated. Therefore constructions need this kind of feed-
back on what words have been output.
</bodyText>
<subsectionHeader confidence="0.963918">
6.6 No Instantiation or Binding
</subsectionHeader>
<bodyText confidence="0.9999774">
It is not obvious that notions of instantiation, binding, em-
bedding, or recursion are essential for the description of nat-
ural language. Nor are mechanisms for these things essen-
tial for the generation task, I conjecture. This subsection
considers a problem which is usually handled with instanti-
ation and shows how it can be handled more simply without.
Consider the problem of generating utterances with mul-
tiple `copies,&apos; for example, several noun phrases, or several
uses of &amp;quot;a&amp;quot;. Note that FIG as described so far would have
problems with this. For example since all words of cate-
gory cnoun have links to noun-phr, that node might re-
ceive more activation than appropriate, in cases when sev-
eral nouns are active. This could result in over-activation of
articles, and thus premature output of &amp;quot;the,&amp;quot; for example.
In fact FIG uses a special rule for activation received
across inherited links: the maximum (not the sum) of these
amounts is used. For example, this rule applies to the &apos;max-
imal&apos; links from nouns to noun-phr, thus noun-phr effec-
tively &apos;ignores&apos; all but the most highly activated noun. (This
was not shown in Figure 5.)
</bodyText>
<page confidence="0.99884">
19
</page>
<figureCaption confidence="0.991844">
Figure 6: An Input to FIG
</figureCaption>
<figure confidence="0.999577909090909">
destinationr
paste
agentr
purposer
toplcc
prag -role
wash-clothesc1
old-womanc1
np-1
np-3 „,4N
noun-phr
old-womanw
go-c1
in-contextw
old-womanc1
topicc
a-w
article
the-w
sp-1
subj-pred
go-w
</figure>
<figureCaption confidence="0.999991">
Figure 7: Selected Paths of Activation Flow Just Before Output of &amp;quot;the&amp;quot;
</figureCaption>
<bodyText confidence="0.999985625">
An earlier version of FIG handled this by actually mak-
ing copies. For example, it would make a copy of noun-
phr for each noun-expressible concept, and bind each copy
to the appropriate concept, and to copies of a-w and the-
w. This worked but it made the program hard to extend.
In particular, it was hard to choose weights such that the
network would behave properly both before and after new
nodes were instantiated and linked in.
</bodyText>
<subsectionHeader confidence="0.987982">
6.7 Low-level Coherence
</subsectionHeader>
<bodyText confidence="0.999959548387097">
Words must stand in the correct relations to their neigh-
bors. For example, a generator must not produce &amp;quot;the big
man went to the mountain&amp;quot; when the input calls for &amp;quot;the
man went to the big mountain&amp;quot;. This is the problem of emit-
ting the right adjective at the right time, or, in other words,
only emitting adjectives that stand in an appropriate relation
to the head noun.
Most generators handle this easily with structure-
mapping or pointer following. For example, a syntax-
directed generator may, whenever building a noun phrase,
traverse the &apos;modified-by&apos; pointer to find the item to turn
into an adjective. FIG, however, eschews structure manip-
ulation and pointer following. Like all connectionist ap-
proaches, therefore, it is potentially subject to problems with
crosstalk.
The way to avoid this is to ensure that related concepts be-
come highly activated together. In the example, bigc should
become activated together with mountainc, not together
with old-manc. Using a more elaborate terminology, this
means that there should be some kind of &apos;focus of attention&apos;
(Chafe 1980), which successively &apos;lights up&apos; groups of re-
lated nodes.
This condition is met in FIG, thanks to the links among
the nodes of the input. For example, if mountaincl is linked
by a sizer link to bigcl, then bigcl will tend to become
highly activated whenever mountaincl is. Thus, when old-
mancl is the most highly activated concept-node, bigcl
will only receive energy from it indirectly (via an inverse-
agentr link, a locationr link, and a sizer link) and thus will
not be activated sufficiently to interfere early in the sen-
tence.
</bodyText>
<sectionHeader confidence="0.992962" genericHeader="method">
7 Example
</sectionHeader>
<bodyText confidence="0.999940181818182">
This section describes how FIG produces &amp;quot;the old woman
went to a stream to wash clothes.&amp;quot; For this example
the input is the set of nodes go-cl, old-womancl, wash-
clothescl, streamcl, and pastc, linked together as shown
in Figure 6. (The names of the concepts have been an-
glicized for the reader&apos;s convenience.) (Boxes are drawn
around nodes in the input so that they can be easily identi-
fied in subsequent diagrams.)
Initially each node of the input has 11 units of activation.
After activation flows, before any word is output, the most
highly activated word node is the-w, primarily for the rea-
sons shown in Figure 7. Figure 8 shows the activation levels
of selected nodes.
After &amp;quot;the&amp;quot; is emitted the update mechanism activates
noun-phr and advances its cursor to np-2. The most highly
activated word becomes old-womanw, largely due to acti-
vation from np-3.
After &amp;quot;old woman&amp;quot; is emitted noun-phr is reset — that
is, the cursor is set back to np-1 and it thereby becomes
ready to guide production of another noun phrase. Also,
now the cursor on subj-pred advances to sp-2. As a result
verbs, in particular go-w, become highly activated.
</bodyText>
<page confidence="0.884741">
20
</page>
<figure confidence="0.995942146341463">
----PATTERNS---
15.6 SUBJ-PRED
SP-1 sp-2
7.6 CAUSATIVEP
CP-1 cp-2 cp-3
6.6 NOUN-PHR
NP-1 np-2 np-3
1.8 GO-P
GP-1 gp-2 gp-3 gp-4
1.4 PURPOSE-CLAUSE
PC-1 pc-2 pc-3
0.2 PREP-PER
PP-1 pp-2
----WORDS
29.7 THE-W
21.0 A-W
18.5 OLD -WOMANW
13.3 STREAMW
10.7 RIVERW
10.0 GO-W
7.5 WASH-CLOTHESW
3.9 TO1W
3.2 MAKEW
2.9 TOWARDSW
2.9 INTOW
2.5 TO2W
0.4 WITHW
---CONCEPTS---
19.7 OLD-WOMANC1
15.0 IKUC1
14.0 KAWAC1
13.2 SENTAKUC1
11.0 PASTC
8.3 VOWEL-INITIAL
6.1 CONSNT-INITIAL
5.8 TOPICC
----OTHER
13.4 CAUSER
10.4 AGENTR
6.9 ARTICLE
4.5 NOUN
</figure>
<figureCaption confidence="0.998645">
Figure 8: Activation Levels of Selected Nodes Just Before Output of &amp;quot;the&amp;quot;
</figureCaption>
<figure confidence="0.97032425">
destinationr
.71r
gP-3 &amp;quot;7.0, pp-1
pp-2
</figure>
<figureCaption confidence="0.994739">
Figure 9: Selected Paths of Activation Flow Just Before Output of &amp;quot;to&amp;quot;
</figureCaption>
<figure confidence="0.941162571428571">
go-p
prep-phr
-----,‘‘
-
preposition..\ 0,- tolw
streamo1
noun 0&apos; streamw
</figure>
<bodyText confidence="0.997333714285714">
go-w is selected. Because pastc has more activation than
presentc, infinitivec and so on, go-w is inflected and emit-
ted as &amp;quot;went&amp;quot; (the inflection mechanism is not described in
this paper). go-p &apos;s cursor advances to its second constituent,
thus it activates directional particles, although there is no se-
mantic input to any such word in this case. tolw becomes
the most highly activated word, primarily for the reasons
shown in Figure 9.
After &amp;quot;to&amp;quot; is emitted, the cursor of prep-phr is advanced.
The key path of activation flow is now from the second con-
stituent of prep-phr to noun to streamw to noun-phr to
article to a-w. Thus a is selected. The inflection mecha-
nism produces &amp;quot;a&amp;quot; not &amp;quot;an&amp;quot; since consnt-initial is more
highly activated than vowel-initial.
Then the cursor of noun-phr advances and &amp;quot;stream&amp;quot; is
emitted. After this the cursor of go-p advances to gp-4.
From this constituent activation flows to purpose-clause,
and in due course &amp;quot;to&amp;quot; and &amp;quot;wash clothes&amp;quot; are emitted.
Now that all the nodes of the input are expressed, FIG
ends, having produced &amp;quot;the old woman went to a stream to
wash clothes.&amp;quot;
</bodyText>
<sectionHeader confidence="0.983513" genericHeader="method">
8 About the Implementation
</sectionHeader>
<bodyText confidence="0.999982533333333">
I have used a connectionist model because it is a good
way to explore interactivity, parallelism, emergents, not be-
cause of fondness for connectionism-for-its-own-sake.
Thus I have not attempted to develop a distributed con-
nectionist model. Distributed models do have various ad-
vantages, such as elegant handling of generalizations and
the potential for learning. Yet the current state of PDP tech-
nology does not seem up to building an interactive model of
a complex task like language generation. I therefore devel-
oped FIG as a structured (localist) connectionist system.
I have also not attempted to make FIG a &apos;pure&apos; connec-
tionist model. For example, updating constructions is cur-
rently done by a special process that goes in and changes
activation levels and moves the cursor. (This process uses
the third elements in the constituent descriptions of Figures
1-3, not previously discussed.) FIG could be made more
&apos;pure&apos; by doing this connectionistically, perhaps by adding
new nodes with special properties. But this change would
not improve F1G&apos;s performance, since there seems no need
for the update process to interact with the other processes.
A connectionist model of computation allows parallelism
and emergents, but it certainly does not require them. In-
deed, other generators built using structured connection-
ism (Kalita &amp; Shastri 1987; Gasser 1988; Kitano 1989;
Stokke 1989) do not appear to exploit parallelism much, nor
do they exhibit emergent properties. For example, Gasser&apos;s
CHIE relies heavily on winner-take-all subnetworks, which
cuts down on the amount of effective parallelism. Also, far
from exploiting emergents, CHIE uses &apos;neuron firings&apos; to
model syntactic choices; these happen sequentially and the
</bodyText>
<page confidence="0.996827">
21
</page>
<bodyText confidence="0.999830833333333">
exact order and timing of firings seems crucial.
Currently FIG has about 350 nodes and 1000 links. Be-
fore each word choice, activation flows until the network
settles down, with cutoff after 9 cycles. This takes about .2
seconds per word on average, simulating parallel activation
flow on a Symbolics 3670 (1.6 seconds on a Sun 3/140).
The correct operation of FIG depends on having correct
link weights. I have no theory of weights, indeed finding
appropriate ones is still largely an empirical process. How-
ever there are regularities, for example, all &apos;inhibit&apos; links
have weight .7, almost all links from syntactic categories
to their members have weight .5, and so on. Many of the
weights have a rationale: for example, the link from np-1
to articles has a relatively high weight because articles get
very little activation from other sources. No single weight
is meaningful; the way it functions in context is. For exam-
ple, the exact weight of the link from the first constituent of
sub j-pred to noun is not crucial, as long as the product of
it and the weight on the agentr relation is appropriate.
FIG&apos;s knowledge is, of course, very limited. Adding new
concepts, words or constructions is generally straightfor-
ward; they can be encoded by analogy to similar nodes, and
usually the same link weights suffice. Occasionally new
nodes and links interact with other knowledge in the system
in unforeseen ways, causing other nodes to get too much
or too little activation. In these cases it is necessary to de-
bug the network. Sometimes trial-and-error experimenta-
tion is required, but often the acceptable range of weights
can be determined by examination. This is a kind of back-
propagation by hand; it could doubtless be automated.
</bodyText>
<sectionHeader confidence="0.998134" genericHeader="conclusions">
9 Summary
</sectionHeader>
<bodyText confidence="0.9999746875">
I have proposed a new way to handle syntax for genera-
tion. The proposal also relies heavily on parallelism: part-
wise parallelism, competition, and cooperation. Also, syn-
tactic considerations are used in parallel with lexical and
world knowledge and there is pervasive interaction among
them. This promises improved output quality without sacri-
ficing speed, on parallel hardware. The proposal also relies
heavily on emergents — it does not make syntactic choices
nor build up representations of syntactic structure. The net-
work representations of linguistic knowledge affect word
choice and order directly.
This work is not traditional linguistics, artificial intelli-
gence, or connectionism, but uses techniques from all three
fields. I hope this will stimulate further work in empirical
computational linguistics, modeling human language pro-
duction, and building useful parallel generation systems.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999933055555556">
Baars, Bernard K. (1980). The Competing Plans Hypoth-
esis: an heuristic viewpoint on the causes of errors in
speech. In Hans W. Dechert &amp; Manfred Raupach, edi-
tors, Temporal Variables in Speech. Mouton.
Chafe, Wallace L. (1980). The Deployment of Conscious-
ness in the Production of a Narrative. In Wallace L. Chafe,
editor, The Pear Stories. Ablex.
De Smedt, Koenrad J.M.J. (1990). Incremental Sentence
Generation: a computer model of grammatical encoding.
Technical Report 90-01, Nijmegen Institute for Cognition
Research and Information Technology.
Fillmore, Charles (1989a). The Mechanisms of &amp;quot;Construc-
tion Grammar&amp;quot;. In Proceedings of the Berkeley Linguistic
Society, volume 15.
Fillmore, Charles (1989b). On Grammatical Constructions.
course notes, UC Berkeley Linguistics Department.
Fitilder, Wolfgang &amp; Giinter Neumann (1989). POPEL-
HOW: A Distributed Parallel Model for Incremental Nat-
ural Language Production with Feedback. In Proceedings
of the Eleventh International Joint Conference on Artifi-
cial Intelligence. Detroit.
Gasser, Micheal (1988). A Connectionist Model of Sen-
tence Generation in a First and Second Language. Tech-
nical Report UCLA-AI-88-13, Los Angeles.
Kalita, Jugal &amp; Lokendra Shastri (1987). Generation of
Simple Sentences in English Using the Connectionist
Model of Computation. In 9th Cognitive Science Con-
ference. Lawrence Erlbaum Associates.
ICitano, Hiroaki (1989). A Massively Parallel Model of
Natural Language Generation for Interpreting Telephony:
Almost Concurrent Processing of Parsing and Genera-
tion. In Proceedings of the Second European Workshop
on Natural Language Generation.
Stemberger, J. P. (1985). An Interactive Activation Model
of Language Production. In Andrew W. Ellis, edi-
tor, Progress in the Psychology of Language, Volume 1.
Lawrence Erlbaum Associates.
Stokke, Andreas (1989). Processing Unification-based
Grammars in a Connectionist Framework. In 11thCogni-
tive Science Conference. Lawrence Erlbaum Associates.
Ward, Nigel (1988). Issues in Word Choice. In Proceedings
12th COLING. Budapest
Ward, Nigel (1989a). Capturing Intuitions about Human
Language Production. In Proceedings, Cognitive Science
Conference. Lawrence Erlbaum Associates. Ann Arbor.
Ward, Nigel (1989b). On the Ordering of Decisions in Ma-
chine Translation. In Proceedings of the Third Annual
Conference of the Japanese Society for Artificial Intelli-
gence, Tokyo.
Ward, Nigel (1989c). Towards Natural Machine Transla-
tion. In Proceedings of the EIC Workshop on Artificial
Intelligence, Tokyo. Institute of Electronics, Information,
and Communication Engineers. Published as Technical
Research Report AI89-30.
</reference>
<page confidence="0.999029">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941028">
<title confidence="0.997534">A Connectionist Treatment of Grammar for Generation: Relying on Emergents</title>
<author confidence="0.999425">Nigel Ward</author>
<affiliation confidence="0.999896">Computer Science Division University of California at Berkeley</affiliation>
<abstract confidence="0.9966915">Parallel treatment of syntactic considerations in generation promises quality and speed. Parallelism should be used not only for simultaneous processing of several sub-parts of the output, but even within single parts. If both types of parallelism are used with incremental generation it becomes unnecessary to build up and manipulate representations of sentence structure — the syntactic form of the output can be emergent. FIG is a structured cormectionist generator built in this way. Constructions and their constituents are represented in the same network which encodes world knowledge and lexical knowledge. Grammatical output results from synergy among many constructions simultaneously active at run-time. FIG incorporates new ways of handling constituency, word order and optional constituents; and simple ways to avoid the problems of instantiation and binding. Syntactic knowledge is expressed in a simple, readable form; this representation straightforwardly defines parts of the network.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernard K Baars</author>
</authors>
<title>The Competing Plans Hypothesis: an heuristic viewpoint on the causes of errors in speech.</title>
<date>1980</date>
<booktitle>Temporal Variables in Speech.</booktitle>
<editor>In Hans W. Dechert &amp; Manfred Raupach, editors,</editor>
<publisher>Mouton.</publisher>
<contexts>
<context position="4773" citStr="Baars 1980" startWordPosition="747" endWordPosition="748">this type of parallelism as &apos;part-wise&apos; parallelism. A second kind of parallelism involves using several constructions to generate even one part of the output. As far as I know, this &apos;within-part&apos; parallelism has not been proposed in the generation literature. It has proven useful in linguistics. In Fillmore&apos;s Construction Grammar the syntactic 15 structure of sentences is accounted for in terms of &apos;superimposition&apos; of constructions (Fillmore 1989b). It has also been used in psycholinguistics, where analysis of speech errors suggests that even normal speech is the result of competing `plans&apos; (Baars 1980). More specifically, (S temberger 1985) suggested that human speakers can be modeled as having many `phrase structure units&apos; being `partially activated&apos; simultaneously. That is, many syntactic alternatives for expressing some piece of meaning are considered in parallel. I propose that a generator should exploit both part-wise and within-part parallelism. Parallel generation is a good idea for several reasons. 1. It has been observed that part-wise parallelism is a good way to improve the speed of response, especially for incremental generation. 2. Part-wise parallelism is also useful for handl</context>
</contexts>
<marker>Baars, 1980</marker>
<rawString>Baars, Bernard K. (1980). The Competing Plans Hypothesis: an heuristic viewpoint on the causes of errors in speech. In Hans W. Dechert &amp; Manfred Raupach, editors, Temporal Variables in Speech. Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wallace L Chafe</author>
</authors>
<title>The Deployment of Consciousness in the Production of a Narrative.</title>
<date>1980</date>
<editor>In Wallace L. Chafe, editor, The Pear Stories.</editor>
<publisher>Ablex.</publisher>
<contexts>
<context position="25379" citStr="Chafe 1980" startWordPosition="4040" endWordPosition="4041">ntaxdirected generator may, whenever building a noun phrase, traverse the &apos;modified-by&apos; pointer to find the item to turn into an adjective. FIG, however, eschews structure manipulation and pointer following. Like all connectionist approaches, therefore, it is potentially subject to problems with crosstalk. The way to avoid this is to ensure that related concepts become highly activated together. In the example, bigc should become activated together with mountainc, not together with old-manc. Using a more elaborate terminology, this means that there should be some kind of &apos;focus of attention&apos; (Chafe 1980), which successively &apos;lights up&apos; groups of related nodes. This condition is met in FIG, thanks to the links among the nodes of the input. For example, if mountaincl is linked by a sizer link to bigcl, then bigcl will tend to become highly activated whenever mountaincl is. Thus, when oldmancl is the most highly activated concept-node, bigcl will only receive energy from it indirectly (via an inverseagentr link, a locationr link, and a sizer link) and thus will not be activated sufficiently to interfere early in the sentence. 7 Example This section describes how FIG produces &amp;quot;the old woman went </context>
</contexts>
<marker>Chafe, 1980</marker>
<rawString>Chafe, Wallace L. (1980). The Deployment of Consciousness in the Production of a Narrative. In Wallace L. Chafe, editor, The Pear Stories. Ablex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koenrad J M J De Smedt</author>
</authors>
<title>Incremental Sentence Generation: a computer model of grammatical encoding.</title>
<date>1990</date>
<tech>Technical Report 90-01,</tech>
<institution>Nijmegen Institute for Cognition Research and Information Technology.</institution>
<marker>De Smedt, 1990</marker>
<rawString>De Smedt, Koenrad J.M.J. (1990). Incremental Sentence Generation: a computer model of grammatical encoding. Technical Report 90-01, Nijmegen Institute for Cognition Research and Information Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>The Mechanisms of &amp;quot;Construction Grammar&amp;quot;.</title>
<date>1989</date>
<booktitle>In Proceedings of the Berkeley Linguistic Society,</booktitle>
<volume>15</volume>
<contexts>
<context position="4613" citStr="Fillmore 1989" startWordPosition="722" endWordPosition="723">the input in parallel, simultaneously building several sub-trees. Recent work in this area includes (De Smedt 1990) and (Finlder &amp; Neumann 1989). I will refer to this type of parallelism as &apos;part-wise&apos; parallelism. A second kind of parallelism involves using several constructions to generate even one part of the output. As far as I know, this &apos;within-part&apos; parallelism has not been proposed in the generation literature. It has proven useful in linguistics. In Fillmore&apos;s Construction Grammar the syntactic 15 structure of sentences is accounted for in terms of &apos;superimposition&apos; of constructions (Fillmore 1989b). It has also been used in psycholinguistics, where analysis of speech errors suggests that even normal speech is the result of competing `plans&apos; (Baars 1980). More specifically, (S temberger 1985) suggested that human speakers can be modeled as having many `phrase structure units&apos; being `partially activated&apos; simultaneously. That is, many syntactic alternatives for expressing some piece of meaning are considered in parallel. I propose that a generator should exploit both part-wise and within-part parallelism. Parallel generation is a good idea for several reasons. 1. It has been observed tha</context>
</contexts>
<marker>Fillmore, 1989</marker>
<rawString>Fillmore, Charles (1989a). The Mechanisms of &amp;quot;Construction Grammar&amp;quot;. In Proceedings of the Berkeley Linguistic Society, volume 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>On Grammatical Constructions. course notes,</title>
<date>1989</date>
<institution>UC Berkeley Linguistics Department.</institution>
<contexts>
<context position="4613" citStr="Fillmore 1989" startWordPosition="722" endWordPosition="723">the input in parallel, simultaneously building several sub-trees. Recent work in this area includes (De Smedt 1990) and (Finlder &amp; Neumann 1989). I will refer to this type of parallelism as &apos;part-wise&apos; parallelism. A second kind of parallelism involves using several constructions to generate even one part of the output. As far as I know, this &apos;within-part&apos; parallelism has not been proposed in the generation literature. It has proven useful in linguistics. In Fillmore&apos;s Construction Grammar the syntactic 15 structure of sentences is accounted for in terms of &apos;superimposition&apos; of constructions (Fillmore 1989b). It has also been used in psycholinguistics, where analysis of speech errors suggests that even normal speech is the result of competing `plans&apos; (Baars 1980). More specifically, (S temberger 1985) suggested that human speakers can be modeled as having many `phrase structure units&apos; being `partially activated&apos; simultaneously. That is, many syntactic alternatives for expressing some piece of meaning are considered in parallel. I propose that a generator should exploit both part-wise and within-part parallelism. Parallel generation is a good idea for several reasons. 1. It has been observed tha</context>
</contexts>
<marker>Fillmore, 1989</marker>
<rawString>Fillmore, Charles (1989b). On Grammatical Constructions. course notes, UC Berkeley Linguistics Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Fitilder</author>
<author>Giinter Neumann</author>
</authors>
<title>POPELHOW: A Distributed Parallel Model for Incremental Natural Language Production with Feedback.</title>
<date>1989</date>
<booktitle>In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence.</booktitle>
<location>Detroit.</location>
<marker>Fitilder, Neumann, 1989</marker>
<rawString>Fitilder, Wolfgang &amp; Giinter Neumann (1989). POPELHOW: A Distributed Parallel Model for Incremental Natural Language Production with Feedback. In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. Detroit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micheal Gasser</author>
</authors>
<title>A Connectionist Model of Sentence Generation in a First and Second Language.</title>
<date>1988</date>
<tech>Technical Report UCLA-AI-88-13,</tech>
<location>Los Angeles.</location>
<contexts>
<context position="30343" citStr="Gasser 1988" startWordPosition="4860" endWordPosition="4861">on levels and moves the cursor. (This process uses the third elements in the constituent descriptions of Figures 1-3, not previously discussed.) FIG could be made more &apos;pure&apos; by doing this connectionistically, perhaps by adding new nodes with special properties. But this change would not improve F1G&apos;s performance, since there seems no need for the update process to interact with the other processes. A connectionist model of computation allows parallelism and emergents, but it certainly does not require them. Indeed, other generators built using structured connectionism (Kalita &amp; Shastri 1987; Gasser 1988; Kitano 1989; Stokke 1989) do not appear to exploit parallelism much, nor do they exhibit emergent properties. For example, Gasser&apos;s CHIE relies heavily on winner-take-all subnetworks, which cuts down on the amount of effective parallelism. Also, far from exploiting emergents, CHIE uses &apos;neuron firings&apos; to model syntactic choices; these happen sequentially and the 21 exact order and timing of firings seems crucial. Currently FIG has about 350 nodes and 1000 links. Before each word choice, activation flows until the network settles down, with cutoff after 9 cycles. This takes about .2 seconds </context>
</contexts>
<marker>Gasser, 1988</marker>
<rawString>Gasser, Micheal (1988). A Connectionist Model of Sentence Generation in a First and Second Language. Technical Report UCLA-AI-88-13, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jugal Kalita</author>
<author>Lokendra Shastri</author>
</authors>
<title>Generation of Simple Sentences in English Using the Connectionist Model of Computation.</title>
<date>1987</date>
<booktitle>In 9th Cognitive Science Conference. Lawrence Erlbaum Associates.</booktitle>
<contexts>
<context position="30330" citStr="Kalita &amp; Shastri 1987" startWordPosition="4856" endWordPosition="4859">in and changes activation levels and moves the cursor. (This process uses the third elements in the constituent descriptions of Figures 1-3, not previously discussed.) FIG could be made more &apos;pure&apos; by doing this connectionistically, perhaps by adding new nodes with special properties. But this change would not improve F1G&apos;s performance, since there seems no need for the update process to interact with the other processes. A connectionist model of computation allows parallelism and emergents, but it certainly does not require them. Indeed, other generators built using structured connectionism (Kalita &amp; Shastri 1987; Gasser 1988; Kitano 1989; Stokke 1989) do not appear to exploit parallelism much, nor do they exhibit emergent properties. For example, Gasser&apos;s CHIE relies heavily on winner-take-all subnetworks, which cuts down on the amount of effective parallelism. Also, far from exploiting emergents, CHIE uses &apos;neuron firings&apos; to model syntactic choices; these happen sequentially and the 21 exact order and timing of firings seems crucial. Currently FIG has about 350 nodes and 1000 links. Before each word choice, activation flows until the network settles down, with cutoff after 9 cycles. This takes abou</context>
</contexts>
<marker>Kalita, Shastri, 1987</marker>
<rawString>Kalita, Jugal &amp; Lokendra Shastri (1987). Generation of Simple Sentences in English Using the Connectionist Model of Computation. In 9th Cognitive Science Conference. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroaki ICitano</author>
</authors>
<title>A Massively Parallel Model of Natural Language Generation for Interpreting Telephony: Almost Concurrent Processing of Parsing and Generation.</title>
<date>1989</date>
<booktitle>In Proceedings of the Second European Workshop on Natural Language Generation.</booktitle>
<marker>ICitano, 1989</marker>
<rawString>ICitano, Hiroaki (1989). A Massively Parallel Model of Natural Language Generation for Interpreting Telephony: Almost Concurrent Processing of Parsing and Generation. In Proceedings of the Second European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Stemberger</author>
</authors>
<title>An Interactive Activation Model of Language Production.</title>
<date>1985</date>
<booktitle>Progress in the Psychology of Language, Volume 1. Lawrence Erlbaum Associates.</booktitle>
<editor>In Andrew W. Ellis, editor,</editor>
<marker>Stemberger, 1985</marker>
<rawString>Stemberger, J. P. (1985). An Interactive Activation Model of Language Production. In Andrew W. Ellis, editor, Progress in the Psychology of Language, Volume 1. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stokke</author>
</authors>
<title>Processing Unification-based Grammars in a Connectionist Framework.</title>
<date>1989</date>
<booktitle>In 11thCognitive Science Conference. Lawrence Erlbaum Associates.</booktitle>
<contexts>
<context position="30370" citStr="Stokke 1989" startWordPosition="4864" endWordPosition="4865">rsor. (This process uses the third elements in the constituent descriptions of Figures 1-3, not previously discussed.) FIG could be made more &apos;pure&apos; by doing this connectionistically, perhaps by adding new nodes with special properties. But this change would not improve F1G&apos;s performance, since there seems no need for the update process to interact with the other processes. A connectionist model of computation allows parallelism and emergents, but it certainly does not require them. Indeed, other generators built using structured connectionism (Kalita &amp; Shastri 1987; Gasser 1988; Kitano 1989; Stokke 1989) do not appear to exploit parallelism much, nor do they exhibit emergent properties. For example, Gasser&apos;s CHIE relies heavily on winner-take-all subnetworks, which cuts down on the amount of effective parallelism. Also, far from exploiting emergents, CHIE uses &apos;neuron firings&apos; to model syntactic choices; these happen sequentially and the 21 exact order and timing of firings seems crucial. Currently FIG has about 350 nodes and 1000 links. Before each word choice, activation flows until the network settles down, with cutoff after 9 cycles. This takes about .2 seconds per word on average, simula</context>
</contexts>
<marker>Stokke, 1989</marker>
<rawString>Stokke, Andreas (1989). Processing Unification-based Grammars in a Connectionist Framework. In 11thCognitive Science Conference. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Ward</author>
</authors>
<title>Issues in Word Choice.</title>
<date>1988</date>
<booktitle>In Proceedings 12th COLING.</booktitle>
<location>Budapest</location>
<contexts>
<context position="7747" citStr="Ward 1988" startWordPosition="1232" endWordPosition="1233">ivation 2. activation flows through the network 3. when the network settles, the most highly activated word is selected and emitted 4. activation levels are updated to represent the new current state 5. steps 2 through 4 repeat until all of the input has been conveyed Thus FIG is an incremental generator. Its network must be designed so that, when it settles, the node which is most highly activated corresponds to the best next word. This paper discusses only the network structures which encode syntactic knowledge. Elsewhere I argue that FIG points the way to accurate and flexible word choice (Ward 1988), producing naturalsounding output for machine translation (Ward 1989c), and modeling the key aspects of the human language production process (Ward 1989a). 4 Connectionist Syntax: Overview In FIG constructions and constituents also are represented as nodes in the knowledge network. Their activation levels represent their current relevance. They interact with other nodes by means of activation flow. Any number of constructions can be simultaneously active. This handles part-wise parallelism, competition, and superimposition. Syntactic considerations manifest themselves only through their effec</context>
<context position="17142" citStr="Ward 1988" startWordPosition="2696" endWordPosition="2697">f specifying where a given concept should appear and what syntactic form it should take. In FIG this is handled by simultaneously activating a concept node and a syntactic construction or category node. For example, the third constituent of go-p specifies that `the direction of the going&apos; be expressed as a &apos;verbal particle.&apos; Activation will thus flow to an appropriate word node, such as downw, both via the concept filling the directionr slot and via the syntactic category vparticle. Thanks to this sort of activation flow FIG tends to select and emit an appropriate word in an appropriate form (Ward 1988). Government, for example, the way that some verbs govern case markers, is handled in the same way. 6.3 Word Order In an incremental connectionist generator, at each time the activation level of a word must represent its current relevance. In particular, words which are currently syntactically appropriate must be strongly activated. In FIG the representation of the current syntactic state is distributed across the constructions. There is no central process which plans or manipulates word order; each construction simply operates 18 independently. More highly activated constructions send out mor</context>
</contexts>
<marker>Ward, 1988</marker>
<rawString>Ward, Nigel (1988). Issues in Word Choice. In Proceedings 12th COLING. Budapest</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Ward</author>
</authors>
<title>Capturing Intuitions about Human Language Production. In Proceedings, Cognitive Science Conference. Lawrence Erlbaum Associates.</title>
<date>1989</date>
<location>Ann Arbor.</location>
<contexts>
<context position="6423" citStr="Ward 1989" startWordPosition="1008" endWordPosition="1009"> utterance, there is the possibility that a `first choice&apos; will not work out when the larger context is considered. This suggests within-part parallelism, so that a generator has available alternative ways to realize some information. Given this it can find a set of choices satisfies all the dependencies, resulting in consistent and natural utterance. 4. If a generator is indeed to consider all the possible dependencies among choices, then parallelism becomes necessary to cope with the amount of computation necessary. 5. Parallelism is the natural way to generate if the input is very complex (Ward 1989a). 3 The FIG Approach to Generation Reduced to bare essentials, a generator&apos;s task is to get from concepts (what the speaker wants to express) to words (what he can say). On this view, the key problem in generation is computing the relevance (pertinence) of a particular word, given the concepts to express. Syntactic and other knowledge mediates this computation of relevance. Accordingly FIG is based on word choice — every other consideration is analyzed in terms of how it affects word choice. FIG is based on a large semantic network. Words are nodes in the network, the activation they receive</context>
<context position="7816" citStr="Ward 1989" startWordPosition="1241" endWordPosition="1242">ttles, the most highly activated word is selected and emitted 4. activation levels are updated to represent the new current state 5. steps 2 through 4 repeat until all of the input has been conveyed Thus FIG is an incremental generator. Its network must be designed so that, when it settles, the node which is most highly activated corresponds to the best next word. This paper discusses only the network structures which encode syntactic knowledge. Elsewhere I argue that FIG points the way to accurate and flexible word choice (Ward 1988), producing naturalsounding output for machine translation (Ward 1989c), and modeling the key aspects of the human language production process (Ward 1989a). 4 Connectionist Syntax: Overview In FIG constructions and constituents also are represented as nodes in the knowledge network. Their activation levels represent their current relevance. They interact with other nodes by means of activation flow. Any number of constructions can be simultaneously active. This handles part-wise parallelism, competition, and superimposition. Syntactic considerations manifest themselves only through their effects on the activation levels of words (directly or indirectly). An utt</context>
<context position="11093" citStr="Ward 1989" startWordPosition="1743" endWordPosition="1744">Phrase Construction (defp go-p obl go-w ((go-w .2))) (constituents (qp-1 opt epart ((vparticle .6) (directionr .2))) (gp-2 opt noun ((prep-phr .6) (destinationr .2))) (gP-3 opt verb ((purpose-clause .7) (purposer .2))) )) (gp-4 Figure 2: Representation of the Valence of &amp;quot;Go&amp;quot; (defp ex-there (inhibit subj-pred passive) (constituents (et-1 obl therew (et-2 obl verb (et-3 obl noun ((therew .5))) ((verb .5))) ((noun .3))) )) Figure 3: Representation of the Existential &amp;quot;There&amp;quot; Construction there are no explicit choices to worry about, and thus there are no problems of ordering or bookkeeping at all(Ward 1989b). In FIG all types of knowledge represented are uniformly in the network, and interact freely at run time. FIG not only allows this kind of interaction among various considerations when generating, it relies on it. It relies on synergy among constructions in the same way that Construction Grammar does. It relies on synergy between semantic and syntactic considerations, as seen below in Section 6.7. It also enables interaction among lexical choices and syntactic considerations. 5 KnowledgeofSyntax This section presents FIG&apos;s representation of knowledge, first presenting it in a declarative fo</context>
</contexts>
<marker>Ward, 1989</marker>
<rawString>Ward, Nigel (1989a). Capturing Intuitions about Human Language Production. In Proceedings, Cognitive Science Conference. Lawrence Erlbaum Associates. Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Ward</author>
</authors>
<title>On the Ordering of Decisions in Machine Translation.</title>
<date>1989</date>
<booktitle>In Proceedings of the Third Annual Conference of the Japanese Society for Artificial Intelligence,</booktitle>
<location>Tokyo.</location>
<contexts>
<context position="6423" citStr="Ward 1989" startWordPosition="1008" endWordPosition="1009"> utterance, there is the possibility that a `first choice&apos; will not work out when the larger context is considered. This suggests within-part parallelism, so that a generator has available alternative ways to realize some information. Given this it can find a set of choices satisfies all the dependencies, resulting in consistent and natural utterance. 4. If a generator is indeed to consider all the possible dependencies among choices, then parallelism becomes necessary to cope with the amount of computation necessary. 5. Parallelism is the natural way to generate if the input is very complex (Ward 1989a). 3 The FIG Approach to Generation Reduced to bare essentials, a generator&apos;s task is to get from concepts (what the speaker wants to express) to words (what he can say). On this view, the key problem in generation is computing the relevance (pertinence) of a particular word, given the concepts to express. Syntactic and other knowledge mediates this computation of relevance. Accordingly FIG is based on word choice — every other consideration is analyzed in terms of how it affects word choice. FIG is based on a large semantic network. Words are nodes in the network, the activation they receive</context>
<context position="7816" citStr="Ward 1989" startWordPosition="1241" endWordPosition="1242">ttles, the most highly activated word is selected and emitted 4. activation levels are updated to represent the new current state 5. steps 2 through 4 repeat until all of the input has been conveyed Thus FIG is an incremental generator. Its network must be designed so that, when it settles, the node which is most highly activated corresponds to the best next word. This paper discusses only the network structures which encode syntactic knowledge. Elsewhere I argue that FIG points the way to accurate and flexible word choice (Ward 1988), producing naturalsounding output for machine translation (Ward 1989c), and modeling the key aspects of the human language production process (Ward 1989a). 4 Connectionist Syntax: Overview In FIG constructions and constituents also are represented as nodes in the knowledge network. Their activation levels represent their current relevance. They interact with other nodes by means of activation flow. Any number of constructions can be simultaneously active. This handles part-wise parallelism, competition, and superimposition. Syntactic considerations manifest themselves only through their effects on the activation levels of words (directly or indirectly). An utt</context>
<context position="11093" citStr="Ward 1989" startWordPosition="1743" endWordPosition="1744">Phrase Construction (defp go-p obl go-w ((go-w .2))) (constituents (qp-1 opt epart ((vparticle .6) (directionr .2))) (gp-2 opt noun ((prep-phr .6) (destinationr .2))) (gP-3 opt verb ((purpose-clause .7) (purposer .2))) )) (gp-4 Figure 2: Representation of the Valence of &amp;quot;Go&amp;quot; (defp ex-there (inhibit subj-pred passive) (constituents (et-1 obl therew (et-2 obl verb (et-3 obl noun ((therew .5))) ((verb .5))) ((noun .3))) )) Figure 3: Representation of the Existential &amp;quot;There&amp;quot; Construction there are no explicit choices to worry about, and thus there are no problems of ordering or bookkeeping at all(Ward 1989b). In FIG all types of knowledge represented are uniformly in the network, and interact freely at run time. FIG not only allows this kind of interaction among various considerations when generating, it relies on it. It relies on synergy among constructions in the same way that Construction Grammar does. It relies on synergy between semantic and syntactic considerations, as seen below in Section 6.7. It also enables interaction among lexical choices and syntactic considerations. 5 KnowledgeofSyntax This section presents FIG&apos;s representation of knowledge, first presenting it in a declarative fo</context>
</contexts>
<marker>Ward, 1989</marker>
<rawString>Ward, Nigel (1989b). On the Ordering of Decisions in Machine Translation. In Proceedings of the Third Annual Conference of the Japanese Society for Artificial Intelligence, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Ward</author>
</authors>
<title>Towards Natural Machine Translation.</title>
<date>1989</date>
<booktitle>In Proceedings of the EIC Workshop on Artificial Intelligence, Tokyo. Institute of Electronics, Information, and Communication Engineers.</booktitle>
<note>Published as Technical Research Report AI89-30.</note>
<contexts>
<context position="6423" citStr="Ward 1989" startWordPosition="1008" endWordPosition="1009"> utterance, there is the possibility that a `first choice&apos; will not work out when the larger context is considered. This suggests within-part parallelism, so that a generator has available alternative ways to realize some information. Given this it can find a set of choices satisfies all the dependencies, resulting in consistent and natural utterance. 4. If a generator is indeed to consider all the possible dependencies among choices, then parallelism becomes necessary to cope with the amount of computation necessary. 5. Parallelism is the natural way to generate if the input is very complex (Ward 1989a). 3 The FIG Approach to Generation Reduced to bare essentials, a generator&apos;s task is to get from concepts (what the speaker wants to express) to words (what he can say). On this view, the key problem in generation is computing the relevance (pertinence) of a particular word, given the concepts to express. Syntactic and other knowledge mediates this computation of relevance. Accordingly FIG is based on word choice — every other consideration is analyzed in terms of how it affects word choice. FIG is based on a large semantic network. Words are nodes in the network, the activation they receive</context>
<context position="7816" citStr="Ward 1989" startWordPosition="1241" endWordPosition="1242">ttles, the most highly activated word is selected and emitted 4. activation levels are updated to represent the new current state 5. steps 2 through 4 repeat until all of the input has been conveyed Thus FIG is an incremental generator. Its network must be designed so that, when it settles, the node which is most highly activated corresponds to the best next word. This paper discusses only the network structures which encode syntactic knowledge. Elsewhere I argue that FIG points the way to accurate and flexible word choice (Ward 1988), producing naturalsounding output for machine translation (Ward 1989c), and modeling the key aspects of the human language production process (Ward 1989a). 4 Connectionist Syntax: Overview In FIG constructions and constituents also are represented as nodes in the knowledge network. Their activation levels represent their current relevance. They interact with other nodes by means of activation flow. Any number of constructions can be simultaneously active. This handles part-wise parallelism, competition, and superimposition. Syntactic considerations manifest themselves only through their effects on the activation levels of words (directly or indirectly). An utt</context>
<context position="11093" citStr="Ward 1989" startWordPosition="1743" endWordPosition="1744">Phrase Construction (defp go-p obl go-w ((go-w .2))) (constituents (qp-1 opt epart ((vparticle .6) (directionr .2))) (gp-2 opt noun ((prep-phr .6) (destinationr .2))) (gP-3 opt verb ((purpose-clause .7) (purposer .2))) )) (gp-4 Figure 2: Representation of the Valence of &amp;quot;Go&amp;quot; (defp ex-there (inhibit subj-pred passive) (constituents (et-1 obl therew (et-2 obl verb (et-3 obl noun ((therew .5))) ((verb .5))) ((noun .3))) )) Figure 3: Representation of the Existential &amp;quot;There&amp;quot; Construction there are no explicit choices to worry about, and thus there are no problems of ordering or bookkeeping at all(Ward 1989b). In FIG all types of knowledge represented are uniformly in the network, and interact freely at run time. FIG not only allows this kind of interaction among various considerations when generating, it relies on it. It relies on synergy among constructions in the same way that Construction Grammar does. It relies on synergy between semantic and syntactic considerations, as seen below in Section 6.7. It also enables interaction among lexical choices and syntactic considerations. 5 KnowledgeofSyntax This section presents FIG&apos;s representation of knowledge, first presenting it in a declarative fo</context>
</contexts>
<marker>Ward, 1989</marker>
<rawString>Ward, Nigel (1989c). Towards Natural Machine Translation. In Proceedings of the EIC Workshop on Artificial Intelligence, Tokyo. Institute of Electronics, Information, and Communication Engineers. Published as Technical Research Report AI89-30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>