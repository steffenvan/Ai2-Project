<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000879">
<note confidence="0.544385">
How to Expand Dictionaries with Web-Mining Techniques
Nicolas Béchet Mathieu Roche
LIRMM, UMR 5506, CNRS, LIRMM, UMR 5506, CNRS,
Univ. Montpellier 2 Univ. Montpellier 2
</note>
<email confidence="0.971406">
bechet@lirmm.fr mroche@lirmm.fr
</email>
<sectionHeader confidence="0.992794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999783923076923">
This paper presents an approach to en-
rich conceptual classes based on the
Web. To test our approach, we first build
conceptual classes using syntactic and
semantic information provided by a cor-
pus. The concepts can be the input of a
dictionary. Our web-mining approach
deals with a cognitive process which
simulates human reasoning based on the
enumeration principle. The experiments
reveal the interest of our approach by
adding new relevant terms to existing
conceptual classes.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999402688888889">
Concepts have several definitions; one of the
most general describes a concept ‘as the mind’s
representation of a thing or an item’ (Desrosiers-
Sabbath, 1984). In a domain such as ours, i.e.
ontology building, semantic webs, and computa-
tional linguistics, it seems appropriate to stick to
the Aristotelian approach to a concept, and con-
sider it as a set of knowledge (gathered informa-
tion) on common semantic features. The choice
of the features and how the knowledge is gath-
ered depend on criteria we will explain below.
In this paper, we deal with the building of
conceptual classes, which can be defined as
gathering semantically close terms. First, we
suggest building specific conceptual classes by
focusing on knowledge extracted from corpora.
Conceptual classes are shaped by the study of
syntactic dependencies between corpus terms (as
described in section 2). Dependencies tackle re-
lations such as Verb/Subject, Noun/Noun Phrase
Complements, Verb/Object, Verb/Complements,
and sometimes Sentence Head/Complements. In
this paper, we focus on the Verb/Object depend-
ency because it is representative of a field. For
instance, in computer science, the verb ‘to load’
takes as objects, nouns of the conceptual class
software (L’Homme, 1998). This feature also
extends to ‘download’ or ‘upload’, which have
the same verbal root.
Corpora are rich sources of terminological in-
formation that can be mined. A terminology ex-
traction of this kind is similar to a Harris-like
distributional analysis (Harris, 1968) and many
works in the literature have been the subject of
distributional analysis to acquire terminological
or ontological knowledge from textual data (e.g
(Bourigault and Lame, 2002) for law, (Naza-
renko et al., 2001; Weeds et al., 2005) for medi-
cine).
After building conceptual classes (section 2),
we describe an approach to expand concepts by
using a Web search engine to discover new
terms (section 3). In section 4, experiments con-
ducted on real data enable us to validate our ap-
proach.
</bodyText>
<sectionHeader confidence="0.600413" genericHeader="method">
2 Building Conceptual Classes
</sectionHeader>
<subsectionHeader confidence="0.971705">
2.1 Principle
</subsectionHeader>
<bodyText confidence="0.999824555555556">
In our approach, a class can be defined as a
gathering of terms with a common field. In this
paper, we focus on objects of verbs judged to be
semantically close by using a measure. These
objects are thus considered as instances of con-
ceptual classes. The first step in building concep-
tual classes consists in extracting Verb/Object
syntactic relations as explained in the following
section.
</bodyText>
<page confidence="0.988928">
33
</page>
<subsectionHeader confidence="0.534105666666667">
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 33–37,
Beijing, August 2010
2.2 Mining for Verb/Object relations
</subsectionHeader>
<bodyText confidence="0.999702941176471">
Our corpora are in French since our team is
mostly devoted to French-based NLP applica-
tions. However, the following method can be
used for any other language, provided a reliable
dependency parser is available. In our case, we
use the SYGFRAN parser developed by
(Chauché, 1984). As an example, in the French
sentence “Thierry Dusautoir brandissant le dra-
peau tricolore sur la pelouse de Cardiff après la
victoire.” (translation: ‘Thierry Dusautoir bran-
dishing the three colored flag on Cardiff lawn
after the victory’), there is a verb-object syntac-
tic relation: “verb: brandir (to brandish), object:
drapeau (flag)”, which is a good candidate for
retrieval. The second step of the building process
corresponds to the gathering of common objects
related to semantically close verbs.
</bodyText>
<figureCaption confidence="0.7349465">
Figure 1: Common and complementary objects
of the verbs “to consume” and “to eat”
</figureCaption>
<bodyText confidence="0.99712435">
Assumption of Semantic Closeness. The un-
derlying linguistic hypothesis is the following:
Verbs with a significant number of common ob-
jects are semantically close.
To measure closeness, the ASIUM score (Faure
and Nedellec, 1999; Faure, 2000) is used (see
figure 1). This type of work is similar to distri-
butional analysis approaches such as that of
(Bourigault and Lame, 2002).
As explained in the introduction, the measure
considers two verbs to be close if they have a
significant number of common features (ob-
jects).
Let p and q be verbs with their respective
p1,...,pn and q1,...,qm objects. NbOCp(qi) is the
number of occurrences of qi objects from q that
are also objects of p (common objects). NbO(qi)
is the number of occurrences of qi objects of q
verb. The Asium measure is then:
Where logAsium(x) is equal to:
</bodyText>
<listItem confidence="0.9993455">
• for x = 0, logAsium(x) = 0
• else logAsium(x) = log(x) + 1
</listItem>
<bodyText confidence="0.99839575">
Therefore, conceptual classes instances are the
common objects of close verbs, according to the
ASIUM proximity measure.
The following section describes the acquisi-
tion of new terms starting with a list of
terms/concepts obtained with the global process
summarized in this section and detailed in (Bé-
chet et al., 2008).
</bodyText>
<sectionHeader confidence="0.788939" genericHeader="method">
3 Expanding conceptual classes
</sectionHeader>
<subsectionHeader confidence="0.99997">
3.1 Acquisition of candidate terms
</subsectionHeader>
<bodyText confidence="0.99848962962963">
The aim of this approach is to provide new can-
didates for a given concept. It is based on enu-
meration on the Web of terms that are semanti-
cally close. For instance, with a query (string)
“bicycle, car, and”, we can find other vehicles.
We propose to use the Web to acquire new can-
didates. This kind of method uses information
regarding the “popularity” of the web and is in-
dependent of a particular corpus.
Our method of acquisition is quite similar to
that of (Nakov and Hearst, 2008). These authors
propose to query the Web using the Google
search engine to characterize the semantic rela-
tion between a pair of nouns. The Google star
operator among others, is used to that end. (Na-
kov and Hearst, 2008) refer to the study of (Lin
and Pantel, 2001) who used a Web mining ap-
proach to discover inference rules missed by
humans.
To apply our method, we first consider the
common objects of semantically close verbs,
which are instances of reference concepts (e.g.
vehicle). Let N concepts Ci,{1, N} and their respec-
tive instances Ij(Ci). For each concept Ci, we
submit to a search engine the following queries:
”IjA(Ci), IjB(Ci), and” and ”IjA(Ci), IjB(Ci), or” with
jA and jB E {1, ..., NbInstanceCi} and jA ≠ jB.
</bodyText>
<page confidence="0.995497">
34
</page>
<bodyText confidence="0.9997174375">
The search engine returns a set of results from
which we extract new candidate instances of a
concept. For example, if we consider the query:
“bicycle, car, and”, one page returned by a
search engine gives the following text:
Listen here for the Great Commuter Race
(17/11/05) between bicycle, car and bus, as part
of...
Having identified the relevant features in the
result returned (in bold in our example), we add
the term “bus” to the initial concept “vehicle”. In
this way, we obtain new candidates for our con-
cepts. The process can be repeated. In order to
automatically determine which candidates are
relevant, the candidates are filtered as shown in
the following section.
</bodyText>
<subsectionHeader confidence="0.999967">
3.2 Filtering of candidates
</subsectionHeader>
<bodyText confidence="0.999011742857143">
The quality of the extracted terms can be vali-
dated by an expert, or automatically by using the
Web to check if the extracted candidates (see
section 3.1) are relevant. The principle is to con-
sider a relevant term if it is often present with the
terms of the original conceptual class (kernel of
words). Thus, our aim is to validate a term “in
the context”. From that point of view, our
method is close to that of (Turney, 2001), which
queries the Web via the AltaVista search engine
to determine appropriate synonyms for a given
term. Like (Turney, 2001), we consider that in-
formation concerning the number of pages re-
turned by the queries can give an indication of
the relevance of a term.
Thus, we submit to a search engine different
strings (using citation marks). A query consists
of the new candidate and both terms of the con-
cept. Formally, our approach can be defined as
follows. Let N concepts Ci E {1, N}, their respec-
tive instances Ij(Ci) and the new candidates for a
concept Ci, Nik E {1, NbNI(Ci)}. For each Ci, each new
candidate Nik is sent as a query to a Web search
engine. In practice the three terms are separated
either by a comma or the word “or” or “and”&apos;.
For each query, the search engine returns a num-
ber of results (i.e. number of web pages). Then,
the sum of these results is calculated using all
possible combinations of “or”, “and”, or of the
three words (words of the kernel plus candidate
&apos; Note that the commas are automatically removed by the
search engines.
word to enrich it). Below is an example with the
kernel words “car”, “bicycle” and the candidate
“bus” to test (using Yahoo):
</bodyText>
<listItem confidence="0.918834625">
• “car, bicycle, and bus”: 71 pages re-
turned
• “car, bicycle, or bus”: 268 pages re-
turned
• “bicycle, bus, and car”: 208 pages re-
turned
• and so forth
Global result: 71 + 268 + 208...
</listItem>
<bodyText confidence="0.9994753">
The filtering of candidates consists in select-
ing the k first candidates by class (i.e. with the
highest sum), they are added as new instances of
the initial concept. We can reiterate the acquisi-
tion approach by including these new terms. The
acquisition/filtering process can be repeated sev-
eral times.
In the next section, we present experiments
conducted to evaluate the quality of our ap-
proach.
</bodyText>
<sectionHeader confidence="0.999865" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999312">
4.1 Evaluation protocol
</subsectionHeader>
<bodyText confidence="0.9998148">
We used a French corpus from the Yahoo site
(http://fr.news.yahoo.com/) composed of 8,948
news items (16.5 MB) from newspapers. Ex-
periments were performed on 60,000 syntactic
relations (Béchet et al., 2008; Béchet et al.,
2009) to build original conceptual classes. We
manually selected five concepts (see Figure 2).
Instances of these concepts are the common ob-
jects of verbs defining the concept (see section
2.2).
</bodyText>
<figureCaption confidence="0.7590525">
Figure 2: The five selected concepts and their
instances.
</figureCaption>
<page confidence="0.997896">
35
</page>
<bodyText confidence="0.999801153846154">
For our experiments, we use an API of the
search engine Yahoo! to obtain new terms. We
apply the following post-treatments for each new
candidate term. They are initially lemmatized.
Therefore, we only keep the nouns, after apply-
ing a PoS (Part of Speech) tagger, the TreeTag-
ger (Schmid, 1995).
After these post-treatments, we manually
validate the new terms using three experts. We
compute the precision of our approach to each
expert. The average is calculated to define the
quality of the terms. Precision is defined as fol-
lows.
</bodyText>
<equation confidence="0.816321">
Precision =
</equation>
<bodyText confidence="0.82716825">
Number of relevant terms given by our system
Number of terms given by our system
In the next section, we present the evaluation
of our method.
</bodyText>
<subsectionHeader confidence="0.987703">
4.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.9996806">
Table 1 gives the results of the term acquisition
method (i.e. for each acquisition step, we apply
our approach to filter candidate terms). For each
step, the table lists the degree of precision ob-
tained after expertise:
</bodyText>
<listItem confidence="0.974695">
• All candidates. We calculate the preci-
sion before the filtering step.
• Filtered candidates. After applying the
</listItem>
<bodyText confidence="0.788346666666667">
automatic filtering by selecting k terms
per class, we calculate the precision ob-
tained. Note that the automatic filtering
(see section 3.2) reduces the number of
terms proposed, and thus reduces the re-
call2.
</bodyText>
<table confidence="0.999528333333333">
Precision Terms number
(without filter)
Steps # All terms Filtered terms
1 0.69 0.83 29
2 0.69 0.77 47
3 0.56 0.65 103
</table>
<tableCaption confidence="0.946374333333333">
Table 1: Results obtained with k=4 (i.e. auto-
matic selection of the k first ranked terms by the
filtering approach).
</tableCaption>
<bodyText confidence="0.993847347826087">
2 The recall is not calculated because in an unsuper-
vised context it is difficult to estimate.
Finally Table 1 shows the number of terms
generated by the acquisition system.
These results show that a significant number
of terms can be generated (i.e. 103 words). For
example, for the concept ‘feeling’, using the ini-
tial terms given in figure 1, we obtained the fol-
lowing eight French terms (in two steps): “hor-
reur (horror), satisfaction (satisfaction), déprime
(depression), faiblesse (weakness), tristesse
(sadness), désenchantement (disenchantment),
folie (madness), fatalisme (fatalism)”.
This approach is appropriate to produce new
relevant terms to enrich conceptual classes, in
particular when we select the first terms (k = 4)
returned by the filtering system. In a future
work, we plan to test other values of the auto-
matic filtering. The precision obtained in the first
two steps was high (i.e. 0.69 to 0.83). The third
step returned lower scores; noise was introduced
because we were too “far” from the initial kernel
words.
</bodyText>
<sectionHeader confidence="0.991032" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999523333333333">
This paper describes an approach for conceptual
enrichment classes based on the Web. We apply
the “enumeration” principle to find new terms
using Web search engines. This approach has the
advantage of being less dependent on the corpus.
Note that as the use of the Web requires valida-
tion of candidates, we propose an automatic fil-
tering method to select relevant terms to add to
the concept. In a future work, we plan to use
other statistical web measures (e.g. Mutual In-
formation, Dice measure, and so forth) to auto-
matically validate terms.
</bodyText>
<sectionHeader confidence="0.998269" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.976700928571429">
Béchet, N., M. Roche, and J. Chauch´e. 2008. How
the ExpLSA approach impacts the document clas-
sification tasks. In Proceedings of the Interna-
tional Conference on Digital Information Man-
agement, ICDIM’08, pages 241–246, University of
East London, London, United Kingdom.
Béchet, N., M. Roche, and J. Chauché. 2009. To-
wards the selection of induced syntactic relations.
In European Conference on Information Retrieval
(ECIR), Poster, pages 786–790.
Bourigault, D. and G. Lame. 2002. Analyse distribu-
tionnelle et structuration de terminologie. Applica-
tion à la construction d’une ontologie documen-
taire du droit. In TAL, pages 43–51.
</reference>
<page confidence="0.976135">
36
</page>
<reference confidence="0.999286545454546">
Chauché, J. 1984. Un outil multidimensionnel de
l’analyse du discours. In Proceedings of COLING,
Standford University, California, pages 11–15.
Desrosiers-Sabbath, R. 1984. Comment enseigner les
concepts. Presses de l’Université du Québec.
Faure, D. and C. Nedellec. 1999. Knowledge acquisi-
tion of predicate argument structures from techni-
cal texts using machine learning: The system
ASIUM. In Proceedings of the 11th European
Workshop, Knowledge Acquisition, Modelling and
Management, number 1937 in LNAI, pages 329–
334.
Faure, D. 2000. Conception de méthode d’appren-
tissage symbolique et automatique pour l’acquisi-
tion de cadres de sous-catégorisation de verbes et
de connaissances sémantiques à partir de textes :
le système ASIUM. Ph.D. thesis, Université Paris-
Sud, 20 Décembre.
Harris, Z. 1968. Mathematical Structures of Lan-
guage. John Wiley &amp; Sons, New-York.
L’Homme, M. C. 1998. Le statut du verbe en langue
de spécialité et sa description lexicographique. In
Cahiers de Lexicologie 73, pages 61–84.
Lin, Dekang and Patrick Pantel. 2001. Discovery of
inference rules for question answering. Natural
Language Engineering, 7:343–360.
Nakov, Preslav and Marti A. Hearst. 2008. Solving
relational similarity problems using the web as a
corpus. In ACL, pages 452–460.
Nazarenko, A., P. Zweigenbaum, B. Habert, and J.
Bouaud. 2001. Corpus-based extension of a termi-
nological semantic lexicon. In Recent Advances in
Computational Terminology, pages 327–351.
Schmid, H. 1995. Improvements in part-of-speech
tagging with an application to german. In Proceed
ngs of the ACL SIGDAT-Workshop, Dublin.
Turney, P.D. 2001. Mining the Web for synonyms:
PMI–IR versus LSA on TOEFL. In Proceedings of
ECML’01, Lecture Notes in Computer Science,
pages 491–502.
Weeds, J., J. Dowdall, G. Schneider, B. Keller, and
D. Weir. 2005. Weir using distributional similarity
to organise biomedical terminology. In Proceed-
ings of Terminology, volume 11, pages 107–141.
</reference>
<page confidence="0.99961">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.854967">
<title confidence="0.99997">How to Expand Dictionaries with Web-Mining Techniques</title>
<author confidence="0.999435">Nicolas Béchet Mathieu Roche</author>
<affiliation confidence="0.9481">LIRMM, UMR 5506, CNRS, LIRMM, UMR 5506, CNRS,</affiliation>
<address confidence="0.924464">Univ. Montpellier 2 Univ. Montpellier 2</address>
<email confidence="0.966402">bechet@lirmm.frmroche@lirmm.fr</email>
<abstract confidence="0.998705142857143">This paper presents an approach to enrich conceptual classes based on the Web. To test our approach, we first build conceptual classes using syntactic and semantic information provided by a corpus. The concepts can be the input of a dictionary. Our web-mining approach deals with a cognitive process which simulates human reasoning based on the enumeration principle. The experiments reveal the interest of our approach by adding new relevant terms to existing conceptual classes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Béchet</author>
<author>M Roche</author>
<author>J Chauch´e</author>
</authors>
<title>How the ExpLSA approach impacts the document classification tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Digital Information Management, ICDIM’08,</booktitle>
<pages>241--246</pages>
<institution>University of East London,</institution>
<location>London, United Kingdom.</location>
<marker>Béchet, Roche, Chauch´e, 2008</marker>
<rawString>Béchet, N., M. Roche, and J. Chauch´e. 2008. How the ExpLSA approach impacts the document classification tasks. In Proceedings of the International Conference on Digital Information Management, ICDIM’08, pages 241–246, University of East London, London, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Béchet</author>
<author>M Roche</author>
<author>J Chauché</author>
</authors>
<title>Towards the selection of induced syntactic relations.</title>
<date>2009</date>
<booktitle>In European Conference on Information Retrieval (ECIR), Poster,</booktitle>
<pages>786--790</pages>
<contexts>
<context position="9827" citStr="Béchet et al., 2009" startWordPosition="1637" endWordPosition="1640">cting the k first candidates by class (i.e. with the highest sum), they are added as new instances of the initial concept. We can reiterate the acquisition approach by including these new terms. The acquisition/filtering process can be repeated several times. In the next section, we present experiments conducted to evaluate the quality of our approach. 4 Experiments 4.1 Evaluation protocol We used a French corpus from the Yahoo site (http://fr.news.yahoo.com/) composed of 8,948 news items (16.5 MB) from newspapers. Experiments were performed on 60,000 syntactic relations (Béchet et al., 2008; Béchet et al., 2009) to build original conceptual classes. We manually selected five concepts (see Figure 2). Instances of these concepts are the common objects of verbs defining the concept (see section 2.2). Figure 2: The five selected concepts and their instances. 35 For our experiments, we use an API of the search engine Yahoo! to obtain new terms. We apply the following post-treatments for each new candidate term. They are initially lemmatized. Therefore, we only keep the nouns, after applying a PoS (Part of Speech) tagger, the TreeTagger (Schmid, 1995). After these post-treatments, we manually validate the </context>
</contexts>
<marker>Béchet, Roche, Chauché, 2009</marker>
<rawString>Béchet, N., M. Roche, and J. Chauché. 2009. Towards the selection of induced syntactic relations. In European Conference on Information Retrieval (ECIR), Poster, pages 786–790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bourigault</author>
<author>G Lame</author>
</authors>
<title>Analyse distributionnelle et structuration de terminologie. Application à la construction d’une ontologie documentaire du droit.</title>
<date>2002</date>
<booktitle>In TAL,</booktitle>
<pages>43--51</pages>
<contexts>
<context position="2399" citStr="Bourigault and Lame, 2002" startWordPosition="363" endWordPosition="366">pendency because it is representative of a field. For instance, in computer science, the verb ‘to load’ takes as objects, nouns of the conceptual class software (L’Homme, 1998). This feature also extends to ‘download’ or ‘upload’, which have the same verbal root. Corpora are rich sources of terminological information that can be mined. A terminology extraction of this kind is similar to a Harris-like distributional analysis (Harris, 1968) and many works in the literature have been the subject of distributional analysis to acquire terminological or ontological knowledge from textual data (e.g (Bourigault and Lame, 2002) for law, (Nazarenko et al., 2001; Weeds et al., 2005) for medicine). After building conceptual classes (section 2), we describe an approach to expand concepts by using a Web search engine to discover new terms (section 3). In section 4, experiments conducted on real data enable us to validate our approach. 2 Building Conceptual Classes 2.1 Principle In our approach, a class can be defined as a gathering of terms with a common field. In this paper, we focus on objects of verbs judged to be semantically close by using a measure. These objects are thus considered as instances of conceptual class</context>
<context position="4542" citStr="Bourigault and Lame, 2002" startWordPosition="711" endWordPosition="714">)”, which is a good candidate for retrieval. The second step of the building process corresponds to the gathering of common objects related to semantically close verbs. Figure 1: Common and complementary objects of the verbs “to consume” and “to eat” Assumption of Semantic Closeness. The underlying linguistic hypothesis is the following: Verbs with a significant number of common objects are semantically close. To measure closeness, the ASIUM score (Faure and Nedellec, 1999; Faure, 2000) is used (see figure 1). This type of work is similar to distributional analysis approaches such as that of (Bourigault and Lame, 2002). As explained in the introduction, the measure considers two verbs to be close if they have a significant number of common features (objects). Let p and q be verbs with their respective p1,...,pn and q1,...,qm objects. NbOCp(qi) is the number of occurrences of qi objects from q that are also objects of p (common objects). NbO(qi) is the number of occurrences of qi objects of q verb. The Asium measure is then: Where logAsium(x) is equal to: • for x = 0, logAsium(x) = 0 • else logAsium(x) = log(x) + 1 Therefore, conceptual classes instances are the common objects of close verbs, according to th</context>
</contexts>
<marker>Bourigault, Lame, 2002</marker>
<rawString>Bourigault, D. and G. Lame. 2002. Analyse distributionnelle et structuration de terminologie. Application à la construction d’une ontologie documentaire du droit. In TAL, pages 43–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chauché</author>
</authors>
<title>Un outil multidimensionnel de l’analyse du discours.</title>
<date>1984</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>11--15</pages>
<institution>Standford University,</institution>
<location>California,</location>
<contexts>
<context position="3579" citStr="Chauché, 1984" startWordPosition="562" endWordPosition="563">d as instances of conceptual classes. The first step in building conceptual classes consists in extracting Verb/Object syntactic relations as explained in the following section. 33 Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 33–37, Beijing, August 2010 2.2 Mining for Verb/Object relations Our corpora are in French since our team is mostly devoted to French-based NLP applications. However, the following method can be used for any other language, provided a reliable dependency parser is available. In our case, we use the SYGFRAN parser developed by (Chauché, 1984). As an example, in the French sentence “Thierry Dusautoir brandissant le drapeau tricolore sur la pelouse de Cardiff après la victoire.” (translation: ‘Thierry Dusautoir brandishing the three colored flag on Cardiff lawn after the victory’), there is a verb-object syntactic relation: “verb: brandir (to brandish), object: drapeau (flag)”, which is a good candidate for retrieval. The second step of the building process corresponds to the gathering of common objects related to semantically close verbs. Figure 1: Common and complementary objects of the verbs “to consume” and “to eat” Assumption o</context>
</contexts>
<marker>Chauché, 1984</marker>
<rawString>Chauché, J. 1984. Un outil multidimensionnel de l’analyse du discours. In Proceedings of COLING, Standford University, California, pages 11–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Desrosiers-Sabbath</author>
</authors>
<title>Comment enseigner les concepts. Presses de l’Université du Québec.</title>
<date>1984</date>
<marker>Desrosiers-Sabbath, 1984</marker>
<rawString>Desrosiers-Sabbath, R. 1984. Comment enseigner les concepts. Presses de l’Université du Québec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Faure</author>
<author>C Nedellec</author>
</authors>
<title>Knowledge acquisition of predicate argument structures from technical texts using machine learning: The system ASIUM.</title>
<date>1999</date>
<booktitle>In Proceedings of the 11th European Workshop, Knowledge Acquisition, Modelling and Management, number 1937 in LNAI,</booktitle>
<pages>329--334</pages>
<contexts>
<context position="4393" citStr="Faure and Nedellec, 1999" startWordPosition="685" endWordPosition="688">ee colored flag on Cardiff lawn after the victory’), there is a verb-object syntactic relation: “verb: brandir (to brandish), object: drapeau (flag)”, which is a good candidate for retrieval. The second step of the building process corresponds to the gathering of common objects related to semantically close verbs. Figure 1: Common and complementary objects of the verbs “to consume” and “to eat” Assumption of Semantic Closeness. The underlying linguistic hypothesis is the following: Verbs with a significant number of common objects are semantically close. To measure closeness, the ASIUM score (Faure and Nedellec, 1999; Faure, 2000) is used (see figure 1). This type of work is similar to distributional analysis approaches such as that of (Bourigault and Lame, 2002). As explained in the introduction, the measure considers two verbs to be close if they have a significant number of common features (objects). Let p and q be verbs with their respective p1,...,pn and q1,...,qm objects. NbOCp(qi) is the number of occurrences of qi objects from q that are also objects of p (common objects). NbO(qi) is the number of occurrences of qi objects of q verb. The Asium measure is then: Where logAsium(x) is equal to: • for </context>
</contexts>
<marker>Faure, Nedellec, 1999</marker>
<rawString>Faure, D. and C. Nedellec. 1999. Knowledge acquisition of predicate argument structures from technical texts using machine learning: The system ASIUM. In Proceedings of the 11th European Workshop, Knowledge Acquisition, Modelling and Management, number 1937 in LNAI, pages 329– 334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Faure</author>
</authors>
<title>Conception de méthode d’apprentissage symbolique et automatique pour l’acquisition de cadres de sous-catégorisation de verbes et de connaissances sémantiques à partir de textes : le système ASIUM.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Université ParisSud, 20 Décembre.</institution>
<contexts>
<context position="4407" citStr="Faure, 2000" startWordPosition="689" endWordPosition="690"> lawn after the victory’), there is a verb-object syntactic relation: “verb: brandir (to brandish), object: drapeau (flag)”, which is a good candidate for retrieval. The second step of the building process corresponds to the gathering of common objects related to semantically close verbs. Figure 1: Common and complementary objects of the verbs “to consume” and “to eat” Assumption of Semantic Closeness. The underlying linguistic hypothesis is the following: Verbs with a significant number of common objects are semantically close. To measure closeness, the ASIUM score (Faure and Nedellec, 1999; Faure, 2000) is used (see figure 1). This type of work is similar to distributional analysis approaches such as that of (Bourigault and Lame, 2002). As explained in the introduction, the measure considers two verbs to be close if they have a significant number of common features (objects). Let p and q be verbs with their respective p1,...,pn and q1,...,qm objects. NbOCp(qi) is the number of occurrences of qi objects from q that are also objects of p (common objects). NbO(qi) is the number of occurrences of qi objects of q verb. The Asium measure is then: Where logAsium(x) is equal to: • for x = 0, logAsiu</context>
</contexts>
<marker>Faure, 2000</marker>
<rawString>Faure, D. 2000. Conception de méthode d’apprentissage symbolique et automatique pour l’acquisition de cadres de sous-catégorisation de verbes et de connaissances sémantiques à partir de textes : le système ASIUM. Ph.D. thesis, Université ParisSud, 20 Décembre.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Mathematical Structures of Language.</title>
<date>1968</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New-York.</location>
<contexts>
<context position="2215" citStr="Harris, 1968" startWordPosition="338" endWordPosition="339">s such as Verb/Subject, Noun/Noun Phrase Complements, Verb/Object, Verb/Complements, and sometimes Sentence Head/Complements. In this paper, we focus on the Verb/Object dependency because it is representative of a field. For instance, in computer science, the verb ‘to load’ takes as objects, nouns of the conceptual class software (L’Homme, 1998). This feature also extends to ‘download’ or ‘upload’, which have the same verbal root. Corpora are rich sources of terminological information that can be mined. A terminology extraction of this kind is similar to a Harris-like distributional analysis (Harris, 1968) and many works in the literature have been the subject of distributional analysis to acquire terminological or ontological knowledge from textual data (e.g (Bourigault and Lame, 2002) for law, (Nazarenko et al., 2001; Weeds et al., 2005) for medicine). After building conceptual classes (section 2), we describe an approach to expand concepts by using a Web search engine to discover new terms (section 3). In section 4, experiments conducted on real data enable us to validate our approach. 2 Building Conceptual Classes 2.1 Principle In our approach, a class can be defined as a gathering of terms</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Harris, Z. 1968. Mathematical Structures of Language. John Wiley &amp; Sons, New-York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C L’Homme</author>
</authors>
<title>Le statut du verbe en langue de spécialité et sa description lexicographique.</title>
<date>1998</date>
<booktitle>In Cahiers de Lexicologie 73,</booktitle>
<pages>61--84</pages>
<marker>L’Homme, 1998</marker>
<rawString>L’Homme, M. C. 1998. Le statut du verbe en langue de spécialité et sa description lexicographique. In Cahiers de Lexicologie 73, pages 61–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<pages>7--343</pages>
<contexts>
<context position="6185" citStr="Lin and Pantel, 2001" startWordPosition="1001" endWordPosition="1004">re semantically close. For instance, with a query (string) “bicycle, car, and”, we can find other vehicles. We propose to use the Web to acquire new candidates. This kind of method uses information regarding the “popularity” of the web and is independent of a particular corpus. Our method of acquisition is quite similar to that of (Nakov and Hearst, 2008). These authors propose to query the Web using the Google search engine to characterize the semantic relation between a pair of nouns. The Google star operator among others, is used to that end. (Nakov and Hearst, 2008) refer to the study of (Lin and Pantel, 2001) who used a Web mining approach to discover inference rules missed by humans. To apply our method, we first consider the common objects of semantically close verbs, which are instances of reference concepts (e.g. vehicle). Let N concepts Ci,{1, N} and their respective instances Ij(Ci). For each concept Ci, we submit to a search engine the following queries: ”IjA(Ci), IjB(Ci), and” and ”IjA(Ci), IjB(Ci), or” with jA and jB E {1, ..., NbInstanceCi} and jA ≠ jB. 34 The search engine returns a set of results from which we extract new candidate instances of a concept. For example, if we consider th</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Lin, Dekang and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7:343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti A Hearst</author>
</authors>
<title>Solving relational similarity problems using the web as a corpus. In</title>
<date>2008</date>
<booktitle>ACL,</booktitle>
<pages>452--460</pages>
<contexts>
<context position="5921" citStr="Nakov and Hearst, 2008" startWordPosition="953" endWordPosition="956">ess summarized in this section and detailed in (Bé- chet et al., 2008). 3 Expanding conceptual classes 3.1 Acquisition of candidate terms The aim of this approach is to provide new candidates for a given concept. It is based on enumeration on the Web of terms that are semantically close. For instance, with a query (string) “bicycle, car, and”, we can find other vehicles. We propose to use the Web to acquire new candidates. This kind of method uses information regarding the “popularity” of the web and is independent of a particular corpus. Our method of acquisition is quite similar to that of (Nakov and Hearst, 2008). These authors propose to query the Web using the Google search engine to characterize the semantic relation between a pair of nouns. The Google star operator among others, is used to that end. (Nakov and Hearst, 2008) refer to the study of (Lin and Pantel, 2001) who used a Web mining approach to discover inference rules missed by humans. To apply our method, we first consider the common objects of semantically close verbs, which are instances of reference concepts (e.g. vehicle). Let N concepts Ci,{1, N} and their respective instances Ij(Ci). For each concept Ci, we submit to a search engine</context>
</contexts>
<marker>Nakov, Hearst, 2008</marker>
<rawString>Nakov, Preslav and Marti A. Hearst. 2008. Solving relational similarity problems using the web as a corpus. In ACL, pages 452–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nazarenko</author>
<author>P Zweigenbaum</author>
<author>B Habert</author>
<author>J Bouaud</author>
</authors>
<title>Corpus-based extension of a terminological semantic lexicon.</title>
<date>2001</date>
<booktitle>In Recent Advances in Computational Terminology,</booktitle>
<pages>327--351</pages>
<contexts>
<context position="2432" citStr="Nazarenko et al., 2001" startWordPosition="369" endWordPosition="373"> of a field. For instance, in computer science, the verb ‘to load’ takes as objects, nouns of the conceptual class software (L’Homme, 1998). This feature also extends to ‘download’ or ‘upload’, which have the same verbal root. Corpora are rich sources of terminological information that can be mined. A terminology extraction of this kind is similar to a Harris-like distributional analysis (Harris, 1968) and many works in the literature have been the subject of distributional analysis to acquire terminological or ontological knowledge from textual data (e.g (Bourigault and Lame, 2002) for law, (Nazarenko et al., 2001; Weeds et al., 2005) for medicine). After building conceptual classes (section 2), we describe an approach to expand concepts by using a Web search engine to discover new terms (section 3). In section 4, experiments conducted on real data enable us to validate our approach. 2 Building Conceptual Classes 2.1 Principle In our approach, a class can be defined as a gathering of terms with a common field. In this paper, we focus on objects of verbs judged to be semantically close by using a measure. These objects are thus considered as instances of conceptual classes. The first step in building co</context>
</contexts>
<marker>Nazarenko, Zweigenbaum, Habert, Bouaud, 2001</marker>
<rawString>Nazarenko, A., P. Zweigenbaum, B. Habert, and J. Bouaud. 2001. Corpus-based extension of a terminological semantic lexicon. In Recent Advances in Computational Terminology, pages 327–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Improvements in part-of-speech tagging with an application to german.</title>
<date>1995</date>
<booktitle>In Proceed ngs of the ACL SIGDAT-Workshop,</booktitle>
<location>Dublin.</location>
<contexts>
<context position="10371" citStr="Schmid, 1995" startWordPosition="1729" endWordPosition="1730">60,000 syntactic relations (Béchet et al., 2008; Béchet et al., 2009) to build original conceptual classes. We manually selected five concepts (see Figure 2). Instances of these concepts are the common objects of verbs defining the concept (see section 2.2). Figure 2: The five selected concepts and their instances. 35 For our experiments, we use an API of the search engine Yahoo! to obtain new terms. We apply the following post-treatments for each new candidate term. They are initially lemmatized. Therefore, we only keep the nouns, after applying a PoS (Part of Speech) tagger, the TreeTagger (Schmid, 1995). After these post-treatments, we manually validate the new terms using three experts. We compute the precision of our approach to each expert. The average is calculated to define the quality of the terms. Precision is defined as follows. Precision = Number of relevant terms given by our system Number of terms given by our system In the next section, we present the evaluation of our method. 4.2 Experimental results Table 1 gives the results of the term acquisition method (i.e. for each acquisition step, we apply our approach to filter candidate terms). For each step, the table lists the degree</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Schmid, H. 1995. Improvements in part-of-speech tagging with an application to german. In Proceed ngs of the ACL SIGDAT-Workshop, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the Web for synonyms: PMI–IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of ECML’01, Lecture Notes in Computer Science,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="7789" citStr="Turney, 2001" startWordPosition="1281" endWordPosition="1282">e process can be repeated. In order to automatically determine which candidates are relevant, the candidates are filtered as shown in the following section. 3.2 Filtering of candidates The quality of the extracted terms can be validated by an expert, or automatically by using the Web to check if the extracted candidates (see section 3.1) are relevant. The principle is to consider a relevant term if it is often present with the terms of the original conceptual class (kernel of words). Thus, our aim is to validate a term “in the context”. From that point of view, our method is close to that of (Turney, 2001), which queries the Web via the AltaVista search engine to determine appropriate synonyms for a given term. Like (Turney, 2001), we consider that information concerning the number of pages returned by the queries can give an indication of the relevance of a term. Thus, we submit to a search engine different strings (using citation marks). A query consists of the new candidate and both terms of the concept. Formally, our approach can be defined as follows. Let N concepts Ci E {1, N}, their respective instances Ij(Ci) and the new candidates for a concept Ci, Nik E {1, NbNI(Ci)}. For each Ci, eac</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Turney, P.D. 2001. Mining the Web for synonyms: PMI–IR versus LSA on TOEFL. In Proceedings of ECML’01, Lecture Notes in Computer Science, pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>J Dowdall</author>
<author>G Schneider</author>
<author>B Keller</author>
<author>D Weir</author>
</authors>
<title>Weir using distributional similarity to organise biomedical terminology.</title>
<date>2005</date>
<booktitle>In Proceedings of Terminology,</booktitle>
<volume>11</volume>
<pages>107--141</pages>
<contexts>
<context position="2453" citStr="Weeds et al., 2005" startWordPosition="374" endWordPosition="377">e, in computer science, the verb ‘to load’ takes as objects, nouns of the conceptual class software (L’Homme, 1998). This feature also extends to ‘download’ or ‘upload’, which have the same verbal root. Corpora are rich sources of terminological information that can be mined. A terminology extraction of this kind is similar to a Harris-like distributional analysis (Harris, 1968) and many works in the literature have been the subject of distributional analysis to acquire terminological or ontological knowledge from textual data (e.g (Bourigault and Lame, 2002) for law, (Nazarenko et al., 2001; Weeds et al., 2005) for medicine). After building conceptual classes (section 2), we describe an approach to expand concepts by using a Web search engine to discover new terms (section 3). In section 4, experiments conducted on real data enable us to validate our approach. 2 Building Conceptual Classes 2.1 Principle In our approach, a class can be defined as a gathering of terms with a common field. In this paper, we focus on objects of verbs judged to be semantically close by using a measure. These objects are thus considered as instances of conceptual classes. The first step in building conceptual classes cons</context>
</contexts>
<marker>Weeds, Dowdall, Schneider, Keller, Weir, 2005</marker>
<rawString>Weeds, J., J. Dowdall, G. Schneider, B. Keller, and D. Weir. 2005. Weir using distributional similarity to organise biomedical terminology. In Proceedings of Terminology, volume 11, pages 107–141.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>