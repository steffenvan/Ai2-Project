<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020705">
<title confidence="0.975177">
The Universit¨at Karlsruhe Translation System for the EACL-WMT 2009
</title>
<author confidence="0.998009">
Jan Niehues, Teresa Herrmann, Muntsin Kolss and Alex Waibel
</author>
<affiliation confidence="0.8443295">
Universit¨at Karlsruhe (TH)
Karlsruhe, Germany
</affiliation>
<email confidence="0.992593">
{jniehues,therrman,kolss,waibel}@ira.uka.de
</email>
<sectionHeader confidence="0.997324" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997568461538461">
In this paper we describe the statistical
machine translation system of the Univer-
sit¨at Karlsruhe developed for the transla-
tion task of the Fourth Workshop on Sta-
tistical Machine Translation. The state-of-
the-art phrase-based SMT system is aug-
mented with alternative word reordering
and alignment mechanisms as well as op-
tional phrase table modifications. We par-
ticipate in the constrained condition of
German-English and English-German as
well as in the constrained condition of
French-English and English-French.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990004">
This paper describes the statistical MT system
used for our participation in the WMT’09 Shared
Translation Task and the particular language-pair-
dependent variations of the system. We use stan-
dard alignment and training tools and a phrase-
based SMT decoder for creating state-of-the-art
MT systems for our contribution in the transla-
tion directions English-German, German-English,
English-French and French-English.
Depending on the language pair, the baseline
system is augmented with part-of-speech (POS)-
based short-range and long-range word reordering
models, discriminative word alignment (DWA)
and several modifications of the phrase table. Ex-
periments with different system variants were con-
ducted including some of those additional system
components. Significantly better translation re-
sults could be achieved compared to the baseline
results.
An overview of the system will follow in Sec-
tion 2, which describes the baseline architecture,
followed by descriptions of the additional system
components. Translation results for the different
languages and system variants are presented in
Section 5.
</bodyText>
<sectionHeader confidence="0.989262" genericHeader="method">
2 Baseline System
</sectionHeader>
<bodyText confidence="0.9997064">
The core of our system is the STTK decoder (Vo-
gel, 2003), a phrase-based SMT decoder with a
local reordering window of 2 words. The de-
coder generates a translation for the input text
or word lattice by searching translation model
and language model for the hypothesis that max-
imizes phrase translation probabilities and target
language probabilities. The translation model, i.e.
the SMT phrase table is created during the training
phase by a modified version of the Moses Toolkit
(Koehn et al., 2007) applying GIZA++ for word
alignment. Language models are built using the
SRILM Toolkit. The POS-tags for the reorder-
ing models were generated with the TreeTagger
(Schmid, 1994) for all languages.
</bodyText>
<subsectionHeader confidence="0.996864">
2.1 Training, Development and Test Data
</subsectionHeader>
<bodyText confidence="0.999979416666667">
We submitted translations for the English-
German, German-English, English-French and
French-English tasks. All systems were trained
on the Europarl and News Commentary corpora
using the Moses Toolkit and apply 4-gram lan-
guage models created from the respective mono-
lingual News corpora. All feature weights are au-
tomatically determined and optimized with respect
to BLEU via MERT (Venugopal et al., 2005).
For development and testing we used data pro-
vided by the WMT’09, news-dev2009a and news-
dev2009b, consisting of 1026 sentences each.
</bodyText>
<sectionHeader confidence="0.993688" genericHeader="method">
3 Word Reordering Model
</sectionHeader>
<bodyText confidence="0.8657372">
One part of our system that differs from the base-
line system is the reordering model. To account
for the different word orders in the languages, we
used the POS-based reordering model presented in
Rottmann and Vogel (2007). This model learns
rules from a parallel text to reorder the source side.
The aim is to generate a reordered source side that
can be translated in a more monotone way.
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 80–84,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.997">
80
</page>
<bodyText confidence="0.999363578947368">
In this framework, first, reordering rules are
extracted from an aligned parallel corpus and
POS information is added to the source side.
These rules are of the form VVIMP VMFIN PPER
—* PPER VMFIN VVIMP and describe how the
source side has to be reordered to match the tar-
get side. Then the rules are scored according to
their relative frequencies.
In a preprocessing step to the actual decoding
different reorderings of the source sentences are
encoded in a word lattice. Therefore, for all re-
ordering rules that can be applied to a sentence the
resulting reorderings are added to the lattice if the
score is better than a given threshold. The decod-
ing is then performed on the resulting word lattice.
This approach does model the reordering well
if only short-range reorderings occur. But espe-
cially when translating from and to German, there
are also long-range reorderings that require the
verb to be shifted nearly across the whole sen-
tence. During this shift of the verb, the rest of
the sentence remains mainly unchanged. It does
not matter which words are in between, since they
are moved as a whole. Furthermore, rules in-
cluding an explicit sequence of POS-tags spanning
the whole sentence would be too specific. A lot
more rules would be needed to cover long-range
reorderings with each rule being applicable only
very sparsely. Therefore, we model long-range re-
ordering by generalizing over the unaffected se-
quences and introduce rules with gaps. (For more
details see Niehues and Kolss (2009)). These are
learned in a way similar to the other type of re-
ordering rules described above, but contain a gap
representing one or several arbitrary words. It is,
for example, possible to have the following rule
VAFIN * VVPP --+ VAFIN VVPP *, which puts
both parts of the German verb next to each other.
</bodyText>
<sectionHeader confidence="0.998921" genericHeader="method">
4 Translation Model
</sectionHeader>
<bodyText confidence="0.999873">
The translation models of all systems we submit-
ted differ in some parts from the baseline system.
The main changes done will be described in this
section.
</bodyText>
<subsectionHeader confidence="0.996251">
4.1 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999968421052632">
The baseline method for creating the word align-
ment is to create the GIZA++ alignments in both
directions and then to combine both alignments
using a heuristic, e.g. grow-diag-final-and heuris-
tic, as provided by the Moses Toolkit. In some
of the submitted systems we used a discrimina-
tive word alignment model (DWA) to generate
the alignments as described in Niehues and Vogel
(2008) instead. This model is trained on a small
amount of hand-aligned data and uses the lexical
probability as well as the fertilities generated by
the GIZA++ Toolkit and POS information. We
used all local features, the GIZA and indicator fer-
tility features as well as first order features for 6
directions. The model was trained in three steps,
first using the maximum likelihood optimization
and afterwards it was optimized towards the align-
ment error rate. For more details see Niehues and
Vogel (2008).
</bodyText>
<subsectionHeader confidence="0.994871">
4.2 Phrase Table Smoothing
</subsectionHeader>
<bodyText confidence="0.999909">
The relative frequencies of the phrase pairs are a
very important feature of the translation model,
but they often overestimate rare phrase pairs.
Therefore, the raw relative frequency estimates
found in the phrase translation tables are smoothed
by applying modified Kneser-Ney discounting as
described in Foster et al. (2006).
</bodyText>
<subsectionHeader confidence="0.998367">
4.3 Lattice Phrase Extraction
</subsectionHeader>
<bodyText confidence="0.999847851851852">
For the test sentences the POS-based reordering
allows us to change the word order in the source
sentence, so that the sentence can be translated
more easily. But this approach does not reorder
the training sentences. This may cause problems
for phrase extraction, especially for long-range re-
orderings. For example, if the English verb is
aligned to both parts of the German verb, this
phrase can not be extracted, since it is not contin-
uous on the German side. In the case of German
as source language, the phrase could be extracted
if we also reorder the training corpus.
Therefore, we build lattices that encode the
different reorderings for every training sentence.
Then we can not only extract phrase pairs from the
monotone source path, but also from the reordered
paths. So it would be possible to extract the ex-
ample mentioned before, if both parts of the verb
were put together by a reordering rule. To limit
the number of extracted phrase pairs, we extract
a source phrase only once per sentence even if it
may be found on different paths. Furthermore, we
do not use the weights in the lattice.
If we use the same rules as for the test sets,
the lattice would be so big that the number of ex-
tracted phrase pairs would be still too high. As
mentioned before, the word reordering is mainly
</bodyText>
<page confidence="0.993487">
81
</page>
<bodyText confidence="0.9998864">
a problem at the phrase extraction stage if one
word is aligned to two words which are far away
from each other in the sentence. Therefore, the
short-range reordering rules do not help much in
this case. So, only the long-range reordering rules
were used to generate the lattice for the training
corpus. This already leads to an increase of the
number of source phrases in the filtered phrase ta-
ble from 724K to 971K. The number of phrase
pairs grows from 5.1M to 6.7M.
</bodyText>
<subsectionHeader confidence="0.999231">
4.4 Phrase Table Adaption
</subsectionHeader>
<bodyText confidence="0.999977210526316">
For most of the different tasks there was a huge
amount of parallel out-of-domain training data
available, but only a much smaller amount of in-
domain training data. Therefore, we tried to adapt
our system to the in-domain data. We want to
make use of the big out-of-domain data, but do
not want to lose the information encoded in the in-
domain data.
To achieve this, we built an additional phrase
table trained only on the in-domain data. Since
the word alignment does not depend heavily on the
domain we used the same word alignment. Then
we combined both phrase tables in the following
way. A phrase pair with features 0 from the first
phrase table is added to the combined one with
features &lt; 0,1 &gt;, where 1 is a vector of ones with
length equal to the number of features in the other
phrase table. The phrase pairs of the other phrase
table were added with the features &lt; 1, 0 &gt;.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9998745">
We submitted system translations for the English-
German, German-English, English-French and
French-English task. Their performance is mea-
sured applying the BLEU metric. All BLEU
scores are computed on the lower-cased transla-
tions.
</bodyText>
<subsectionHeader confidence="0.995435">
5.1 English-German
</subsectionHeader>
<bodyText confidence="0.99993336">
The system translating from English to German
was trained on the data described in Section 2.1.
The first system already uses the POS-based re-
ordering model for short-range reorderings. The
results of the different systems are shown in Ta-
ble 1.
We could improve the translation quality on the
test set by using the smoothed relative frequen-
cies in the phrase table as described before and
by adapting the phrase table. Then we used the
discriminative word alignment to generate a new
word alignment. For the training of the model
we used 500 hand-aligned sentences from the Eu-
roparl corpus. By training a translation model
based on this word alignment we could improve
the translation quality further. At last we added
the model for long-range reorderings, which per-
forms best on the test set.
The improvement achieved by smoothing is sig-
nificant at a level of 5%, the remaining changes are
not significant on their own. In all language pairs,
the problem occurs that some features do not lead
to an improvement on the development set, but on
the test set. One reason for this may be that the
development set is quite small.
</bodyText>
<tableCaption confidence="0.998678">
Table 1: Translation results for English-German
</tableCaption>
<table confidence="0.999890714285714">
(BLEU Score)
System Dev Test
Short-range 13.96 14.99
+ Smoothing 14.36 15.38
+ Adaptation 13.96 15.44
+ Discrim. WA 14.45 15.61
+ Long-range reordering 14.58 15.70
</table>
<subsectionHeader confidence="0.997506">
5.2 German-English
</subsectionHeader>
<bodyText confidence="0.999932875">
The German-English system was trained on the
same data as the English-German except that we
perform compound splitting as an additional pre-
processing step. The compound splitting was
done with the frequency-based method described
in Koehn et al. (2003). For this language di-
rection, the initial system already uses phrase ta-
ble smoothing, adaptation and discriminative word
alignment, in addition to the techniques of the
English-German baseline system. The results are
shown in Table 2.
For this language pair, we could improve the
translation quality, first, by adding the long-range
reordering model. Further improvements could be
achieved by using lattice phrase extraction as de-
scribed before.
</bodyText>
<subsectionHeader confidence="0.999206">
5.3 English-French
</subsectionHeader>
<bodyText confidence="0.999263">
For creating the English-French translations, first,
the baseline system as described in Section 2
was used. This baseline was then augmented
with phrase table smoothing, short-range word re-
ordering and phrase table adaptation as described
above. In addition, the adapted phrase table was
</bodyText>
<page confidence="0.999299">
82
</page>
<tableCaption confidence="0.937766">
Table 2: Translation results for German-English
</tableCaption>
<table confidence="0.9426254">
(BLEU Score)
System Dev Test
Initial System 20.52 22.01
+ Long-range reordering 21.04 22.36
+ Lattice phrase extraction 20.69 22.64
</table>
<bodyText confidence="0.95225175">
postprocessed such that phrase table entries in-
clude the same amount of punctuation marks, es-
pecially quotation marks, in both source and tar-
get phrase. In contrast to the EnglishHGerman
language pairs, the word reordering required
in EnglishHFrench translations are restricted to
rather local word shifts which can be covered by
the short-range reordering feature. Applying addi-
tional long-range reordering is scarcely expected
to yield further improvements for these language
pairs and was not applied specifically in this task.
Table 3 shows the results of the system variants.
</bodyText>
<tableCaption confidence="0.988293">
Table 3: Translation results for English-French
</tableCaption>
<table confidence="0.949144714285714">
(BLEU Score)
System Dev Test
Baseline 20.97 20.87
+ Smoothing 21.42 21.32
+ Short-range reordering 20.79 22.26
+ Adaptation 21.05 21.97
+ cleanPT 21.50 21.98
</table>
<bodyText confidence="0.999478133333333">
Both on development and test set, smoothing
the probabilities in the phrase table resulted in an
increase of nearly 0.5 BLEU points. Applying
short-range word reordering did not lead to an im-
provement on the development set. However, the
increase in BLEU on the test set is substantial. The
opposite is the case when adapting the phrase ta-
ble: While phrase table adaptation improves the
translation quality on the development set, adapta-
tion leads to lower scores on the test set.
Thus, the system configuration that performed
best on the test set applies phrase table smoothing
and short-range word reordering. For creating the
translations for our submission, this configuration
was used.
</bodyText>
<subsectionHeader confidence="0.99355">
5.4 French-English
</subsectionHeader>
<bodyText confidence="0.998375272727273">
For the French-English task, similar experiments
have been conducted. With respect to the base-
line system, improvements in translation quality
could be measured when applying phrase table
smoothing. An increase of 0.43 BLEU points was
achieved using short-range word reordering. Ad-
ditional experiments with adapting the phrase ta-
ble to the domain of the test set led to further im-
provement. Submissions for the shared task were
created using the system including all mentioned
features.
</bodyText>
<tableCaption confidence="0.984395">
Table 4: Translation results for French-English
</tableCaption>
<table confidence="0.938602714285714">
(BLEU Score)
System Dev Test
Baseline 21.29 22.41
+ Smoothing 21.55 22.59
+ Short-range reordering 22.55 23.02
+ Adaptation 21.72 23.20
+ cleanPT 22.60 23.21
</table>
<sectionHeader confidence="0.993888" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99996084">
We have presented our system for the WMT’09
Shared Translation Task. The submissions for the
language pairs English-German, German-English,
English-French and French-English have been
created by the STTK decoder applying different
additional methods for each individual language
pair to enhance translation quality.
Word reordering models covering short-
range reordering for the EnglishHFrench and
EnglishHGerman and long-range reordering for
EnglishHGerman respectively proved to result in
better translations.
Smoothing the phrase probabilities in the phrase
table also increased the scores in all cases, while
adapting the phrase table to the test domain only
showed a positive influence on translation quality
in some of our experiments. Further tuning of the
adaptation procedure could help to clarify the ben-
efit of this method.
Using discriminative word alignment as an
alternative to performing word alignment with
GIZA++ did also improve the systems translating
between English and German. Future experiments
will be conducted applying discriminative word
alignment also in the EnglishHFrench systems.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999916">
This work was partly supported by Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
</bodyText>
<page confidence="0.998419">
83
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999953512820513">
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proc. of Empirical Methods in
Natural Language Processing. Sydney, Australia.
Philipp Koehn, Franz Josef Och and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
HLT/NAACL 2003. Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of Second ACL Workshop on Statistical Ma-
chine Translation. Prague, Czech Republic.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation. Columbus, OH, USA.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proc. of Forth ACL Workshop on Statistical Machine
Translation. Athens, Greece.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI. Sk¨ovde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing. Manchester, UK.
Ashish Venugopal, Andreas Zollman and Alex Waibel.
2005. Training and Evaluation Error Minimiza-
tion Rules for Statistical Machine Translation. In
Proc. of ACL 2005, Workshop on Data-drive Ma-
chine Translation and Beyond (WPT-05). Ann Ar-
bor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In NLP-KE’03. Beijing, China.
</reference>
<page confidence="0.999242">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.533385">
<title confidence="0.759964">The Universit¨at Karlsruhe Translation System for the EACL-WMT 2009</title>
<author confidence="0.992465">Jan Niehues</author>
<author confidence="0.992465">Teresa Herrmann</author>
<author confidence="0.992465">Muntsin Kolss</author>
<author confidence="0.992465">Alex</author>
<affiliation confidence="0.997652">Universit¨at Karlsruhe</affiliation>
<address confidence="0.875253">Karlsruhe,</address>
<abstract confidence="0.984022642857143">In this paper we describe the statistical machine translation system of the Universit¨at Karlsruhe developed for the translation task of the Fourth Workshop on Statistical Machine Translation. The state-ofthe-art phrase-based SMT system is augmented with alternative word reordering and alignment mechanisms as well as optional phrase table modifications. We participate in the constrained condition of German-English and English-German as well as in the constrained condition of French-English and English-French.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrasetable Smoothing for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="6989" citStr="Foster et al. (2006)" startWordPosition="1098" endWordPosition="1101">rtility features as well as first order features for 6 directions. The model was trained in three steps, first using the maximum likelihood optimization and afterwards it was optimized towards the alignment error rate. For more details see Niehues and Vogel (2008). 4.2 Phrase Table Smoothing The relative frequencies of the phrase pairs are a very important feature of the translation model, but they often overestimate rare phrase pairs. Therefore, the raw relative frequency estimates found in the phrase translation tables are smoothed by applying modified Kneser-Ney discounting as described in Foster et al. (2006). 4.3 Lattice Phrase Extraction For the test sentences the POS-based reordering allows us to change the word order in the source sentence, so that the sentence can be translated more easily. But this approach does not reorder the training sentences. This may cause problems for phrase extraction, especially for long-range reorderings. For example, if the English verb is aligned to both parts of the German verb, this phrase can not be extracted, since it is not continuous on the German side. In the case of German as source language, the phrase could be extracted if we also reorder the training c</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George Foster, Roland Kuhn, and Howard Johnson. 2006. Phrasetable Smoothing for Statistical Machine Translation. In Proc. of Empirical Methods in Natural Language Processing. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In HLT/NAACL 2003.</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="11569" citStr="Koehn et al. (2003)" startWordPosition="1889" endWordPosition="1892">lead to an improvement on the development set, but on the test set. One reason for this may be that the development set is quite small. Table 1: Translation results for English-German (BLEU Score) System Dev Test Short-range 13.96 14.99 + Smoothing 14.36 15.38 + Adaptation 13.96 15.44 + Discrim. WA 14.45 15.61 + Long-range reordering 14.58 15.70 5.2 German-English The German-English system was trained on the same data as the English-German except that we perform compound splitting as an additional preprocessing step. The compound splitting was done with the frequency-based method described in Koehn et al. (2003). For this language direction, the initial system already uses phrase table smoothing, adaptation and discriminative word alignment, in addition to the techniques of the English-German baseline system. The results are shown in Table 2. For this language pair, we could improve the translation quality, first, by adding the long-range reordering model. Further improvements could be achieved by using lattice phrase extraction as described before. 5.3 English-French For creating the English-French translations, first, the baseline system as described in Section 2 was used. This baseline was then au</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In HLT/NAACL 2003. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst.</title>
<date>2007</date>
<booktitle>In Proc. of Second ACL Workshop on Statistical Machine Translation.</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2380" citStr="Koehn et al., 2007" startWordPosition="341" endWordPosition="344">nts. Translation results for the different languages and system variants are presented in Section 5. 2 Baseline System The core of our system is the STTK decoder (Vogel, 2003), a phrase-based SMT decoder with a local reordering window of 2 words. The decoder generates a translation for the input text or word lattice by searching translation model and language model for the hypothesis that maximizes phrase translation probabilities and target language probabilities. The translation model, i.e. the SMT phrase table is created during the training phase by a modified version of the Moses Toolkit (Koehn et al., 2007) applying GIZA++ for word alignment. Language models are built using the SRILM Toolkit. The POS-tags for the reordering models were generated with the TreeTagger (Schmid, 1994) for all languages. 2.1 Training, Development and Test Data We submitted translations for the EnglishGerman, German-English, English-French and French-English tasks. All systems were trained on the Europarl and News Commentary corpora using the Moses Toolkit and apply 4-gram language models created from the respective monolingual News corpora. All feature weights are automatically determined and optimized with respect to</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. of Second ACL Workshop on Statistical Machine Translation. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Stephan Vogel</author>
</authors>
<title>Discriminative Word Alignment via Alignment Matrix Modeling.</title>
<date>2008</date>
<booktitle>In Proc. of Third ACL Workshop on Statistical Machine Translation.</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="6132" citStr="Niehues and Vogel (2008)" startWordPosition="962" endWordPosition="965">h parts of the German verb next to each other. 4 Translation Model The translation models of all systems we submitted differ in some parts from the baseline system. The main changes done will be described in this section. 4.1 Word Alignment The baseline method for creating the word alignment is to create the GIZA++ alignments in both directions and then to combine both alignments using a heuristic, e.g. grow-diag-final-and heuristic, as provided by the Moses Toolkit. In some of the submitted systems we used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the GIZA++ Toolkit and POS information. We used all local features, the GIZA and indicator fertility features as well as first order features for 6 directions. The model was trained in three steps, first using the maximum likelihood optimization and afterwards it was optimized towards the alignment error rate. For more details see Niehues and Vogel (2008). 4.2 Phrase Table Smoothing The relative frequencies of the phrase pairs are a very important feat</context>
</contexts>
<marker>Niehues, Vogel, 2008</marker>
<rawString>Jan Niehues and Stephan Vogel. 2008. Discriminative Word Alignment via Alignment Matrix Modeling. In Proc. of Third ACL Workshop on Statistical Machine Translation. Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Muntsin Kolss</author>
</authors>
<title>A POS-Based Model for Long-Range Reorderings in SMT.</title>
<date>2009</date>
<booktitle>In Proc. of Forth ACL Workshop on Statistical Machine Translation.</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="5251" citStr="Niehues and Kolss (2009)" startWordPosition="810" endWordPosition="813">t require the verb to be shifted nearly across the whole sentence. During this shift of the verb, the rest of the sentence remains mainly unchanged. It does not matter which words are in between, since they are moved as a whole. Furthermore, rules including an explicit sequence of POS-tags spanning the whole sentence would be too specific. A lot more rules would be needed to cover long-range reorderings with each rule being applicable only very sparsely. Therefore, we model long-range reordering by generalizing over the unaffected sequences and introduce rules with gaps. (For more details see Niehues and Kolss (2009)). These are learned in a way similar to the other type of reordering rules described above, but contain a gap representing one or several arbitrary words. It is, for example, possible to have the following rule VAFIN * VVPP --+ VAFIN VVPP *, which puts both parts of the German verb next to each other. 4 Translation Model The translation models of all systems we submitted differ in some parts from the baseline system. The main changes done will be described in this section. 4.1 Word Alignment The baseline method for creating the word alignment is to create the GIZA++ alignments in both directi</context>
</contexts>
<marker>Niehues, Kolss, 2009</marker>
<rawString>Jan Niehues and Muntsin Kolss. 2009. A POS-Based Model for Long-Range Reorderings in SMT. In Proc. of Forth ACL Workshop on Statistical Machine Translation. Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kay Rottmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model.</title>
<date>2007</date>
<booktitle>In TMI. Sk¨ovde,</booktitle>
<contexts>
<context position="3399" citStr="Rottmann and Vogel (2007)" startWordPosition="500" endWordPosition="503">mmentary corpora using the Moses Toolkit and apply 4-gram language models created from the respective monolingual News corpora. All feature weights are automatically determined and optimized with respect to BLEU via MERT (Venugopal et al., 2005). For development and testing we used data provided by the WMT’09, news-dev2009a and newsdev2009b, consisting of 1026 sentences each. 3 Word Reordering Model One part of our system that differs from the baseline system is the reordering model. To account for the different word orders in the languages, we used the POS-based reordering model presented in Rottmann and Vogel (2007). This model learns rules from a parallel text to reorder the source side. The aim is to generate a reordered source side that can be translated in a more monotone way. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 80–84, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 80 In this framework, first, reordering rules are extracted from an aligned parallel corpus and POS information is added to the source side. These rules are of the form VVIMP VMFIN PPER —* PPER VMFIN VVIMP and describe how the source side has to be reorde</context>
</contexts>
<marker>Rottmann, Vogel, 2007</marker>
<rawString>Kay Rottmann and Stephan Vogel. 2007. Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model. In TMI. Sk¨ovde, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing.</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="2556" citStr="Schmid, 1994" startWordPosition="370" endWordPosition="371">ase-based SMT decoder with a local reordering window of 2 words. The decoder generates a translation for the input text or word lattice by searching translation model and language model for the hypothesis that maximizes phrase translation probabilities and target language probabilities. The translation model, i.e. the SMT phrase table is created during the training phase by a modified version of the Moses Toolkit (Koehn et al., 2007) applying GIZA++ for word alignment. Language models are built using the SRILM Toolkit. The POS-tags for the reordering models were generated with the TreeTagger (Schmid, 1994) for all languages. 2.1 Training, Development and Test Data We submitted translations for the EnglishGerman, German-English, English-French and French-English tasks. All systems were trained on the Europarl and News Commentary corpora using the Moses Toolkit and apply 4-gram language models created from the respective monolingual News corpora. All feature weights are automatically determined and optimized with respect to BLEU via MERT (Venugopal et al., 2005). For development and testing we used data provided by the WMT’09, news-dev2009a and newsdev2009b, consisting of 1026 sentences each. 3 W</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollman</author>
<author>Alex Waibel</author>
</authors>
<title>Training and Evaluation Error Minimization Rules for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL 2005, Workshop on Data-drive Machine Translation and Beyond (WPT-05).</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="3019" citStr="Venugopal et al., 2005" startWordPosition="437" endWordPosition="440"> for word alignment. Language models are built using the SRILM Toolkit. The POS-tags for the reordering models were generated with the TreeTagger (Schmid, 1994) for all languages. 2.1 Training, Development and Test Data We submitted translations for the EnglishGerman, German-English, English-French and French-English tasks. All systems were trained on the Europarl and News Commentary corpora using the Moses Toolkit and apply 4-gram language models created from the respective monolingual News corpora. All feature weights are automatically determined and optimized with respect to BLEU via MERT (Venugopal et al., 2005). For development and testing we used data provided by the WMT’09, news-dev2009a and newsdev2009b, consisting of 1026 sentences each. 3 Word Reordering Model One part of our system that differs from the baseline system is the reordering model. To account for the different word orders in the languages, we used the POS-based reordering model presented in Rottmann and Vogel (2007). This model learns rules from a parallel text to reorder the source side. The aim is to generate a reordered source side that can be translated in a more monotone way. Proceedings of the Fourth Workshop on Statistical M</context>
</contexts>
<marker>Venugopal, Zollman, Waibel, 2005</marker>
<rawString>Ashish Venugopal, Andreas Zollman and Alex Waibel. 2005. Training and Evaluation Error Minimization Rules for Statistical Machine Translation. In Proc. of ACL 2005, Workshop on Data-drive Machine Translation and Beyond (WPT-05). Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
</authors>
<title>SMT Decoder Dissected: Word Reordering.</title>
<date>2003</date>
<booktitle>In NLP-KE’03.</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="1936" citStr="Vogel, 2003" startWordPosition="271" endWordPosition="273">criminative word alignment (DWA) and several modifications of the phrase table. Experiments with different system variants were conducted including some of those additional system components. Significantly better translation results could be achieved compared to the baseline results. An overview of the system will follow in Section 2, which describes the baseline architecture, followed by descriptions of the additional system components. Translation results for the different languages and system variants are presented in Section 5. 2 Baseline System The core of our system is the STTK decoder (Vogel, 2003), a phrase-based SMT decoder with a local reordering window of 2 words. The decoder generates a translation for the input text or word lattice by searching translation model and language model for the hypothesis that maximizes phrase translation probabilities and target language probabilities. The translation model, i.e. the SMT phrase table is created during the training phase by a modified version of the Moses Toolkit (Koehn et al., 2007) applying GIZA++ for word alignment. Language models are built using the SRILM Toolkit. The POS-tags for the reordering models were generated with the TreeT</context>
</contexts>
<marker>Vogel, 2003</marker>
<rawString>Stephan Vogel. 2003. SMT Decoder Dissected: Word Reordering. In NLP-KE’03. Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>