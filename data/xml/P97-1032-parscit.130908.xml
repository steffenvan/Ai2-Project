<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.700861">
Comparing a Linguistic and a Stochastic Tagger
</title>
<figure confidence="0.319777">
Christer Samuelsson
Lucent Technologies
Bell Laboratories
600 Mountain Ave, Room 2D-:339
Murray Hill, NJ 07974, USA
christerOresearch.bell-labs.com
</figure>
<title confidence="0.285958">
Atro Voutilainen
Research Unit for Multilingual Language Technology
</title>
<author confidence="0.487667">
P.O. Box 4
</author>
<affiliation confidence="0.704512">
FIN-00014 University of Helsinki
</affiliation>
<address confidence="0.469099">
Finland
</address>
<email confidence="0.580595">
Atro.Voutilainen0Helsinki.FI
</email>
<sectionHeader confidence="0.987056" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930785714286">
Concerning different approaches to auto-
matic PoS tagging: EngCG-2, a constraint-
based morphological tagger, is compared in
a double-blind test with a state-of-the-art
statistical tagger on a common disambigua-
tion task using a common tag set. The ex-
periments show that for the same amount
of remaining ambiguity, the error rate of
the statistical tagger is one order of mag-
nitude greater than that of the rule-based
one. The two related issues of priming
effects compromising the results and dis-
agreement between human annotators are
also addressed.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998784153846154">
There are currently two main methods for auto-
matic part-of-speech tagging. The prevailing one
uses essentially statistical language models automat-
ically derived from usually hand-annotated corpora.
These corpus-based models can be represented e.g.
as collocational matrices (Garside et al. (eds.) 1987:
Church 1988), Hidden Markov models (cf. Cutting
et al. 1992), local rules (e.g. Hindle 1989) and neu-
ral networks (e.g. Schmid 1994). Taggers using these
statistical language models are generally reported to
assign the correct and unique tag to 95-97% of words
in running text, using tag sets ranging from some
dozens to about 130 tags.
The less popular approach is based on hand-coded
linguistic rules. Pioneering work was done in the
1960&apos;s (e.g. Greene and Rubin 1971). Recently, new
interest in the linguistic approach has been shown
e.g. in the work of (Karlsson 1990: Voutilainen et
al. 1992; Oflazer and Kuril&amp; 1994: Chanod and
Tapanainen 1995: Karlsson et al. (eds.) 1995: Vouti-
lainen 1995). The first serious linguistic competitor
to data-driven statistical taggers is the English Con-
straint Grammar parser. EngCG (cf. Voutilainen et
al. 1992; Karlsson et al. (eds.) 1995). The tagger
consists of the following sequentially applied mod-
ules:
</bodyText>
<listItem confidence="0.822978">
1. Tokenisation
2. Morphological analysis
(a) Lexical component
(b) Rule-based guesser for unknown words
3. Resolution of morphological ambiguities
</listItem>
<bodyText confidence="0.999683157894736">
The tagger uses a two-level morphological anal-
yser with a large lexicon and a morphological
description that introduces about 180 different
ambiguity-forming morphological analyses, as a re-
sult of which each word gets 1.7-2.2 different analy-
ses on an average. Morphological analyses are as-
signed to unknown words with an accurate rule-
based &apos;guesser&apos;. The morphological disambiguator
uses constraint rules that discard illegitimate mor-
phological analyses on the basis of local or global
context conditions. The rules can be grouped as
ordered subgrammars: e.g. heuristic subgrammar 2
can be applied for resolving ambiguities left pending
by the more &apos;careful&apos; subgrammar 1.
Older versions of EngCG (using about 1,150 con-
straints) are reported (Voutilainen et al. 1992; Vouti-
lainen and Heikkila 1994; Tapanainen and Vouti-
lainen 1994; Voutilainen 1995) to assign a correct
analysis to about 99.7% of all words while each word
in the output retains 1.04-1.09 alternative analyses
on an average, i.e. some of the ambiguities remain
unresolved.
These results have been seriously questioned. One
doubt concerns the notion -.correct analysis&amp;quot;. For
example Church (1992) argues that linguists who
manually perform the tagging task using the double-
blind method disagree about the correct analysis in
at least 3% of all words even after they have nego-
tiated about the initial disagreements. If this were
the case, reporting accuracies above this 97% &apos;upper
bound&apos; would make no sense.
However, Voutilainen and Jarvinen (1995) empir-
ically show that an interjudge agreement virtually
of 100% is possible. at least with the EngCG tag set
if not with the original Brown Corpus tag set. This
consistent applicability of the EngCG tag set is ex-
plained by characterising it as grammatically rather
than semantically motivated.
</bodyText>
<page confidence="0.997167">
246
</page>
<bodyText confidence="0.946319407407408">
Another main reservation about the EngCG fig-
ures is the suspicion that, perhaps partly due to the
somewhat underspecific nature of the EngCG tag
set, it must be so easy to disambiguate that also a
statistical tagger using the EngCG tags would reach
at least as good results. This argument will be ex-
amined in this paper. It will be empirically shown
(i) that the EngCG tag set is about as difficult for a
probabilistic tagger as more generally used tag sets
and (ii) that the EngCG disambiguator has a clearly
smaller error rate than the probabilistic tagger when
a similar (small) amount of ambiguity is permitted
in the output.
A state-of-the-art statistical tagger is trained on
a corpus of over 350,000 words hand-annotated with
EngCG tags. then both taggers (a new version
known as Eno-CG-21 with 3,600 constraints as five
subgrammars-, and a statistical tagger) are applied
to the same held-out benchmark corpus of 55.000
words, and their performances are compared. The
results disconfirm the suspected &apos;easiness&apos; of the
EngCG tag set: the statistical tagger&apos;s performance
figures are no better than is the case with better
known tag sets.
Two caveats are in order. What we are not ad-
dressing in this paper is the work load required for
making a rule-based or a data-driven tagger. The
rules in EngCG certainly took a considerable effort
to write, and though at the present state of knowl-
edge rules could be written and tested with less ef-
fort, it may well be the case that a tagger with an
accuracy of 95-97% can be produced with less effort
by using data-driven techniques.3
Another caveat is that EngCG alone does not re-
solve all ambiguities, so it cannot be compared to a
typical statistical tagger if full disambiguation is re-
quired. However, Voutilainen (1995) has shown that
EngCG combined with a syntactic parser produces
morphologically unambiguous output with an accu-
racy of 99.3%, a figure clearly better than that of the
statistical tagger in the experiments below (however,
the test data was not the same).
Before examining the statistical tagger. two prac-
tical points are addressed: the annotation of the cor-
pora used, and the modification of the EngCG tag
set for use in a statistical tagger.
&apos;An online version of EngCG-2 can be found at
ht tp://www.ling.helsinki.fir avoutila/engcg-2.html.
2The first three subgrarnmars are generally highly re-
liable and almost all of the total grammar development
time was spent on them: the last two contain rather
rough heuristic constraints.
3However, for an interesting experiment suggesting
otherwise, see (Chanod and Tapanainen 1995).
</bodyText>
<sectionHeader confidence="0.623093" genericHeader="method">
2 Preparation of Corpus Resources
</sectionHeader>
<subsectionHeader confidence="0.99643">
2.1 Annotation of training corpus
</subsectionHeader>
<bodyText confidence="0.999963125">
The stochastic tagger was trained on a sample of
357,000 words from the Brown University Corpus
of Present-Day English (Francis and Kueera 1982)
that was annotated using the EngCG tags. The cor-
pus was first analysed with the EngCG lexical anal-
yser, and then it was fully disambiguated and, when
necessary, corrected by a human expert. This an-
notation took place a few years ago. Since then, it
has been used in the development of new EngCG
constraints (the present version, EngCG-2, contains
about 3,600 constraints): new constraints were ap-
plied to the training corpus, and whenever a reading
marked as correct was discarded, either the analysis
in the corpus, or the constraint itself, was corrected.
In this way, the tagging quality of the corpus was
continuously improved.
</bodyText>
<subsectionHeader confidence="0.999543">
2.2 Annotation of benchmark corpus
</subsectionHeader>
<bodyText confidence="0.999989789473684">
Our comparisons use a held-out benchmark corpus
of about :55,000 words of journalistic, scientific and
manual texts, i.e., no &apos;training effects are expected
for either system. The benchmark corpus was an-
notated by first applying the preprocessor and mor-
phological analyser, but not the morphological dis-
ambiguator, to the text. This morphologically am-
biguous text was then independently and fully dis-
ambiguated by two experts whose task was also to
detect any errors potentially produced by the pre-
viously applied components. They worked indepen-
dently, consulting written documentation of the tag
set when necessary. Then these manually disam-
biguated versions were automatically compared with
each other. At this stage, about 99.3% of all anal-
yses were identical. When the differences were col-
lectively examined, virtually all were agreed to be
due to clerical mistakes. Only in the analysis of 21
words, different (meaning-level) interpretations per-
sisted, and even here both judges agreed the ambigu-
ity to be genuine. One of these two corpus versions
was modified to represent the consensus. and this
&apos;consensus corpus&apos; was used as a benchmark in the
evaluations.
As explained in Voutilainen and Jarvinen (1995).
this high agreement rate is due to two main factors.
Firstly, distinctions based on some kind of vague se-
mantics are avoided, which is not always case with
better known tag sets. Secondly. the adopted analy-
sis of most of the constructions where humans tend
to be uncertain is documented as a collection of tag
application principles in the form of a grammar-
ian&apos;s manual (for further details, cf. Voutilainen and
Jarvinen 199:5).
The corpus-annotation procedure allows us to per-
form a text-book statistical hypothesis test. Let
the null hypothesis be that any two human eval-
uators will necessarily disagree in at least 3% of
</bodyText>
<page confidence="0.98419">
247
</page>
<bodyText confidence="0.99948675">
the cases. Under this assumption, the probability
of an observed disagreement of less than 2.88% is
less than 5%. This can be seen as follows: For
the relative frequency of disagreement, fn, we have
</bodyText>
<equation confidence="0.911033">
ti-p)
that fn is approximately •-••• N(p, n ), where p
</equation>
<bodyText confidence="0.9431775">
is the actual disagreement probability and n is the
number of trials, i.e., the corpus size. This means
</bodyText>
<equation confidence="0.983684333333333">
fri — P
VT1. &lt; (1)(x) where 4&gt; is the
that P(( 03(1 11)
</equation>
<bodyText confidence="0.9512005">
standard normal distribution function. This in turn
means that
</bodyText>
<equation confidence="0.971391">
P(fn P+ 12(1 —P))
V n
</equation>
<bodyText confidence="0.869216">
Here n is 55,000 and 4)(-1.645) = 0.05. Under the
null hypothesis, p is at least 3% and thus:
</bodyText>
<equation confidence="0.81394725">
•
P(f, 5. 0.03 — 1.645 \/13.030000.97 ) =
55,
= P(fn &lt; 0.0288) &lt; 0.05
</equation>
<bodyText confidence="0.998905555555555">
We can thus discard the null hypothesis at signifi-
cance level 5% if the observed disagreement is less
than 2.88%. It was in fact 0.7% before error cor-
rection, and virtually zero (21 ) after negotia-
tion. This means that we can actually discard the
hypotheses that the human evaluators in average
disagree in at least 0.8% of the cases before error
correction, and in at least 0.1% of the cases after
negotiations, at significance level 5%.
</bodyText>
<subsectionHeader confidence="0.975429">
2.3 Tag set conversion
</subsectionHeader>
<bodyText confidence="0.999997">
The EngCG morphological analyser&apos;s output for-
mally differs from most tagged corpora: consider the
following 5-ways ambiguous analysis of &amp;quot;walk&amp;quot;:
</bodyText>
<equation confidence="0.481067333333333">
walk
walk &lt;SV&gt; &lt;SVO&gt; V SUBJUNCTIVE VFIN
walk &lt;SV&gt; &lt;SVO&gt; V IMP VFIN
walk &lt;SV&gt; &lt;SVO&gt; V INF
walk &lt;SV&gt; &lt;SVO&gt; V FRES -SG3 VFIN
walk N NOM SG
</equation>
<bodyText confidence="0.928950705882353">
Statistical taggers usually employ single tags to
indicate analyses (e.g. &amp;quot;NN&amp;quot; for &amp;quot;N NOM SG&amp;quot;).
Therefore a simple conversion program was made for
producing the following kind of output, where each
reading is represented as a single tag:
walk V-SUBJUNCTIVE V-IMP V-INF
V-PRES-BASE N-NOM-SG
The conversion program reduces the multipart
EngCG tags into a set of 80 word tags and 17 punc-
tuation tags (see Appendix) that retain the central
linguistic characteristics of the original EngCG tag
set.
A reduced version of the benchmark corpus was
prepared with this conversion program for the sta-
tistical tagger&apos;s use. Also EngCG&apos;s output was con-
verted into this format to enable direct comparison
with the statistical tagger.
</bodyText>
<sectionHeader confidence="0.857048" genericHeader="method">
3 The Statistical Tagger
</sectionHeader>
<bodyText confidence="0.992110333333333">
The statistical tagger used in the experiments is a
classical trigram-based 11MN1 decoder of the kind
described in e.g. (Church 1988), (DeRose 1988) and
numerous other articles. Following conventional no-
tation, e.g. (Rabiner 1989, pp. 272-274) and (Krenn
and Samuelsson 1996. pp. 42-46), the tagger recur-
sively calculates the a, 3, 7 and b variables for each
word string position t = 1.....T and each possible
state4 si : i = 1.....n:
</bodyText>
<equation confidence="0.997274583333333">
at(i) = si)
;3*(0 P(W&gt; ISt = si)
7t(i) = P(St = si 1W) = P(W; St = si)
P(W)
at(i)
Ecet(i) • 3t(i)
6t(i) max P(S&lt;t-I,St = st;W&lt;t)
Here
W = = wki, . ,WT = Wk7.
W&lt;t = = Wki, • • • ,Wt = Wkt
W&gt;t = Wt+1 = I • • • I WT = WkT
S&lt; t = Si Si, St =
</equation>
<bodyText confidence="0.9920058">
where St = st is the event of the tth word being
emitted from state si and VITt = wk, is the event of
the tth word being the particular word wk, that was
actually observed in the word string.
Note that for t — 1 T — 1 ; j — 1
</bodyText>
<equation confidence="0.9977042">
at+1(j)
1=1
3t(i) =E 3t+I(J) • Pij
esfil?.x t(i) • Pu] at+t(i) a -k
t+1
</equation>
<bodyText confidence="0.999526">
where pi.; = P(St+t = si St = si) are the transi-
tion probabilities, encoding the tag N-gram proba-
bilities, and
</bodyText>
<equation confidence="0.4200515">
a •k =
= P(Wt = Wk I St = si) = P(Wt = u&apos;k IXt = xi)
</equation>
<footnote confidence="0.99147">
4The N-I th-order HNINI corresponding to an N-gram
tagger is encoded as a first-order MANI, where each state
corresponds to a sequence of N-1 tags, i.e.. for a trigram
</footnote>
<equation confidence="0.721136">
tagger, each state corresponds to a tag pair.
0,(i) • Pii
</equation>
<page confidence="0.983135">
248
</page>
<bodyText confidence="0.999933833333333">
are the lexical probabilities. Here Xt is the random
variable of assigning a tag to the tth word and x1 is
the last tag of the tag sequence encoded as state sj.
Note that si sj need not imply xi xj.
More precisely, the tagger employs the converse
lexical probabilities
</bodyText>
<equation confidence="0.940111428571429">
P(Xt = xj I Wt = wk)
P(Xt = xj)
This results in slight variants a&apos;, , 7&apos; and of the
original quantities:
and thus Vi, t
= a(i) • gW
E ait(i) g(i)
i.1
at(i). fit(i) 7t(z)
E at(i) • A(i)
t=1
and Vt
argmax 5,1(i) = argmax6t(i)
1&lt;i&lt;n 1&lt;i&lt;n
</equation>
<bodyText confidence="0.999909148936171">
The rationale behind this is to facilitate estimat-
ing the model parameters from sparse data. In more
detail, it is easy to estimate P(tag I word) for a pre-
viously unseen word by backing off to statistics de-
rived from words that end with the same sequence
of letters (or based on other surface cues), whereas
directly estimating P(word I tag) is more difficult.
This is particularly useful for languages with a rich
inflectional and derivational morphology, but also
for English: for example, the suffix &amp;quot;-tion&amp;quot; is a
strong indicator that the word in question is a noun;
the suffix &amp;quot;-able&amp;quot; that it is an adjective.
More technically, the lexicon is organised as a
reverse-suffix tree, and smoothing the probability es-
timates is accomplished by blending the distribution
at the current node of the tree with that of higher-
level nodes, corresponding to (shorter) suffixes of the
current word (suffix). The scheme also incorporates
probability distributions for the set of capitalized
words, the set of all-caps words and the set of in-
frequent words, all of which are used to improve the
estimates for unknown words. Employing a small
amount of back-off smoothing also for the known
words is useful to reduce lexical tag omissions. Em-
pirically, looking two branching points up the tree
for known words, and all the way up to the root
for unknown words, proved optimal. The method
for blending the distributions applies equally well to
smoothing the transition probabilities /NJ, i.e., the
tag N-gram probabilities, and both the scheme and
its application to these two tasks are described in de-
tail in (Samuelsson 1996), where it was also shown
to compare favourably to (deleted) interpolation, see
(Jelinek and Mercer 1980), even when the back-off
weights of the latter were optimal.
The 5 variables enable finding the most probable
state sequence under the HMM, from which the most
likely assignment of tags to words can be directly es-
tablished. This is the normal modus operandi of an
HMM decoder. Using the 7 variables, we can calcu-
late the probability of being in state si at string po-
sition t, and thus having emitted wk, from this state,
conditional on the entire word string. By summing
over all states that would assign the same tag to this
word, the individual probability of each tag being as-
signed to any particular input word, conditional on
the entire word string, can be calculated:
</bodyText>
<equation confidence="0.999743">
P(Xt = IW) =
E P(St = .5) W) = E 7t(i)
</equation>
<bodyText confidence="0.999538888888889">
This allows retaining multiple tags for each word by
simply discarding only low-probability tags; those
whose probabilities are below some threshold value.
Of course, the most probable tag is never discarded,
even if its probability happens to be less than the
threshold value. By varying the threshold, we can
perform a recall-precision, or error-rate-ambiguity,
tradeoff. A similar strategy is adopted in (de Mar-
cken 1990).
</bodyText>
<sectionHeader confidence="0.998718" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9997995">
The statistical tagger was trained on 357,000 words
from the Brown corpus (Francis and KuEera 1982),
reannotated using the EngCG annotation scheme
(see above). In a first set of experiments, a 35,000
word subset of this corpus was set aside and used to
evaluate the tagger &apos;s performance when trained on
successively larger portions of the remaining 322,000
words. The learning curve, showing the error rate af-
ter full disambiguation as a function of the amount
of training data used, see Figure 1, has levelled off at
322,000 words, indicating that little is to be gained
from further training. We also note that the ab-
solute value of the error rate is 3.51% — a typi-
cal state-of-the-art figure. Here, previously unseen
words contribute 1.08% to the total error rate, while
the contribution from lexical tag omissions is 0.08%.
95% confidence intervals for the error rates would
range from ± 0.30% for 30,000 words to ± 0.20% at
322,000 words.
The tagger was then trained on the entire set
of 357,000 words and confronted with the separate
55,000-word benchmark corpus, and run both in full
</bodyText>
<figure confidence="0.793882">
ajk
P( Wt = Wk)
&apos;MO
bi(i) =
11 P(Wu wk„)
u=t+i
P(Wu =Wk)
tt=i
249
Error rate (%)
Error rate
Learning curve
50 100 150 200 250 300
Training set (kWords)
</figure>
<figureCaption confidence="0.9966435">
Figure 1: Learning curve for the statistical tagger
on the Brown corpus.
</figureCaption>
<table confidence="0.993429875">
Ambiguity Errorrate(T)
(Tags/word) Statistical Tagger EngCG
(6) (7)
1.000 4.72 4.68
1.012 4.20
1.025 3.75
1.026 (3.72) 0.43
1.035 (3.48) 0.29
1.038 3.40
1.048 (3.20) 0.15
1.051 3.14
1.059 (2.99) 0.12
1.065 2.87
1.070 (2.80) 0.10
1.078 2.69
1.093 2.55
</table>
<tableCaption confidence="0.682751">
Table 1: Error-rate-ambiguity tradeoff for both tag-
gers on the benchmark corpus. Parenthesized num-
bers are interpolated.
</tableCaption>
<bodyText confidence="0.9998582">
and partial disambiguation mode. Table 1 shows
the error rate as a function of remaining ambiguity
(tags/word) both for the statistical tagger, and for
the EngCG-2 tagger. The error rate for full disam-
biguation using the b variables is 4.72% and using
the -y variables is 4.68%, both ±0.18% with confi-
dence degree 95%. Note that the optimal tag se-
quence obtained using the 7 variables need not equal
the optimal tag sequence obtained using the 6 vari-
ables. In fact, the former sequence may be assigned
zero probability by the HIM, namely if one of its
state transitions has zero probability.
Previously unseen words account for 2.01%, and
lexical tag omissions for 0.15% of the total error rate.
These two error sources are together exactly 1.00%
higher on the benchmark corpus than on the Brown
corpus, and account for almost the entire difference
in error rate. They stem from using less complete
lexical information sources, and are most likely the
effect of a larger vocabulary overlap between the test
and training portions of the Brown corpus than be-
tween the Brown and benchmark corpora.
The ratio between the error rates of the two tag-
gers with the same amount of remaining ambiguity
ranges from 8.6 at 1.026 tags/word to 28.0 at 1.070
tags/word. The error rate of the statistical tagger
can be further decreased, at the price of increased
remaining ambiguity, see Figure 2. In the limit of
retaining all possible tags, the residual error rate is
entirely due to lexical tag omissions, i.e., it is 0.15%,
with in average 14.24 tags per word. The reason
that this figure is so high is that the unknown words,
which comprise 10% of the corpus, are assigned all
possible tags as they are backed off all the way to
the root of the reverse-suffix tree.
</bodyText>
<figure confidence="0.9911555">
Error-rate-ambiguity trade-off
4
3
2
1
0
0 2 4 6 8 10 12 14
Remaining ambiguity (Tags/Word)
</figure>
<figureCaption confidence="0.992254">
Figure 2: Error-rate-ambiguity tradeoff for the sta-
tistical tagger on the benchmark corpus.
</figureCaption>
<sectionHeader confidence="0.978478" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9556985">
Recently voiced scepticisms concerning the superior
EngCG tagging results boil down to the following:
</bodyText>
<listItem confidence="0.81331625">
• The reported results are due to the simplicity
of the tag set employed by the EngCG system.
• The reported results are an effect of trading
high ambiguity resolution for lower error rate.
• The results are an effect of so-called priming
of the human annotators when preparing the
test corpora, compromising the integrity of the
experimental evaluations.
</listItem>
<bodyText confidence="0.999350857142857">
In the current article, these points of criticism
were investigated. A state-of-the-art statistical
tagger, capable of performing error-rate-ambiguity
tradeoff, was trained on a 357,000-word portion of
the Brown corpus reannotated with the EngCG tag
set, and both taggers were evaluated using a sep-
arate 55,000-word benchmark corpus new to both
</bodyText>
<page confidence="0.985584">
250
</page>
<bodyText confidence="0.999972333333333">
systems. This benchmark corpus was independently
disambiguated by two linguists, without access to
the results of the automatic taggers. The initial
differences between the linguists&apos; outputs (0.7% of
all words) were jointly examined by the linguists;
practically all of them turned out to be clerical er-
rors (rather than the product of genuine difference
of opinion).
In the experiments, the performance of the
EngCG-2 tagger was radically better than that of
the statistical tagger: at ambiguity levels common
to both systems, the error rate of the statistical tag-
ger was 8.6 to 28 times higher than that of EngCG-
2. We conclude that neither the tag set used by
EngCG-2, nor the error-rate-ambiguity tradeoff, nor
any priming effects can possibly explain the observed
difference in performance.
Instead we must conclude that the lexical and con-
textual information sources at the disposal of the
EngCG system are superior. Investigating this em-
pirically by granting the statistical tagger access to
the same information sources as those available in
the Constraint Grammar framework constitutes fu-
ture work.
</bodyText>
<sectionHeader confidence="0.954452" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998715">
Though Voutilainen is the main author of the
EngCG-2 tagger, the development of the system
has benefited from several other contributions too.
Fred Karlsson proposed the Constraint Grammar
framework in the late 1980s. Juha Heikkila and
Timo Jarvinen contributed with their work on En-
glish morphology and lexicon. Kimmo Koskenniemi
wrote the software for morphological analysis. Pasi
Tapanainen has written various implementations of
the CG parser, including the recent CG-2 parser
(Tapanainen 1996).
The quality of the investigation and presentation
was boosted by a number of suggestions to improve-
ments and (often sceptical) comments from numer-
ous ACL reviewers and UPenn associates, in partic-
ular from Mark Liberman.
</bodyText>
<sectionHeader confidence="0.894217" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.897960636363636">
J-P Chanod and P. Tapanainen. 1995. Tagging
French: comparing a statistical and a constraint-
based method. In Procs. 7th Conference of the
European Chapter of the Association for Compu-
tational Linguistics, pp. 149-157, ACL, 1995.
K. W. Church. 1988. &amp;quot;A Stochastic Parts Program
and Noun Phrase Parser for Unrestricted Text&amp;quot;.
In Procs. 2nd Conference on Applied Natural Lan-
guage Processing, pp. 136-143, ACL, 1988.
K. Church. 1992. Current Practice in Part of
Speech Tagging and Suggestions for the Future. In
</bodyText>
<reference confidence="0.985324111111111">
Simmons (ed.), Sbornik praci: In Honor of Henry
Kueera. Michigan Slavic Studies, 1992.
D. Cutting, J. Kupiec, J. Pedersen and P. Sibun.
1992. A Practical Part-of-Speech Tagger. In
Procs. 3rd Conference on Applied Natural Lan-
guage Processing, pp. 133-140, ACL, 1992.
S. J. DeRose. 1988. &amp;quot;Grammatical Category
Disambiguation by Statistical Optimization&amp;quot;. In
Computational Linguistics 14(1), pp. 31-39, ACL,
1988.
N. W. Francis and H. Kueera. 1982. Fre-
quency Analysis of English Usage, Houghton Mif-
flin, Boston, 1982.
R. Garside, G. Leech and G. Sampson (eds.). 1987.
The Computational Analysis of English. London
and New York: Longman, 1987.
B. Greene and G. Rubin. 1971. Automatic gram-
matical tagging of English. Brown University,
Providence, 1971.
D. Hindle. 1989. Acquiring disambiguation rules
from text. In Procs. 27th Annual Meeting of the
Association for Computational Linguistics, pp.
118-125, ACL, 1989.
F. Jelinek and R. L. Mercer. 1980. &amp;quot;Interpolated
Estimation of Markov Source Paramenters from
Sparse Data&amp;quot;. Pattern Recognition in Practice:
381-397. North Holland, 1980.
F. Karlsson. 1990. Constraint Grammar as a
Framework for Parsing Running Text. In Procs.
CoLing&apos;90. In Procs. 14th International Confer-
ence on Computational Linguistics, ICCL, 1990.
F. Karlsson, A. Voutilainen, J. Heikkila and A.
Anttila (eds.). 1995. Constraint Grammar. A
Language-Independent System for Parsing Unre-
stricted Text. Berlin and New York: Mouton de
Gruyter, 1995.
B. Krenn and C. Samuelsson. The Linguist&apos;s
Guide to Statistics. Version of April 23, 1996.
http://coli.uni-sb.de/-christer.
C. G. de Marcken. 1990. &amp;quot;Parsing the LOB Cor-
pus&amp;quot;. In Procs. 28th Annual Meeting of the .4s-
sociation for Computational Linguistics. pp. 243-
251, ACL, 1990.
K. Oflazer and I. Kuruoz. 1994. Tagging and
morphological disambiguation of Turkish text. In
Procs. 4th Conference on Applied Natural Lan-
guage Processing. ACL, 1994.
L. R. Rabiner. 1989. -A Tutorial on Hid-
den Markov Models and Selected Applications
in Speech Recognition&amp;quot;. In Readings in Speech
Recognition, pp. 267-296. Alex Waibel and Kai-
Fu Lee (eds), Morgan Kaufmann, 1990.
G. Sampson. 1995. English for the Computer, Ox-
ford University Press. 1995.
</reference>
<page confidence="0.98471">
251
</page>
<reference confidence="0.983164060606061">
C. Samuelsson. 1996. &amp;quot;Handling Sparse Data by
Successive Abstraction&amp;quot;. In Procs. 16th Interna-
tional Conference on Computational Linguistics,
pp. 895-900, ICCL, 1996.
H. Schmid. 1994. Part-of-speech tagging with neu-
ral networks. In Procs. 15th International Confer-
ence on Computational Linguistics, pp. 172-176,
ICCL, 1994.
P. Tapanainen. 1996. The Constraint Grammar
Parser CG-2. Publ. 27, Dept. General Linguistics,
University of Helsinki, 1996.
P. Tapanainen and A. Voutilainen. 1994. Tagging
accurately — don&apos;t guess if you know. In Procs. 4th
Conference on Applied Natural Language Process-
ing, ACL, 1994.
A. Voutilainen. 1995. &amp;quot;A syntax-based part of
speech analyser&amp;quot;. In Procs. 7th Conference of the
European Chapter of the Association for Compu-
tational Linguistics, pp. 157-164, ACL, 1995.
A. Voutilainen and J. Heikkila. 1994. An English
constraint grammar (EngCG): a surface-syntactic
parser of English. In Fries, Tottie and Schneider
(eds.), Creating and using English language cor-
pora, Rodopi, 1994.
A. Voutilainen, J. Heikkila and A. Anttila. 1992.
Constraint Grammar of English. A Performance-
Oriented Introduction. Publ. 21, Dept. General
Linguistics, University of Helsinki, 1992.
A. Voutilainen and T. Jarvinen. &amp;quot;Specifying a shal-
low grammatical representation for parsing pur-
poses&amp;quot;. In Procs. 7th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics, pp. 210-214, ACL, 1995.
</reference>
<page confidence="0.992293">
252
</page>
<figure confidence="0.992350766990292">
Appendix: Reduced EngCG tag set
Punctuation tags:
©colon
©comma
©dash
4dotdot
©dquote
©exclamation
©fullstop
©Iparen
©rparen
arparen
Urparen
drparen
Olquote
Orquote
©slash
Onewlines
©question
©semicolon
Word tags:
A-ABS
A-CMP
A-SUP
ABBR-GEN-SG/PL
ABBR-GEN-PL
ABBR-GEN-SG
ABBR-NOM-SG/PL
ABBR-NOM-PL
ABBR-NOM-SG
ADV-ABS
ADV-CMP
ADV-SUP
ADV-WH
BE-EN
BE-IMP
BE-INF
BE-ING
BE-PAST-BASE
BE-PAST-WAS
BE-PRES-AM
BE-PRES-ARE
BE-PRES-IS
BE-SUBJUNCTIVE
CC
CCX
CS
DET-SG/PL
DET-SG
DET-WH
DO-EN
DO-IMP
DO-INF
DO-ING
DO-PAST
DO-PRES-BASE
DO-PRES-SG3
DO-SUBJUNCTIVE
EN
HAVE-EN
HAVE-IMP
HAVE-INF
HAVE-ING
HAVE-PAST
HAVE-PRES-BASE
HAVE-PRES-SG3
HAVE-SUBJUNCTIVE
INFNIARK
ING
N-GEN-SG/PL
N-GEN-PL
N-GEN-SG
N-NOM-SG/PL
N-NOM-PL
N-NOM-SG
MEG
NUM-CARD
NUM-FRA-PL
NUM-FRA-SG
NUM-ORD
PREP
PRON
PRON-ACC
PRON-CMP
PRON-DEM-PL
PRON-DEM-SG
PRON-GEN
PRON-INTERR
PRON-NOM-SG/PL
PRON-NOM-PL
PRON-NOM-SG
PRON-REL
PRON-SUP
PRON-WH
V-AUXMOD
V-IMP
V-INF
V-PAST
V-PRES-BASE
V-PRES-SGI
V-PRES-SG2
V-PRES-SG3
V-SUBJUNCTIVE
</figure>
<page confidence="0.989674">
253
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.211592">
<title confidence="0.999699">Comparing a Linguistic and a Stochastic Tagger</title>
<author confidence="0.610997">Christer Samuelsson</author>
<affiliation confidence="0.755889">Lucent Technologies Bell Laboratories</affiliation>
<address confidence="0.999642">600 Mountain Ave, Room 2D-:339 Murray Hill, NJ 07974, USA</address>
<author confidence="0.992187">Atro Voutilainen</author>
<affiliation confidence="0.903022">Research Unit for Multilingual Language Technology</affiliation>
<address confidence="0.964595">P.O. Box 4</address>
<affiliation confidence="0.93835">FIN-00014 University of Helsinki Finland</affiliation>
<address confidence="0.516095">Atro.Voutilainen0Helsinki.FI</address>
<abstract confidence="0.998253466666666">Concerning different approaches to automatic PoS tagging: EngCG-2, a constraintbased morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambiguation task using a common tag set. The experiments show that for the same amount of remaining ambiguity, the error rate of the statistical tagger is one order of magnitude greater than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Simmons</author>
</authors>
<title>Sbornik praci:</title>
<date>1992</date>
<booktitle>In Honor of Henry Kueera. Michigan Slavic Studies,</booktitle>
<marker>Simmons, 1992</marker>
<rawString>Simmons (ed.), Sbornik praci: In Honor of Henry Kueera. Michigan Slavic Studies, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>P Sibun</author>
</authors>
<title>A Practical Part-of-Speech Tagger.</title>
<date>1992</date>
<booktitle>In Procs. 3rd Conference on Applied Natural Language Processing,</booktitle>
<pages>133--140</pages>
<location>ACL,</location>
<contexts>
<context position="1279" citStr="Cutting et al. 1992" startWordPosition="178" endWordPosition="181">ity, the error rate of the statistical tagger is one order of magnitude greater than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed. 1 Introduction There are currently two main methods for automatic part-of-speech tagging. The prevailing one uses essentially statistical language models automatically derived from usually hand-annotated corpora. These corpus-based models can be represented e.g. as collocational matrices (Garside et al. (eds.) 1987: Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994). Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text, using tag sets ranging from some dozens to about 130 tags. The less popular approach is based on hand-coded linguistic rules. Pioneering work was done in the 1960&apos;s (e.g. Greene and Rubin 1971). Recently, new interest in the linguistic approach has been shown e.g. in the work of (Karlsson 1990: Voutilainen et al. 1992; Oflazer and Kuril&amp; 1994: Chanod and Tapanainen 1995: Karlsson </context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>D. Cutting, J. Kupiec, J. Pedersen and P. Sibun. 1992. A Practical Part-of-Speech Tagger. In Procs. 3rd Conference on Applied Natural Language Processing, pp. 133-140, ACL, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J DeRose</author>
</authors>
<title>Grammatical Category Disambiguation by Statistical Optimization&amp;quot;.</title>
<date>1988</date>
<journal>In Computational Linguistics</journal>
<volume>14</volume>
<issue>1</issue>
<pages>31--39</pages>
<location>ACL,</location>
<contexts>
<context position="11749" citStr="DeRose 1988" startWordPosition="1888" endWordPosition="1889">N-NOM-SG The conversion program reduces the multipart EngCG tags into a set of 80 word tags and 17 punctuation tags (see Appendix) that retain the central linguistic characteristics of the original EngCG tag set. A reduced version of the benchmark corpus was prepared with this conversion program for the statistical tagger&apos;s use. Also EngCG&apos;s output was converted into this format to enable direct comparison with the statistical tagger. 3 The Statistical Tagger The statistical tagger used in the experiments is a classical trigram-based 11MN1 decoder of the kind described in e.g. (Church 1988), (DeRose 1988) and numerous other articles. Following conventional notation, e.g. (Rabiner 1989, pp. 272-274) and (Krenn and Samuelsson 1996. pp. 42-46), the tagger recursively calculates the a, 3, 7 and b variables for each word string position t = 1.....T and each possible state4 si : i = 1.....n: at(i) = si) ;3*(0 P(W&gt; ISt = si) 7t(i) = P(St = si 1W) = P(W; St = si) P(W) at(i) Ecet(i) • 3t(i) 6t(i) max P(S&lt;t-I,St = st;W&lt;t) Here W = = wki, . ,WT = Wk7. W&lt;t = = Wki, • • • ,Wt = Wkt W&gt;t = Wt+1 = I • • • I WT = WkT S&lt; t = Si Si, St = where St = st is the event of the tth word being emitted from state si and </context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>S. J. DeRose. 1988. &amp;quot;Grammatical Category Disambiguation by Statistical Optimization&amp;quot;. In Computational Linguistics 14(1), pp. 31-39, ACL, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N W Francis</author>
<author>H Kueera</author>
</authors>
<title>Frequency Analysis of English Usage,</title>
<date>1982</date>
<location>Houghton Mifflin, Boston,</location>
<contexts>
<context position="6939" citStr="Francis and Kueera 1982" startWordPosition="1083" endWordPosition="1086">se in a statistical tagger. &apos;An online version of EngCG-2 can be found at ht tp://www.ling.helsinki.fir avoutila/engcg-2.html. 2The first three subgrarnmars are generally highly reliable and almost all of the total grammar development time was spent on them: the last two contain rather rough heuristic constraints. 3However, for an interesting experiment suggesting otherwise, see (Chanod and Tapanainen 1995). 2 Preparation of Corpus Resources 2.1 Annotation of training corpus The stochastic tagger was trained on a sample of 357,000 words from the Brown University Corpus of Present-Day English (Francis and Kueera 1982) that was annotated using the EngCG tags. The corpus was first analysed with the EngCG lexical analyser, and then it was fully disambiguated and, when necessary, corrected by a human expert. This annotation took place a few years ago. Since then, it has been used in the development of new EngCG constraints (the present version, EngCG-2, contains about 3,600 constraints): new constraints were applied to the training corpus, and whenever a reading marked as correct was discarded, either the analysis in the corpus, or the constraint itself, was corrected. In this way, the tagging quality of the c</context>
</contexts>
<marker>Francis, Kueera, 1982</marker>
<rawString>N. W. Francis and H. Kueera. 1982. Frequency Analysis of English Usage, Houghton Mifflin, Boston, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>G Leech</author>
<author>G Sampson</author>
</authors>
<date>1987</date>
<booktitle>The Computational Analysis of English. London and</booktitle>
<location>New York: Longman,</location>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>R. Garside, G. Leech and G. Sampson (eds.). 1987. The Computational Analysis of English. London and New York: Longman, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Greene</author>
<author>G Rubin</author>
</authors>
<title>Automatic grammatical tagging of English.</title>
<date>1971</date>
<institution>Brown University,</institution>
<location>Providence,</location>
<contexts>
<context position="1689" citStr="Greene and Rubin 1971" startWordPosition="246" endWordPosition="249">y derived from usually hand-annotated corpora. These corpus-based models can be represented e.g. as collocational matrices (Garside et al. (eds.) 1987: Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994). Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text, using tag sets ranging from some dozens to about 130 tags. The less popular approach is based on hand-coded linguistic rules. Pioneering work was done in the 1960&apos;s (e.g. Greene and Rubin 1971). Recently, new interest in the linguistic approach has been shown e.g. in the work of (Karlsson 1990: Voutilainen et al. 1992; Oflazer and Kuril&amp; 1994: Chanod and Tapanainen 1995: Karlsson et al. (eds.) 1995: Voutilainen 1995). The first serious linguistic competitor to data-driven statistical taggers is the English Constraint Grammar parser. EngCG (cf. Voutilainen et al. 1992; Karlsson et al. (eds.) 1995). The tagger consists of the following sequentially applied modules: 1. Tokenisation 2. Morphological analysis (a) Lexical component (b) Rule-based guesser for unknown words 3. Resolution of</context>
</contexts>
<marker>Greene, Rubin, 1971</marker>
<rawString>B. Greene and G. Rubin. 1971. Automatic grammatical tagging of English. Brown University, Providence, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Acquiring disambiguation rules from text.</title>
<date>1989</date>
<booktitle>In Procs. 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>118--125</pages>
<location>ACL,</location>
<contexts>
<context position="1311" citStr="Hindle 1989" startWordPosition="185" endWordPosition="186">agger is one order of magnitude greater than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed. 1 Introduction There are currently two main methods for automatic part-of-speech tagging. The prevailing one uses essentially statistical language models automatically derived from usually hand-annotated corpora. These corpus-based models can be represented e.g. as collocational matrices (Garside et al. (eds.) 1987: Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994). Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text, using tag sets ranging from some dozens to about 130 tags. The less popular approach is based on hand-coded linguistic rules. Pioneering work was done in the 1960&apos;s (e.g. Greene and Rubin 1971). Recently, new interest in the linguistic approach has been shown e.g. in the work of (Karlsson 1990: Voutilainen et al. 1992; Oflazer and Kuril&amp; 1994: Chanod and Tapanainen 1995: Karlsson et al. (eds.) 1995: Voutilainen </context>
</contexts>
<marker>Hindle, 1989</marker>
<rawString>D. Hindle. 1989. Acquiring disambiguation rules from text. In Procs. 27th Annual Meeting of the Association for Computational Linguistics, pp. 118-125, ACL, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated Estimation of Markov Source Paramenters from Sparse Data&amp;quot;. Pattern Recognition in Practice:</title>
<date>1980</date>
<pages>381--397</pages>
<publisher>North</publisher>
<location>Holland,</location>
<contexts>
<context position="15189" citStr="Jelinek and Mercer 1980" startWordPosition="2536" endWordPosition="2539">or unknown words. Employing a small amount of back-off smoothing also for the known words is useful to reduce lexical tag omissions. Empirically, looking two branching points up the tree for known words, and all the way up to the root for unknown words, proved optimal. The method for blending the distributions applies equally well to smoothing the transition probabilities /NJ, i.e., the tag N-gram probabilities, and both the scheme and its application to these two tasks are described in detail in (Samuelsson 1996), where it was also shown to compare favourably to (deleted) interpolation, see (Jelinek and Mercer 1980), even when the back-off weights of the latter were optimal. The 5 variables enable finding the most probable state sequence under the HMM, from which the most likely assignment of tags to words can be directly established. This is the normal modus operandi of an HMM decoder. Using the 7 variables, we can calculate the probability of being in state si at string position t, and thus having emitted wk, from this state, conditional on the entire word string. By summing over all states that would assign the same tag to this word, the individual probability of each tag being assigned to any particu</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R. L. Mercer. 1980. &amp;quot;Interpolated Estimation of Markov Source Paramenters from Sparse Data&amp;quot;. Pattern Recognition in Practice: 381-397. North Holland, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
</authors>
<title>Constraint Grammar as a Framework for Parsing Running Text.</title>
<date>1990</date>
<booktitle>In Procs. CoLing&apos;90. In Procs. 14th International Conference on Computational Linguistics, ICCL,</booktitle>
<contexts>
<context position="1790" citStr="Karlsson 1990" startWordPosition="265" endWordPosition="266">nal matrices (Garside et al. (eds.) 1987: Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994). Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text, using tag sets ranging from some dozens to about 130 tags. The less popular approach is based on hand-coded linguistic rules. Pioneering work was done in the 1960&apos;s (e.g. Greene and Rubin 1971). Recently, new interest in the linguistic approach has been shown e.g. in the work of (Karlsson 1990: Voutilainen et al. 1992; Oflazer and Kuril&amp; 1994: Chanod and Tapanainen 1995: Karlsson et al. (eds.) 1995: Voutilainen 1995). The first serious linguistic competitor to data-driven statistical taggers is the English Constraint Grammar parser. EngCG (cf. Voutilainen et al. 1992; Karlsson et al. (eds.) 1995). The tagger consists of the following sequentially applied modules: 1. Tokenisation 2. Morphological analysis (a) Lexical component (b) Rule-based guesser for unknown words 3. Resolution of morphological ambiguities The tagger uses a two-level morphological analyser with a large lexicon an</context>
</contexts>
<marker>Karlsson, 1990</marker>
<rawString>F. Karlsson. 1990. Constraint Grammar as a Framework for Parsing Running Text. In Procs. CoLing&apos;90. In Procs. 14th International Conference on Computational Linguistics, ICCL, 1990.</rawString>
</citation>
<citation valid="true">
<title>Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text. Berlin and New York: Mouton de Gruyter,</title>
<date>1995</date>
<editor>F. Karlsson, A. Voutilainen, J. Heikkila and A. Anttila (eds.).</editor>
<contexts>
<context position="3830" citStr="(1995)" startWordPosition="575" endWordPosition="575">hile each word in the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remain unresolved. These results have been seriously questioned. One doubt concerns the notion -.correct analysis&amp;quot;. For example Church (1992) argues that linguists who manually perform the tagging task using the doubleblind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements. If this were the case, reporting accuracies above this 97% &apos;upper bound&apos; would make no sense. However, Voutilainen and Jarvinen (1995) empirically show that an interjudge agreement virtually of 100% is possible. at least with the EngCG tag set if not with the original Brown Corpus tag set. This consistent applicability of the EngCG tag set is explained by characterising it as grammatically rather than semantically motivated. 246 Another main reservation about the EngCG figures is the suspicion that, perhaps partly due to the somewhat underspecific nature of the EngCG tag set, it must be so easy to disambiguate that also a statistical tagger using the EngCG tags would reach at least as good results. This argument will be exam</context>
<context position="5901" citStr="(1995)" startWordPosition="926" endWordPosition="926">rder. What we are not addressing in this paper is the work load required for making a rule-based or a data-driven tagger. The rules in EngCG certainly took a considerable effort to write, and though at the present state of knowledge rules could be written and tested with less effort, it may well be the case that a tagger with an accuracy of 95-97% can be produced with less effort by using data-driven techniques.3 Another caveat is that EngCG alone does not resolve all ambiguities, so it cannot be compared to a typical statistical tagger if full disambiguation is required. However, Voutilainen (1995) has shown that EngCG combined with a syntactic parser produces morphologically unambiguous output with an accuracy of 99.3%, a figure clearly better than that of the statistical tagger in the experiments below (however, the test data was not the same). Before examining the statistical tagger. two practical points are addressed: the annotation of the corpora used, and the modification of the EngCG tag set for use in a statistical tagger. &apos;An online version of EngCG-2 can be found at ht tp://www.ling.helsinki.fir avoutila/engcg-2.html. 2The first three subgrarnmars are generally highly reliable</context>
<context position="8820" citStr="(1995)" startWordPosition="1385" endWordPosition="1385">ecessary. Then these manually disambiguated versions were automatically compared with each other. At this stage, about 99.3% of all analyses were identical. When the differences were collectively examined, virtually all were agreed to be due to clerical mistakes. Only in the analysis of 21 words, different (meaning-level) interpretations persisted, and even here both judges agreed the ambiguity to be genuine. One of these two corpus versions was modified to represent the consensus. and this &apos;consensus corpus&apos; was used as a benchmark in the evaluations. As explained in Voutilainen and Jarvinen (1995). this high agreement rate is due to two main factors. Firstly, distinctions based on some kind of vague semantics are avoided, which is not always case with better known tag sets. Secondly. the adopted analysis of most of the constructions where humans tend to be uncertain is documented as a collection of tag application principles in the form of a grammarian&apos;s manual (for further details, cf. Voutilainen and Jarvinen 199:5). The corpus-annotation procedure allows us to perform a text-book statistical hypothesis test. Let the null hypothesis be that any two human evaluators will necessarily d</context>
</contexts>
<marker>1995</marker>
<rawString>F. Karlsson, A. Voutilainen, J. Heikkila and A. Anttila (eds.). 1995. Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text. Berlin and New York: Mouton de Gruyter, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Krenn</author>
<author>C Samuelsson</author>
</authors>
<title>The Linguist&apos;s Guide to Statistics.</title>
<date>1996</date>
<journal>Version of April</journal>
<volume>23</volume>
<note>http://coli.uni-sb.de/-christer.</note>
<contexts>
<context position="11875" citStr="Krenn and Samuelsson 1996" startWordPosition="1904" endWordPosition="1907">gs (see Appendix) that retain the central linguistic characteristics of the original EngCG tag set. A reduced version of the benchmark corpus was prepared with this conversion program for the statistical tagger&apos;s use. Also EngCG&apos;s output was converted into this format to enable direct comparison with the statistical tagger. 3 The Statistical Tagger The statistical tagger used in the experiments is a classical trigram-based 11MN1 decoder of the kind described in e.g. (Church 1988), (DeRose 1988) and numerous other articles. Following conventional notation, e.g. (Rabiner 1989, pp. 272-274) and (Krenn and Samuelsson 1996. pp. 42-46), the tagger recursively calculates the a, 3, 7 and b variables for each word string position t = 1.....T and each possible state4 si : i = 1.....n: at(i) = si) ;3*(0 P(W&gt; ISt = si) 7t(i) = P(St = si 1W) = P(W; St = si) P(W) at(i) Ecet(i) • 3t(i) 6t(i) max P(S&lt;t-I,St = st;W&lt;t) Here W = = wki, . ,WT = Wk7. W&lt;t = = Wki, • • • ,Wt = Wkt W&gt;t = Wt+1 = I • • • I WT = WkT S&lt; t = Si Si, St = where St = st is the event of the tth word being emitted from state si and VITt = wk, is the event of the tth word being the particular word wk, that was actually observed in the word string. Note that</context>
</contexts>
<marker>Krenn, Samuelsson, 1996</marker>
<rawString>B. Krenn and C. Samuelsson. The Linguist&apos;s Guide to Statistics. Version of April 23, 1996. http://coli.uni-sb.de/-christer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G de Marcken</author>
</authors>
<title>Parsing the LOB Corpus&amp;quot;.</title>
<date>1990</date>
<booktitle>In Procs. 28th Annual Meeting of the .4ssociation for Computational Linguistics.</booktitle>
<pages>243--251</pages>
<location>ACL,</location>
<marker>de Marcken, 1990</marker>
<rawString>C. G. de Marcken. 1990. &amp;quot;Parsing the LOB Corpus&amp;quot;. In Procs. 28th Annual Meeting of the .4ssociation for Computational Linguistics. pp. 243-251, ACL, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>I Kuruoz</author>
</authors>
<title>Tagging and morphological disambiguation of Turkish text.</title>
<date>1994</date>
<booktitle>In Procs. 4th Conference on Applied Natural Language Processing. ACL,</booktitle>
<marker>Oflazer, Kuruoz, 1994</marker>
<rawString>K. Oflazer and I. Kuruoz. 1994. Tagging and morphological disambiguation of Turkish text. In Procs. 4th Conference on Applied Natural Language Processing. ACL, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition&amp;quot;.</title>
<date>1989</date>
<booktitle>In Readings in Speech Recognition,</booktitle>
<pages>267--296</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="11830" citStr="Rabiner 1989" startWordPosition="1899" endWordPosition="1900"> word tags and 17 punctuation tags (see Appendix) that retain the central linguistic characteristics of the original EngCG tag set. A reduced version of the benchmark corpus was prepared with this conversion program for the statistical tagger&apos;s use. Also EngCG&apos;s output was converted into this format to enable direct comparison with the statistical tagger. 3 The Statistical Tagger The statistical tagger used in the experiments is a classical trigram-based 11MN1 decoder of the kind described in e.g. (Church 1988), (DeRose 1988) and numerous other articles. Following conventional notation, e.g. (Rabiner 1989, pp. 272-274) and (Krenn and Samuelsson 1996. pp. 42-46), the tagger recursively calculates the a, 3, 7 and b variables for each word string position t = 1.....T and each possible state4 si : i = 1.....n: at(i) = si) ;3*(0 P(W&gt; ISt = si) 7t(i) = P(St = si 1W) = P(W; St = si) P(W) at(i) Ecet(i) • 3t(i) 6t(i) max P(S&lt;t-I,St = st;W&lt;t) Here W = = wki, . ,WT = Wk7. W&lt;t = = Wki, • • • ,Wt = Wkt W&gt;t = Wt+1 = I • • • I WT = WkT S&lt; t = Si Si, St = where St = st is the event of the tth word being emitted from state si and VITt = wk, is the event of the tth word being the particular word wk, that was ac</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. -A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition&amp;quot;. In Readings in Speech Recognition, pp. 267-296. Alex Waibel and KaiFu Lee (eds), Morgan Kaufmann, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>English for the Computer,</title>
<date>1995</date>
<publisher>University Press.</publisher>
<location>Oxford</location>
<marker>Sampson, 1995</marker>
<rawString>G. Sampson. 1995. English for the Computer, Oxford University Press. 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
</authors>
<title>Handling Sparse Data by Successive Abstraction&amp;quot;.</title>
<date>1996</date>
<booktitle>In Procs. 16th International Conference on Computational Linguistics,</booktitle>
<pages>895--900</pages>
<contexts>
<context position="11875" citStr="Samuelsson 1996" startWordPosition="1906" endWordPosition="1907">pendix) that retain the central linguistic characteristics of the original EngCG tag set. A reduced version of the benchmark corpus was prepared with this conversion program for the statistical tagger&apos;s use. Also EngCG&apos;s output was converted into this format to enable direct comparison with the statistical tagger. 3 The Statistical Tagger The statistical tagger used in the experiments is a classical trigram-based 11MN1 decoder of the kind described in e.g. (Church 1988), (DeRose 1988) and numerous other articles. Following conventional notation, e.g. (Rabiner 1989, pp. 272-274) and (Krenn and Samuelsson 1996. pp. 42-46), the tagger recursively calculates the a, 3, 7 and b variables for each word string position t = 1.....T and each possible state4 si : i = 1.....n: at(i) = si) ;3*(0 P(W&gt; ISt = si) 7t(i) = P(St = si 1W) = P(W; St = si) P(W) at(i) Ecet(i) • 3t(i) 6t(i) max P(S&lt;t-I,St = st;W&lt;t) Here W = = wki, . ,WT = Wk7. W&lt;t = = Wki, • • • ,Wt = Wkt W&gt;t = Wt+1 = I • • • I WT = WkT S&lt; t = Si Si, St = where St = st is the event of the tth word being emitted from state si and VITt = wk, is the event of the tth word being the particular word wk, that was actually observed in the word string. Note that</context>
<context position="15084" citStr="Samuelsson 1996" startWordPosition="2522" endWordPosition="2523"> all-caps words and the set of infrequent words, all of which are used to improve the estimates for unknown words. Employing a small amount of back-off smoothing also for the known words is useful to reduce lexical tag omissions. Empirically, looking two branching points up the tree for known words, and all the way up to the root for unknown words, proved optimal. The method for blending the distributions applies equally well to smoothing the transition probabilities /NJ, i.e., the tag N-gram probabilities, and both the scheme and its application to these two tasks are described in detail in (Samuelsson 1996), where it was also shown to compare favourably to (deleted) interpolation, see (Jelinek and Mercer 1980), even when the back-off weights of the latter were optimal. The 5 variables enable finding the most probable state sequence under the HMM, from which the most likely assignment of tags to words can be directly established. This is the normal modus operandi of an HMM decoder. Using the 7 variables, we can calculate the probability of being in state si at string position t, and thus having emitted wk, from this state, conditional on the entire word string. By summing over all states that wou</context>
</contexts>
<marker>Samuelsson, 1996</marker>
<rawString>C. Samuelsson. 1996. &amp;quot;Handling Sparse Data by Successive Abstraction&amp;quot;. In Procs. 16th International Conference on Computational Linguistics, pp. 895-900, ICCL, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Part-of-speech tagging with neural networks.</title>
<date>1994</date>
<booktitle>In Procs. 15th International Conference on Computational Linguistics,</booktitle>
<pages>172--176</pages>
<contexts>
<context position="1350" citStr="Schmid 1994" startWordPosition="192" endWordPosition="193"> than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed. 1 Introduction There are currently two main methods for automatic part-of-speech tagging. The prevailing one uses essentially statistical language models automatically derived from usually hand-annotated corpora. These corpus-based models can be represented e.g. as collocational matrices (Garside et al. (eds.) 1987: Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994). Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text, using tag sets ranging from some dozens to about 130 tags. The less popular approach is based on hand-coded linguistic rules. Pioneering work was done in the 1960&apos;s (e.g. Greene and Rubin 1971). Recently, new interest in the linguistic approach has been shown e.g. in the work of (Karlsson 1990: Voutilainen et al. 1992; Oflazer and Kuril&amp; 1994: Chanod and Tapanainen 1995: Karlsson et al. (eds.) 1995: Voutilainen 1995). The first serious linguistic com</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Part-of-speech tagging with neural networks. In Procs. 15th International Conference on Computational Linguistics, pp. 172-176, ICCL, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen</author>
</authors>
<title>The Constraint Grammar Parser CG-2.</title>
<date>1996</date>
<tech>Publ. 27,</tech>
<institution>Dept. General Linguistics, University of Helsinki,</institution>
<contexts>
<context position="22414" citStr="Tapanainen 1996" startWordPosition="3731" endWordPosition="3732"> information sources as those available in the Constraint Grammar framework constitutes future work. Acknowledgements Though Voutilainen is the main author of the EngCG-2 tagger, the development of the system has benefited from several other contributions too. Fred Karlsson proposed the Constraint Grammar framework in the late 1980s. Juha Heikkila and Timo Jarvinen contributed with their work on English morphology and lexicon. Kimmo Koskenniemi wrote the software for morphological analysis. Pasi Tapanainen has written various implementations of the CG parser, including the recent CG-2 parser (Tapanainen 1996). The quality of the investigation and presentation was boosted by a number of suggestions to improvements and (often sceptical) comments from numerous ACL reviewers and UPenn associates, in particular from Mark Liberman. References J-P Chanod and P. Tapanainen. 1995. Tagging French: comparing a statistical and a constraintbased method. In Procs. 7th Conference of the European Chapter of the Association for Computational Linguistics, pp. 149-157, ACL, 1995. K. W. Church. 1988. &amp;quot;A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text&amp;quot;. In Procs. 2nd Conference on Applied Natural</context>
</contexts>
<marker>Tapanainen, 1996</marker>
<rawString>P. Tapanainen. 1996. The Constraint Grammar Parser CG-2. Publ. 27, Dept. General Linguistics, University of Helsinki, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen</author>
<author>A Voutilainen</author>
</authors>
<title>Tagging accurately — don&apos;t guess if you know.</title>
<date>1994</date>
<booktitle>In Procs. 4th Conference on Applied Natural Language Processing, ACL,</booktitle>
<contexts>
<context position="3146" citStr="Tapanainen and Voutilainen 1994" startWordPosition="464" endWordPosition="468">ich each word gets 1.7-2.2 different analyses on an average. Morphological analyses are assigned to unknown words with an accurate rulebased &apos;guesser&apos;. The morphological disambiguator uses constraint rules that discard illegitimate morphological analyses on the basis of local or global context conditions. The rules can be grouped as ordered subgrammars: e.g. heuristic subgrammar 2 can be applied for resolving ambiguities left pending by the more &apos;careful&apos; subgrammar 1. Older versions of EngCG (using about 1,150 constraints) are reported (Voutilainen et al. 1992; Voutilainen and Heikkila 1994; Tapanainen and Voutilainen 1994; Voutilainen 1995) to assign a correct analysis to about 99.7% of all words while each word in the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remain unresolved. These results have been seriously questioned. One doubt concerns the notion -.correct analysis&amp;quot;. For example Church (1992) argues that linguists who manually perform the tagging task using the doubleblind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements. If this were the case, reporting accuracies above </context>
</contexts>
<marker>Tapanainen, Voutilainen, 1994</marker>
<rawString>P. Tapanainen and A. Voutilainen. 1994. Tagging accurately — don&apos;t guess if you know. In Procs. 4th Conference on Applied Natural Language Processing, ACL, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
</authors>
<title>A syntax-based part of speech analyser&amp;quot;.</title>
<date>1995</date>
<booktitle>In Procs. 7th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>157--164</pages>
<location>ACL,</location>
<contexts>
<context position="1916" citStr="Voutilainen 1995" startWordPosition="284" endWordPosition="286">Hindle 1989) and neural networks (e.g. Schmid 1994). Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text, using tag sets ranging from some dozens to about 130 tags. The less popular approach is based on hand-coded linguistic rules. Pioneering work was done in the 1960&apos;s (e.g. Greene and Rubin 1971). Recently, new interest in the linguistic approach has been shown e.g. in the work of (Karlsson 1990: Voutilainen et al. 1992; Oflazer and Kuril&amp; 1994: Chanod and Tapanainen 1995: Karlsson et al. (eds.) 1995: Voutilainen 1995). The first serious linguistic competitor to data-driven statistical taggers is the English Constraint Grammar parser. EngCG (cf. Voutilainen et al. 1992; Karlsson et al. (eds.) 1995). The tagger consists of the following sequentially applied modules: 1. Tokenisation 2. Morphological analysis (a) Lexical component (b) Rule-based guesser for unknown words 3. Resolution of morphological ambiguities The tagger uses a two-level morphological analyser with a large lexicon and a morphological description that introduces about 180 different ambiguity-forming morphological analyses, as a result of whi</context>
<context position="3165" citStr="Voutilainen 1995" startWordPosition="469" endWordPosition="470">ent analyses on an average. Morphological analyses are assigned to unknown words with an accurate rulebased &apos;guesser&apos;. The morphological disambiguator uses constraint rules that discard illegitimate morphological analyses on the basis of local or global context conditions. The rules can be grouped as ordered subgrammars: e.g. heuristic subgrammar 2 can be applied for resolving ambiguities left pending by the more &apos;careful&apos; subgrammar 1. Older versions of EngCG (using about 1,150 constraints) are reported (Voutilainen et al. 1992; Voutilainen and Heikkila 1994; Tapanainen and Voutilainen 1994; Voutilainen 1995) to assign a correct analysis to about 99.7% of all words while each word in the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remain unresolved. These results have been seriously questioned. One doubt concerns the notion -.correct analysis&amp;quot;. For example Church (1992) argues that linguists who manually perform the tagging task using the doubleblind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements. If this were the case, reporting accuracies above this 97% &apos;upper bou</context>
<context position="5901" citStr="Voutilainen (1995)" startWordPosition="925" endWordPosition="926">ats are in order. What we are not addressing in this paper is the work load required for making a rule-based or a data-driven tagger. The rules in EngCG certainly took a considerable effort to write, and though at the present state of knowledge rules could be written and tested with less effort, it may well be the case that a tagger with an accuracy of 95-97% can be produced with less effort by using data-driven techniques.3 Another caveat is that EngCG alone does not resolve all ambiguities, so it cannot be compared to a typical statistical tagger if full disambiguation is required. However, Voutilainen (1995) has shown that EngCG combined with a syntactic parser produces morphologically unambiguous output with an accuracy of 99.3%, a figure clearly better than that of the statistical tagger in the experiments below (however, the test data was not the same). Before examining the statistical tagger. two practical points are addressed: the annotation of the corpora used, and the modification of the EngCG tag set for use in a statistical tagger. &apos;An online version of EngCG-2 can be found at ht tp://www.ling.helsinki.fir avoutila/engcg-2.html. 2The first three subgrarnmars are generally highly reliable</context>
</contexts>
<marker>Voutilainen, 1995</marker>
<rawString>A. Voutilainen. 1995. &amp;quot;A syntax-based part of speech analyser&amp;quot;. In Procs. 7th Conference of the European Chapter of the Association for Computational Linguistics, pp. 157-164, ACL, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
<author>J Heikkila</author>
</authors>
<title>An English constraint grammar (EngCG): a surface-syntactic parser of English.</title>
<date>1994</date>
<booktitle>In Fries, Tottie and Schneider (eds.), Creating and using English language corpora, Rodopi,</booktitle>
<contexts>
<context position="3113" citStr="Voutilainen and Heikkila 1994" startWordPosition="459" endWordPosition="463">cal analyses, as a result of which each word gets 1.7-2.2 different analyses on an average. Morphological analyses are assigned to unknown words with an accurate rulebased &apos;guesser&apos;. The morphological disambiguator uses constraint rules that discard illegitimate morphological analyses on the basis of local or global context conditions. The rules can be grouped as ordered subgrammars: e.g. heuristic subgrammar 2 can be applied for resolving ambiguities left pending by the more &apos;careful&apos; subgrammar 1. Older versions of EngCG (using about 1,150 constraints) are reported (Voutilainen et al. 1992; Voutilainen and Heikkila 1994; Tapanainen and Voutilainen 1994; Voutilainen 1995) to assign a correct analysis to about 99.7% of all words while each word in the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remain unresolved. These results have been seriously questioned. One doubt concerns the notion -.correct analysis&amp;quot;. For example Church (1992) argues that linguists who manually perform the tagging task using the doubleblind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements. If this were the </context>
</contexts>
<marker>Voutilainen, Heikkila, 1994</marker>
<rawString>A. Voutilainen and J. Heikkila. 1994. An English constraint grammar (EngCG): a surface-syntactic parser of English. In Fries, Tottie and Schneider (eds.), Creating and using English language corpora, Rodopi, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
<author>J Heikkila</author>
<author>A Anttila</author>
</authors>
<title>Constraint Grammar of English. A PerformanceOriented Introduction.</title>
<date>1992</date>
<tech>Publ. 21,</tech>
<institution>Dept. General Linguistics, University of Helsinki,</institution>
<contexts>
<context position="1815" citStr="Voutilainen et al. 1992" startWordPosition="267" endWordPosition="270">arside et al. (eds.) 1987: Church 1988), Hidden Markov models (cf. Cutting et al. 1992), local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994). Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text, using tag sets ranging from some dozens to about 130 tags. The less popular approach is based on hand-coded linguistic rules. Pioneering work was done in the 1960&apos;s (e.g. Greene and Rubin 1971). Recently, new interest in the linguistic approach has been shown e.g. in the work of (Karlsson 1990: Voutilainen et al. 1992; Oflazer and Kuril&amp; 1994: Chanod and Tapanainen 1995: Karlsson et al. (eds.) 1995: Voutilainen 1995). The first serious linguistic competitor to data-driven statistical taggers is the English Constraint Grammar parser. EngCG (cf. Voutilainen et al. 1992; Karlsson et al. (eds.) 1995). The tagger consists of the following sequentially applied modules: 1. Tokenisation 2. Morphological analysis (a) Lexical component (b) Rule-based guesser for unknown words 3. Resolution of morphological ambiguities The tagger uses a two-level morphological analyser with a large lexicon and a morphological descrip</context>
<context position="3082" citStr="Voutilainen et al. 1992" startWordPosition="455" endWordPosition="458">iguity-forming morphological analyses, as a result of which each word gets 1.7-2.2 different analyses on an average. Morphological analyses are assigned to unknown words with an accurate rulebased &apos;guesser&apos;. The morphological disambiguator uses constraint rules that discard illegitimate morphological analyses on the basis of local or global context conditions. The rules can be grouped as ordered subgrammars: e.g. heuristic subgrammar 2 can be applied for resolving ambiguities left pending by the more &apos;careful&apos; subgrammar 1. Older versions of EngCG (using about 1,150 constraints) are reported (Voutilainen et al. 1992; Voutilainen and Heikkila 1994; Tapanainen and Voutilainen 1994; Voutilainen 1995) to assign a correct analysis to about 99.7% of all words while each word in the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remain unresolved. These results have been seriously questioned. One doubt concerns the notion -.correct analysis&amp;quot;. For example Church (1992) argues that linguists who manually perform the tagging task using the doubleblind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial d</context>
</contexts>
<marker>Voutilainen, Heikkila, Anttila, 1992</marker>
<rawString>A. Voutilainen, J. Heikkila and A. Anttila. 1992. Constraint Grammar of English. A PerformanceOriented Introduction. Publ. 21, Dept. General Linguistics, University of Helsinki, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
<author>T Jarvinen</author>
</authors>
<title>Specifying a shallow grammatical representation for parsing purposes&amp;quot;.</title>
<date>1995</date>
<booktitle>In Procs. 7th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>210--214</pages>
<location>ACL,</location>
<contexts>
<context position="3830" citStr="Voutilainen and Jarvinen (1995)" startWordPosition="572" endWordPosition="575">bout 99.7% of all words while each word in the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remain unresolved. These results have been seriously questioned. One doubt concerns the notion -.correct analysis&amp;quot;. For example Church (1992) argues that linguists who manually perform the tagging task using the doubleblind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements. If this were the case, reporting accuracies above this 97% &apos;upper bound&apos; would make no sense. However, Voutilainen and Jarvinen (1995) empirically show that an interjudge agreement virtually of 100% is possible. at least with the EngCG tag set if not with the original Brown Corpus tag set. This consistent applicability of the EngCG tag set is explained by characterising it as grammatically rather than semantically motivated. 246 Another main reservation about the EngCG figures is the suspicion that, perhaps partly due to the somewhat underspecific nature of the EngCG tag set, it must be so easy to disambiguate that also a statistical tagger using the EngCG tags would reach at least as good results. This argument will be exam</context>
<context position="8820" citStr="Voutilainen and Jarvinen (1995)" startWordPosition="1382" endWordPosition="1385">ion of the tag set when necessary. Then these manually disambiguated versions were automatically compared with each other. At this stage, about 99.3% of all analyses were identical. When the differences were collectively examined, virtually all were agreed to be due to clerical mistakes. Only in the analysis of 21 words, different (meaning-level) interpretations persisted, and even here both judges agreed the ambiguity to be genuine. One of these two corpus versions was modified to represent the consensus. and this &apos;consensus corpus&apos; was used as a benchmark in the evaluations. As explained in Voutilainen and Jarvinen (1995). this high agreement rate is due to two main factors. Firstly, distinctions based on some kind of vague semantics are avoided, which is not always case with better known tag sets. Secondly. the adopted analysis of most of the constructions where humans tend to be uncertain is documented as a collection of tag application principles in the form of a grammarian&apos;s manual (for further details, cf. Voutilainen and Jarvinen 199:5). The corpus-annotation procedure allows us to perform a text-book statistical hypothesis test. Let the null hypothesis be that any two human evaluators will necessarily d</context>
</contexts>
<marker>Voutilainen, Jarvinen, 1995</marker>
<rawString>A. Voutilainen and T. Jarvinen. &amp;quot;Specifying a shallow grammatical representation for parsing purposes&amp;quot;. In Procs. 7th Conference of the European Chapter of the Association for Computational Linguistics, pp. 210-214, ACL, 1995.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>