<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002516">
<title confidence="0.915923">
CFILT: Resource Conscious Approaches for All-Words Domain Specific
WSD
</title>
<author confidence="0.985056">
Anup Kulkarni Mitesh M. Khapra Saurabh Sohoney Pushpak Bhattacharyya
</author>
<affiliation confidence="0.92455075">
Department of Computer Science and Engineering,
Indian Institute of Technology Bombay,
Powai, Mumbai 400076,
India
</affiliation>
<email confidence="0.990605">
{anup,miteshk,saurabhsohoney,pb}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.993675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998776142857143">
We describe two approaches for All-words
Word Sense Disambiguation on a Spe-
cific Domain. The first approach is a
knowledge based approach which extracts
domain-specific largest connected com-
ponents from the Wordnet graph by ex-
ploiting the semantic relations between all
candidate synsets appearing in a domain-
specific untagged corpus. Given a test
word, disambiguation is performed by
considering only those candidate synsets
that belong to the top-k largest connected
components.
The second approach is a weakly super-
vised approach which relies on the “One
Sense Per Domain” heuristic and uses a
few hand labeled examples for the most
frequently appearing words in the target
domain. Once the most frequent words
have been disambiguated they can pro-
vide strong clues for disambiguating other
words in the sentence using an iterative
disambiguation algorithm. Our weakly
supervised system gave the best perfor-
mance across all systems that participated
in the task even when it used as few as 100
hand labeled examples from the target do-
main.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995782">
Domain specific WSD exhibits high level of ac-
curacy even for the all-words scenario (Khapra et
al., 2010) - provided training and testing are on the
same domain. However, the effort of creating the
training corpus - annotated sense marked corpora
- for every domain of interest has always been a
matter of concern. Therefore, attempts have been
made to develop unsupervised (McCarthy et al.,
2007; Koeling et al., 2005) and knowledge based
techniques (Agirre et al., 2009) for WSD which
do not need sense marked corpora. However, such
approaches have not proved effective, since they
typically do not perform better than the Wordnet
first sense baseline accuracy in the all-words sce-
nario.
Motivated by the desire to develop annotation-
lean all-words domain specific techniques for
WSD we propose two resource conscious ap-
proaches. The first approach is a knowledge based
approach which focuses on retaining only domain
specific synsets in the Wordnet using a two step
pruning process. In the first step, the Wordnet
graph is restricted to only those synsets which
contain words appearing in an untagged domain-
specific corpus. In the second step, the graph is
pruned further by retaining only the largest con-
nected components of the pruned graph. Each tar-
get word in a given sentence is then disambiguated
using an iterative disambiguation process by con-
sidering only those candidate synsets which ap-
pear in the top-k largest connected components.
Our knowledge based approach performed better
than current state of the art knowledge based ap-
proach (Agirre et al., 2009). Also, the precision
was better than the Wordnet first sense baseline
even though the F-score was slightly lower than
the baseline.
The second approach is a weakly supervised ap-
proach which uses a few hand labeled examples
for the most frequent words in the target domain
in addition to the publicly available mixed-domain
SemCor (Miller et al., 1993) corpus. The underly-
ing assumption is that words exhibit “One Sense
Per Domain” phenomenon and hence even as few
as 5 training examples per word would be suffi-
cient to identify the predominant sense of the most
frequent words in the target domain. Further, once
the most frequent words have been disambiguated
using the predominant sense, they can provide
strong clues for disambiguating other words in the
</bodyText>
<page confidence="0.983927">
421
</page>
<subsubsectionHeader confidence="0.3248">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 421–426,
</subsubsectionHeader>
<bodyText confidence="0.9734024">
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
sentence. Our weakly supervised system gave the
best performance across all systems that partici-
pated in the task even when it used as few as 100
hand labeled examples from the target domain.
The remainder of this paper is organized as fol-
lows. In section 2 we describe related work on
domain-specific WSD. In section 3 we discuss an
Iterative Word Sense Disambiguation algorithm
which lies at the heart of both our approaches. In
section 4 we describe our knowledge based ap-
proach. In section 5 we describe our weakly su-
pervised approach. In section 6 we present results
and discussions followed by conclusion in section
7.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999990163265307">
There are two important lines of work for do-
main specific WSD. The first focuses on target
word specific WSD where the results are reported
on a handful of target words (41-191 words) on
three lexical sample datasets, viz., DSO corpus
(Ng and Lee, 1996), MEDLINE corpus (Weeber et
al., 2001) and the corpus of Koeling et al. (2005).
The second focuses on all-words domain specific
WSD where the results are reported on large anno-
tated corpora from two domains, viz., TOURISM
and HEALTH (Khapra et al., 2010).
In the target word setting, it has been shown that
unsupervised methods (McCarthy et al., 2007) and
knowledge based methods (Agirre et al., 2009)
can do better than wordnet first sense baseline and
in some cases can also outperform supervised ap-
proaches. However, since these systems have been
tested only for certain target words, the question of
their utility in all words WSD it still open .
In the all words setting, Khapra et al. (2010)
have shown significant improvements over the
wordnet first sense baseline using a fully super-
vised approach. However, the need for sense anno-
tated corpus in the domain of interest is a matter of
concern and provides motivation for adapting their
approach to annotation scarce scenarios. Here, we
take inspiration from the target-word specific re-
sults reported by Chan and Ng (2007) where by
using just 30% of the target data they obtained the
same performance as that obtained by using the
entire target data.
We take the fully supervised approach of
(Khapra et al., 2010) and convert it to a weakly su-
pervised approach by using only a handful of hand
labeled examples for the most frequent words ap-
pearing in the target domain. For the remaining
words we use the sense distributions learnt from
SemCor (Miller et al., 1993) which is a publicly
available mixed domain corpus. Our approach is
thus based on the “annotate-little from the target
domain” paradigm and does better than all the sys-
tems that participated in the shared task.
Even our knowledge based approach does better
than current state of the art knowledge based ap-
proaches (Agirre et al., 2009). Here, we use an un-
tagged corpus to prune the Wordnet graph thereby
reducing the number of candidate synsets for each
target word. To the best of our knowledge such an
approach has not been tried earlier.
</bodyText>
<sectionHeader confidence="0.997127" genericHeader="method">
3 Iterative Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.9998472">
The Iterative Word Sense Disambiguation (IWSD)
algorithm proposed by Khapra et al. (2010) lies at
the heart of both our approaches. They use a scor-
ing function which combines corpus based param-
eters (such as, sense distributions and corpus co-
occurrence) and Wordnet based parameters (such
as, semantic similarity, conceptual distance, etc.)
for ranking the candidates synsets of a word. The
algorithm is iterative in nature and involves the
following steps:
</bodyText>
<listItem confidence="0.997023142857143">
• Tag all monosemous words in the sentence.
• Iteratively disambiguate the remaining words
in the sentence in increasing order of their de-
gree of polysemy.
• At each stage rank the candidate senses of a
word using the scoring function of Equation
(1).
</listItem>
<equation confidence="0.969309083333333">
�
S∗ = arg max(OiVi +
i
j∈J
where,
i E Candidate Synsets
J = Set of disambiguated words
Oi = BelongingnessToDominantConcept(Si)
Vi = P(Si|word)
Wij = CorpusCooccurrence(Si, Sj)
* 1/WNConceptualDistance(Si, Sj)
* 1/WNSemanticGraphDistance(Si, Sj)
</equation>
<bodyText confidence="0.99417">
The scoring function as given above cleanly
separates the self-merit of a synset (P(Si|word))
</bodyText>
<equation confidence="0.941472">
Wij * Vi * Vj) (1)
</equation>
<page confidence="0.985293">
422
</page>
<bodyText confidence="0.999991136363636">
as learnt from a tagged corpus and its interaction-
merit in the form of corpus co-occurrence, con-
ceptual distance, and wordnet-based semantic dis-
tance with the senses of other words in the sen-
tence. The scoring function can thus be easily
adapted depending upon the amount of informa-
tion available. For example, in the weakly su-
pervised setting, P(Si|word) will be available for
some words for which either manually hand la-
beled training data from environment domain is
used or which appear in the SemCor corpus. For
such words, all the parameters in Equation (1) will
be used for scoring the candidate synsets and for
remaining words only the interaction parameters
will be used. Similarly, in the knowledge based
setting, P(Si|word) will never be available and
hence only the wordnet based interaction parame-
ters (i.e., WNConceptualDistance(Si, Sj) and
WNSemanticGraphDistance(Si, Sj)) will be
used for scoring the pruned list of candidate
synsets. Please refer to (Khapra et al., 2010) for
the details of how each parameter is calculated.
</bodyText>
<sectionHeader confidence="0.9198275" genericHeader="method">
4 Knowledge-Based WSD using Graph
Pruning
</sectionHeader>
<bodyText confidence="0.999333532258064">
Wordnet can be viewed as a graph where synsets
act as nodes and the semantic relations between
them act as edges. It should be easy to see
that given a domain-specific corpus, synsets from
some portions of this graph would be more likely
to occur than synsets from other portions. For
example, given a corpus from the HEALTH do-
main one might expect synsets belonging to the
sub-trees of “doctor”, “medicine”, “disease” to
appear more frequently than the synsets belonging
to the sub-tree of “politics”. Such dominance ex-
hibited by different components can be harnessed
for domain-specific WSD and is the motivation for
our work.
The crux of the approach is to identify such do-
main specific components using a two step prun-
ing process as described below:
Step 1: First, we use an untagged corpus from
the environment domain to identify the unique
words appearing in the domain. Note that, by
unique words we mean all content words which
appear at least once in the environment corpus
(these words may or may not appear in a gen-
eral mixed domain corpus). This untagged corpus
containing 15 documents (22K words) was down-
loaded from the websites of WWF1 and ECNC2
and contained articles on Climate Change, De-
forestation, Species Extinction, Marine Life and
Ecology. Once the unique words appearing in
this environment-specific corpus are identified, we
restrict the Wordnet graph to only those synsets
which contain one or more of these unique words
as members. This step thus eliminates all spurious
synsets which are not related to the environment
domain.
Step 2: In the second step, we perform a Breadth-
First-Search on the pruned graph to identify the
connected components of the graph. While
traversing the graph we consider only those edges
which correspond to the hypernymy-hyponymy re-
lation and ignore all other semantic relations as we
observed that such relations add noise to the com-
ponents. The top-5 largest components thus iden-
tified were considered to be environment-specific
components. A subset of synsets appearing in one
such sample component is listed in Table 1.
Each target word in a given sentence is then disam-
biguated using the IWSD algorithm described in
section 3. However, now the arg max of Equation
(1) is computed only over those candidate synsets
which belong to the top-5 largest components and
all other candidate synsets are ignored. The sug-
gested pruning technique is indeed very harsh and
as a result there are many words for which none
of their candidate synsets belong to these top-5
largest components. These are typically domain-
invariant words for which pruning does not make
sense as the synsets of such generic words do
not belong to domain-specific components of the
Wordnet graph. In such cases, we consider all the
candidate synsets of these words while computing
the arg max of Equation (1).
</bodyText>
<sectionHeader confidence="0.980836" genericHeader="method">
5 Weakly Supervised WSD
</sectionHeader>
<bodyText confidence="0.9999742">
Words are known to exhibit “One Sense Per Do-
main”. For example, in the HEALTH domain the
word cancer will invariably occur in the disease
sense and almost never in the sense of a zodiac
sign. This is especially true for the most frequently
appearing nouns in the domain as these are typi-
cally domain specific nouns. For example, nouns
such as farmer, species, population, conservation,
nature, etc. appear very frequently in the envi-
ronment domain and exhibit a clear predominant
</bodyText>
<footnote confidence="0.999993">
1http://www.wwf.org
2http://www.ecnc.org
</footnote>
<page confidence="0.999338">
423
</page>
<bodyText confidence="0.958685666666667">
{ safety} - NOUN - the state of being certain that adverse effects will not be caused by some agent
under defined conditions; ”insure the safety of the children”; ”the reciprocal of safety is risk”
{preservation, saving} - NOUN - the activity of protecting something from loss or danger
{environment} - NOUN - the totality of surrounding conditions; ”he longed for the comfortable
environment of his living room”
{animation, life, living, aliveness} - NOUN - the condition of living or the state of being alive;
”while there’s life there’s hope”; ”life depends on many chemical and physical processes”
{renovation, restoration, refurbishment} - NOUN - the state of being restored to its former good
condition; ”the inn was a renovation of a Colonial house”
{ecology} - NOUN - the environment as it relates to living organisms; ”it changed the ecology of
the island”
{development} - NOUN - a state in which things are improving; the result of developing (as in the
early part of a game of chess); ”after he saw the latest development he changed his mind and be-
came a supporter”; ”in chess your should take care of your development before moving your queen”
{survival, endurance} - NOUN - a state of surviving; remaining alive
</bodyText>
<equation confidence="0.8541525">
. . . . . . . . . . . .
. . . . . . . . . . . .
</equation>
<tableCaption confidence="0.982666">
Table 1: Environment specific component identified after pruning
</tableCaption>
<bodyText confidence="0.999823368421053">
sense in the domain. As a result as few as 5 hand
labeled examples per noun are sufficient for find-
ing the predominant sense of these nouns. Further,
once these most frequently occurring nouns have
been disambiguated they can help in disambiguat-
ing other words in the sentence by contributing to
the interaction-merit of Equation (1) (note that in
Equation (1), J = Set of disambiguated words).
Based on the above intuition, we slightly mod-
ified the IWSD algorithm and converted it to a
weakly supervised algorithm. The original algo-
rithm as described in section 3 uses monosemous
words as seed input (refer to the first step of the al-
gorithm). Instead, we use the most frequently ap-
pearing nouns as the seed input. These nouns are
disambiguated using their pre-dominant sense as
calculated from the hand labeled examples. Our
weakly supervised IWSD algorithm can thus be
summarized as follows
</bodyText>
<listItem confidence="0.817289923076923">
• If a word w in a test sentence belongs to
the list of most frequently appearing domain-
specific nouns then disambiguate it first us-
ing its self-merit (i.e., P(Si|word)) as learnt
from the hand labeled examples.
• Iteratively disambiguate the remaining words
in the sentence in increasing order of their de-
gree of polysemy.
• While disambiguating the remaining words
rank the candidate senses of a word using
the self-merit learnt from SemCor and the
interaction-merit based on previously disam-
biguated words.
</listItem>
<bodyText confidence="0.99979475">
The most frequent words and the corresponding
examples to be hand labeled are extracted from the
same 15 documents (22K words) as described in
section 4.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.998151272727273">
We report the performance of our systems in the
SEMEVAL task on All-words Word Sense Dis-
ambiguation on a Specific Domain (Agirre et al.,
2010). The task involved sense tagging 1398
nouns and verbs from 3 documents extracted from
the environment domain. We submitted one run
for the knowledge based system and 2 runs for the
weakly supervised system. For the weakly super-
vised system, in one run we used 5 training ex-
amples each for the 80 most frequently appear-
ing nouns in the domain and in the second run we
</bodyText>
<page confidence="0.997836">
424
</page>
<bodyText confidence="0.999965368421053">
used 5 training examples each for the 200 most
frequently appearing nouns. Both our submis-
sions in the weakly supervised setting performed
better than all other systems that participated in
the shared task. Post-submission we even exper-
imented with using 5 training examples each for
as few as 20 most frequent nouns and even in
this case we found that our weakly supervised sys-
tem performed better than all other systems that
participated in the shared task.
The precision of our knowledge based system
was slightly better than the most frequent sense
(MFS) baseline reported by the task organizers
but the recall was slightly lower than the baseline.
Also, our approach does better than the current
state of the art knowledge based approach (Person-
alized Page Rank approach of Agirre et al. (2009)).
All results are summarized in Table 2. The fol-
lowing guide specifies the systems reported:
</bodyText>
<listItem confidence="0.997555636363636">
• WS-k: Weakly supervised approach using 5
training examples for the k most frequently
appearing nouns in the environment domain.
• KB: Knowledge based approach using graph
based pruning.
• PPR: Personalized PageRank approach of
Agirre et al. (2009).
• MFS: Most Frequent Sense baseline pro-
vided by the task organizers.
• Random: Random baseline provided by the
task organizers.
</listItem>
<table confidence="0.998971875">
System Precision Recall Rank in shared task
WS-200 0.570 0.555 1
WS-80 0.554 0.540 2
WS-20 0.548 0.535 3 (Post submission)
KB 0.512 0.495 7
PPR 0.373 0.368 24 (Post submission)
MFS 0.505 0.505 6
Random 0.23 0.23 30
</table>
<tableCaption confidence="0.9155365">
Table 2: The performance of our systems in the
shared task
</tableCaption>
<bodyText confidence="0.898353333333333">
In Table 3 we provide the results of WS-200 for
each POS category. As expected, the results for
nouns are much better than those for verbs mainly
because nouns are more likely to stick to the “One
sense per domain” property than verbs.
Nouns 59.64 59.01
</bodyText>
<tableCaption confidence="0.978596">
Table 3: The performance of WS-200 on each
POS category
</tableCaption>
<sectionHeader confidence="0.994603" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999978947368421">
We presented two resource conscious approaches
for All-words Word Sense Disambiguation on a
Specific Domain. The first approach is a knowl-
edge based approach which retains only domain
specific synsets from the Wordnet by using a two
step pruning process. This approach does better
than the current state of the art knowledge based
approaches although its performance is slightly
lower than the Most Frequent Sense baseline. The
second approach which is a weakly supervised ap-
proach based on the “annotate-little from the tar-
get domain” paradigm performed better than all
systems that participated in the task even when it
used as few as 100 hand labeled examples from
the target domain. This approach establishes the
veracity of the “One sense per domain” phe-
nomenon by showing that even as few as five ex-
amples per word are sufficient for predicting the
predominant sense of a word.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999989">
We would like to thank Siva Reddy and Abhilash
Inumella (from IIIT Hyderabad, India) for provid-
ing us the results of Personalized PageRank (PPR)
for comparison.
</bodyText>
<sectionHeader confidence="0.999272" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999483333333333">
Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa.
2009. Knowledge-based wsd on specific domains:
Performing better than generic supervised wsd.
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 49–56.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
</reference>
<figure confidence="0.7185275">
Category Precision Recall
Verbs 45.37 42.89
</figure>
<page confidence="0.987899">
425
</page>
<reference confidence="0.998932129032258">
sense disambiguation combining corpus based and
wordnet based parameters. In 5th International
Conference on Global Wordnet (GWC2010).
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In HLT ’05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 419–426, Morristown, NJ, USA.
Association for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of predom-
inant word senses. Comput. Linguist., 33(4):553–
590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
HLT ’93: Proceedings of the workshop on Human
Language Technology, pages 303–308, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
40–47.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In In Proceedings ofthe
American Medical Informatics Association Annual
Symposium (AMIA 2001), pages 746–750.
</reference>
<page confidence="0.999162">
426
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.745750">
<title confidence="0.987433">CFILT: Resource Conscious Approaches for All-Words Domain Specific WSD</title>
<author confidence="0.999177">Anup Kulkarni Mitesh M Khapra Saurabh Sohoney Pushpak Bhattacharyya</author>
<affiliation confidence="0.999534">Department of Computer Science and Engineering, Indian Institute of Technology Bombay,</affiliation>
<address confidence="0.9323105">Powai, Mumbai 400076, India</address>
<abstract confidence="0.995948206896552">describe two approaches for Word Sense Disambiguation on a Spe- The first approach is a knowledge based approach which extracts domain-specific largest connected components from the Wordnet graph by exploiting the semantic relations between all candidate synsets appearing in a domainspecific untagged corpus. Given a test word, disambiguation is performed by considering only those candidate synsets belong to the connected components. The second approach is a weakly superapproach which relies on the Per heuristic and uses a few hand labeled examples for the most frequently appearing words in the target domain. Once the most frequent words have been disambiguated they can provide strong clues for disambiguating other words in the sentence using an iterative disambiguation algorithm. Our weakly system gave the perforall systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez De Lacalle, and Aitor Soroa.</title>
<date>2009</date>
<marker>Agirre, 2009</marker>
<rawString>Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa. 2009. Knowledge-based wsd on specific domains: Performing better than generic supervised wsd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez de Lacalle, Christiane Fellbaum,</title>
<date>2010</date>
<location>Shu</location>
<marker>Agirre, 2010</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, Christiane Fellbaum, Shu kai Hsieh, Maurizio Tesconi, Monica Monachini, Piek Vossen, and Roxanne Segers. 2010. Semeval-2010 task 17: All-words word sense disambiguation on a specific domain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation. In</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="5851" citStr="Chan and Ng (2007)" startWordPosition="937" endWordPosition="940"> cases can also outperform supervised approaches. However, since these systems have been tested only for certain target words, the question of their utility in all words WSD it still open . In the all words setting, Khapra et al. (2010) have shown significant improvements over the wordnet first sense baseline using a fully supervised approach. However, the need for sense annotated corpus in the domain of interest is a matter of concern and provides motivation for adapting their approach to annotation scarce scenarios. Here, we take inspiration from the target-word specific results reported by Chan and Ng (2007) where by using just 30% of the target data they obtained the same performance as that obtained by using the entire target data. We take the fully supervised approach of (Khapra et al., 2010) and convert it to a weakly supervised approach by using only a handful of hand labeled examples for the most frequent words appearing in the target domain. For the remaining words we use the sense distributions learnt from SemCor (Miller et al., 1993) which is a publicly available mixed domain corpus. Our approach is thus based on the “annotate-little from the target domain” paradigm and does better than </context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense disambiguation. In In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh Khapra</author>
<author>Sapan Shah</author>
<author>Piyush Kedia</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Domain-specific word sense disambiguation combining corpus based and wordnet based parameters.</title>
<date>2010</date>
<booktitle>In 5th International Conference on Global Wordnet (GWC2010).</booktitle>
<contexts>
<context position="1474" citStr="Khapra et al., 2010" startWordPosition="216" endWordPosition="219">ies on the “One Sense Per Domain” heuristic and uses a few hand labeled examples for the most frequently appearing words in the target domain. Once the most frequent words have been disambiguated they can provide strong clues for disambiguating other words in the sentence using an iterative disambiguation algorithm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. 1 Introduction Domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. However, the effort of creating the training corpus - annotated sense marked corpora - for every domain of interest has always been a matter of concern. Therefore, attempts have been made to develop unsupervised (McCarthy et al., 2007; Koeling et al., 2005) and knowledge based techniques (Agirre et al., 2009) for WSD which do not need sense marked corpora. However, such approaches have not proved effective, since they typically do not perform better than the Wordnet first sense baseline accuracy in the all-words scenario. Motivated by th</context>
<context position="5026" citStr="Khapra et al., 2010" startWordPosition="800" endWordPosition="803">ed approach. In section 6 we present results and discussions followed by conclusion in section 7. 2 Related Work There are two important lines of work for domain specific WSD. The first focuses on target word specific WSD where the results are reported on a handful of target words (41-191 words) on three lexical sample datasets, viz., DSO corpus (Ng and Lee, 1996), MEDLINE corpus (Weeber et al., 2001) and the corpus of Koeling et al. (2005). The second focuses on all-words domain specific WSD where the results are reported on large annotated corpora from two domains, viz., TOURISM and HEALTH (Khapra et al., 2010). In the target word setting, it has been shown that unsupervised methods (McCarthy et al., 2007) and knowledge based methods (Agirre et al., 2009) can do better than wordnet first sense baseline and in some cases can also outperform supervised approaches. However, since these systems have been tested only for certain target words, the question of their utility in all words WSD it still open . In the all words setting, Khapra et al. (2010) have shown significant improvements over the wordnet first sense baseline using a fully supervised approach. However, the need for sense annotated corpus in</context>
<context position="6962" citStr="Khapra et al. (2010)" startWordPosition="1127" endWordPosition="1130">us. Our approach is thus based on the “annotate-little from the target domain” paradigm and does better than all the systems that participated in the shared task. Even our knowledge based approach does better than current state of the art knowledge based approaches (Agirre et al., 2009). Here, we use an untagged corpus to prune the Wordnet graph thereby reducing the number of candidate synsets for each target word. To the best of our knowledge such an approach has not been tried earlier. 3 Iterative Word Sense Disambiguation The Iterative Word Sense Disambiguation (IWSD) algorithm proposed by Khapra et al. (2010) lies at the heart of both our approaches. They use a scoring function which combines corpus based parameters (such as, sense distributions and corpus cooccurrence) and Wordnet based parameters (such as, semantic similarity, conceptual distance, etc.) for ranking the candidates synsets of a word. The algorithm is iterative in nature and involves the following steps: • Tag all monosemous words in the sentence. • Iteratively disambiguate the remaining words in the sentence in increasing order of their degree of polysemy. • At each stage rank the candidate senses of a word using the scoring funct</context>
<context position="8933" citStr="Khapra et al., 2010" startWordPosition="1443" endWordPosition="1446">ilable for some words for which either manually hand labeled training data from environment domain is used or which appear in the SemCor corpus. For such words, all the parameters in Equation (1) will be used for scoring the candidate synsets and for remaining words only the interaction parameters will be used. Similarly, in the knowledge based setting, P(Si|word) will never be available and hence only the wordnet based interaction parameters (i.e., WNConceptualDistance(Si, Sj) and WNSemanticGraphDistance(Si, Sj)) will be used for scoring the pruned list of candidate synsets. Please refer to (Khapra et al., 2010) for the details of how each parameter is calculated. 4 Knowledge-Based WSD using Graph Pruning Wordnet can be viewed as a graph where synsets act as nodes and the semantic relations between them act as edges. It should be easy to see that given a domain-specific corpus, synsets from some portions of this graph would be more likely to occur than synsets from other portions. For example, given a corpus from the HEALTH domain one might expect synsets belonging to the sub-trees of “doctor”, “medicine”, “disease” to appear more frequently than the synsets belonging to the sub-tree of “politics”. S</context>
</contexts>
<marker>Khapra, Shah, Kedia, Bhattacharyya, 2010</marker>
<rawString>Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak Bhattacharyya. 2010. Domain-specific word sense disambiguation combining corpus based and wordnet based parameters. In 5th International Conference on Global Wordnet (GWC2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>419--426</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1788" citStr="Koeling et al., 2005" startWordPosition="268" endWordPosition="271">thm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. 1 Introduction Domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. However, the effort of creating the training corpus - annotated sense marked corpora - for every domain of interest has always been a matter of concern. Therefore, attempts have been made to develop unsupervised (McCarthy et al., 2007; Koeling et al., 2005) and knowledge based techniques (Agirre et al., 2009) for WSD which do not need sense marked corpora. However, such approaches have not proved effective, since they typically do not perform better than the Wordnet first sense baseline accuracy in the all-words scenario. Motivated by the desire to develop annotationlean all-words domain specific techniques for WSD we propose two resource conscious approaches. The first approach is a knowledge based approach which focuses on retaining only domain specific synsets in the Wordnet using a two step pruning process. In the first step, the Wordnet gra</context>
<context position="4850" citStr="Koeling et al. (2005)" startWordPosition="771" endWordPosition="774">ense Disambiguation algorithm which lies at the heart of both our approaches. In section 4 we describe our knowledge based approach. In section 5 we describe our weakly supervised approach. In section 6 we present results and discussions followed by conclusion in section 7. 2 Related Work There are two important lines of work for domain specific WSD. The first focuses on target word specific WSD where the results are reported on a handful of target words (41-191 words) on three lexical sample datasets, viz., DSO corpus (Ng and Lee, 1996), MEDLINE corpus (Weeber et al., 2001) and the corpus of Koeling et al. (2005). The second focuses on all-words domain specific WSD where the results are reported on large annotated corpora from two domains, viz., TOURISM and HEALTH (Khapra et al., 2010). In the target word setting, it has been shown that unsupervised methods (McCarthy et al., 2007) and knowledge based methods (Agirre et al., 2009) can do better than wordnet first sense baseline and in some cases can also outperform supervised approaches. However, since these systems have been tested only for certain target words, the question of their utility in all words WSD it still open . In the all words setting, K</context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 419–426, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Unsupervised acquisition of predominant word senses.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>4</issue>
<pages>590</pages>
<contexts>
<context position="1765" citStr="McCarthy et al., 2007" startWordPosition="264" endWordPosition="267">e disambiguation algorithm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. 1 Introduction Domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. However, the effort of creating the training corpus - annotated sense marked corpora - for every domain of interest has always been a matter of concern. Therefore, attempts have been made to develop unsupervised (McCarthy et al., 2007; Koeling et al., 2005) and knowledge based techniques (Agirre et al., 2009) for WSD which do not need sense marked corpora. However, such approaches have not proved effective, since they typically do not perform better than the Wordnet first sense baseline accuracy in the all-words scenario. Motivated by the desire to develop annotationlean all-words domain specific techniques for WSD we propose two resource conscious approaches. The first approach is a knowledge based approach which focuses on retaining only domain specific synsets in the Wordnet using a two step pruning process. In the firs</context>
<context position="5123" citStr="McCarthy et al., 2007" startWordPosition="816" endWordPosition="819">7. 2 Related Work There are two important lines of work for domain specific WSD. The first focuses on target word specific WSD where the results are reported on a handful of target words (41-191 words) on three lexical sample datasets, viz., DSO corpus (Ng and Lee, 1996), MEDLINE corpus (Weeber et al., 2001) and the corpus of Koeling et al. (2005). The second focuses on all-words domain specific WSD where the results are reported on large annotated corpora from two domains, viz., TOURISM and HEALTH (Khapra et al., 2010). In the target word setting, it has been shown that unsupervised methods (McCarthy et al., 2007) and knowledge based methods (Agirre et al., 2009) can do better than wordnet first sense baseline and in some cases can also outperform supervised approaches. However, since these systems have been tested only for certain target words, the question of their utility in all words WSD it still open . In the all words setting, Khapra et al. (2010) have shown significant improvements over the wordnet first sense baseline using a fully supervised approach. However, the need for sense annotated corpus in the domain of interest is a matter of concern and provides motivation for adapting their approac</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2007</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of predominant word senses. Comput. Linguist., 33(4):553– 590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In HLT ’93: Proceedings of the workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3290" citStr="Miller et al., 1993" startWordPosition="511" endWordPosition="514">ted using an iterative disambiguation process by considering only those candidate synsets which appear in the top-k largest connected components. Our knowledge based approach performed better than current state of the art knowledge based approach (Agirre et al., 2009). Also, the precision was better than the Wordnet first sense baseline even though the F-score was slightly lower than the baseline. The second approach is a weakly supervised approach which uses a few hand labeled examples for the most frequent words in the target domain in addition to the publicly available mixed-domain SemCor (Miller et al., 1993) corpus. The underlying assumption is that words exhibit “One Sense Per Domain” phenomenon and hence even as few as 5 training examples per word would be sufficient to identify the predominant sense of the most frequent words in the target domain. Further, once the most frequent words have been disambiguated using the predominant sense, they can provide strong clues for disambiguating other words in the 421 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 421–426, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics sentence. O</context>
<context position="6294" citStr="Miller et al., 1993" startWordPosition="1016" endWordPosition="1019">nd provides motivation for adapting their approach to annotation scarce scenarios. Here, we take inspiration from the target-word specific results reported by Chan and Ng (2007) where by using just 30% of the target data they obtained the same performance as that obtained by using the entire target data. We take the fully supervised approach of (Khapra et al., 2010) and convert it to a weakly supervised approach by using only a handful of hand labeled examples for the most frequent words appearing in the target domain. For the remaining words we use the sense distributions learnt from SemCor (Miller et al., 1993) which is a publicly available mixed domain corpus. Our approach is thus based on the “annotate-little from the target domain” paradigm and does better than all the systems that participated in the shared task. Even our knowledge based approach does better than current state of the art knowledge based approaches (Agirre et al., 2009). Here, we use an untagged corpus to prune the Wordnet graph thereby reducing the number of candidate synsets for each target word. To the best of our knowledge such an approach has not been tried earlier. 3 Iterative Word Sense Disambiguation The Iterative Word Se</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In HLT ’93: Proceedings of the workshop on Human Language Technology, pages 303–308, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach. In</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>40--47</pages>
<contexts>
<context position="4772" citStr="Ng and Lee, 1996" startWordPosition="757" endWordPosition="760">d work on domain-specific WSD. In section 3 we discuss an Iterative Word Sense Disambiguation algorithm which lies at the heart of both our approaches. In section 4 we describe our knowledge based approach. In section 5 we describe our weakly supervised approach. In section 6 we present results and discussions followed by conclusion in section 7. 2 Related Work There are two important lines of work for domain specific WSD. The first focuses on target word specific WSD where the results are reported on a handful of target words (41-191 words) on three lexical sample datasets, viz., DSO corpus (Ng and Lee, 1996), MEDLINE corpus (Weeber et al., 2001) and the corpus of Koeling et al. (2005). The second focuses on all-words domain specific WSD where the results are reported on large annotated corpora from two domains, viz., TOURISM and HEALTH (Khapra et al., 2010). In the target word setting, it has been shown that unsupervised methods (McCarthy et al., 2007) and knowledge based methods (Agirre et al., 2009) can do better than wordnet first sense baseline and in some cases can also outperform supervised approaches. However, since these systems have been tested only for certain target words, the question</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach. In In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL), pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Weeber</author>
<author>James G Mork</author>
<author>Alan R Aronson</author>
</authors>
<title>Developing a test collection for biomedical word sense disambiguation. In</title>
<date>2001</date>
<booktitle>In Proceedings ofthe American Medical Informatics Association Annual Symposium (AMIA</booktitle>
<pages>746--750</pages>
<contexts>
<context position="4810" citStr="Weeber et al., 2001" startWordPosition="763" endWordPosition="766">ection 3 we discuss an Iterative Word Sense Disambiguation algorithm which lies at the heart of both our approaches. In section 4 we describe our knowledge based approach. In section 5 we describe our weakly supervised approach. In section 6 we present results and discussions followed by conclusion in section 7. 2 Related Work There are two important lines of work for domain specific WSD. The first focuses on target word specific WSD where the results are reported on a handful of target words (41-191 words) on three lexical sample datasets, viz., DSO corpus (Ng and Lee, 1996), MEDLINE corpus (Weeber et al., 2001) and the corpus of Koeling et al. (2005). The second focuses on all-words domain specific WSD where the results are reported on large annotated corpora from two domains, viz., TOURISM and HEALTH (Khapra et al., 2010). In the target word setting, it has been shown that unsupervised methods (McCarthy et al., 2007) and knowledge based methods (Agirre et al., 2009) can do better than wordnet first sense baseline and in some cases can also outperform supervised approaches. However, since these systems have been tested only for certain target words, the question of their utility in all words WSD it </context>
</contexts>
<marker>Weeber, Mork, Aronson, 2001</marker>
<rawString>Marc Weeber, James G. Mork, and Alan R. Aronson. 2001. Developing a test collection for biomedical word sense disambiguation. In In Proceedings ofthe American Medical Informatics Association Annual Symposium (AMIA 2001), pages 746–750.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>