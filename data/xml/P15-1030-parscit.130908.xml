<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000055">
<title confidence="0.991178">
Neural CRF Parsing
</title>
<author confidence="0.999203">
Greg Durrett and Dan Klein
</author>
<affiliation confidence="0.9988375">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.995056">
{gdurrett,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997333" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982625">
This paper describes a parsing model that
combines the exact dynamic programming
of CRF parsing with the rich nonlinear fea-
turization of neural net approaches. Our
model is structurally a CRF that factors
over anchored rule productions, but in-
stead of linear potential functions based
on sparse features, we use nonlinear po-
tentials computed via a feedforward neu-
ral network. Because potentials are still
local to anchored rules, structured infer-
ence (CKY) is unchanged from the sparse
case. Computing gradients during learn-
ing involves backpropagating an error sig-
nal formed from standard CRF sufficient
statistics (expected rule counts). Us-
ing only dense features, our neural CRF
already exceeds a strong baseline CRF
model (Hall et al., 2014). In combination
with sparse features, our system1 achieves
91.1 Fs on section 23 of the Penn Tree-
bank, and more generally outperforms the
best prior single parser results on a range
of languages.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976519666666667">
Neural network-based approaches to structured
NLP tasks have both strengths and weaknesses
when compared to more conventional models,
such conditional random fields (CRFs). A key
strength of neural approaches is their ability to
learn nonlinear interactions between underlying
features. In the case of unstructured output spaces,
this capability has led to gains in problems rang-
ing from syntax (Chen and Manning, 2014; Be-
linkov et al., 2014) to lexical semantics (Kalch-
brenner et al., 2014; Kim, 2014). Neural methods
are also powerful tools in the case of structured
</bodyText>
<footnote confidence="0.816288">
1System available at http://nlp.cs.berkeley.edu
</footnote>
<bodyText confidence="0.999202463414634">
output spaces. Here, past work has often relied on
recurrent architectures (Henderson, 2003; Socher
et al., 2013; ˙Irsoy and Cardie, 2014), which can
propagate information through structure via real-
valued hidden state, but as a result do not admit ef-
ficient dynamic programming (Socher et al., 2013;
Le and Zuidema, 2014). However, there is a nat-
ural marriage of nonlinear induced features and
efficient structured inference, as explored by Col-
lobert et al. (2011) for the case of sequence mod-
eling: feedforward neural networks can be used to
score local decisions which are then “reconciled”
in a discrete structured modeling framework, al-
lowing inference via dynamic programming.
In this work, we present a CRF constituency
parser based on these principles, where individ-
ual anchored rule productions are scored based
on nonlinear features computed with a feedfor-
ward neural network. A separate, identically-
parameterized replicate of the network exists for
each possible span and split point. As input, it
takes vector representations of words at the split
point and span boundaries; it then outputs scores
for anchored rules applied to that span and split
point. These scores can be thought of as non-
linear potentials analogous to linear potentials in
conventional CRFs. Crucially, while the network
replicates are connected in a unified model, their
computations factor along the same substructures
as in standard CRFs.
Prior work on parsing using neural network
models has often sidestepped the problem of struc-
tured inference by making sequential decisions
(Henderson, 2003; Chen and Manning, 2014;
Tsuboi, 2014) or by doing reranking (Socher et
al., 2013; Le and Zuidema, 2014); by contrast, our
framework permits exact inference via CKY, since
the model’s structured interactions are purely dis-
crete and do not involve continuous hidden state.
Therefore, we can exploit a neural net’s capac-
ity to learn nonlinear features without modifying
</bodyText>
<page confidence="0.969084">
302
</page>
<note confidence="0.98474">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 302–312,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.990155">
personality .
reflected the flip side of the Stoltzman
NP
r = NP ! NP PP
NP PP
W
h
fw
fo
v(fw)
S
NP VP
�
DT NNP VBZ NP
The Fed issued
Structured inference
(discrete)
É
i j k
reflected the side of personality .
Feature extraction (continuous)
</figure>
<figureCaption confidence="0.997584">
Figure 1: Neural CRF model. On the right, each
</figureCaption>
<bodyText confidence="0.939766833333333">
anchored rule (r, s) in the tree is independently
scored by a function 0, so we can perform in-
ference with CKY to compute marginals or the
Viterbi tree. On the left, we show the process
for scoring an anchored rule with neural features:
words in fw (see Figure 2) are embedded, then fed
through a neural network with one hidden layer to
compute dense intermediate features, whose con-
junctions with sparse rule indicator features fo are
scored according to parameters W.
our core inference mechanism, allowing us to use
tricks like coarse pruning that make inference ef-
ficient in the purely sparse model. Our model can
be trained by gradient descent exactly as in a con-
ventional CRF, with the gradient of the network
parameters naturally computed by backpropagat-
ing a difference of expected anchored rule counts
through the network for each span and split point.
Using dense learned features alone, the neu-
ral CRF model obtains high performance, out-
performing the CRF parser of Hall et al. (2014).
When sparse indicators are used in addition, the
resulting model gets 91.1 F1 on section 23 of
the Penn Treebank, outperforming the parser of
Socher et al. (2013) as well as the Berkeley Parser
(Petrov and Klein, 2007) and matching the dis-
criminative parser of Carreras et al. (2008). The
model also obtains the best single parser results
on nine other languages, again outperforming the
system of Hall et al. (2014).
</bodyText>
<sectionHeader confidence="0.988525" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.926893">
Figure 1 shows our neural CRF model. The
model decomposes over anchored rules, and it
scores each of these with a potential function; in
a standard CRF, these potentials are typically lin-
ear functions of sparse indicator features, whereas
fg [[PreviousWord = reflected]], [[SpanLength = 7]], É
</bodyText>
<figureCaption confidence="0.80986">
Figure 2: Example of an anchored rule production
</figureCaption>
<bodyText confidence="0.98936125">
for the rule NP → NP PP. From the anchoring s =
(i, j, k), we extract either sparse surface features
fs or a sequence of word indicators fw which are
embedded to form a vector representation v(fw)
of the anchoring’s lexical properties.
in our approach they are nonlinear functions of
word embeddings.2 Section 2.1 describes our no-
tation for anchored rules, and Section 2.2 talks
about how they are scored. We then discuss spe-
cific choices of our featurization (Section 2.3) and
the backbone grammar used for structured infer-
ence (Section 2.4).
</bodyText>
<subsectionHeader confidence="0.995318">
2.1 Anchored Rules
</subsectionHeader>
<bodyText confidence="0.996363818181818">
The fundamental units that our parsing models
consider are anchored rules. As shown in Fig-
ure 2, we define an anchored rule as a tuple (r, s),
where r is an indicator of the rule’s identity and
s = (i, j, k) indicates the span (i, k) and split
point j of the rule.3 A tree T is simply a collec-
tion of anchored rules subject to the constraint that
those rules form a tree. All of our parsing models
are CRFs that decompose over anchored rule pro-
ductions and place a probability distribution over
trees conditioned on a sentence w as follows:
</bodyText>
<equation confidence="0.978665">
⎛ P(T |w) ∝ exp ⎝0(w, r, s) ⎞
⎠
(r,s)∈T
</equation>
<footnote confidence="0.979711">
2Throughout this work, we will primarily consider two
potential functions: linear functions of sparse indicators and
nonlinear neural networks over dense, continuous features.
Although other modeling choices are possible, these two
points in the design space reflect common choices in NLP,
and past work has suggested that nonlinear functions of indi-
cators or linear functions of dense features may perform less
well (Wang and Manning, 2013).
3For simplicity of exposition, we ignore unary rules; how-
ever, they are easily supported in this framework by simply
specifying a null value for the split point.
</footnote>
<page confidence="0.9994">
303
</page>
<bodyText confidence="0.999972814814814">
where φ is a scoring function that considers the
input sentence and the anchored rule in question.
Figure 1 shows this scoring process schematically.
As we will see, the module on the left can be be
a neural net, a linear function of surface features,
or a combination of the two, as long as it provides
anchored rule scores, and the structured inference
component is the same regardless (CKY).
A PCFG estimated with maximum likelihood
has φ(w, r, s) = log P(r|parent(r)), which is in-
dependent of the anchoring s and the words w ex-
cept for preterminal productions; a basic discrimi-
native parser might let this be a learned parameter
but still disregard the surface information. How-
ever, surface features can capture useful syntactic
cues (Finkel et al., 2008; Hall et al., 2014). Con-
sider the example in Figure 2: the proposed parent
NP is preceded by the word reflected and followed
by a period, which is a surface context character-
istic of NPs or PPs in object position. Beginning
with the and ending with personality are typical
properties of NPs as well, and the choice of the
particular rule NP —* NP PP is supported by the
fact that the proposed child PP begins with of. This
information can be captured with sparse features
(fs in Figure 2) or, as we describe below, with a
neural network taking lexical context as input.
</bodyText>
<subsectionHeader confidence="0.999872">
2.2 Scoring Anchored Rules
</subsectionHeader>
<bodyText confidence="0.958076571428571">
Following Hall et al. (2014), our baseline sparse
scoring function takes the following bilinear form:
φsparse(w, r, s; W) = fs(w, s)TW fo(r)
where fo(r) E {0,11no is a sparse vector of
features expressing properties of r (such as the
rule’s identity or its parent label) and fs(w, s) E
10, 11ns is a sparse vector of surface features as-
sociated with the words in the sentence and the
anchoring, as shown in Figure 2. W is a ns x no
matrix of weights.4 The scoring of a particular an-
chored rule is depicted in Figure 3a; note that sur-
face features and rule indicators are conjoined in a
systematic way.
The role of fs can be equally well played by a
vector of dense features learned via a neural net-
4A more conventional expression of the scoring function
for a CRF is O(w, r, s) = 0 f (w, r, s), with a vector 0 for
the parameters and a single feature extractor f that jointly
inspects the surface and the rule. However, when the feature
representation conjoins each rule r with surface properties of
the sentence in a systematic way (an assumption that holds in
our case as well as for standard CRF models for POS tagging
and NER), this is equivalent to our formalism.
a) 0 = fa W fo b) 0 = g(Hv(fw))&gt;W fo
Figure 3: Our sparse (left) and neural (right) scor-
ing functions for CRF parsing. fs and fw are
raw surface feature vectors for the sparse and neu-
ral models (respectively) extracted over anchored
spans with split points. (a) In the sparse case,
we multiply fs by a weight matrix W and then
a sparse output vector fo to score the rule produc-
tion. (b) In the neural case, we first embed fw and
then transform it with a one-layer neural network
in order to produce an intermediate feature repre-
sentation h before combining with W and fo.
work. We will now describe how to compute these
features, which represent a transformation of sur-
face lexical indicators fw. Define fw(w, s) E Nnw
to be a function that produces a fixed-length se-
quence of word indicators based on the input sen-
tence and the anchoring. This vector of word
identities is then passed to an embedding function
v : N —* Rne and the dense representations of
the words are subsequently concatenated to form
a vector we denote by v(fw).5 Finally, we mul-
tiply this by a matrix H E Rnh×(nwne) of real-
valued parameters and pass it through an elemen-
twise nonlinearity g(·). We use rectified linear
units g(x) = max(x, 0) and discuss this choice
more in Section 6.
Replacing fs with the end result of this compu-
tation h(w, s; H) = g(Hv(fw(w, s))), our scor-
ing function becomes
φneural(w, r, s; H, W) = h(w, s; H)TW fo(r)
as shown in Figure 3b. For a fixed H, this model
can be viewed as a basic CRF with dense input fea-
tures. By learning H, we learn intermediate fea-
ture representations that provide the model with
5Embedding words allows us to use standard pre-trained
vectors more easily and tying embeddings across word posi-
tions substantially reduces the number of model parameters.
However, embedding features rather than words has also been
shown to be effective (Chen et al., 2014).
</bodyText>
<figure confidence="0.972011">
fo
Wij = e ght(IIf.,i f.,jff)
fo
W
h
v(f�)
fw
W
fl,
</figure>
<page confidence="0.995896">
304
</page>
<bodyText confidence="0.9925676">
more discriminating power. Also note that it is
possible to use deeper networks or more sophis-
ticated architectures here; we will return to this in
Section 6.
Our two models can be easily combined:
</bodyText>
<equation confidence="0.6758195">
O(w, r, s; W1, H, W2) = Osparse(w, r, s; W1)
+ Oneural(w, r, s; H, W2)
</equation>
<bodyText confidence="0.999584333333333">
Weights for each component of the scoring func-
tion can be learned fully jointly and inference pro-
ceeds as before.
</bodyText>
<subsectionHeader confidence="0.974184">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.999992875">
We take fs to be the set of features described in
Hall et al. (2014). At the preterminal layer, the
model considers prefixes and suffixes up to length
5 of the current word and neighboring words, as
well as the words’ identities. For nonterminal pro-
ductions, we fire indicators on the words6 before
and after the start, end, and split point of the an-
chored rule (as shown in Figure 2) as well as on
two other span properties, span length and span
shape (an indicator of where capitalized words,
numbers, and punctuation occur in the span).
For our neural model, we take fw for all pro-
ductions (preterminal and nonterminal) to be the
words surrounding the beginning and end of a span
and the split point, as shown in Figure 2; in partic-
ular, we look two words in either direction around
each point of interest, meaning the neural net takes
12 words as input.7 For our word embeddings v,
we use pre-trained word vectors from Bansal et al.
(2014). We compare with other sources of word
vectors in Section 5. Contrary to standard practice,
we do not update these vectors during training; we
found that doing so did not provide an accuracy
benefit and slowed down training considerably.
</bodyText>
<subsectionHeader confidence="0.996019">
2.4 Grammar Refinements
</subsectionHeader>
<bodyText confidence="0.999865833333333">
A recurring issue in discriminative constituency
parsing is the granularity of annotation in the base
grammar (Finkel et al., 2008; Petrov and Klein,
2008; Hall et al., 2014). Using finer-grained sym-
bols in our rules r gives the model greater capacity,
but also introduces more parameters into W and
</bodyText>
<footnote confidence="0.744242714285714">
6The model actually uses the longest suffix of each word
occurring at least 100 times in the training set, up to the entire
word. Removing this abstraction of rare words harms perfor-
mance.
7The sparse model did not benefit from using this larger
neighborhood, so improvements from the neural net are not
simply due to considering more lexical context.
</footnote>
<bodyText confidence="0.999925909090909">
increases the ability to overfit. Following Hall et
al. (2014), we use grammars with very little anno-
tation: we use no horizontal Markovization for any
of experiments, and all of our English experiments
with the neural CRF use no vertical Markovization
(V = 0). This also has the benefit of making the
system much faster, due to the smaller state space
for dynamic programming. We do find that using
parent annotation (V = 1) is useful on other lan-
guages (see Section 7.2), but this is the only gram-
mar refinement we consider.
</bodyText>
<sectionHeader confidence="0.986532" genericHeader="method">
3 Learning
</sectionHeader>
<bodyText confidence="0.999889">
To learn weights for our neural model, we maxi-
mize the conditional log likelihood of our D train-
ing trees T*:
</bodyText>
<equation confidence="0.9994595">
L(H,W) = XD log P(Ti* |wi; H, W)
i=1
</equation>
<bodyText confidence="0.9999228">
Because we are using rectified linear units as our
nonlinearity, our objective is not everywhere dif-
ferentiable. The interaction of the parameters and
the nonlinearity also makes the objective non-
convex. However, in spite of this, we can still fol-
low subgradients to optimize this objective, as is
standard practice.
Recall that h(w, s; H) are the hidden layer ac-
tivations. The gradient of W takes the standard
form of log-linear models:
</bodyText>
<equation confidence="0.993667625">
⎛ ⎞
⎝ X
= h(w, s; H)fo(r)T ⎠−
(r,s)ET*
⎛ ⎞
⎝XX
P(T |w; H, W) h(w, s; H)fo(r)T ⎠
T (r,s)ET
</equation>
<bodyText confidence="0.999813">
Note that the outer products give matrices of fea-
ture counts isomorphic to W. The second expres-
sion can be simplified to be in terms of expected
feature counts. To update H, we use standard
backpropagation by first computing:
</bodyText>
<equation confidence="0.927828857142857">
⎛⎞
⎝ X =Wfo(r) ⎠−
(r,s)ET*
⎛ ⎞
⎝XX
P(T |w; H, W) W fo(r) ⎠
T (r,s)ET
</equation>
<bodyText confidence="0.993993666666667">
Since h is the output of the neural network, we can
then apply the chain rule to compute gradients for
H and any other parameters in the neural network.
</bodyText>
<figure confidence="0.752049">
aL
aW
aL
ah
</figure>
<page confidence="0.991447">
305
</page>
<bodyText confidence="0.999855473684211">
Learning uses Adadelta (Zeiler, 2012), which
has been employed in past work (Kim, 2014). We
found that Adagrad (Duchi et al., 2011) performed
equally well with tuned regularization and step
size parameters, but Adadelta worked better out
of the box. We set the momentum term p = 0.95
(as suggested by Zeiler (2012)) and did not reg-
ularize the weights at all. We used a minibatch
size of 200 trees, although the system was not par-
ticularly sensitive to this. For each treebank, we
trained for either 10 passes through the treebank
or 1000 minibatches, whichever is shorter.
We initialized the output weight matrix W to
zero. To break symmetry, the lower level neural
network parameters H were initialized with each
entry being independently sampled from a Gaus-
sian with mean 0 and variance 0.01; Gaussian per-
formed better than uniform initialization, but the
variance was not important.
</bodyText>
<sectionHeader confidence="0.999941" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.989289975609756">
Our baseline and neural model both score an-
chored rule productions. We can use CKY in the
standard fashion to compute either expected an-
chored rule counts EP(T|,)[(r, s)] or the Viterbi
tree arg maxT P(T Iw).
We speed up inference by using a coarse prun-
ing pass. We follow Hall et al. (2014) and
prune according to an X-bar grammar with head-
outward binarization, ruling out any constituent
whose max marginal probability is less than a−9.
With this pruning, the number of spans and split
points to be considered is greatly reduced; how-
ever, we still need to compute the neural network
activations for each remaining span and split point,
of which there may be thousands for a given sen-
tence.8 We can improve efficiency further by not-
ing that the same word will appear in the same po-
sition in a large number of span/split point combi-
nations, and cache the contribution to the hidden
layer caused by that word (Chen and Manning,
2014). Computing the hidden layer then simply
requires adding nw vectors together and applying
the nonlinearity, instead of a more costly matrix
multiply.
Because the number of rule indicators no is
fairly large (approximately 4000 in the Penn Tree-
bank), the multiplication by W in the model is also
8One reason we did not choose to include the rule identity
fo as an input to the network is that it requires computing an
even larger number of network activations, since we cannot
reuse them across rules over the same span and split point.
expensive. However, because only a small number
of rules can apply to a given span and split point,
fo is sparse and we can selectively compute the
terms necessary for the final bilinear product.
Our combined sparse and neural model trains on
the Penn Treebank in 24 hours on a single machine
with a parallelized CPU implementation. For ref-
erence, the purely sparse model with a parent-
annotated grammar (necessary for the best results)
takes around 15 hours on the same machine.
</bodyText>
<sectionHeader confidence="0.995405" genericHeader="method">
5 System Ablations
</sectionHeader>
<bodyText confidence="0.999889342105263">
Table 1 shows results on section 22 (the develop-
ment set) of the English Penn Treebank (Marcus
et al., 1993), computed using evalb. Full test re-
sults and comparisons to other systems are shown
in Table 4. We compare variants of our system
along two axes: whether they use standard linear
sparse features, nonlinear dense features from the
neural net, or both, and whether any word repre-
sentations (vectors or clusters) are used.
Sparse vs. neural The neural CRF (line (d) in
Table 1) on its own outperforms the sparse CRF
(a, b) even when the sparse CRF has a more heav-
ily annotated grammar. This is a surprising re-
sult: the features in the sparse CRF have been
carefully engineered to capture a range of linguis-
tic phenomena (Hall et al., 2014), and there is
no guarantee that word vectors will capture the
same. For example, at the POS tagging layer,
the sparse model looks at prefixes and suffixes of
words, which give the model access to morphol-
ogy for predicting tags of unknown words, which
typically have regular inflection patterns. By con-
trast, the neural model must rely on the geometry
of the vector space exposing useful regularities.
At the same time, the strong performance of the
combination of the two systems (g) indicates that
not only are both featurization approaches high-
performing on their own, but that they have com-
plementary strengths.
Unlabeled data Much attention has been paid
to the choice of word vectors for various NLP
tasks, notably whether they capture more syntac-
tic or semantic phenomena (Bansal et al., 2014;
Levy and Goldberg, 2014). We primarily use vec-
tors from Bansal et al. (2014), who train the skip-
gram model of Mikolov et al. (2013) using con-
texts from dependency links; a similar approach
was also suggested by Levy and Goldberg (2014).
</bodyText>
<page confidence="0.991257">
306
</page>
<table confidence="0.9996696">
Sparse Neural V Word Reps Fl len ≤ 40 Fl all
Hall et al. (2014), V = 1 90.5
a ✓ 0 89.89 89.22
b ✓ 1 90.82 90.13
c ✓ 1 Brown 90.80 90.17
d ✓ 0 Bansal 90.97 90.44
e ✓ 0 Collobert 90.25 89.63
f ✓ 0 PTB 89.34 88.99
g ✓ ✓ 0 Bansal 92.04 91.34
h ✓ ✓ 0 PTB 91.39 90.91
</table>
<tableCaption confidence="0.999362">
Table 1: Results of our sparse CRF, neural CRF,
</tableCaption>
<bodyText confidence="0.989803256410256">
and combined parsing models on section 22 of
the Penn Treebank. Systems are broken down
by whether local potentials come from sparse
features and/or the neural network (the primary
contribution of this work), their level of vertical
Markovization, and what kind of word represen-
tations they use. The neural CRF (d) outperforms
the sparse CRF (a, b) even when a more heavily
annotated grammar is used, and the combined ap-
proach (g) is substantially better than either indi-
vidual model. The contribution of the neural ar-
chitecture cannot be replaced by Brown clusters
(c), and even word representations learned just on
the Penn Treebank are surprisingly effective (f, h).
However, as these embeddings are trained on a
relatively small corpus (BLLIP minus the Penn
Treebank), it is natural to wonder whether less-
syntactic embeddings trained on a larger corpus
might be more useful. This is not the case: line
(e) in Table 1 shows the performance of the neu-
ral CRF using the Wikipedia-trained word embed-
dings of Collobert et al. (2011), which do not per-
form better than the vectors of Bansal et al. (2014).
To isolate the contribution of continuous word
representations themselves, we also experimented
with vectors trained on just the text from the train-
ing set of the Penn Treebank using the skip-gram
model with a window size of 1. While these vec-
tors are somewhat lower performing on their own
(f), they still provide a surprising and noticeable
gain when stacked on top of sparse features (h),
again suggesting that dense and sparse represen-
tations have complementary strengths. This result
also reinforces the notion that the utility of word
vectors does not come primarily from importing
information about out-of-vocabulary words (An-
dreas and Klein, 2014).
Since the neural features incorporate informa-
tion from unlabeled data, we should provide the
</bodyText>
<table confidence="0.998614888888889">
F1 len &lt; 40 A
Neural CRF 90.97 —
ReLU 90.97 —
Nonlinearity Tanh 90.74 −0.23
Cube 89.94 −1.03
0 HL 90.54 −0.43
Depth 1 HL 90.97 —
2 HL 90.58 −0.39
Embed output 88.81 −2.16
</table>
<tableCaption confidence="0.997947">
Table 2: Exploration of other implementation
</tableCaption>
<bodyText confidence="0.98892547826087">
choices in the feedforward neural network on sen-
tences of length &lt; 40 from section 22 of the Penn
Treebank. Rectified linear units perform better
than tanh or cubic units, a network with one hid-
den layer performs best, and embedding the output
feature vector gives worse performance.
sparse model with similar information for a true
apples-to-apples comparison. Brown clusters have
been shown to be effective vehicles in the past
(Koo et al., 2008; Turian et al., 2010; Bansal et al.,
2014). We can incorporate Brown clusters into the
baseline CRF model in an analogous way to how
embedding features are used in the dense model:
surface features are fired on Brown cluster iden-
tities (we use prefixes of length 4 and 10) of key
words. We use the Brown clusters from Koo et al.
(2008), which are trained on the same data as the
vectors of Bansal et al. (2014). However, Table 1
shows that these features provide no benefit to the
baseline model, which suggests either that it is dif-
ficult to learn reliable weights for these as sparse
features or that different regularities are being cap-
tured by the word embeddings.
</bodyText>
<sectionHeader confidence="0.998806" genericHeader="method">
6 Design Choices
</sectionHeader>
<bodyText confidence="0.999917454545454">
The neural net design space is large, so we wish
to analyze the particular design choices we made
for this system by examining the performance of
several variants of the neural net architecture used
in our system. Table 2 shows development re-
sults from potential alternate architectural choices,
which we now discuss.
Choice of nonlinearity The choice of nonlin-
earity g has been frequently discussed in the neural
network literature. Our choice g(x) = max(x, 0),
a rectified linear unit, is increasingly popular in
</bodyText>
<page confidence="0.996801">
307
</page>
<bodyText confidence="0.999453266666667">
computer vision (Krizhevsky et al., 2012). g(x) =
tanh(x) is a traditional nonlinearity widely used
throughout the history of neural nets (Bengio et
al., 2003). g(x) = x3 (cube) was found to be most
successful by Chen and Manning (2014).
Table 2 compares the performance of these
three nonlinearities. We see that rectified linear
units perform the best, followed by tanh units,
followed by cubic units.9 One drawback of tanh
as an activation function is that it is easily “satu-
rated” if the input to the unit is too far away from
zero, causing the backpropagation of derivatives
through that unit to essentially cease; this is known
to cause problems for training, requiring special
purpose machinery for use in deep networks (Ioffe
and Szegedy, 2015).
Depth Given that we are using rectified linear
units, it bears asking whether or not our imple-
mentation is improving substantially over linear
features of the continuous input. We can use the
embedding vector of an anchored span v(fw) di-
rectly as input to a basic linear CRF, as shown in
Figure 4a. Table 1 shows that the purely linear ar-
chitecture (0 HL) performs surprisingly well, but
is still less effective than the network with one hid-
den layer. This agrees with the results of Wang
and Manning (2013), who noted that dense fea-
tures typically benefit from nonlinear modeling.
We also compare against a two-layer neural net-
work, but find that this also performs worse than
the one-layer architecture.
Densifying output features Overall, it appears
beneficial to use dense representations of surface
features; a natural question that one might ask is
whether the same technique can be applied to the
sparse output feature vector fo. We can apply the
approach of Srikumar and Manning (2014) and
multiply the sparse output vector by a dense matrix
K, giving the following scoring function (shown
in Figure 4b):
O(w, r, s; H, W, K) = g(Hv(fw(w, s)))TWKfo(r)
where W is now nh × noe and K is noe × no.
WK can be seen a low-rank approximation of the
original W at the output layer, similar to low-rank
factorizations of parameter matrices used in past
</bodyText>
<footnote confidence="0.8823405">
9The performance of cube decreased substantially late in
learning; it peaked at around 90.52. Dropout may be useful
for alleviating this type of overfitting, but in our experiments
we did not find dropout to be beneficial overall.
</footnote>
<figure confidence="0.697336">
a) 0 = v(fw)&gt;Wfo b) 0 = g(Hv(fw))&gt;WKfo
</figure>
<figureCaption confidence="0.99688">
Figure 4: Two additional forms of the scoring
function. a) Linear version of the dense model,
equivalent to a CRF with continuous-valued input
features. b) Version of the dense model where out-
puts are also embedded according to a learned ma-
trix K.
</figureCaption>
<bodyText confidence="0.999792714285714">
work (Lei et al., 2014). This approach saves us
from having to learn a separate row of W for ev-
ery rule in the grammar; if rules are given similar
embeddings, then they will behave similarly ac-
cording to the model.
We experimented with noe = 20 and show the
results in Table 2. Unfortunately, this approach
does not seem to work well for parsing. Learn-
ing the output representation was empirically very
unstable, and it also required careful initialization.
We tried Gaussian initialization (as in the rest of
our model) and initializing the model by clustering
rules either randomly or according to their parent
symbol. The latter is what is shown in the table,
and gave substantially better performance. We hy-
pothesize that blurring distinctions between output
classes may harm the model’s ability to differenti-
ate between closely-related symbols, which is re-
quired for good parsing performance. Using pre-
trained rule embeddings at this layer might also
improve performance of this method.
</bodyText>
<sectionHeader confidence="0.542508" genericHeader="method">
7 Test Results
</sectionHeader>
<bodyText confidence="0.9999645">
We evaluate our system under two conditions:
first, on the English Penn Treebank, and second,
on the nine languages used in the SPMRL 2013
and 2014 shared tasks.
</bodyText>
<subsectionHeader confidence="0.998817">
7.1 Penn Treebank
</subsectionHeader>
<bodyText confidence="0.9985434">
Table 4 reports results on section 23 of the Penn
Treebank (PTB). We focus our comparison on sin-
gle parser systems as opposed to rerankers, ensem-
bles, or self-trained methods (though these are also
mentioned for context). First, we compare against
</bodyText>
<figure confidence="0.994073">
v(f.)
fw
Kfo
W
h
fo
fo
W
v(f.)
fw
</figure>
<page confidence="0.99096">
308
</page>
<table confidence="0.999220333333333">
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Avg
Dev, all lengths
Hall et al. (2014) 78.89 83.74 79.40 83.28 88.06 87.44 81.85 91.10 75.95 83.30
This work* 80.68 84.37 80.65 85.25 89.37 89.46 82.35 92.10 77.93 84.68
Test, all lengths
Berkeley 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
Berkeley-Tags 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
Crabb´e and Seddah (2014) 77.66 85.35 79.68 77.15 86.19 87.51 79.35 91.60 82.72 83.02
Hall et al. (2014) 78.75 83.39 79.70 78.43 87.18 88.25 80.18 90.66 82.00 83.17
This work* 80.24 85.41 81.25 80.95 88.61 90.66 82.23 92.97 83.45 85.08
Reranked ensemble
2014 Best 81.32 88.24 82.53 81.66 89.80 91.72 83.81 90.50 85.50 86.12
</table>
<tableCaption confidence="0.584197666666667">
Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores
for sentences of all lengths using the version of evalb distributed with the shared task. Our parser
substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb´e and
Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task
(Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the
best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014).
</tableCaption>
<table confidence="0.963899333333333">
F1 all
Single model, PTB only
Hall et al. (2014) 89.2
Berkeley 90.1
Carreras et al. (2008) 91.1
Shindo et al. (2012) single 91.1
Single model, PTB + vectors/clusters
Zhu et al. (2013) 91.3
This work* 91.1
Extended conditions
Charniak and Johnson (2005) 91.5
Socher et al. (2013) 90.4
Vinyals et al. (2014) single 90.5
Vinyals et al. (2014) ensemble 91.6
Shindo et al. (2012) ensemble 92.4
</table>
<tableCaption confidence="0.974484">
Table 4: Test results on section 23 of the Penn
</tableCaption>
<bodyText confidence="0.998271625">
Treebank. We compare to several categories of
parsers from the literatures. We outperform strong
baselines such as the Berkeley Parser (Petrov and
Klein, 2007) and the CVG Stanford parser (Socher
et al., 2013) and we match the performance of so-
phisticated generative (Shindo et al., 2012) and
discriminative (Carreras et al., 2008) parsers.
four parsers trained only on the PTB with no aux-
iliary data: the CRF parser of Hall et al. (2014),
the Berkeley parser (Petrov and Klein, 2007), the
discriminative parser of Carreras et al. (2008), and
the single TSG parser of Shindo et al. (2012). To
our knowledge, the latter two systems are the high-
est performing in this PTB-only, single parser data
condition; we match their performance at 91.1 F1,
though we also use word vectors computed from
unlabeled data. We further compare to the shift-
reduce parser of Zhu et al. (2013), which uses un-
labeled data in the form of Brown clusters. Our
method achieves performance close to that of their
parser.
We also compare to the compositional vector
grammar (CVG) parser of Socher et al. (2013)
as well as the LSTM-based parser of Vinyals et
al. (2014). The conditions these parsers are op-
erating under are slightly different: the former is
a reranker on top of the Stanford Parser (Klein
and Manning, 2003) and the latter trains on much
larger amounts of data parsed by a product of
Berkeley parsers (Petrov, 2010). Regardless, we
outperform the CVG parser as well as the single
parser results from Vinyals et al. (2014).
</bodyText>
<subsectionHeader confidence="0.940846">
7.2 SPMRL
</subsectionHeader>
<bodyText confidence="0.999974111111111">
We also examine the performance of our
parser on other languages, specifically the
nine morphologically-rich languages used in the
SPMRL 2013/2014 shared tasks (Seddah et al.,
2013; Seddah et al., 2014). We train word vec-
tors on the monolingual data distributed with the
SPMRL 2014 shared task (typically 100M-200M
tokens per language) using the skip-gram ap-
proach of word2vec with a window size of 1
</bodyText>
<page confidence="0.997379">
309
</page>
<bodyText confidence="0.999416">
(Mikolov et al., 2013).10 Here we use V = 1
in the backbone grammar, which we found to be
beneficial overall. Table 3 shows that our system
improves upon the performance of the parser from
Hall et al. (2014) as well as the top single parser
from the shared task (Crabb´e and Seddah, 2014),
with robust improvements on all languages.
</bodyText>
<sectionHeader confidence="0.998507" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99991475">
In this work, we presented a CRF parser that
scores anchored rule productions using dense in-
put features computed from a feedforward neu-
ral net. Because the neural component is mod-
ularized, we can easily integrate it into a pre-
existing learning and inference framework based
around dynamic programming of a discrete parse
chart. Our combined neural and sparse model
gives strong performance both on English and on
other languages.
Our system is publicly available at
http://nlp.cs.berkeley.edu.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999943454545455">
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014, by a
Facebook fellowship for the first author, and by
a Google Faculty Research Award to the second
author. Thanks to David Hall for assistance with
the Epic parsing framework and for a preliminary
implementation of the neural architecture, to Kush
Rastogi for training word vectors on the SPMRL
data, to Dan Jurafsky for helpful discussions, and
to the anonymous reviewers for their insightful
comments.
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9976733">
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax? In Pro-
ceedings of the Association for Computational Lin-
guistics.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring Continuous Word Representations
for Dependency Parsing. In Proceedings of the As-
sociation for Computational Linguistics.
Yonatan Belinkov, Tao Lei, Regina Barzilay, and Amir
Globerson. 2014. Exploring Compositional Archi-
tectures and Word Vector Representations for Prepo-
sitional Phrase Attachment. Transactions of the As-
sociation for Computational Linguistics, 2:561–572.
10Training vectors with the SKIPDEP method of Bansal et
al. (2014) did not substantially improve performance here.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137–1155, March.
Anders Bj¨orkelund, Ozlem Cetinoglu, Rich´ard Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking Meets Morphosyntax: State-of-the-art
Results from the SPMRL 2013 Shared Task. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages.
Anders Bj¨orkelund, ¨Ozlem C¸etino˘glu, Agnieszka
Fale´nska, Rich´ard Farkas, Thomas Mueller, Wolf-
gang Seeker, and Zsolt Sz´ant´o. 2014. Introducing
the IMS-Wrocław-Szeged-CIS entry at the SPMRL
2014 Shared Task: Reranking and Morpho-syntax
meet Unlabeled Data. In Proceedings of the First
Joint Workshop on Statistical Parsing of Morpho-
logically Rich Languages and Syntactic Analysis of
Non-Canonical Languages.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, Dynamic Programming, and the Per-
ceptron for Efficient, Feature-rich Parsing. In Pro-
ceedings of the Conference on Computational Natu-
ral Language Learning.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the Association for
Computational Linguistics.
Danqi Chen and Christopher D Manning. 2014. A
Fast and Accurate Dependency Parser using Neural
Networks. In Proceedings of Empirical Methods in
Natural Language Processing.
Wenliang Chen, Yue Zhang, and Min Zhang. 2014.
Feature Embedding for Dependency Parsing. In
Proceedings of the International Conference on
Computational Linguistics.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493–2537.
Benoit Crabb´e and Djam´e Seddah. 2014. Multilingual
Discriminative Shift-Reduce Phrase Structure Pars-
ing for the SPMRL 2014 Shared Task. In Proceed-
ings of the First Joint Workshop on Statistical Pars-
ing of Morphologically Rich Languages and Syntac-
tic Analysis of Non-Canonical Languages.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121–2159, July.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, Feature-based, Condi-
tional Random Field Parsing. In Proceedings of the
Association for Computational Linguistics.
</reference>
<page confidence="0.99138">
310
</page>
<reference confidence="0.999871785714285">
David Hall, Greg Durrett, and Dan Klein. 2014. Less
Grammar, More Features. In Proceedings of the As-
sociation for Computational Linguistics.
James Henderson. 2003. Inducing History Represen-
tations for Broad Coverage Statistical Parsing. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics.
Sergey Ioffe and Christian Szegedy. 2015. Batch Nor-
malization: Accelerating Deep Network Training by
Reducing Internal Covariate Shift. arXiv preprint,
arXiv:1502.03167.
Ozan ˙Irsoy and Claire Cardie. 2014. Opinion Min-
ing with Deep Recurrent Neural Networks. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A Convolutional Neural Network for
Modelling Sentences. In Proceedings of the Associ-
ation for Computational Linguistics.
Yoon Kim. 2014. Convolutional Neural Networks for
Sentence Classification. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Pars-
ing. In Proceedings of the Association for Compu-
tational Linguistics.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. ImageNet Classification with Deep Con-
volutional Neural Networks. In Advances in Neural
Information Processing Systems.
Phong Le and Willem Zuidema. 2014. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-Rank Tensors for
Scoring Dependency Structures. In Proceedings of
the Association for Computational Linguistics.
Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of the As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of the
International Conference on Learning Representa-
tions.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics.
Slav Petrov and Dan Klein. 2008. Sparse Multi-Scale
Grammars for Discriminative Latent Variable Pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Slav Petrov. 2010. Products of Random Latent Vari-
able Grammars. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie
Candito, Jinho D. Choi, Rich´ard Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepi´orkowski, Ryan Roth, Wolf-
gang Seeker, Yannick Versley, Veronika Vincze,
Marcin Woli´nski, and Alina Wr´oblewska. 2013.
Overview of the SPMRL 2013 Shared Task: A
Cross-Framework Evaluation of Parsing Morpho-
logically Rich Languages. In Proceedings of
the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages.
Djam´e Seddah, Sandra K¨ubler, and Reut Tsarfaty.
2014. Introducing the SPMRL 2014 Shared Task on
Parsing Morphologically-rich Languages. In Pro-
ceedings of the First Joint Workshop on Statisti-
cal Parsing of Morphologically Rich Languages and
Syntactic Analysis of Non-Canonical Languages.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian Symbol-refined
Tree Substitution Grammars for Syntactic Parsing.
In Proceedings of the Association for Computational
Linguistics.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing With Composi-
tional Vector Grammars. In Proceedings of the As-
sociation for Computational Linguistics.
Vivek Srikumar and Christopher D Manning. 2014.
Learning Distributed Representations for Structured
Output Prediction. In Advances in Neural Informa-
tion Processing Systems.
Yuta Tsuboi. 2014. Neural Networks Leverage
Corpus-wide Information for Part-of-speech Tag-
ging. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-supervised Learning. In Proceed-
ings of the Association for Computational Linguis-
tics.
Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav
Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
2014. Grammar as a Foreign Language. CoRR,
abs/1412.7449.
</reference>
<page confidence="0.984531">
311
</page>
<reference confidence="0.9984736">
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of Non-linear Deep Architecture in Sequence
Labeling. In Proceedings of the International Joint
Conference on Natural Language Processing.
Matthew D. Zeiler. 2012. ADADELTA: An Adaptive
Learning Rate Method. CoRR, abs/1212.5701.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and Accurate Shift-
Reduce Constituent Parsing. In Proceedings of the
Association for Computational Linguistics.
</reference>
<page confidence="0.998655">
312
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916545">
<title confidence="0.997738">Neural CRF Parsing</title>
<author confidence="0.925857">Durrett</author>
<affiliation confidence="0.999891">Computer Science University of California,</affiliation>
<abstract confidence="0.999599">This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient (expected rule counts). ing only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination sparse features, our section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Dan Klein</author>
</authors>
<title>How much do word embeddings encode about syntax?</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22857" citStr="Andreas and Klein, 2014" startWordPosition="3939" endWordPosition="3943">continuous word representations themselves, we also experimented with vectors trained on just the text from the training set of the Penn Treebank using the skip-gram model with a window size of 1. While these vectors are somewhat lower performing on their own (f), they still provide a surprising and noticeable gain when stacked on top of sparse features (h), again suggesting that dense and sparse representations have complementary strengths. This result also reinforces the notion that the utility of word vectors does not come primarily from importing information about out-of-vocabulary words (Andreas and Klein, 2014). Since the neural features incorporate information from unlabeled data, we should provide the F1 len &lt; 40 A Neural CRF 90.97 — ReLU 90.97 — Nonlinearity Tanh 90.74 −0.23 Cube 89.94 −1.03 0 HL 90.54 −0.43 Depth 1 HL 90.97 — 2 HL 90.58 −0.39 Embed output 88.81 −2.16 Table 2: Exploration of other implementation choices in the feedforward neural network on sentences of length &lt; 40 from section 22 of the Penn Treebank. Rectified linear units perform better than tanh or cubic units, a network with one hidden layer performs best, and embedding the output feature vector gives worse performance. spars</context>
</contexts>
<marker>Andreas, Klein, 2014</marker>
<rawString>Jacob Andreas and Dan Klein. 2014. How much do word embeddings encode about syntax? In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring Continuous Word Representations for Dependency Parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13546" citStr="Bansal et al. (2014)" startWordPosition="2311" endWordPosition="2314">, and split point of the anchored rule (as shown in Figure 2) as well as on two other span properties, span length and span shape (an indicator of where capitalized words, numbers, and punctuation occur in the span). For our neural model, we take fw for all productions (preterminal and nonterminal) to be the words surrounding the beginning and end of a span and the split point, as shown in Figure 2; in particular, we look two words in either direction around each point of interest, meaning the neural net takes 12 words as input.7 For our word embeddings v, we use pre-trained word vectors from Bansal et al. (2014). We compare with other sources of word vectors in Section 5. Contrary to standard practice, we do not update these vectors during training; we found that doing so did not provide an accuracy benefit and slowed down training considerably. 2.4 Grammar Refinements A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014). Using finer-grained symbols in our rules r gives the model greater capacity, but also introduces more parameters into W and 6The model actually uses the longest </context>
<context position="20546" citStr="Bansal et al., 2014" startWordPosition="3531" endWordPosition="3534"> model access to morphology for predicting tags of unknown words, which typically have regular inflection patterns. By contrast, the neural model must rely on the geometry of the vector space exposing useful regularities. At the same time, the strong performance of the combination of the two systems (g) indicates that not only are both featurization approaches highperforming on their own, but that they have complementary strengths. Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al. (2013) using contexts from dependency links; a similar approach was also suggested by Levy and Goldberg (2014). 306 Sparse Neural V Word Reps Fl len ≤ 40 Fl all Hall et al. (2014), V = 1 90.5 a ✓ 0 89.89 89.22 b ✓ 1 90.82 90.13 c ✓ 1 Brown 90.80 90.17 d ✓ 0 Bansal 90.97 90.44 e ✓ 0 Collobert 90.25 89.63 f ✓ 0 PTB 89.34 88.99 g ✓ ✓ 0 Bansal 92.04 91.34 h ✓ ✓ 0 PTB 91.39 90.91 Table 1: Results of our sparse CRF, neural CRF, and combined parsing models on section 22 of th</context>
<context position="22200" citStr="Bansal et al. (2014)" startWordPosition="3836" endWordPosition="3839">el. The contribution of the neural architecture cannot be replaced by Brown clusters (c), and even word representations learned just on the Penn Treebank are surprisingly effective (f, h). However, as these embeddings are trained on a relatively small corpus (BLLIP minus the Penn Treebank), it is natural to wonder whether lesssyntactic embeddings trained on a larger corpus might be more useful. This is not the case: line (e) in Table 1 shows the performance of the neural CRF using the Wikipedia-trained word embeddings of Collobert et al. (2011), which do not perform better than the vectors of Bansal et al. (2014). To isolate the contribution of continuous word representations themselves, we also experimented with vectors trained on just the text from the training set of the Penn Treebank using the skip-gram model with a window size of 1. While these vectors are somewhat lower performing on their own (f), they still provide a surprising and noticeable gain when stacked on top of sparse features (h), again suggesting that dense and sparse representations have complementary strengths. This result also reinforces the notion that the utility of word vectors does not come primarily from importing informatio</context>
<context position="23658" citStr="Bansal et al., 2014" startWordPosition="4078" endWordPosition="4081"> −1.03 0 HL 90.54 −0.43 Depth 1 HL 90.97 — 2 HL 90.58 −0.39 Embed output 88.81 −2.16 Table 2: Exploration of other implementation choices in the feedforward neural network on sentences of length &lt; 40 from section 22 of the Penn Treebank. Rectified linear units perform better than tanh or cubic units, a network with one hidden layer performs best, and embedding the output feature vector gives worse performance. sparse model with similar information for a true apples-to-apples comparison. Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al. (2014). However, Table 1 shows that these features provide no benefit to the baseline model, which suggests either that it is difficult to learn reliable weights for these as sparse features or that different regularities are being captur</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring Continuous Word Representations for Dependency Parsing. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonatan Belinkov</author>
<author>Tao Lei</author>
<author>Regina Barzilay</author>
<author>Amir Globerson</author>
</authors>
<title>Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<booktitle>10Training vectors with the SKIPDEP method of Bansal</booktitle>
<volume>2</volume>
<contexts>
<context position="1548" citStr="Belinkov et al., 2014" startWordPosition="231" endWordPosition="235">tures, our system1 achieves 91.1 Fs on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1System available at http://nlp.cs.berkeley.edu output spaces. Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; ˙Irsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014). However, there is a natural marriage of nonlinear induced features and efficient structured inference,</context>
</contexts>
<marker>Belinkov, Lei, Barzilay, Globerson, 2014</marker>
<rawString>Yonatan Belinkov, Tao Lei, Regina Barzilay, and Amir Globerson. 2014. Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment. Transactions of the Association for Computational Linguistics, 2:561–572. 10Training vectors with the SKIPDEP method of Bansal et al. (2014) did not substantially improve performance here.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="24980" citStr="Bengio et al., 2003" startWordPosition="4303" endWordPosition="4306">e the particular design choices we made for this system by examining the performance of several variants of the neural net architecture used in our system. Table 2 shows development results from potential alternate architectural choices, which we now discuss. Choice of nonlinearity The choice of nonlinearity g has been frequently discussed in the neural network literature. Our choice g(x) = max(x, 0), a rectified linear unit, is increasingly popular in 307 computer vision (Krizhevsky et al., 2012). g(x) = tanh(x) is a traditional nonlinearity widely used throughout the history of neural nets (Bengio et al., 2003). g(x) = x3 (cube) was found to be most successful by Chen and Manning (2014). Table 2 compares the performance of these three nonlinearities. We see that rectified linear units perform the best, followed by tanh units, followed by cubic units.9 One drawback of tanh as an activation function is that it is easily “saturated” if the input to the unit is too far away from zero, causing the backpropagation of derivatives through that unit to essentially cease; this is known to cause problems for training, requiring special purpose machinery for use in deep networks (Ioffe and Szegedy, 2015). Depth</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research, 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Ozlem Cetinoglu</author>
<author>Rich´ard Farkas</author>
<author>Thomas Mueller</author>
<author>Wolfgang Seeker</author>
</authors>
<title>(Re)ranking Meets Morphosyntax: State-of-the-art Results from the SPMRL 2013 Shared Task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages.</booktitle>
<marker>Bj¨orkelund, Cetinoglu, Farkas, Mueller, Seeker, 2013</marker>
<rawString>Anders Bj¨orkelund, Ozlem Cetinoglu, Rich´ard Farkas, Thomas Mueller, and Wolfgang Seeker. 2013. (Re)ranking Meets Morphosyntax: State-of-the-art Results from the SPMRL 2013 Shared Task. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anders Bj¨orkelund</author>
<author>¨Ozlem C¸etino˘glu</author>
<author>Agnieszka Fale´nska</author>
<author>Rich´ard Farkas</author>
<author>Thomas Mueller</author>
<author>Wolfgang Seeker</author>
<author>Zsolt Sz´ant´o</author>
</authors>
<title>Introducing the IMS-Wrocław-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and Morpho-syntax meet Unlabeled Data.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages.</booktitle>
<marker>Bj¨orkelund, C¸etino˘glu, Fale´nska, Farkas, Mueller, Seeker, Sz´ant´o, 2014</marker>
<rawString>Anders Bj¨orkelund, ¨Ozlem C¸etino˘glu, Agnieszka Fale´nska, Rich´ard Farkas, Thomas Mueller, Wolfgang Seeker, and Zsolt Sz´ant´o. 2014. Introducing the IMS-Wrocław-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and Morpho-syntax meet Unlabeled Data. In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="5505" citStr="Carreras et al. (2008)" startWordPosition="874" endWordPosition="877">s in a conventional CRF, with the gradient of the network parameters naturally computed by backpropagating a difference of expected anchored rule counts through the network for each span and split point. Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014). 2 Model Figure 1 shows our neural CRF model. The model decomposes over anchored rules, and it scores each of these with a potential function; in a standard CRF, these potentials are typically linear functions of sparse indicator features, whereas fg [[PreviousWord = reflected]], [[SpanLength = 7]], É Figure 2: Example of an anchored rule production for the rule NP → NP PP. From the anchoring s = (i, j, k), we extract either sparse surface features fs or a seque</context>
<context position="30344" citStr="Carreras et al. (2008)" startWordPosition="5206" endWordPosition="5209">alues are F-scores for sentences of all lengths using the version of evalb distributed with the shared task. Our parser substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb´e and Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). F1 all Single model, PTB only Hall et al. (2014) 89.2 Berkeley 90.1 Carreras et al. (2008) 91.1 Shindo et al. (2012) single 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) 91.3 This work* 91.1 Extended conditions Charniak and Johnson (2005) 91.5 Socher et al. (2013) 90.4 Vinyals et al. (2014) single 90.5 Vinyals et al. (2014) ensemble 91.6 Shindo et al. (2012) ensemble 92.4 Table 4: Test results on section 23 of the Penn Treebank. We compare to several categories of parsers from the literatures. We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al., 2013) and we match the performance of sophisticat</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing. In Proceedings of the Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="30506" citStr="Charniak and Johnson (2005)" startWordPosition="5232" endWordPosition="5235">st single parser results on this dataset (Hall et al., 2014; Crabb´e and Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). F1 all Single model, PTB only Hall et al. (2014) 89.2 Berkeley 90.1 Carreras et al. (2008) 91.1 Shindo et al. (2012) single 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) 91.3 This work* 91.1 Extended conditions Charniak and Johnson (2005) 91.5 Socher et al. (2013) 90.4 Vinyals et al. (2014) single 90.5 Vinyals et al. (2014) ensemble 91.6 Shindo et al. (2012) ensemble 92.4 Table 4: Test results on section 23 of the Penn Treebank. We compare to several categories of parsers from the literatures. We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al., 2013) and we match the performance of sophisticated generative (Shindo et al., 2012) and discriminative (Carreras et al., 2008) parsers. four parsers trained only on the PTB with no auxiliary data: the CRF parse</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A Fast and Accurate Dependency Parser using Neural Networks.</title>
<date>2014</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1524" citStr="Chen and Manning, 2014" startWordPosition="227" endWordPosition="230">bination with sparse features, our system1 achieves 91.1 Fs on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1System available at http://nlp.cs.berkeley.edu output spaces. Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; ˙Irsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014). However, there is a natural marriage of nonlinear induced features and efficie</context>
<context position="3326" citStr="Chen and Manning, 2014" startWordPosition="507" endWordPosition="510"> split point. As input, it takes vector representations of words at the split point and span boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying 302 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 302–312, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Li</context>
<context position="17968" citStr="Chen and Manning, 2014" startWordPosition="3088" endWordPosition="3091">ng to an X-bar grammar with headoutward binarization, ruling out any constituent whose max marginal probability is less than a−9. With this pruning, the number of spans and split points to be considered is greatly reduced; however, we still need to compute the neural network activations for each remaining span and split point, of which there may be thousands for a given sentence.8 We can improve efficiency further by noting that the same word will appear in the same position in a large number of span/split point combinations, and cache the contribution to the hidden layer caused by that word (Chen and Manning, 2014). Computing the hidden layer then simply requires adding nw vectors together and applying the nonlinearity, instead of a more costly matrix multiply. Because the number of rule indicators no is fairly large (approximately 4000 in the Penn Treebank), the multiplication by W in the model is also 8One reason we did not choose to include the rule identity fo as an input to the network is that it requires computing an even larger number of network activations, since we cannot reuse them across rules over the same span and split point. expensive. However, because only a small number of rules can app</context>
<context position="25057" citStr="Chen and Manning (2014)" startWordPosition="4318" endWordPosition="4321">erformance of several variants of the neural net architecture used in our system. Table 2 shows development results from potential alternate architectural choices, which we now discuss. Choice of nonlinearity The choice of nonlinearity g has been frequently discussed in the neural network literature. Our choice g(x) = max(x, 0), a rectified linear unit, is increasingly popular in 307 computer vision (Krizhevsky et al., 2012). g(x) = tanh(x) is a traditional nonlinearity widely used throughout the history of neural nets (Bengio et al., 2003). g(x) = x3 (cube) was found to be most successful by Chen and Manning (2014). Table 2 compares the performance of these three nonlinearities. We see that rectified linear units perform the best, followed by tanh units, followed by cubic units.9 One drawback of tanh as an activation function is that it is easily “saturated” if the input to the unit is too far away from zero, causing the backpropagation of derivatives through that unit to essentially cease; this is known to cause problems for training, requiring special purpose machinery for use in deep networks (Ioffe and Szegedy, 2015). Depth Given that we are using rectified linear units, it bears asking whether or n</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A Fast and Accurate Dependency Parser using Neural Networks. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Yue Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Feature Embedding for Dependency Parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="12149" citStr="Chen et al., 2014" startWordPosition="2053" endWordPosition="2056">cing fs with the end result of this computation h(w, s; H) = g(Hv(fw(w, s))), our scoring function becomes φneural(w, r, s; H, W) = h(w, s; H)TW fo(r) as shown in Figure 3b. For a fixed H, this model can be viewed as a basic CRF with dense input features. By learning H, we learn intermediate feature representations that provide the model with 5Embedding words allows us to use standard pre-trained vectors more easily and tying embeddings across word positions substantially reduces the number of model parameters. However, embedding features rather than words has also been shown to be effective (Chen et al., 2014). fo Wij = e ght(IIf.,i f.,jff) fo W h v(f�) fw W fl, 304 more discriminating power. Also note that it is possible to use deeper networks or more sophisticated architectures here; we will return to this in Section 6. Our two models can be easily combined: O(w, r, s; W1, H, W2) = Osparse(w, r, s; W1) + Oneural(w, r, s; H, W2) Weights for each component of the scoring function can be learned fully jointly and inference proceeds as before. 2.3 Features We take fs to be the set of features described in Hall et al. (2014). At the preterminal layer, the model considers prefixes and suffixes up to le</context>
</contexts>
<marker>Chen, Zhang, Zhang, 2014</marker>
<rawString>Wenliang Chen, Yue Zhang, and Min Zhang. 2014. Feature Embedding for Dependency Parsing. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural Language Processing (Almost) from Scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2187" citStr="Collobert et al. (2011)" startWordPosition="330" endWordPosition="334">ntics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1System available at http://nlp.cs.berkeley.edu output spaces. Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; ˙Irsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014). However, there is a natural marriage of nonlinear induced features and efficient structured inference, as explored by Collobert et al. (2011) for the case of sequence modeling: feedforward neural networks can be used to score local decisions which are then “reconciled” in a discrete structured modeling framework, allowing inference via dynamic programming. In this work, we present a CRF constituency parser based on these principles, where individual anchored rule productions are scored based on nonlinear features computed with a feedforward neural network. A separate, identicallyparameterized replicate of the network exists for each possible span and split point. As input, it takes vector representations of words at the split point</context>
<context position="22130" citStr="Collobert et al. (2011)" startWordPosition="3822" endWordPosition="3825"> combined approach (g) is substantially better than either individual model. The contribution of the neural architecture cannot be replaced by Brown clusters (c), and even word representations learned just on the Penn Treebank are surprisingly effective (f, h). However, as these embeddings are trained on a relatively small corpus (BLLIP minus the Penn Treebank), it is natural to wonder whether lesssyntactic embeddings trained on a larger corpus might be more useful. This is not the case: line (e) in Table 1 shows the performance of the neural CRF using the Wikipedia-trained word embeddings of Collobert et al. (2011), which do not perform better than the vectors of Bansal et al. (2014). To isolate the contribution of continuous word representations themselves, we also experimented with vectors trained on just the text from the training set of the Penn Treebank using the skip-gram model with a window size of 1. While these vectors are somewhat lower performing on their own (f), they still provide a surprising and noticeable gain when stacked on top of sparse features (h), again suggesting that dense and sparse representations have complementary strengths. This result also reinforces the notion that the uti</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Crabb´e</author>
<author>Djam´e Seddah</author>
</authors>
<title>Multilingual Discriminative Shift-Reduce Phrase Structure Parsing for the SPMRL 2014 Shared Task.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages.</booktitle>
<marker>Crabb´e, Seddah, 2014</marker>
<rawString>Benoit Crabb´e and Djam´e Seddah. 2014. Multilingual Discriminative Shift-Reduce Phrase Structure Parsing for the SPMRL 2014 Shared Task. In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="16269" citStr="Duchi et al., 2011" startWordPosition="2794" endWordPosition="2797">s; H)fo(r)T ⎠ T (r,s)ET Note that the outer products give matrices of feature counts isomorphic to W. The second expression can be simplified to be in terms of expected feature counts. To update H, we use standard backpropagation by first computing: ⎛⎞ ⎝ X =Wfo(r) ⎠− (r,s)ET* ⎛ ⎞ ⎝XX P(T |w; H, W) W fo(r) ⎠ T (r,s)ET Since h is the output of the neural network, we can then apply the chain rule to compute gradients for H and any other parameters in the neural network. aL aW aL ah 305 Learning uses Adadelta (Zeiler, 2012), which has been employed in past work (Kim, 2014). We found that Adagrad (Duchi et al., 2011) performed equally well with tuned regularization and step size parameters, but Adadelta worked better out of the box. We set the momentum term p = 0.95 (as suggested by Zeiler (2012)) and did not regularize the weights at all. We used a minibatch size of 200 trees, although the system was not particularly sensitive to this. For each treebank, we trained for either 10 passes through the treebank or 1000 minibatches, whichever is shorter. We initialized the output weight matrix W to zero. To break symmetry, the lower level neural network parameters H were initialized with each entry being indep</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, Feature-based, Conditional Random Field Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8502" citStr="Finkel et al., 2008" startWordPosition="1387" endWordPosition="1390">matically. As we will see, the module on the left can be be a neural net, a linear function of surface features, or a combination of the two, as long as it provides anchored rule scores, and the structured inference component is the same regardless (CKY). A PCFG estimated with maximum likelihood has φ(w, r, s) = log P(r|parent(r)), which is independent of the anchoring s and the words w except for preterminal productions; a basic discriminative parser might let this be a learned parameter but still disregard the surface information. However, surface features can capture useful syntactic cues (Finkel et al., 2008; Hall et al., 2014). Consider the example in Figure 2: the proposed parent NP is preceded by the word reflected and followed by a period, which is a surface context characteristic of NPs or PPs in object position. Beginning with the and ending with personality are typical properties of NPs as well, and the choice of the particular rule NP —* NP PP is supported by the fact that the proposed child PP begins with of. This information can be captured with sparse features (fs in Figure 2) or, as we describe below, with a neural network taking lexical context as input. 2.2 Scoring Anchored Rules Fo</context>
<context position="13939" citStr="Finkel et al., 2008" startWordPosition="2373" endWordPosition="2376">igure 2; in particular, we look two words in either direction around each point of interest, meaning the neural net takes 12 words as input.7 For our word embeddings v, we use pre-trained word vectors from Bansal et al. (2014). We compare with other sources of word vectors in Section 5. Contrary to standard practice, we do not update these vectors during training; we found that doing so did not provide an accuracy benefit and slowed down training considerably. 2.4 Grammar Refinements A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014). Using finer-grained symbols in our rules r gives the model greater capacity, but also introduces more parameters into W and 6The model actually uses the longest suffix of each word occurring at least 100 times in the training set, up to the entire word. Removing this abstraction of rare words harms performance. 7The sparse model did not benefit from using this larger neighborhood, so improvements from the neural net are not simply due to considering more lexical context. increases the ability to overfit. Following Hall et al. (2014), we use grammar</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, Feature-based, Conditional Random Field Parsing. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Less Grammar, More Features.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="894" citStr="Hall et al., 2014" startWordPosition="130" endWordPosition="133">zation of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system1 achieves 91.1 Fs on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from </context>
<context position="5220" citStr="Hall et al. (2014)" startWordPosition="825" endWordPosition="828">se conjunctions with sparse rule indicator features fo are scored according to parameters W. our core inference mechanism, allowing us to use tricks like coarse pruning that make inference efficient in the purely sparse model. Our model can be trained by gradient descent exactly as in a conventional CRF, with the gradient of the network parameters naturally computed by backpropagating a difference of expected anchored rule counts through the network for each span and split point. Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014). 2 Model Figure 1 shows our neural CRF model. The model decomposes over anchored rules, and it scores each of these with a potential function; in a standard CRF, these potentials ar</context>
<context position="8522" citStr="Hall et al., 2014" startWordPosition="1391" endWordPosition="1394"> see, the module on the left can be be a neural net, a linear function of surface features, or a combination of the two, as long as it provides anchored rule scores, and the structured inference component is the same regardless (CKY). A PCFG estimated with maximum likelihood has φ(w, r, s) = log P(r|parent(r)), which is independent of the anchoring s and the words w except for preterminal productions; a basic discriminative parser might let this be a learned parameter but still disregard the surface information. However, surface features can capture useful syntactic cues (Finkel et al., 2008; Hall et al., 2014). Consider the example in Figure 2: the proposed parent NP is preceded by the word reflected and followed by a period, which is a surface context characteristic of NPs or PPs in object position. Beginning with the and ending with personality are typical properties of NPs as well, and the choice of the particular rule NP —* NP PP is supported by the fact that the proposed child PP begins with of. This information can be captured with sparse features (fs in Figure 2) or, as we describe below, with a neural network taking lexical context as input. 2.2 Scoring Anchored Rules Following Hall et al. </context>
<context position="12671" citStr="Hall et al. (2014)" startWordPosition="2155" endWordPosition="2158">er, embedding features rather than words has also been shown to be effective (Chen et al., 2014). fo Wij = e ght(IIf.,i f.,jff) fo W h v(f�) fw W fl, 304 more discriminating power. Also note that it is possible to use deeper networks or more sophisticated architectures here; we will return to this in Section 6. Our two models can be easily combined: O(w, r, s; W1, H, W2) = Osparse(w, r, s; W1) + Oneural(w, r, s; H, W2) Weights for each component of the scoring function can be learned fully jointly and inference proceeds as before. 2.3 Features We take fs to be the set of features described in Hall et al. (2014). At the preterminal layer, the model considers prefixes and suffixes up to length 5 of the current word and neighboring words, as well as the words’ identities. For nonterminal productions, we fire indicators on the words6 before and after the start, end, and split point of the anchored rule (as shown in Figure 2) as well as on two other span properties, span length and span shape (an indicator of where capitalized words, numbers, and punctuation occur in the span). For our neural model, we take fw for all productions (preterminal and nonterminal) to be the words surrounding the beginning and</context>
<context position="13983" citStr="Hall et al., 2014" startWordPosition="2381" endWordPosition="2384">either direction around each point of interest, meaning the neural net takes 12 words as input.7 For our word embeddings v, we use pre-trained word vectors from Bansal et al. (2014). We compare with other sources of word vectors in Section 5. Contrary to standard practice, we do not update these vectors during training; we found that doing so did not provide an accuracy benefit and slowed down training considerably. 2.4 Grammar Refinements A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014). Using finer-grained symbols in our rules r gives the model greater capacity, but also introduces more parameters into W and 6The model actually uses the longest suffix of each word occurring at least 100 times in the training set, up to the entire word. Removing this abstraction of rare words harms performance. 7The sparse model did not benefit from using this larger neighborhood, so improvements from the neural net are not simply due to considering more lexical context. increases the ability to overfit. Following Hall et al. (2014), we use grammars with very little annotation: we use no hor</context>
<context position="17327" citStr="Hall et al. (2014)" startWordPosition="2976" endWordPosition="2979">. We initialized the output weight matrix W to zero. To break symmetry, the lower level neural network parameters H were initialized with each entry being independently sampled from a Gaussian with mean 0 and variance 0.01; Gaussian performed better than uniform initialization, but the variance was not important. 4 Inference Our baseline and neural model both score anchored rule productions. We can use CKY in the standard fashion to compute either expected anchored rule counts EP(T|,)[(r, s)] or the Viterbi tree arg maxT P(T Iw). We speed up inference by using a coarse pruning pass. We follow Hall et al. (2014) and prune according to an X-bar grammar with headoutward binarization, ruling out any constituent whose max marginal probability is less than a−9. With this pruning, the number of spans and split points to be considered is greatly reduced; however, we still need to compute the neural network activations for each remaining span and split point, of which there may be thousands for a given sentence.8 We can improve efficiency further by noting that the same word will appear in the same position in a large number of span/split point combinations, and cache the contribution to the hidden layer cau</context>
<context position="19747" citStr="Hall et al., 2014" startWordPosition="3398" endWordPosition="3401">alb. Full test results and comparisons to other systems are shown in Table 4. We compare variants of our system along two axes: whether they use standard linear sparse features, nonlinear dense features from the neural net, or both, and whether any word representations (vectors or clusters) are used. Sparse vs. neural The neural CRF (line (d) in Table 1) on its own outperforms the sparse CRF (a, b) even when the sparse CRF has a more heavily annotated grammar. This is a surprising result: the features in the sparse CRF have been carefully engineered to capture a range of linguistic phenomena (Hall et al., 2014), and there is no guarantee that word vectors will capture the same. For example, at the POS tagging layer, the sparse model looks at prefixes and suffixes of words, which give the model access to morphology for predicting tags of unknown words, which typically have regular inflection patterns. By contrast, the neural model must rely on the geometry of the vector space exposing useful regularities. At the same time, the strong performance of the combination of the two systems (g) indicates that not only are both featurization approaches highperforming on their own, but that they have complemen</context>
<context position="29023" citStr="Hall et al. (2014)" startWordPosition="4991" endWordPosition="4994">ve performance of this method. 7 Test Results We evaluate our system under two conditions: first, on the English Penn Treebank, and second, on the nine languages used in the SPMRL 2013 and 2014 shared tasks. 7.1 Penn Treebank Table 4 reports results on section 23 of the Penn Treebank (PTB). We focus our comparison on single parser systems as opposed to rerankers, ensembles, or self-trained methods (though these are also mentioned for context). First, we compare against v(f.) fw Kfo W h fo fo W v(f.) fw 308 Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Avg Dev, all lengths Hall et al. (2014) 78.89 83.74 79.40 83.28 88.06 87.44 81.85 91.10 75.95 83.30 This work* 80.68 84.37 80.65 85.25 89.37 89.46 82.35 92.10 77.93 84.68 Test, all lengths Berkeley 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53 Berkeley-Tags 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89 Crabb´e and Seddah (2014) 77.66 85.35 79.68 77.15 86.19 87.51 79.35 91.60 82.72 83.02 Hall et al. (2014) 78.75 83.39 79.70 78.43 87.18 88.25 80.18 90.66 82.00 83.17 This work* 80.24 85.41 81.25 80.95 88.61 90.66 82.23 92.97 83.45 85.08 Reranked ensemble 2014 Best 81.32 88.24 82.53 81.66 89.80 91.72 83.81 9</context>
<context position="30302" citStr="Hall et al. (2014)" startWordPosition="5199" endWordPosition="5202">he SPMRL 2013/2014 Shared Tasks; all values are F-scores for sentences of all lengths using the version of evalb distributed with the shared task. Our parser substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb´e and Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). F1 all Single model, PTB only Hall et al. (2014) 89.2 Berkeley 90.1 Carreras et al. (2008) 91.1 Shindo et al. (2012) single 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) 91.3 This work* 91.1 Extended conditions Charniak and Johnson (2005) 91.5 Socher et al. (2013) 90.4 Vinyals et al. (2014) single 90.5 Vinyals et al. (2014) ensemble 91.6 Shindo et al. (2012) ensemble 92.4 Table 4: Test results on section 23 of the Penn Treebank. We compare to several categories of parsers from the literatures. We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al., 2013) a</context>
<context position="32824" citStr="Hall et al. (2014)" startWordPosition="5630" endWordPosition="5633">SPMRL We also examine the performance of our parser on other languages, specifically the nine morphologically-rich languages used in the SPMRL 2013/2014 shared tasks (Seddah et al., 2013; Seddah et al., 2014). We train word vectors on the monolingual data distributed with the SPMRL 2014 shared task (typically 100M-200M tokens per language) using the skip-gram approach of word2vec with a window size of 1 309 (Mikolov et al., 2013).10 Here we use V = 1 in the backbone grammar, which we found to be beneficial overall. Table 3 shows that our system improves upon the performance of the parser from Hall et al. (2014) as well as the top single parser from the shared task (Crabb´e and Seddah, 2014), with robust improvements on all languages. 8 Conclusion In this work, we presented a CRF parser that scores anchored rule productions using dense input features computed from a feedforward neural net. Because the neural component is modularized, we can easily integrate it into a preexisting learning and inference framework based around dynamic programming of a discrete parse chart. Our combined neural and sparse model gives strong performance both on English and on other languages. Our system is publicly availab</context>
</contexts>
<marker>Hall, Durrett, Klein, 2014</marker>
<rawString>David Hall, Greg Durrett, and Dan Klein. 2014. Less Grammar, More Features. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Inducing History Representations for Broad Coverage Statistical Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<marker>Henderson, 2003</marker>
<rawString>James Henderson. 2003. Inducing History Representations for Broad Coverage Statistical Parsing. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Ioffe</author>
<author>Christian Szegedy</author>
</authors>
<title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv preprint,</title>
<date>2015</date>
<pages>1502--03167</pages>
<contexts>
<context position="25573" citStr="Ioffe and Szegedy, 2015" startWordPosition="4403" endWordPosition="4406">ural nets (Bengio et al., 2003). g(x) = x3 (cube) was found to be most successful by Chen and Manning (2014). Table 2 compares the performance of these three nonlinearities. We see that rectified linear units perform the best, followed by tanh units, followed by cubic units.9 One drawback of tanh as an activation function is that it is easily “saturated” if the input to the unit is too far away from zero, causing the backpropagation of derivatives through that unit to essentially cease; this is known to cause problems for training, requiring special purpose machinery for use in deep networks (Ioffe and Szegedy, 2015). Depth Given that we are using rectified linear units, it bears asking whether or not our implementation is improving substantially over linear features of the continuous input. We can use the embedding vector of an anchored span v(fw) directly as input to a basic linear CRF, as shown in Figure 4a. Table 1 shows that the purely linear architecture (0 HL) performs surprisingly well, but is still less effective than the network with one hidden layer. This agrees with the results of Wang and Manning (2013), who noted that dense features typically benefit from nonlinear modeling. We also compare </context>
</contexts>
<marker>Ioffe, Szegedy, 2015</marker>
<rawString>Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv preprint, arXiv:1502.03167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan ˙Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Opinion Mining with Deep Recurrent Neural Networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>˙Irsoy, Cardie, 2014</marker>
<rawString>Ozan ˙Irsoy and Claire Cardie. 2014. Opinion Mining with Deep Recurrent Neural Networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A Convolutional Neural Network for Modelling Sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1596" citStr="Kalchbrenner et al., 2014" startWordPosition="239" endWordPosition="243">n 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1System available at http://nlp.cs.berkeley.edu output spaces. Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; ˙Irsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014). However, there is a natural marriage of nonlinear induced features and efficient structured inference, as explored by Collobert et al. (2011) for the </context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A Convolutional Neural Network for Modelling Sentences. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional Neural Networks for Sentence Classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1608" citStr="Kim, 2014" startWordPosition="244" endWordPosition="245">and more generally outperforms the best prior single parser results on a range of languages. 1 Introduction Neural network-based approaches to structured NLP tasks have both strengths and weaknesses when compared to more conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1System available at http://nlp.cs.berkeley.edu output spaces. Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; ˙Irsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014). However, there is a natural marriage of nonlinear induced features and efficient structured inference, as explored by Collobert et al. (2011) for the case of sequ</context>
<context position="16225" citStr="Kim, 2014" startWordPosition="2788" endWordPosition="2789">r,s)ET* ⎛ ⎞ ⎝XX P(T |w; H, W) h(w, s; H)fo(r)T ⎠ T (r,s)ET Note that the outer products give matrices of feature counts isomorphic to W. The second expression can be simplified to be in terms of expected feature counts. To update H, we use standard backpropagation by first computing: ⎛⎞ ⎝ X =Wfo(r) ⎠− (r,s)ET* ⎛ ⎞ ⎝XX P(T |w; H, W) W fo(r) ⎠ T (r,s)ET Since h is the output of the neural network, we can then apply the chain rule to compute gradients for H and any other parameters in the neural network. aL aW aL ah 305 Learning uses Adadelta (Zeiler, 2012), which has been employed in past work (Kim, 2014). We found that Adagrad (Duchi et al., 2011) performed equally well with tuned regularization and step size parameters, but Adadelta worked better out of the box. We set the momentum term p = 0.95 (as suggested by Zeiler (2012)) and did not regularize the weights at all. We used a minibatch size of 200 trees, although the system was not particularly sensitive to this. For each treebank, we trained for either 10 passes through the treebank or 1000 minibatches, whichever is shorter. We initialized the output weight matrix W to zero. To break symmetry, the lower level neural network parameters H </context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31986" citStr="Klein and Manning, 2003" startWordPosition="5486" endWordPosition="5489">nly, single parser data condition; we match their performance at 91.1 F1, though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014). 7.2 SPMRL We also examine the performance of our parser on other languages, specifically the nine morphologically-rich languages used in the SPMRL 2013/2014 shared tasks (Seddah et al., 2013; Seddah et al., 2014). We train word vectors on the monolingual data distributed with the SPMRL 2014 shared task (typically 100M-200M tokens per language) using the skip-gram approach of word2v</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23615" citStr="Koo et al., 2008" startWordPosition="4070" endWordPosition="4073">onlinearity Tanh 90.74 −0.23 Cube 89.94 −1.03 0 HL 90.54 −0.43 Depth 1 HL 90.97 — 2 HL 90.58 −0.39 Embed output 88.81 −2.16 Table 2: Exploration of other implementation choices in the feedforward neural network on sentences of length &lt; 40 from section 22 of the Penn Treebank. Rectified linear units perform better than tanh or cubic units, a network with one hidden layer performs best, and embedding the output feature vector gives worse performance. sparse model with similar information for a true apples-to-apples comparison. Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al. (2014). However, Table 1 shows that these features provide no benefit to the baseline model, which suggests either that it is difficult to learn reliable weights for these as sparse features or t</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>ImageNet Classification with Deep Convolutional Neural Networks.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="24862" citStr="Krizhevsky et al., 2012" startWordPosition="4284" endWordPosition="4287">ies are being captured by the word embeddings. 6 Design Choices The neural net design space is large, so we wish to analyze the particular design choices we made for this system by examining the performance of several variants of the neural net architecture used in our system. Table 2 shows development results from potential alternate architectural choices, which we now discuss. Choice of nonlinearity The choice of nonlinearity g has been frequently discussed in the neural network literature. Our choice g(x) = max(x, 0), a rectified linear unit, is increasingly popular in 307 computer vision (Krizhevsky et al., 2012). g(x) = tanh(x) is a traditional nonlinearity widely used throughout the history of neural nets (Bengio et al., 2003). g(x) = x3 (cube) was found to be most successful by Chen and Manning (2014). Table 2 compares the performance of these three nonlinearities. We see that rectified linear units perform the best, followed by tanh units, followed by cubic units.9 One drawback of tanh as an activation function is that it is easily “saturated” if the input to the unit is too far away from zero, causing the backpropagation of derivatives through that unit to essentially cease; this is known to caus</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>The insideoutside recursive neural network model for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2044" citStr="Zuidema, 2014" startWordPosition="310" endWordPosition="311">aces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1System available at http://nlp.cs.berkeley.edu output spaces. Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; ˙Irsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014). However, there is a natural marriage of nonlinear induced features and efficient structured inference, as explored by Collobert et al. (2011) for the case of sequence modeling: feedforward neural networks can be used to score local decisions which are then “reconciled” in a discrete structured modeling framework, allowing inference via dynamic programming. In this work, we present a CRF constituency parser based on these principles, where individual anchored rule productions are scored based on nonlinear features computed with a feedforward neural network. A separate, identicallyparameterize</context>
<context position="3407" citStr="Zuidema, 2014" startWordPosition="523" endWordPosition="524">pan boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying 302 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 302–312, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics personality . reflected the flip side of the Stoltzman NP r = NP ! NP P</context>
</contexts>
<marker>Zuidema, 2014</marker>
<rawString>Phong Le and Willem Zuidema. 2014. The insideoutside recursive neural network model for dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-Rank Tensors for Scoring Dependency Structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27467" citStr="Lei et al., 2014" startWordPosition="4730" endWordPosition="4733">ayer, similar to low-rank factorizations of parameter matrices used in past 9The performance of cube decreased substantially late in learning; it peaked at around 90.52. Dropout may be useful for alleviating this type of overfitting, but in our experiments we did not find dropout to be beneficial overall. a) 0 = v(fw)&gt;Wfo b) 0 = g(Hv(fw))&gt;WKfo Figure 4: Two additional forms of the scoring function. a) Linear version of the dense model, equivalent to a CRF with continuous-valued input features. b) Version of the dense model where outputs are also embedded according to a learned matrix K. work (Lei et al., 2014). This approach saves us from having to learn a separate row of W for every rule in the grammar; if rules are given similar embeddings, then they will behave similarly according to the model. We experimented with noe = 20 and show the results in Table 2. Unfortunately, this approach does not seem to work well for parsing. Learning the output representation was empirically very unstable, and it also required careful initialization. We tried Gaussian initialization (as in the rest of our model) and initializing the model by clustering rules either randomly or according to their parent symbol. Th</context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-Rank Tensors for Scoring Dependency Structures. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>DependencyBased Word Embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="20572" citStr="Levy and Goldberg, 2014" startWordPosition="3535" endWordPosition="3538">hology for predicting tags of unknown words, which typically have regular inflection patterns. By contrast, the neural model must rely on the geometry of the vector space exposing useful regularities. At the same time, the strong performance of the combination of the two systems (g) indicates that not only are both featurization approaches highperforming on their own, but that they have complementary strengths. Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al. (2013) using contexts from dependency links; a similar approach was also suggested by Levy and Goldberg (2014). 306 Sparse Neural V Word Reps Fl len ≤ 40 Fl all Hall et al. (2014), V = 1 90.5 a ✓ 0 89.89 89.22 b ✓ 1 90.82 90.13 c ✓ 1 Brown 90.80 90.17 d ✓ 0 Bansal 90.97 90.44 e ✓ 0 Collobert 90.25 89.63 f ✓ 0 PTB 89.34 88.99 g ✓ ✓ 0 Bansal 92.04 91.34 h ✓ ✓ 0 PTB 91.39 90.91 Table 1: Results of our sparse CRF, neural CRF, and combined parsing models on section 22 of the Penn Treebank. Systems a</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. DependencyBased Word Embeddings. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="19110" citStr="Marcus et al., 1993" startWordPosition="3285" endWordPosition="3288"> split point. expensive. However, because only a small number of rules can apply to a given span and split point, fo is sparse and we can selectively compute the terms necessary for the final bilinear product. Our combined sparse and neural model trains on the Penn Treebank in 24 hours on a single machine with a parallelized CPU implementation. For reference, the purely sparse model with a parentannotated grammar (necessary for the best results) takes around 15 hours on the same machine. 5 System Ablations Table 1 shows results on section 22 (the development set) of the English Penn Treebank (Marcus et al., 1993), computed using evalb. Full test results and comparisons to other systems are shown in Table 4. We compare variants of our system along two axes: whether they use standard linear sparse features, nonlinear dense features from the neural net, or both, and whether any word representations (vectors or clusters) are used. Sparse vs. neural The neural CRF (line (d) in Table 1) on its own outperforms the sparse CRF (a, b) even when the sparse CRF has a more heavily annotated grammar. This is a surprising result: the features in the sparse CRF have been carefully engineered to capture a range of lin</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Learning Representations.</booktitle>
<contexts>
<context position="20679" citStr="Mikolov et al. (2013)" startWordPosition="3556" endWordPosition="3559">e neural model must rely on the geometry of the vector space exposing useful regularities. At the same time, the strong performance of the combination of the two systems (g) indicates that not only are both featurization approaches highperforming on their own, but that they have complementary strengths. Unlabeled data Much attention has been paid to the choice of word vectors for various NLP tasks, notably whether they capture more syntactic or semantic phenomena (Bansal et al., 2014; Levy and Goldberg, 2014). We primarily use vectors from Bansal et al. (2014), who train the skipgram model of Mikolov et al. (2013) using contexts from dependency links; a similar approach was also suggested by Levy and Goldberg (2014). 306 Sparse Neural V Word Reps Fl len ≤ 40 Fl all Hall et al. (2014), V = 1 90.5 a ✓ 0 89.89 89.22 b ✓ 1 90.82 90.13 c ✓ 1 Brown 90.80 90.17 d ✓ 0 Bansal 90.97 90.44 e ✓ 0 Collobert 90.25 89.63 f ✓ 0 PTB 89.34 88.99 g ✓ ✓ 0 Bansal 92.04 91.34 h ✓ ✓ 0 PTB 91.39 90.91 Table 1: Results of our sparse CRF, neural CRF, and combined parsing models on section 22 of the Penn Treebank. Systems are broken down by whether local potentials come from sparse features and/or the neural network (the primary</context>
<context position="32639" citStr="Mikolov et al., 2013" startWordPosition="5595" endWordPosition="5598">arger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014). 7.2 SPMRL We also examine the performance of our parser on other languages, specifically the nine morphologically-rich languages used in the SPMRL 2013/2014 shared tasks (Seddah et al., 2013; Seddah et al., 2014). We train word vectors on the monolingual data distributed with the SPMRL 2014 shared task (typically 100M-200M tokens per language) using the skip-gram approach of word2vec with a window size of 1 309 (Mikolov et al., 2013).10 Here we use V = 1 in the backbone grammar, which we found to be beneficial overall. Table 3 shows that our system improves upon the performance of the parser from Hall et al. (2014) as well as the top single parser from the shared task (Crabb´e and Seddah, 2014), with robust improvements on all languages. 8 Conclusion In this work, we presented a CRF parser that scores anchored rule productions using dense input features computed from a feedforward neural net. Because the neural component is modularized, we can easily integrate it into a preexisting learning and inference framework based a</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. In Proceedings of the International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5440" citStr="Petrov and Klein, 2007" startWordPosition="863" endWordPosition="866">arse model. Our model can be trained by gradient descent exactly as in a conventional CRF, with the gradient of the network parameters naturally computed by backpropagating a difference of expected anchored rule counts through the network for each span and split point. Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014). 2 Model Figure 1 shows our neural CRF model. The model decomposes over anchored rules, and it scores each of these with a potential function; in a standard CRF, these potentials are typically linear functions of sparse indicator features, whereas fg [[PreviousWord = reflected]], [[SpanLength = 7]], É Figure 2: Example of an anchored rule production for the rule NP → NP PP. From the anchoring s = (</context>
<context position="30850" citStr="Petrov and Klein, 2007" startWordPosition="5290" endWordPosition="5293">Bj¨orkelund et al., 2014). F1 all Single model, PTB only Hall et al. (2014) 89.2 Berkeley 90.1 Carreras et al. (2008) 91.1 Shindo et al. (2012) single 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) 91.3 This work* 91.1 Extended conditions Charniak and Johnson (2005) 91.5 Socher et al. (2013) 90.4 Vinyals et al. (2014) single 90.5 Vinyals et al. (2014) ensemble 91.6 Shindo et al. (2012) ensemble 92.4 Table 4: Test results on section 23 of the Penn Treebank. We compare to several categories of parsers from the literatures. We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al., 2013) and we match the performance of sophisticated generative (Shindo et al., 2012) and discriminative (Carreras et al., 2008) parsers. four parsers trained only on the PTB with no auxiliary data: the CRF parser of Hall et al. (2014), the Berkeley parser (Petrov and Klein, 2007), the discriminative parser of Carreras et al. (2008), and the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.1 F1, though we also</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="13963" citStr="Petrov and Klein, 2008" startWordPosition="2377" endWordPosition="2380">r, we look two words in either direction around each point of interest, meaning the neural net takes 12 words as input.7 For our word embeddings v, we use pre-trained word vectors from Bansal et al. (2014). We compare with other sources of word vectors in Section 5. Contrary to standard practice, we do not update these vectors during training; we found that doing so did not provide an accuracy benefit and slowed down training considerably. 2.4 Grammar Refinements A recurring issue in discriminative constituency parsing is the granularity of annotation in the base grammar (Finkel et al., 2008; Petrov and Klein, 2008; Hall et al., 2014). Using finer-grained symbols in our rules r gives the model greater capacity, but also introduces more parameters into W and 6The model actually uses the longest suffix of each word occurring at least 100 times in the training set, up to the entire word. Removing this abstraction of rare words harms performance. 7The sparse model did not benefit from using this larger neighborhood, so improvements from the neural net are not simply due to considering more lexical context. increases the ability to overfit. Following Hall et al. (2014), we use grammars with very little annot</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008. Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Products of Random Latent Variable Grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="32094" citStr="Petrov, 2010" startWordPosition="5507" endWordPosition="5508"> unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014). 7.2 SPMRL We also examine the performance of our parser on other languages, specifically the nine morphologically-rich languages used in the SPMRL 2013/2014 shared tasks (Seddah et al., 2013; Seddah et al., 2014). We train word vectors on the monolingual data distributed with the SPMRL 2014 shared task (typically 100M-200M tokens per language) using the skip-gram approach of word2vec with a window size of 1 309 (Mikolov et al., 2013).10 Here we use V = 1 in the backbone grammar, which we</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Products of Random Latent Variable Grammars. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Djam´e Seddah</author>
<author>Reut Tsarfaty</author>
<author>Sandra K¨ubler</author>
<author>Marie Candito</author>
<author>Jinho D Choi</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
</authors>
<title>Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepi´orkowski, Ryan Roth, Wolfgang Seeker,</title>
<date>2013</date>
<journal>Overview of the SPMRL</journal>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages.</booktitle>
<institution>Yannick Versley, Veronika Vincze, Marcin Woli´nski, and Alina Wr´oblewska.</institution>
<marker>Seddah, Tsarfaty, K¨ubler, Candito, Choi, Farkas, Foster, 2013</marker>
<rawString>Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie Candito, Jinho D. Choi, Rich´ard Farkas, Jennifer Foster, Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepi´orkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, and Alina Wr´oblewska. 2013. Overview of the SPMRL 2013 Shared Task: A Cross-Framework Evaluation of Parsing Morphologically Rich Languages. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam´e Seddah</author>
<author>Sandra K¨ubler</author>
<author>Reut Tsarfaty</author>
</authors>
<title>Introducing the SPMRL 2014 Shared Task on Parsing Morphologically-rich Languages.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages.</booktitle>
<marker>Seddah, K¨ubler, Tsarfaty, 2014</marker>
<rawString>Djam´e Seddah, Sandra K¨ubler, and Reut Tsarfaty. 2014. Introducing the SPMRL 2014 Shared Task on Parsing Morphologically-rich Languages. In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shindo</author>
<author>Yusuke Miyao</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Bayesian Symbol-refined Tree Substitution Grammars for Syntactic Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="30370" citStr="Shindo et al. (2012)" startWordPosition="5211" endWordPosition="5214">nces of all lengths using the version of evalb distributed with the shared task. Our parser substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb´e and Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). F1 all Single model, PTB only Hall et al. (2014) 89.2 Berkeley 90.1 Carreras et al. (2008) 91.1 Shindo et al. (2012) single 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) 91.3 This work* 91.1 Extended conditions Charniak and Johnson (2005) 91.5 Socher et al. (2013) 90.4 Vinyals et al. (2014) single 90.5 Vinyals et al. (2014) ensemble 91.6 Shindo et al. (2012) ensemble 92.4 Table 4: Test results on section 23 of the Penn Treebank. We compare to several categories of parsers from the literatures. We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al., 2013) and we match the performance of sophisticated generative (Shindo et a</context>
</contexts>
<marker>Shindo, Miyao, Fujino, Nagata, 2012</marker>
<rawString>Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and Masaaki Nagata. 2012. Bayesian Symbol-refined Tree Substitution Grammars for Syntactic Parsing. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing With Compositional Vector Grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1835" citStr="Socher et al., 2013" startWordPosition="274" endWordPosition="277">re conventional models, such conditional random fields (CRFs). A key strength of neural approaches is their ability to learn nonlinear interactions between underlying features. In the case of unstructured output spaces, this capability has led to gains in problems ranging from syntax (Chen and Manning, 2014; Belinkov et al., 2014) to lexical semantics (Kalchbrenner et al., 2014; Kim, 2014). Neural methods are also powerful tools in the case of structured 1System available at http://nlp.cs.berkeley.edu output spaces. Here, past work has often relied on recurrent architectures (Henderson, 2003; Socher et al., 2013; ˙Irsoy and Cardie, 2014), which can propagate information through structure via realvalued hidden state, but as a result do not admit efficient dynamic programming (Socher et al., 2013; Le and Zuidema, 2014). However, there is a natural marriage of nonlinear induced features and efficient structured inference, as explored by Collobert et al. (2011) for the case of sequence modeling: feedforward neural networks can be used to score local decisions which are then “reconciled” in a discrete structured modeling framework, allowing inference via dynamic programming. In this work, we present a CRF</context>
<context position="3384" citStr="Socher et al., 2013" startWordPosition="517" endWordPosition="520">rds at the split point and span boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying 302 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 302–312, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics personality . reflected the flip side of the Sto</context>
<context position="5384" citStr="Socher et al. (2013)" startWordPosition="853" endWordPosition="856">runing that make inference efficient in the purely sparse model. Our model can be trained by gradient descent exactly as in a conventional CRF, with the gradient of the network parameters naturally computed by backpropagating a difference of expected anchored rule counts through the network for each span and split point. Using dense learned features alone, the neural CRF model obtains high performance, outperforming the CRF parser of Hall et al. (2014). When sparse indicators are used in addition, the resulting model gets 91.1 F1 on section 23 of the Penn Treebank, outperforming the parser of Socher et al. (2013) as well as the Berkeley Parser (Petrov and Klein, 2007) and matching the discriminative parser of Carreras et al. (2008). The model also obtains the best single parser results on nine other languages, again outperforming the system of Hall et al. (2014). 2 Model Figure 1 shows our neural CRF model. The model decomposes over anchored rules, and it scores each of these with a potential function; in a standard CRF, these potentials are typically linear functions of sparse indicator features, whereas fg [[PreviousWord = reflected]], [[SpanLength = 7]], É Figure 2: Example of an anchored rule prod</context>
<context position="30532" citStr="Socher et al. (2013)" startWordPosition="5237" endWordPosition="5240">dataset (Hall et al., 2014; Crabb´e and Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). F1 all Single model, PTB only Hall et al. (2014) 89.2 Berkeley 90.1 Carreras et al. (2008) 91.1 Shindo et al. (2012) single 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) 91.3 This work* 91.1 Extended conditions Charniak and Johnson (2005) 91.5 Socher et al. (2013) 90.4 Vinyals et al. (2014) single 90.5 Vinyals et al. (2014) ensemble 91.6 Shindo et al. (2012) ensemble 92.4 Table 4: Test results on section 23 of the Penn Treebank. We compare to several categories of parsers from the literatures. We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al., 2013) and we match the performance of sophisticated generative (Shindo et al., 2012) and discriminative (Carreras et al., 2008) parsers. four parsers trained only on the PTB with no auxiliary data: the CRF parser of Hall et al. (2014), t</context>
<context position="31773" citStr="Socher et al. (2013)" startWordPosition="5449" endWordPosition="5452">etrov and Klein, 2007), the discriminative parser of Carreras et al. (2008), and the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.1 F1, though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014). 7.2 SPMRL We also examine the performance of our parser on other languages, specifically the nine morphologically-rich languages used in the SPMRL 2013/2014 shared tasks (</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing With Compositional Vector Grammars. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Srikumar</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning Distributed Representations for Structured Output Prediction.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="26568" citStr="Srikumar and Manning (2014)" startWordPosition="4570" endWordPosition="4573">urprisingly well, but is still less effective than the network with one hidden layer. This agrees with the results of Wang and Manning (2013), who noted that dense features typically benefit from nonlinear modeling. We also compare against a two-layer neural network, but find that this also performs worse than the one-layer architecture. Densifying output features Overall, it appears beneficial to use dense representations of surface features; a natural question that one might ask is whether the same technique can be applied to the sparse output feature vector fo. We can apply the approach of Srikumar and Manning (2014) and multiply the sparse output vector by a dense matrix K, giving the following scoring function (shown in Figure 4b): O(w, r, s; H, W, K) = g(Hv(fw(w, s)))TWKfo(r) where W is now nh × noe and K is noe × no. WK can be seen a low-rank approximation of the original W at the output layer, similar to low-rank factorizations of parameter matrices used in past 9The performance of cube decreased substantially late in learning; it peaked at around 90.52. Dropout may be useful for alleviating this type of overfitting, but in our experiments we did not find dropout to be beneficial overall. a) 0 = v(fw</context>
</contexts>
<marker>Srikumar, Manning, 2014</marker>
<rawString>Vivek Srikumar and Christopher D Manning. 2014. Learning Distributed Representations for Structured Output Prediction. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuta Tsuboi</author>
</authors>
<title>Neural Networks Leverage Corpus-wide Information for Part-of-speech Tagging.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3341" citStr="Tsuboi, 2014" startWordPosition="511" endWordPosition="512">it takes vector representations of words at the split point and span boundaries; it then outputs scores for anchored rules applied to that span and split point. These scores can be thought of as nonlinear potentials analogous to linear potentials in conventional CRFs. Crucially, while the network replicates are connected in a unified model, their computations factor along the same substructures as in standard CRFs. Prior work on parsing using neural network models has often sidestepped the problem of structured inference by making sequential decisions (Henderson, 2003; Chen and Manning, 2014; Tsuboi, 2014) or by doing reranking (Socher et al., 2013; Le and Zuidema, 2014); by contrast, our framework permits exact inference via CKY, since the model’s structured interactions are purely discrete and do not involve continuous hidden state. Therefore, we can exploit a neural net’s capacity to learn nonlinear features without modifying 302 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 302–312, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics perso</context>
</contexts>
<marker>Tsuboi, 2014</marker>
<rawString>Yuta Tsuboi. 2014. Neural Networks Leverage Corpus-wide Information for Part-of-speech Tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representations: A Simple and General Method for Semi-supervised Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23636" citStr="Turian et al., 2010" startWordPosition="4074" endWordPosition="4077">0.74 −0.23 Cube 89.94 −1.03 0 HL 90.54 −0.43 Depth 1 HL 90.97 — 2 HL 90.58 −0.39 Embed output 88.81 −2.16 Table 2: Exploration of other implementation choices in the feedforward neural network on sentences of length &lt; 40 from section 22 of the Penn Treebank. Rectified linear units perform better than tanh or cubic units, a network with one hidden layer performs best, and embedding the output feature vector gives worse performance. sparse model with similar information for a true apples-to-apples comparison. Brown clusters have been shown to be effective vehicles in the past (Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014). We can incorporate Brown clusters into the baseline CRF model in an analogous way to how embedding features are used in the dense model: surface features are fired on Brown cluster identities (we use prefixes of length 4 and 10) of key words. We use the Brown clusters from Koo et al. (2008), which are trained on the same data as the vectors of Bansal et al. (2014). However, Table 1 shows that these features provide no benefit to the baseline model, which suggests either that it is difficult to learn reliable weights for these as sparse features or that different regular</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-supervised Learning. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Lukasz Kaiser</author>
<author>Terry Koo</author>
<author>Slav Petrov</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Grammar as a Foreign Language. CoRR,</title>
<date>2014</date>
<contexts>
<context position="30559" citStr="Vinyals et al. (2014)" startWordPosition="5242" endWordPosition="5245">; Crabb´e and Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). F1 all Single model, PTB only Hall et al. (2014) 89.2 Berkeley 90.1 Carreras et al. (2008) 91.1 Shindo et al. (2012) single 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) 91.3 This work* 91.1 Extended conditions Charniak and Johnson (2005) 91.5 Socher et al. (2013) 90.4 Vinyals et al. (2014) single 90.5 Vinyals et al. (2014) ensemble 91.6 Shindo et al. (2012) ensemble 92.4 Table 4: Test results on section 23 of the Penn Treebank. We compare to several categories of parsers from the literatures. We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al., 2013) and we match the performance of sophisticated generative (Shindo et al., 2012) and discriminative (Carreras et al., 2008) parsers. four parsers trained only on the PTB with no auxiliary data: the CRF parser of Hall et al. (2014), the Berkeley parser (Petrov </context>
<context position="31831" citStr="Vinyals et al. (2014)" startWordPosition="5460" endWordPosition="5463">eras et al. (2008), and the single TSG parser of Shindo et al. (2012). To our knowledge, the latter two systems are the highest performing in this PTB-only, single parser data condition; we match their performance at 91.1 F1, though we also use word vectors computed from unlabeled data. We further compare to the shiftreduce parser of Zhu et al. (2013), which uses unlabeled data in the form of Brown clusters. Our method achieves performance close to that of their parser. We also compare to the compositional vector grammar (CVG) parser of Socher et al. (2013) as well as the LSTM-based parser of Vinyals et al. (2014). The conditions these parsers are operating under are slightly different: the former is a reranker on top of the Stanford Parser (Klein and Manning, 2003) and the latter trains on much larger amounts of data parsed by a product of Berkeley parsers (Petrov, 2010). Regardless, we outperform the CVG parser as well as the single parser results from Vinyals et al. (2014). 7.2 SPMRL We also examine the performance of our parser on other languages, specifically the nine morphologically-rich languages used in the SPMRL 2013/2014 shared tasks (Seddah et al., 2013; Seddah et al., 2014). We train word v</context>
</contexts>
<marker>Vinyals, Kaiser, Koo, Petrov, Sutskever, Hinton, 2014</marker>
<rawString>Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton. 2014. Grammar as a Foreign Language. CoRR, abs/1412.7449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Effect of Non-linear Deep Architecture in Sequence Labeling.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="7576" citStr="Wang and Manning, 2013" startWordPosition="1229" endWordPosition="1232">re CRFs that decompose over anchored rule productions and place a probability distribution over trees conditioned on a sentence w as follows: ⎛ P(T |w) ∝ exp ⎝0(w, r, s) ⎞ ⎠ (r,s)∈T 2Throughout this work, we will primarily consider two potential functions: linear functions of sparse indicators and nonlinear neural networks over dense, continuous features. Although other modeling choices are possible, these two points in the design space reflect common choices in NLP, and past work has suggested that nonlinear functions of indicators or linear functions of dense features may perform less well (Wang and Manning, 2013). 3For simplicity of exposition, we ignore unary rules; however, they are easily supported in this framework by simply specifying a null value for the split point. 303 where φ is a scoring function that considers the input sentence and the anchored rule in question. Figure 1 shows this scoring process schematically. As we will see, the module on the left can be be a neural net, a linear function of surface features, or a combination of the two, as long as it provides anchored rule scores, and the structured inference component is the same regardless (CKY). A PCFG estimated with maximum likelih</context>
<context position="26082" citStr="Wang and Manning (2013)" startWordPosition="4493" endWordPosition="4496">ause problems for training, requiring special purpose machinery for use in deep networks (Ioffe and Szegedy, 2015). Depth Given that we are using rectified linear units, it bears asking whether or not our implementation is improving substantially over linear features of the continuous input. We can use the embedding vector of an anchored span v(fw) directly as input to a basic linear CRF, as shown in Figure 4a. Table 1 shows that the purely linear architecture (0 HL) performs surprisingly well, but is still less effective than the network with one hidden layer. This agrees with the results of Wang and Manning (2013), who noted that dense features typically benefit from nonlinear modeling. We also compare against a two-layer neural network, but find that this also performs worse than the one-layer architecture. Densifying output features Overall, it appears beneficial to use dense representations of surface features; a natural question that one might ask is whether the same technique can be applied to the sparse output feature vector fo. We can apply the approach of Srikumar and Manning (2014) and multiply the sparse output vector by a dense matrix K, giving the following scoring function (shown in Figure</context>
</contexts>
<marker>Wang, Manning, 2013</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2013. Effect of Non-linear Deep Architecture in Sequence Labeling. In Proceedings of the International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<title>ADADELTA: An Adaptive Learning Rate Method.</title>
<date>2012</date>
<location>CoRR, abs/1212.5701.</location>
<contexts>
<context position="16175" citStr="Zeiler, 2012" startWordPosition="2779" endWordPosition="2780">of log-linear models: ⎛ ⎞ ⎝ X = h(w, s; H)fo(r)T ⎠− (r,s)ET* ⎛ ⎞ ⎝XX P(T |w; H, W) h(w, s; H)fo(r)T ⎠ T (r,s)ET Note that the outer products give matrices of feature counts isomorphic to W. The second expression can be simplified to be in terms of expected feature counts. To update H, we use standard backpropagation by first computing: ⎛⎞ ⎝ X =Wfo(r) ⎠− (r,s)ET* ⎛ ⎞ ⎝XX P(T |w; H, W) W fo(r) ⎠ T (r,s)ET Since h is the output of the neural network, we can then apply the chain rule to compute gradients for H and any other parameters in the neural network. aL aW aL ah 305 Learning uses Adadelta (Zeiler, 2012), which has been employed in past work (Kim, 2014). We found that Adagrad (Duchi et al., 2011) performed equally well with tuned regularization and step size parameters, but Adadelta worked better out of the box. We set the momentum term p = 0.95 (as suggested by Zeiler (2012)) and did not regularize the weights at all. We used a minibatch size of 200 trees, although the system was not particularly sensitive to this. For each treebank, we trained for either 10 passes through the treebank or 1000 minibatches, whichever is shorter. We initialized the output weight matrix W to zero. To break symm</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D. Zeiler. 2012. ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yue Zhang</author>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Fast and Accurate ShiftReduce Constituent Parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="30437" citStr="Zhu et al. (2013)" startWordPosition="5222" endWordPosition="5225">ared task. Our parser substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabb´e and Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the best published numbers on this dataset (Bj¨orkelund et al., 2013; Bj¨orkelund et al., 2014). F1 all Single model, PTB only Hall et al. (2014) 89.2 Berkeley 90.1 Carreras et al. (2008) 91.1 Shindo et al. (2012) single 91.1 Single model, PTB + vectors/clusters Zhu et al. (2013) 91.3 This work* 91.1 Extended conditions Charniak and Johnson (2005) 91.5 Socher et al. (2013) 90.4 Vinyals et al. (2014) single 90.5 Vinyals et al. (2014) ensemble 91.6 Shindo et al. (2012) ensemble 92.4 Table 4: Test results on section 23 of the Penn Treebank. We compare to several categories of parsers from the literatures. We outperform strong baselines such as the Berkeley Parser (Petrov and Klein, 2007) and the CVG Stanford parser (Socher et al., 2013) and we match the performance of sophisticated generative (Shindo et al., 2012) and discriminative (Carreras et al., 2008) parsers. four </context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and Accurate ShiftReduce Constituent Parsing. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>