<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008956">
<title confidence="0.72357">
Passage Selection to Improve Question Answering
</title>
<bodyText confidence="0.704330133333333">
Fernando LLopis
Departamento de Lenguajes y
Sistemas Informáticos
Alicante (Spain) 03800
llopis@dlsi.ua.es
José Luis Vicedo
Departamento de Lenguajes y
Sistemas Informáticos
Alicante (Spain) 03800
vicedo@dlsi.ua.es
Antonio Ferrández
Departamento de Lenguajes y
Sistemas Informáticos
Alicante (Spain) 03800
antonio@dlsi.ua.es
</bodyText>
<sectionHeader confidence="0.891992" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.980313625">
Open-Domain Question Answering systems
(QA) performs the task of detecting text
fragments in a collection of documents that
contain the response to user’s queries.
These systems use high complexity tools that
reduce its applicability to the treatment of
small amounts of text. Consequently, when
working on large document collections, QA
systems apply Information Retrieval (IR)
techniques to reduce drastically text
collections to a tractable quantity of
relevant text. In this paper, we propose a
novel Passage Retrieval (PR) model that
performs this task with better performance
for QA purposes than current best IR
systems
</bodyText>
<sectionHeader confidence="0.99514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99916883018868">
Information Retrieval (IR) systems receive as
input a user’s query, and they have to return a
set of documents sorted by their relevance to the
query. There are different techniques to carry
out the document extraction process, but most of
them are based on pattern matching modules that
depend on the number of times that a query term
appear in each document, as well as the
importance or discrimination value of each term
in the document collection. Question Answering
(QA) systems try to improve the output
generated by IR systems by means of returning
just small pieces of text that are supposed to
contain the response. Usually, QA systems
combine IR and Natural Language Processing
(NLP) techniques to perform their task. This
combination allows text understanding until a
minimum level that permits a precise answer
detection and extraction. Nevertheless, since
NLP techniques are computationally expensive,
QA systems need to reduce the amount of text
where these techniques have to be applied. In
this way, they usually work on the output of IR
systems [10] that select the most relevant
documents to the query by supposing that they
will contain the answer required. Most applied
IR systems are mainly based on three models:
the cosine model [15], the pivoted cosine model1
[17], and the probabilistic model (OKAPI [18]).
Moreover, IR systems usually employ query
expansion techniques that frequently improve
their precision. These techniques can be based
on thesaurus [21] or on the incorporation of the
most frequent terms in the top M relevant
documents [7].
Currently, several Passage Retrieval (PR)
systems have also been proposed for this task
[2][5][8][9]. PR systems deal with fragments of
text in order to determine the relevance of a
document to a query, as well as to detect
document extracts that are likely to contain the
expected answer (instead of full documents).
Although PR systems apply IR-based techniques
to perform their work, they have revealed to be
more effective than IR systems for QA tasks.
In this paper, we are analysing the importance of
the IR-n PR system for QA n [11] as it was used
in last TREC-10 Conference [19]. The following
section briefly presents the backgrounds in IR,
PR and QA. Section 3 shows the architecture of
IR-n. Section 4 presents the evaluation
accomplished and finally, section 5 details
conclusions and work in progress.
</bodyText>
<footnote confidence="0.854655">
1 It is a modification of the cosine model. It tries to
reduce the problem of the preference for bigger
documents.
2 Backgrounds in Question Answering and
</footnote>
<subsectionHeader confidence="0.8866755">
Passage Retrieval
2.1 Information Retrieval and Passage Retrieval
</subsectionHeader>
<bodyText confidence="0.999990227272728">
Given a question, an IR system sorts the
documents by its relevance to the query. It
computes the similarity between each document
and the question by taking into account the
frequency of each query term in the document.
This fact usually produces that bigger
documents are preferred. A possible alternative
to IR models is based on obtaining the similarity
in accordance with the relevance of the passages
contained in the document. This new approach,
called Passage Retrieval (PR), has several
advantages. When used for document retrieval,
as the relevance of a document will depend on
the relevance of the passages it contains, this
measure will not be affected by the length of the
full document. Moreover, these techniques allow
to detect high relevant information embedded in
a long document obtaining, this way, better
performance than IR approaches [2][9]. On the
other hand, when applied for QA tasks, PR
systems allow reducing the amount of text to be
processed with costly NLP tools by returning
passages instead of whole documents.
Two classifications can be accomplished in PR.
The first one is in accordance with the way of
dividing the documents into passages. The
second one is in accordance with the moment in
which the passage segmentation is carried out.
With reference to the first one, PR community
generally agrees with the classification proposed
in [2], where the author distinguishes between
discourse models, semantic models, and window
models. The first one uses the structural
properties of the documents, such as sentences
or paragraphs [13][16] in order to define the
passages. The second one divides each
document into semantic pieces according to the
different topics in the document [5]. The last one
uses windows of a fixed size (usually a number
of terms) to determine passage boundaries [2]
[8].
At first glance, we could think that discourse-
based models would be the most effective, in
retrieval terms, since they use the structure of
the document itself. However, this model
greatest problem relies on detecting passage
boundaries since it depends on the writing style
of the author of each document. On the other
hand, window models have as main advantage
that they are simpler to accomplish, since the
passages have a previously known size, whereas
the remaining models have to bear in mind the
variable size of each passage. Nevertheless,
discourse-based and semantic models have the
main advantage that they return full information
units of the document, which is quite important
if these units are used as input by other
applications such as QA.
According to the second classification, we can
distinguish between approaches that segment
documents into passages for indexing purposes,
and those that perform segmentation after the
query is posed. The first one allows a quicker
calculation; nevertheless, the second one allows
different segmentation models in accordance
with the kind of query.
The passage extraction model that we propose
(IR-n) allows us to benefit from the advantages
of discourse-based models since self-contained
information units of text, such as sentences, are
used for building passages. Moreover, another
novel proposal in our PR system is the relevance
measure which, unlike other discourse-based
models, is not based on the number of passage
terms, but on a fixed number of passage
sentences. This fact allows a simpler calculation
of this measure unlike other discourse-based or
semantic models. Although each passage is
made up by a fixed number of sentences, we
consider that our proposal differs from the
window models since our passages do not have a
fixed size (i.e. a fixed number of words) since
we use sentences with a variable size.
Furthermore, IR-n document segmentation into
passages is accomplished after the query is
posed, which allows us to determine the number
of sentences to be considered in accordance with
the kind of the query.
</bodyText>
<subsectionHeader confidence="0.99987">
2.2 Question Answering
</subsectionHeader>
<bodyText confidence="0.999795771428571">
Open domain QA systems are defined as tools
capable of extracting the answer to user queries
directly from unrestricted domain documents. Or
at least, systems that can extract text snippets
from texts, from whose content it are possible to
infer the answer to a specific question. In both
cases, these systems try to reduce the amount of
time users spend to locate a concrete
information.
Interest in QA systems is quite recent. We had
little information about this kind of systems until
the “First Question Answering Track” was held
in TREC-8 Conference. This track tries to
benefit from large-scale evaluation that was
previously carried out on IR systems, in
previous TREC conferences.
If a QA system wants to successfully obtain a
user’s request, it needs to understand both texts
and questions to a minimum level. From a
linguistic perspective, “understanding” means to
carry out many of the typical steps on natural
language analysis: lexical, syntactic and
semantic. This analysis takes much more time
than the statistical analysis that is usually carried
out in IR. Besides, as QA systems have to
manage with as much text as done for IR tasks,
and the user needs the answer in a limited
interval of time, it is nearly mandatory that first,
an IR system processes the query and second,
the QA process continues with its output. In this
way, the time of analysis is highly decreased.
The analysis of current best systems [3] [4] [14]
[6] allows identifying main QA sub-components
where document retrieval is accomplished by
using IR technology:
</bodyText>
<listItem confidence="0.9999795">
• Question Analysis.
• Document Retrieval.
• Passage Selection.
• Answer Extraction.
</listItem>
<sectionHeader confidence="0.980599" genericHeader="method">
3 IR-n overview
</sectionHeader>
<bodyText confidence="0.999916">
In this section, we describe the architecture of
the proposed PR system, namely IR-n, focusing
on its three main modules: indexing, passage
retrieval and query expansion.
</bodyText>
<subsectionHeader confidence="0.999692">
3.1 Indexing module
</subsectionHeader>
<bodyText confidence="0.9976662">
The main aim of this module is to generate the
dictionaries that contain all the required
information for the passage retrieval module. It
requires the following information for each
term:
</bodyText>
<listItem confidence="0.994338">
• The number of documents that contain
this term.
• For each document:
</listItem>
<bodyText confidence="0.989986615384616">
− The number of times this term
appears in the document.
− The position of each term in the
document represented as the number
of sentence it appears in.
As term, we consider the stem produced by the
Porter stemmer on those words that do not
appear in a list of stop-words, list that is similar
to those generally used for IR. On the other
hand, query terms are also extracted in the same
way, that is to say, we only consider the stems of
query words that do not appear in the stop-words
list.
</bodyText>
<subsectionHeader confidence="0.99985">
3.2 Passage retrieval module
</subsectionHeader>
<bodyText confidence="0.999606333333333">
This module extracts the passages according to
its similarity with the user’s query. The scheme
in this process is the following:
</bodyText>
<listItem confidence="0.964752">
1. Query terms are sorted according to the
number of documents they appear in. Terms that
appear in fewer documents are processed firstly.
2. The documents that contain any query term
are selected.
3. The following similarity measure is calculated
for each passage p (contained in the selected
documents) with the query q:
</listItem>
<equation confidence="0.93831825">
Similarity_measure(p, q) = ∑t∈p ∧ q Wp,t ·Wq,t
Wp,t = loge( fp,t + 1).
Wq, t= loge( fq,t + 1) · idf
idf = loge( N / ft + 1)
</equation>
<bodyText confidence="0.999688666666667">
Where fp,t is the number of times that the term t
appears in the passage p. fq,t represents the
number of times that the term t appears in the
query q. N is the number of documents in the
collection and ft is refers to the number of
documents that contain the term t.
</bodyText>
<listItem confidence="0.944592714285714">
4. Only the most relevant passage of each
document is selected for retrieval.
5. The selected passages are sorted by their
similarity measure.
6. Passages are associated with the document
they pertain and they are presented in a ranked
list form.
</listItem>
<bodyText confidence="0.999982533333333">
As we can notice, the similarity measure is
similar to the cosine measure presented in [15].
The only difference is that the size of each
passage (the number of terms) is not used to
normalise the results. This proposal performs
normalization according to the fixed number of
sentences per passage. This difference makes the
calculation simpler than other discourse-based
PR or IR systems. Another important detail to
remark is that we are using N as the number of
documents in the collection, instead of the
number of passages according to the
considerations presented in [9].
As it has been commented, our PR system uses
variable-sized passages that are based on a fixed
number of sentences (with different number of
terms per passage). The passages overlap each
other, that is to say, if a passage contains N
sentences, the first passage will be formed by the
sentences from 1 to N, the second one from 2 to
N+1, and so on. We decided to overlap just one
sentence according to the experiments and
results presented in [12]. This work studied the
optimum number of overlapping sentences in
each passage for retrieval purposes concluding,
that best results were obtained when only one
overlapping sentence was used. Regarding to the
optimum number (N) of sentences per passage
considered in this paper, it will be
experimentally obtained.
</bodyText>
<sectionHeader confidence="0.994156" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999976">
This section presents the experiments developed
for training and evaluating our approach. The
experiments have been run on the TREC-9 QA
Track question set and document collections.
</bodyText>
<subsectionHeader confidence="0.999471">
4.1 Data collection
</subsectionHeader>
<bodyText confidence="0.99117575">
TREC-9 question test set is made up by 682
questions with answers included in the
document collection. The document set consists
of 978,952 documents from the TIPSTER and
TREC following collections: AP Newswire,
Wall Street Journal, San Jose Mercury News,
Financial Times, Los Angeles Times, Foreign
Broadcast Information Service.
</bodyText>
<subsectionHeader confidence="0.997593">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.9999583">
Training experiments had two objectives. They
were designed (1) to calculate the optimum
number of sentences (N) that define passage
length and (2) to test two different possible ways
of applying our method.
First training experiment consists of working on
the output of one of the current best performing
IR systems (the ATT system). This experiment
re-sorts its output (the first 1,000 ranked
documents) by using IR-n. Second experiment
consists of using our proposal as the main IR
system, that is, indexing the whole collections
by means of IR-n. For each experiment, a
different number of sentences per passage were
tested: 5, 10, 15 and 20 sentences. The relevance
of each returned document was measured by
means of the tool provided by TREC
organization that allows us to determine if a
passage contains the right answer. The two
experiments are summed up in Figure 1.
</bodyText>
<figureCaption confidence="0.997156">
Figure 1. Training Experiments
</figureCaption>
<bodyText confidence="0.999982555555555">
These experiments were performed using only
the first 100 questions included in the data
collection. Table 1 shows training results for
passages of 5, 10, 15 and 20 sentences using
both approaches. This results measure the
number of questions whose correct answer was
included into the top n retrieved passages (or
documents) for the training question set. The
first experiment (IR-n Ref) uses IR-n on the
1,000 documents returned by ATT system while
the second one (IR-n) applies passage retrieval
overall collections.
As we can see, IR-n Ref and IR-n test obtain
similar results although using our approach to
re-rank the output of a good IR system presents
a slight better performance than applying IR-n
overall document collection. Regarding to the
number of sentences to be taken into account to
</bodyText>
<figure confidence="0.987322333333333">
Documents
ATTIR-system
Questions
Documents
IR-n system
Answers
200 morerelevant
passages
QA system
1000 more
relevant
documents
IR-n system
200 more relevant
passages
</figure>
<bodyText confidence="0.998458888888889">
define passage length, we can observe that best
results are obtained with passages of 20
sentences. In this case, both tests improve
significantly the performance of ATT-system. It
ranges from 12 (IR-n Ref) and 10 (IR-n) points
on a passage length of 20 sentences (for only the
first 5 documents retrieved) to 8 and 7 points
when the first 200 documents are taken into
account respectively.
</bodyText>
<table confidence="0.9992596">
Answer At At At At At At At
included 5 10 20 30 50 100 200
docs docs docs docs docs docs docs
IR-n Ref.
5 Sent 57 66 78 83 85 88 93
10 Sent 63 76 80 89 93 96 97
15 Sent 70 78 83 89 94 95 96
20 Sent 74 83 87 91 93 96 97
IR-n
5 Sent 55 63 75 80 84 89 90
10 Sent 60 73 78 87 92 95 97
15 Sent 70 76 82 87 93 95 95
20 Sent 72 80 86 90 92 96 96
ATT system
62 69 77 82 83 87 89
</table>
<tableCaption confidence="0.9904265">
Table 1. Number of questions rightly answered
(training set of 100 questions).
</tableCaption>
<subsectionHeader confidence="0.997402">
4.3 Experiment
</subsectionHeader>
<bodyText confidence="0.999384588235294">
In order to evaluate our proposal we decided to
compare the quality of the information retrieved
by our approaches with the ranked list retrieved
by the ATT IR system. For this evaluation, the
682 questions included in the data collection
were processed and the number N of sentences
per passage was set to 20. Table 2 shows the
results of this evaluation experiment. This table
shows the percentage of questions whose answer
can be found into the first n documents returned
by the ATT IR system and the best n passages
returned by IR-n Ref and IR-n respectively.
These results are also presented in Figure 2
These data confirm training results. In this case,
both approaches perform better than ATT
system and improvements range form 6 to 12
points for 20 sentences passage length.
</bodyText>
<table confidence="0.9996728">
Answer ATT IR-n Ref IR-n
system
IncludedAt 64.90% 74.59% 72.21%
5 docs
At 10 docs 70.33% 82.73% 80.37%
At 20 docs 75.91% 87.37% 86.35%
At 30 docs 79.14% 89.96% 89.31%
At 50 docs 83.70% 91.62% 91.52%
At 100 docs 87.37% 94.56% 95.55%
At 200 docs 90.01% 96.03% 95.92%
</table>
<tableCaption confidence="0.999437">
Table 2. ATT-system versus IR-n systems.
</tableCaption>
<figureCaption confidence="0.992729">
Figure 2. Comparative of ATT-system and
experiments with IR-n (Passages of 20 sentences)
</figureCaption>
<sectionHeader confidence="0.955284" genericHeader="conclusions">
5 Conclusions and future works
</sectionHeader>
<bodyText confidence="0.999815764705883">
In this paper, we have analysed the improvement
obtained by our passage retrieval system, called
IR-n, with reference to a high-performance IR
system (ATT) regarding to is application for QA
tasks. This improvement has been evaluated on
the TREC-9 QA track data set. The achieved
improvements are twofold: First, our approach
obtains a better precision by retrieving more
passages that contain the answer to users’
queries than ATT system does. Second, since
our approach returns passages (instead of
documents), it significantly reduces the amount
of text to be processed with costly techniques by
the QA system. The related experiments show
that the optimal passage length for this task is 20
when passages are made up by a fixed number
of sentences. Moreover, we have tested two
</bodyText>
<figure confidence="0.992932692307692">
%Questions
100
95
90
85
80
75
70
65
60
IR-n Ref IR-n ATT system
5 10 20 30 50 100 200
Number of documents
</figure>
<bodyText confidence="0.999592944444444">
different ways of applying our model. As we
have seen, IR-n presents similar results when it
works on the output of an IR system, than when
it works on the whole collections. Nevertheless,
in both cases, benefits range from 6 to 12 points
with reference to ATT system depending on the
number of first documents or passages retrieved
to be processed for QA tasks.
As future work, in order to improve our system
precision, we intend to obtain the optimum size
of passages in accordance with the kind of
question. Besides, we need to investigate the
effects of query expansion techniques on IR-n
system. Furthermore, we are also trying to
improve the relationship between IR-n and the
following QA system, in order to detect the
minimum number of passages to extract for each
query without affecting QA performance.
</bodyText>
<sectionHeader confidence="0.99015" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999116678571429">
[1] Bertoldi, N and Federico, M. ITC-irst at CLEF-2001 ,
Working Notes for the Clef 2001 Darmstdt, Germany ,
pp 41-44
[2] Callan, J. Passage-Level Evidence in Document
Retrieval. In Proceedings of the 17 th Annual ACM
SIGIR Conference on Research and Development in
Information Retrieval, Dublin, Ireland 1994, pp. 302-
310.
[3] Clarke, C.; Cormack, g, Kisman, D and Lynam, T.
Question Answering by Passage Selection(Multitext
Experiments for TREC-9) Proceedings of the Tenth
Text REtrieval Conference, TREC-9. Gaithersburg ,
USA 2000, pp 673-683
[4] Harabagiu, S.; Moldovan, D.; Pasca, M.; Mihalcea, R.;
Surdeanu, M.; Bunescu, R.; Gîrju, R.; Rus, V. and
Morarescu, P. FALCON: Boosting Knowledge for
Answer Engines. In Nineth Text REtrieval Conference,
Gaithersburg USA 2000.pp 479-
[5] Hearst, M. and Plaunt, C. Subtopic structuring for full-
length document access. Proceedings of the Sixteenth
Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
Pittsburgh, PA USA 1993, pp 59-68
[6] Ittycheriah, A.; Franz, M.; Zu, W. and Ratnaparkhi, A.
IBM&apos;s Statistical Question Answering System. In
Nineth Text REtrieval Conference, Gaithersburg USA
2000., pp 231-236
[7] J. Xu and W. Croft. Query expansion using local and
global document analysis. In Proceedings of the 19th
Annual International ACM SIGIR, Zurich,
Switzerland, 1996 pp 4—11, 18—22.
[8] Kaskiel, M. and Zobel, J. Passage Retrieval Revisited
SIGIR &apos;97: Proceedings of the 20th Annual
International ACM Philadelphia, PA 1997, USA, pp
27-31
[9] KaszKiel, M. and Zobel, J. Effective Ranking with
Arbitrary Passages. Journal of the American Society
for Information Science, Vol 52, No. 4, February
2001, pp 344-364.
[10] Litkowski, k, Syntactic Clues and Lexical Resources
in Question-Answering In Nineth Text REtrieval
Conference, Gaithersburg USA 2000 pp177-188
[11] Llopis, F. and Vicedo, J. Ir-n system, a passage
retrieval system at CLEF 2001 Working Notes for
the Clef 2001 Darmstdt, Germany 2001, pp 115-120 .
To appear in Lecture Notes in Computer Science
[12] Llopis, F.; Ferrández, and Vicedo, J. Text
Segmentation for efficient Information Retrieval Third
International Conference on
Intelligent Text Processing and
Computational Linguistics. Mexico 2002 To appear in
Lecture Notes in Computer Science
[13] Namba, I Fujitsu Laboratories TREC9 Report.
Proceedings of the Nineth Text REtrieval Conference,
TREC-9. Gaithersburg,USA.2000, pp 203-208
[14] Prager, J.; Brown, E.; Radev, D. and Czuba, K. One
Search Engine or Two for QuestionAnswering. In
Nineth Text REtrieval Conference,
Gaithersburg,USA. 2000.
[15] Salton G. Automatic Text Processing: The
Transformation, Analysis, and Retrieval of
Information by Computer, Addison Wesley
Publishing, New York. 1989
[16] Salton, G.; Allan, J. Buckley Approaches to passage
retrieval in full text information systems. In R
Korfhage, E Rasmussen &amp; P Willet (Eds.)
Prodeedings of the 16 th annual international ACM-
SIGIR conference on research and development in
information retrieval. Pittsburgh PA USA , pp 49-58
[17] Singhal, A.; Buckley, C. and Mitra, M. Pivoted
document length normalization. Proceedings of the
19th annual international ACM- 1996.
[18] Venner, G. and Walker, S. Okapi &apos;84: `Best match&apos;
system. Microcomputer networking in libraries II.
Vine, 48,1983, pp 22-26.
[19] Vicedo, J.; Ferrandez, A and Llopis, F. University of
Alicante al TREC-10. In Tenth Text REtrieval
Conference, Gaithersburg,USA. 2001
[20] Vicedo, J.; Ferrandez, A; A semantic approach to
Question Answering systems. In Nineth Text REtrieval
Conference, 2000 pp 440-444.
[21] Y. Jing and W. B. Croft. An association thesaurus for
information retrieval. In RIAO 94 Conference
Proceedings, , New York, 1994. pp 146--160
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.149384">
<title confidence="0.999925">Passage Selection to Improve Question Answering</title>
<author confidence="0.973361">Fernando</author>
<affiliation confidence="0.764721333333333">Departamento de Lenguajes Sistemas Alicante (Spain)</affiliation>
<email confidence="0.950036">llopis@dlsi.ua.es</email>
<author confidence="0.987819">José Luis Vicedo</author>
<affiliation confidence="0.887186">Departamento de Lenguajes y Sistemas Informáticos</affiliation>
<address confidence="0.95138">Alicante (Spain) 03800</address>
<email confidence="0.804628">vicedo@dlsi.ua.es</email>
<author confidence="0.9966">Antonio Ferrández</author>
<affiliation confidence="0.8857055">Departamento de Lenguajes y Sistemas Informáticos</affiliation>
<address confidence="0.945795">Alicante (Spain) 03800</address>
<email confidence="0.941501">antonio@dlsi.ua.es</email>
<abstract confidence="0.9993575">Open-Domain Question Answering systems (QA) performs the task of detecting text fragments in a collection of documents that contain the response to user’s queries. These systems use high complexity tools that reduce its applicability to the treatment of small amounts of text. Consequently, when working on large document collections, QA systems apply Information Retrieval (IR) techniques to reduce drastically text collections to a tractable quantity of relevant text. In this paper, we propose a novel Passage Retrieval (PR) model that performs this task with better performance for QA purposes than current best IR</abstract>
<intro confidence="0.678969">systems</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
<author>M Federico</author>
</authors>
<title>ITC-irst at CLEF-2001 , Working Notes for the Clef</title>
<date>2001</date>
<pages>41--44</pages>
<location>Darmstdt, Germany ,</location>
<marker>[1]</marker>
<rawString>Bertoldi, N and Federico, M. ITC-irst at CLEF-2001 , Working Notes for the Clef 2001 Darmstdt, Germany , pp 41-44</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Callan</author>
</authors>
<title>Passage-Level Evidence in Document Retrieval.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17 th Annual ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>302--310</pages>
<location>Dublin, Ireland</location>
<contexts>
<context position="2687" citStr="[2]" startWordPosition="407" endWordPosition="407">s [10] that select the most relevant documents to the query by supposing that they will contain the answer required. Most applied IR systems are mainly based on three models: the cosine model [15], the pivoted cosine model1 [17], and the probabilistic model (OKAPI [18]). Moreover, IR systems usually employ query expansion techniques that frequently improve their precision. These techniques can be based on thesaurus [21] or on the incorporation of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents). Although PR systems apply IR-based techniques to perform their work, they have revealed to be more effective than IR systems for QA tasks. In this paper, we are analysing the importance of the IR-n PR system for QA n [11] as it was used in last TREC-10 Conference [19]. The following section briefly presents the backgrounds in IR, PR and QA. Section 3 shows the architectu</context>
<context position="4492" citStr="[2]" startWordPosition="702" endWordPosition="702">er documents are preferred. A possible alternative to IR models is based on obtaining the similarity in accordance with the relevance of the passages contained in the document. This new approach, called Passage Retrieval (PR), has several advantages. When used for document retrieval, as the relevance of a document will depend on the relevance of the passages it contains, this measure will not be affected by the length of the full document. Moreover, these techniques allow to detect high relevant information embedded in a long document obtaining, this way, better performance than IR approaches [2][9]. On the other hand, when applied for QA tasks, PR systems allow reducing the amount of text to be processed with costly NLP tools by returning passages instead of whole documents. Two classifications can be accomplished in PR. The first one is in accordance with the way of dividing the documents into passages. The second one is in accordance with the moment in which the passage segmentation is carried out. With reference to the first one, PR community generally agrees with the classification proposed in [2], where the author distinguishes between discourse models, semantic models, and wind</context>
</contexts>
<marker>[2]</marker>
<rawString>Callan, J. Passage-Level Evidence in Document Retrieval. In Proceedings of the 17 th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, Dublin, Ireland 1994, pp. 302-310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Clarke</author>
<author>g Cormack</author>
<author>D Kisman</author>
<author>T Lynam</author>
</authors>
<title>Question Answering by Passage Selection(Multitext Experiments for</title>
<date>2000</date>
<booktitle>TREC-9) Proceedings of the Tenth Text REtrieval Conference, TREC-9.</booktitle>
<pages>673--683</pages>
<location>Gaithersburg , USA</location>
<contexts>
<context position="9006" citStr="[3]" startWordPosition="1436" endWordPosition="1436">inguistic perspective, “understanding” means to carry out many of the typical steps on natural language analysis: lexical, syntactic and semantic. This analysis takes much more time than the statistical analysis that is usually carried out in IR. Besides, as QA systems have to manage with as much text as done for IR tasks, and the user needs the answer in a limited interval of time, it is nearly mandatory that first, an IR system processes the query and second, the QA process continues with its output. In this way, the time of analysis is highly decreased. The analysis of current best systems [3] [4] [14] [6] allows identifying main QA sub-components where document retrieval is accomplished by using IR technology: • Question Analysis. • Document Retrieval. • Passage Selection. • Answer Extraction. 3 IR-n overview In this section, we describe the architecture of the proposed PR system, namely IR-n, focusing on its three main modules: indexing, passage retrieval and query expansion. 3.1 Indexing module The main aim of this module is to generate the dictionaries that contain all the required information for the passage retrieval module. It requires the following information for each term</context>
</contexts>
<marker>[3]</marker>
<rawString>Clarke, C.; Cormack, g, Kisman, D and Lynam, T. Question Answering by Passage Selection(Multitext Experiments for TREC-9) Proceedings of the Tenth Text REtrieval Conference, TREC-9. Gaithersburg , USA 2000, pp 673-683</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>M Surdeanu</author>
<author>R Bunescu</author>
<author>R Gîrju</author>
<author>V Rus</author>
<author>P FALCON Morarescu</author>
</authors>
<title>Boosting Knowledge for Answer Engines.</title>
<booktitle>In Nineth Text REtrieval Conference, Gaithersburg USA 2000.pp 479-</booktitle>
<contexts>
<context position="9010" citStr="[4]" startWordPosition="1437" endWordPosition="1437">istic perspective, “understanding” means to carry out many of the typical steps on natural language analysis: lexical, syntactic and semantic. This analysis takes much more time than the statistical analysis that is usually carried out in IR. Besides, as QA systems have to manage with as much text as done for IR tasks, and the user needs the answer in a limited interval of time, it is nearly mandatory that first, an IR system processes the query and second, the QA process continues with its output. In this way, the time of analysis is highly decreased. The analysis of current best systems [3] [4] [14] [6] allows identifying main QA sub-components where document retrieval is accomplished by using IR technology: • Question Analysis. • Document Retrieval. • Passage Selection. • Answer Extraction. 3 IR-n overview In this section, we describe the architecture of the proposed PR system, namely IR-n, focusing on its three main modules: indexing, passage retrieval and query expansion. 3.1 Indexing module The main aim of this module is to generate the dictionaries that contain all the required information for the passage retrieval module. It requires the following information for each term: • </context>
</contexts>
<marker>[4]</marker>
<rawString>Harabagiu, S.; Moldovan, D.; Pasca, M.; Mihalcea, R.; Surdeanu, M.; Bunescu, R.; Gîrju, R.; Rus, V. and Morarescu, P. FALCON: Boosting Knowledge for Answer Engines. In Nineth Text REtrieval Conference, Gaithersburg USA 2000.pp 479-</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
<author>C Plaunt</author>
</authors>
<title>Subtopic structuring for fulllength document access.</title>
<date>1993</date>
<booktitle>Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>59--68</pages>
<location>Pittsburgh, PA USA</location>
<contexts>
<context position="2690" citStr="[5]" startWordPosition="407" endWordPosition="407">10] that select the most relevant documents to the query by supposing that they will contain the answer required. Most applied IR systems are mainly based on three models: the cosine model [15], the pivoted cosine model1 [17], and the probabilistic model (OKAPI [18]). Moreover, IR systems usually employ query expansion techniques that frequently improve their precision. These techniques can be based on thesaurus [21] or on the incorporation of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents). Although PR systems apply IR-based techniques to perform their work, they have revealed to be more effective than IR systems for QA tasks. In this paper, we are analysing the importance of the IR-n PR system for QA n [11] as it was used in last TREC-10 Conference [19]. The following section briefly presents the backgrounds in IR, PR and QA. Section 3 shows the architecture </context>
<context position="5351" citStr="[5]" startWordPosition="840" endWordPosition="840">dance with the way of dividing the documents into passages. The second one is in accordance with the moment in which the passage segmentation is carried out. With reference to the first one, PR community generally agrees with the classification proposed in [2], where the author distinguishes between discourse models, semantic models, and window models. The first one uses the structural properties of the documents, such as sentences or paragraphs [13][16] in order to define the passages. The second one divides each document into semantic pieces according to the different topics in the document [5]. The last one uses windows of a fixed size (usually a number of terms) to determine passage boundaries [2] [8]. At first glance, we could think that discoursebased models would be the most effective, in retrieval terms, since they use the structure of the document itself. However, this model greatest problem relies on detecting passage boundaries since it depends on the writing style of the author of each document. On the other hand, window models have as main advantage that they are simpler to accomplish, since the passages have a previously known size, whereas the remaining models have to b</context>
</contexts>
<marker>[5]</marker>
<rawString>Hearst, M. and Plaunt, C. Subtopic structuring for fulllength document access. Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Pittsburgh, PA USA 1993, pp 59-68</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>M Franz</author>
<author>W Zu</author>
<author>A Ratnaparkhi</author>
</authors>
<title>IBM&apos;s Statistical Question Answering System.</title>
<date>2000</date>
<booktitle>In Nineth Text REtrieval Conference,</booktitle>
<pages>231--236</pages>
<location>Gaithersburg USA</location>
<contexts>
<context position="9019" citStr="[6]" startWordPosition="1439" endWordPosition="1439">spective, “understanding” means to carry out many of the typical steps on natural language analysis: lexical, syntactic and semantic. This analysis takes much more time than the statistical analysis that is usually carried out in IR. Besides, as QA systems have to manage with as much text as done for IR tasks, and the user needs the answer in a limited interval of time, it is nearly mandatory that first, an IR system processes the query and second, the QA process continues with its output. In this way, the time of analysis is highly decreased. The analysis of current best systems [3] [4] [14] [6] allows identifying main QA sub-components where document retrieval is accomplished by using IR technology: • Question Analysis. • Document Retrieval. • Passage Selection. • Answer Extraction. 3 IR-n overview In this section, we describe the architecture of the proposed PR system, namely IR-n, focusing on its three main modules: indexing, passage retrieval and query expansion. 3.1 Indexing module The main aim of this module is to generate the dictionaries that contain all the required information for the passage retrieval module. It requires the following information for each term: • The numbe</context>
</contexts>
<marker>[6]</marker>
<rawString>Ittycheriah, A.; Franz, M.; Zu, W. and Ratnaparkhi, A. IBM&apos;s Statistical Question Answering System. In Nineth Text REtrieval Conference, Gaithersburg USA 2000., pp 231-236</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>W Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of the 19th Annual International ACM SIGIR,</booktitle>
<pages>4--11</pages>
<location>Zurich, Switzerland,</location>
<contexts>
<context position="2594" citStr="[7]" startWordPosition="393" endWordPosition="393">hese techniques have to be applied. In this way, they usually work on the output of IR systems [10] that select the most relevant documents to the query by supposing that they will contain the answer required. Most applied IR systems are mainly based on three models: the cosine model [15], the pivoted cosine model1 [17], and the probabilistic model (OKAPI [18]). Moreover, IR systems usually employ query expansion techniques that frequently improve their precision. These techniques can be based on thesaurus [21] or on the incorporation of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents). Although PR systems apply IR-based techniques to perform their work, they have revealed to be more effective than IR systems for QA tasks. In this paper, we are analysing the importance of the IR-n PR system for QA n [11] as it was used in last TREC-10 Conference [19]. The follow</context>
</contexts>
<marker>[7]</marker>
<rawString>J. Xu and W. Croft. Query expansion using local and global document analysis. In Proceedings of the 19th Annual International ACM SIGIR, Zurich, Switzerland, 1996 pp 4—11, 18—22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kaskiel</author>
<author>J Zobel</author>
</authors>
<title>Passage Retrieval Revisited</title>
<date>1997</date>
<booktitle>SIGIR &apos;97: Proceedings of the 20th Annual International ACM</booktitle>
<pages>27--31</pages>
<location>Philadelphia, PA</location>
<contexts>
<context position="2693" citStr="[8]" startWordPosition="407" endWordPosition="407"> that select the most relevant documents to the query by supposing that they will contain the answer required. Most applied IR systems are mainly based on three models: the cosine model [15], the pivoted cosine model1 [17], and the probabilistic model (OKAPI [18]). Moreover, IR systems usually employ query expansion techniques that frequently improve their precision. These techniques can be based on thesaurus [21] or on the incorporation of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents). Although PR systems apply IR-based techniques to perform their work, they have revealed to be more effective than IR systems for QA tasks. In this paper, we are analysing the importance of the IR-n PR system for QA n [11] as it was used in last TREC-10 Conference [19]. The following section briefly presents the backgrounds in IR, PR and QA. Section 3 shows the architecture of </context>
<context position="5462" citStr="[8]" startWordPosition="860" endWordPosition="860">which the passage segmentation is carried out. With reference to the first one, PR community generally agrees with the classification proposed in [2], where the author distinguishes between discourse models, semantic models, and window models. The first one uses the structural properties of the documents, such as sentences or paragraphs [13][16] in order to define the passages. The second one divides each document into semantic pieces according to the different topics in the document [5]. The last one uses windows of a fixed size (usually a number of terms) to determine passage boundaries [2] [8]. At first glance, we could think that discoursebased models would be the most effective, in retrieval terms, since they use the structure of the document itself. However, this model greatest problem relies on detecting passage boundaries since it depends on the writing style of the author of each document. On the other hand, window models have as main advantage that they are simpler to accomplish, since the passages have a previously known size, whereas the remaining models have to bear in mind the variable size of each passage. Nevertheless, discourse-based and semantic models have the main </context>
</contexts>
<marker>[8]</marker>
<rawString>Kaskiel, M. and Zobel, J. Passage Retrieval Revisited SIGIR &apos;97: Proceedings of the 20th Annual International ACM Philadelphia, PA 1997, USA, pp 27-31</rawString>
</citation>
<citation valid="true">
<authors>
<author>M KaszKiel</author>
<author>J Zobel</author>
</authors>
<title>Effective Ranking with Arbitrary Passages.</title>
<date>2001</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>52</volume>
<pages>344--364</pages>
<contexts>
<context position="2696" citStr="[9]" startWordPosition="407" endWordPosition="407">at select the most relevant documents to the query by supposing that they will contain the answer required. Most applied IR systems are mainly based on three models: the cosine model [15], the pivoted cosine model1 [17], and the probabilistic model (OKAPI [18]). Moreover, IR systems usually employ query expansion techniques that frequently improve their precision. These techniques can be based on thesaurus [21] or on the incorporation of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents). Although PR systems apply IR-based techniques to perform their work, they have revealed to be more effective than IR systems for QA tasks. In this paper, we are analysing the importance of the IR-n PR system for QA n [11] as it was used in last TREC-10 Conference [19]. The following section briefly presents the backgrounds in IR, PR and QA. Section 3 shows the architecture of IR-</context>
<context position="4495" citStr="[9]" startWordPosition="702" endWordPosition="702">documents are preferred. A possible alternative to IR models is based on obtaining the similarity in accordance with the relevance of the passages contained in the document. This new approach, called Passage Retrieval (PR), has several advantages. When used for document retrieval, as the relevance of a document will depend on the relevance of the passages it contains, this measure will not be affected by the length of the full document. Moreover, these techniques allow to detect high relevant information embedded in a long document obtaining, this way, better performance than IR approaches [2][9]. On the other hand, when applied for QA tasks, PR systems allow reducing the amount of text to be processed with costly NLP tools by returning passages instead of whole documents. Two classifications can be accomplished in PR. The first one is in accordance with the way of dividing the documents into passages. The second one is in accordance with the moment in which the passage segmentation is carried out. With reference to the first one, PR community generally agrees with the classification proposed in [2], where the author distinguishes between discourse models, semantic models, and window </context>
<context position="11871" citStr="[9]" startWordPosition="1933" endWordPosition="1933">ted in a ranked list form. As we can notice, the similarity measure is similar to the cosine measure presented in [15]. The only difference is that the size of each passage (the number of terms) is not used to normalise the results. This proposal performs normalization according to the fixed number of sentences per passage. This difference makes the calculation simpler than other discourse-based PR or IR systems. Another important detail to remark is that we are using N as the number of documents in the collection, instead of the number of passages according to the considerations presented in [9]. As it has been commented, our PR system uses variable-sized passages that are based on a fixed number of sentences (with different number of terms per passage). The passages overlap each other, that is to say, if a passage contains N sentences, the first passage will be formed by the sentences from 1 to N, the second one from 2 to N+1, and so on. We decided to overlap just one sentence according to the experiments and results presented in [12]. This work studied the optimum number of overlapping sentences in each passage for retrieval purposes concluding, that best results were obtained when</context>
</contexts>
<marker>[9]</marker>
<rawString>KaszKiel, M. and Zobel, J. Effective Ranking with Arbitrary Passages. Journal of the American Society for Information Science, Vol 52, No. 4, February 2001, pp 344-364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>k Litkowski</author>
</authors>
<title>Syntactic Clues and Lexical Resources in Question-Answering</title>
<date>2000</date>
<booktitle>In Nineth Text REtrieval Conference,</booktitle>
<pages>177--188</pages>
<location>Gaithersburg USA</location>
<contexts>
<context position="2090" citStr="[10]" startWordPosition="313" endWordPosition="313">nswering (QA) systems try to improve the output generated by IR systems by means of returning just small pieces of text that are supposed to contain the response. Usually, QA systems combine IR and Natural Language Processing (NLP) techniques to perform their task. This combination allows text understanding until a minimum level that permits a precise answer detection and extraction. Nevertheless, since NLP techniques are computationally expensive, QA systems need to reduce the amount of text where these techniques have to be applied. In this way, they usually work on the output of IR systems [10] that select the most relevant documents to the query by supposing that they will contain the answer required. Most applied IR systems are mainly based on three models: the cosine model [15], the pivoted cosine model1 [17], and the probabilistic model (OKAPI [18]). Moreover, IR systems usually employ query expansion techniques that frequently improve their precision. These techniques can be based on thesaurus [21] or on the incorporation of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5]</context>
</contexts>
<marker>[10]</marker>
<rawString>Litkowski, k, Syntactic Clues and Lexical Resources in Question-Answering In Nineth Text REtrieval Conference, Gaithersburg USA 2000 pp177-188</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Llopis</author>
<author>J Vicedo</author>
</authors>
<title>Ir-n system, a passage retrieval system at CLEF</title>
<date>2001</date>
<journal>Lecture Notes in Computer Science</journal>
<pages>115--120</pages>
<location>Darmstdt, Germany</location>
<note>To appear in</note>
<contexts>
<context position="3135" citStr="[11]" startWordPosition="485" endWordPosition="485"> of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents). Although PR systems apply IR-based techniques to perform their work, they have revealed to be more effective than IR systems for QA tasks. In this paper, we are analysing the importance of the IR-n PR system for QA n [11] as it was used in last TREC-10 Conference [19]. The following section briefly presents the backgrounds in IR, PR and QA. Section 3 shows the architecture of IR-n. Section 4 presents the evaluation accomplished and finally, section 5 details conclusions and work in progress. 1 It is a modification of the cosine model. It tries to reduce the problem of the preference for bigger documents. 2 Backgrounds in Question Answering and Passage Retrieval 2.1 Information Retrieval and Passage Retrieval Given a question, an IR system sorts the documents by its relevance to the query. It computes the simil</context>
</contexts>
<marker>[11]</marker>
<rawString>Llopis, F. and Vicedo, J. Ir-n system, a passage retrieval system at CLEF 2001 Working Notes for the Clef 2001 Darmstdt, Germany 2001, pp 115-120 . To appear in Lecture Notes in Computer Science</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Llopis</author>
<author>Ferrández</author>
<author>J Vicedo</author>
</authors>
<title>Text Segmentation for efficient Information Retrieval</title>
<date>2002</date>
<journal>Lecture Notes in Computer Science</journal>
<booktitle>Third International Conference on Intelligent Text Processing and Computational Linguistics. Mexico</booktitle>
<note>To appear in</note>
<contexts>
<context position="12320" citStr="[12]" startWordPosition="2014" endWordPosition="2014">ark is that we are using N as the number of documents in the collection, instead of the number of passages according to the considerations presented in [9]. As it has been commented, our PR system uses variable-sized passages that are based on a fixed number of sentences (with different number of terms per passage). The passages overlap each other, that is to say, if a passage contains N sentences, the first passage will be formed by the sentences from 1 to N, the second one from 2 to N+1, and so on. We decided to overlap just one sentence according to the experiments and results presented in [12]. This work studied the optimum number of overlapping sentences in each passage for retrieval purposes concluding, that best results were obtained when only one overlapping sentence was used. Regarding to the optimum number (N) of sentences per passage considered in this paper, it will be experimentally obtained. 4 Evaluation This section presents the experiments developed for training and evaluating our approach. The experiments have been run on the TREC-9 QA Track question set and document collections. 4.1 Data collection TREC-9 question test set is made up by 682 questions with answers incl</context>
</contexts>
<marker>[12]</marker>
<rawString>Llopis, F.; Ferrández, and Vicedo, J. Text Segmentation for efficient Information Retrieval Third International Conference on Intelligent Text Processing and Computational Linguistics. Mexico 2002 To appear in Lecture Notes in Computer Science</rawString>
</citation>
<citation valid="false">
<authors>
<author>I Namba</author>
</authors>
<title>Fujitsu Laboratories TREC9 Report.</title>
<booktitle>Proceedings of the Nineth Text REtrieval Conference, TREC-9. Gaithersburg,USA.2000,</booktitle>
<pages>203--208</pages>
<contexts>
<context position="5202" citStr="[13]" startWordPosition="816" endWordPosition="816">essed with costly NLP tools by returning passages instead of whole documents. Two classifications can be accomplished in PR. The first one is in accordance with the way of dividing the documents into passages. The second one is in accordance with the moment in which the passage segmentation is carried out. With reference to the first one, PR community generally agrees with the classification proposed in [2], where the author distinguishes between discourse models, semantic models, and window models. The first one uses the structural properties of the documents, such as sentences or paragraphs [13][16] in order to define the passages. The second one divides each document into semantic pieces according to the different topics in the document [5]. The last one uses windows of a fixed size (usually a number of terms) to determine passage boundaries [2] [8]. At first glance, we could think that discoursebased models would be the most effective, in retrieval terms, since they use the structure of the document itself. However, this model greatest problem relies on detecting passage boundaries since it depends on the writing style of the author of each document. On the other hand, window model</context>
</contexts>
<marker>[13]</marker>
<rawString>Namba, I Fujitsu Laboratories TREC9 Report. Proceedings of the Nineth Text REtrieval Conference, TREC-9. Gaithersburg,USA.2000, pp 203-208</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>E Brown</author>
<author>D Radev</author>
<author>K Czuba</author>
</authors>
<title>One Search Engine or Two for QuestionAnswering.</title>
<date>2000</date>
<booktitle>In Nineth Text REtrieval Conference, Gaithersburg,USA.</booktitle>
<contexts>
<context position="9015" citStr="[14]" startWordPosition="1438" endWordPosition="1438">c perspective, “understanding” means to carry out many of the typical steps on natural language analysis: lexical, syntactic and semantic. This analysis takes much more time than the statistical analysis that is usually carried out in IR. Besides, as QA systems have to manage with as much text as done for IR tasks, and the user needs the answer in a limited interval of time, it is nearly mandatory that first, an IR system processes the query and second, the QA process continues with its output. In this way, the time of analysis is highly decreased. The analysis of current best systems [3] [4] [14] [6] allows identifying main QA sub-components where document retrieval is accomplished by using IR technology: • Question Analysis. • Document Retrieval. • Passage Selection. • Answer Extraction. 3 IR-n overview In this section, we describe the architecture of the proposed PR system, namely IR-n, focusing on its three main modules: indexing, passage retrieval and query expansion. 3.1 Indexing module The main aim of this module is to generate the dictionaries that contain all the required information for the passage retrieval module. It requires the following information for each term: • The n</context>
</contexts>
<marker>[14]</marker>
<rawString>Prager, J.; Brown, E.; Radev, D. and Czuba, K. One Search Engine or Two for QuestionAnswering. In Nineth Text REtrieval Conference, Gaithersburg,USA. 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer,</title>
<date>1989</date>
<publisher>Addison Wesley Publishing,</publisher>
<location>New York.</location>
<contexts>
<context position="2280" citStr="[15]" startWordPosition="345" endWordPosition="345"> IR and Natural Language Processing (NLP) techniques to perform their task. This combination allows text understanding until a minimum level that permits a precise answer detection and extraction. Nevertheless, since NLP techniques are computationally expensive, QA systems need to reduce the amount of text where these techniques have to be applied. In this way, they usually work on the output of IR systems [10] that select the most relevant documents to the query by supposing that they will contain the answer required. Most applied IR systems are mainly based on three models: the cosine model [15], the pivoted cosine model1 [17], and the probabilistic model (OKAPI [18]). Moreover, IR systems usually employ query expansion techniques that frequently improve their precision. These techniques can be based on thesaurus [21] or on the incorporation of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected an</context>
<context position="11386" citStr="[15]" startWordPosition="1854" endWordPosition="1854">ere fp,t is the number of times that the term t appears in the passage p. fq,t represents the number of times that the term t appears in the query q. N is the number of documents in the collection and ft is refers to the number of documents that contain the term t. 4. Only the most relevant passage of each document is selected for retrieval. 5. The selected passages are sorted by their similarity measure. 6. Passages are associated with the document they pertain and they are presented in a ranked list form. As we can notice, the similarity measure is similar to the cosine measure presented in [15]. The only difference is that the size of each passage (the number of terms) is not used to normalise the results. This proposal performs normalization according to the fixed number of sentences per passage. This difference makes the calculation simpler than other discourse-based PR or IR systems. Another important detail to remark is that we are using N as the number of documents in the collection, instead of the number of passages according to the considerations presented in [9]. As it has been commented, our PR system uses variable-sized passages that are based on a fixed number of sentence</context>
</contexts>
<marker>[15]</marker>
<rawString>Salton G. Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer, Addison Wesley Publishing, New York. 1989</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Salton</author>
<author>J Buckley Allan</author>
</authors>
<title>Approaches to passage retrieval in full text information systems.</title>
<booktitle>In R Korfhage, E Rasmussen &amp; P Willet (Eds.) Prodeedings of the 16 th annual international ACMSIGIR conference on research and development in information retrieval. Pittsburgh PA USA ,</booktitle>
<pages>49--58</pages>
<contexts>
<context position="5206" citStr="[16]" startWordPosition="816" endWordPosition="816">d with costly NLP tools by returning passages instead of whole documents. Two classifications can be accomplished in PR. The first one is in accordance with the way of dividing the documents into passages. The second one is in accordance with the moment in which the passage segmentation is carried out. With reference to the first one, PR community generally agrees with the classification proposed in [2], where the author distinguishes between discourse models, semantic models, and window models. The first one uses the structural properties of the documents, such as sentences or paragraphs [13][16] in order to define the passages. The second one divides each document into semantic pieces according to the different topics in the document [5]. The last one uses windows of a fixed size (usually a number of terms) to determine passage boundaries [2] [8]. At first glance, we could think that discoursebased models would be the most effective, in retrieval terms, since they use the structure of the document itself. However, this model greatest problem relies on detecting passage boundaries since it depends on the writing style of the author of each document. On the other hand, window models ha</context>
</contexts>
<marker>[16]</marker>
<rawString>Salton, G.; Allan, J. Buckley Approaches to passage retrieval in full text information systems. In R Korfhage, E Rasmussen &amp; P Willet (Eds.) Prodeedings of the 16 th annual international ACMSIGIR conference on research and development in information retrieval. Pittsburgh PA USA , pp 49-58</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Singhal</author>
<author>C Buckley</author>
<author>M Mitra</author>
</authors>
<title>Pivoted document length normalization.</title>
<date>1996</date>
<booktitle>Proceedings of the 19th annual international ACM-</booktitle>
<contexts>
<context position="2312" citStr="[17]" startWordPosition="350" endWordPosition="350">ing (NLP) techniques to perform their task. This combination allows text understanding until a minimum level that permits a precise answer detection and extraction. Nevertheless, since NLP techniques are computationally expensive, QA systems need to reduce the amount of text where these techniques have to be applied. In this way, they usually work on the output of IR systems [10] that select the most relevant documents to the query by supposing that they will contain the answer required. Most applied IR systems are mainly based on three models: the cosine model [15], the pivoted cosine model1 [17], and the probabilistic model (OKAPI [18]). Moreover, IR systems usually employ query expansion techniques that frequently improve their precision. These techniques can be based on thesaurus [21] or on the incorporation of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents)</context>
</contexts>
<marker>[17]</marker>
<rawString>Singhal, A.; Buckley, C. and Mitra, M. Pivoted document length normalization. Proceedings of the 19th annual international ACM- 1996.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Venner</author>
<author>S Walker</author>
</authors>
<title>Okapi &apos;84: `Best match&apos; system. Microcomputer networking in libraries II.</title>
<journal>Vine,</journal>
<volume>48</volume>
<pages>22--26</pages>
<contexts>
<context position="2353" citStr="[18]" startWordPosition="356" endWordPosition="356">k. This combination allows text understanding until a minimum level that permits a precise answer detection and extraction. Nevertheless, since NLP techniques are computationally expensive, QA systems need to reduce the amount of text where these techniques have to be applied. In this way, they usually work on the output of IR systems [10] that select the most relevant documents to the query by supposing that they will contain the answer required. Most applied IR systems are mainly based on three models: the cosine model [15], the pivoted cosine model1 [17], and the probabilistic model (OKAPI [18]). Moreover, IR systems usually employ query expansion techniques that frequently improve their precision. These techniques can be based on thesaurus [21] or on the incorporation of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents). Although PR systems apply IR-based tech</context>
</contexts>
<marker>[18]</marker>
<rawString>Venner, G. and Walker, S. Okapi &apos;84: `Best match&apos; system. Microcomputer networking in libraries II. Vine, 48,1983, pp 22-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Vicedo</author>
<author>A Ferrandez</author>
<author>F Llopis</author>
</authors>
<date>2001</date>
<booktitle>University of Alicante al TREC-10. In Tenth Text REtrieval Conference,</booktitle>
<location>Gaithersburg,USA.</location>
<contexts>
<context position="3182" citStr="[19]" startWordPosition="494" endWordPosition="494">nt documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents). Although PR systems apply IR-based techniques to perform their work, they have revealed to be more effective than IR systems for QA tasks. In this paper, we are analysing the importance of the IR-n PR system for QA n [11] as it was used in last TREC-10 Conference [19]. The following section briefly presents the backgrounds in IR, PR and QA. Section 3 shows the architecture of IR-n. Section 4 presents the evaluation accomplished and finally, section 5 details conclusions and work in progress. 1 It is a modification of the cosine model. It tries to reduce the problem of the preference for bigger documents. 2 Backgrounds in Question Answering and Passage Retrieval 2.1 Information Retrieval and Passage Retrieval Given a question, an IR system sorts the documents by its relevance to the query. It computes the similarity between each document and the question by</context>
</contexts>
<marker>[19]</marker>
<rawString>Vicedo, J.; Ferrandez, A and Llopis, F. University of Alicante al TREC-10. In Tenth Text REtrieval Conference, Gaithersburg,USA. 2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Vicedo</author>
<author>Ferrandez</author>
</authors>
<title>A; A semantic approach to Question Answering systems.</title>
<date>2000</date>
<booktitle>In Nineth Text REtrieval Conference,</booktitle>
<pages>440--444</pages>
<marker>[20]</marker>
<rawString>Vicedo, J.; Ferrandez, A; A semantic approach to Question Answering systems. In Nineth Text REtrieval Conference, 2000 pp 440-444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Jing</author>
<author>W B Croft</author>
</authors>
<title>An association thesaurus for information retrieval.</title>
<date>1994</date>
<booktitle>In RIAO 94 Conference Proceedings, ,</booktitle>
<pages>146--160</pages>
<location>New York,</location>
<contexts>
<context position="2507" citStr="[21]" startWordPosition="377" endWordPosition="377">ques are computationally expensive, QA systems need to reduce the amount of text where these techniques have to be applied. In this way, they usually work on the output of IR systems [10] that select the most relevant documents to the query by supposing that they will contain the answer required. Most applied IR systems are mainly based on three models: the cosine model [15], the pivoted cosine model1 [17], and the probabilistic model (OKAPI [18]). Moreover, IR systems usually employ query expansion techniques that frequently improve their precision. These techniques can be based on thesaurus [21] or on the incorporation of the most frequent terms in the top M relevant documents [7]. Currently, several Passage Retrieval (PR) systems have also been proposed for this task [2][5][8][9]. PR systems deal with fragments of text in order to determine the relevance of a document to a query, as well as to detect document extracts that are likely to contain the expected answer (instead of full documents). Although PR systems apply IR-based techniques to perform their work, they have revealed to be more effective than IR systems for QA tasks. In this paper, we are analysing the importance of the </context>
</contexts>
<marker>[21]</marker>
<rawString>Y. Jing and W. B. Croft. An association thesaurus for information retrieval. In RIAO 94 Conference Proceedings, , New York, 1994. pp 146--160</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>