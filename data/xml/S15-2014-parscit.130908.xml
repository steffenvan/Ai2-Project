<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048439">
<title confidence="0.994505">
yiGou: A Semantic Text Similarity Computing System Based on SVM
</title>
<author confidence="0.999833">
Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang
</author>
<affiliation confidence="0.9981865">
School of Computer Science and Technology
Harbin Institute of Technology, China
</affiliation>
<email confidence="0.990509">
{yliu,cjsun,linl,wangxl}@insun.hit.edu.cn
</email>
<sectionHeader confidence="0.997284" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966">
This paper describes the yiGou system we de-
veloped to compute the semantic similarity of
two English sentences, which we submitted to
the SemEval 2015 Task 2 (English subtask).
The system uses a support vector machine
model with literal similarity, shallow syntactic
similarity, WordNet-based similarity and la-
tent semantic similarity to predict the seman-
tic similarity score of two short texts. In our
experiments, WordNet-based and LSA-based
features performed better than other features.
Out of the 73 submitted runs, our two runs
ranked 38th and 42th, with mean Pearson corre-
lation 0.7114 and 0.6964 respectively.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999579205128205">
Semantic Text Similarity (STS) plays an important
role in many Natural language processing tasks,
such as Question Answering (Narayanan and
Harabagiu, 2004), Machine Translation (Beale et
al., 1995), Automatic Summarization (Wang et al.,
2008) and Word Sense Disambiguation (Navigli
and Velardi, 2005). Since STS is an essential chal-
lenge in NLP, that has attracted a significant
amount of attention by the research community.
SemEval has held tasks about STS for four years in
a row, from which we can see the importance and
difficulty of this challenge. Particularly, SemEval
focuses on semantic similarity of short texts as a
lot of researches about long texts have been done
in past years and the demand of finding new meth-
ods to measure short texts similarity has become
stronger in many new applications.
In this paper, we proposed a SVM-based solu-
tion to compute the semantic similarity between
two sentences which is the goal of SemEval 2015
Task 2. Knowledge-based and corpus-based fea-
tures were involved in our solution. We used the
combination of the word similarity to estimate sen-
tence similarity. And the training data of SemEval
2012 (Agirre et al., 2012) was used to train our
model. In our experiments, WordNet-based and
LSA-based features performed better than other
features. Out of the 73 submitted runs, our two
runs ranked 38th and 42th, with mean Pearson cor-
relation 0.7114 and 0.6964 respectively. The eval-
uation results showed that our solution has good
generalization ability on the test dataset of
SemEval 2015 which is very different from our
training set in terms of the sources of the sentences.
Some of the relatively new technologies such as
Word2Vec (Mikolov et al., 2013) and Sen-
tence2Vec (Le and Mikolov, 2014) are potential
methods to represent sentences and will be includ-
ed in our further works.
</bodyText>
<sectionHeader confidence="0.940298" genericHeader="introduction">
2 Data and Metrics
</sectionHeader>
<bodyText confidence="0.999961">
In SemEval 2015, the trial dataset comprises the
2012, 2013 and 2014 datasets, which can be used
to develop and train models. Because of the limita-
tion of the time, we only used the training data of
SemEval 2012 as our training set. The training data
of SemEval 2012 contained 2000 sentence pairs
from existing paraphrase datasets and machine
translation evaluation resources, while the test set
of SemEval 2015 coming from image description,
news headlines, student answers paired with refer-
ence answer, answers to questions posted in stack
exchange forums and English discussion forum
data exhibiting committed belief. The evaluation
metric of SemEval 2015 task 2 is mean Pearson
correlation, which is calculated by averaging the
Pearson correlations of each subset in the test set.
</bodyText>
<page confidence="0.983158">
80
</page>
<note confidence="0.6058425">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 80–84,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.984553" genericHeader="method">
3 Feature engineering
</sectionHeader>
<bodyText confidence="0.999868">
Considering the training set used in our system, we
were trying to generate features which have little
relation with the sources where the sentences came
from. Four kinds of features are included in our
model. They are literal similarity, shallow syntactic
similarity, WordNet-based similarity and latent
semantic similarity.
</bodyText>
<subsectionHeader confidence="0.999338">
3.1 Literal Similarity
</subsectionHeader>
<bodyText confidence="0.879515">
Intuitively, a pair of sentences that look similar to
each other may be similar semantically. For exam-
ple:
</bodyText>
<listItem confidence="0.986844666666667">
S1: A boy is playing a guitar.
S2: A man is playing a guitar.
S3: Someone is drawing.
</listItem>
<bodyText confidence="0.982674333333333">
Apparently, S1 and S2 look more similar and they
are closer in semantics than S1 and S3. We chose
the Edit Distance (also known as Levenshtein Dis-
tance) over characters to measure the similarity
between two sentences. The higher the value is, the
less similar the two sentences are. As this measure
is case sensitive, we lowercase all letters in the
sentences before computing the similarity. Alt-
hough this method may draw opposite conclusions
to our expectations in some specific occasions (For
example, I hate it VS I have it, the Edit Distance of
this pair of sentences is two, but they express very
different meaning), the feature was still kept as we
observed that it contributed to the overall perfor-
mance in our experiments.
</bodyText>
<subsectionHeader confidence="0.999669">
3.2 Shallow Syntactic Similarity
</subsectionHeader>
<bodyText confidence="0.99848875">
It is quite a common phenomenon that two sen-
tences only differ in one or two syntactic constitu-
ents and have very similar syntactic structures. For
example (example comes from training set):
</bodyText>
<listItem confidence="0.968945">
S1: A man is peeling a potato.
S2: A man is slicing a potato.
</listItem>
<bodyText confidence="0.9989661">
This pair of sentences got very high score in gold-
en standard file. As we can see, only the predicates
of the two sentences are different, and the rest of
the sentences are the same. This gives us a clue
that using syntactic similarity to build the feature
could be feasible. Moreover, two sentences may
express exactly the same meaning, but use differ-
ent English voices. This situation was also consid-
ered in our model. Jaccard Distance was chosen to
compute this feature, which is defined as follows:
</bodyText>
<equation confidence="0.990662666666667">
ȁܵଵ ת ܵଶȁ
ܬሺܵଵǡ ܵଶሻ ൌ
ȁܵଵ ׫ ܵଶȁ
</equation>
<bodyText confidence="0.999939">
Where ܵଵ and ܵଶ are the collections of Part-Of-
Speech tags of each sentence. We used the NLTK
toolkit (Bird, 2006) to tag each sentence. Since
Jaccard distance measure only cares about the ap-
pearance of the tags, and ignores the order of them,
it can reduce the impact of the tense change.
</bodyText>
<subsectionHeader confidence="0.997958">
3.3 WordNet-based Similarity
</subsectionHeader>
<bodyText confidence="0.964639592592593">
WordNet (Miller, 1995) is a widely used lexical
database for English, and it’s a convenient tool to
find synonyms of nouns, verbs, adjectives and ad-
verbs. WordNet supports numerous lexical similar-
ity measures (Pedersen et al., 2004). In this work,
we explore using two of these similarity measures:
res_similarity and path_similarity. The core idea
behind the path_similarity measure is that the simi-
larity between two concepts can be derived from
the length of the path linking the concepts and the
position of the concepts in the WordNet taxonomy.
(Meng et al., 2013). While res_similarity (Resnik,
2011) is a similarity measure based on information
content. The result of res_similarity is dependent
on the corpus that generates the information con-
tent.
Figure 1 An example of word alignment using maxi-
mum path_similarity. The upper part of the figure is
showing the alignment candidates for tomato scored
with path_similarity and the lower part of the figure is
showing the max path_similarity alignment for the con-
tent words in the sentence pair.
In our system, we used the NLTK WordNet
API to compute WordNet-based similarity. Based
on WordNet and Brown corpus, the computing of
res_similarity and path_similarity involve follow-
ing steps:
</bodyText>
<listItem confidence="0.9926418">
• Partition a pair of sentences into two lists of
tokens.
• Part-of-speech tagging.
• Find out the most appropriate sense for every
word according to the tagging results; put the
</listItem>
<page confidence="0.978953">
81
</page>
<table confidence="0.9996134">
Features MSRpar MSRvid SMTeuroparl Sur.OnWN Sur.SMTnews Mean
All 0.51237 0.83766 0.48213 0.67070 0.47941 0.596454
w/o res_similarity 0.50939 0.83920 0.47976 0.66406 0.47976 0.594434
w/o path_similarity 0.37667 0.78555 0.38714 0.64145 0.45963 0.530088
w/o WN-based sim 0.37583 0.79046 0.38930 0.64348 0.45767 0.531348
</table>
<tableCaption confidence="0.881465">
Table 1 Results of comparing the importance of res_similarity and path_similarity on test set of SemEval 2012. The
WN-based sim included both res_similarity and path_similarity.
</tableCaption>
<table confidence="0.999812428571428">
Corpus MSRpar MSRvid SMTeuroparl Sur.OnWN Sur.SMTnews Mean
Brown 0.51237 0.83766 0.48213 0.67070 0.47941 0.596454
Bnc 0.51199 0.83770 0.48157 0.66719 0.48050 0.595790
Treebank 0.51199 0.83781 0.48181 0.66689 0.48066 0.595832
Semcor 0.51269 0.83768 0.48017 0.66763 0.48017 0.595668
Semcorraw 0.51274 0.83792 0.48138 0.66691 0.47997 0.595784
Shaks 0.51120 0.83746 0.48229 0.66665 0.48105 0.595730
</table>
<tableCaption confidence="0.999223">
Table 2 Results of using different corpus in res_similarity on test set of SemEval 2012.
</tableCaption>
<bodyText confidence="0.864344">
results into two lists S1 and S2.
</bodyText>
<listItem confidence="0.992031384615385">
• For every word w in S1, find out the word in
S2 that has the maximum res_similarity/
path_similarity with w. Adding all of the simi-
larity values together, and then average this
value with the length of S1.
• For every word w in S2, find out the word in
S1 that has the maximum res_similarity/
path_similarity with w. Adding all of the simi-
larity values together, and then average this
value with the length of S2.
• Computing the harmonic mean of the two av-
erage values, and the result is the value of this
feature.
</listItem>
<bodyText confidence="0.99412808">
Figure 1 is an example shows how we find the
corresponding word which has the maximum
res_similarity/path_similarity with the words in
the second sentence. In this example, potato has
the maximum path_similarity score with tomato,
compared to girl and slicing (0.33 vs. 0.0077 and
0.0). In the bottom part of the figure, each word in
the first sentence would find one word which has
the maximum similarity score in the second sen-
tence, these scores would then be used to compute
this feature.
To compare the importance of the two measures,
we separately exclude one of the two features from
all the features used in our solution to train two
models and compare their performance. The results
are shown in Table 1. As we can see from the table,
path_similarity contributes more to our overall
performance than res_similarity. According to the
definition of res_similarity, we changed the corpus
to find out the influence of the corpus on our over-
all performance. The results are showed in Table 2,
from which we can see that the results varied very
little with different corpora. In our submitted mod-
el, Brown corpus (Francis and Kucera, 1979) was
used to compute information content.
</bodyText>
<subsectionHeader confidence="0.999393">
3.4 Latent Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.9999705">
All of the features generated above contained little
semantic information. While sentences from some
sources such as headlines and image descriptions
are always have various forms which may not be
easily compared through some string match
measures or shallow syntactic oriented measures.
So, a new feature that measures similarity in se-
mantic space is necessary. Latent semantic analysis
(Landauer et al., 1998) is a very popular technique
to convert the term-document matrix which de-
scribes the occurrences of terms in document into
three smaller matrixes like follows:
</bodyText>
<equation confidence="0.948205">
X _ UEVT
</equation>
<bodyText confidence="0.9999915">
Where U could be preserved as the semantic space
of words. Each word could be represented as a row
vector in U. When measuring semantic similarity
of two sentences, all word vectors appeared in the
sentence were summed and then averaged with the
length of the sentences. Thus we can get vector
representations of the two sentences V1 and V2.
With V1 and V2, the similarity of the two sentences
can be measured with cosine similarity. Cosine
similarity defined as follows:
</bodyText>
<equation confidence="0.894690333333333">
Cos(V1,V2) _
V1 • V2
pV1ppV2p
</equation>
<page confidence="0.995048">
82
</page>
<table confidence="0.999508">
Features MSRpar MSRvid SMTeuroparl Sur.OnWN Sur.SMTnews Mean
1 to 2 -0.05064 0.23562 -0.13259 0.07697 -0.03636 0.018600
1 to 3 0.50225 0.82813 0.41859 0.57242 0.35525 0.535328
1 to 4 0.50593 0.82628 0.41881 0.57676 0.35390 0.536336
1 to 5 0.51120 0.83746 0.48229 0.66665 0.48105 0.595730
1 to 7 0.51237 0.83766 0.48213 0.67070 0.47941 0.596454
</table>
<tableCaption confidence="0.9933735">
Table 4 Results of SVR on SemEval 2012 test set with different feature combinations.
Table 3 All features we used in our submitted model.
</tableCaption>
<bodyText confidence="0.9999538">
In our experiment, we directly used the LSA model
provided by SEMILAR1. A word is represented as
a row vector in the LSA model (Niraula et al.,
2014), and the model was decomposed from the
whole Wikipedia articles.
We also developed two weighted LSA features
to further use semantic information, they are IDF-
weighted-LSA and Freq-weighted-LSA. IDF-
weighted-LSA weighted the words (one word is
represented as a 200-dimension vector generated
from LSA) using inverse document frequency and
then summed up all the weighted vectors of words
which appeared in the sentence to be the represen-
tation of the sentence. The cosine distance of two
sentence representations is the value of this feature.
Freq-weighted-LSA used word frequency to weight
the words and following the same steps mentioned
above. In our experiment, the IDF and Word-
Frequency values were calculated on Wikipedia
corpus dumped in December of 2012 (Jin et al.,
2014). These features were only included in our
second run yiGou-midbaitu. Unfortunately, this
system got worse performance than the first run in
official estimation. This may be caused by the
overfitting of our model on the training data.
</bodyText>
<sectionHeader confidence="0.997183" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.997383">
Due to the limitation of the time, in our submitted
system, we trained Support Vector Regression
(SVR) models using Scikit-learn toolkit (Pedrego
</bodyText>
<footnote confidence="0.911544">
1 http://www.semanticsimilarity.org/
</footnote>
<table confidence="0.7656295">
parameter kernel gamma C epsilon
value rbf 0.0 1.0 0.1
</table>
<tableCaption confidence="0.956297">
Table 5 Parameter setting in our models.
</tableCaption>
<bodyText confidence="0.998577">
sa et al., 2011). Table 3 shows the features used in
our submitted models. The results with different
feature combinations on the test set of SemEval
2012 are shown in Table 4. Table 5 is our parame-
ter settings.
The performance of the best system in SemEval
2012 is 0.67 (Mean) with 19 features, and our best
performance is 0.596 (Mean) with 7 features. In
SemEval 2015, out of the 73 submitted runs, our
two runs ranked 38th and 42th (with mean Pearson
correlation 0.7114 and 0.6964 respectively). And
the best performance in 2015 is 0.8015.
</bodyText>
<sectionHeader confidence="0.992632" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999958">
In this paper, we presented our system that partici-
pated in the Semantic Text Similarity task in
SemEval 2015. We proposed a method using SVR
to combine various features to evaluate the similar-
ity between two sentences. We found that Word-
Net based and LSA-based features are very useful
for semantic similarity computing. For future work,
we would like to further explore features about
semantic representations of words, generate more
features related to sentence structures and try to
employ some new technologies such as Word2Vec
and Sentence2Vec in our model. Besides, using a
single model is not adequate to get a better accura-
cy, other models will be tried and compared in our
further work.
</bodyText>
<sectionHeader confidence="0.991711" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999821333333333">
The authors would like to thank the SemEval-2015
Task 2 organizers for their hard work. We also
thank Daniel Cer and the anonymous reviewers for
their helpful suggestions and comments. This work
is supported by the National Natural Science
Foundation of China (61100094 &amp; 61300114).
</bodyText>
<figure confidence="0.998066">
Feature_ID
1
2
3
4
5
6
7
Feature_Name
Edit Distance
Jaccard Distance
path_similarity
res_similarity
Latent Semantic Similarity
IDF-weighted-LSA
Freq-weighted-LSA
</figure>
<page confidence="0.995631">
83
</page>
<sectionHeader confidence="0.989278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999445976470589">
Eneko Agirre, Mona Diab, Daniel Cer, &amp; Aitor
Gonzalez-Agirre. (2012). Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings of
the First Joint Conference on Lexical and
Computational Semantics-Volume 1: Proceedings of
the main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation.
Stephen Beale, Sergei Nirenburg, &amp; Kavi Mahesh.
(1995). Semantic analysis in the Mikrokosmos
machine translation project. In Proceedings of the
2nd Symposium on Natural Language Processing.
Steven Bird. (2006). NLTK: the natural language toolkit.
In Proceedings of the COLING/ACL on Interactive
presentation sessions.
W Nelson Francis, &amp; Henry Kucera. (1979). Brown
corpus manual. Brown University Department of
Linguistics.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., ... &amp; Duchesnay, É. (2011).
Scikit-learn: Machine learning in Python. The
Journal of Machine Learning Research, 12, 2825-
2830.
Meng, L., Huang, R., &amp; Gu, J. (2013). A review of
semantic similarity measures in
wordnet. International Journal of Hybrid Information
Technology, 6(1), 1-12.
Xiaoqiang Jin, Chengjie Sun, Lei Lin, &amp; Xiaolong
Wang. (2014). Exploiting Multiple Resources for
Word-Phrase Semantic Similarity Evaluation Chinese
Computational Linguistics and Natural Language
Processing Based on Naturally Annotated Big Data
(pp. 46-57): Springer.
Thomas K Landauer, Peter W Foltz, &amp; Darrell Laham.
(1998). An introduction to latent semantic analysis.
Discourse processes, 25(2-3), 259-284.
Quoc V Le, &amp; Tomas Mikolov. (2014). Distributed
Representations of Sentences and Documents. arXiv
preprint arXiv:1405.4053.
Tomas Mikolov, Kai Chen, Greg Corrado, &amp; Jeffrey
Dean. (2013). Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
George A Miller. (1995). WordNet: a lexical database
for English. Communications of the ACM, 38(11),
39-41.
Srini Narayanan, &amp; Sanda Harabagiu. (2004). Question
answering based on semantic structures. In
Proceedings of the 20th international conference on
Computational Linguistics.
Roberto Navigli, &amp; Paola Velardi. (2005). Structural
semantic interconnections: a knowledge-based
approach to word sense disambiguation. Pattern
Analysis and Machine Intelligence, IEEE
Transactions on, 27(7), 1075-1086.
Nobal B Niraula, Vasile Rus, Rajendra Banjade, Dan
Stefanescu, William Baggett, &amp; Brent Morgan.
(2014). The dare corpus: A resource for anaphora
resolution in dialogue based intelligent tutoring
systems. Proceedings of Language Resources and
Evaluation, LREC.
Ted Pedersen, Siddharth Patwardhan, &amp; Jason
Michelizzi. (2004). WordNet:: Similarity: measuring
the relatedness of concepts. In Proceedings of the
Demonstration Papers at HLT-NAACL 2004.
Philip Resnik. (2011). Semantic similarity in a
taxonomy: An information-based measure and its
application to problems of ambiguity in natural
language. arXiv preprint arXiv:1105.5444.
Sheldon Ross. (2009). A First Course in Probability 8th
Edition: Pearson.
Frane garić, Goran GlavaУ, Mladen Karan, Jan gnajder,
&amp; Bojana Dalbelo Bašić. (2012). Takelab: Systems
for measuring semantic text similarity. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation.
Dingding Wang, Tao Li, Shenghuo Zhu, &amp; Chris Ding.
(2008). Multi-document summarization via sentence-
level semantic analysis and symmetric matrix
factorization. In Proceedings of the 31st annual
international ACM SIGIR conference on Research
and development in information retrieval.
</reference>
<page confidence="0.999226">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.786356">
<title confidence="0.998702">yiGou: A Semantic Text Similarity Computing System Based on SVM</title>
<author confidence="0.982607">Yang Liu</author>
<author confidence="0.982607">Chengjie Sun</author>
<author confidence="0.982607">Lei Lin</author>
<author confidence="0.982607">Xiaolong</author>
<affiliation confidence="0.9813115">School of Computer Science and Harbin Institute of Technology, China</affiliation>
<email confidence="0.968901">yliu@insun.hit.edu.cn</email>
<email confidence="0.968901">cjsun@insun.hit.edu.cn</email>
<email confidence="0.968901">linl@insun.hit.edu.cn</email>
<email confidence="0.968901">wangxl@insun.hit.edu.cn</email>
<abstract confidence="0.9892572">This paper describes the yiGou system we developed to compute the semantic similarity of two English sentences, which we submitted to the SemEval 2015 Task 2 (English subtask). The system uses a support vector machine model with literal similarity, shallow syntactic similarity, WordNet-based similarity and latent semantic similarity to predict the semantic similarity score of two short texts. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs and with mean Pearson correlation 0.7114 and 0.6964 respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="2049" citStr="Agirre et al., 2012" startWordPosition="315" endWordPosition="318">enge. Particularly, SemEval focuses on semantic similarity of short texts as a lot of researches about long texts have been done in past years and the demand of finding new methods to measure short texts similarity has become stronger in many new applications. In this paper, we proposed a SVM-based solution to compute the semantic similarity between two sentences which is the goal of SemEval 2015 Task 2. Knowledge-based and corpus-based features were involved in our solution. We used the combination of the word similarity to estimate sentence similarity. And the training data of SemEval 2012 (Agirre et al., 2012) was used to train our model. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. The evaluation results showed that our solution has good generalization ability on the test dataset of SemEval 2015 which is very different from our training set in terms of the sources of the sentences. Some of the relatively new technologies such as Word2Vec (Mikolov et al., 2013) and Sentence2Vec (Le and Mikolov, 2014) are potential methods to</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, &amp; Aitor Gonzalez-Agirre. (2012). Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Beale</author>
<author>Sergei Nirenburg</author>
<author>Kavi Mahesh</author>
</authors>
<title>Semantic analysis in the Mikrokosmos machine translation project.</title>
<date>1995</date>
<booktitle>In Proceedings of the 2nd Symposium on Natural Language Processing.</booktitle>
<contexts>
<context position="1077" citStr="Beale et al., 1995" startWordPosition="153" endWordPosition="156">chine model with literal similarity, shallow syntactic similarity, WordNet-based similarity and latent semantic similarity to predict the semantic similarity score of two short texts. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. 1 Introduction Semantic Text Similarity (STS) plays an important role in many Natural language processing tasks, such as Question Answering (Narayanan and Harabagiu, 2004), Machine Translation (Beale et al., 1995), Automatic Summarization (Wang et al., 2008) and Word Sense Disambiguation (Navigli and Velardi, 2005). Since STS is an essential challenge in NLP, that has attracted a significant amount of attention by the research community. SemEval has held tasks about STS for four years in a row, from which we can see the importance and difficulty of this challenge. Particularly, SemEval focuses on semantic similarity of short texts as a lot of researches about long texts have been done in past years and the demand of finding new methods to measure short texts similarity has become stronger in many new a</context>
</contexts>
<marker>Beale, Nirenburg, Mahesh, 1995</marker>
<rawString>Stephen Beale, Sergei Nirenburg, &amp; Kavi Mahesh. (1995). Semantic analysis in the Mikrokosmos machine translation project. In Proceedings of the 2nd Symposium on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>NLTK: the natural language toolkit.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions.</booktitle>
<contexts>
<context position="5931" citStr="Bird, 2006" startWordPosition="963" endWordPosition="964">score in golden standard file. As we can see, only the predicates of the two sentences are different, and the rest of the sentences are the same. This gives us a clue that using syntactic similarity to build the feature could be feasible. Moreover, two sentences may express exactly the same meaning, but use different English voices. This situation was also considered in our model. Jaccard Distance was chosen to compute this feature, which is defined as follows: ȁܵଵ ת ܵଶȁ ܬሺܵଵǡ ܵଶሻ ൌ ȁܵଵ  ܵଶȁ Where ܵଵ and ܵଶ are the collections of Part-OfSpeech tags of each sentence. We used the NLTK toolkit (Bird, 2006) to tag each sentence. Since Jaccard distance measure only cares about the appearance of the tags, and ignores the order of them, it can reduce the impact of the tense change. 3.3 WordNet-based Similarity WordNet (Miller, 1995) is a widely used lexical database for English, and it’s a convenient tool to find synonyms of nouns, verbs, adjectives and adverbs. WordNet supports numerous lexical similarity measures (Pedersen et al., 2004). In this work, we explore using two of these similarity measures: res_similarity and path_similarity. The core idea behind the path_similarity measure is that the</context>
</contexts>
<marker>Bird, 2006</marker>
<rawString>Steven Bird. (2006). NLTK: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive presentation sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<date>1979</date>
<institution>Brown University Department of Linguistics.</institution>
<note>Brown corpus manual.</note>
<contexts>
<context position="10236" citStr="Francis and Kucera, 1979" startWordPosition="1651" endWordPosition="1654">e of the two measures, we separately exclude one of the two features from all the features used in our solution to train two models and compare their performance. The results are shown in Table 1. As we can see from the table, path_similarity contributes more to our overall performance than res_similarity. According to the definition of res_similarity, we changed the corpus to find out the influence of the corpus on our overall performance. The results are showed in Table 2, from which we can see that the results varied very little with different corpora. In our submitted model, Brown corpus (Francis and Kucera, 1979) was used to compute information content. 3.4 Latent Semantic Similarity All of the features generated above contained little semantic information. While sentences from some sources such as headlines and image descriptions are always have various forms which may not be easily compared through some string match measures or shallow syntactic oriented measures. So, a new feature that measures similarity in semantic space is necessary. Latent semantic analysis (Landauer et al., 1998) is a very popular technique to convert the term-document matrix which describes the occurrences of terms in documen</context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>W Nelson Francis, &amp; Henry Kucera. (1979). Brown corpus manual. Brown University Department of Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2825--2830</pages>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, 2011</marker>
<rawString>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... &amp; Duchesnay, É. (2011). Scikit-learn: Machine learning in Python. The Journal of Machine Learning Research, 12, 2825-2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Meng</author>
<author>R Huang</author>
<author>J Gu</author>
</authors>
<title>A review of semantic similarity measures in wordnet.</title>
<date>2013</date>
<journal>International Journal of Hybrid Information Technology,</journal>
<volume>6</volume>
<issue>1</issue>
<pages>1--12</pages>
<contexts>
<context position="6705" citStr="Meng et al., 2013" startWordPosition="1088" endWordPosition="1091">f the tense change. 3.3 WordNet-based Similarity WordNet (Miller, 1995) is a widely used lexical database for English, and it’s a convenient tool to find synonyms of nouns, verbs, adjectives and adverbs. WordNet supports numerous lexical similarity measures (Pedersen et al., 2004). In this work, we explore using two of these similarity measures: res_similarity and path_similarity. The core idea behind the path_similarity measure is that the similarity between two concepts can be derived from the length of the path linking the concepts and the position of the concepts in the WordNet taxonomy. (Meng et al., 2013). While res_similarity (Resnik, 2011) is a similarity measure based on information content. The result of res_similarity is dependent on the corpus that generates the information content. Figure 1 An example of word alignment using maximum path_similarity. The upper part of the figure is showing the alignment candidates for tomato scored with path_similarity and the lower part of the figure is showing the max path_similarity alignment for the content words in the sentence pair. In our system, we used the NLTK WordNet API to compute WordNet-based similarity. Based on WordNet and Brown corpus, t</context>
</contexts>
<marker>Meng, Huang, Gu, 2013</marker>
<rawString>Meng, L., Huang, R., &amp; Gu, J. (2013). A review of semantic similarity measures in wordnet. International Journal of Hybrid Information Technology, 6(1), 1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Jin</author>
<author>Chengjie Sun</author>
<author>Lei Lin</author>
<author>Xiaolong Wang</author>
</authors>
<title>Exploiting Multiple Resources for Word-Phrase Semantic Similarity Evaluation</title>
<date>2014</date>
<booktitle>Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data</booktitle>
<pages>46--57</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="12804" citStr="Jin et al., 2014" startWordPosition="2065" endWordPosition="2068">ghted-LSA and Freq-weighted-LSA. IDFweighted-LSA weighted the words (one word is represented as a 200-dimension vector generated from LSA) using inverse document frequency and then summed up all the weighted vectors of words which appeared in the sentence to be the representation of the sentence. The cosine distance of two sentence representations is the value of this feature. Freq-weighted-LSA used word frequency to weight the words and following the same steps mentioned above. In our experiment, the IDF and WordFrequency values were calculated on Wikipedia corpus dumped in December of 2012 (Jin et al., 2014). These features were only included in our second run yiGou-midbaitu. Unfortunately, this system got worse performance than the first run in official estimation. This may be caused by the overfitting of our model on the training data. 4 Experiments and Results Due to the limitation of the time, in our submitted system, we trained Support Vector Regression (SVR) models using Scikit-learn toolkit (Pedrego 1 http://www.semanticsimilarity.org/ parameter kernel gamma C epsilon value rbf 0.0 1.0 0.1 Table 5 Parameter setting in our models. sa et al., 2011). Table 3 shows the features used in our sub</context>
</contexts>
<marker>Jin, Sun, Lin, Wang, 2014</marker>
<rawString>Xiaoqiang Jin, Chengjie Sun, Lei Lin, &amp; Xiaolong Wang. (2014). Exploiting Multiple Resources for Word-Phrase Semantic Similarity Evaluation Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data (pp. 46-57): Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis. Discourse processes,</title>
<date>1998</date>
<pages>25--2</pages>
<contexts>
<context position="10720" citStr="Landauer et al., 1998" startWordPosition="1723" endWordPosition="1726">ich we can see that the results varied very little with different corpora. In our submitted model, Brown corpus (Francis and Kucera, 1979) was used to compute information content. 3.4 Latent Semantic Similarity All of the features generated above contained little semantic information. While sentences from some sources such as headlines and image descriptions are always have various forms which may not be easily compared through some string match measures or shallow syntactic oriented measures. So, a new feature that measures similarity in semantic space is necessary. Latent semantic analysis (Landauer et al., 1998) is a very popular technique to convert the term-document matrix which describes the occurrences of terms in document into three smaller matrixes like follows: X _ UEVT Where U could be preserved as the semantic space of words. Each word could be represented as a row vector in U. When measuring semantic similarity of two sentences, all word vectors appeared in the sentence were summed and then averaged with the length of the sentences. Thus we can get vector representations of the two sentences V1 and V2. With V1 and V2, the similarity of the two sentences can be measured with cosine similarit</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K Landauer, Peter W Foltz, &amp; Darrell Laham. (1998). An introduction to latent semantic analysis. Discourse processes, 25(2-3), 259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le</author>
<author>Tomas Mikolov</author>
</authors>
<date>2014</date>
<booktitle>Distributed Representations of Sentences and Documents. arXiv preprint arXiv:1405.4053.</booktitle>
<contexts>
<context position="2624" citStr="Le and Mikolov, 2014" startWordPosition="410" endWordPosition="413">ing data of SemEval 2012 (Agirre et al., 2012) was used to train our model. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. The evaluation results showed that our solution has good generalization ability on the test dataset of SemEval 2015 which is very different from our training set in terms of the sources of the sentences. Some of the relatively new technologies such as Word2Vec (Mikolov et al., 2013) and Sentence2Vec (Le and Mikolov, 2014) are potential methods to represent sentences and will be included in our further works. 2 Data and Metrics In SemEval 2015, the trial dataset comprises the 2012, 2013 and 2014 datasets, which can be used to develop and train models. Because of the limitation of the time, we only used the training data of SemEval 2012 as our training set. The training data of SemEval 2012 contained 2000 sentence pairs from existing paraphrase datasets and machine translation evaluation resources, while the test set of SemEval 2015 coming from image description, news headlines, student answers paired with refer</context>
</contexts>
<marker>Le, Mikolov, 2014</marker>
<rawString>Quoc V Le, &amp; Tomas Mikolov. (2014). Distributed Representations of Sentences and Documents. arXiv preprint arXiv:1405.4053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="2584" citStr="Mikolov et al., 2013" startWordPosition="403" endWordPosition="406">imate sentence similarity. And the training data of SemEval 2012 (Agirre et al., 2012) was used to train our model. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. The evaluation results showed that our solution has good generalization ability on the test dataset of SemEval 2015 which is very different from our training set in terms of the sources of the sentences. Some of the relatively new technologies such as Word2Vec (Mikolov et al., 2013) and Sentence2Vec (Le and Mikolov, 2014) are potential methods to represent sentences and will be included in our further works. 2 Data and Metrics In SemEval 2015, the trial dataset comprises the 2012, 2013 and 2014 datasets, which can be used to develop and train models. Because of the limitation of the time, we only used the training data of SemEval 2012 as our training set. The training data of SemEval 2012 contained 2000 sentence pairs from existing paraphrase datasets and machine translation evaluation resources, while the test set of SemEval 2015 coming from image description, news head</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, &amp; Jeffrey Dean. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="6158" citStr="Miller, 1995" startWordPosition="1001" endWordPosition="1002"> be feasible. Moreover, two sentences may express exactly the same meaning, but use different English voices. This situation was also considered in our model. Jaccard Distance was chosen to compute this feature, which is defined as follows: ȁܵଵ ת ܵଶȁ ܬሺܵଵǡ ܵଶሻ ൌ ȁܵଵ  ܵଶȁ Where ܵଵ and ܵଶ are the collections of Part-OfSpeech tags of each sentence. We used the NLTK toolkit (Bird, 2006) to tag each sentence. Since Jaccard distance measure only cares about the appearance of the tags, and ignores the order of them, it can reduce the impact of the tense change. 3.3 WordNet-based Similarity WordNet (Miller, 1995) is a widely used lexical database for English, and it’s a convenient tool to find synonyms of nouns, verbs, adjectives and adverbs. WordNet supports numerous lexical similarity measures (Pedersen et al., 2004). In this work, we explore using two of these similarity measures: res_similarity and path_similarity. The core idea behind the path_similarity measure is that the similarity between two concepts can be derived from the length of the path linking the concepts and the position of the concepts in the WordNet taxonomy. (Meng et al., 2013). While res_similarity (Resnik, 2011) is a similarity</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. (1995). WordNet: a lexical database for English. Communications of the ACM, 38(11), 39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics.</booktitle>
<contexts>
<context position="1035" citStr="Narayanan and Harabagiu, 2004" startWordPosition="147" endWordPosition="150">English subtask). The system uses a support vector machine model with literal similarity, shallow syntactic similarity, WordNet-based similarity and latent semantic similarity to predict the semantic similarity score of two short texts. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. 1 Introduction Semantic Text Similarity (STS) plays an important role in many Natural language processing tasks, such as Question Answering (Narayanan and Harabagiu, 2004), Machine Translation (Beale et al., 1995), Automatic Summarization (Wang et al., 2008) and Word Sense Disambiguation (Navigli and Velardi, 2005). Since STS is an essential challenge in NLP, that has attracted a significant amount of attention by the research community. SemEval has held tasks about STS for four years in a row, from which we can see the importance and difficulty of this challenge. Particularly, SemEval focuses on semantic similarity of short texts as a lot of researches about long texts have been done in past years and the demand of finding new methods to measure short texts si</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Srini Narayanan, &amp; Sanda Harabagiu. (2004). Question answering based on semantic structures. In Proceedings of the 20th international conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Structural semantic interconnections: a knowledge-based approach to word sense disambiguation. Pattern Analysis and Machine Intelligence,</title>
<date>2005</date>
<journal>IEEE Transactions on,</journal>
<volume>27</volume>
<issue>7</issue>
<pages>1075--1086</pages>
<contexts>
<context position="1180" citStr="Navigli and Velardi, 2005" startWordPosition="167" endWordPosition="170">latent semantic similarity to predict the semantic similarity score of two short texts. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. 1 Introduction Semantic Text Similarity (STS) plays an important role in many Natural language processing tasks, such as Question Answering (Narayanan and Harabagiu, 2004), Machine Translation (Beale et al., 1995), Automatic Summarization (Wang et al., 2008) and Word Sense Disambiguation (Navigli and Velardi, 2005). Since STS is an essential challenge in NLP, that has attracted a significant amount of attention by the research community. SemEval has held tasks about STS for four years in a row, from which we can see the importance and difficulty of this challenge. Particularly, SemEval focuses on semantic similarity of short texts as a lot of researches about long texts have been done in past years and the demand of finding new methods to measure short texts similarity has become stronger in many new applications. In this paper, we proposed a SVM-based solution to compute the semantic similarity between</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Roberto Navigli, &amp; Paola Velardi. (2005). Structural semantic interconnections: a knowledge-based approach to word sense disambiguation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 27(7), 1075-1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobal B Niraula</author>
<author>Vasile Rus</author>
<author>Rajendra Banjade</author>
<author>Dan Stefanescu</author>
<author>William Baggett</author>
<author>Brent Morgan</author>
</authors>
<title>The dare corpus: A resource for anaphora resolution in dialogue based intelligent tutoring systems.</title>
<date>2014</date>
<booktitle>Proceedings of Language Resources and Evaluation, LREC.</booktitle>
<contexts>
<context position="12025" citStr="Niraula et al., 2014" startWordPosition="1943" endWordPosition="1946">SRpar MSRvid SMTeuroparl Sur.OnWN Sur.SMTnews Mean 1 to 2 -0.05064 0.23562 -0.13259 0.07697 -0.03636 0.018600 1 to 3 0.50225 0.82813 0.41859 0.57242 0.35525 0.535328 1 to 4 0.50593 0.82628 0.41881 0.57676 0.35390 0.536336 1 to 5 0.51120 0.83746 0.48229 0.66665 0.48105 0.595730 1 to 7 0.51237 0.83766 0.48213 0.67070 0.47941 0.596454 Table 4 Results of SVR on SemEval 2012 test set with different feature combinations. Table 3 All features we used in our submitted model. In our experiment, we directly used the LSA model provided by SEMILAR1. A word is represented as a row vector in the LSA model (Niraula et al., 2014), and the model was decomposed from the whole Wikipedia articles. We also developed two weighted LSA features to further use semantic information, they are IDFweighted-LSA and Freq-weighted-LSA. IDFweighted-LSA weighted the words (one word is represented as a 200-dimension vector generated from LSA) using inverse document frequency and then summed up all the weighted vectors of words which appeared in the sentence to be the representation of the sentence. The cosine distance of two sentence representations is the value of this feature. Freq-weighted-LSA used word frequency to weight the words </context>
</contexts>
<marker>Niraula, Rus, Banjade, Stefanescu, Baggett, Morgan, 2014</marker>
<rawString>Nobal B Niraula, Vasile Rus, Rajendra Banjade, Dan Stefanescu, William Baggett, &amp; Brent Morgan. (2014). The dare corpus: A resource for anaphora resolution in dialogue based intelligent tutoring systems. Proceedings of Language Resources and Evaluation, LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet:: Similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Demonstration Papers at HLT-NAACL</booktitle>
<contexts>
<context position="6368" citStr="Pedersen et al., 2004" startWordPosition="1033" endWordPosition="1036">s feature, which is defined as follows: ȁܵଵ ת ܵଶȁ ܬሺܵଵǡ ܵଶሻ ൌ ȁܵଵ  ܵଶȁ Where ܵଵ and ܵଶ are the collections of Part-OfSpeech tags of each sentence. We used the NLTK toolkit (Bird, 2006) to tag each sentence. Since Jaccard distance measure only cares about the appearance of the tags, and ignores the order of them, it can reduce the impact of the tense change. 3.3 WordNet-based Similarity WordNet (Miller, 1995) is a widely used lexical database for English, and it’s a convenient tool to find synonyms of nouns, verbs, adjectives and adverbs. WordNet supports numerous lexical similarity measures (Pedersen et al., 2004). In this work, we explore using two of these similarity measures: res_similarity and path_similarity. The core idea behind the path_similarity measure is that the similarity between two concepts can be derived from the length of the path linking the concepts and the position of the concepts in the WordNet taxonomy. (Meng et al., 2013). While res_similarity (Resnik, 2011) is a similarity measure based on information content. The result of res_similarity is dependent on the corpus that generates the information content. Figure 1 An example of word alignment using maximum path_similarity. The up</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, &amp; Jason Michelizzi. (2004). WordNet:: Similarity: measuring the relatedness of concepts. In Proceedings of the Demonstration Papers at HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. arXiv preprint arXiv:1105.5444.</title>
<date>2011</date>
<contexts>
<context position="6742" citStr="Resnik, 2011" startWordPosition="1094" endWordPosition="1095">larity WordNet (Miller, 1995) is a widely used lexical database for English, and it’s a convenient tool to find synonyms of nouns, verbs, adjectives and adverbs. WordNet supports numerous lexical similarity measures (Pedersen et al., 2004). In this work, we explore using two of these similarity measures: res_similarity and path_similarity. The core idea behind the path_similarity measure is that the similarity between two concepts can be derived from the length of the path linking the concepts and the position of the concepts in the WordNet taxonomy. (Meng et al., 2013). While res_similarity (Resnik, 2011) is a similarity measure based on information content. The result of res_similarity is dependent on the corpus that generates the information content. Figure 1 An example of word alignment using maximum path_similarity. The upper part of the figure is showing the alignment candidates for tomato scored with path_similarity and the lower part of the figure is showing the max path_similarity alignment for the content words in the sentence pair. In our system, we used the NLTK WordNet API to compute WordNet-based similarity. Based on WordNet and Brown corpus, the computing of res_similarity and pa</context>
</contexts>
<marker>Resnik, 2011</marker>
<rawString>Philip Resnik. (2011). Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. arXiv preprint arXiv:1105.5444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheldon Ross</author>
</authors>
<date>2009</date>
<booktitle>A First Course in Probability 8th Edition:</booktitle>
<publisher>Pearson.</publisher>
<marker>Ross, 2009</marker>
<rawString>Sheldon Ross. (2009). A First Course in Probability 8th Edition: Pearson.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane garić</author>
<author>Goran GlavaУ</author>
<author>Mladen Karan</author>
<author>Jan gnajder</author>
<author>Bojana Dalbelo Bašić</author>
</authors>
<title>Takelab: Systems for measuring semantic text similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation.</booktitle>
<marker>garić, GlavaУ, Karan, gnajder, Bašić, 2012</marker>
<rawString>Frane garić, Goran GlavaУ, Mladen Karan, Jan gnajder, &amp; Bojana Dalbelo Bašić. (2012). Takelab: Systems for measuring semantic text similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Tao Li</author>
<author>Shenghuo Zhu</author>
<author>Chris Ding</author>
</authors>
<title>Multi-document summarization via sentencelevel semantic analysis and symmetric matrix factorization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="1122" citStr="Wang et al., 2008" startWordPosition="159" endWordPosition="162">yntactic similarity, WordNet-based similarity and latent semantic similarity to predict the semantic similarity score of two short texts. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. 1 Introduction Semantic Text Similarity (STS) plays an important role in many Natural language processing tasks, such as Question Answering (Narayanan and Harabagiu, 2004), Machine Translation (Beale et al., 1995), Automatic Summarization (Wang et al., 2008) and Word Sense Disambiguation (Navigli and Velardi, 2005). Since STS is an essential challenge in NLP, that has attracted a significant amount of attention by the research community. SemEval has held tasks about STS for four years in a row, from which we can see the importance and difficulty of this challenge. Particularly, SemEval focuses on semantic similarity of short texts as a lot of researches about long texts have been done in past years and the demand of finding new methods to measure short texts similarity has become stronger in many new applications. In this paper, we proposed a SVM</context>
</contexts>
<marker>Wang, Li, Zhu, Ding, 2008</marker>
<rawString>Dingding Wang, Tao Li, Shenghuo Zhu, &amp; Chris Ding. (2008). Multi-document summarization via sentencelevel semantic analysis and symmetric matrix factorization. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>