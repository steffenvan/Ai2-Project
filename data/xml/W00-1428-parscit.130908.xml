<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9899965">
Integrating a Large-scale, Reusable Lexicon with a Natural
Language Generator
</title>
<author confidence="0.997165">
Hongyan Jing
</author>
<affiliation confidence="0.996623">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.98615">
New York, NY 10027, USA
</address>
<email confidence="0.998728">
hjing@cs.columbia.edu
</email>
<author confidence="0.995045">
Michael Elhadad
</author>
<affiliation confidence="0.71998275">
Department of Computer Science
Ben-Gurion University
Be&apos;er-Sheva, 84105, Israel
elhadadacs.bgu.ac.il
</affiliation>
<sectionHeader confidence="0.992656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983923076923">
This paper presents the integration of a large-
scale, reusable lexicon for generation with the
FUF/SURGE unification-based syntactic realizer.
The lexicon was combined from multiple existing re-
sources in a semi-automatic process. The integra-
tion is a multi-step unification process. This inte-
gration allows the reuse of lexical, syntactic, and
semantic knowledge encoded in the lexicon in the
development of lexical chooser module in a genera-
tion system. The lexicon also brings other benefits
to a generation system: for example, the ability to
generate many lexical and syntactic paraphrases and
the ability to avoid non-grammatical output.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999898954545455">
Natural language generation requires lexical, syn-
tactic, and semantic knowledge in order to produce
meaningful and fluent output. Such knowledge is
often hand-coded anew when a different application
is developed. We present in this paper the integra-
tion of a large-scale, reusable lexicon with a natural
language generator, FUF/SURGE (Elhadad, 1992;
Robin, 1994): we show that by integrating the lexi-
con with FUF/SURGE as a tactical component, we
can reuse the knowledge encoded in the lexicon and
automate to some extent the development of the lex-
ical realization component in a generation applica-
tion_
The integration of the lexicon with FIT/SURGE
also brings other benefits to generation, including
the possibility to accept a semantic input at the
level of WordNet synsets, the production of lexical
and syntactic paraphrases, the prevention of non-
grammatical output. reuse across applications, and
wide coverage.
We present the process of integrating the lexicon
with FUF/SURGE_ including how to represent. the
</bodyText>
<note confidence="0.5570094">
Yael Dahan Netzer
Department of Computer Science
Ben-Gurion University
Be&apos;er-Sheva, 84105, Israel
yaelrincs.bgu.ac.il
</note>
<author confidence="0.978203">
Kathleen R. McKeown
</author>
<affiliation confidence="0.996619">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.750165">
New York, NY 10027, USA
</address>
<email confidence="0.461192">
kathyOcs.columbia.edu
</email>
<bodyText confidence="0.96456212">
lexicon in FUF format, how to unify input with the
lexicon incrementally to generate more sophisticated
and informative representations, and how to design
an appropriate semantic input format so that the
integration of the lexicon and FUF/SURGE can be
done easily.
This paper is organized as follows. In Section 2,
we explain why a reusable lexical chooser for gen-
eration needs to be developed. In Section 3, we
present the large-scale, reusable lexicon which we
combined from multiple resources, and illustrate its
benefits to generation by examples. In Section 4, we
describe the process of integrating the lexicon with
FUF/SURGE, which includes four unification steps,
with each step adding additional lexical or syntac-
tic information. Other applications and comparison
with related work are presented in Section 5. Finally,
we conclude by discussing future work.
2 Building a reusable lexical chooser
for generation
While reusable components have been widely used in
generation applications, the concept of a &amp;quot;reusable
lexical chooser&amp;quot; for generation remains novel.
There are two main reasons why such a lexical
chooser has not been developed in the past:
</bodyText>
<listItem confidence="0.995055090909091">
1. In the overall architecture of a generator, the
lexical chooser is an internal component that
depends on the semantic representation and for-
.:rnaliarn and on the syntactic realizer used by the
application.
2. The lexical chooser links conceptual elements to
lexical items. Conceptual elements are bv defi-
nition domain and application dependent (they
are the primitive concepts used in an applica-
tion knowledge base). These primitives are not
easily ported from application to application.
</listItem>
<page confidence="0.99854">
209
</page>
<bodyText confidence="0.999874333333334">
The emergence of standard architectures for gen-
erators (RAGS, (Reiter, 1994)) and the possibility
to use a standard syntactic realizer answer the first
issue.
To address the second issue, one must realize that
if the whole lexical chooser can not be made domain-
independent, major parts can be made reusable.
The main argument is that lexical knowledge is mod-
ular. Therefore, while choice of words is constrained
by domain-specific conceptual knowledge (what in-
formation the sentences are to represent) on the one
hand, it is also affected by several other dimensions:
</bodyText>
<listItem confidence="0.961369">
6 inter-lexical constraints: collocations among
words
0 pragmatic constraints: connotations of words
6 stylistic constraints: familiarity of words
6 syntactic constraints: government patterns of
words, e.g., thematic structure of verbs.
</listItem>
<bodyText confidence="0.99876325">
We show in this paper how the separation of the
syntactic and conceptual interfaces of lexical item
definitions allows us to reuse a large amount of lex-
ical knowledge across applications.
</bodyText>
<sectionHeader confidence="0.7059115" genericHeader="method">
3 The lexicon and its benefits to
generation
</sectionHeader>
<subsectionHeader confidence="0.622931">
3.1 A large-scale, reusable lexicon for
generation
</subsectionHeader>
<bodyText confidence="0.974754866666667">
Natural Language generation starts from semantic
concepts and then finds words to realize such seman-
tic concepts. Most existing lexical resources, how-
ever, are indexed by words rather than by semantic
concepts. Such resources, therefore, can not be used
for generation directly. Moreover, generation needs
different types of knowledge, which typically are en-
coded in different resources. However, the different
representation formats used by these resources make
it impossible to use them simultaneously in a single
system.
To overcome these limitations, we built a large-
scale, reusable lexicon for generation by combining
multiple existing resources. The resources that are
combined include:
</bodyText>
<listItem confidence="0.834687181818182">
O The WordNet, Lexical Database (Miller et al.,
1990). WordNet is the largest lexical database
to date, consisting of over 120,000 unique words
(version 1.6). It also encodes many types of
lexical relations between words, including syn-
onymy, antonymy, and many more.
• English Verb Classes and Alternations
(EVCA) (Levin, 1993), It categorized 3.104
verbs into classes based on their syntactic
properties and studied verb alternations. An
alternation is a variation in the realization of
verb arguments. For example, the alternation
&amp;quot;there-insertion&amp;quot; transforms A ship appeared
on the horizonzto There,appeared ship. on.the
horizon. A total of 80 alternations for 3,104
verbs were studied.
• The COMLEX syntax dictionary (Grishman et
al., 1994). COMLEX contains syntactic infor-
mation for over 38,000 English words.
* The Brown Corpus tagged with WordNet senses
(Miller et al., 1993). We use this corpus for
frequency measurement.
</listItem>
<bodyText confidence="0.999856142857143">
In combining these resources, we focused on verbs,
since they play a more important role in deciding
sentence structures. The combined lexicon includes
rich lexical and syntactic knowledge for 5,676 verbs.
It is indexed by WordNet synsets(which are at the
semantic concept level) as required by the generation
task. The knowledge in the lexicon includes:
</bodyText>
<listItem confidence="0.9787675">
O A complete list of subcategorizations for each
sense of a verb.
• A large variety of alternations for each sense of
a verb.
• Frequency of lexical items and verb subeatego-
rizations in the tagged Brown corpus
</listItem>
<bodyText confidence="0.972201379310345">
O Rich lexical relations between words
The sample entry for the verb &amp;quot;appear&amp;quot; is shown
in Figure 1. It shows that the verb appear has eight
senses (the sense distinctions come from WordNet).
For each sense, the lexicon lists all the applicable
stibcategorization for that particular sense of the
verb. The subcategorizations are represented using
the same format as in COMLEX. For each sense,
the lexicon also lists applicable alternations, which
we encoded based on the information in EVCA. In
addition, for each subcategorization and alternation,
the lexicon lists the semantic category constraints on
verb arguments. In the figure, we omitted the fre-
quency information derived from Brown Corpus and
lexical relations (the lexical relations are encoded in
WordNet).
The construction of the lexicon is seiiii-atitomatic.
First, COMLEX and EVCA were merged, produc-
ing a list of syntactic subcate.gorizations and alter-
nations for each verb. Distinctions in these syntac-
tic restrictions according to each .sease of a verb
are achieved in the second stage, where WordNet
is merged with the result of the first step. Finally,
the corpus information is added, complementing the
static resources with actual usage counts for each
syntactic pattern. For a detailed description of the
combination process, refer to (Jung and 1&amp;quot; R
_eown,
1998).
</bodyText>
<page confidence="0.988712">
210
</page>
<figure confidence="0.953336083333333">
appear:
sense I give an impression
((PP-TO-INF-RS :PVAL (&amp;quot;to&amp;quot;) :SO ((sb, -)))
(TO-INF-RS :SO ((sb, -)))
(NP-PRED-RS :SO ((sb, -)))
(ADJP-PRED-RS :SO ((sb, -) (sth, -)))))
sense 2 become visible
((PP-TO-INF-RS :PVAL (&amp;quot;to&amp;quot;)
:SO ((sb, -) (sth, -)))
(INTRANS THERE-V-SUBJ
:ALT there-insertion
:SO ((sb, (sth, -))))
</figure>
<figureCaption confidence="0.8844054">
sense 8 have an outward expression
((NP-PRED-RS :SO ((sth, -)))
(ADJP-PRED-RS :SO ((sb, -) (sth, -))))
Figure 1: Lexicon entry for the verb appear
3.2 The benefits of the lexicon
</figureCaption>
<bodyText confidence="0.937691">
There are a number of benefits that this combined
lexicon can bring to language generation.
First, the use of synsets as semantic tags can
help map an application conceptual model to lexi-
cal items. Whenever application concepts are repre-
sented at the abstraction level of a WordNet synset,
they can be directly accepted as input to the lexi-
con. By this way, the lexicon can actually lead to
the generation of many lexical paraphrases. For ex-
ample, {look, seem, appear) is a WordNet synset; it
includes a list of words that can convey the seman-
tic concept &amp;quot;give an impression of&apos;&apos;. We can
use synsets to find words that can lexicalize the se-
mantic concepts in the semantic input. By choosing
different words in a synset. we can therefore gen-
erate lexical paraphrases. For instance, using the
above synset, the system can generate the following
paraphrases:
&amp;quot;He seems happy.&amp;quot;
&apos;lie looks happy. &amp;quot;
&amp;quot;He appears happy.
Secondly, the subcategorization information in the
lexicon prevents generating a non-grammatical out-
put_ As shown in Figure 1, the lexicon lists appli-
cable subcategorizations for each sense of a verb. It
will not allow the generation of sentences like
&amp;quot;&apos;Re convinced me in his innocence&amp;quot;
(wrong preposition)
&amp;quot;*He convinced to go to the party&amp;quot;
(missing object)
&amp;quot;*The bread cuts&amp;quot;
(missing adverb (e.g., &amp;quot;easily&amp;quot; ))
&amp;quot;*The book consists three parts
-
(missing preposition)
In addition, alternation information can help gen-
erate -syntactic paraphrases. For instance, using
the &amp;quot;simple reciprocal intransitive&amp;quot; alternation, the
system can generate the following syntactic para-
phrases:
&amp;quot;Brenda agreed with Molly.&amp;quot;
&amp;quot;Brenda and Molly agreed.&amp;quot;
&amp;quot;Brenda and Molly agreed with each other.&amp;quot;
Finally, the corpus frequency information can help
_thelexicatrehoice process,- Whearnultiple words can
be used to realize a semantic concept, the system
can use corpus frequency information in addition
to other constraints to choose the most appropriate
word.
The knowledge encoded in the lexicon is general,
thus it can be used in different applications. The
lexicon has wide coverage: the final lexicon consists
of 5,676 verbs in total, over 14,100 senses (on average
2.5 senses/verb), and over 11,000 semantic concepts
(synsets). It uses 147 patterns to represent the sub-
categorizations and includes 80 alternations.
To exploit the lexicon&apos;s many benefits, its format
must be made compatible with the architecture of a
generator. We have integrated the lexicon with the
FUF/SURGE syntactic realizer to form a combined
lexico-grammar.
</bodyText>
<sectionHeader confidence="0.999389" genericHeader="method">
4 Integration Process
</sectionHeader>
<bodyText confidence="0.987863851851852">
In this section, we first explain how lexical choosers
are interfaced with FUF/SURGE. We then describe
step by step how the lexicon is integrated with
FUF/SURGE and show that this integration pro-
cess helps to automate the development of a lexical
realization component.
4.1 FUF/SURGE and the lexical chooser
FUF (Elhadad, 1992) uses a functional unification
formalism for generation. It unifies the input, that a
user provides with a grammar to generate sentences.
SURGE (Elhadad and Robin, 1996) is a comprehen-
sive English Grammar written in FUF. The role of
a lexical realization component is to map a semantic
representation drawn from the application domain
to an input format acceptable by SURGE, adding
necessary lexical and syntactic information during
this process.
Figure 2 shows a sample semantic input (a), the
lexicalization module that. is used to map this se-
mantic input to SURGE input (h), and the final
SURGE input (c) — taken from a real application
system(Passoneau et al., 1996). The functions of the
lexicalizaticin module inchide selecting words that
can be used to realize the semantic concepts in the
input, adding syntactic features, and mapping the
arguments in the semantic-. input to the thematic
roles in SURGE.
</bodyText>
<page confidence="0.992992">
211
</page>
<table confidence="0.837965885714286">
Sentence: It has 24 activities, including 20 tasks and four decisions.
concept total -node-count concept process-flowgraph
theme
ref pronoun
concept elaboration
_
concept cardinality
args
rheme theme I theme [1] 1
args
args [ value [2] 1
concept subset-node-count -1
expansion
args
[concept flownode
ref fun
[concept cardinal
12] = cardinal 24
ref full
(a) The semantic input (i.e., input of lexicalization module)
concept #(under total-node -count) value [11 present-participle
proc [ type possessive cat pronoun lex &amp;quot;activity&amp;quot; type locative
partic possessor [ cat common cat clause lex &amp;quot;include&amp;quot;
possessed cardinal [ mood location cat
definite no proc
head [ partic
qualifier
(b) The lexicalization module
cat clause type possessive 1 value 21 present-participle
proc [ possessor [ cat pronoun lex -activity&amp;quot; type locative
partic possessed cat (70tIlmon Cat clause lex &amp;quot;include&amp;quot;
cardinal [ mood location [ Cat lop]]
definite no proc •
head panic
qualifier
</table>
<figure confidence="0.544179">
(c) The SURGE input u. output of lexicalization module
</figure>
<figureCaption confidence="0.860624">
Figure 2-A sample lexicalization component
</figureCaption>
<page confidence="0.99524">
212
</page>
<bodyText confidence="0.999989">
The development of the lexicalizer component was
done by hand in the past Furthermore, for each
new application, a new lexicalizer component had
to be written despite the fact that some lexical and
syntactic information is repeatedly used in different
applications. The integration process we describe,
however, partially automates this process.
</bodyText>
<subsectionHeader confidence="0.957911">
4.2 The integration steps
</subsectionHeader>
<bodyText confidence="0.996709033898305">
The integration of the lexicon with FUF/SURCE
is done through incremental unification, using four
unification steps as shown in Figure 3. Each step
adds information to the semantic input, and at the
end of the four unification steps, the semantic input
has been mapped to the SURGE input format.
(1) The semantic input
Different generation systems usually use different
representation formats for semantic input. Some
systems use case roles ; some systems use flat
attribute-value representation (Kukich et al., 1994).
For the integrated lexicon and FUF/SURGE pack-
age to be easily pluggable in applications, we need to
define a standard semantic input format. It should
be designed in such a way that applications can eas-
ily adapt their particular semantic inputs to this
standard format. It should also be easily mapped
to the SURGE input format.
In this paper, we only consider the issue of seman-
tic input format for the expression of the predicate-
argument relation. Two questions need to be an-
swered in the design of the standard semantic input
format: one, how to represent semantic concepts:
and two, how to represent the predicate-argument
relation.
We use WordNet synsets to represent semantic
concepts. The input can refer to synsets in several
ways: either using a globally unique synset num-
ber&apos; or by specifying a word and its sense number
in WordNet.
The representation of verb arguments is a more
complicated issue. Case roles are frequently used in
generation systems to represent verb arguments in
semantic inputs. For example, (Dorr et al., 1998)
used 20 case roles in their lexical conceptual struc-
ture corresponding to underlying positions in a com-
positional lexical structure. (Langkilde and Knight.
1998) use a list of case roles in their interlingua rep-
resentations.
We decided to use numbered arguments (similar to
the DSyntR, in !VITT (Nlercuk and Perstov, 1987))
instead of case roles. The difference between the two
&apos;Since there are a huge number of synsets in WordNet, we
will provide a searchable database of synsets so that users can
look up a synset and its index number easily_ For a particular
application, users can adapt the synsets to their specific ilta-
main, such as removi rig non. relevant Sy 1150i S. merging synsets.
and relabeling the synsets for cowvenience, as discussed in
19913).
is not critical but the numbered argument approach
avoids the need. to commit the: lexicon to a specific
ontology and seems to be easier to learn&apos;.
Figure 4 shows a sample semantic input. For easy
understanding, we refer to the semantic concepts
using their definitions rather than numerical index
numbers. There are two arguments in the input.
The intended output sentence for this semantic in-
put is &amp;quot;A boat appeared on the horizon&amp;quot; or its para-
phrases.
</bodyText>
<listItem confidence="0.941427">
(2) Lexical unification
</listItem>
<bodyText confidence="0.999334055555555">
In this step, we map the semantic concepts in the
semantic input to concrete words. To do this, we use
the synsets in WordNet, All the words in the same
synset can be used to convey the same semantic con-
cept. For the above example, the semantic concepts
&amp;quot;become visible&amp;quot; and &amp;quot;a small vessel for travel on
water&amp;quot; can be realized by the the verb appear and
the noun boat respectively. This is the step that can
produce lexical paraphrases. Note that when the
system chooses a word, it also determines the par-
ticular sense number of the word, since a word as
it belongs to a synset has a unique sense number in
WordNet.
We represented all the synsets in Wordnet in FUF
format. Each synset includes its numerical index
number and the list of word senses included in the
synsets. This lexical unification, works for both
nouns and verbs.
</bodyText>
<listItem confidence="0.986053">
(3) Structural unification
</listItem>
<bodyText confidence="0.999797714285714">
After the system has chosen a verb (actually a
particular sense of a verb), it uses that information
as an index to unify with the subcategorization and
alternations the particular verb sense has. This step
adds additional syntactic information to the origi-
nal input and has the capacity to produce syntactic
paraphrases using alternation information.
</bodyText>
<listItem confidence="0.912467">
(4) Constraints on the number of arguments
</listItem>
<bodyText confidence="0.999911928571429">
Next, we use the constraints that a subcategoriza-
tion has on the number of arguments it requires to
restrict unification with subcategorization patterns.
We use 147 possible patterns. For example, the in-
put in Figure 4 has two arguments. Although IN-
TRANS (meaning intransitive) is listed as a possi-
ble subcateg,orization pattern for &amp;quot;appear&amp;quot; (see sense
2 in Figure 1), the input will fail to unify with it
since INTRANS requires a single argument only.
This prevents the generation of non-gra.mmatical
sentences. This step adds a feature which specifies
the transitivity of the verb to FUF/SURGE input,
selecting one from the lexicon when there is more
than one possibility for the given verb.
</bodyText>
<footnote confidence="0.8680475">
2The difference between numbered arguments and labeled
roles is similar to that between named semantic primitives and
synsets ILL VordNet. Verb classes share the same definition
of which argument is denoted by 1, 2 etc. if they share some
syntactic properties as far as argument taking properties are
concerned .
</footnote>
<page confidence="0.997905">
213
</page>
<figure confidence="0.381103">
Semantic input Synsets verbs lexicon siructs input for SURGE
</figure>
<figureCaption confidence="0.997178">
Figure 3: The integration process
</figureCaption>
<figure confidence="0.7156518">
[ concept &amp;quot;become visible&apos;&apos; 1
[1 [ concept &amp;quot;a small vessel for travel on water&amp;quot; ]
2 [ concept &amp;quot;the line at which the sky and Earth appear to meet&amp;quot; ]
[relation
args
</figure>
<figureCaption confidence="0.999694">
Figure 4: The semantic input using numbered arguments
</figureCaption>
<bodyText confidence="0.998450548387097">
(5) Mapping structures to SURGE input
In the last step, the subcategorization and alter-
nations are mapped to SURGE input format. The
mapping from subcategorizations to SURGE input
was manually encoded in the lexicon for each one
of the 147 patterns. This mapping information can
be reused for all applications, which is more effi-
cient than composing SURGE input in the lexical-
ization component of each different application. Fig-
ure 5 shows how the subcategorization NP-WITH-
NP (e.g., The clown amused the children with his
antics) is mapped to the SURGE input format. This
mapping mainly involves matching the numbered ar-
guments in the semantic input to appropriate lexical
roles and syntactic categories so that EUF/SURGE
can generate them in the correct order.
The final SURGE input for the sentence &amp;quot;A boat
appeared on the horizon&amp;quot; is shown in Figure 6. Us-
ing the &amp;quot;THERE-1NSERTION&amp;quot; alternation that the
verb &amp;quot;appear&amp;quot; (sense 2) authorizes, the system can
also generate the syntactic paraphrase &amp;quot;There ap-
peared a boat on the horizon&amp;quot; . The SURGE input
the system generates for &amp;quot;There appeared a boat on
the horizon&amp;quot; is very different from that for &amp;quot;A boat
appeared on the horizon&amp;quot; .
It is possible that for a given application some
generated paraphrases are not appropriate. In this
case, users can edit the synsets and the alternations
to filter out the paraphrases they do not want
The four unification steps are completely auto-
matic. The system can send feedback upon failure
</bodyText>
<figure confidence="0.975491066666667">
struct np-with-np
relation word 111.&lt;...&gt;
1 121&lt;...&gt;
args 2 131&lt;--.&gt;
3 141&lt;...&gt;
type lexical
lex [II
[ cal rip
I 121
2 [ cat rip
2 [31
- cat pp
3 prep [ Lex &apos;with&apos; 1
Hp [4]
lex-roles
</figure>
<figureCaption confidence="0.995478">
Figure 5: Mapping suhcategorization &amp;quot;NP-WITH-
NP&amp;quot; to SURGE input
</figureCaption>
<bodyText confidence="0.944997">
of unification.
</bodyText>
<sectionHeader confidence="0.999884" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.891332888888889">
The lexicon, after it is integrated with
FUF/SURGE, can also be used for other tasks in
language generation. For example, revision (Robin,
1994) is a technique for building semantic inputs
incrementally. The revision process decides whether
it is appropriate to attach a new constituent. to the
current semantic input, for example. by adding an
proc
iabcat
</bodyText>
<page confidence="0.800344">
214
</page>
<figure confidence="0.912951">
struct PP°
r I given
argl
[ 2&apos; given
- cat clause&apos;
lexical-roles ..d
</figure>
<figureCaption confidence="0.682156">
Enriched in first step
bEnriched in second step
&apos;Enriched in third step
dEnriched in fourth step
Figure 6: SURGE input for &amp;quot;A boat appeared on the horizon&amp;quot;
</figureCaption>
<figure confidence="0.980202272727273">
I. concept &amp;quot; &amp;quot;
become visible ]
word &amp;quot;appear&amp;quot;&apos;
1
2
relation
args
concept Ca small vessel for travel on water&amp;quot;
word &amp;quot;boat&amp;quot;&apos;
concept &apos;&apos;the line at which the sky and Earth appear to meet
word &amp;quot;horizon&amp;quot;&apos;
</figure>
<bodyText confidence="0.999250578947369">
object or an adverb. Such decisions are constrained
by syntactic properties of verbs. The integrated
lexicon is useful to verify these properties.
Nitrogen (Langkilde and Knight, 1998), a natural
language generation system developed at ISI, also
includes a large-scale lexicon to support the genera-
tion process. Given that Nitrogen and FUF/SURGE
use very different methods for generation, the way
that we integrate the lexicon with the generation sys-
tem is also very different. Nitrogen combines sym-
bolic rules with statistics learned from text corpora,
while FIJF/SURGE is based on Functional Unifica-
tion Grammar. Other related work includes (Stede,
1998), which suggests a lexicon structure for multi-
lingual generation in a knowledge-based generation
system. The main idea is to handle multilingual gen-
eration in the same way as paraphrasing of the same
language. Stede&apos;s work concerns mostly the lexical
semantics of the transitivity alternations.
</bodyText>
<sectionHeader confidence="0.999377" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999883035714286">
We have presented in this paper the integration of
a large-scale, reusable lexicon for generation with
FUF/SURGE, a unification-based natural language
generator. This integration makes it possible to
reuse major parts of a lexical chooser, which is the
component in a generation system that is responsi-
ble for mapping semantic inputs to surface genera-
tor inputs. We show that although the -whole lexical-
chooser can not be made domain-independent, it is
possible to reuse a large amount of lexical, syntactic,
and semantic knowledge across applications.
In addition, the lexicon other benefits to a genera-
tion system, including the abilities to generate many
lexical paraphrases automatically, generate syntac-
tic paraphrases. avoid nlin-granimatical output. and
choose the most frequently used word when there is
more than one candidate words. Since the lexical,
syntactic, and semantic knowledge encoded in the •
lexicon is general and the lexicon has a wide cover-
age, it can be reused for different applications.
In. the future, we plan to validate the paraphrases
the lexicon can generate by asking human subjects to
read the generated paraphrases and judge whether
they are acceptable. We would like to investigate
ways that can systematically filter out paraphrases
that are considered unacceptable. We are also inter-
ested in exploring the usage of this system in multi-
lingual generation.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.977474238095238">
B. J. Dorr, N. Habash, and D. Traum. 1998.
A thematic hierarchy for efficient generation
from lexical-conceptual. Technical Report CS-
TR-3934, Institute for Advanced Computer Stud-
ies, Department of Computer Science, University
of Maryland, October.
M. Elhadad and J. Robin. 1996. An overview of
SURGE: a re-usable comprehensive syntactic re-
alization component. In liVLG&apos;96, Brighton, UK.
(demonstration session).
M. Elhadad. 1992. Using Argumentation to Control
Lexical Choice: A Functional Unification-Based
Approach. Ph.D. thesis, Department of Computer
Science, Columbia University.
R. Grishman, C. Macleod, and A. Meyers. 1994.
COMLEX syntax: Building a computational
lexicon. In Proceedings of COLIN-0&apos;94. Kyoto.
Japan.
H. Jing and K. McKeov.,11, 1998. Combining mul-
tiple, large-scale resources in a reusable lexicon
for natural language generation. In Proceedings
</reference>
<page confidence="0.987428">
215
</page>
<reference confidence="0.999763210526316">
of the 36th Annual Meeting of the Association for
Computational Linguistics and the .17th Interna-
tional Conference on Computational Linguistics,
volume 1, pages 607-613, Universite de Montréal,
Quebec, Canada, August.
H. Jing. 1998. Applying wordnet to natural lan-
guage generation. In Proceedings of COLING-
ACL&apos;98 workshop on the Usage of WordNet in
Natural Language Processing Systems, University
of Montreal, Montreal, Canada, August.
K. Kukich, K. McKeown, J. Shaw, J. Robin, N. Mor-
gan, and J. Phillips. 1994. User-needs analysis
and design methodology for an automated doc-
ument generator. In A. Zampolli, N. Calzolari,
and M. Palmer, editors, Current Issues in Com-
putational Linguistics: In Honour of Don Walker.
Kluwer Academic Press, Boston.
I. Langkilde and K. Knight. 1998. The practical
value of n-grams in generation. In INLG&apos;98, pages
248-255, Niagara-on-the-Lake, Canada, August.
13. Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University of
Chicago Press, Chicago, Illinois.
I.A. Mel&apos;cuk and N.V. Perstov, 1987. Surface-
syntax of English, a formal model in the
Meaning Text Theory. Benjarnins, Amster-
dam/Philadelphia.
G. Miller, R. Beckwith C. Fellbaum, and D. Gross IC
Miller. 1990. Introduction to WordNet: An on-
line lexical database. International Journal of
Lexicography (special issue), 3(4):235-312.
G.A. Miller, C. Leacock, R. Tengi, and R.T. Bunker.
1993. A semantic concordance. Cognitive Science
Laboratory, Princeton University.
R. Passoneau, K. Kukich, J. Robin, V. Hatzivas-
silogiou, L. Lefkowitz, and H. Jing. 1996. Gen-
erating summaries of workflow diagrams. In Pro-
ceedings of the International Conference on Nat-
ural Language Processing and Industrial Appli-
cations (NLP-IA &apos;96), Moncton, New Brunswick,
Canada.
E. Reiter. 1994. Ras a consensus n1 generation ar-
chitecture appeared, and is it psycholinguistically
plausible? In Proceedings of the Seventh Interna-
tional Workshop on Natural Language Generation
(INLGW-1994), pages 163-170, Kennebunkport,
Maine, USA. available from the cmp-Ig archive as
paper cmp-lg/9411032.
J. Robin. 1994. Revision-Based Generation of Nat-
ural Language Summaries Providing Historical
Background: Corpus-Based Analysis, Design, Im-
plementation, and Evaluation. Ph.D. thesis, De-
partment of Computer Science, Columbia Univer-
sity. Also Technical Report CU-CS-03-1-94.
Steck. 1998. A generative perspective on verb al-
ternations. Computational Linguiqics. 24( 3),-101
430. September.
</reference>
<page confidence="0.999145">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.617775">
<title confidence="0.9988055">Integrating a Large-scale, Reusable Lexicon with a Natural Language Generator</title>
<author confidence="0.953005">Hongyan</author>
<affiliation confidence="0.999704">Department of Computer</affiliation>
<address confidence="0.8969535">Columbia New York, NY 10027,</address>
<email confidence="0.999461">hjing@cs.columbia.edu</email>
<author confidence="0.985444">Michael</author>
<affiliation confidence="0.9757765">Department of Computer Ben-Gurion</affiliation>
<address confidence="0.997193">Be&apos;er-Sheva, 84105,</address>
<email confidence="0.998668">elhadadacs.bgu.ac.il</email>
<abstract confidence="0.990519357142857">This paper presents the integration of a largescale, reusable lexicon for generation with the FUF/SURGE unification-based syntactic realizer. The lexicon was combined from multiple existing resources in a semi-automatic process. The integration is a multi-step unification process. This integration allows the reuse of lexical, syntactic, and semantic knowledge encoded in the lexicon in the development of lexical chooser module in a generation system. The lexicon also brings other benefits to a generation system: for example, the ability to generate many lexical and syntactic paraphrases and the ability to avoid non-grammatical output.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B J Dorr</author>
<author>N Habash</author>
<author>D Traum</author>
</authors>
<title>A thematic hierarchy for efficient generation from lexical-conceptual.</title>
<date>1998</date>
<tech>Technical Report CSTR-3934,</tech>
<institution>Institute for Advanced Computer Studies, Department of Computer Science, University of Maryland,</institution>
<contexts>
<context position="15772" citStr="Dorr et al., 1998" startWordPosition="2427" endWordPosition="2430">edicateargument relation. Two questions need to be answered in the design of the standard semantic input format: one, how to represent semantic concepts: and two, how to represent the predicate-argument relation. We use WordNet synsets to represent semantic concepts. The input can refer to synsets in several ways: either using a globally unique synset number&apos; or by specifying a word and its sense number in WordNet. The representation of verb arguments is a more complicated issue. Case roles are frequently used in generation systems to represent verb arguments in semantic inputs. For example, (Dorr et al., 1998) used 20 case roles in their lexical conceptual structure corresponding to underlying positions in a compositional lexical structure. (Langkilde and Knight. 1998) use a list of case roles in their interlingua representations. We decided to use numbered arguments (similar to the DSyntR, in !VITT (Nlercuk and Perstov, 1987)) instead of case roles. The difference between the two &apos;Since there are a huge number of synsets in WordNet, we will provide a searchable database of synsets so that users can look up a synset and its index number easily_ For a particular application, users can adapt the syns</context>
</contexts>
<marker>Dorr, Habash, Traum, 1998</marker>
<rawString>B. J. Dorr, N. Habash, and D. Traum. 1998. A thematic hierarchy for efficient generation from lexical-conceptual. Technical Report CSTR-3934, Institute for Advanced Computer Studies, Department of Computer Science, University of Maryland, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
<author>J Robin</author>
</authors>
<title>An overview of SURGE: a re-usable comprehensive syntactic realization component.</title>
<date>1996</date>
<booktitle>In liVLG&apos;96,</booktitle>
<location>Brighton, UK.</location>
<note>(demonstration session).</note>
<contexts>
<context position="12031" citStr="Elhadad and Robin, 1996" startWordPosition="1835" endWordPosition="1838">. We have integrated the lexicon with the FUF/SURGE syntactic realizer to form a combined lexico-grammar. 4 Integration Process In this section, we first explain how lexical choosers are interfaced with FUF/SURGE. We then describe step by step how the lexicon is integrated with FUF/SURGE and show that this integration process helps to automate the development of a lexical realization component. 4.1 FUF/SURGE and the lexical chooser FUF (Elhadad, 1992) uses a functional unification formalism for generation. It unifies the input, that a user provides with a grammar to generate sentences. SURGE (Elhadad and Robin, 1996) is a comprehensive English Grammar written in FUF. The role of a lexical realization component is to map a semantic representation drawn from the application domain to an input format acceptable by SURGE, adding necessary lexical and syntactic information during this process. Figure 2 shows a sample semantic input (a), the lexicalization module that. is used to map this semantic input to SURGE input (h), and the final SURGE input (c) — taken from a real application system(Passoneau et al., 1996). The functions of the lexicalizaticin module inchide selecting words that can be used to realize t</context>
</contexts>
<marker>Elhadad, Robin, 1996</marker>
<rawString>M. Elhadad and J. Robin. 1996. An overview of SURGE: a re-usable comprehensive syntactic realization component. In liVLG&apos;96, Brighton, UK. (demonstration session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
</authors>
<title>Using Argumentation to Control Lexical Choice: A Functional Unification-Based Approach.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Columbia University.</institution>
<contexts>
<context position="1319" citStr="Elhadad, 1992" startWordPosition="184" endWordPosition="185">development of lexical chooser module in a generation system. The lexicon also brings other benefits to a generation system: for example, the ability to generate many lexical and syntactic paraphrases and the ability to avoid non-grammatical output. 1 Introduction Natural language generation requires lexical, syntactic, and semantic knowledge in order to produce meaningful and fluent output. Such knowledge is often hand-coded anew when a different application is developed. We present in this paper the integration of a large-scale, reusable lexicon with a natural language generator, FUF/SURGE (Elhadad, 1992; Robin, 1994): we show that by integrating the lexicon with FUF/SURGE as a tactical component, we can reuse the knowledge encoded in the lexicon and automate to some extent the development of the lexical realization component in a generation application_ The integration of the lexicon with FIT/SURGE also brings other benefits to generation, including the possibility to accept a semantic input at the level of WordNet synsets, the production of lexical and syntactic paraphrases, the prevention of nongrammatical output. reuse across applications, and wide coverage. We present the process of inte</context>
<context position="11862" citStr="Elhadad, 1992" startWordPosition="1811" endWordPosition="1812">ubcategorizations and includes 80 alternations. To exploit the lexicon&apos;s many benefits, its format must be made compatible with the architecture of a generator. We have integrated the lexicon with the FUF/SURGE syntactic realizer to form a combined lexico-grammar. 4 Integration Process In this section, we first explain how lexical choosers are interfaced with FUF/SURGE. We then describe step by step how the lexicon is integrated with FUF/SURGE and show that this integration process helps to automate the development of a lexical realization component. 4.1 FUF/SURGE and the lexical chooser FUF (Elhadad, 1992) uses a functional unification formalism for generation. It unifies the input, that a user provides with a grammar to generate sentences. SURGE (Elhadad and Robin, 1996) is a comprehensive English Grammar written in FUF. The role of a lexical realization component is to map a semantic representation drawn from the application domain to an input format acceptable by SURGE, adding necessary lexical and syntactic information during this process. Figure 2 shows a sample semantic input (a), the lexicalization module that. is used to map this semantic input to SURGE input (h), and the final SURGE in</context>
</contexts>
<marker>Elhadad, 1992</marker>
<rawString>M. Elhadad. 1992. Using Argumentation to Control Lexical Choice: A Functional Unification-Based Approach. Ph.D. thesis, Department of Computer Science, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>C Macleod</author>
<author>A Meyers</author>
</authors>
<title>COMLEX syntax: Building a computational lexicon.</title>
<date>1994</date>
<booktitle>In Proceedings of COLIN-0&apos;94. Kyoto.</booktitle>
<contexts>
<context position="6404" citStr="Grishman et al., 1994" startWordPosition="950" endWordPosition="953"> 120,000 unique words (version 1.6). It also encodes many types of lexical relations between words, including synonymy, antonymy, and many more. • English Verb Classes and Alternations (EVCA) (Levin, 1993), It categorized 3.104 verbs into classes based on their syntactic properties and studied verb alternations. An alternation is a variation in the realization of verb arguments. For example, the alternation &amp;quot;there-insertion&amp;quot; transforms A ship appeared on the horizonzto There,appeared ship. on.the horizon. A total of 80 alternations for 3,104 verbs were studied. • The COMLEX syntax dictionary (Grishman et al., 1994). COMLEX contains syntactic information for over 38,000 English words. * The Brown Corpus tagged with WordNet senses (Miller et al., 1993). We use this corpus for frequency measurement. In combining these resources, we focused on verbs, since they play a more important role in deciding sentence structures. The combined lexicon includes rich lexical and syntactic knowledge for 5,676 verbs. It is indexed by WordNet synsets(which are at the semantic concept level) as required by the generation task. The knowledge in the lexicon includes: O A complete list of subcategorizations for each sense of a</context>
</contexts>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>R. Grishman, C. Macleod, and A. Meyers. 1994. COMLEX syntax: Building a computational lexicon. In Proceedings of COLIN-0&apos;94. Kyoto. Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>K McKeov</author>
</authors>
<title>Combining multiple, large-scale resources in a reusable lexicon for natural language generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the .17th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>607--613</pages>
<institution>Universite de Montréal,</institution>
<location>Quebec, Canada,</location>
<marker>Jing, McKeov, 1998</marker>
<rawString>H. Jing and K. McKeov.,11, 1998. Combining multiple, large-scale resources in a reusable lexicon for natural language generation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the .17th International Conference on Computational Linguistics, volume 1, pages 607-613, Universite de Montréal, Quebec, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
</authors>
<title>Applying wordnet to natural language generation.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL&apos;98 workshop on the Usage of WordNet in Natural Language Processing Systems,</booktitle>
<institution>University of Montreal,</institution>
<location>Montreal, Canada,</location>
<marker>Jing, 1998</marker>
<rawString>H. Jing. 1998. Applying wordnet to natural language generation. In Proceedings of COLINGACL&apos;98 workshop on the Usage of WordNet in Natural Language Processing Systems, University of Montreal, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
<author>K McKeown</author>
<author>J Shaw</author>
<author>J Robin</author>
<author>N Morgan</author>
<author>J Phillips</author>
</authors>
<title>User-needs analysis and design methodology for an automated document generator.</title>
<date>1994</date>
<booktitle>Current Issues in Computational Linguistics: In Honour of Don Walker.</booktitle>
<editor>In A. Zampolli, N. Calzolari, and M. Palmer, editors,</editor>
<publisher>Kluwer Academic Press,</publisher>
<location>Boston.</location>
<contexts>
<context position="14728" citStr="Kukich et al., 1994" startWordPosition="2253" endWordPosition="2256"> integration process we describe, however, partially automates this process. 4.2 The integration steps The integration of the lexicon with FUF/SURCE is done through incremental unification, using four unification steps as shown in Figure 3. Each step adds information to the semantic input, and at the end of the four unification steps, the semantic input has been mapped to the SURGE input format. (1) The semantic input Different generation systems usually use different representation formats for semantic input. Some systems use case roles ; some systems use flat attribute-value representation (Kukich et al., 1994). For the integrated lexicon and FUF/SURGE package to be easily pluggable in applications, we need to define a standard semantic input format. It should be designed in such a way that applications can easily adapt their particular semantic inputs to this standard format. It should also be easily mapped to the SURGE input format. In this paper, we only consider the issue of semantic input format for the expression of the predicateargument relation. Two questions need to be answered in the design of the standard semantic input format: one, how to represent semantic concepts: and two, how to repr</context>
</contexts>
<marker>Kukich, McKeown, Shaw, Robin, Morgan, Phillips, 1994</marker>
<rawString>K. Kukich, K. McKeown, J. Shaw, J. Robin, N. Morgan, and J. Phillips. 1994. User-needs analysis and design methodology for an automated document generator. In A. Zampolli, N. Calzolari, and M. Palmer, editors, Current Issues in Computational Linguistics: In Honour of Don Walker. Kluwer Academic Press, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>The practical value of n-grams in generation.</title>
<date>1998</date>
<booktitle>In INLG&apos;98,</booktitle>
<pages>248--255</pages>
<location>Niagara-on-the-Lake, Canada,</location>
<contexts>
<context position="22317" citStr="Langkilde and Knight, 1998" startWordPosition="3529" endWordPosition="3532">n proc iabcat 214 struct PP° r I given argl [ 2&apos; given - cat clause&apos; lexical-roles ..d Enriched in first step bEnriched in second step &apos;Enriched in third step dEnriched in fourth step Figure 6: SURGE input for &amp;quot;A boat appeared on the horizon&amp;quot; I. concept &amp;quot; &amp;quot; become visible ] word &amp;quot;appear&amp;quot;&apos; 1 2 relation args concept Ca small vessel for travel on water&amp;quot; word &amp;quot;boat&amp;quot;&apos; concept &apos;&apos;the line at which the sky and Earth appear to meet word &amp;quot;horizon&amp;quot;&apos; object or an adverb. Such decisions are constrained by syntactic properties of verbs. The integrated lexicon is useful to verify these properties. Nitrogen (Langkilde and Knight, 1998), a natural language generation system developed at ISI, also includes a large-scale lexicon to support the generation process. Given that Nitrogen and FUF/SURGE use very different methods for generation, the way that we integrate the lexicon with the generation system is also very different. Nitrogen combines symbolic rules with statistics learned from text corpora, while FIJF/SURGE is based on Functional Unification Grammar. Other related work includes (Stede, 1998), which suggests a lexicon structure for multilingual generation in a knowledge-based generation system. The main idea is to han</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998. The practical value of n-grams in generation. In INLG&apos;98, pages 248-255, Niagara-on-the-Lake, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, Illinois.</location>
<contexts>
<context position="5987" citStr="Levin, 1993" startWordPosition="891" endWordPosition="892">nt representation formats used by these resources make it impossible to use them simultaneously in a single system. To overcome these limitations, we built a largescale, reusable lexicon for generation by combining multiple existing resources. The resources that are combined include: O The WordNet, Lexical Database (Miller et al., 1990). WordNet is the largest lexical database to date, consisting of over 120,000 unique words (version 1.6). It also encodes many types of lexical relations between words, including synonymy, antonymy, and many more. • English Verb Classes and Alternations (EVCA) (Levin, 1993), It categorized 3.104 verbs into classes based on their syntactic properties and studied verb alternations. An alternation is a variation in the realization of verb arguments. For example, the alternation &amp;quot;there-insertion&amp;quot; transforms A ship appeared on the horizonzto There,appeared ship. on.the horizon. A total of 80 alternations for 3,104 verbs were studied. • The COMLEX syntax dictionary (Grishman et al., 1994). COMLEX contains syntactic information for over 38,000 English words. * The Brown Corpus tagged with WordNet senses (Miller et al., 1993). We use this corpus for frequency measuremen</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>13. Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel&apos;cuk</author>
<author>N V Perstov</author>
</authors>
<title>Surfacesyntax of English, a formal model in the Meaning Text Theory.</title>
<date>1987</date>
<location>Benjarnins, Amsterdam/Philadelphia.</location>
<marker>Mel&apos;cuk, Perstov, 1987</marker>
<rawString>I.A. Mel&apos;cuk and N.V. Perstov, 1987. Surfacesyntax of English, a formal model in the Meaning Text Theory. Benjarnins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith C Fellbaum</author>
<author>D Gross IC Miller</author>
</authors>
<title>Introduction to WordNet: An online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography (special issue),</journal>
<pages>3--4</pages>
<contexts>
<context position="5713" citStr="Miller et al., 1990" startWordPosition="847" endWordPosition="850">g lexical resources, however, are indexed by words rather than by semantic concepts. Such resources, therefore, can not be used for generation directly. Moreover, generation needs different types of knowledge, which typically are encoded in different resources. However, the different representation formats used by these resources make it impossible to use them simultaneously in a single system. To overcome these limitations, we built a largescale, reusable lexicon for generation by combining multiple existing resources. The resources that are combined include: O The WordNet, Lexical Database (Miller et al., 1990). WordNet is the largest lexical database to date, consisting of over 120,000 unique words (version 1.6). It also encodes many types of lexical relations between words, including synonymy, antonymy, and many more. • English Verb Classes and Alternations (EVCA) (Levin, 1993), It categorized 3.104 verbs into classes based on their syntactic properties and studied verb alternations. An alternation is a variation in the realization of verb arguments. For example, the alternation &amp;quot;there-insertion&amp;quot; transforms A ship appeared on the horizonzto There,appeared ship. on.the horizon. A total of 80 altern</context>
</contexts>
<marker>Miller, Fellbaum, Miller, 1990</marker>
<rawString>G. Miller, R. Beckwith C. Fellbaum, and D. Gross IC Miller. 1990. Introduction to WordNet: An online lexical database. International Journal of Lexicography (special issue), 3(4):235-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>C Leacock</author>
<author>R Tengi</author>
<author>R T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<institution>Cognitive Science Laboratory, Princeton University.</institution>
<contexts>
<context position="6542" citStr="Miller et al., 1993" startWordPosition="972" endWordPosition="975">ore. • English Verb Classes and Alternations (EVCA) (Levin, 1993), It categorized 3.104 verbs into classes based on their syntactic properties and studied verb alternations. An alternation is a variation in the realization of verb arguments. For example, the alternation &amp;quot;there-insertion&amp;quot; transforms A ship appeared on the horizonzto There,appeared ship. on.the horizon. A total of 80 alternations for 3,104 verbs were studied. • The COMLEX syntax dictionary (Grishman et al., 1994). COMLEX contains syntactic information for over 38,000 English words. * The Brown Corpus tagged with WordNet senses (Miller et al., 1993). We use this corpus for frequency measurement. In combining these resources, we focused on verbs, since they play a more important role in deciding sentence structures. The combined lexicon includes rich lexical and syntactic knowledge for 5,676 verbs. It is indexed by WordNet synsets(which are at the semantic concept level) as required by the generation task. The knowledge in the lexicon includes: O A complete list of subcategorizations for each sense of a verb. • A large variety of alternations for each sense of a verb. • Frequency of lexical items and verb subeategorizations in the tagged </context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>G.A. Miller, C. Leacock, R. Tengi, and R.T. Bunker. 1993. A semantic concordance. Cognitive Science Laboratory, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passoneau</author>
<author>K Kukich</author>
<author>J Robin</author>
<author>V Hatzivassilogiou</author>
<author>L Lefkowitz</author>
<author>H Jing</author>
</authors>
<title>Generating summaries of workflow diagrams.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Natural Language Processing and Industrial Applications (NLP-IA &apos;96),</booktitle>
<location>Moncton, New Brunswick, Canada.</location>
<contexts>
<context position="12532" citStr="Passoneau et al., 1996" startWordPosition="1918" endWordPosition="1921">neration. It unifies the input, that a user provides with a grammar to generate sentences. SURGE (Elhadad and Robin, 1996) is a comprehensive English Grammar written in FUF. The role of a lexical realization component is to map a semantic representation drawn from the application domain to an input format acceptable by SURGE, adding necessary lexical and syntactic information during this process. Figure 2 shows a sample semantic input (a), the lexicalization module that. is used to map this semantic input to SURGE input (h), and the final SURGE input (c) — taken from a real application system(Passoneau et al., 1996). The functions of the lexicalizaticin module inchide selecting words that can be used to realize the semantic concepts in the input, adding syntactic features, and mapping the arguments in the semantic-. input to the thematic roles in SURGE. 211 Sentence: It has 24 activities, including 20 tasks and four decisions. concept total -node-count concept process-flowgraph theme ref pronoun concept elaboration _ concept cardinality args rheme theme I theme [1] 1 args args [ value [2] 1 concept subset-node-count -1 expansion args [concept flownode ref fun [concept cardinal 12] = cardinal 24 ref full </context>
</contexts>
<marker>Passoneau, Kukich, Robin, Hatzivassilogiou, Lefkowitz, Jing, 1996</marker>
<rawString>R. Passoneau, K. Kukich, J. Robin, V. Hatzivassilogiou, L. Lefkowitz, and H. Jing. 1996. Generating summaries of workflow diagrams. In Proceedings of the International Conference on Natural Language Processing and Industrial Applications (NLP-IA &apos;96), Moncton, New Brunswick, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
</authors>
<title>Ras a consensus n1 generation architecture appeared, and is it psycholinguistically plausible?</title>
<date>1994</date>
<booktitle>In Proceedings of the Seventh International Workshop on Natural Language Generation (INLGW-1994),</booktitle>
<pages>163--170</pages>
<location>Kennebunkport, Maine, USA.</location>
<contexts>
<context position="3954" citStr="Reiter, 1994" startWordPosition="583" endWordPosition="584">oser has not been developed in the past: 1. In the overall architecture of a generator, the lexical chooser is an internal component that depends on the semantic representation and for.:rnaliarn and on the syntactic realizer used by the application. 2. The lexical chooser links conceptual elements to lexical items. Conceptual elements are bv definition domain and application dependent (they are the primitive concepts used in an application knowledge base). These primitives are not easily ported from application to application. 209 The emergence of standard architectures for generators (RAGS, (Reiter, 1994)) and the possibility to use a standard syntactic realizer answer the first issue. To address the second issue, one must realize that if the whole lexical chooser can not be made domainindependent, major parts can be made reusable. The main argument is that lexical knowledge is modular. Therefore, while choice of words is constrained by domain-specific conceptual knowledge (what information the sentences are to represent) on the one hand, it is also affected by several other dimensions: 6 inter-lexical constraints: collocations among words 0 pragmatic constraints: connotations of words 6 styli</context>
</contexts>
<marker>Reiter, 1994</marker>
<rawString>E. Reiter. 1994. Ras a consensus n1 generation architecture appeared, and is it psycholinguistically plausible? In Proceedings of the Seventh International Workshop on Natural Language Generation (INLGW-1994), pages 163-170, Kennebunkport, Maine, USA. available from the cmp-Ig archive as paper cmp-lg/9411032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
</authors>
<title>Revision-Based Generation of Natural Language Summaries Providing Historical Background: Corpus-Based Analysis, Design, Implementation, and Evaluation.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Columbia University. Also</institution>
<contexts>
<context position="1333" citStr="Robin, 1994" startWordPosition="186" endWordPosition="187">lexical chooser module in a generation system. The lexicon also brings other benefits to a generation system: for example, the ability to generate many lexical and syntactic paraphrases and the ability to avoid non-grammatical output. 1 Introduction Natural language generation requires lexical, syntactic, and semantic knowledge in order to produce meaningful and fluent output. Such knowledge is often hand-coded anew when a different application is developed. We present in this paper the integration of a large-scale, reusable lexicon with a natural language generator, FUF/SURGE (Elhadad, 1992; Robin, 1994): we show that by integrating the lexicon with FUF/SURGE as a tactical component, we can reuse the knowledge encoded in the lexicon and automate to some extent the development of the lexical realization component in a generation application_ The integration of the lexicon with FIT/SURGE also brings other benefits to generation, including the possibility to accept a semantic input at the level of WordNet synsets, the production of lexical and syntactic paraphrases, the prevention of nongrammatical output. reuse across applications, and wide coverage. We present the process of integrating the le</context>
<context position="21491" citStr="Robin, 1994" startWordPosition="3392" endWordPosition="3393"> users can edit the synsets and the alternations to filter out the paraphrases they do not want The four unification steps are completely automatic. The system can send feedback upon failure struct np-with-np relation word 111.&lt;...&gt; 1 121&lt;...&gt; args 2 131&lt;--.&gt; 3 141&lt;...&gt; type lexical lex [II [ cal rip I 121 2 [ cat rip 2 [31 - cat pp 3 prep [ Lex &apos;with&apos; 1 Hp [4] lex-roles Figure 5: Mapping suhcategorization &amp;quot;NP-WITHNP&amp;quot; to SURGE input of unification. 5 Related Work The lexicon, after it is integrated with FUF/SURGE, can also be used for other tasks in language generation. For example, revision (Robin, 1994) is a technique for building semantic inputs incrementally. The revision process decides whether it is appropriate to attach a new constituent. to the current semantic input, for example. by adding an proc iabcat 214 struct PP° r I given argl [ 2&apos; given - cat clause&apos; lexical-roles ..d Enriched in first step bEnriched in second step &apos;Enriched in third step dEnriched in fourth step Figure 6: SURGE input for &amp;quot;A boat appeared on the horizon&amp;quot; I. concept &amp;quot; &amp;quot; become visible ] word &amp;quot;appear&amp;quot;&apos; 1 2 relation args concept Ca small vessel for travel on water&amp;quot; word &amp;quot;boat&amp;quot;&apos; concept &apos;&apos;the line at which the sky</context>
</contexts>
<marker>Robin, 1994</marker>
<rawString>J. Robin. 1994. Revision-Based Generation of Natural Language Summaries Providing Historical Background: Corpus-Based Analysis, Design, Implementation, and Evaluation. Ph.D. thesis, Department of Computer Science, Columbia University. Also Technical Report CU-CS-03-1-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steck</author>
</authors>
<title>A generative perspective on verb alternations.</title>
<date>1998</date>
<journal>Computational Linguiqics.</journal>
<volume>24</volume>
<pages>3--101</pages>
<marker>Steck, 1998</marker>
<rawString>Steck. 1998. A generative perspective on verb alternations. Computational Linguiqics. 24( 3),-101 430. September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>