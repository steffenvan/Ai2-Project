<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000710">
<title confidence="0.99832">
Aspect-Level Cross-lingual Sentiment Classification
with Constrained SMT
</title>
<author confidence="0.914433">
Patrik Lambert
</author>
<affiliation confidence="0.872594">
Universitat Pompeu Fabra, Barcelona, Spain
</affiliation>
<email confidence="0.992429">
patrik.lambert@upf.edu
</email>
<sectionHeader confidence="0.993763" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926">
Most cross-lingual sentiment classifica-
tion (CLSC) research so far has been per-
formed at sentence or document level.
Aspect-level CLSC, which is more appro-
priate for many applications, presents the
additional difficulty that we consider sub-
sentential opinionated units which have to
be mapped across languages. In this pa-
per, we extend the possible cross-lingual
sentiment analysis settings to aspect-level
specific use cases. We propose a method,
based on constrained SMT, to transfer
opinionated units across languages by pre-
serving their boundaries. We show that
cross-language sentiment classifiers built
with this method achieve comparable re-
sults to monolingual ones, and we com-
pare different cross-lingual settings.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980596">
Sentiment analysis (SA) is the task of analysing
opinions, sentiments or emotions expressed to-
wards entities such as products, services, organi-
sations, issues, and the various attributes of these
entities (Liu, 2012). The analysis may be per-
formed at the level of a document (blog post, re-
view) or sentence. However, this is not appropriate
for many applications because the same document
or sentence can contain positive opinions towards
specific aspects and negative ones towards other
aspects. Thus a finer analysis can be conducted
at the level of the aspects of the entities towards
which opinions are expressed, identifying for each
opinionated unit elements such as its target, polar-
ity and the polar words used to qualify the target.
The two main SA approaches presented in the
literature are (i) a machine learning approach,
mostly supervised learning with features such as
opinion words, dependency information, opinion
shifters and quantifiers and (ii) a lexicon-based ap-
proach, based on rules involving opinion words
and phrases, opinion shifters, contrary clauses
(but), etc. Thus in most SA systems we may dis-
tinguish three types of resources and text:
TRAIN Resources (collection of training exam-
ples, lexicons) used to train the classifier.
TEST Opinions to be analysed.
OUT Outcome of the analysis. It depends on the
level of granularity. At the document or sentence
level, it is the polarity of each document or sen-
tence. At the aspect level, it may the set of opinion
targets with their polarity.
The internet multilingualism and the globalisa-
tion of products and services create situations in
which these three types of resources are not all
in the same language. In these situations, a lan-
guage transfer is needed at some point to perform
the SA analysis or to understand its results, thus
called cross-lingual sentiment analysis (CLSA).
Sentences or documents are handy granularity
levels for CLSA because the labels are not related
to specific tokens and thus are not affected by a
language transfer. At the aspect level, labels are
attached to a specific opinionated unit formed by
a sequence of tokens. When transferring these an-
notations into another language, the opinionated
units in the two languages have thus to be mapped.
This paper is one of the first ones to address
CLSA at aspect level (see Section 3). It makes
the following specific contributions:
</bodyText>
<listItem confidence="0.740781444444444">
(i) an extended definition of CLSA including
use cases and settings specific to aspect-level
analyses (Section 2);
(ii) a method to perform the language transfer
preserving the opinionated unit boundaries.
This avoids the need of mapping source and
target opinionated units after the language
transfer via methods such as word alignment
(Section 4);
</listItem>
<page confidence="0.885287">
781
</page>
<bodyText confidence="0.848076333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 781–787,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
The paper also reports (in Section 5) experiments
comparing different settings described in Sec-
tion 2.
sult of the analysis in a different language. Here,
the inequality of Eq. 2 applies, yielding CLSA set-
tings c and d as follows (see also Figure 2).
</bodyText>
<sectionHeader confidence="0.5902" genericHeader="introduction">
2 Use Cases and Settings
</sectionHeader>
<bodyText confidence="0.9996384">
We can think of the following use cases for CLSA:
Use case I. There are opinions we want to ana-
lyse, but we do not avail of a SA system to perform
this analysis. We thus want to predict the polarity
of opinions expressed in a language LTEST us-
ing a classifier in another language LTRAIN. We
can assume that the language LOUT of the analysis
outcome1 is the same as the one of the opinions. In
this case, equation 1 applies, yielding CLSA set-
tings a and b as follows (see also Figure 1).
</bodyText>
<equation confidence="0.720671">
LTRAIN =� LTEST; LOUT = LTEST (1)
</equation>
<bodyText confidence="0.630474">
(a) available training resources are transferred
into the test language to build a classifier in the
test language.
(b) we translate the test into the language of the
classifier, classify the opinions in the test, and then
transfer back the analysis outcome into the source
language by projecting the labels or/and opinion-
ated units onto the test set.
</bodyText>
<equation confidence="0.9881755">
TAIN T
R TRAINL0
TEST
TEST OUT 0
LTEST
Proj
T T
SALT RAIN
TEST0 OUTLTRAIN
LTRAIN
</equation>
<bodyText confidence="0.898070571428571">
Figure 1: Use case I settings. SA refers to Senti-
ment Analisys, T to Translation, Proj to Projec-
tion and Learn to Learning, and the prime sym-
bol designs a language into which a set has been
automatically translated.
Use case II. We may have training resources in
the language of the opinions, but we need the re-
</bodyText>
<footnote confidence="0.866347">
1As mentioned above, at the aspect level, the outcome of
the analysis may be a set of opinion targets with their polar-
ity. It may also be more complex, such as a set of opinion
expressions with their respective target, polarity, holder and
time (Liu, 2012). The outcome may need to be in another lan-
guage as the opinions themselves. For example, a company
based in China may survey the opinions of their Spanish-
speaking customers, and then transfer the SA outcome into
Chinese so that their marketing department can understand it.
</footnote>
<equation confidence="0.85323">
LOUT =� LTEST (2)
</equation>
<listItem confidence="0.988115166666667">
(d) LTRAIN = LTEST; the test opinions are
first analysed in their language, then the analysis
outcome is transferred into the desired language.
(e) LTRAIN = LOUT; the test set is first trans-
ferred into the desired outcome language, and the
SA is performed in this language.
</listItem>
<figure confidence="0.9433558">
SALTEST T
(c) TEST OUTLTEST OUTL0
OUT
(d) TEST T TEST 0 SAL OUT OUT
LOUT
</figure>
<figureCaption confidence="0.998756">
Figure 2: Use case II settings.
</figureCaption>
<bodyText confidence="0.983218375">
Use case II only makes sense for aspect-level
analysis,2 and to our knowledge, it was not ad-
dressed in the literature so far.
Use case III. We want to benefit from data
available in several languages, either to have more
examples and improve the classifier accuracy, or to
have a broader view of the opinions under study.
In this paper we focus on use cases I and II.
</bodyText>
<sectionHeader confidence="0.999885" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999918222222222">
The main CLSC approaches described in the liter-
ature are via lexicon transfer, via corpus transfer,
via test translation and via joint classification.
In the lexicon transfer approach, a source senti-
ment lexicon is transferred into the target language
and a lexicon-based classifier is build in the tar-
get language. Approaches to transfer lexica in-
clude machine translation (MT) (Mihalcea et al.,
2007), Wordnet (Banea et al., 2011; Hassan et al.,
2011; Perez-Rosas et al., 2012), relations between
dictionaries represented in graphs (Scheible et al.,
2010), or triangulation (Steinberger et al., 2012).
The corpus transfer approach consists of trans-
ferring a source training corpus into the target lan-
guage and building a corpus-based classifier in the
target language. Banea et al. (2008) follow this
approach, translating an annotated corpus via MT.
Balamurali et al. (2012) use linked Wordnets to
</bodyText>
<footnote confidence="0.9854565">
2For document and sentence-level classification, the out-
come is a set of polarity labels independent on language.
</footnote>
<figure confidence="0.96302075">
Learn
SA
TEST L0
TEST OUTLTEST
</figure>
<page confidence="0.983452">
782
</page>
<bodyText confidence="0.999314487804878">
replace words in training and test corpora by their
(language-independent) synset identifiers. Gui et
al. (2014) reduce negative transfer in the process
of transfer learning. Popat et al. (2013) perform
CLSA with clusters as features, bridging target
and source language clusters with word alignment.
In the test translation approach, test sentences
from the target language are translated into the
source language and they are classified using a
source language classifier (Bautin et al., 2008).
Work on joint classification includes train-
ing a classifier with features from multilingual
views (Banea et al., 2010; Xiao and Guo, 2012),
co-training (Wan, 2009; Demirtas and Pech-
enizkiy, 2013), joint learning (Lu et al., 2011),
structural correspondence learning (Wei and Pal,
2010; Prettenhofer and Stein, 2010) or mixture
models (Meng et al., 2012). Gui et al. (2013) com-
pare several of these approaches.
Brooke et al. (2009) and Balamurali et al.
(2013) conclude that at document level, it is
cheaper to annotate resources in the target lan-
guage than building CLSA systems. This may
not be true at aspect level, in which the annota-
tion cost is much higher. In any case, when the
skills to build such annotated resources are lack-
ing, CLSA may be the only option. In language
pairs in which no high-quality MT systems are
available, MT may not be an appropriate trans-
fer method (Popat et al., 2013; Balamurali et al.,
2012). However, Balahur and Turchi (2014) con-
clude that MT systems can be used to build senti-
ment analysis systems that can obtain comparable
performances to the one obtained for English.
All this work was performed at sentence or doc-
ument level. Zhou et al. (2012) and Lin et al.
(2014) work at the aspect level, but they focus on
cross-lingual aspect extraction. Haas and Versley
(2015) use CLSA for individual syntactic nodes,
however they need to map target-language and
source-language nodes with word alignment.
</bodyText>
<sectionHeader confidence="0.941152" genericHeader="method">
4 Language Transfer
</sectionHeader>
<bodyText confidence="0.999995166666667">
In aspect-level SA, there may be several opinion-
ated segments in each sentence. When perform-
ing a language transfer, each segment in the target
language has to be mapped to its corresponding
segment in the source language. This may not be
an obvious task at all. For example, if a standard
MT system is used for language translation, the
source opinionated segment may be reordered and
split in several parts in the target language. Then
the different parts have to be mapped to the orig-
inal segment with a method such as word align-
ment, which may introduce errors and may leave
some parts without a corresponding segment in
the source language. To avoid these problems, we
could translate only the opinionated segments, in-
dependently of each other. However, the context
of these segments, which may be useful for some
applications, would then be lost. Furthermore, the
translation quality would be worse than when the
segments are translated within the whole sentence
context.
To solve these problems, we translate the whole
sentences but with reordering constraints ensur-
ing that the opinionated segments are preserved
during translation. That is, the text between the
relevant segment boundaries is not reordered nor
mixed with the text outside these boundaries.3
Thus the text in the target language segment comes
only from the corresponding source language seg-
ment. We use the Moses statistical MT (SMT)
toolkit (Koehn et al., 2007) to perform the trans-
lation. In Moses, these reordering constraints are
implemented with the zone and wall tags, as in-
dicated in Figure 3. Moses also allows mark-up
to be directly passed to the translation, via the x
tag. We use this functionality to keep track, via the
tags &lt;ou[id][-label]&gt; and &lt;/ou[id]&gt;, of
the segment boundaries (ou stands for Opinion-
ated Unit), of the opinionated segment identifier
([id]) and, for training and evaluation purposes,
of the polarity label ([-label]). In the example
of Figure 3, the id is 1 and the label is P.
</bodyText>
<sectionHeader confidence="0.997028" genericHeader="method">
5 CLSA experiments
</sectionHeader>
<bodyText confidence="0.9997566">
In order to compare CLSA settings a and b (of use
case I), we needed data with opinion annotations at
the aspect level, in two different languages and in
the same domain. We used the OpeNER4 opinion
corpus,5 and more specifically the opinion expres-
sion and polarity label annotations of the hotel re-
view component, in Spanish and English. We split
the data in training (train) and evaluation (test) sets
as indicated in Table 1.
The SMT system was trained on freely avail-
</bodyText>
<footnote confidence="0.9806015">
3However, reordering within the segment text is allowed.
4http://www.opener-project.eu/
5Described in deliverable D5.42 (page 6) at:
http://www.opener-project.eu/project/publications.html.
This corpus will be freely available from June 2016 on, and
until then can be used for research purposes.
</footnote>
<page confidence="0.996622">
783
</page>
<bodyText confidence="0.919842">
Source: On the other hand &lt;zone&gt; &lt;x translation=&amp;quot;ou1-P&amp;quot;&gt;x&lt;/x&gt; &lt;wall/&gt; a big ad-
vantage &lt;wall/&gt; &lt;x translation=&amp;quot;/ou1&amp;quot;&gt;x&lt;/x&gt; &lt;/zone&gt; of the hostel is its placement
Translation: por otra parte &lt;ou1-P&gt;una gran ventaja&lt;/ou1&gt; del hostal es su colocaci´on
</bodyText>
<figureCaption confidence="0.949805">
Figure 3: Source text with reordering constraint mark-up as well as code to pass tags, and its translation.
</figureCaption>
<table confidence="0.9994984">
Lang Docs Words Op. Units
Train EN 346 32149 3643
ES 359 31511 3905
Test EN 49 4256 496
ES 50 3733 484
</table>
<tableCaption confidence="0.982375333333333">
Table 1: Number of documents (Docs), words and
opinionated units (Op. Units) in the OpeNER an-
notated data for English (EN) and Spanish (ES).
</tableCaption>
<bodyText confidence="0.9985515625">
able data from the 2013 workshop on Statisti-
cal Machine Translation6 (WMT 2013). We also
crawled monolingual data in the hotel booking
domain, from booking.com and TripAdvisor.com.
From these in-domain data we extracted 100k and
50k word corpora, respectively for data selec-
tion and language model (LM) interpolation tun-
ing. We selected the data closest to the domain in
the English-Spanish parallel corpora via a cross-
entropy-based method (Moore and Lewis, 2010),
using the open source XenC tool (Rousseau,
2013). The size of available and selected corpora
are indicated in the first 4 rows of Table 2. The LM
was an interpolation of LMs trained with the target
part of the parallel corpora and with the rest of the
Booking and Trip Advisor data (last 2 rows of Ta-
ble 2). We used Moses Experiment Management
System (Koehn, 2010) with all default options to
build the SMT system.7
Because the common crawl corpus contained
English sentences in the Spanish side, we applied
an LM-based filter to select only sentence pairs in
which the Spanish side was better scored by the
Spanish LM than with the English LM, and con-
versely for the English side.
We conducted supervised sentiment classifica-
tion experiments for settings a and b of use case
I (see Section 2). We trained and evaluated clas-
sifiers on the annotated data (Table 1), using as
features the tokens (unigrams) within opinion ex-
pressions, and SP (Strong Positive), P (Positive),
N (Negative) and SN (Strong Negative) as la-
</bodyText>
<footnote confidence="0.9957925">
6http://www.statmt.org/wmt13/translation-task.html
7We kept selected parallel data of the common crawl cor-
pus for tuning and test. We obtained BLEU scores of 42 and
45 in the English–Spanish and Spanish–English directions.
</footnote>
<table confidence="0.9995285">
Available Selected
Corpus EN ES EN ES
Common Crawl 46.7 49.5 6.7 7.0
Europarl v7 54.6 57.1 1.7 1.7
News Commentary 4.5 5.1 4.5 5.1
UN 321.7 368.6 3.4 3.5
Booking 1.7 2.6 1.7 2.6
Trip Advisor 23.4 4.4 23.4 4.4
</table>
<tableCaption confidence="0.948301333333333">
Table 2: Size of the available and selected corpora
(in million words) in English (EN) and Spanish
(ES) used to train the SMT system.
</tableCaption>
<figure confidence="0.90808825">
MT
TESTEN TESTES,
1 mono 1 CL a 1 CL b
TRAINEN TRAINEN TRAINES
</figure>
<figureCaption confidence="0.99550625">
Figure 4: Experiments corresponding to group of
rows 1 of Table 3. “mono” refers to monolingual
and “CL a” and “CL b” refer to settings a and b of
use case I (Sec. 2).
</figureCaption>
<bodyText confidence="0.7591506">
bels. We performed the experiments with the weka
toolkit (Hall et al., 2009), using a filter to con-
vert strings into word vectors, and two learning al-
gorithms: SVMs and bagging with Fast Decision
Tree Learner as base algorithm.
</bodyText>
<figureCaption confidence="0.73870975">
Figure 4 represents the experiments conducted
with the EN test set. A monolingual classifier in
English is trained with the EN training set, and
evaluated with the EN test set (1 mono). The re-
</figureCaption>
<table confidence="0.97272275">
Config Train Test LM Filter No Fil
Bag. SVM SVM
1 mono EN EN 77.2 83.4 83.4
1 CL a ENS EN 70.3 75.4 75.8
1 CL b ES ESQ 73.0 75.8 73.6
2 mono ES ES 76.8 81.1 81.1
2 CL a ESQ ES 66.2 72.5 73.0
2 CL b EN ENS 74.5 77.6 76.8
</table>
<tableCaption confidence="0.88628075">
Table 3: Accuracy (in %) achieved by the different
systems. LM Filter and No Fil(ter) refer to the
presence or not of the LM filter for the common
crawl parallel corpus. “Bag.” refers to bagging.
</tableCaption>
<page confidence="0.997695">
784
</page>
<bodyText confidence="0.999980048780488">
sults are reported in the first row of Table 3. To
evaluate cross-lingual setting a, the ES training set
is translated into English (see Section 4), and an
English classifier is trained on the translated data
and evaluated on the EN test set (1 CL a). To eval-
uate setting b, the EN test set is translated into
Spanish, and this translated test is used to evalu-
ate a classifier trained on the ES training set (1 CL
b). With this very simple classifier, we achieve
up to 83.4% accuracy in the monolingual case.
With cross-lingual settings, we loose from about
4% to 8% accuracy, and with the higher quality
SMT system (LM filter), CL-b setting is slightly
better than CL-a.
The same three experiments were conducted for
the ES test set (last three rows of Table 3). We
achieved an accuracy of 81.1% in the monolin-
gual case. Here the CL-b setting achieved a clearly
better accuracy than the CL-a setting (at least 5%
more), and only from 2.3% to 3.5% below the
monolingual one. Thus with the higher quality
SMT system, it is always better to translate the test
data (CL-b setting) than the training corpus.
Comparing the SVM classification accuracy in
the “LM Filter” and “No Fil” columns, we can see
the effect of introducing noise in the MT system.
We observe that the results were more affected by
the translation of the test (-2.2% and -0.8% accu-
racy) than the training set (+0.5% accuracy in both
cases). This agrees with the intuition than errors in
the test directly affect the results and thus may be
more harmful than in the training set, where they
may hardly affect the results if they represent in-
frequent examples.
Regarding use case II, setting c implies a trans-
lation of the analysis outcome. We can use our
method to translate the relevant opinionated units
with their predicted label in their test sentence
context, and extract the relevant information in the
outcome language. In setting d, the test is trans-
lated in the same way as in setting b.
</bodyText>
<sectionHeader confidence="0.970631" genericHeader="conclusions">
6 Conclusions and Perspectives
</sectionHeader>
<bodyText confidence="0.99999335">
We extended the possible CLSA settings to aspect-
level specific use cases. We proposed a method,
based on constrained SMT, to transfer opinionated
units across languages by preserving their bound-
aries. With this method, we built cross-language
sentiment classifiers achieving comparable results
to monolingual ones (from about 4 to 8% and 2.3
to 3.5% loss in accuracy depending on the lan-
guage and machine learning algorithm). We ob-
served that improving the MT quality had more
impact in settings using a translated test than a
translated training corpus. With the higher MT
quality system, we achieved better accuracy by
translating the test than the training corpus.
As future work, we plan to investigate the ex-
act effect of the reordering constraints in terms of
possible translation model phrase pairs and target
language model n-grams which may not be used
depending on the constraint parameters, in order
to find the best configuration.
</bodyText>
<sectionHeader confidence="0.969304" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999954">
This work has received funding from the Sev-
enth Framework Program of the European Com-
mission through the Intra-European Fellowship
(CrossLingMind-2011-300828) Marie Curie Ac-
tions. We also acknowledge partners of the
OpeNER project, in particular Montse Cuadros,
for providing us with the aspect-level annotated
data.
</bodyText>
<sectionHeader confidence="0.864682" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.7817792">
Alexandra Balahur and Marco Turchi. 2014. Com-
parative experiments using supervised learning and
machine translation for multilingual sentiment anal-
ysis. Computer Speech &amp; Language, 28(1):56–75.
A.R. Balamurali, Aditya Joshi, and Pushpak Bhat-
tacharyya. 2012. Cross-lingual sentiment analysis
for Indian languages using linked wordnets. In Proc.
of the International Conference on Computational
Linguistics (COLING), pages 73–82, Mumbai, In-
dia.
</bodyText>
<reference confidence="0.842764705882353">
A. R. Balamurali, Mitesh M Khapra, and Pushpak Bat-
tacharyya. 2013. Lost in Translation: Viability of
Machine Translation for Cross Language Sentiment
Analysis. In Proc. of International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLing), pages 38–49, Samos, Greece.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In Proc. of the International Conference on Linguis-
tic Resources and Evaluation (LREC), pages 2764–
2767, Marrakech, Morocco, May.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2010. Multilingual subjectivity: Are more lan-
guages better? In Proc. of the International Con-
ference on Computational Linguistics (COLING),
pages 28–36, Beijing, China.
</reference>
<page confidence="0.991486">
785
</page>
<reference confidence="0.999784274336283">
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2011. Multilingual sentiment and subjectivity anal-
ysis. In D. M. Bikel and I. Zitouni, editors, Multilin-
gual Natural Language Applications: From Theory
to Practice. Prentice-Hall.
Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena.
2008. International sentiment analysis for news and
blogs. In Proc. of the International Conference on
Weblogs and Social Media, pages 19–26, Seattle,
U.S.A.
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-Linguistic Sentiment Analysis: From
English to Spanish. In Proc. of the International
Conference on Recent Advances in Natural Lan-
guage Processing (RANLP), pages 50–54, Borovets,
Bulgaria.
Erkin Demirtas and Mykola Pechenizkiy. 2013. Cross-
lingual Polarity Detection with Machine Transla-
tion. In Proc. of the International Workshop on Is-
sues of Sentiment Discovery and Opinion Mining
- WISDOM ’13, pages 9:1–9:8, Chicago, Illinois,
USA. ACM Press.
Lin Gui, Ruifeng Xu, Jun Xu, Li Yuan, Yuanlin Yao,
Jiyun Zhou, Qiaoyun Qiu, Shuwei Wang, Kam-fai
Wong, and Ricky Cheung. 2013. A Mixed Model
for Cross Lingual Opinion Analysis. In Second CCF
Conference, Natural Language Processing and Chi-
nese Computing, pages 93–104.
Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu, Bin Liu,
and Xiaolong Wang. 2014. Cross-lingual opinion
analysis via negative transfer detection. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics, pages 860–865, Baltimore, Mary-
land.
Michael Haas and Yannick Versley. 2015. Subsen-
tential sentiment on a shoestring: A crosslingual
analysis of compositional classification. In Proc. of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 694–704, Denver, Colorado.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1).
Ahmed Hassan, Amjad AbuJbara, Rahul Jha, and
Dragomir Radev. 2011. Identifying the semantic
orientation of foreign words. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
592–597, Portland, Oregon, USA, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proc. of the 45th Annual Meeting of the
Association for Computational Linguistics (Demo
and Poster Sessions), pages 177–180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Philipp Koehn. 2010. An experimental management
system. Prague Bulletin of Mathematical Linguis-
tics (PBML), (94):87–96.
Zheng Lin, Xiaolong Jin, Xueke Xu, Yuanzhuo Wang,
Weiping Wang, and Xueqi Cheng. 2014. A cross-
lingual joint aspect/sentiment model for sentiment
analysis. In Proc. of the ACM International Confer-
ence on Conference on Information and Knowledge
Management, CIKM ’14, pages 1089–1098, Shang-
hai, China.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan &amp; Claypool Publishers.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin
K. Tsou. 2011. Joint bilingual sentiment classifi-
cation with unlabeled parallel corpora. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 320–330, Portland, Oregon, USA.
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-lingual
mixture model for sentiment classification. In Proc.
of the Annual Meeting of the Association for Com-
putational Linguistics, pages 572–581, Jeju Island,
Korea.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 976–983, Prague, Czech Republic,
June.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220–224, Uppsala, Sweden.
Veronica Perez-Rosas, Carmen Banea, and Rada Mi-
halcea. 2012. Learning sentiment lexicons in span-
ish. In Proc. of the International Conference on
Linguistic Resources and Evaluation (LREC), pages
3077–3081, Istanbul, Turkey, may.
Kashyap Popat, Balamurali A.R, Pushpak Bhat-
tacharyya, and Gholamreza Haffari. 2013. The
haves and the have-nots: Leveraging unlabelled cor-
pora for sentiment analysis. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 412–422, Sofia, Bulgaria.
Peter Prettenhofer and Benno Stein. 2010. Cross-
language text classification using structural corre-
spondence learning. In Proc. of the Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1118–1127, Uppsala, Sweden. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.981862">
786
</page>
<reference confidence="0.999714815789474">
A Rousseau. 2013. XenC: An Open-Source Tool
for Data Selection in Natural Language Process-
ing. Prague Bulletin of Mathematical Linguistics
(PBML), (100):73–82.
Christian Scheible, Florian Laws, Lukas Michelbacher,
and Hinrich Sch¨utze. 2010. Sentiment translation
through multi-edge graphs. In Proc. of the Inter-
national Conference on Computational Linguistics
(COLING), pages 1104–1112, Beijing, China, Au-
gust.
Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova,
Ralf Steinberger, Hristo Tanev, Silvia V´azquez, and
Vanni Zavarella. 2012. Creating sentiment dictio-
naries via triangulation. Decision Support Systems,
53(4):689 – 694.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 235–
243, Suntec, Singapore.
Bin Wei and Christopher Pal. 2010. Cross lingual
adaptation: An experiment on sentiment classifica-
tions. In Proc. of the ACL 2010 Conference Short
Papers, pages 258–262, Uppsala, Sweden. Proc. of
the Annual Meeting of the Association for Compu-
tational Linguistics.
Min Xiao and Yuhong Guo. 2012. Multi-view ad-
aboost for multilingual subjectivity analysis. In
Proc. of the International Conference on Compu-
tational Linguistics (COLING), pages 2851–2866,
Mumbai, India.
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2012.
Cross-Language Opinion Target Extraction in Re-
view Texts. In IEEE 12th International Conference
on Data Mining, pages 1200–1205, Brussels, Bel-
gium.
</reference>
<page confidence="0.997313">
787
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957047">
<title confidence="0.998691">Aspect-Level Cross-lingual Sentiment with Constrained SMT</title>
<author confidence="0.989366">Patrik Lambert</author>
<affiliation confidence="0.994264">Universitat Pompeu Fabra, Barcelona,</affiliation>
<email confidence="0.999854">patrik.lambert@upf.edu</email>
<abstract confidence="0.998559631578947">Most cross-lingual sentiment classification (CLSC) research so far has been performed at sentence or document level. Aspect-level CLSC, which is more appropriate for many applications, presents the additional difficulty that we consider subsentential opinionated units which have to be mapped across languages. In this paper, we extend the possible cross-lingual sentiment analysis settings to aspect-level specific use cases. We propose a method, based on constrained SMT, to transfer opinionated units across languages by preserving their boundaries. We show that cross-language sentiment classifiers built with this method achieve comparable results to monolingual ones, and we compare different cross-lingual settings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A R Balamurali</author>
<author>Mitesh M Khapra</author>
<author>Pushpak Battacharyya</author>
</authors>
<title>Lost in Translation: Viability of Machine Translation for Cross Language Sentiment Analysis.</title>
<date>2013</date>
<booktitle>In Proc. of International Conference on Intelligent Text Processing and Computational Linguistics (CICLing),</booktitle>
<pages>38--49</pages>
<location>Samos, Greece.</location>
<contexts>
<context position="8775" citStr="Balamurali et al. (2013)" startWordPosition="1427" endWordPosition="1430">h, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012). However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis systems that can obtain compar</context>
</contexts>
<marker>Balamurali, Khapra, Battacharyya, 2013</marker>
<rawString>A. R. Balamurali, Mitesh M Khapra, and Pushpak Battacharyya. 2013. Lost in Translation: Viability of Machine Translation for Cross Language Sentiment Analysis. In Proc. of International Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pages 38–49, Samos, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>A bootstrapping method for building subjectivity lexicons for languages with scarce resources.</title>
<date>2008</date>
<booktitle>In Proc. of the International Conference on Linguistic Resources and Evaluation (LREC),</booktitle>
<pages>2764--2767</pages>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="7559" citStr="Banea et al. (2008)" startWordPosition="1239" endWordPosition="1242">fer approach, a source sentiment lexicon is transferred into the target language and a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to 2For document and sentence-level classification, the outcome is a set of polarity labels independent on language. Learn SA TEST L0 TEST OUTLTEST 782 replace words in training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test </context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2008. A bootstrapping method for building subjectivity lexicons for languages with scarce resources. In Proc. of the International Conference on Linguistic Resources and Evaluation (LREC), pages 2764– 2767, Marrakech, Morocco, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multilingual subjectivity: Are more languages better?</title>
<date>2010</date>
<booktitle>In Proc. of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>28--36</pages>
<location>Beijing, China.</location>
<contexts>
<context position="8433" citStr="Banea et al., 2010" startWordPosition="1372" endWordPosition="1375">782 replace words in training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated r</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, 2010</marker>
<rawString>Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2010. Multilingual subjectivity: Are more languages better? In Proc. of the International Conference on Computational Linguistics (COLING), pages 28–36, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multilingual sentiment and subjectivity analysis.</title>
<date>2011</date>
<booktitle>Multilingual Natural Language Applications: From Theory to Practice.</booktitle>
<editor>In D. M. Bikel and I. Zitouni, editors,</editor>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="7200" citStr="Banea et al., 2011" startWordPosition="1185" endWordPosition="1188">ges, either to have more examples and improve the classifier accuracy, or to have a broader view of the opinions under study. In this paper we focus on use cases I and II. 3 Related Work The main CLSC approaches described in the literature are via lexicon transfer, via corpus transfer, via test translation and via joint classification. In the lexicon transfer approach, a source sentiment lexicon is transferred into the target language and a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to 2For document and sentence-level classification, the outcome is a set of polarity labels independent on language. Learn SA TEST L0</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, 2011</marker>
<rawString>Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2011. Multilingual sentiment and subjectivity analysis. In D. M. Bikel and I. Zitouni, editors, Multilingual Natural Language Applications: From Theory to Practice. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Bautin</author>
<author>Lohit Vijayarenu</author>
<author>Steven Skiena</author>
</authors>
<title>International sentiment analysis for news and blogs.</title>
<date>2008</date>
<booktitle>In Proc. of the International Conference on Weblogs and Social Media,</booktitle>
<pages>pages</pages>
<location>Seattle, U.S.A.</location>
<contexts>
<context position="8314" citStr="Bautin et al., 2008" startWordPosition="1354" endWordPosition="1357">ce-level classification, the outcome is a set of polarity labels independent on language. Learn SA TEST L0 TEST OUTLTEST 782 replace words in training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be tr</context>
</contexts>
<marker>Bautin, Vijayarenu, Skiena, 2008</marker>
<rawString>Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena. 2008. International sentiment analysis for news and blogs. In Proc. of the International Conference on Weblogs and Social Media, pages 19–26, Seattle, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Maite Taboada</author>
</authors>
<title>Cross-Linguistic Sentiment Analysis: From English to Spanish.</title>
<date>2009</date>
<booktitle>In Proc. of the International Conference on Recent Advances in Natural Language Processing (RANLP),</booktitle>
<pages>50--54</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="8746" citStr="Brooke et al. (2009)" startWordPosition="1422" endWordPosition="1425"> test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012). However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis s</context>
</contexts>
<marker>Brooke, Tofiloski, Taboada, 2009</marker>
<rawString>Julian Brooke, Milan Tofiloski, and Maite Taboada. 2009. Cross-Linguistic Sentiment Analysis: From English to Spanish. In Proc. of the International Conference on Recent Advances in Natural Language Processing (RANLP), pages 50–54, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erkin Demirtas</author>
<author>Mykola Pechenizkiy</author>
</authors>
<title>Crosslingual Polarity Detection with Machine Translation.</title>
<date>2013</date>
<booktitle>In Proc. of the International Workshop on Issues of Sentiment Discovery and Opinion Mining - WISDOM ’13,</booktitle>
<pages>9--1</pages>
<publisher>ACM Press.</publisher>
<location>Chicago, Illinois, USA.</location>
<contexts>
<context position="8511" citStr="Demirtas and Pechenizkiy, 2013" startWordPosition="1383" endWordPosition="1387">-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which </context>
</contexts>
<marker>Demirtas, Pechenizkiy, 2013</marker>
<rawString>Erkin Demirtas and Mykola Pechenizkiy. 2013. Crosslingual Polarity Detection with Machine Translation. In Proc. of the International Workshop on Issues of Sentiment Discovery and Opinion Mining - WISDOM ’13, pages 9:1–9:8, Chicago, Illinois, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Gui</author>
<author>Ruifeng Xu</author>
<author>Jun Xu</author>
<author>Li Yuan</author>
<author>Yuanlin Yao</author>
<author>Jiyun Zhou</author>
<author>Qiaoyun Qiu</author>
<author>Shuwei Wang</author>
<author>Kam-fai Wong</author>
<author>Ricky Cheung</author>
</authors>
<title>A Mixed Model for Cross Lingual Opinion Analysis.</title>
<date>2013</date>
<booktitle>In Second CCF Conference, Natural Language Processing and Chinese Computing,</booktitle>
<pages>93--104</pages>
<contexts>
<context position="8688" citStr="Gui et al. (2013)" startWordPosition="1412" endWordPosition="1415">nd source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012). However, Balahur and Turchi (2014) conclude</context>
</contexts>
<marker>Gui, Xu, Xu, Yuan, Yao, Zhou, Qiu, Wang, Wong, Cheung, 2013</marker>
<rawString>Lin Gui, Ruifeng Xu, Jun Xu, Li Yuan, Yuanlin Yao, Jiyun Zhou, Qiaoyun Qiu, Shuwei Wang, Kam-fai Wong, and Ricky Cheung. 2013. A Mixed Model for Cross Lingual Opinion Analysis. In Second CCF Conference, Natural Language Processing and Chinese Computing, pages 93–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Gui</author>
<author>Ruifeng Xu</author>
<author>Qin Lu</author>
<author>Jun Xu</author>
<author>Jian Xu</author>
<author>Bin Liu</author>
<author>Xiaolong Wang</author>
</authors>
<title>Cross-lingual opinion analysis via negative transfer detection.</title>
<date>2014</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>860--865</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="7931" citStr="Gui et al. (2014)" startWordPosition="1296" endWordPosition="1299">t al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to 2For document and sentence-level classification, the outcome is a set of polarity labels independent on language. Learn SA TEST L0 TEST OUTLTEST 782 replace words in training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu</context>
</contexts>
<marker>Gui, Xu, Lu, Xu, Xu, Liu, Wang, 2014</marker>
<rawString>Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu, Bin Liu, and Xiaolong Wang. 2014. Cross-lingual opinion analysis via negative transfer detection. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 860–865, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Haas</author>
<author>Yannick Versley</author>
</authors>
<title>Subsentential sentiment on a shoestring: A crosslingual analysis of compositional classification.</title>
<date>2015</date>
<booktitle>In Proc. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>694--704</pages>
<location>Denver, Colorado.</location>
<contexts>
<context position="9626" citStr="Haas and Versley (2015)" startWordPosition="1578" endWordPosition="1581">ills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012). However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English. All this work was performed at sentence or document level. Zhou et al. (2012) and Lin et al. (2014) work at the aspect level, but they focus on cross-lingual aspect extraction. Haas and Versley (2015) use CLSA for individual syntactic nodes, however they need to map target-language and source-language nodes with word alignment. 4 Language Transfer In aspect-level SA, there may be several opinionated segments in each sentence. When performing a language transfer, each segment in the target language has to be mapped to its corresponding segment in the source language. This may not be an obvious task at all. For example, if a standard MT system is used for language translation, the source opinionated segment may be reordered and split in several parts in the target language. Then the differen</context>
</contexts>
<marker>Haas, Versley, 2015</marker>
<rawString>Michael Haas and Yannick Versley. 2015. Subsentential sentiment on a shoestring: A crosslingual analysis of compositional classification. In Proc. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 694–704, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="15506" citStr="Hall et al., 2009" startWordPosition="2556" endWordPosition="2559">EN ES Common Crawl 46.7 49.5 6.7 7.0 Europarl v7 54.6 57.1 1.7 1.7 News Commentary 4.5 5.1 4.5 5.1 UN 321.7 368.6 3.4 3.5 Booking 1.7 2.6 1.7 2.6 Trip Advisor 23.4 4.4 23.4 4.4 Table 2: Size of the available and selected corpora (in million words) in English (EN) and Spanish (ES) used to train the SMT system. MT TESTEN TESTES, 1 mono 1 CL a 1 CL b TRAINEN TRAINEN TRAINES Figure 4: Experiments corresponding to group of rows 1 of Table 3. “mono” refers to monolingual and “CL a” and “CL b” refer to settings a and b of use case I (Sec. 2). bels. We performed the experiments with the weka toolkit (Hall et al., 2009), using a filter to convert strings into word vectors, and two learning algorithms: SVMs and bagging with Fast Decision Tree Learner as base algorithm. Figure 4 represents the experiments conducted with the EN test set. A monolingual classifier in English is trained with the EN training set, and evaluated with the EN test set (1 mono). The reConfig Train Test LM Filter No Fil Bag. SVM SVM 1 mono EN EN 77.2 83.4 83.4 1 CL a ENS EN 70.3 75.4 75.8 1 CL b ES ESQ 73.0 75.8 73.6 2 mono ES ES 76.8 81.1 81.1 2 CL a ESQ ES 66.2 72.5 73.0 2 CL b EN ENS 74.5 77.6 76.8 Table 3: Accuracy (in %) achieved by</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Amjad AbuJbara</author>
<author>Rahul Jha</author>
<author>Dragomir Radev</author>
</authors>
<title>Identifying the semantic orientation of foreign words.</title>
<date>2011</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>592--597</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="7221" citStr="Hassan et al., 2011" startWordPosition="1189" endWordPosition="1192">more examples and improve the classifier accuracy, or to have a broader view of the opinions under study. In this paper we focus on use cases I and II. 3 Related Work The main CLSC approaches described in the literature are via lexicon transfer, via corpus transfer, via test translation and via joint classification. In the lexicon transfer approach, a source sentiment lexicon is transferred into the target language and a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to 2For document and sentence-level classification, the outcome is a set of polarity labels independent on language. Learn SA TEST L0 TEST OUTLTEST 782 re</context>
</contexts>
<marker>Hassan, AbuJbara, Jha, Radev, 2011</marker>
<rawString>Ahmed Hassan, Amjad AbuJbara, Rahul Jha, and Dragomir Radev. 2011. Identifying the semantic orientation of foreign words. In Proc. of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 592–597, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics (Demo and Poster Sessions),</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="11213" citStr="Koehn et al., 2007" startWordPosition="1834" endWordPosition="1837"> applications, would then be lost. Furthermore, the translation quality would be worse than when the segments are translated within the whole sentence context. To solve these problems, we translate the whole sentences but with reordering constraints ensuring that the opinionated segments are preserved during translation. That is, the text between the relevant segment boundaries is not reordered nor mixed with the text outside these boundaries.3 Thus the text in the target language segment comes only from the corresponding source language segment. We use the Moses statistical MT (SMT) toolkit (Koehn et al., 2007) to perform the translation. In Moses, these reordering constraints are implemented with the zone and wall tags, as indicated in Figure 3. Moses also allows mark-up to be directly passed to the translation, via the x tag. We use this functionality to keep track, via the tags &lt;ou[id][-label]&gt; and &lt;/ou[id]&gt;, of the segment boundaries (ou stands for Opinionated Unit), of the opinionated segment identifier ([id]) and, for training and evaluation purposes, of the polarity label ([-label]). In the example of Figure 3, the id is 1 and the label is P. 5 CLSA experiments In order to compare CLSA settin</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics (Demo and Poster Sessions), pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>An experimental management system.</title>
<date>2010</date>
<booktitle>Prague Bulletin of Mathematical Linguistics (PBML),</booktitle>
<pages>94--87</pages>
<contexts>
<context position="13981" citStr="Koehn, 2010" startWordPosition="2292" endWordPosition="2293">ed 100k and 50k word corpora, respectively for data selection and language model (LM) interpolation tuning. We selected the data closest to the domain in the English-Spanish parallel corpora via a crossentropy-based method (Moore and Lewis, 2010), using the open source XenC tool (Rousseau, 2013). The size of available and selected corpora are indicated in the first 4 rows of Table 2. The LM was an interpolation of LMs trained with the target part of the parallel corpora and with the rest of the Booking and Trip Advisor data (last 2 rows of Table 2). We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system.7 Because the common crawl corpus contained English sentences in the Spanish side, we applied an LM-based filter to select only sentence pairs in which the Spanish side was better scored by the Spanish LM than with the English LM, and conversely for the English side. We conducted supervised sentiment classification experiments for settings a and b of use case I (see Section 2). We trained and evaluated classifiers on the annotated data (Table 1), using as features the tokens (unigrams) within opinion expressions, and SP (Strong Positive), P (Po</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. An experimental management system. Prague Bulletin of Mathematical Linguistics (PBML), (94):87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Lin</author>
<author>Xiaolong Jin</author>
<author>Xueke Xu</author>
<author>Yuanzhuo Wang</author>
<author>Weiping Wang</author>
<author>Xueqi Cheng</author>
</authors>
<title>A crosslingual joint aspect/sentiment model for sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proc. of the ACM International Conference on Conference on Information and Knowledge Management, CIKM ’14,</booktitle>
<pages>1089--1098</pages>
<location>Shanghai, China.</location>
<contexts>
<context position="9525" citStr="Lin et al. (2014)" startWordPosition="1562" endWordPosition="1565"> be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012). However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English. All this work was performed at sentence or document level. Zhou et al. (2012) and Lin et al. (2014) work at the aspect level, but they focus on cross-lingual aspect extraction. Haas and Versley (2015) use CLSA for individual syntactic nodes, however they need to map target-language and source-language nodes with word alignment. 4 Language Transfer In aspect-level SA, there may be several opinionated segments in each sentence. When performing a language transfer, each segment in the target language has to be mapped to its corresponding segment in the source language. This may not be an obvious task at all. For example, if a standard MT system is used for language translation, the source opin</context>
</contexts>
<marker>Lin, Jin, Xu, Wang, Wang, Cheng, 2014</marker>
<rawString>Zheng Lin, Xiaolong Jin, Xueke Xu, Yuanzhuo Wang, Weiping Wang, and Xueqi Cheng. 2014. A crosslingual joint aspect/sentiment model for sentiment analysis. In Proc. of the ACM International Conference on Conference on Information and Knowledge Management, CIKM ’14, pages 1089–1098, Shanghai, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1117" citStr="Liu, 2012" startWordPosition="154" endWordPosition="155">ible cross-lingual sentiment analysis settings to aspect-level specific use cases. We propose a method, based on constrained SMT, to transfer opinionated units across languages by preserving their boundaries. We show that cross-language sentiment classifiers built with this method achieve comparable results to monolingual ones, and we compare different cross-lingual settings. 1 Introduction Sentiment analysis (SA) is the task of analysing opinions, sentiments or emotions expressed towards entities such as products, services, organisations, issues, and the various attributes of these entities (Liu, 2012). The analysis may be performed at the level of a document (blog post, review) or sentence. However, this is not appropriate for many applications because the same document or sentence can contain positive opinions towards specific aspects and negative ones towards other aspects. Thus a finer analysis can be conducted at the level of the aspects of the entities towards which opinions are expressed, identifying for each opinionated unit elements such as its target, polarity and the polar words used to qualify the target. The two main SA approaches presented in the literature are (i) a machine l</context>
<context position="5714" citStr="Liu, 2012" startWordPosition="929" endWordPosition="930"> Proj T T SALT RAIN TEST0 OUTLTRAIN LTRAIN Figure 1: Use case I settings. SA refers to Sentiment Analisys, T to Translation, Proj to Projection and Learn to Learning, and the prime symbol designs a language into which a set has been automatically translated. Use case II. We may have training resources in the language of the opinions, but we need the re1As mentioned above, at the aspect level, the outcome of the analysis may be a set of opinion targets with their polarity. It may also be more complex, such as a set of opinion expressions with their respective target, polarity, holder and time (Liu, 2012). The outcome may need to be in another language as the opinions themselves. For example, a company based in China may survey the opinions of their Spanishspeaking customers, and then transfer the SA outcome into Chinese so that their marketing department can understand it. LOUT =� LTEST (2) (d) LTRAIN = LTEST; the test opinions are first analysed in their language, then the analysis outcome is transferred into the desired language. (e) LTRAIN = LOUT; the test set is first transferred into the desired outcome language, and the SA is performed in this language. SALTEST T (c) TEST OUTLTEST OUTL0</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Lu</author>
<author>Chenhao Tan</author>
<author>Claire Cardie</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Joint bilingual sentiment classification with unlabeled parallel corpora.</title>
<date>2011</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>320--330</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="8545" citStr="Lu et al., 2011" startWordPosition="1390" endWordPosition="1393">4) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are ava</context>
</contexts>
<marker>Lu, Tan, Cardie, Tsou, 2011</marker>
<rawString>Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K. Tsou. 2011. Joint bilingual sentiment classification with unlabeled parallel corpora. In Proc. of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 320–330, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinfan Meng</author>
<author>Furu Wei</author>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Ge Xu</author>
<author>Houfeng Wang</author>
</authors>
<title>Cross-lingual mixture model for sentiment classification.</title>
<date>2012</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>572--581</pages>
<location>Jeju Island,</location>
<contexts>
<context position="8669" citStr="Meng et al., 2012" startWordPosition="1408" endWordPosition="1411">s, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012). However, Balahur and Tur</context>
</contexts>
<marker>Meng, Wei, Liu, Zhou, Xu, Wang, 2012</marker>
<rawString>Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, Ge Xu, and Houfeng Wang. 2012. Cross-lingual mixture model for sentiment classification. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 572–581, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning multilingual subjective language via cross-lingual projections.</title>
<date>2007</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>976--983</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7171" citStr="Mihalcea et al., 2007" startWordPosition="1180" endWordPosition="1183"> data available in several languages, either to have more examples and improve the classifier accuracy, or to have a broader view of the opinions under study. In this paper we focus on use cases I and II. 3 Related Work The main CLSC approaches described in the literature are via lexicon transfer, via corpus transfer, via test translation and via joint classification. In the lexicon transfer approach, a source sentiment lexicon is transferred into the target language and a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to 2For document and sentence-level classification, the outcome is a set of polarity labels independent </context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007. Learning multilingual subjective language via cross-lingual projections. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 976–983, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>220--224</pages>
<location>Uppsala,</location>
<contexts>
<context position="13615" citStr="Moore and Lewis, 2010" startWordPosition="2223" endWordPosition="2226">96 ES 50 3733 484 Table 1: Number of documents (Docs), words and opinionated units (Op. Units) in the OpeNER annotated data for English (EN) and Spanish (ES). able data from the 2013 workshop on Statistical Machine Translation6 (WMT 2013). We also crawled monolingual data in the hotel booking domain, from booking.com and TripAdvisor.com. From these in-domain data we extracted 100k and 50k word corpora, respectively for data selection and language model (LM) interpolation tuning. We selected the data closest to the domain in the English-Spanish parallel corpora via a crossentropy-based method (Moore and Lewis, 2010), using the open source XenC tool (Rousseau, 2013). The size of available and selected corpora are indicated in the first 4 rows of Table 2. The LM was an interpolation of LMs trained with the target part of the parallel corpora and with the rest of the Booking and Trip Advisor data (last 2 rows of Table 2). We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system.7 Because the common crawl corpus contained English sentences in the Spanish side, we applied an LM-based filter to select only sentence pairs in which the Spanish side was better scor</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, pages 220–224, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronica Perez-Rosas</author>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning sentiment lexicons in spanish.</title>
<date>2012</date>
<booktitle>In Proc. of the International Conference on Linguistic Resources and Evaluation (LREC),</booktitle>
<pages>3077--3081</pages>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="7248" citStr="Perez-Rosas et al., 2012" startWordPosition="1193" endWordPosition="1196">rove the classifier accuracy, or to have a broader view of the opinions under study. In this paper we focus on use cases I and II. 3 Related Work The main CLSC approaches described in the literature are via lexicon transfer, via corpus transfer, via test translation and via joint classification. In the lexicon transfer approach, a source sentiment lexicon is transferred into the target language and a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to 2For document and sentence-level classification, the outcome is a set of polarity labels independent on language. Learn SA TEST L0 TEST OUTLTEST 782 replace words in training and</context>
</contexts>
<marker>Perez-Rosas, Banea, Mihalcea, 2012</marker>
<rawString>Veronica Perez-Rosas, Carmen Banea, and Rada Mihalcea. 2012. Learning sentiment lexicons in spanish. In Proc. of the International Conference on Linguistic Resources and Evaluation (LREC), pages 3077–3081, Istanbul, Turkey, may.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashyap Popat</author>
<author>A R Balamurali</author>
<author>Pushpak Bhattacharyya</author>
<author>Gholamreza Haffari</author>
</authors>
<title>The haves and the have-nots: Leveraging unlabelled corpora for sentiment analysis.</title>
<date>2013</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>412--422</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="8013" citStr="Popat et al. (2013)" startWordPosition="1309" endWordPosition="1312">pproach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to 2For document and sentence-level classification, the outcome is a set of polarity labels independent on language. Learn SA TEST L0 TEST OUTLTEST 782 replace words in training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofe</context>
</contexts>
<marker>Popat, Balamurali, Bhattacharyya, Haffari, 2013</marker>
<rawString>Kashyap Popat, Balamurali A.R, Pushpak Bhattacharyya, and Gholamreza Haffari. 2013. The haves and the have-nots: Leveraging unlabelled corpora for sentiment analysis. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Prettenhofer</author>
<author>Benno Stein</author>
</authors>
<title>Crosslanguage text classification using structural correspondence learning.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1118--1127</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="8631" citStr="Prettenhofer and Stein, 2010" startWordPosition="1401" endWordPosition="1404"> al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali e</context>
</contexts>
<marker>Prettenhofer, Stein, 2010</marker>
<rawString>Peter Prettenhofer and Benno Stein. 2010. Crosslanguage text classification using structural correspondence learning. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 1118–1127, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rousseau</author>
</authors>
<title>XenC: An Open-Source Tool for Data Selection</title>
<date>2013</date>
<booktitle>in Natural Language Processing. Prague Bulletin of Mathematical Linguistics (PBML),</booktitle>
<pages>100--73</pages>
<contexts>
<context position="13665" citStr="Rousseau, 2013" startWordPosition="2233" endWordPosition="2234">rds and opinionated units (Op. Units) in the OpeNER annotated data for English (EN) and Spanish (ES). able data from the 2013 workshop on Statistical Machine Translation6 (WMT 2013). We also crawled monolingual data in the hotel booking domain, from booking.com and TripAdvisor.com. From these in-domain data we extracted 100k and 50k word corpora, respectively for data selection and language model (LM) interpolation tuning. We selected the data closest to the domain in the English-Spanish parallel corpora via a crossentropy-based method (Moore and Lewis, 2010), using the open source XenC tool (Rousseau, 2013). The size of available and selected corpora are indicated in the first 4 rows of Table 2. The LM was an interpolation of LMs trained with the target part of the parallel corpora and with the rest of the Booking and Trip Advisor data (last 2 rows of Table 2). We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system.7 Because the common crawl corpus contained English sentences in the Spanish side, we applied an LM-based filter to select only sentence pairs in which the Spanish side was better scored by the Spanish LM than with the English LM, and</context>
</contexts>
<marker>Rousseau, 2013</marker>
<rawString>A Rousseau. 2013. XenC: An Open-Source Tool for Data Selection in Natural Language Processing. Prague Bulletin of Mathematical Linguistics (PBML), (100):73–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Scheible</author>
<author>Florian Laws</author>
<author>Lukas Michelbacher</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Sentiment translation through multi-edge graphs.</title>
<date>2010</date>
<booktitle>In Proc. of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1104--1112</pages>
<location>Beijing, China,</location>
<marker>Scheible, Laws, Michelbacher, Sch¨utze, 2010</marker>
<rawString>Christian Scheible, Florian Laws, Lukas Michelbacher, and Hinrich Sch¨utze. 2010. Sentiment translation through multi-edge graphs. In Proc. of the International Conference on Computational Linguistics (COLING), pages 1104–1112, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Steinberger</author>
<author>Mohamed Ebrahim</author>
<author>Maud Ehrmann</author>
</authors>
<title>Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova, Ralf Steinberger, Hristo Tanev,</title>
<date>2012</date>
<volume>53</volume>
<issue>4</issue>
<pages>694</pages>
<location>Silvia V´azquez, and</location>
<contexts>
<context position="7371" citStr="Steinberger et al., 2012" startWordPosition="1209" endWordPosition="1212">and II. 3 Related Work The main CLSC approaches described in the literature are via lexicon transfer, via corpus transfer, via test translation and via joint classification. In the lexicon transfer approach, a source sentiment lexicon is transferred into the target language and a lexicon-based classifier is build in the target language. Approaches to transfer lexica include machine translation (MT) (Mihalcea et al., 2007), Wordnet (Banea et al., 2011; Hassan et al., 2011; Perez-Rosas et al., 2012), relations between dictionaries represented in graphs (Scheible et al., 2010), or triangulation (Steinberger et al., 2012). The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language. Banea et al. (2008) follow this approach, translating an annotated corpus via MT. Balamurali et al. (2012) use linked Wordnets to 2For document and sentence-level classification, the outcome is a set of polarity labels independent on language. Learn SA TEST L0 TEST OUTLTEST 782 replace words in training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process</context>
</contexts>
<marker>Steinberger, Ebrahim, Ehrmann, 2012</marker>
<rawString>Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann, Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova, Ralf Steinberger, Hristo Tanev, Silvia V´azquez, and Vanni Zavarella. 2012. Creating sentiment dictionaries via triangulation. Decision Support Systems, 53(4):689 – 694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>235--243</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="8478" citStr="Wan, 2009" startWordPosition="1381" endWordPosition="1382">r (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only op</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 235– 243, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Wei</author>
<author>Christopher Pal</author>
</authors>
<title>Cross lingual adaptation: An experiment on sentiment classifications.</title>
<date>2010</date>
<booktitle>In Proc. of the ACL 2010 Conference Short Papers,</booktitle>
<pages>258--262</pages>
<location>Uppsala,</location>
<contexts>
<context position="8600" citStr="Wei and Pal, 2010" startWordPosition="1397" endWordPosition="1400"> learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (P</context>
</contexts>
<marker>Wei, Pal, 2010</marker>
<rawString>Bin Wei and Christopher Pal. 2010. Cross lingual adaptation: An experiment on sentiment classifications. In Proc. of the ACL 2010 Conference Short Papers, pages 258–262, Uppsala, Sweden. Proc. of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Xiao</author>
<author>Yuhong Guo</author>
</authors>
<title>Multi-view adaboost for multilingual subjectivity analysis.</title>
<date>2012</date>
<booktitle>In Proc. of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>2851--2866</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="8454" citStr="Xiao and Guo, 2012" startWordPosition="1376" endWordPosition="1379"> training and test corpora by their (language-independent) synset identifiers. Gui et al. (2014) reduce negative transfer in the process of transfer learning. Popat et al. (2013) perform CLSA with clusters as features, bridging target and source language clusters with word alignment. In the test translation approach, test sentences from the target language are translated into the source language and they are classified using a source language classifier (Bautin et al., 2008). Work on joint classification includes training a classifier with features from multilingual views (Banea et al., 2010; Xiao and Guo, 2012), co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013), joint learning (Lu et al., 2011), structural correspondence learning (Wei and Pal, 2010; Prettenhofer and Stein, 2010) or mixture models (Meng et al., 2012). Gui et al. (2013) compare several of these approaches. Brooke et al. (2009) and Balamurali et al. (2013) conclude that at document level, it is cheaper to annotate resources in the target language than building CLSA systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking,</context>
</contexts>
<marker>Xiao, Guo, 2012</marker>
<rawString>Min Xiao and Yuhong Guo. 2012. Multi-view adaboost for multilingual subjectivity analysis. In Proc. of the International Conference on Computational Linguistics (COLING), pages 2851–2866, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinjie Zhou</author>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Cross-Language Opinion Target Extraction in Review Texts.</title>
<date>2012</date>
<booktitle>In IEEE 12th International Conference on Data Mining,</booktitle>
<pages>1200--1205</pages>
<location>Brussels, Belgium.</location>
<contexts>
<context position="9503" citStr="Zhou et al. (2012)" startWordPosition="1557" endWordPosition="1560">A systems. This may not be true at aspect level, in which the annotation cost is much higher. In any case, when the skills to build such annotated resources are lacking, CLSA may be the only option. In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012). However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English. All this work was performed at sentence or document level. Zhou et al. (2012) and Lin et al. (2014) work at the aspect level, but they focus on cross-lingual aspect extraction. Haas and Versley (2015) use CLSA for individual syntactic nodes, however they need to map target-language and source-language nodes with word alignment. 4 Language Transfer In aspect-level SA, there may be several opinionated segments in each sentence. When performing a language transfer, each segment in the target language has to be mapped to its corresponding segment in the source language. This may not be an obvious task at all. For example, if a standard MT system is used for language transl</context>
</contexts>
<marker>Zhou, Wan, Xiao, 2012</marker>
<rawString>Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2012. Cross-Language Opinion Target Extraction in Review Texts. In IEEE 12th International Conference on Data Mining, pages 1200–1205, Brussels, Belgium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>