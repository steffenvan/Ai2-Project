<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015903">
<title confidence="0.99531">
Linguistic Considerations in Automatic Question Generation
</title>
<author confidence="0.822402">
Karen Mazidi
</author>
<affiliation confidence="0.7139535">
HiLT Lab
University of North Texas
</affiliation>
<address confidence="0.768714">
Denton TX 76207, USA
</address>
<email confidence="0.998737">
KarenMazidi@my.unt.edu
</email>
<sectionHeader confidence="0.993883" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927666666667">
As students read expository text, compre-
hension is improved by pausing to answer
questions that reinforce the material. We
describe an automatic question generator
that uses semantic pattern recognition to
create questions of varying depth and type
for self-study or tutoring. Throughout, we
explore how linguistic considerations in-
form system design. In the described sys-
tem, semantic role labels of source sen-
tences are used in a domain-independent
manner to generate both questions and an-
swers related to the source sentence. Eval-
uation results show a 44% reduction in the
error rate relative to the best prior systems,
averaging over all metrics, and up to 61%
reduction in the error rate on grammatical-
ity judgments.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993">
Studies of student learning show that answering
questions increases depth of student learning, fa-
cilitates transfer learning, and improves students’
retention of material (McDaniel et al., 2007; Car-
penter, 2012; Roediger and Pyc, 2012). The aim
of this work is to automatically generate questions
for such pedagogical purposes.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999128090909091">
Approaches to automatic question generation from
text span nearly four decades. The vast ma-
jority of systems generate questions by select-
ing one sentence at a time, extracting portions
of the source sentence, then applying transfor-
mation rules or patterns in order to construct a
question. A well-known early work is Wolfe’s
AUTOQUEST (Wolfe, 1976), a syntactic pattern
matching system. A recent approach from Heil-
man and Smith (2009, 2010) uses syntactic pars-
ing and transformation rules to generate questions.
</bodyText>
<note confidence="0.48597475">
Rodney D. Nielsen
HiLT Lab
University of North Texas
Denton TX 76207, USA
</note>
<email confidence="0.974643">
Rodney.Nielsen@unt.edu
</email>
<bodyText confidence="0.999895676470588">
Syntactic, sentence-level approaches outnumber
other approaches as seen in the Question Gen-
eration Shared Task Evaluation Challenge 2010
(Boyer and Piwek, 2010) which received only one
paragraph-level, semantic entry. Argawal, Shah
and Mannem (2011) continue the paragraph-level
approach using discourse cues to find appropriate
text segments upon which to construct questions
at a deeper conceptual level. The uniqueness of
their work lies in their use of discourse cues to
extract semantic content for question generation.
They generate questions of types: why, when, give
an example, and yes/no.
In contrast to the above systems, other ap-
proaches have an intermediate step of transform-
ing input into some sort of semantic represen-
tation. Examples of this intermediate step can
be found in Yao and Zhang (2010) which uses
Minimal Recursive Semantics, and in Olney et
al. (2012) which uses concept maps. These ap-
proaches can potentially ask deeper questions due
to their focus on semantics. A novel question gen-
erator by Curto et al. (2012) leverages lexico-
syntactic patterns gleaned from the web with seed
question-answer pairs.
Another recent approach is Lindberg et al.
(2013), which used semantic role labeling to iden-
tify patterns in the source text from which ques-
tions can be generated. This work most closely
parallels our own with a few exceptions: our sys-
tem only asks questions that can be answered
from the source text, our approach is domain-
independent, and the patterns also identify the an-
swer to the question.
</bodyText>
<sectionHeader confidence="0.996284" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999233">
The system consists of a straightforward pipeline.
First, the source text is divided into sentences
which are processed by SENNA1 software, de-
</bodyText>
<footnote confidence="0.992504">
1http://ml.nec-labs.com/senna/
</footnote>
<page confidence="0.98377">
321
</page>
<bodyText confidence="0.991249860465116">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 321–326,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
scribed in (Collobert et al., 2011). SENNA pro-
vides the tokenizing, pos tagging, syntactic con-
stituency parsing and semantic role labeling used
in the system. SENNA produces separate seman-
tic role labels for each predicate in the sentence.
For each predicate and its associated semantic ar-
guments, a matcher function is called which will
return a list of patterns that match the source sen-
tence’s predicate-argument structure. Then ques-
tions are generated and stored by question type in
a question hash table.
Generation patterns specify the text, verb forms
and semantic arguments from the source sentence
to form the question. Additionally, patterns indi-
cate the semantic arguments that provide the an-
swer to the question, required fields, and filter con-
dition fields. As these patterns are matched, they
will be rejected as candidates for generation for a
particular sentence if the required arguments are
absent or if filter conditions are present. For ex-
ample, a filter for personal pronouns will prevent
a question being generated with an argument that
starts with a personal pronoun. From: It means
that the universe is expanding, we do not want to
generate a vague question such as: What does it
mean? Coreference resolution, which could help
avoid vague question generation, is discussed in
Section 5. Table 1 shows selected required and fil-
ter fields, Section 3.3 gives examples of their use.
Patterns specify whether verbs should be in-
cluded in their lexical form or as they appear in the
source text. Either form will include subsequent
particles such as: The lungs take in air. The most
common use of the verb as it appears in the sen-
tence is with the verb be, as in: What were fused
into helium nuclei? This pattern takes the copu-
lar be as it appears in the source text. However,
most patterns use the lexical form of the main verb
along with the appropriate form of the auxiliary do
(do, does, did), for the subject-auxiliary inversion
required in forming interrogatives.
</bodyText>
<subsectionHeader confidence="0.998533">
3.1 Pattern Authoring
</subsectionHeader>
<bodyText confidence="0.999918">
The system at the time of this evaluation had 42
patterns. SENNA uses the 2005 PropBank cod-
ing scheme and we followed the documentation in
(Babko-Malaya, 2005) for the patterns. The most
commonly used semantic roles are A0, A1 and A2,
as well as the ArgM modifiers. 2
</bodyText>
<footnote confidence="0.92098">
2Within PropBank, the precise roles of A0 - A6 vary by
predicate.
</footnote>
<table confidence="0.999363909090909">
Field Meaning
Ax Sentence must contain an Ax
!Ax Sentence must not contain an Ax
AxPER Ax must refer to a person
AxGER Ax must contain a gerund
AxNN Ax must contain nouns
!AxIN Ax cannot start with a preposition
!AxPRP Ax cannot start with per. pronoun
V=verb Verb must be a form of verb
!be Verb cannot be a form of be
negation Sentence cannot contain negation
</table>
<tableCaption confidence="0.8489995">
Table 1: Selected required and filter fields (Ax is a
semantic argument such as A0 or ArgM)
</tableCaption>
<subsectionHeader confidence="0.999878">
3.2 Software Tools and Source Text
</subsectionHeader>
<bodyText confidence="0.999950684210526">
The system was created using SENNA and
Python. Importing NLTK within Python provides
a simple interface to WordNet from which we de-
termine the lexical form of verbs. SENNA pro-
vided all the necessary processing of the data,
quickly, accurately and in one run.
In order to generate questions, passages were
selected from science textbooks downloaded from
www.ck12.org. Textbooks were chosen rather
than hand-crafted source material so that a more
realistic assessment of performance could be
achieved. For the experiments in this paper, we
selected three passages from the subjects of bi-
ology, chemistry, and earth science, filtering out
references to equations and figures. The passages
average around 60 sentences each, and represent
chapter sections. The average grade level is ap-
proximately grade 10 as indicated by the on-line
readability scorer read-able.com.
</bodyText>
<subsectionHeader confidence="0.996636">
3.3 Examples
</subsectionHeader>
<bodyText confidence="0.999928428571428">
Table 2 provides examples of generated questions.
The pattern that generated Question 1 requires ar-
gument A1 (underlined in Table 2) and a causation
ArgM (italicized). The pattern also filters out sen-
tences with A0 or A2. The patterns are designed
to match only the arguments used as part of the
question or the answer, in order to prevent over
generation of questions. The system inserted the
correct forms of release and do, and ignored the
phrase As this occurs since it is not part of the se-
mantic argument.
The pattern that generated Question 2 requires
A0, A1 and a verb whose lexical form is mean
(V=mean in Table 1). In this pattern, A1 (itali-
</bodyText>
<page confidence="0.995407">
322
</page>
<table confidence="0.899385466666667">
Question 1: Why did potential energy release?
Answer: because the new bonds have lower potential energy than the original bonds
Source: As this occurs, potential energy is released because the new bonds have lower potential
energy than the original bonds.
Question 2: What does an increased surface area to volume ratio indicate?
Answer: increased exposure to the environment
Source: An increased surface area to volume ratio means increased exposure to the environment.
Question 3: What is another term for electrically neutral particles?
Answer: neutrons
Source: The nucleus contains positively charged particles called protons and
electrically neutral particles called neutrons.
Question 4: What happens if you continue to move atoms closer and closer together?
Answer: eventually the two nuclei will begin to repel each other
Source: If you continue to move atoms closer and closer together, eventually the two nuclei will
begin to repel each other.
</table>
<tableCaption confidence="0.999071">
Table 2: Selected generated questions with source sentences
</tableCaption>
<bodyText confidence="0.998843">
cized) forms the answer and A0 (underlined) be-
comes part of the question along with the appro-
priate form of do. This pattern supplies the word
indicate instead of the source text’s mean which
broadens the question context.
Question 3 is from the source sentence’s 3rd
predicate-argument set because this matched the
pattern requirements: A1, A2, V=call. The answer
is the text from the A2 argument. The ability to
generate questions from any predicate-argument
set means that sentence simplification is not re-
quired as a preprocessing step, and that the sen-
tence can match multiple patterns. For example,
this sentence could also match patterns to gener-
ate questions such as: What are positively charged
particles called? or Describe the nucleus.
Question 4 requires A1 and an ArgM that in-
cludes the discourse cue if. The ArgM (under-
lined) becomes part of the question, while the rest
of the source sentence forms the answer. This pat-
tern also requires that ArgM contain nouns (AxNN
from Table 1), which helps filter vague questions.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999882222222222">
This paper focuses on evaluating generated ques-
tions primarily in terms of their linguistic quality,
as did Heilman and Smith (2010a). In a related
work (Mazidi and Nielsen, 2014) we evaluated
the quality of the questions and answers from a
pedagogical perspective, and our approach outper-
formed comparable systems in both linguistic and
pedagogical evaluations. However, the task here
is to explore the linguistic quality of generated
questions. The annotators are university students
who are science majors and native speakers of En-
glish. Annotators were given instructions to read a
paragraph, then the questions based on that para-
graph. Two annotators evaluated each set of ques-
tions using Likert-scale ratings from 1 to 5, where
5 is the best rating, for grammaticality, clarity, and
naturalness. The average inter-annotator agree-
ment, allowing a difference of one between the
annotators’ ratings was 88% and Pearson’s r=0.47
was statistically significant (p&lt;0.001), suggesting
a high correlation and agreement between annota-
tors. The two annotator ratings were averaged for
all the evaluations reported here.
We present results on three linguistic evalua-
tions: (1) evaluation of our generated questions,
(2) comparison of our generated questions with
those from Heilman and Smith’s question gener-
ator, and (3) comparison of our generated ques-
tions with those from Lindberg, Popowich, Nesbit
and Winne. We compared our system to the H&amp;S
and LPN&amp;W systems because they produce ques-
tions that are the most similar to ours, and for the
same purpose: reading comprehension reinforce-
ment. The Heilman and Smith system is available
online;3 Lindberg graciously shared his code with
us.
</bodyText>
<subsectionHeader confidence="0.997439">
4.1 Evaluation of our Generated Questions
</subsectionHeader>
<bodyText confidence="0.99949">
This evaluation was conducted with one file
(Chemistry: Bonds) which had 59 sentences, from
which the system generated 142 questions. The
</bodyText>
<footnote confidence="0.964654">
3http://www.ark.cs.cmu.edu/mheilman/questions/
</footnote>
<page confidence="0.998162">
323
</page>
<bodyText confidence="0.996951">
purpose of this evaluation was to determine if any
patterns consistently produce poor questions. The
average linguistics score per pattern in this evalu-
ation was 5.0 to 4.18. We were also interested to
know if first predicates make better questions than
later ones. The average score by predicate position
is shown in Table 3. Note that the Rating column
gives the average of the grammaticality, clarity and
naturalness scores.
</bodyText>
<table confidence="0.9990014">
Predicate Questions Rating
First 58 4.7
Second 35 4.7
Third 23 4.5
Higher 26 4.6
</table>
<tableCaption confidence="0.999937">
Table 3: Predicate depth and question quality
</tableCaption>
<bodyText confidence="0.999482857142857">
Based on this sample of questions there is
no significant difference in linguistic scores for
questions generated at various predicate positions.
Some question generation systems simplify com-
plex sentences in initial stages of their system. In
our approach this is unnecessary, and simplifying
could miss many valid questions.
</bodyText>
<subsectionHeader confidence="0.99934">
4.2 Comparison with Heilman and Smith
</subsectionHeader>
<bodyText confidence="0.99999105">
This task utilized a file (Biology: the body) with
56 source sentences from which our system gener-
ated 102 questions. The Heilman and Smith sys-
tem, as they describe it, takes an over-generate and
rank approach. We only took questions that scored
a 2.0 or better with their ranking system,4 which
resulted in less than 27% of their top questions.
In all, 84 of their questions were evaluated. The
questions again were presented with accompany-
ing paragraphs of the source text. Questions from
the two systems were randomly intermingled. An-
notators gave 1 - 5 scores for each category of
grammaticality, clarity and naturalness.
As seen in Table 4, our results represent a 44%
reduction in the error rate relative to Heilman and
Smith on the average rating over all metrics, and
as high as 61% reduction in the error rate on gram-
maticality judgments. The error reduction calcu-
lation is shown below. Note that rating* is the
maximum rating of 5.0.
</bodyText>
<footnote confidence="0.84407425">
ratingsystem2 − ratingsystem1
rating* − ratingsystem1
4In our experiments, their rankings ranged from very
small negative numbers to 3.0.
</footnote>
<table confidence="0.99796525">
System Gram Clarity Natural Avg
H&amp;S 4.38 4.13 3.94 4.15
M&amp;N 4.76 4.26 4.53 4.52
Err. Red. 61% 15% 56% 44%
</table>
<tableCaption confidence="0.999625">
Table 4: Comparison with Heilman and Smith
</tableCaption>
<table confidence="0.99965125">
System Gram Clarity Natural Avg
LPN&amp;W 4.57 4.56 4.55 4.57
M&amp;N 4.80 4.69 4.78 4.76
Err. Red. 54% 30% 51% 44%
</table>
<tableCaption confidence="0.999977">
Table 5: Comparison with Lindberg et al.
</tableCaption>
<subsectionHeader confidence="0.998586">
4.3 Comparison with Lindberg et al.
</subsectionHeader>
<bodyText confidence="0.999968315789474">
For a comparison with the Lindberg, Popowich,
Nesbit and Winne system we used a file (Earth
science: weather fronts) that seemed most sim-
ilar to the text files for which their system was
designed. The file has 93 sentences and our sys-
tem generated 184 questions; the LPN&amp;W sys-
tem generated roughly 4 times as many questions.
From each system, 100 questions were randomly
selected, making sure that the LPN&amp;W questions
did not include questions generated from domain-
specific templates such as: Summarize the influ-
ence of the maximum amount on the environment.
The phrases Summarize the influence of and on
the environment are part of a domain-specific tem-
plate. The comparison results are shown in Table
5. Interestingly, our system again achieved a 44%
reduction in the error rate when averaging over all
metrics, just as it did in the Heilman and Smith
comparison.
</bodyText>
<sectionHeader confidence="0.981089" genericHeader="method">
5 Linguistic Challenges
</sectionHeader>
<bodyText confidence="0.999612">
Natural language generation faces many linguistic
challenges. Here we briefly describe three chal-
lenges: negation detection, coreference resolution,
and verb forms.
</bodyText>
<subsectionHeader confidence="0.994532">
5.1 Negation Detection
</subsectionHeader>
<bodyText confidence="0.999282">
Negation detection is a complicated task because
negation can occur at the word, phrase or clause
level, and because there are subtle shades of nega-
tion between definite positive and negative polar-
ities (Blanco and Moldovan, 2011). For our pur-
poses we focused on negation as identified by the
NEG label in SENNA which identified not in verb
phrases. We have left for future work the task of
</bodyText>
<equation confidence="0.891057">
× 100.0 (1)
</equation>
<page confidence="0.994779">
324
</page>
<bodyText confidence="0.9915824">
identifying other negative indicators, which occa-
sionally does lead to poor question/answer quality
as in the following:
Source sentence: In Darwin’s time and to-
day, many people incorrectly believe that evolu-
tion means humans come from monkeys.
Question: What does evolution mean?
Answer: that humans come from monkeys
The negation in the word incorrectly is not iden-
tified.
</bodyText>
<subsectionHeader confidence="0.999829">
5.2 Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.999541304347826">
Currently, our system does not use any type of
coreference resolution. Experiments with existing
coreference software performed well only for per-
sonal pronouns, which occur infrequently in most
expository text. Not having coreference resolution
leads to vague questions, some of which can be
filtered as discussed previously. However, further
work on filters is needed to avoid questions such
as:
Source sentence: Air cools when it comes into
contact with a cold surface or when it rises.
Question: What happens when it comes into
contact with a cold surface or when it rises?
Heilman and Smith chose to filter out ques-
tions with personal pronouns, possessive pronouns
and noun phrases composed simply of determiners
such as those. Lindberg et al. used the emPronoun
system from Charniak and Elsner, which only han-
dles personal pronouns. Since current state-of-the-
art systems do not deal well with relative and pos-
sessive pronouns, this will continue to be a limi-
tation of natural language generation systems for
the time being.
</bodyText>
<subsectionHeader confidence="0.999274">
5.3 Verb Forms
</subsectionHeader>
<bodyText confidence="0.999478485714286">
Since our focus is on expository text, system pat-
terns deal primarily with the present and simple
past tenses. Some patterns look for modals and so
can handle future tense:
Source sentence: If you continue to move
atoms closer and closer together, eventually the
two nuclei will begin to repel each other.
Question: Discuss what the two nuclei will re-
pel.
Light verbs pose complications in NLG because
they are highly idiosyncratic and subject to syn-
tactic variability (Sag et al., 2002). Light verbs
can either carry semantic meaning (take your pass-
port) or can be bleached of semantic content when
combined with other words as in: make a deci-
sion, have a drink, take a walk. Common English
verbs that can be light verbs include give, have,
make, take. Handling these constructions as well
as other multi-word expressions may require both
rule-based and statistical approaches. The catena-
tive construction also potentially adds complexity
(Huddleston and Pullum, 2005), as shown in this
example: As the universe expanded, it became less
dense and began to cool. Care must be taken not
to generate questions based on one predicate in the
catenative construction.
We are also hindered at times by the perfor-
mance of the part of speech tagging and parsing
software. The most common error observed was
confusion between the noun and verb roles of a
word. For example in: Plant roots and bacterial
decay use carbon dioxide in the process of respira-
tion, the word use was classified as NN, leaving no
predicate and no semantic role labels in this sen-
tence.
</bodyText>
<sectionHeader confidence="0.999534" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999973045454546">
Roediger and Pyc (2012) advocate assisting stu-
dents in building a strong knowledge base be-
cause creative discoveries are unlikely to occur
when students do not have a sound set of facts
and principles at their command. To that end, au-
tomatic question generation systems can facilitate
the learning process by alternating passages of text
with questions that reinforce the material learned.
We have demonstrated a semantic approach to
automatic question generation that outperforms
similar systems. We evaluated our system on
text extracted from open domain STEM textbooks
rather than hand-crafted text, showing the robust-
ness of our approach. Our system achieved a 44%
reduction in the error rate relative to both the Heil-
man and Smith, and the Lindberg et al. system on
the average over all metrics. The results shows are
statistically significant (p&lt;0.001). Our question
generator can be used for self-study or tutoring,
or by teachers to generate questions for classroom
discussion or assessment. Finally, we addressed
linguistic challenges to question generation.
</bodyText>
<sectionHeader confidence="0.998232" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.84482025">
This research was supported by the Institute of
Education Sciences, U.S. Dept. of Ed., Grant
R305A120808 to UNT. The opinions expressed
are those of the authors.
</bodyText>
<page confidence="0.998388">
325
</page>
<sectionHeader confidence="0.990083" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999318183908046">
Agarwal, M., Shah, R., and Mannem, P. 2011. Auto-
matic question generation using discourse cues. In
Proceedings of the 6th Workshop on Innovative Use
of NLP for Building Educational Applications, As-
sociation for Computational Linguistics.
Babko-Malaya, O. 2005. Propbank annotation guide-
lines. URL: http://verbs.colorado.edu
Blanco, E., and Moldovan, D. 2011. Some issues on
detecting negation from text. In FLAIRS Confer-
ence.
Boyer, K. E., and Piwek, P., editors. 2010. In Proceed-
ings of QG2010: The Third Workshop on Question
Generation. Pittsburgh: questiongeneration.org
Carpenter, S. 2012. Testing enhances the transfer of
learning. In Current directions in psychological sci-
ence, 21(5), 279-283.
Charniak, E., and Elsner, M. 2009. EM works for pro-
noun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Collobert, R., Weston, J., Bottou, L., Karlen, M.,
Kavukcuoglu, K., &amp; Kuksa, P. 2011. Natural lan-
guage processing (almost) from scratch. The Jour-
nal of Machine Learning Research, 12, 2493-2537.
Curto, S., Mendes, A., and Coheur, L. 2012. Ques-
tion generation based on lexico-syntactic patterns
learned from the web. Dialogue &amp; Discourse, 3(2),
147-175.
Heilman, M., and Smith, N. 2009. Question gener-
ation via overgenerating transformations and rank-
ing. Technical Report CMU-LTI-09-013, Language
Technologies Institute, Carnegie-Mellon University.
Heilman, M., and Smith, N. 2010a. Good ques-
tion! statistical ranking for question generation. In
Proceedings of NAACL/HLT 2010. Association for
Computational Linguistics.
Heilman, M., and Smith, N. 2010b. Rating computer-
generated questions with Mechanical Turk. In Pro-
ceedings of the NAACL-HLT Workshop on Creating
Speech and Language Data with Amazon’s Mechan-
ical Turk. Association for Computational Linguis-
tics.
Huddleston, R. and Pullum, G. 2005. A Student’s In-
troduction to English Grammar, Cambridge Univer-
sity Press.
Lindberg, D., Popowich, F., Nesbit, J., and Winne, P.
2013. Generating natural language questions to sup-
port learning on-line. In Proceedings of the 14th Eu-
ropean Workshop on Natural Language Generation,
(2013): 105-114.
Mannem, P., Prasad, R. and Joshi, A. 2010. Question
generation from paragraphs at UPenn: QGSTEC
system description. In Proceedings of QG2010: The
Third Workshop on Question Generation.
Mazidi, K. and Nielsen, R.D. 2014. Pedagogical eval-
uation of automatically generated questions. In In-
telligent Tutoring Systems. LNCS 8474, Springer In-
ternational Publishing Switzerland.
McDaniel, M. A., Anderson, J. L., Derbish, M. H., and
Morrisette, N. 2007. Testing the testing effect in the
classroom. European Journal of Cognitive Psychol-
ogy, 19(4-5), 494-513.
Olney, A., Graesser, A., and Person, N. 2012. Ques-
tion generation from concept maps. Dialogue &amp;
Discourse, 3(2), 75-99.
Roediger III, H. L., and Pyc, M. 2012. Inexpensive
techniques to improve education: Applying cog-
nitive psychology to enhance educational practice.
Journal of Applied Research in Memory and Cogni-
tion, 1.4: 242-248.
Sag, I. A., Baldwin, T., Bond, F., Copestake, A., and
Flickinger, D. 2002. Multiword expressions: A pain
in the neck for NLP. In Computational Linguistics
and Intelligent Text Processing, (pp. 1-15). Springer
Berlin Heidelberg.
Sternberg, R. J., &amp; Grigorenko, E. L. 2003. Teach-
ing for successful intelligence: Principles, proce-
dures, and practices. Journal for the Education of
the Gifted, 27, 207-228.
Wolfe, J. 1976. Automatic question generation from
text-an aid to independent study. In Proceedings of
ACM SIGCSE-SIGCUE.
Yao, X., and Zhang, Y. 2010. Question generation
with minimal recursion semantics. In Proceedings
of QG2010: The Third Workshop on Question Gen-
eration.
</reference>
<page confidence="0.999088">
326
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.575676">
<title confidence="0.999899">Linguistic Considerations in Automatic Question Generation</title>
<author confidence="0.983937">Karen</author>
<affiliation confidence="0.867889666666667">HiLT University of North Denton TX 76207,</affiliation>
<email confidence="0.992891">KarenMazidi@my.unt.edu</email>
<abstract confidence="0.995324684210526">As students read expository text, comprehension is improved by pausing to answer questions that reinforce the material. We describe an automatic question generator that uses semantic pattern recognition to create questions of varying depth and type for self-study or tutoring. Throughout, we explore how linguistic considerations inform system design. In the described system, semantic role labels of source sentences are used in a domain-independent manner to generate both questions and answers related to the source sentence. Evaluation results show a 44% reduction in the error rate relative to the best prior systems, averaging over all metrics, and up to 61% reduction in the error rate on grammaticality judgments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Agarwal</author>
<author>R Shah</author>
<author>P Mannem</author>
</authors>
<title>Automatic question generation using discourse cues.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications, Association for Computational Linguistics.</booktitle>
<marker>Agarwal, Shah, Mannem, 2011</marker>
<rawString>Agarwal, M., Shah, R., and Mannem, P. 2011. Automatic question generation using discourse cues. In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Babko-Malaya</author>
</authors>
<title>Propbank annotation guidelines.</title>
<date>2005</date>
<note>URL: http://verbs.colorado.edu</note>
<contexts>
<context position="5943" citStr="Babko-Malaya, 2005" startWordPosition="939" endWordPosition="940">particles such as: The lungs take in air. The most common use of the verb as it appears in the sentence is with the verb be, as in: What were fused into helium nuclei? This pattern takes the copular be as it appears in the source text. However, most patterns use the lexical form of the main verb along with the appropriate form of the auxiliary do (do, does, did), for the subject-auxiliary inversion required in forming interrogatives. 3.1 Pattern Authoring The system at the time of this evaluation had 42 patterns. SENNA uses the 2005 PropBank coding scheme and we followed the documentation in (Babko-Malaya, 2005) for the patterns. The most commonly used semantic roles are A0, A1 and A2, as well as the ArgM modifiers. 2 2Within PropBank, the precise roles of A0 - A6 vary by predicate. Field Meaning Ax Sentence must contain an Ax !Ax Sentence must not contain an Ax AxPER Ax must refer to a person AxGER Ax must contain a gerund AxNN Ax must contain nouns !AxIN Ax cannot start with a preposition !AxPRP Ax cannot start with per. pronoun V=verb Verb must be a form of verb !be Verb cannot be a form of be negation Sentence cannot contain negation Table 1: Selected required and filter fields (Ax is a semantic </context>
</contexts>
<marker>Babko-Malaya, 2005</marker>
<rawString>Babko-Malaya, O. 2005. Propbank annotation guidelines. URL: http://verbs.colorado.edu</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Blanco</author>
<author>D Moldovan</author>
</authors>
<title>Some issues on detecting negation from text.</title>
<date>2011</date>
<booktitle>In FLAIRS Conference.</booktitle>
<contexts>
<context position="15757" citStr="Blanco and Moldovan, 2011" startWordPosition="2524" endWordPosition="2527">s are shown in Table 5. Interestingly, our system again achieved a 44% reduction in the error rate when averaging over all metrics, just as it did in the Heilman and Smith comparison. 5 Linguistic Challenges Natural language generation faces many linguistic challenges. Here we briefly describe three challenges: negation detection, coreference resolution, and verb forms. 5.1 Negation Detection Negation detection is a complicated task because negation can occur at the word, phrase or clause level, and because there are subtle shades of negation between definite positive and negative polarities (Blanco and Moldovan, 2011). For our purposes we focused on negation as identified by the NEG label in SENNA which identified not in verb phrases. We have left for future work the task of × 100.0 (1) 324 identifying other negative indicators, which occasionally does lead to poor question/answer quality as in the following: Source sentence: In Darwin’s time and today, many people incorrectly believe that evolution means humans come from monkeys. Question: What does evolution mean? Answer: that humans come from monkeys The negation in the word incorrectly is not identified. 5.2 Coreference Resolution Currently, our system</context>
</contexts>
<marker>Blanco, Moldovan, 2011</marker>
<rawString>Blanco, E., and Moldovan, D. 2011. Some issues on detecting negation from text. In FLAIRS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Boyer</author>
<author>P Piwek</author>
<author>editors</author>
</authors>
<date>2010</date>
<booktitle>In Proceedings of QG2010: The Third Workshop on Question Generation.</booktitle>
<location>Pittsburgh: questiongeneration.org</location>
<marker>Boyer, Piwek, editors, 2010</marker>
<rawString>Boyer, K. E., and Piwek, P., editors. 2010. In Proceedings of QG2010: The Third Workshop on Question Generation. Pittsburgh: questiongeneration.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Carpenter</author>
</authors>
<title>Testing enhances the transfer of learning.</title>
<date>2012</date>
<booktitle>In Current directions in psychological science,</booktitle>
<volume>21</volume>
<issue>5</issue>
<pages>279--283</pages>
<contexts>
<context position="1107" citStr="Carpenter, 2012" startWordPosition="164" endWordPosition="166">form system design. In the described system, semantic role labels of source sentences are used in a domain-independent manner to generate both questions and answers related to the source sentence. Evaluation results show a 44% reduction in the error rate relative to the best prior systems, averaging over all metrics, and up to 61% reduction in the error rate on grammaticality judgments. 1 Introduction Studies of student learning show that answering questions increases depth of student learning, facilitates transfer learning, and improves students’ retention of material (McDaniel et al., 2007; Carpenter, 2012; Roediger and Pyc, 2012). The aim of this work is to automatically generate questions for such pedagogical purposes. 2 Related Work Approaches to automatic question generation from text span nearly four decades. The vast majority of systems generate questions by selecting one sentence at a time, extracting portions of the source sentence, then applying transformation rules or patterns in order to construct a question. A well-known early work is Wolfe’s AUTOQUEST (Wolfe, 1976), a syntactic pattern matching system. A recent approach from Heilman and Smith (2009, 2010) uses syntactic parsing and</context>
</contexts>
<marker>Carpenter, 2012</marker>
<rawString>Carpenter, S. 2012. Testing enhances the transfer of learning. In Current directions in psychological science, 21(5), 279-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Charniak, E., and Elsner, M. 2009. EM works for pronoun anaphora resolution. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2493--2537</pages>
<contexts>
<context position="3810" citStr="Collobert et al., 2011" startWordPosition="581" endWordPosition="584">th a few exceptions: our system only asks questions that can be answered from the source text, our approach is domainindependent, and the patterns also identify the answer to the question. 3 Approach The system consists of a straightforward pipeline. First, the source text is divided into sentences which are processed by SENNA1 software, de1http://ml.nec-labs.com/senna/ 321 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 321–326, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics scribed in (Collobert et al., 2011). SENNA provides the tokenizing, pos tagging, syntactic constituency parsing and semantic role labeling used in the system. SENNA produces separate semantic role labels for each predicate in the sentence. For each predicate and its associated semantic arguments, a matcher function is called which will return a list of patterns that match the source sentence’s predicate-argument structure. Then questions are generated and stored by question type in a question hash table. Generation patterns specify the text, verb forms and semantic arguments from the source sentence to form the question. Additi</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp; Kuksa, P. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12, 2493-2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Curto</author>
<author>A Mendes</author>
<author>L Coheur</author>
</authors>
<title>Question generation based on lexico-syntactic patterns learned from the web.</title>
<date>2012</date>
<journal>Dialogue &amp; Discourse,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>147--175</pages>
<contexts>
<context position="2890" citStr="Curto et al. (2012)" startWordPosition="441" endWordPosition="444">of their work lies in their use of discourse cues to extract semantic content for question generation. They generate questions of types: why, when, give an example, and yes/no. In contrast to the above systems, other approaches have an intermediate step of transforming input into some sort of semantic representation. Examples of this intermediate step can be found in Yao and Zhang (2010) which uses Minimal Recursive Semantics, and in Olney et al. (2012) which uses concept maps. These approaches can potentially ask deeper questions due to their focus on semantics. A novel question generator by Curto et al. (2012) leverages lexicosyntactic patterns gleaned from the web with seed question-answer pairs. Another recent approach is Lindberg et al. (2013), which used semantic role labeling to identify patterns in the source text from which questions can be generated. This work most closely parallels our own with a few exceptions: our system only asks questions that can be answered from the source text, our approach is domainindependent, and the patterns also identify the answer to the question. 3 Approach The system consists of a straightforward pipeline. First, the source text is divided into sentences whi</context>
</contexts>
<marker>Curto, Mendes, Coheur, 2012</marker>
<rawString>Curto, S., Mendes, A., and Coheur, L. 2012. Question generation based on lexico-syntactic patterns learned from the web. Dialogue &amp; Discourse, 3(2), 147-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N Smith</author>
</authors>
<title>Question generation via overgenerating transformations and ranking.</title>
<date>2009</date>
<tech>Technical Report CMU-LTI-09-013,</tech>
<institution>Language Technologies Institute, Carnegie-Mellon University.</institution>
<contexts>
<context position="1673" citStr="Heilman and Smith (2009" startWordPosition="252" endWordPosition="256">ntion of material (McDaniel et al., 2007; Carpenter, 2012; Roediger and Pyc, 2012). The aim of this work is to automatically generate questions for such pedagogical purposes. 2 Related Work Approaches to automatic question generation from text span nearly four decades. The vast majority of systems generate questions by selecting one sentence at a time, extracting portions of the source sentence, then applying transformation rules or patterns in order to construct a question. A well-known early work is Wolfe’s AUTOQUEST (Wolfe, 1976), a syntactic pattern matching system. A recent approach from Heilman and Smith (2009, 2010) uses syntactic parsing and transformation rules to generate questions. Rodney D. Nielsen HiLT Lab University of North Texas Denton TX 76207, USA Rodney.Nielsen@unt.edu Syntactic, sentence-level approaches outnumber other approaches as seen in the Question Generation Shared Task Evaluation Challenge 2010 (Boyer and Piwek, 2010) which received only one paragraph-level, semantic entry. Argawal, Shah and Mannem (2011) continue the paragraph-level approach using discourse cues to find appropriate text segments upon which to construct questions at a deeper conceptual level. The uniqueness of</context>
</contexts>
<marker>Heilman, Smith, 2009</marker>
<rawString>Heilman, M., and Smith, N. 2009. Question generation via overgenerating transformations and ranking. Technical Report CMU-LTI-09-013, Language Technologies Institute, Carnegie-Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N Smith</author>
</authors>
<title>Good question! statistical ranking for question generation.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL/HLT 2010. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10328" citStr="Heilman and Smith (2010" startWordPosition="1659" endWordPosition="1662">e sentence can match multiple patterns. For example, this sentence could also match patterns to generate questions such as: What are positively charged particles called? or Describe the nucleus. Question 4 requires A1 and an ArgM that includes the discourse cue if. The ArgM (underlined) becomes part of the question, while the rest of the source sentence forms the answer. This pattern also requires that ArgM contain nouns (AxNN from Table 1), which helps filter vague questions. 4 Results This paper focuses on evaluating generated questions primarily in terms of their linguistic quality, as did Heilman and Smith (2010a). In a related work (Mazidi and Nielsen, 2014) we evaluated the quality of the questions and answers from a pedagogical perspective, and our approach outperformed comparable systems in both linguistic and pedagogical evaluations. However, the task here is to explore the linguistic quality of generated questions. The annotators are university students who are science majors and native speakers of English. Annotators were given instructions to read a paragraph, then the questions based on that paragraph. Two annotators evaluated each set of questions using Likert-scale ratings from 1 to 5, whe</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Heilman, M., and Smith, N. 2010a. Good question! statistical ranking for question generation. In Proceedings of NAACL/HLT 2010. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N Smith</author>
</authors>
<title>Rating computergenerated questions with Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10328" citStr="Heilman and Smith (2010" startWordPosition="1659" endWordPosition="1662">e sentence can match multiple patterns. For example, this sentence could also match patterns to generate questions such as: What are positively charged particles called? or Describe the nucleus. Question 4 requires A1 and an ArgM that includes the discourse cue if. The ArgM (underlined) becomes part of the question, while the rest of the source sentence forms the answer. This pattern also requires that ArgM contain nouns (AxNN from Table 1), which helps filter vague questions. 4 Results This paper focuses on evaluating generated questions primarily in terms of their linguistic quality, as did Heilman and Smith (2010a). In a related work (Mazidi and Nielsen, 2014) we evaluated the quality of the questions and answers from a pedagogical perspective, and our approach outperformed comparable systems in both linguistic and pedagogical evaluations. However, the task here is to explore the linguistic quality of generated questions. The annotators are university students who are science majors and native speakers of English. Annotators were given instructions to read a paragraph, then the questions based on that paragraph. Two annotators evaluated each set of questions using Likert-scale ratings from 1 to 5, whe</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Heilman, M., and Smith, N. 2010b. Rating computergenerated questions with Mechanical Turk. In Proceedings of the NAACL-HLT Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Huddleston</author>
<author>G Pullum</author>
</authors>
<title>A Student’s Introduction to English Grammar,</title>
<date>2005</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="18349" citStr="Huddleston and Pullum, 2005" startWordPosition="2946" endWordPosition="2949">ei will repel. Light verbs pose complications in NLG because they are highly idiosyncratic and subject to syntactic variability (Sag et al., 2002). Light verbs can either carry semantic meaning (take your passport) or can be bleached of semantic content when combined with other words as in: make a decision, have a drink, take a walk. Common English verbs that can be light verbs include give, have, make, take. Handling these constructions as well as other multi-word expressions may require both rule-based and statistical approaches. The catenative construction also potentially adds complexity (Huddleston and Pullum, 2005), as shown in this example: As the universe expanded, it became less dense and began to cool. Care must be taken not to generate questions based on one predicate in the catenative construction. We are also hindered at times by the performance of the part of speech tagging and parsing software. The most common error observed was confusion between the noun and verb roles of a word. For example in: Plant roots and bacterial decay use carbon dioxide in the process of respiration, the word use was classified as NN, leaving no predicate and no semantic role labels in this sentence. 6 Conclusions Roe</context>
</contexts>
<marker>Huddleston, Pullum, 2005</marker>
<rawString>Huddleston, R. and Pullum, G. 2005. A Student’s Introduction to English Grammar, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lindberg</author>
<author>F Popowich</author>
<author>J Nesbit</author>
<author>P Winne</author>
</authors>
<title>Generating natural language questions to support learning on-line.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th European Workshop on Natural Language Generation,</booktitle>
<pages>105--114</pages>
<contexts>
<context position="3029" citStr="Lindberg et al. (2013)" startWordPosition="461" endWordPosition="464"> why, when, give an example, and yes/no. In contrast to the above systems, other approaches have an intermediate step of transforming input into some sort of semantic representation. Examples of this intermediate step can be found in Yao and Zhang (2010) which uses Minimal Recursive Semantics, and in Olney et al. (2012) which uses concept maps. These approaches can potentially ask deeper questions due to their focus on semantics. A novel question generator by Curto et al. (2012) leverages lexicosyntactic patterns gleaned from the web with seed question-answer pairs. Another recent approach is Lindberg et al. (2013), which used semantic role labeling to identify patterns in the source text from which questions can be generated. This work most closely parallels our own with a few exceptions: our system only asks questions that can be answered from the source text, our approach is domainindependent, and the patterns also identify the answer to the question. 3 Approach The system consists of a straightforward pipeline. First, the source text is divided into sentences which are processed by SENNA1 software, de1http://ml.nec-labs.com/senna/ 321 Proceedings of the 52nd Annual Meeting of the Association for Com</context>
</contexts>
<marker>Lindberg, Popowich, Nesbit, Winne, 2013</marker>
<rawString>Lindberg, D., Popowich, F., Nesbit, J., and Winne, P. 2013. Generating natural language questions to support learning on-line. In Proceedings of the 14th European Workshop on Natural Language Generation, (2013): 105-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mannem</author>
<author>R Prasad</author>
<author>A Joshi</author>
</authors>
<title>Question generation from paragraphs at UPenn: QGSTEC system description.</title>
<date>2010</date>
<booktitle>In Proceedings of QG2010: The Third Workshop on Question Generation.</booktitle>
<marker>Mannem, Prasad, Joshi, 2010</marker>
<rawString>Mannem, P., Prasad, R. and Joshi, A. 2010. Question generation from paragraphs at UPenn: QGSTEC system description. In Proceedings of QG2010: The Third Workshop on Question Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Mazidi</author>
<author>R D Nielsen</author>
</authors>
<title>Pedagogical evaluation of automatically generated questions.</title>
<date>2014</date>
<booktitle>In Intelligent Tutoring Systems. LNCS 8474,</booktitle>
<publisher>Springer International Publishing Switzerland.</publisher>
<contexts>
<context position="10376" citStr="Mazidi and Nielsen, 2014" startWordPosition="1667" endWordPosition="1670">ample, this sentence could also match patterns to generate questions such as: What are positively charged particles called? or Describe the nucleus. Question 4 requires A1 and an ArgM that includes the discourse cue if. The ArgM (underlined) becomes part of the question, while the rest of the source sentence forms the answer. This pattern also requires that ArgM contain nouns (AxNN from Table 1), which helps filter vague questions. 4 Results This paper focuses on evaluating generated questions primarily in terms of their linguistic quality, as did Heilman and Smith (2010a). In a related work (Mazidi and Nielsen, 2014) we evaluated the quality of the questions and answers from a pedagogical perspective, and our approach outperformed comparable systems in both linguistic and pedagogical evaluations. However, the task here is to explore the linguistic quality of generated questions. The annotators are university students who are science majors and native speakers of English. Annotators were given instructions to read a paragraph, then the questions based on that paragraph. Two annotators evaluated each set of questions using Likert-scale ratings from 1 to 5, where 5 is the best rating, for grammaticality, cla</context>
</contexts>
<marker>Mazidi, Nielsen, 2014</marker>
<rawString>Mazidi, K. and Nielsen, R.D. 2014. Pedagogical evaluation of automatically generated questions. In Intelligent Tutoring Systems. LNCS 8474, Springer International Publishing Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A McDaniel</author>
<author>J L Anderson</author>
<author>M H Derbish</author>
<author>N Morrisette</author>
</authors>
<title>Testing the testing effect in the classroom.</title>
<date>2007</date>
<journal>European Journal of Cognitive Psychology,</journal>
<pages>494--513</pages>
<contexts>
<context position="1090" citStr="McDaniel et al., 2007" startWordPosition="160" endWordPosition="163">istic considerations inform system design. In the described system, semantic role labels of source sentences are used in a domain-independent manner to generate both questions and answers related to the source sentence. Evaluation results show a 44% reduction in the error rate relative to the best prior systems, averaging over all metrics, and up to 61% reduction in the error rate on grammaticality judgments. 1 Introduction Studies of student learning show that answering questions increases depth of student learning, facilitates transfer learning, and improves students’ retention of material (McDaniel et al., 2007; Carpenter, 2012; Roediger and Pyc, 2012). The aim of this work is to automatically generate questions for such pedagogical purposes. 2 Related Work Approaches to automatic question generation from text span nearly four decades. The vast majority of systems generate questions by selecting one sentence at a time, extracting portions of the source sentence, then applying transformation rules or patterns in order to construct a question. A well-known early work is Wolfe’s AUTOQUEST (Wolfe, 1976), a syntactic pattern matching system. A recent approach from Heilman and Smith (2009, 2010) uses synt</context>
</contexts>
<marker>McDaniel, Anderson, Derbish, Morrisette, 2007</marker>
<rawString>McDaniel, M. A., Anderson, J. L., Derbish, M. H., and Morrisette, N. 2007. Testing the testing effect in the classroom. European Journal of Cognitive Psychology, 19(4-5), 494-513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Olney</author>
<author>A Graesser</author>
<author>N Person</author>
</authors>
<title>Question generation from concept maps.</title>
<date>2012</date>
<journal>Dialogue &amp; Discourse,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>75--99</pages>
<contexts>
<context position="2728" citStr="Olney et al. (2012)" startWordPosition="413" endWordPosition="416">he paragraph-level approach using discourse cues to find appropriate text segments upon which to construct questions at a deeper conceptual level. The uniqueness of their work lies in their use of discourse cues to extract semantic content for question generation. They generate questions of types: why, when, give an example, and yes/no. In contrast to the above systems, other approaches have an intermediate step of transforming input into some sort of semantic representation. Examples of this intermediate step can be found in Yao and Zhang (2010) which uses Minimal Recursive Semantics, and in Olney et al. (2012) which uses concept maps. These approaches can potentially ask deeper questions due to their focus on semantics. A novel question generator by Curto et al. (2012) leverages lexicosyntactic patterns gleaned from the web with seed question-answer pairs. Another recent approach is Lindberg et al. (2013), which used semantic role labeling to identify patterns in the source text from which questions can be generated. This work most closely parallels our own with a few exceptions: our system only asks questions that can be answered from the source text, our approach is domainindependent, and the pat</context>
</contexts>
<marker>Olney, Graesser, Person, 2012</marker>
<rawString>Olney, A., Graesser, A., and Person, N. 2012. Question generation from concept maps. Dialogue &amp; Discourse, 3(2), 75-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H L Roediger</author>
<author>M Pyc</author>
</authors>
<title>Inexpensive techniques to improve education: Applying cognitive psychology to enhance educational practice.</title>
<date>2012</date>
<journal>Journal of Applied Research in Memory and Cognition,</journal>
<volume>1</volume>
<pages>242--248</pages>
<contexts>
<context position="1132" citStr="Roediger and Pyc, 2012" startWordPosition="167" endWordPosition="170">n. In the described system, semantic role labels of source sentences are used in a domain-independent manner to generate both questions and answers related to the source sentence. Evaluation results show a 44% reduction in the error rate relative to the best prior systems, averaging over all metrics, and up to 61% reduction in the error rate on grammaticality judgments. 1 Introduction Studies of student learning show that answering questions increases depth of student learning, facilitates transfer learning, and improves students’ retention of material (McDaniel et al., 2007; Carpenter, 2012; Roediger and Pyc, 2012). The aim of this work is to automatically generate questions for such pedagogical purposes. 2 Related Work Approaches to automatic question generation from text span nearly four decades. The vast majority of systems generate questions by selecting one sentence at a time, extracting portions of the source sentence, then applying transformation rules or patterns in order to construct a question. A well-known early work is Wolfe’s AUTOQUEST (Wolfe, 1976), a syntactic pattern matching system. A recent approach from Heilman and Smith (2009, 2010) uses syntactic parsing and transformation rules to </context>
<context position="18969" citStr="Roediger and Pyc (2012)" startWordPosition="3056" endWordPosition="3059">05), as shown in this example: As the universe expanded, it became less dense and began to cool. Care must be taken not to generate questions based on one predicate in the catenative construction. We are also hindered at times by the performance of the part of speech tagging and parsing software. The most common error observed was confusion between the noun and verb roles of a word. For example in: Plant roots and bacterial decay use carbon dioxide in the process of respiration, the word use was classified as NN, leaving no predicate and no semantic role labels in this sentence. 6 Conclusions Roediger and Pyc (2012) advocate assisting students in building a strong knowledge base because creative discoveries are unlikely to occur when students do not have a sound set of facts and principles at their command. To that end, automatic question generation systems can facilitate the learning process by alternating passages of text with questions that reinforce the material learned. We have demonstrated a semantic approach to automatic question generation that outperforms similar systems. We evaluated our system on text extracted from open domain STEM textbooks rather than hand-crafted text, showing the robustne</context>
</contexts>
<marker>Roediger, Pyc, 2012</marker>
<rawString>Roediger III, H. L., and Pyc, M. 2012. Inexpensive techniques to improve education: Applying cognitive psychology to enhance educational practice. Journal of Applied Research in Memory and Cognition, 1.4: 242-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Sag</author>
<author>T Baldwin</author>
<author>F Bond</author>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>1--15</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="17867" citStr="Sag et al., 2002" startWordPosition="2870" endWordPosition="2873">sessive pronouns, this will continue to be a limitation of natural language generation systems for the time being. 5.3 Verb Forms Since our focus is on expository text, system patterns deal primarily with the present and simple past tenses. Some patterns look for modals and so can handle future tense: Source sentence: If you continue to move atoms closer and closer together, eventually the two nuclei will begin to repel each other. Question: Discuss what the two nuclei will repel. Light verbs pose complications in NLG because they are highly idiosyncratic and subject to syntactic variability (Sag et al., 2002). Light verbs can either carry semantic meaning (take your passport) or can be bleached of semantic content when combined with other words as in: make a decision, have a drink, take a walk. Common English verbs that can be light verbs include give, have, make, take. Handling these constructions as well as other multi-word expressions may require both rule-based and statistical approaches. The catenative construction also potentially adds complexity (Huddleston and Pullum, 2005), as shown in this example: As the universe expanded, it became less dense and began to cool. Care must be taken not t</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Sag, I. A., Baldwin, T., Bond, F., Copestake, A., and Flickinger, D. 2002. Multiword expressions: A pain in the neck for NLP. In Computational Linguistics and Intelligent Text Processing, (pp. 1-15). Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Sternberg</author>
<author>E L Grigorenko</author>
</authors>
<title>Teaching for successful intelligence: Principles, procedures, and practices.</title>
<date>2003</date>
<journal>Journal for the Education of the Gifted,</journal>
<volume>27</volume>
<pages>207--228</pages>
<marker>Sternberg, Grigorenko, 2003</marker>
<rawString>Sternberg, R. J., &amp; Grigorenko, E. L. 2003. Teaching for successful intelligence: Principles, procedures, and practices. Journal for the Education of the Gifted, 27, 207-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wolfe</author>
</authors>
<title>Automatic question generation from text-an aid to independent study.</title>
<date>1976</date>
<booktitle>In Proceedings of ACM SIGCSE-SIGCUE.</booktitle>
<contexts>
<context position="1588" citStr="Wolfe, 1976" startWordPosition="241" endWordPosition="242">udent learning, facilitates transfer learning, and improves students’ retention of material (McDaniel et al., 2007; Carpenter, 2012; Roediger and Pyc, 2012). The aim of this work is to automatically generate questions for such pedagogical purposes. 2 Related Work Approaches to automatic question generation from text span nearly four decades. The vast majority of systems generate questions by selecting one sentence at a time, extracting portions of the source sentence, then applying transformation rules or patterns in order to construct a question. A well-known early work is Wolfe’s AUTOQUEST (Wolfe, 1976), a syntactic pattern matching system. A recent approach from Heilman and Smith (2009, 2010) uses syntactic parsing and transformation rules to generate questions. Rodney D. Nielsen HiLT Lab University of North Texas Denton TX 76207, USA Rodney.Nielsen@unt.edu Syntactic, sentence-level approaches outnumber other approaches as seen in the Question Generation Shared Task Evaluation Challenge 2010 (Boyer and Piwek, 2010) which received only one paragraph-level, semantic entry. Argawal, Shah and Mannem (2011) continue the paragraph-level approach using discourse cues to find appropriate text segme</context>
</contexts>
<marker>Wolfe, 1976</marker>
<rawString>Wolfe, J. 1976. Automatic question generation from text-an aid to independent study. In Proceedings of ACM SIGCSE-SIGCUE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yao</author>
<author>Y Zhang</author>
</authors>
<title>Question generation with minimal recursion semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of QG2010: The Third Workshop on Question Generation.</booktitle>
<contexts>
<context position="2661" citStr="Yao and Zhang (2010)" startWordPosition="402" endWordPosition="405">ph-level, semantic entry. Argawal, Shah and Mannem (2011) continue the paragraph-level approach using discourse cues to find appropriate text segments upon which to construct questions at a deeper conceptual level. The uniqueness of their work lies in their use of discourse cues to extract semantic content for question generation. They generate questions of types: why, when, give an example, and yes/no. In contrast to the above systems, other approaches have an intermediate step of transforming input into some sort of semantic representation. Examples of this intermediate step can be found in Yao and Zhang (2010) which uses Minimal Recursive Semantics, and in Olney et al. (2012) which uses concept maps. These approaches can potentially ask deeper questions due to their focus on semantics. A novel question generator by Curto et al. (2012) leverages lexicosyntactic patterns gleaned from the web with seed question-answer pairs. Another recent approach is Lindberg et al. (2013), which used semantic role labeling to identify patterns in the source text from which questions can be generated. This work most closely parallels our own with a few exceptions: our system only asks questions that can be answered f</context>
</contexts>
<marker>Yao, Zhang, 2010</marker>
<rawString>Yao, X., and Zhang, Y. 2010. Question generation with minimal recursion semantics. In Proceedings of QG2010: The Third Workshop on Question Generation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>