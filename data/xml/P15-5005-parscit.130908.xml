<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.213211">
<title confidence="0.992638">
Matrix and Tensor Factorization Methods for
Natural Language Processing
</title>
<author confidence="0.809398">
Guillaume Bouchard* Jason Naradowsky# Sebastian Riedel#
Tim Rockt¨aschel# and Andreas Vlachos#
</author>
<affiliation confidence="0.77098575">
* Xerox Research Centre Europe
guillaume.bouchard@xerox.com
# Computer Science Department
University College London
</affiliation>
<email confidence="0.981023">
{j.narad, s.riedel, t.rocktaschel, a.vlachos}@cs.ucl.ac.uk
</email>
<sectionHeader confidence="0.998255" genericHeader="method">
1 Tutorial Objectives
</sectionHeader>
<bodyText confidence="0.99994325">
Tensor and matrix factorization methods have at-
tracted a lot of attention recently thanks to their
successful applications to information extraction,
knowledge base population, lexical semantics and
dependency parsing. In the first part, we will first
cover the basics of matrix and tensor factorization
theory and optimization, and then proceed to more
advanced topics involving convex surrogates and
alternative losses. In the second part we will dis-
cuss recent NLP applications of these methods and
show the connections with other popular methods
such as transductive learning, topic models and
neural networks. The aim of this tutorial is to
present in detail applied factorization methods, as
well as to introduce more recently proposed meth-
ods that are likely to be useful to NLP applications.
</bodyText>
<sectionHeader confidence="0.999145" genericHeader="method">
2 Tutorial Overview
</sectionHeader>
<subsectionHeader confidence="0.994694">
2.1 Matrix/Tensor Factorization Basics
</subsectionHeader>
<bodyText confidence="0.9999720625">
In this part, we first remind essential results on
bilinear forms, spectral representations of matri-
ces and low-rank approximation theorems, which
are often omitted in undergraduate linear algebra
courses. This includes the link between eigen-
value decomposition and singular value decompo-
sition and the trace-norm (a.k.a. nuclear norm) as
a convex surrogate of the low-rank constraint on
optimization problems. Then, an overview of the
most efficient algorithms to solve low-rank con-
strained problems is made, from the power itera-
tion method, the Lanczos algorithm and the im-
plicitly restarted Arnoldi method that is imple-
mented in the LAPACK library (Anderson et al.,
1999). We show how to interpret low-rank models
as probabilistic models (Bishop, 1999) and how
we can extend SVD algorithms that can factor-
ize non-standard matrices (i.e. with non-Gaussian
noise and missing data) using gradient descent, re-
weighted SVD or Frank-Wolfe algorithms. We
then show that combining different convex objec-
tives can be a powerful tool, and we illustrate it by
deriving the robust PCA algorithm by adding an
L1 penalty term in the objective function (Cand`es
and Recht, 2009). Furthermore, we introduce
Bayesian Personalized Ranking (BPR) for matrix
and tensor factorization which deals with implicit
feedback in ranking tasks (Rendle et al., 2009). Fi-
nally, will introduce the collective matrix factor-
ization model (Singh and Gordon, 2008) and ten-
sor extensions (Nickel et al., 2011) for relational
learning.
</bodyText>
<subsectionHeader confidence="0.999289">
2.2 Applications in NLP
</subsectionHeader>
<bodyText confidence="0.999954619047619">
In this part we will discuss recent work apply-
ing matrix/tensor factorization methods in the con-
text of NLP. We will review the Universal Schema
paradigm for knowledge base construction (Riedel
et al., 2013) which relies on matrix factoriza-
tion and BPR, as well as recent extensions of
the RESCAL tensor factorization (Nickel et al.,
2011) approach and methods of injecting logic
into the embeddings learned (Rockt¨aschel et al.,
2015). These applications will motivate the con-
nections between matrix factorization and trans-
ductive learning (Goldberg et al., 2010), as well
as tensor factorization and multi-task learning
(Romera-Paredes et al., 2013). Furthermore, we
will review work on applying matrix and tensor
factorization to sparsity reduction in syntactic de-
pendency parsing (Lei et al., 2014) and word rep-
resentation learning (Pennington et al., 2014). In
addition, we will discuss the connections between
matrix factorization, latent semantic analysis and
topic modeling (Stevens et al., 2012).
</bodyText>
<page confidence="0.93679">
16
</page>
<note confidence="0.8532725">
Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 16–18,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.948281" genericHeader="method">
3 Structure
</sectionHeader>
<listItem confidence="0.965611285714286">
Part I: Matrix/Tensor Factorization Basics (90
minutes)
• Matrix factorization basics (40 min): bilin-
ear forms, spectral representations, low rank
approximations theorems, optimization with
stochastic gradient descent, losses
• Tensor factorization basics (20 minutes):
representations,notation decompositions
(Tucker etc.)
• Advanced topics (30 minutes): convex sur-
rogates, L1 regularization, alternative losses
(ranking loss, logistic loss)
Break (15 minutes)
Part II: Applications in NLP (75 minutes)
• Information extraction, knowledge base pop-
ulation with connections to transductive
learning and multitask learning (35 minutes)
• Lexical semantics with connections to neural
networks, latent semantic analysis and topic
models (30 minutes)
• Structured prediction (10 minutes)
</listItem>
<sectionHeader confidence="0.977755" genericHeader="method">
4 About the Speakers
</sectionHeader>
<bodyText confidence="0.999763643835617">
Guillaume Bouchard is a senior researcher in
statistics and machine learning at Xerox, focusing
on statistical learning using low-rank model for
large relational databases. His research includes
text understanding, user modeling, and social me-
dia analytics. The theoretical part of his work is
related to the efficient algorithms to compute high
dimensional integrals, essential to deal with un-
certainty (missing and noisy data, latent variable
models, Bayesian inference). The main applica-
tion areas of his work includes the design of vir-
tual conversational agents, link prediction (predic-
tive algorithms for relational data), social media
monitoring and transportation analytics. His web
page is available at www.xrce.xerox.com/
people/bouchard.
Jason Naradowsky is a postdoc at the Machine
Reading group at UCL. Having previously ob-
tained a PhD at UMass Amherst under the supervi-
sion of David Smith and Mark Johnson, his current
research aims to improve natural language under-
standing by performing task-specific training of
word representations and parsing models. He is
also interested in semi-supervised learning, joint
inference, and semantic parsing. His web page is
available at http://narad.github.io/.
Sebastian Riedel is a senior lecturer at Univer-
sity College London and an Allen Distinguished
Investigator, leading the Machine Reading Lab.
Before, he was a postdoc and research scientist
with Andrew McCallum at UMass Amherst, a re-
searcher at Tokyo University and DBCLS with
Tsujii Junichi, and a PhD student with Ewan Klein
at the University of Edinburgh. He is interested
in teaching machines how to read and works at
the intersection of Natural Language Processing
(NLP) and Machine Learning, investigating vari-
ous stages of the NLP pipeline, in particular those
that require structured prediction, as well as fully
probabilistic architectures of end-to-end reading
and reasoning systems. Recently he became inter-
ested in new ways to represent textual knowledge
using low-rank embeddings and how to reason
with such representations. His web page is avail-
able at http://www.riedelcastro.org/.
Tim Rockt¨aschel is a PhD student in Sebas-
tian Riedel’s Machine Reading group at Univer-
sity College London. Before that he worked as
research assistant in the Knowledge Management
in Bioinformatics group at Humboldt-Universit¨at
zu Berlin, where he also obtained his Diploma
in Computer Science. He is broadly interested
in representation learning (e.g. matrix/tensor fac-
torization, deep learning) for NLP and automated
knowledge base completion, and how these meth-
ods can take advantage of symbolic background
knowledge. His webpage is available at http:
//rockt.github.io/.
Andreas Vlachos is postdoc at the Machine
Reading group at UCL working with Sebastian
Riedel on automated fact-checking using low-
rank factorization methods. Before that he was
a postdoc at the Natural Language and Infor-
mation Processing group at the University of
Cambridge and at the University of Wisconsin-
Madison. He is broadly interested in natural lan-
guage understanding (e.g. information extraction,
semantic parsing) and in machine learning ap-
proaches that would help us towards this goal.
He has also worked on active learning, cluster-
ing and biomedical text mining. His web page
is available at http://sites.google.com/
site/andreasvlachos/.
</bodyText>
<page confidence="0.999127">
17
</page>
<sectionHeader confidence="0.941191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990557845070422">
[Anderson et al.1999] Edward Anderson, Zhaojun Bai,
Christian Bischof, Susan Blackford, James Demmel,
Jack Dongarra, Jeremy Du Croz, Anne Greenbaum,
S Hammerling, Alan McKenney, et al. 1999. LA-
PACK Users’ guide, volume 9. SIAM.
[Bishop1999] Christopher M Bishop. 1999. Bayesian
PCA. In Advances in Neural Information Process-
ing Systems, pages 382–388.
[Cand`es and Recht2009] Emmanuel J Cand`es and Ben-
jamin Recht. 2009. Exact matrix completion via
convex optimization. Foundations of Computational
mathematics, 9(6):717–772.
[Goldberg et al.2010] Andrew Goldberg, Ben Recht,
Junming Xu, Robert Nowak, and Xiaojin Zhu.
2010. Transduction with matrix completion: Three
birds with one stone. In Advances in Neural Infor-
mation Processing Systems 23, pages 757–765.
[Lei et al.2014] Tao Lei, Yu Xin, Yuan Zhang, Regina
Barzilay, and Tommi Jaakkola. 2014. Low-rank
tensors for scoring dependency structures. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1381–
1391.
[Nickel et al.2011] Maximilian Nickel, Volker Tresp,
and Hans-Peter Kriegel. 2011. A three-way model
for collective learning on multi-relational data. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 809–816.
[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher D. Manning. 2014. Glove:
Global vectors for word representation. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing.
[Rendle et al.2009] Steffen Rendle, Christoph Freuden-
thaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. Bpr: Bayesian personalized ranking from im-
plicit feedback. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence,
pages 452–461.
[Riedel et al.2013] Sebastian Riedel, Limin Yao, Ben-
jamin M. Marlin, and Andrew McCallum. 2013.
Relation extraction with matrix factorization and
universal schemas. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Atlanta, GA.
[Rockt¨aschel et al.2015] Tim Rockt¨aschel, Sameer
Singh, and Sebastian Riedel. 2015. Injecting
Logical Background Knowledge into Embeddings
for Relation Extraction . In Proceedings of the
2015 Human Language Technology Conference of
the North American Chapter of the Association of
Computational Linguistics.
[Romera-Paredes et al.2013] Bernardino Romera-
Paredes, Hane Aung, Nadia Bianchi-Berthouze,
and Massimiliano Pontil. 2013. Multilinear
multitask learning. In Proceedings of the 30th
International Conference on Machine Learning,
pages 1444–1452.
[Singh and Gordon2008] Ajit P. Singh and Geoffrey J.
Gordon. 2008. Relational learning via collective
matrix factorization. In Proceedings of the 14th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 650–658.
[Stevens et al.2012] Keith Stevens, Philip Kegelmeyer,
David Andrzejewski, and David Buttler. 2012. Ex-
ploring topic coherence over many models and many
topics. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 952–961.
</reference>
<page confidence="0.999285">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001992">
<title confidence="0.9998285">Matrix and Tensor Factorization Methods Natural Language Processing</title>
<author confidence="0.997837">Jason</author>
<affiliation confidence="0.995902">Research Centre Science University College</affiliation>
<abstract confidence="0.990794573099415">s.riedel, t.rocktaschel, 1 Tutorial Objectives Tensor and matrix factorization methods have attracted a lot of attention recently thanks to their successful applications to information extraction, knowledge base population, lexical semantics and dependency parsing. In the first part, we will first cover the basics of matrix and tensor factorization theory and optimization, and then proceed to more advanced topics involving convex surrogates and alternative losses. In the second part we will discuss recent NLP applications of these methods and show the connections with other popular methods such as transductive learning, topic models and neural networks. The aim of this tutorial is to present in detail applied factorization methods, as well as to introduce more recently proposed methods that are likely to be useful to NLP applications. 2 Tutorial Overview 2.1 Matrix/Tensor Factorization Basics In this part, we first remind essential results on bilinear forms, spectral representations of matrices and low-rank approximation theorems, which are often omitted in undergraduate linear algebra courses. This includes the link between eigenvalue decomposition and singular value decomposition and the trace-norm (a.k.a. nuclear norm) as a convex surrogate of the low-rank constraint on optimization problems. Then, an overview of the most efficient algorithms to solve low-rank constrained problems is made, from the power iteration method, the Lanczos algorithm and the implicitly restarted Arnoldi method that is implemented in the LAPACK library (Anderson et al., 1999). We show how to interpret low-rank models as probabilistic models (Bishop, 1999) and how can extend SVD algorithms that can factorize non-standard matrices (i.e. with non-Gaussian noise and missing data) using gradient descent, reweighted SVD or Frank-Wolfe algorithms. We then show that combining different convex objectives can be a powerful tool, and we illustrate it by deriving the robust PCA algorithm by adding an term in the objective function (Cand`es and Recht, 2009). Furthermore, we introduce Bayesian Personalized Ranking (BPR) for matrix and tensor factorization which deals with implicit feedback in ranking tasks (Rendle et al., 2009). Finally, will introduce the collective matrix factorization model (Singh and Gordon, 2008) and tensor extensions (Nickel et al., 2011) for relational learning. 2.2 Applications in NLP In this part we will discuss recent work applying matrix/tensor factorization methods in the context of NLP. We will review the Universal Schema paradigm for knowledge base construction (Riedel et al., 2013) which relies on matrix factorization and BPR, as well as recent extensions of the RESCAL tensor factorization (Nickel et al., 2011) approach and methods of injecting logic into the embeddings learned (Rockt¨aschel et al., 2015). These applications will motivate the connections between matrix factorization and transductive learning (Goldberg et al., 2010), as well as tensor factorization and multi-task learning (Romera-Paredes et al., 2013). Furthermore, we will review work on applying matrix and tensor factorization to sparsity reduction in syntactic dependency parsing (Lei et al., 2014) and word representation learning (Pennington et al., 2014). In addition, we will discuss the connections between matrix factorization, latent semantic analysis and topic modeling (Stevens et al., 2012). 16 of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th pages China, July 26-31, 2015. Association for Computational Linguistics 3 Structure Matrix/Tensor Factorization Basics (90 minutes) • Matrix factorization basics (40 min): bilinear forms, spectral representations, low rank approximations theorems, optimization with stochastic gradient descent, losses • Tensor factorization basics (20 minutes): representations,notation (Tucker etc.) • Advanced topics (30 minutes): convex surrogates, L1 regularization, alternative losses (ranking loss, logistic loss) minutes) Applications in NLP (75 minutes) • Information extraction, knowledge base population with connections to transductive learning and multitask learning (35 minutes) • Lexical semantics with connections to neural networks, latent semantic analysis and topic models (30 minutes) • Structured prediction (10 minutes) 4 About the Speakers Guillaume Bouchard is a senior researcher in statistics and machine learning at Xerox, focusing on statistical learning using low-rank model for large relational databases. His research includes text understanding, user modeling, and social media analytics. The theoretical part of his work is related to the efficient algorithms to compute high dimensional integrals, essential to deal with uncertainty (missing and noisy data, latent variable models, Bayesian inference). The main application areas of his work includes the design of virtual conversational agents, link prediction (predictive algorithms for relational data), social media monitoring and transportation analytics. His web is available at Jason Naradowsky is a postdoc at the Machine Reading group at UCL. Having previously obtained a PhD at UMass Amherst under the supervision of David Smith and Mark Johnson, his current research aims to improve natural language understanding by performing task-specific training of word representations and parsing models. He is also interested in semi-supervised learning, joint inference, and semantic parsing. His web page is at Sebastian Riedel is a senior lecturer at University College London and an Allen Distinguished Investigator, leading the Machine Reading Lab. Before, he was a postdoc and research scientist with Andrew McCallum at UMass Amherst, a researcher at Tokyo University and DBCLS with Tsujii Junichi, and a PhD student with Ewan Klein at the University of Edinburgh. He is interested in teaching machines how to read and works at the intersection of Natural Language Processing (NLP) and Machine Learning, investigating various stages of the NLP pipeline, in particular those that require structured prediction, as well as fully probabilistic architectures of end-to-end reading and reasoning systems. Recently he became interested in new ways to represent textual knowledge using low-rank embeddings and how to reason with such representations. His web page is availat Tim Rockt¨aschel is a PhD student in Sebastian Riedel’s Machine Reading group at University College London. Before that he worked as research assistant in the Knowledge Management in Bioinformatics group at Humboldt-Universit¨at zu Berlin, where he also obtained his Diploma in Computer Science. He is broadly interested in representation learning (e.g. matrix/tensor factorization, deep learning) for NLP and automated knowledge base completion, and how these methods can take advantage of symbolic background His webpage is available at Andreas Vlachos is postdoc at the Machine Reading group at UCL working with Sebastian Riedel on automated fact-checking using lowrank factorization methods. Before that he was a postdoc at the Natural Language and Information Processing group at the University of Cambridge and at the University of Wisconsin- Madison. He is broadly interested in natural language understanding (e.g. information extraction, semantic parsing) and in machine learning approaches that would help us towards this goal. He has also worked on active learning, clustering and biomedical text mining. His web page available at 17</abstract>
<title confidence="0.948348">References</title>
<address confidence="0.533327">Hammerling, Alan McKenney, et al. 1999. LA-</address>
<note confidence="0.887299126984127">Users’ volume 9. SIAM. [Bishop1999] Christopher M Bishop. 1999. Bayesian In in Neural Information Processpages 382–388. [Cand`es and Recht2009] Emmanuel J Cand`es and Benjamin Recht. 2009. Exact matrix completion via optimization. of Computational 9(6):717–772. [Goldberg et al.2010] Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, and Xiaojin Zhu. 2010. Transduction with matrix completion: Three with one stone. In in Neural Infor- Processing Systems pages 757–765. [Lei et al.2014] Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Assofor Computational pages 1381– 1391. [Nickel et al.2011] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on Learning pages 809–816. [Pennington et al.2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods Natural Language [Rendle et al.2009] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. Bpr: Bayesian personalized ranking from imfeedback. In of the Twenty-Fifth on Uncertainty in Artificial pages 452–461. [Riedel et al.2013] Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and Andrew McCallum. 2013. Relation extraction with matrix factorization and schemas. In of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- Atlanta, GA. [Rockt¨aschel et al.2015] Tim Rockt¨aschel, Sameer Singh, and Sebastian Riedel. 2015. Injecting Logical Background Knowledge into Embeddings Relation Extraction . In of the 2015 Human Language Technology Conference of the North American Chapter of the Association of [Romera-Paredes et al.2013] Bernardino Romera- Paredes, Hane Aung, Nadia Bianchi-Berthouze, and Massimiliano Pontil. 2013. learning. In of the 30th Conference on Machine pages 1444–1452. [Singh and Gordon2008] Ajit P. Singh and Geoffrey J. Gordon. 2008. Relational learning via collective factorization. In of the 14th ACM SIGKDD International Conference on Knowl- Discovery and Data pages 650–658. [Stevens et al.2012] Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. 2012. Exploring topic coherence over many models and many In of the 2012 Joint Confer-</note>
<title confidence="0.733149">ence on Empirical Methods in Natural Language Processing and Computational Natural Language</title>
<note confidence="0.6343875">pages 952–961. 18</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Edward Anderson</author>
<author>Zhaojun Bai</author>
<author>Christian Bischof</author>
<author>Susan Blackford</author>
<author>James Demmel</author>
<author>Jack Dongarra</author>
<author>Jeremy Du Croz</author>
<author>Anne Greenbaum</author>
<author>S Hammerling</author>
<author>Alan McKenney</author>
</authors>
<date>1999</date>
<journal>LAPACK Users’ guide,</journal>
<volume>9</volume>
<publisher>SIAM.</publisher>
<marker>[Anderson et al.1999]</marker>
<rawString>Edward Anderson, Zhaojun Bai, Christian Bischof, Susan Blackford, James Demmel, Jack Dongarra, Jeremy Du Croz, Anne Greenbaum, S Hammerling, Alan McKenney, et al. 1999. LAPACK Users’ guide, volume 9. SIAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Bayesian PCA.</title>
<date>1999</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>382--388</pages>
<marker>[Bishop1999]</marker>
<rawString>Christopher M Bishop. 1999. Bayesian PCA. In Advances in Neural Information Processing Systems, pages 382–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel J Cand`es</author>
<author>Benjamin Recht</author>
</authors>
<title>Exact matrix completion via convex optimization.</title>
<date>2009</date>
<booktitle>Foundations of Computational mathematics,</booktitle>
<pages>9--6</pages>
<marker>[Cand`es and Recht2009]</marker>
<rawString>Emmanuel J Cand`es and Benjamin Recht. 2009. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717–772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Goldberg</author>
<author>Ben Recht</author>
<author>Junming Xu</author>
<author>Robert Nowak</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Transduction with matrix completion: Three birds with one stone.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems 23,</booktitle>
<pages>757--765</pages>
<marker>[Goldberg et al.2010]</marker>
<rawString>Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, and Xiaojin Zhu. 2010. Transduction with matrix completion: Three birds with one stone. In Advances in Neural Information Processing Systems 23, pages 757–765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-rank tensors for scoring dependency structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1381--1391</pages>
<marker>[Lei et al.2014]</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1381– 1391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Nickel</author>
<author>Volker Tresp</author>
<author>Hans-Peter Kriegel</author>
</authors>
<title>A three-way model for collective learning on multi-relational data.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>809--816</pages>
<marker>[Nickel et al.2011]</marker>
<rawString>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 809–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>[Pennington et al.2014]</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Rendle</author>
<author>Christoph Freudenthaler</author>
<author>Zeno Gantner</author>
<author>Lars Schmidt-Thieme</author>
</authors>
<title>Bpr: Bayesian personalized ranking from implicit feedback.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>452--461</pages>
<marker>[Rendle et al.2009]</marker>
<rawString>Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. Bpr: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 452–461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Benjamin M Marlin</author>
<author>Andrew McCallum</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Atlanta, GA.</location>
<marker>[Riedel et al.2013]</marker>
<rawString>Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and Andrew McCallum. 2013. Relation extraction with matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Rockt¨aschel</author>
<author>Sameer Singh</author>
<author>Sebastian Riedel</author>
</authors>
<title>Injecting Logical Background Knowledge into Embeddings for Relation Extraction .</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics.</booktitle>
<marker>[Rockt¨aschel et al.2015]</marker>
<rawString>Tim Rockt¨aschel, Sameer Singh, and Sebastian Riedel. 2015. Injecting Logical Background Knowledge into Embeddings for Relation Extraction . In Proceedings of the 2015 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardino RomeraParedes</author>
<author>Hane Aung</author>
<author>Nadia Bianchi-Berthouze</author>
<author>Massimiliano Pontil</author>
</authors>
<title>Multilinear multitask learning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 30th International Conference on Machine Learning,</booktitle>
<pages>1444--1452</pages>
<marker>[Romera-Paredes et al.2013]</marker>
<rawString>Bernardino RomeraParedes, Hane Aung, Nadia Bianchi-Berthouze, and Massimiliano Pontil. 2013. Multilinear multitask learning. In Proceedings of the 30th International Conference on Machine Learning, pages 1444–1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ajit P Singh</author>
<author>Geoffrey J Gordon</author>
</authors>
<title>Relational learning via collective matrix factorization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>650--658</pages>
<marker>[Singh and Gordon2008]</marker>
<rawString>Ajit P. Singh and Geoffrey J. Gordon. 2008. Relational learning via collective matrix factorization. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 650–658.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Stevens</author>
<author>Philip Kegelmeyer</author>
<author>David Andrzejewski</author>
<author>David Buttler</author>
</authors>
<title>Exploring topic coherence over many models and many topics.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>952--961</pages>
<marker>[Stevens et al.2012]</marker>
<rawString>Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. 2012. Exploring topic coherence over many models and many topics. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 952–961.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>