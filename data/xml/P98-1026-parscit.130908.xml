<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000191">
<title confidence="0.9936975">
Separating Surface Order and Syntactic Relations
in a Dependency Grammar
</title>
<author confidence="0.957404">
Norbert Broker
</author>
<affiliation confidence="0.841643">
Universitat Stuttgart
</affiliation>
<address confidence="0.844287">
Azenbergstr. 12
D-70174 Stuttgart
</address>
<email confidence="0.997802">
NOBI@IMS.UNI-STUTTGART.DE
</email>
<sectionHeader confidence="0.995606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874692307692">
This paper proposes decoupling the dependency
tree from word order, such that surface ordering
is not determined by traversing the dependency
tree. We develop the notion of a word order do-
main structure, which is linked but structurally
dissimilar to the syntactic dependency tree. The
proposal results in a lexicalized, declarative, and
formally precise description of word order; fea-
tures which lack previous proposals for depen-
dency grammars. Contrary to other lexicalized
approaches to word order, our proposal does not
require lexical ambiguities for ordering alterna-
tives.
</bodyText>
<sectionHeader confidence="0.997905" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978">
Recently, the concept of valency has gained con-
siderable attention. Not only do all linguis-
tic theories refer to some reformulation of the
traditional notion of valency (in the form of 0-
grid, subcategorization list, argument list, or ex-
tended domain of locality); there is a growing
number of parsers based on binary relations be-
tween words (Eisner, 1997; Maruyama, 1990).
Given this interest in the valency concept,
and the fact that word order is one of the
main difference between phrase-structure based
approaches (henceforth PSG) and dependency
grammar (DG), it is valid to ask whether DG
can capture word order phenomena without re-
course to phrasal nodes, traces, slashed cate-
gories, etc. A very early result on the weak
generative equivalence of context-free grammars
and DGs suggested that DGs are incapable of
describing surface word order (Gaifman, 1965).
This result has recently been critizised to apply
only to impoverished DGs which do not properly
represent formally the expressivity of contempo-
rary DG variants (Neuhaus Sz Braker, 1997).
Our position will be that dependency re-
lations are motivated semantically (Tesniere,
1959), and need not be projective (i.e., may
cross if projected onto the surface ordering). We
argue for so-called word order domains, consist-
ing of partially ordered sets of words and associ-
ated with nodes in the dependency tree. These
order domains constitute a tree defined by set in-
clusion, and surface word order is determined by
traversing this tree. A syntactic analysis there-
for consists of two linked, but dissimilar trees.
Sec. 2 will briefly review approaches to word
order in DG. In Sec. 3, word order domains will
be defined, and Sec. 4 introduces a modal logic
to describe dependency structures. Sec. 5 ap-
plies our approach to the German clause and
Sec. 6 relates it to some PSG approaches.
</bodyText>
<sectionHeader confidence="0.845601" genericHeader="method">
2 Word Order in DG
</sectionHeader>
<bodyText confidence="0.9998629">
A very brief characterization of DG is that
it recognizes only lexical, not phrasal nodes,
which are linked by directed, typed, binary rela-
tions to form a dependency tree (Tesniere, 1959;
Hudson, 1993). The following overview of DG
flavors shows that various mechanisms (global
rules, general graphs, procedural means) are
generally employed to lift the limitation of pro-
jectivity and discusses some shortcomings of
these proposals.
</bodyText>
<sectionHeader confidence="0.641277" genericHeader="method">
Functional Generative Description (Sgall
</sectionHeader>
<bodyText confidence="0.9998748">
et al., 1986) assumes a language-independent
underlying order, which is represented as a pro-
jective dependency tree. This abstract represen-
tation of the sentence is mapped via ordering
rules to the concrete surface realization. Re-
cently, Kruijff (1997) has given a categorial-
style formulation of these ordering rules. He
assumes associative categorial operators, per-
muting the arguments to yield the surface or-
dering. One difference to our proposal is that
</bodyText>
<page confidence="0.997185">
174
</page>
<bodyText confidence="0.999896814814815">
we argue for a representational account of word
order (based on valid structures representing
word order), eschewing the non-determinism in-
troduced by unary operators; the second differ-
ence is the avoidance of an underlying structure,
which stratifies the theory and makes incremen-
tal processing difficult.
Meaning-Text Theory (Melc&apos;ilk, 1988) as-
sumes seven strata of representation. The rules
mapping from the unordered dependency trees
of surface-syntactic representations onto the an-
notated lexeme sequences of deep-morphological
representations include global ordering rules
which allow discontinuities. These rules have
not yet been formally specified (Melc&apos;ilk
Pertsov, 1987p.187f).
Word Grammar (WG, Hudson (1990)) is
based on general graphs instead of trees. The
ordering of two linked words is specified together
with their dependency relation, as in the propo-
sition &amp;quot;object of verb follows it&amp;quot;. Extrac-
tion of, e.g., objects is analyzed by establish-
ing an additional dependency called visitor
between the verb and the extractee, which re-
quires the reverse order, as in &amp;quot;visitor of
verb precedes it&amp;quot;. This results in inconsis-
tencies, since an extracted object must follow
the verb (being its object) and at the same time
precede it (being its visitor). The approach
compromises the semantic motivation of depen-
dencies by adding purely order-induced depen-
dencies. WG is similar to our proposal in that it
also distinguishes a propositional meta language
describing the graph-based analysis structures.
Dependency Unification Grammar
(DUG, Hellwig (1986)) defines a tree-like
data structure for the representation of syntac-
tic analyses. &apos; Using morphosyntactic features
with special interpretations, a word defines
abstract positions into which modifiers are
mapped. Partial orderings and even discon-
tinuities can thus be described by allowing a
modifier to occupy a position defined by some
transitive head. The approach requires that the
parser interpretes several features specially, and
it cannot restrict the scope of discontinuities.
Slot Grammar (McCord, 1990) employs a
number of rule types, some of which are ex-
clusively concerned with precedence. So-called
head/slot and slot/slot ordering rules describe
the precedence in projective trees, referring to
arbitrary predicates over head and modifiers.
Extractions (i.e., discontinuities) are merely
handled by a mechanism built into the parser.
</bodyText>
<sectionHeader confidence="0.967665" genericHeader="method">
3 Word Order Domains
</sectionHeader>
<bodyText confidence="0.9887385">
Summarizing the previous discussion, we require
the following of a word order description for DG:
</bodyText>
<listItem confidence="0.9997016">
• not to compromise the semantic motivation
of dependencies,
• to be able to restrict discontinuities to cer-
tain constructions and delimit their scope,
• to be lexicalized without requiring lexical
ambiguities for the representation of order-
ing alternatives,
• to be declarative (i.e., independent of an
analysis procedure), and
• to be formally precise and consistent.
</listItem>
<bodyText confidence="0.961536333333333">
The subsequent definition of an order domain
structure and its linking to the dependency tree
satisify these requirements.
</bodyText>
<subsectionHeader confidence="0.998832">
3.1 The Order Domain Structure
</subsectionHeader>
<bodyText confidence="0.99990044">
A word order domain is a set of words, general-
izing the notion of positions in DUG. The cardi-
nality of an order domain may be restricted to
at most one element, at least one element, or -
by conjunction - to exactly one element. Each
word is associated with a sequence of order do-
mains, one of which must contain the word itself,
and each of these domains may require that its
elements have certain features. Order domains
can be partially ordered based on set inclusion:
If an order domain d contains word w (which
is not associated with d), every word w&apos; con-
tained in a domain d&apos; associated with w is also
contained in d; therefor, d&apos; C d for each d&apos; asso-
ciated with w. This partial ordering induces a
tree on order domains, which we call the order
domain structure.
Take the example of German &amp;quot;Den Mann hat
der Junge gesehen&amp;quot; (&amp;quot;the manAcc - has - the
boyNom - seen&amp;quot;). Its dependency tree is shown
in Fig.1, with word order domains indicated
by dashed circles. The finite verb, &amp;quot;hat&amp;quot;, de-
fines a sequence of domains, (di, d2, d3), which
roughly correspond to the topological fields in
the German main clause. The nouns &amp;quot;Mann&amp;quot;
</bodyText>
<page confidence="0.954774">
175
</page>
<figure confidence="0.36870575">
--h- --- a -
dt
&apos; &amp;quot; -at 2 _ _
t
su
der Junge,&amp;quot;, ges hen; 1,
d4_ _ _ _ • Ob) &amp;quot;-cf6 ■
&apos; den Mann ,
</figure>
<figureCaption confidence="0.995258">
Figure 1: Dependency Tree and Order Domains
</figureCaption>
<figure confidence="0.8897096">
for &amp;quot;Den Mann hat der Junge gesehen&amp;quot;
d2 d3
I I -
,4 hat Is 16
Mann Junge gesehen
</figure>
<figureCaption confidence="0.742364">
Figure 2: Order Domain Structure for &amp;quot;Den
Mann hat der Junge gesehen&amp;quot;
</figureCaption>
<bodyText confidence="0.997003">
and &amp;quot;Junge&amp;quot; and the participle &amp;quot;gesehen&amp;quot; each
define one order domain (d4, d5, d6, resp.). Set
inclusion gives rise to the domain structure in
Fig.2, where the individual words are attached
by dashed lines to their including domains (di
and d4 collapse, being identical).&apos;
</bodyText>
<subsectionHeader confidence="0.999611">
3.2 Surface Ordering
</subsectionHeader>
<bodyText confidence="0.999957823529412">
How is the surface order derived from an or-
der domain structure? First of all, the ordering
of domains is inherited by their respective ele-
ments, i.e., &amp;quot;Mann&amp;quot; precedes (any element of)
d2, &amp;quot;hat&amp;quot; follows (any element of) d,, etc.
Ordering within a domain, e.g., of &amp;quot;hat&amp;quot; and
d6, or d5 and d6, is based on precedence pred-
icates (adapting the precedence predicates of
WG). There are two different types, one order-
ing a word w.r.t. any other element of the do-
main it is associated with (e.g., &amp;quot;hat&amp;quot; w.r.t. d6),
and another ordering two modifiers, referring to
the dependency relations they occupy (d5 and
d6, referring to subj and vpart). A verb like
&amp;quot;hat&amp;quot; introduces two precedence predicates, re-
quiring other words to follow itself and the par-
ticiple to follow subject and object, resp.:2
</bodyText>
<equation confidence="0.650745">
&amp;quot;hat&amp;quot; = (&lt;. A (vpart) &gt;{subj,obj})
</equation>
<bodyText confidence="0.982021380952381">
&apos;Note that in this case, we have not a single rooted
tree, but rather an ordered sequence of trees (by virtue
of ordering d1, d2, and d3) as domain structure. In gen-
eral, we assume the sentence period to govern the finite
verb and to introduce a single domain for the complete
sentence.
2For details of the notation, please refer to Sec. 4.
Informally, the first conjunct is satisfied by
any domain in which no word precedes &amp;quot;hat&amp;quot;,
and the second conjunct is satisfied by any do-
main in which no subject or object follows a
participle. The domain structure in Fig.2 satis-
fies these restrictions since nothing follows the
participle, and because &amp;quot;den Mann&amp;quot; is not an el-
ement of d2, which contains &amp;quot;hat&amp;quot;. This is an im-
portant interaction of order domains and prece-
dence predicates: Order domains define scopes
for precedence predicates. In this way, we take
into account that dependency trees are flatter
than PS-based ones3 and avoid the formal in-
consistencies noted above for WG.
</bodyText>
<subsectionHeader confidence="0.9548225">
3.3 Linking Domain Structure and
Dependency Tree
</subsectionHeader>
<bodyText confidence="0.998156741935484">
Order domains easily extend to discontinuous
dependencies. Consider the non-projective tree
in Fig.l. Assuming that the finite verb gov-
erns the participle, no projective dependency
between the object &amp;quot;den Mann&amp;quot; and the partici-
ple &amp;quot;gesehen&amp;quot; can be established. We allow non-
projectivity by loosening the linking between de-
pendency tree and domain structure: A modi-
fier (e.g., &amp;quot;Mann&amp;quot;) may not only be inserted into
a domain associated with its direct head (&amp;quot;gese-
hen&amp;quot;), but also into a domain of a transitive head
(&amp;quot;hat&amp;quot;), which we will call the positional head.
The possibility of inserting a word into a do-
main of some transitive head raises the ques-
tions of how to require contiguity (as needed
in most cases), and how to limit the distance
between the governor and the modifier in the
case of discontinuity. From a descriptive view-
point, the syntactic construction is often cited to
determine the possibility and scope of disconti-
nuities (Bhatt, 1990; Matthews, 1981). In PS-
based accounts, the construction is represented
by phrasal categories, and extraction is lim-
ited by bounding nodes (e.g., Haegeman (1994),
Becker et al. (1991)). In dependency-based ac-
counts, the construction is represented by the
dependency relation, which is typed or labelled
to indicate constructional distinctions which are
configurationally defined in PSG. Given this cor-
respondence, it is natural to employ dependen-
cies in the description of discontinuities as fol-
</bodyText>
<footnote confidence="0.989362333333333">
3Note that each phrasal level in PS-based trees defines
a scope for linear precedence rules, which only apply to
sister nodes.
</footnote>
<page confidence="0.996519">
176
</page>
<bodyText confidence="0.9999734">
lows: For each modifier of a certain head, a set
of dependency types is defined which may link
the direct head and the positional head of the
modifier (&amp;quot;gesehen&amp;quot; and &amp;quot;hat&amp;quot;, resp.). If this set
is empty, both heads are identical and a con-
tiguous attachment results. The impossibility of
extraction from, e.g., a finite verb phrase may
follow from the fact that the dependency embed-
ding finite verbs, propo, may not appear on any
path between a direct and a positional head.4
</bodyText>
<sectionHeader confidence="0.959912" genericHeader="method">
4 The Description Language
</sectionHeader>
<bodyText confidence="0.999958583333333">
This section sketches a logical language describ-
ing the dependency structure. It is based on
modal logic and owes much to work of Blackburn
(1994). As he argues, standard Kripke models
can be regarded as directed graphs with node
annotations. We will use this interpretation to
represent dependency structures. Dependencies
and the mapping from dependency tree to order
domain structure are described by modal opera-
tors, while simple properties such as word class,
features, and cardinality of order domains are
described by modal propositions.
</bodyText>
<subsectionHeader confidence="0.996822">
4.1 Model Structures
</subsectionHeader>
<bodyText confidence="0.992387">
In the following, we assume a set of words, W,
ordered by a precedence relation, a set of
dependency types, D, a set of atomic feature
values A, and a set of word classes, C. We
define a family of dependency relations Rd C
</bodyText>
<listItem confidence="0.601044461538461">
x W, d E D and for convenience abbreviate
the union Uder, Rd as RD .
Def: A dependency tree is a tuple
,wr, RD , VA, Vc) , where RT, forms a tree over
W rooted in W r , VA : W H 2A maps words to
sets of features, and Vc : W C maps words to
word classes.
Def: An order domain (over W) m is a set of
words from W where Vw1,w2,w3 E W : (wi
W2 W3 A Wi EmAw3 E w2 Em.
Def: An order domain structure (over W) .A4
is a set of order domains where Vm,m&apos; E .A4 :
mnmi= 0 VmCm&apos;vm&apos;Cm.
</listItem>
<bodyText confidence="0.968624375">
40ne review pointed out that some verbs may allow
extractions, i.e., that this restriction is lexical, not uni-
versal. This fact can easily be accomodated because the
possibility of discontinuity (and the dependency types
across which the modifier may be extracted) is described
in the lexical entry of the verb. In fact, a universal re-
striction could not even be stated because the treatment
is completely lexicalized.
</bodyText>
<construct confidence="0.895448333333333">
Def: A dependency structure T is a
tuple (W, W r RD VA VC A4 V.A4) where
(W, Wr, RD, VA, Vc) is a dependency tree, M
is an order domain structure over W, and
VM : W I--+ M n maps words to order domain
sequences.
</construct>
<bodyText confidence="0.999963363636364">
Additionally, we require for a dependency
structure four more conditions: (1) Each word w
is contained in exactly one of the domains from
VM (W), (2) all domains in VM(w) are pairwise
disjoint, (3) each word (except wr) is contained
in at least two domains, one of which is associ-
ated with a (transitive) head, and (4) the (par-
tial) ordering of domains (as described by I&lt;A4)
is consistent with the precedence of the words
contained in the domains (see (Broker, 1997) for
more details).
</bodyText>
<subsectionHeader confidence="0.984844">
4.2 The Language Li,
</subsectionHeader>
<bodyText confidence="0.999969652173913">
Fig.3 defines the logical language Li, used to
describe dependency structures. Although they
have been presented differently, they can eas-
ily be rewritten as (multimodal) Kripke models:
The dependency relation Rd is represented as
modality (d) and the mapping from a word to
its ith order domain as modality oim .5 All other
formulae denote properties of nodes, and can be
formulated as unary predicates — most evident
for word class and feature assignment. For the
precedence predicates and &lt;6, there are in-
verses &gt;. and &gt;6. For presentation, the relation
places c Wx1/1) has been introduced, which
holds between two words if the first argument
is the positional head of the second argument.
A more elaborate definition of dependency
structures and LI, defines two more dimensions,
a feature graph mapped off the dependency tree
much like the proposal of Blackburn (1994), and
a conceptual representation based on termino-
logical logic, linking content words with refer-
ence objects and dependencies with conceptual
roles.
</bodyText>
<sectionHeader confidence="0.968044" genericHeader="method">
5 The German Clause
</sectionHeader>
<bodyText confidence="0.972227428571429">
Traditionally, the German main clause is de-
scribed using three topological fields; the ini-
tial and middle fields are separated by the fi-
nite (auxiliary) verb, and the middle and the
&apos;The modality EI&apos;m can be viewed as an abbreviation
of otm Om, composed of a mapping from a word to its ith
order domain and from that domain to all its elements.
</bodyText>
<page confidence="0.980859">
177
</page>
<figure confidence="0.526479411764706">
Syntax (valid formulae) Semantics (satisfaction relation)
c E LT), Vc E C T,w c :a c = lie (w)
a E GD, Va E A T,w =a :a a E VA ( w)
(d) 0 E LD, Vd E D, 0 E Gv T,w (d) 0 :a 3w&apos; E W : wRovi A T,w&apos; =0
&lt;,. E GD, T,w &lt;., :a 3m E .A/1 : (VA4 (w) = (• • • m • • .)
AVw&apos; Em: (u; = w&apos; V w -&lt;w&apos;))
&lt;6 E LD, V5 C 1) T,w j= &lt;6 :47&gt; -,3wi, w&amp;quot;, ww E W : places(w&apos;,w)
Aplaces(w1,w&amp;quot;) A wwRow A le -&lt; w
ts E CD, Vo c D T,w =to :., 3w&apos;, w&amp;quot; E W : wRDwA
places(w&amp;quot;,w) A w&amp;quot;Mw&apos;o
w E
osingle E CD, Vi E IN T,w o&apos;m single :a w&apos; --aw&amp;quot; : (w&amp;quot;RDw&apos;n &lt;1
w&amp;quot; E (I)i(Vm(w)))
olmfilled E CD, Vi E N T,w =oimfilled :&lt;#. (1)i(V.A4(w)) I&gt; 1
CVA,la E .CD, Vi e EV, a E A T,w Erma :a Vw&apos; E 4:13i(Vm(w)) : T,w&apos; =a
6 A 0 E GD, V0,0 E .CD T,w =q5 A /p :&lt;=&gt; T,w =q5 and T,w 0
-0 E LD, VO E .CD T,w =—,cb :•;=&gt; not T,w =&apos;0
</figure>
<figureCaption confidence="0.997726">
Figure 3: Syntax and Semantics of Ly Formulae
</figureCaption>
<figure confidence="0.984585333333333">
Vf in olm (single A filled) A 0 1A4 init ial [1] &amp;quot;hat&amp;quot; A Vf in [7]
A EIL (middle A norel) [2] A (alibi) (&amp;quot;Junge&amp;quot; A to) [8]
A osingle A 1:1(f inal A norel) [3] A(vpart) (&amp;quot;gesehen&amp;quot; A To [9]
A V2 &lt;=&gt; (middleA AOlmnorel) [4] A -&apos;final A &gt;{subj,obj} [10]
A VEnd (middleA &gt;.) [5] A (obi) (&amp;quot;Mann&amp;quot; A t{vpart})) [11]
A V1 •;=&gt; (initial A norel) [6]
</figure>
<figureCaption confidence="0.999969">
Figure 4: Domain Description of finite verbs
</figureCaption>
<bodyText confidence="0.999965291666667">
final fields by infinite verb parts such as sepa-
rable prefixes or participles. We will generalize
this field structure to verb-initial and verb-final
clauses as well, without going into the linguistic
motivation due to space limits.
The formula in Fig.4 states that all finite
verbs (word class Vf in E C) define three order
domains, of which the first requires exactly one
element with the feature initial [1], the second
allows an unspecified number of elements with
features middle and norel [2], and the third al-
lows at most one element with features final
and norel [3]. The features initial, middle,
and final E A serve to restrict placement of
certain phrases in specific fields; e.g., no reflex-
ive pronouns can appear in the final field. The
norel E A feature controls placement of a rela-
tive NP or PP, which may appear in the initial
field only in verb-final clauses. The order types
are defined as follows: In a verb-second clause
(feature V2), the verb is placed at the beginning
(&lt;4 of the middle field (middle), and the el-
ement of the initial field cannot be a relative
phrase (oLnorel in [4]). In a verb-final clause
</bodyText>
<figureCaption confidence="0.985308">
Figure 5: Hierachical Structure
</figureCaption>
<bodyText confidence="0.99986776">
(VEnd), the verb is placed at the end (&gt;4 of the
middle field, with no restrictions for the initial
field (relative clauses and non-relative verb-final
clauses are subordinated to the noun and con-
junction, resp.) [5]. In a verb-initial clause (v1.),
the verb occupies the initial field [6].
The formula in Fig.5 encodes the hierarchical
structure from Fig.1 and contains lexical restric-
tions on placement and extraction (the surface is
used to identify the word). Given this, the order
type of &amp;quot;hat&amp;quot; is determined as follows: The par-
ticiple may not be extraposed(final in [10];
a restriction from the lexical entry of &amp;quot;hat&amp;quot;), it
must follow &amp;quot;hat&amp;quot; in d2. Thus, the verb can-
not be of order type VEnd, which would require
it to be the last element in its domain (&gt;,, in
[5]). &amp;quot;Mann&amp;quot; is not adjacent to &amp;quot;gesehen&amp;quot;, but
may be extracted across the dependency vpart
(t{vpart} in [11]), allowing its insertion into
a domain defined by &amp;quot;hat&amp;quot;. It cannot precede
&amp;quot;hat&amp;quot; in d2, because &amp;quot;hat&amp;quot; must either begin d2
(due to &lt;,, in [4]) or itself go into d1. But d1 al-
lows only one phrase (single), leaving only the
domain structure from Fig.2, and thus the order
type V2 for &amp;quot;hat&amp;quot;.
</bodyText>
<page confidence="0.995755">
178
</page>
<sectionHeader confidence="0.647039" genericHeader="method">
6 Comparison to PSG Approaches
</sectionHeader>
<bodyText confidence="0.99965298245614">
One feature of word order domains is that they
factor ordering alternatives from the syntactic
tree, much like feature annotations do for mor-
phological alternatives. Other lexicalized gram-
mars collapse syntactic and ordering informa-
tion and are forced to represent ordering alterna-
tives by lexical ambiguity, most notable L-TAG
(Schabes et al., 1988) and some versions of CG
(Hepple, 1994). This is not necessary in our
approach, which drastically reduces the search
space for parsing.
This property is shared by the proposal of
Reape (1993) to associate HPSG signs with se-
quences of constituents, also called word or-
der domains. Surface ordering is determined
by the sequence of constituents associated with
the root node. The order domain of a mother
node is the sequence union of the order domains
of the daughter nodes, which means that the
relative order of elements in an order domain
is retained, but material from several domains
may be interleaved, resulting in discontinuities.
Whether an order domain allows interleaving
with other domains is a parameter of the con-
stituent. This approach is very similar to ours
in that order domains separate word order from
the syntactic tree, but there is one important
difference: Word order domains in HPSG do not
completely free the hierarchical structure from
ordering considerations, because discontinuity is
specified per phrase, not per modifier. For ex-
ample, two projections are required for an NP,
the lower one for the continuous material (de-
terminer, adjective, noun, genitival and prepo-
sitional attributes) and the higher one for the
possibly discontinuous relative clause. This de-
pendence of hierarchical structure on ordering is
absent from our proposal.
We may also compare our approach with the
projection architecture of LFG (Kaplan &amp; Bres-
nan, 1982; Kaplan, 1995). There is a close sim-
ilarity of the LFG projections (c-structure and
f-structure) to the dimensions used here (order
domain structure and dependency tree, respec-
tively). C-structure and order domains repre-
sent surface ordering, whereas f-structure and
dependency tree show the subcategorization or
valence requirements. What is more, these pro-
jections or dimensions are linked in both ac-
counts by &apos;an element-wise mapping. The dif-
ference between the two architectures lies in the
linkage of the projections or dimensions: LFG
maps f-structure off c-structure. In contrast,
the dependency relation is taken to be primi-
tive here, and ordering restrictions are taken to
be indicators or consequences of dependency re-
lations (see also Broker (1998b, 1998a)).
</bodyText>
<sectionHeader confidence="0.996421" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999945">
We have presented an approach to word or-
der for DG which combines traditional notions
(semantically motivated dependencies, topolog-
ical fields) with contemporary techniques (log-
ical description language, model-theoretic se-
mantics). Word order domains are sets of par-
tially ordered words associated with words. A
word is contained in an order domain of its head,
or may float into an order domain of a transi-
tive head, resulting in a discontinuous depen-
dency tree while retaining a projective order
domain structure. Restrictions on the floating
are expressed in a lexicalized fashion in terms of
dependency relations. An important benefit is
that the proposal is lexicalized without reverting
to lexical ambiguity to represent order variation,
thus profiting even more from the efficiency con-
siderations discussed by Schabes et al. (1988).
It is not yet clear what the generative capac-
ity of such lexicalized discontinuous DGs is, but
at least some index languages (such as an bn
can be characterized. Neuhaus &amp; Broker (1997)
have shown that recognition and parsing of such
grammars is ArP-complete. A parser operating
on the model structures is described in (Hahn
et al., 1997).
</bodyText>
<sectionHeader confidence="0.997976" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99229875">
Becker, T., A. Joshi &amp; 0. Rambow (1991). Long-
Distance scrambling and tree-adjoining gram-
mar. In Proc. 5th Conf. of the European Chap-
ter of the ACL, pp. 21-26.
Bhatt, C. (1990). Die syntaktische Struktur der
Nominalphrase im Deutschen. Studien zur
deutschen Grammatik 38. Tubingen: Narr.
Blackburn, P. (1994). Structures, Languages and
Translations: The Structural Approach to Fea-
ture Logic. In C. Rupp, M. Rosner &amp; R. John-
son (Eds.), Constraints, Language and Compu-
tation, pp. 1-27. London: Academic Press.
Broker, N. (1997). Eine Dependenzgrammatik
zur Kopplung heterogener Wissenssysteme auf
modallogischer Basis. Dissertation, Deutsches
Seminar, Universitat Freiburg.
</reference>
<page confidence="0.994392">
179
</page>
<reference confidence="0.999297157303371">
Broker, N. (1998a). How to define a context-free
backbone for DGs: An experiment in gram-
mar conversion. In Proc. of the COLING-
ACL&apos;98 workshop &amp;quot;Processing of Dependency-
based Grammars&amp;quot;. Montreal/CAN, Aug 15,
1998.
Broker, N. (1998b). A Projection Architecture for
Dependency Grammar and How it Compares
to LFG. In Proc. 1998 Int&apos;l Lexical-Functional
Grammar Conference. (accepted as alternate
paper) Brisbane/AUS: Jun 30-Jul 2,1998.
Eisner, J. (1997). Bilexical Grammars and a Cubic-
Time Probabilistic Parser. In Proc. of Int&apos;l
Workshop on Parsing Technologies, pp. 54-65.
Boston/MA: MIT.
Gaifman, H. (1965). Dependency Systems and
Phrase Structure Systems. Information and
Control, 8:304-337.
Haegeman, L. (1994). Introduction to Government
and Binding. Oxford/UK: Basil Blackwell.
Hahn, U., P. Neuhaus &amp; N. Broker (1997). Message-
Passing Protocols for Real-World Parsing -
An Object-Oriented Model and its Preliminary
Evaluation. In Proc. Int&apos;l Workshop on Parsing
Technology, pp. 101-112. Boston/MA: MIT,
Sep 17-21,1997.
Hellwig, P. (1986). Dependency Unification Gram-
mar. In Proc. 11th Int&apos;l Conf. on Computa-
tional Linguistics, pp. 195-198.
Hepple, M. (1994). Discontinuity and the Lambek
Calculus. In Proc. 15th Int&apos;l Conf. on Compu-
tational Linguistics, pp. 1235-1239. Kyoto/JP.
Hudson, R. (1990). English Word Grammar. Ox-
ford/UK: Basil Blackwell.
Hudson, R. (1993). Recent developments in depen-
dency theory. In J. Jacobs, A. v. Stechow,
W. Sternefeld &amp; T. Vennemann (Eds.), Syn-
tax. Ein intern ationales Handbuch zeitgenossis-
cher Forschung, pp. 329-338. Berlin: Walter de
Gruyter.
Kaplan, R. (1995). The formal architecture of
Lexical-Functional Grammar. In M. Dalrym-
ple, R. Kaplan, J. I. Maxwell &amp; A. Zae-
nen (Eds.), Formal Issues in Lexical-Functional
Grammar, pp. 7-27. Stanford University.
Kaplan, R. &amp; J. Bresnan (1982). Lexical-Functional
Grammar: A Formal System for Grammatical
Representation. In J. Bresnan &amp; R. Kaplan
(Eds.), The Mental Representation of Gram-
matical Relations, pp. 173-281. Cambridge,
MA: MIT Press.
Kruijff, G.-J. v. (1997). A Basic Dependency-Based
Logical Grammar. Draft Manuscript. Prague:
Charles University.
Maruyama, H. (1990). Structural Disambiguation
with Constraint Propagation. In Proc. 28th
Annual Meeting of the ACL, pp. 31-38. Pitts-
burgh/PA.
Matthews, P. (1981). Syntax. Cambridge Text-
books in Linguistics, Cambridge/UK: Cam-
bridge Univ. Press.
McCord, M. (1990). Slot Grammar: A System for
Simpler Construction of Practical Natural Lan-
guage Grammars. In R. Studer (Ed.), Natural
Language and Logic, pp. 118-145. Berlin, Hei-
delberg: Springer.
Melc&apos;ilk, I. (1988). Dependency Syntax: Theory and
Practice. Albany/NY: State Univ. Press of New
York.
Melc&apos;fik, I. &amp; N. Pertsov (1987). Surface Syntax
of English: A Formal Model within the MTT
Framework. Philadelphia/PA: John Benjamins.
Neuhaus, P. &amp; N. Broker (1997). The Complexity of
Recognition of Linguistically Adequate Depen-
dency Grammars. In Proc. 35th Annual Meet-
ing of the ACL and 8th Conf. of the EACL, pp.
337-343. Madrid, July 7-12,1997.
Reape, M. (1993). A Formal Theory of Word Order:
A Case Study in West Germanic. Doctoral Dis-
sertation. Univ. of Edinburg.
Schabes, Y., A. Abeille 8z A. Joshi (1988). Parsing
Strategies with &apos;Lexicalized&apos; Grammars: Appli-
cation to TAGs. In Proc. 12th Int&apos;l Conf. on
Computational Linguistics, pp. 578-583.
Sgall, P., E. Hajicova &amp; J. Panevova (1986). The
Meaning of the Sentence in its Semantic and
Pragmatic Aspects. Dordrecht/NL: D.Reidel.
Tesniere, L. (1959). Elements de syntaxe structurale.
Paris: Klincksiek.
</reference>
<page confidence="0.997756">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.687642">
<title confidence="0.998949">Separating Surface Order and Syntactic Relations in a Dependency Grammar</title>
<author confidence="0.999844">Norbert Broker</author>
<affiliation confidence="0.999698">Universitat Stuttgart</affiliation>
<address confidence="0.95552">Azenbergstr. 12</address>
<email confidence="0.820921">NOBI@IMS.UNI-STUTTGART.DE</email>
<abstract confidence="0.990313428571429">This paper proposes decoupling the dependency tree from word order, such that surface ordering is not determined by traversing the dependency We develop the notion of a order dostructure, is linked but structurally dissimilar to the syntactic dependency tree. The proposal results in a lexicalized, declarative, and formally precise description of word order; features which lack previous proposals for dependency grammars. Contrary to other lexicalized approaches to word order, our proposal does not require lexical ambiguities for ordering alternatives.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Becker</author>
<author>A Joshi</author>
</authors>
<title>LongDistance scrambling and tree-adjoining grammar.</title>
<date>1991</date>
<booktitle>In Proc. 5th Conf. of the European Chapter of the ACL,</booktitle>
<pages>21--26</pages>
<marker>Becker, Joshi, 1991</marker>
<rawString>Becker, T., A. Joshi &amp; 0. Rambow (1991). LongDistance scrambling and tree-adjoining grammar. In Proc. 5th Conf. of the European Chapter of the ACL, pp. 21-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bhatt</author>
</authors>
<date>1990</date>
<booktitle>Die syntaktische Struktur der Nominalphrase im Deutschen. Studien zur deutschen Grammatik 38.</booktitle>
<location>Tubingen: Narr.</location>
<contexts>
<context position="11142" citStr="Bhatt, 1990" startWordPosition="1806" endWordPosition="1807">omain structure: A modifier (e.g., &amp;quot;Mann&amp;quot;) may not only be inserted into a domain associated with its direct head (&amp;quot;gesehen&amp;quot;), but also into a domain of a transitive head (&amp;quot;hat&amp;quot;), which we will call the positional head. The possibility of inserting a word into a domain of some transitive head raises the questions of how to require contiguity (as needed in most cases), and how to limit the distance between the governor and the modifier in the case of discontinuity. From a descriptive viewpoint, the syntactic construction is often cited to determine the possibility and scope of discontinuities (Bhatt, 1990; Matthews, 1981). In PSbased accounts, the construction is represented by phrasal categories, and extraction is limited by bounding nodes (e.g., Haegeman (1994), Becker et al. (1991)). In dependency-based accounts, the construction is represented by the dependency relation, which is typed or labelled to indicate constructional distinctions which are configurationally defined in PSG. Given this correspondence, it is natural to employ dependencies in the description of discontinuities as fol3Note that each phrasal level in PS-based trees defines a scope for linear precedence rules, which only a</context>
</contexts>
<marker>Bhatt, 1990</marker>
<rawString>Bhatt, C. (1990). Die syntaktische Struktur der Nominalphrase im Deutschen. Studien zur deutschen Grammatik 38. Tubingen: Narr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blackburn</author>
</authors>
<title>Structures, Languages and Translations: The Structural Approach to Feature Logic. In</title>
<date>1994</date>
<booktitle>Constraints, Language and Computation,</booktitle>
<pages>1--27</pages>
<publisher>Academic Press.</publisher>
<location>London:</location>
<contexts>
<context position="12415" citStr="Blackburn (1994)" startWordPosition="2013" endWordPosition="2014">rtain head, a set of dependency types is defined which may link the direct head and the positional head of the modifier (&amp;quot;gesehen&amp;quot; and &amp;quot;hat&amp;quot;, resp.). If this set is empty, both heads are identical and a contiguous attachment results. The impossibility of extraction from, e.g., a finite verb phrase may follow from the fact that the dependency embedding finite verbs, propo, may not appear on any path between a direct and a positional head.4 4 The Description Language This section sketches a logical language describing the dependency structure. It is based on modal logic and owes much to work of Blackburn (1994). As he argues, standard Kripke models can be regarded as directed graphs with node annotations. We will use this interpretation to represent dependency structures. Dependencies and the mapping from dependency tree to order domain structure are described by modal operators, while simple properties such as word class, features, and cardinality of order domains are described by modal propositions. 4.1 Model Structures In the following, we assume a set of words, W, ordered by a precedence relation, a set of dependency types, D, a set of atomic feature values A, and a set of word classes, C. We de</context>
<context position="15542" citStr="Blackburn (1994)" startWordPosition="2576" endWordPosition="2577">ing from a word to its ith order domain as modality oim .5 All other formulae denote properties of nodes, and can be formulated as unary predicates — most evident for word class and feature assignment. For the precedence predicates and &lt;6, there are inverses &gt;. and &gt;6. For presentation, the relation places c Wx1/1) has been introduced, which holds between two words if the first argument is the positional head of the second argument. A more elaborate definition of dependency structures and LI, defines two more dimensions, a feature graph mapped off the dependency tree much like the proposal of Blackburn (1994), and a conceptual representation based on terminological logic, linking content words with reference objects and dependencies with conceptual roles. 5 The German Clause Traditionally, the German main clause is described using three topological fields; the initial and middle fields are separated by the finite (auxiliary) verb, and the middle and the &apos;The modality EI&apos;m can be viewed as an abbreviation of otm Om, composed of a mapping from a word to its ith order domain and from that domain to all its elements. 177 Syntax (valid formulae) Semantics (satisfaction relation) c E LT), Vc E C T,w c :</context>
</contexts>
<marker>Blackburn, 1994</marker>
<rawString>Blackburn, P. (1994). Structures, Languages and Translations: The Structural Approach to Feature Logic. In C. Rupp, M. Rosner &amp; R. Johnson (Eds.), Constraints, Language and Computation, pp. 1-27. London: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Broker</author>
</authors>
<title>Eine Dependenzgrammatik zur Kopplung heterogener Wissenssysteme auf modallogischer Basis. Dissertation, Deutsches Seminar,</title>
<date>1997</date>
<location>Universitat Freiburg.</location>
<contexts>
<context position="14629" citStr="Broker, 1997" startWordPosition="2427" endWordPosition="2428"> where (W, Wr, RD, VA, Vc) is a dependency tree, M is an order domain structure over W, and VM : W I--+ M n maps words to order domain sequences. Additionally, we require for a dependency structure four more conditions: (1) Each word w is contained in exactly one of the domains from VM (W), (2) all domains in VM(w) are pairwise disjoint, (3) each word (except wr) is contained in at least two domains, one of which is associated with a (transitive) head, and (4) the (partial) ordering of domains (as described by I&lt;A4) is consistent with the precedence of the words contained in the domains (see (Broker, 1997) for more details). 4.2 The Language Li, Fig.3 defines the logical language Li, used to describe dependency structures. Although they have been presented differently, they can easily be rewritten as (multimodal) Kripke models: The dependency relation Rd is represented as modality (d) and the mapping from a word to its ith order domain as modality oim .5 All other formulae denote properties of nodes, and can be formulated as unary predicates — most evident for word class and feature assignment. For the precedence predicates and &lt;6, there are inverses &gt;. and &gt;6. For presentation, the relation pl</context>
</contexts>
<marker>Broker, 1997</marker>
<rawString>Broker, N. (1997). Eine Dependenzgrammatik zur Kopplung heterogener Wissenssysteme auf modallogischer Basis. Dissertation, Deutsches Seminar, Universitat Freiburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Broker</author>
</authors>
<title>How to define a context-free backbone for DGs: An experiment in grammar conversion.</title>
<date>1998</date>
<journal>Montreal/CAN, Aug</journal>
<booktitle>In Proc. of the COLINGACL&apos;98</booktitle>
<volume>15</volume>
<contexts>
<context position="22205" citStr="Broker (1998" startWordPosition="3772" endWordPosition="3773">re and dependency tree, respectively). C-structure and order domains represent surface ordering, whereas f-structure and dependency tree show the subcategorization or valence requirements. What is more, these projections or dimensions are linked in both accounts by &apos;an element-wise mapping. The difference between the two architectures lies in the linkage of the projections or dimensions: LFG maps f-structure off c-structure. In contrast, the dependency relation is taken to be primitive here, and ordering restrictions are taken to be indicators or consequences of dependency relations (see also Broker (1998b, 1998a)). 7 Conclusion We have presented an approach to word order for DG which combines traditional notions (semantically motivated dependencies, topological fields) with contemporary techniques (logical description language, model-theoretic semantics). Word order domains are sets of partially ordered words associated with words. A word is contained in an order domain of its head, or may float into an order domain of a transitive head, resulting in a discontinuous dependency tree while retaining a projective order domain structure. Restrictions on the floating are expressed in a lexicalized</context>
</contexts>
<marker>Broker, 1998</marker>
<rawString>Broker, N. (1998a). How to define a context-free backbone for DGs: An experiment in grammar conversion. In Proc. of the COLINGACL&apos;98 workshop &amp;quot;Processing of Dependencybased Grammars&amp;quot;. Montreal/CAN, Aug 15, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Broker</author>
</authors>
<title>A Projection Architecture for Dependency Grammar and How it Compares to LFG.</title>
<date>1998</date>
<booktitle>In Proc.</booktitle>
<contexts>
<context position="22205" citStr="Broker (1998" startWordPosition="3772" endWordPosition="3773">re and dependency tree, respectively). C-structure and order domains represent surface ordering, whereas f-structure and dependency tree show the subcategorization or valence requirements. What is more, these projections or dimensions are linked in both accounts by &apos;an element-wise mapping. The difference between the two architectures lies in the linkage of the projections or dimensions: LFG maps f-structure off c-structure. In contrast, the dependency relation is taken to be primitive here, and ordering restrictions are taken to be indicators or consequences of dependency relations (see also Broker (1998b, 1998a)). 7 Conclusion We have presented an approach to word order for DG which combines traditional notions (semantically motivated dependencies, topological fields) with contemporary techniques (logical description language, model-theoretic semantics). Word order domains are sets of partially ordered words associated with words. A word is contained in an order domain of its head, or may float into an order domain of a transitive head, resulting in a discontinuous dependency tree while retaining a projective order domain structure. Restrictions on the floating are expressed in a lexicalized</context>
</contexts>
<marker>Broker, 1998</marker>
<rawString>Broker, N. (1998b). A Projection Architecture for Dependency Grammar and How it Compares to LFG. In Proc. 1998 Int&apos;l Lexical-Functional Grammar Conference. (accepted as alternate paper) Brisbane/AUS: Jun 30-Jul 2,1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical Grammars and a CubicTime Probabilistic Parser.</title>
<date>1997</date>
<booktitle>In Proc. of Int&apos;l Workshop on Parsing Technologies,</booktitle>
<pages>54--65</pages>
<publisher>MIT.</publisher>
<location>Boston/MA:</location>
<contexts>
<context position="1129" citStr="Eisner, 1997" startWordPosition="166" endWordPosition="167">formally precise description of word order; features which lack previous proposals for dependency grammars. Contrary to other lexicalized approaches to word order, our proposal does not require lexical ambiguities for ordering alternatives. 1 Introduction Recently, the concept of valency has gained considerable attention. Not only do all linguistic theories refer to some reformulation of the traditional notion of valency (in the form of 0- grid, subcategorization list, argument list, or extended domain of locality); there is a growing number of parsers based on binary relations between words (Eisner, 1997; Maruyama, 1990). Given this interest in the valency concept, and the fact that word order is one of the main difference between phrase-structure based approaches (henceforth PSG) and dependency grammar (DG), it is valid to ask whether DG can capture word order phenomena without recourse to phrasal nodes, traces, slashed categories, etc. A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965). This result has recently been critizised to apply only to impoverished DGs which do not </context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>Eisner, J. (1997). Bilexical Grammars and a CubicTime Probabilistic Parser. In Proc. of Int&apos;l Workshop on Parsing Technologies, pp. 54-65. Boston/MA: MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gaifman</author>
</authors>
<date>1965</date>
<booktitle>Dependency Systems and Phrase Structure Systems. Information and Control,</booktitle>
<pages>8--304</pages>
<contexts>
<context position="1639" citStr="Gaifman, 1965" startWordPosition="246" endWordPosition="247">of locality); there is a growing number of parsers based on binary relations between words (Eisner, 1997; Maruyama, 1990). Given this interest in the valency concept, and the fact that word order is one of the main difference between phrase-structure based approaches (henceforth PSG) and dependency grammar (DG), it is valid to ask whether DG can capture word order phenomena without recourse to phrasal nodes, traces, slashed categories, etc. A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965). This result has recently been critizised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus Sz Braker, 1997). Our position will be that dependency relations are motivated semantically (Tesniere, 1959), and need not be projective (i.e., may cross if projected onto the surface ordering). We argue for so-called word order domains, consisting of partially ordered sets of words and associated with nodes in the dependency tree. These order domains constitute a tree defined by set inclusion, and surface word order is dete</context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Gaifman, H. (1965). Dependency Systems and Phrase Structure Systems. Information and Control, 8:304-337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Haegeman</author>
</authors>
<title>Introduction to Government and Binding. Oxford/UK:</title>
<date>1994</date>
<publisher>Basil Blackwell.</publisher>
<contexts>
<context position="11303" citStr="Haegeman (1994)" startWordPosition="1830" endWordPosition="1831">ansitive head (&amp;quot;hat&amp;quot;), which we will call the positional head. The possibility of inserting a word into a domain of some transitive head raises the questions of how to require contiguity (as needed in most cases), and how to limit the distance between the governor and the modifier in the case of discontinuity. From a descriptive viewpoint, the syntactic construction is often cited to determine the possibility and scope of discontinuities (Bhatt, 1990; Matthews, 1981). In PSbased accounts, the construction is represented by phrasal categories, and extraction is limited by bounding nodes (e.g., Haegeman (1994), Becker et al. (1991)). In dependency-based accounts, the construction is represented by the dependency relation, which is typed or labelled to indicate constructional distinctions which are configurationally defined in PSG. Given this correspondence, it is natural to employ dependencies in the description of discontinuities as fol3Note that each phrasal level in PS-based trees defines a scope for linear precedence rules, which only apply to sister nodes. 176 lows: For each modifier of a certain head, a set of dependency types is defined which may link the direct head and the positional head </context>
</contexts>
<marker>Haegeman, 1994</marker>
<rawString>Haegeman, L. (1994). Introduction to Government and Binding. Oxford/UK: Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>P Neuhaus</author>
<author>N Broker</author>
</authors>
<title>MessagePassing Protocols for Real-World Parsing -An Object-Oriented Model and its Preliminary Evaluation.</title>
<date>1997</date>
<booktitle>In Proc. Int&apos;l Workshop on Parsing Technology,</booktitle>
<pages>101--112</pages>
<location>Boston/MA: MIT,</location>
<marker>Hahn, Neuhaus, Broker, 1997</marker>
<rawString>Hahn, U., P. Neuhaus &amp; N. Broker (1997). MessagePassing Protocols for Real-World Parsing -An Object-Oriented Model and its Preliminary Evaluation. In Proc. Int&apos;l Workshop on Parsing Technology, pp. 101-112. Boston/MA: MIT, Sep 17-21,1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hellwig</author>
</authors>
<title>Dependency Unification Grammar.</title>
<date>1986</date>
<booktitle>In Proc. 11th Int&apos;l Conf. on Computational Linguistics,</booktitle>
<pages>195--198</pages>
<contexts>
<context position="5133" citStr="Hellwig (1986)" startWordPosition="785" endWordPosition="786">by establishing an additional dependency called visitor between the verb and the extractee, which requires the reverse order, as in &amp;quot;visitor of verb precedes it&amp;quot;. This results in inconsistencies, since an extracted object must follow the verb (being its object) and at the same time precede it (being its visitor). The approach compromises the semantic motivation of dependencies by adding purely order-induced dependencies. WG is similar to our proposal in that it also distinguishes a propositional meta language describing the graph-based analysis structures. Dependency Unification Grammar (DUG, Hellwig (1986)) defines a tree-like data structure for the representation of syntactic analyses. &apos; Using morphosyntactic features with special interpretations, a word defines abstract positions into which modifiers are mapped. Partial orderings and even discontinuities can thus be described by allowing a modifier to occupy a position defined by some transitive head. The approach requires that the parser interpretes several features specially, and it cannot restrict the scope of discontinuities. Slot Grammar (McCord, 1990) employs a number of rule types, some of which are exclusively concerned with precedenc</context>
</contexts>
<marker>Hellwig, 1986</marker>
<rawString>Hellwig, P. (1986). Dependency Unification Grammar. In Proc. 11th Int&apos;l Conf. on Computational Linguistics, pp. 195-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hepple</author>
</authors>
<title>Discontinuity and the Lambek Calculus. In</title>
<date>1994</date>
<booktitle>Proc. 15th Int&apos;l Conf. on Computational Linguistics,</booktitle>
<pages>1235--1239</pages>
<publisher>Kyoto/JP.</publisher>
<contexts>
<context position="20022" citStr="Hepple, 1994" startWordPosition="3428" endWordPosition="3429">&amp;quot; must either begin d2 (due to &lt;,, in [4]) or itself go into d1. But d1 allows only one phrase (single), leaving only the domain structure from Fig.2, and thus the order type V2 for &amp;quot;hat&amp;quot;. 178 6 Comparison to PSG Approaches One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives. Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG (Schabes et al., 1988) and some versions of CG (Hepple, 1994). This is not necessary in our approach, which drastically reduces the search space for parsing. This property is shared by the proposal of Reape (1993) to associate HPSG signs with sequences of constituents, also called word order domains. Surface ordering is determined by the sequence of constituents associated with the root node. The order domain of a mother node is the sequence union of the order domains of the daughter nodes, which means that the relative order of elements in an order domain is retained, but material from several domains may be interleaved, resulting in discontinuities. W</context>
</contexts>
<marker>Hepple, 1994</marker>
<rawString>Hepple, M. (1994). Discontinuity and the Lambek Calculus. In Proc. 15th Int&apos;l Conf. on Computational Linguistics, pp. 1235-1239. Kyoto/JP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hudson</author>
</authors>
<title>English Word Grammar. Oxford/UK:</title>
<date>1990</date>
<publisher>Basil Blackwell.</publisher>
<contexts>
<context position="4293" citStr="Hudson (1990)" startWordPosition="653" endWordPosition="654">eschewing the non-determinism introduced by unary operators; the second difference is the avoidance of an underlying structure, which stratifies the theory and makes incremental processing difficult. Meaning-Text Theory (Melc&apos;ilk, 1988) assumes seven strata of representation. The rules mapping from the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities. These rules have not yet been formally specified (Melc&apos;ilk Pertsov, 1987p.187f). Word Grammar (WG, Hudson (1990)) is based on general graphs instead of trees. The ordering of two linked words is specified together with their dependency relation, as in the proposition &amp;quot;object of verb follows it&amp;quot;. Extraction of, e.g., objects is analyzed by establishing an additional dependency called visitor between the verb and the extractee, which requires the reverse order, as in &amp;quot;visitor of verb precedes it&amp;quot;. This results in inconsistencies, since an extracted object must follow the verb (being its object) and at the same time precede it (being its visitor). The approach compromises the semantic motivation of depende</context>
</contexts>
<marker>Hudson, 1990</marker>
<rawString>Hudson, R. (1990). English Word Grammar. Oxford/UK: Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hudson</author>
</authors>
<title>Recent developments in dependency theory. In</title>
<date>1993</date>
<booktitle>Vennemann (Eds.), Syntax. Ein intern ationales Handbuch zeitgenossischer Forschung,</booktitle>
<pages>329--338</pages>
<location>Berlin: Walter</location>
<note>de Gruyter.</note>
<contexts>
<context position="2839" citStr="Hudson, 1993" startWordPosition="447" endWordPosition="448">order is determined by traversing this tree. A syntactic analysis therefor consists of two linked, but dissimilar trees. Sec. 2 will briefly review approaches to word order in DG. In Sec. 3, word order domains will be defined, and Sec. 4 introduces a modal logic to describe dependency structures. Sec. 5 applies our approach to the German clause and Sec. 6 relates it to some PSG approaches. 2 Word Order in DG A very brief characterization of DG is that it recognizes only lexical, not phrasal nodes, which are linked by directed, typed, binary relations to form a dependency tree (Tesniere, 1959; Hudson, 1993). The following overview of DG flavors shows that various mechanisms (global rules, general graphs, procedural means) are generally employed to lift the limitation of projectivity and discusses some shortcomings of these proposals. Functional Generative Description (Sgall et al., 1986) assumes a language-independent underlying order, which is represented as a projective dependency tree. This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. He assume</context>
</contexts>
<marker>Hudson, 1993</marker>
<rawString>Hudson, R. (1993). Recent developments in dependency theory. In J. Jacobs, A. v. Stechow, W. Sternefeld &amp; T. Vennemann (Eds.), Syntax. Ein intern ationales Handbuch zeitgenossischer Forschung, pp. 329-338. Berlin: Walter de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
</authors>
<title>The formal architecture of Lexical-Functional Grammar. In</title>
<date>1995</date>
<booktitle>Formal Issues in Lexical-Functional Grammar,</booktitle>
<pages>7--27</pages>
<institution>Stanford University.</institution>
<contexts>
<context position="21461" citStr="Kaplan, 1995" startWordPosition="3658" endWordPosition="3659">nt difference: Word order domains in HPSG do not completely free the hierarchical structure from ordering considerations, because discontinuity is specified per phrase, not per modifier. For example, two projections are required for an NP, the lower one for the continuous material (determiner, adjective, noun, genitival and prepositional attributes) and the higher one for the possibly discontinuous relative clause. This dependence of hierarchical structure on ordering is absent from our proposal. We may also compare our approach with the projection architecture of LFG (Kaplan &amp; Bresnan, 1982; Kaplan, 1995). There is a close similarity of the LFG projections (c-structure and f-structure) to the dimensions used here (order domain structure and dependency tree, respectively). C-structure and order domains represent surface ordering, whereas f-structure and dependency tree show the subcategorization or valence requirements. What is more, these projections or dimensions are linked in both accounts by &apos;an element-wise mapping. The difference between the two architectures lies in the linkage of the projections or dimensions: LFG maps f-structure off c-structure. In contrast, the dependency relation is</context>
</contexts>
<marker>Kaplan, 1995</marker>
<rawString>Kaplan, R. (1995). The formal architecture of Lexical-Functional Grammar. In M. Dalrymple, R. Kaplan, J. I. Maxwell &amp; A. Zaenen (Eds.), Formal Issues in Lexical-Functional Grammar, pp. 7-27. Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>Lexical-Functional Grammar: A Formal System for Grammatical Representation. In</title>
<date>1982</date>
<pages>173--281</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="21446" citStr="Kaplan &amp; Bresnan, 1982" startWordPosition="3653" endWordPosition="3657">but there is one important difference: Word order domains in HPSG do not completely free the hierarchical structure from ordering considerations, because discontinuity is specified per phrase, not per modifier. For example, two projections are required for an NP, the lower one for the continuous material (determiner, adjective, noun, genitival and prepositional attributes) and the higher one for the possibly discontinuous relative clause. This dependence of hierarchical structure on ordering is absent from our proposal. We may also compare our approach with the projection architecture of LFG (Kaplan &amp; Bresnan, 1982; Kaplan, 1995). There is a close similarity of the LFG projections (c-structure and f-structure) to the dimensions used here (order domain structure and dependency tree, respectively). C-structure and order domains represent surface ordering, whereas f-structure and dependency tree show the subcategorization or valence requirements. What is more, these projections or dimensions are linked in both accounts by &apos;an element-wise mapping. The difference between the two architectures lies in the linkage of the projections or dimensions: LFG maps f-structure off c-structure. In contrast, the depende</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Kaplan, R. &amp; J. Bresnan (1982). Lexical-Functional Grammar: A Formal System for Grammatical Representation. In J. Bresnan &amp; R. Kaplan (Eds.), The Mental Representation of Grammatical Relations, pp. 173-281. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G-J v Kruijff</author>
</authors>
<title>A Basic Dependency-Based Logical Grammar. Draft Manuscript.</title>
<date>1997</date>
<institution>Charles University.</institution>
<location>Prague:</location>
<contexts>
<context position="3364" citStr="Kruijff (1997)" startWordPosition="522" endWordPosition="523">irected, typed, binary relations to form a dependency tree (Tesniere, 1959; Hudson, 1993). The following overview of DG flavors shows that various mechanisms (global rules, general graphs, procedural means) are generally employed to lift the limitation of projectivity and discusses some shortcomings of these proposals. Functional Generative Description (Sgall et al., 1986) assumes a language-independent underlying order, which is represented as a projective dependency tree. This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. He assumes associative categorial operators, permuting the arguments to yield the surface ordering. One difference to our proposal is that 174 we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary operators; the second difference is the avoidance of an underlying structure, which stratifies the theory and makes incremental processing difficult. Meaning-Text Theory (Melc&apos;ilk, 1988) assumes seven strata of representation. The rul</context>
</contexts>
<marker>Kruijff, 1997</marker>
<rawString>Kruijff, G.-J. v. (1997). A Basic Dependency-Based Logical Grammar. Draft Manuscript. Prague: Charles University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Maruyama</author>
</authors>
<title>Structural Disambiguation with Constraint Propagation.</title>
<date>1990</date>
<booktitle>In Proc. 28th Annual Meeting of the ACL,</booktitle>
<pages>31--38</pages>
<publisher>Pittsburgh/PA.</publisher>
<contexts>
<context position="1146" citStr="Maruyama, 1990" startWordPosition="168" endWordPosition="169">se description of word order; features which lack previous proposals for dependency grammars. Contrary to other lexicalized approaches to word order, our proposal does not require lexical ambiguities for ordering alternatives. 1 Introduction Recently, the concept of valency has gained considerable attention. Not only do all linguistic theories refer to some reformulation of the traditional notion of valency (in the form of 0- grid, subcategorization list, argument list, or extended domain of locality); there is a growing number of parsers based on binary relations between words (Eisner, 1997; Maruyama, 1990). Given this interest in the valency concept, and the fact that word order is one of the main difference between phrase-structure based approaches (henceforth PSG) and dependency grammar (DG), it is valid to ask whether DG can capture word order phenomena without recourse to phrasal nodes, traces, slashed categories, etc. A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965). This result has recently been critizised to apply only to impoverished DGs which do not properly represen</context>
</contexts>
<marker>Maruyama, 1990</marker>
<rawString>Maruyama, H. (1990). Structural Disambiguation with Constraint Propagation. In Proc. 28th Annual Meeting of the ACL, pp. 31-38. Pittsburgh/PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Matthews</author>
</authors>
<title>Syntax. Cambridge Textbooks in Linguistics, Cambridge/UK:</title>
<date>1981</date>
<publisher>Cambridge Univ. Press.</publisher>
<contexts>
<context position="11159" citStr="Matthews, 1981" startWordPosition="1808" endWordPosition="1809">re: A modifier (e.g., &amp;quot;Mann&amp;quot;) may not only be inserted into a domain associated with its direct head (&amp;quot;gesehen&amp;quot;), but also into a domain of a transitive head (&amp;quot;hat&amp;quot;), which we will call the positional head. The possibility of inserting a word into a domain of some transitive head raises the questions of how to require contiguity (as needed in most cases), and how to limit the distance between the governor and the modifier in the case of discontinuity. From a descriptive viewpoint, the syntactic construction is often cited to determine the possibility and scope of discontinuities (Bhatt, 1990; Matthews, 1981). In PSbased accounts, the construction is represented by phrasal categories, and extraction is limited by bounding nodes (e.g., Haegeman (1994), Becker et al. (1991)). In dependency-based accounts, the construction is represented by the dependency relation, which is typed or labelled to indicate constructional distinctions which are configurationally defined in PSG. Given this correspondence, it is natural to employ dependencies in the description of discontinuities as fol3Note that each phrasal level in PS-based trees defines a scope for linear precedence rules, which only apply to sister no</context>
</contexts>
<marker>Matthews, 1981</marker>
<rawString>Matthews, P. (1981). Syntax. Cambridge Textbooks in Linguistics, Cambridge/UK: Cambridge Univ. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M McCord</author>
</authors>
<title>Slot Grammar: A System for Simpler Construction of Practical Natural Language Grammars. In</title>
<date>1990</date>
<booktitle>Natural Language and Logic,</booktitle>
<pages>118--145</pages>
<editor>R. Studer (Ed.),</editor>
<publisher>Springer.</publisher>
<location>Berlin, Heidelberg:</location>
<contexts>
<context position="5646" citStr="McCord, 1990" startWordPosition="859" endWordPosition="860"> describing the graph-based analysis structures. Dependency Unification Grammar (DUG, Hellwig (1986)) defines a tree-like data structure for the representation of syntactic analyses. &apos; Using morphosyntactic features with special interpretations, a word defines abstract positions into which modifiers are mapped. Partial orderings and even discontinuities can thus be described by allowing a modifier to occupy a position defined by some transitive head. The approach requires that the parser interpretes several features specially, and it cannot restrict the scope of discontinuities. Slot Grammar (McCord, 1990) employs a number of rule types, some of which are exclusively concerned with precedence. So-called head/slot and slot/slot ordering rules describe the precedence in projective trees, referring to arbitrary predicates over head and modifiers. Extractions (i.e., discontinuities) are merely handled by a mechanism built into the parser. 3 Word Order Domains Summarizing the previous discussion, we require the following of a word order description for DG: • not to compromise the semantic motivation of dependencies, • to be able to restrict discontinuities to certain constructions and delimit their </context>
</contexts>
<marker>McCord, 1990</marker>
<rawString>McCord, M. (1990). Slot Grammar: A System for Simpler Construction of Practical Natural Language Grammars. In R. Studer (Ed.), Natural Language and Logic, pp. 118-145. Berlin, Heidelberg: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Melc&apos;ilk</author>
</authors>
<title>Dependency Syntax: Theory and Practice. Albany/NY: State Univ.</title>
<date>1988</date>
<publisher>Press of</publisher>
<location>New York.</location>
<contexts>
<context position="3916" citStr="Melc&apos;ilk, 1988" startWordPosition="604" endWordPosition="605">s to the concrete surface realization. Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. He assumes associative categorial operators, permuting the arguments to yield the surface ordering. One difference to our proposal is that 174 we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary operators; the second difference is the avoidance of an underlying structure, which stratifies the theory and makes incremental processing difficult. Meaning-Text Theory (Melc&apos;ilk, 1988) assumes seven strata of representation. The rules mapping from the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities. These rules have not yet been formally specified (Melc&apos;ilk Pertsov, 1987p.187f). Word Grammar (WG, Hudson (1990)) is based on general graphs instead of trees. The ordering of two linked words is specified together with their dependency relation, as in the proposition &amp;quot;object of verb follows it&amp;quot;. Extraction of, e.g., objects is analyz</context>
</contexts>
<marker>Melc&apos;ilk, 1988</marker>
<rawString>Melc&apos;ilk, I. (1988). Dependency Syntax: Theory and Practice. Albany/NY: State Univ. Press of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Melc&apos;fik</author>
<author>N Pertsov</author>
</authors>
<title>Surface Syntax of English: A Formal Model within the MTT Framework. Philadelphia/PA: John Benjamins.</title>
<date>1987</date>
<marker>Melc&apos;fik, Pertsov, 1987</marker>
<rawString>Melc&apos;fik, I. &amp; N. Pertsov (1987). Surface Syntax of English: A Formal Model within the MTT Framework. Philadelphia/PA: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Neuhaus</author>
<author>N Broker</author>
</authors>
<title>The Complexity of Recognition of Linguistically Adequate Dependency Grammars.</title>
<date>1997</date>
<booktitle>In Proc. 35th Annual Meeting of the ACL and 8th Conf. of the EACL,</booktitle>
<pages>337--343</pages>
<location>Madrid,</location>
<marker>Neuhaus, Broker, 1997</marker>
<rawString>Neuhaus, P. &amp; N. Broker (1997). The Complexity of Recognition of Linguistically Adequate Dependency Grammars. In Proc. 35th Annual Meeting of the ACL and 8th Conf. of the EACL, pp. 337-343. Madrid, July 7-12,1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Reape</author>
</authors>
<title>A Formal Theory of Word Order: A Case Study in West Germanic. Doctoral Dissertation.</title>
<date>1993</date>
<institution>Univ. of Edinburg.</institution>
<contexts>
<context position="20174" citStr="Reape (1993)" startWordPosition="3453" endWordPosition="3454">d thus the order type V2 for &amp;quot;hat&amp;quot;. 178 6 Comparison to PSG Approaches One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives. Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG (Schabes et al., 1988) and some versions of CG (Hepple, 1994). This is not necessary in our approach, which drastically reduces the search space for parsing. This property is shared by the proposal of Reape (1993) to associate HPSG signs with sequences of constituents, also called word order domains. Surface ordering is determined by the sequence of constituents associated with the root node. The order domain of a mother node is the sequence union of the order domains of the daughter nodes, which means that the relative order of elements in an order domain is retained, but material from several domains may be interleaved, resulting in discontinuities. Whether an order domain allows interleaving with other domains is a parameter of the constituent. This approach is very similar to ours in that order dom</context>
</contexts>
<marker>Reape, 1993</marker>
<rawString>Reape, M. (1993). A Formal Theory of Word Order: A Case Study in West Germanic. Doctoral Dissertation. Univ. of Edinburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>A</author>
</authors>
<title>Abeille 8z A. Joshi</title>
<date>1988</date>
<booktitle>In Proc. 12th Int&apos;l Conf. on Computational Linguistics,</booktitle>
<pages>578--583</pages>
<marker>Schabes, A, 1988</marker>
<rawString>Schabes, Y., A. Abeille 8z A. Joshi (1988). Parsing Strategies with &apos;Lexicalized&apos; Grammars: Application to TAGs. In Proc. 12th Int&apos;l Conf. on Computational Linguistics, pp. 578-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sgall</author>
<author>E Hajicova</author>
<author>J Panevova</author>
</authors>
<title>The Meaning of the Sentence in its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<tech>Dordrecht/NL: D.Reidel.</tech>
<contexts>
<context position="3125" citStr="Sgall et al., 1986" startWordPosition="485" endWordPosition="488">dency structures. Sec. 5 applies our approach to the German clause and Sec. 6 relates it to some PSG approaches. 2 Word Order in DG A very brief characterization of DG is that it recognizes only lexical, not phrasal nodes, which are linked by directed, typed, binary relations to form a dependency tree (Tesniere, 1959; Hudson, 1993). The following overview of DG flavors shows that various mechanisms (global rules, general graphs, procedural means) are generally employed to lift the limitation of projectivity and discusses some shortcomings of these proposals. Functional Generative Description (Sgall et al., 1986) assumes a language-independent underlying order, which is represented as a projective dependency tree. This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. He assumes associative categorial operators, permuting the arguments to yield the surface ordering. One difference to our proposal is that 174 we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by u</context>
</contexts>
<marker>Sgall, Hajicova, Panevova, 1986</marker>
<rawString>Sgall, P., E. Hajicova &amp; J. Panevova (1986). The Meaning of the Sentence in its Semantic and Pragmatic Aspects. Dordrecht/NL: D.Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesniere</author>
</authors>
<title>Elements de syntaxe structurale.</title>
<date>1959</date>
<location>Paris: Klincksiek.</location>
<contexts>
<context position="1919" citStr="Tesniere, 1959" startWordPosition="288" endWordPosition="289"> PSG) and dependency grammar (DG), it is valid to ask whether DG can capture word order phenomena without recourse to phrasal nodes, traces, slashed categories, etc. A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965). This result has recently been critizised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus Sz Braker, 1997). Our position will be that dependency relations are motivated semantically (Tesniere, 1959), and need not be projective (i.e., may cross if projected onto the surface ordering). We argue for so-called word order domains, consisting of partially ordered sets of words and associated with nodes in the dependency tree. These order domains constitute a tree defined by set inclusion, and surface word order is determined by traversing this tree. A syntactic analysis therefor consists of two linked, but dissimilar trees. Sec. 2 will briefly review approaches to word order in DG. In Sec. 3, word order domains will be defined, and Sec. 4 introduces a modal logic to describe dependency structu</context>
</contexts>
<marker>Tesniere, 1959</marker>
<rawString>Tesniere, L. (1959). Elements de syntaxe structurale. Paris: Klincksiek.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>