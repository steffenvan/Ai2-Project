<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056798">
<title confidence="0.987389">
Detecting Verbal Participation in Diathesis Alternations
</title>
<author confidence="0.992656">
Diana McCarthy Anna Korhonen
</author>
<affiliation confidence="0.9890005">
Cognitive &amp; Computing Sciences, Computer Laboratory,
University of Sussex University of Cambridge, Pembroke Street,
</affiliation>
<address confidence="0.924464">
Brighton BN1 9QH, UK Cambridge CB2 3QG, UK
</address>
<sectionHeader confidence="0.970736" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877428571429">
We present a method for automatically identi-
fying verbal participation in diathesis alterna-
tions. Automatically acquired subcategoriza-
tion frames are compared to a hand-crafted clas-
sification for selecting candidate verbs. The
minimum description length principle is then
used to produce a model and cost for storing the
head noun instances from a training corpus at
the relevant argument slots. Alternating sub-
categorization frames are identified where the
data from corresponding argument slots in the
respective frames can be combined to produce
a cheaper model than that produced if the data
is encoded separately.&apos;.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999618157894737">
Diathesis alternations are regular variations in
the syntactic expressions of verbal arguments,
for example The boy broke the window The
window broke. Levin&apos;s (1993) investigation of
alternations summarises the research done and
demonstrates the utility of alternation informa-
tion for classifying verbs. Some studies have re-
cently recognised the potential for using diathe-
sis alternations within automatic lexical acquisi-
tion (Ribas, 1995; Korhonen, 1997; Briscoe and
Carroll, 1997).
This paper shows how corpus data can be
used to automatically detect which verbs un-
dergo these alternations. Automatic acquisi-
tion avoids the costly overheads of a manual
approach and allows for the fact that pred-
icate behaviour varies between sublanguages,
domains and across time. Subcategorization
frames (scFs) are acquired for each verb and
</bodyText>
<footnote confidence="0.997477">
1This work was partially funded by CEC LE1 project
&amp;quot;SPARKLE&amp;quot;. We also acknowledge support from UK
EPSRC project &amp;quot;PSET: Practical Simplification of En-
glish Text&amp;quot;.
</footnote>
<bodyText confidence="0.9993619">
a hand-crafted classification of diathesis alter-
nations filters potential candidates with the
correct scFs. Models representing the selec-
tional preferences of each verb for the argument
slots under consideration are then used to indi-
cate cases where the underlying arguments have
switched position in alternating scFs. The se-
lectional preferences models are produced from
argument head data stored specific to SCF and
slot.
The preference models are obtained using the
minimum description length (MDL) principle.
MDL selects an appropriate model by compar-
ing potential candidates in terms of the cost of
storing the model and the data stored using that
model for each set of argument head data. We
compare the cost of representing the data at al-
ternating argument slots separately with that
when the data is combined to indicate evidence
for participation in an alternation.
</bodyText>
<sectionHeader confidence="0.992844" genericHeader="method">
2 SCF Identification
</sectionHeader>
<bodyText confidence="0.999042454545455">
The scFs applicable to each verb are extracted
automatically from corpus data using the sys-
tem of Briscoe and Carroll (1997). This compre-
hensive verbal acquisition system distinguishes
160 verbal scFs. It produces a lexicon of verb
entries each organised by SCF with argument
head instances enumerated at each slot.
The hand-crafted diathesis alternation clas-
sification links Levin&apos;s (1993) index of alterna-
tions with the 160 SCFS to indicate which classes
are involved in alternations.
</bodyText>
<sectionHeader confidence="0.985313" genericHeader="method">
3 Selectional Preference Acquisition
</sectionHeader>
<bodyText confidence="0.999969">
Selectional preferences can be obtained for the
subject, object and prepositional phrase slots
for any specified SCF classes. The input data
includes the target verb, SCF and slot along
with the noun frequency data and any prepo-
</bodyText>
<page confidence="0.927086">
1493
</page>
<bodyText confidence="0.989299652173913">
sition (for PPs). Selectional preferences are
represented as Association Tree Cut Models
(ATcms) as described by Abe and Li (1996).
These are sets of classes which cut across the
WordNet hypernym noun hierarchy (Miller et
al., 1993) covering all leaves disjointly. Associ-
ation scores, given by P(*) are calculated for
p(c)
the classes. These scores are calculated from
the frequency of nouns occurring with the tar-
get verb and irrespective of the verb. The score
indicates the degree of preference between the
class (c) and the verb (v) at the specified slot.
Part of the ATCM for the direct object slot of
build is shown in Figure 1. For another verb a
different level for the cut might be required. For
example eat might require a cut at the FOOD
hyponym of OBJECT.
Finding the best set of classes is key to ob-
taining a good preference model. Abe and Li
use MDL to do this. MDL is a principle from in-
formation theory (Rissanen, 1978) which states
that the best model minimises the sum of i the
number of bits to encode the model, and ii the
number of bits to encode the data in the model.
This makes the compromise between a simple
model and one which describes the data effi-
ciently.
Abe and Li use a method of encoding tree cut
models using estimated frequency and probabil-
ity distributions for the data description length.
The sample size and number of classes in the
cut are used for the model description length.
They provide a way of obtaining the ATcms us-
ing the identity p(c1v) = A(c, v) x p(c). Initially
a tree cut model is obtained for the marginal
probability p(c) for the target slot irrespective
of the verb. This is then used with the condi-
tional data and probability distribution p(civ)
to obtain an ATCM as a by-product of obtaining
the model for the conditional data. The actual
comparison used to decide between two cuts is
calculated as in equation 1 where C represents
the set of classes on the cut model currently
being examined and S, represents the sample
specific to the target verb.2.
</bodyText>
<equation confidence="0.929108333333333">
—ICI logIS,1+ E —freqc x log P(civ) (1)
2
ccc p(c)
</equation>
<bodyText confidence="0.970308">
In determining the preferences the actual en-
</bodyText>
<footnote confidence="0.900896">
2A11 logarithms are to the base 2
</footnote>
<note confidence="0.467374">
root
ATCM
</note>
<figureCaption confidence="0.999455">
Figure 1: ATCM for build Object slot
</figureCaption>
<bodyText confidence="0.999946384615385">
coding in bits is not required, only the relative
cost of the cut models being considered. The
WordNet hierarchy is searched top down to find
the best set of classes under each node by locally
comparing the description length at the node
with the best found beneath. The final com-
parison is done between a cut at the root and
the best cut found beneath this. Where detail
is warranted by the specificity of the data this
is manifested in an appropriate level of general-
isation. The description length of the resultant
cut model is then used for detecting diathesis
alternations.
</bodyText>
<sectionHeader confidence="0.9952495" genericHeader="method">
4 Evidence for Diathesis
Alternations
</sectionHeader>
<bodyText confidence="0.99351504">
For verbs participating in an alternation one
might expect that the data in the alternating
slots of the respective scFs might be rather ho-
mogenous. This will depend on the extent to
which the alternation applies to the predomi-
nant sense of the verb and the majority of senses
of the arguments. The hypothesis here is that
if the alternation is reasonably productive and
could occur for a substantial majority of the in-
stances then the preferences at the correspond-
ing slots should be similar. Moreover we hy-
pothesis that if the data at the alternating slots
is combined then the cost of encoding this data
in one ATCM will be less than the cost of encod-
ing the data in separate models, for the respec-
tive slot and SCF.
Taking the causative-inchoative alternation
as an example, the object of the transitive frame
switches to the subject of the intransitive frame:
The boy broke the window +.4 The window broke.
Our strategy is to find the cost of encoding the
data from both slots in separate ATcms and
compare it to the cost of encoding the combined
data. Thus the cost of an ATCM for i the sub-
abstraction entity
</bodyText>
<page confidence="0.9837">
1494
</page>
<tableCaption confidence="0.998103">
Table 1: Causative-Inchoative Evaluation
</tableCaption>
<table confidence="0.522380444444444">
verbs
true positives begin end change 4
swing
false positives cut 1
true negatives choose like help 9
charge expect add
feel believe ask
false negatives move 1
total 15
</table>
<bodyText confidence="0.98963775">
ject of the intransitive and ii the object of the
transitive should exceed the cost of an ATCM for
the combined data only for verbs to which the
alternation applies.
</bodyText>
<sectionHeader confidence="0.994534" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999761025">
A subcategorization lexicon was produced from
10.8 million words of parsed text from the
British National Corpus. In this preliminary
work a small sample of 30 verbs were examined.
These were selected for the range of scFs that
they exhibit. The primary alternation selected
was the causative-inchoative because a reason-
able number of these verbs (15) take both sub-
categorization frames involved. ATCM models
were obtained for the data at the subject of the
intransitive frame and object of the transitive.
The cost of these models was then compared to
the cost of the model produced when the two
data sets were combined.
Table 1 shows the results for the 15 verbs
which took both the necessary frames. The sys-
tem&apos;s decision as to whether the verb partici-
pates in the alternation or not was compared
to the verdict of a human judge. The accuracy
was 87% (441). Random choice would give
a baseline of 50%. The cause for the one false
positive cut was that cut takes the middle alter-
nation (The butcher cuts the meat 4-* the meat
cuts easily). This alternation cannot be distin-
guished from the causative-inchoative because
the SCF acquisition system drops the adverbial
and provides the intransitive classification.
Performance on the simple reciprocal in-
transitive alternation (John agreed with Mary
Mary and John agreed) was less satisfac-
tory. Three potential candidates were selected
by virtue of their scFs swing;with add;to and
agree;with. None of these were identified as tak-
ing the alternation which gave rise to 2 true neg-
atives and 1 false negative. From examining the
results it seems that many of the senses found at
the intransitive slot of agree e.g. policy would
not be capable of alternating. It is at least en-
couraging that the difference in the cost of the
separate and combined models was low.
</bodyText>
<sectionHeader confidence="0.999455" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9987801">
Using MDL to detect alternations seems to be
a useful strategy in cases where the majority of
senses in alternating slot position do indeed per-
mit the alternation. In other cases the method
is at least conservative. Further work will ex-
tend the results to include a wider range of al-
ternations and verbs. We also plan to use this
method to investigate the degree of compression
that the respective alternations can make to the
lexicon as a whole.
</bodyText>
<sectionHeader confidence="0.999093" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999681464285714">
Naoki Abe and Hang Li. 1996. Learning word
association norms using tree cut pair models.
In Proceedings of the 13th International Con-
ference on Machine Learning ICML, pages 3-
11.
Ted Briscoe and John Carroll. 1997. Automatic
extraction of subcategorization from corpora.
In Fifth Applied Natural Language Processing
Conference., pages 356-363.
Anna Korhonen. 1997. Acquiring subcategori-
sation from textual corpora. Master&apos;s thesis,
University of Cambridge.
Beth Levin. 1993. English Verb Classes and Al-
ternations: a preliminary investigation. Uni-
versity of Chicago Press, Chicago and Lon-
don.
George Miller, Richard Beckwith, Christine
Felbaum, David Gross, and Katherine
Miller, 1993. Introduction to Word-
Net: An On-Line Lexical Database.
ftp//clarity.princeton.eduipub/WordNet/
5papers.ps.
Francesc Ribas. 1995. On Acquiring Appropri-
ate Selectional Restrictions from Corpora Us-
ing a Semantic Taxonomy. Ph.D. thesis, Uni-
versity of Catalonia.
J. Rissanen. 1978. Modeling by shortest data
description. Automatica, 14:465-471.
</reference>
<page confidence="0.992822">
1495
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929448">
<title confidence="0.999596">Detecting Verbal Participation in Diathesis Alternations</title>
<author confidence="0.999982">Diana McCarthy Anna Korhonen</author>
<affiliation confidence="0.991225">Cognitive &amp; Computing Sciences, Computer Laboratory, University of Sussex University of Cambridge, Pembroke Street,</affiliation>
<address confidence="0.999551">Brighton BN1 9QH, UK Cambridge CB2 3QG, UK</address>
<abstract confidence="0.996405133333333">We present a method for automatically identifying verbal participation in diathesis alternations. Automatically acquired subcategorization frames are compared to a hand-crafted classification for selecting candidate verbs. The minimum description length principle is then used to produce a model and cost for storing the head noun instances from a training corpus at the relevant argument slots. Alternating subcategorization frames are identified where the data from corresponding argument slots in the respective frames can be combined to produce a cheaper model than that produced if the data is encoded separately.&apos;.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Naoki Abe</author>
<author>Hang Li</author>
</authors>
<title>Learning word association norms using tree cut pair models.</title>
<date>1996</date>
<booktitle>In Proceedings of the 13th International Conference on Machine Learning ICML,</booktitle>
<pages>3--11</pages>
<contexts>
<context position="3665" citStr="Abe and Li (1996)" startWordPosition="548" endWordPosition="551">ument head instances enumerated at each slot. The hand-crafted diathesis alternation classification links Levin&apos;s (1993) index of alternations with the 160 SCFS to indicate which classes are involved in alternations. 3 Selectional Preference Acquisition Selectional preferences can be obtained for the subject, object and prepositional phrase slots for any specified SCF classes. The input data includes the target verb, SCF and slot along with the noun frequency data and any prepo1493 sition (for PPs). Selectional preferences are represented as Association Tree Cut Models (ATcms) as described by Abe and Li (1996). These are sets of classes which cut across the WordNet hypernym noun hierarchy (Miller et al., 1993) covering all leaves disjointly. Association scores, given by P(*) are calculated for p(c) the classes. These scores are calculated from the frequency of nouns occurring with the target verb and irrespective of the verb. The score indicates the degree of preference between the class (c) and the verb (v) at the specified slot. Part of the ATCM for the direct object slot of build is shown in Figure 1. For another verb a different level for the cut might be required. For example eat might require</context>
</contexts>
<marker>Abe, Li, 1996</marker>
<rawString>Naoki Abe and Hang Li. 1996. Learning word association norms using tree cut pair models. In Proceedings of the 13th International Conference on Machine Learning ICML, pages 3-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Fifth Applied Natural Language Processing Conference.,</booktitle>
<pages>356--363</pages>
<contexts>
<context position="1372" citStr="Briscoe and Carroll, 1997" startWordPosition="192" endWordPosition="195">lots in the respective frames can be combined to produce a cheaper model than that produced if the data is encoded separately.&apos;. 1 Introduction Diathesis alternations are regular variations in the syntactic expressions of verbal arguments, for example The boy broke the window The window broke. Levin&apos;s (1993) investigation of alternations summarises the research done and demonstrates the utility of alternation information for classifying verbs. Some studies have recently recognised the potential for using diathesis alternations within automatic lexical acquisition (Ribas, 1995; Korhonen, 1997; Briscoe and Carroll, 1997). This paper shows how corpus data can be used to automatically detect which verbs undergo these alternations. Automatic acquisition avoids the costly overheads of a manual approach and allows for the fact that predicate behaviour varies between sublanguages, domains and across time. Subcategorization frames (scFs) are acquired for each verb and 1This work was partially funded by CEC LE1 project &amp;quot;SPARKLE&amp;quot;. We also acknowledge support from UK EPSRC project &amp;quot;PSET: Practical Simplification of English Text&amp;quot;. a hand-crafted classification of diathesis alternations filters potential candidates with </context>
<context position="2902" citStr="Briscoe and Carroll (1997)" startWordPosition="432" endWordPosition="435">red specific to SCF and slot. The preference models are obtained using the minimum description length (MDL) principle. MDL selects an appropriate model by comparing potential candidates in terms of the cost of storing the model and the data stored using that model for each set of argument head data. We compare the cost of representing the data at alternating argument slots separately with that when the data is combined to indicate evidence for participation in an alternation. 2 SCF Identification The scFs applicable to each verb are extracted automatically from corpus data using the system of Briscoe and Carroll (1997). This comprehensive verbal acquisition system distinguishes 160 verbal scFs. It produces a lexicon of verb entries each organised by SCF with argument head instances enumerated at each slot. The hand-crafted diathesis alternation classification links Levin&apos;s (1993) index of alternations with the 160 SCFS to indicate which classes are involved in alternations. 3 Selectional Preference Acquisition Selectional preferences can be obtained for the subject, object and prepositional phrase slots for any specified SCF classes. The input data includes the target verb, SCF and slot along with the noun </context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Ted Briscoe and John Carroll. 1997. Automatic extraction of subcategorization from corpora. In Fifth Applied Natural Language Processing Conference., pages 356-363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Acquiring subcategorisation from textual corpora. Master&apos;s thesis,</title>
<date>1997</date>
<institution>University of Cambridge.</institution>
<contexts>
<context position="1344" citStr="Korhonen, 1997" startWordPosition="190" endWordPosition="191">nding argument slots in the respective frames can be combined to produce a cheaper model than that produced if the data is encoded separately.&apos;. 1 Introduction Diathesis alternations are regular variations in the syntactic expressions of verbal arguments, for example The boy broke the window The window broke. Levin&apos;s (1993) investigation of alternations summarises the research done and demonstrates the utility of alternation information for classifying verbs. Some studies have recently recognised the potential for using diathesis alternations within automatic lexical acquisition (Ribas, 1995; Korhonen, 1997; Briscoe and Carroll, 1997). This paper shows how corpus data can be used to automatically detect which verbs undergo these alternations. Automatic acquisition avoids the costly overheads of a manual approach and allows for the fact that predicate behaviour varies between sublanguages, domains and across time. Subcategorization frames (scFs) are acquired for each verb and 1This work was partially funded by CEC LE1 project &amp;quot;SPARKLE&amp;quot;. We also acknowledge support from UK EPSRC project &amp;quot;PSET: Practical Simplification of English Text&amp;quot;. a hand-crafted classification of diathesis alternations filter</context>
</contexts>
<marker>Korhonen, 1997</marker>
<rawString>Anna Korhonen. 1997. Acquiring subcategorisation from textual corpora. Master&apos;s thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: a preliminary investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago and London.</location>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: a preliminary investigation. University of Chicago Press, Chicago and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Richard Beckwith</author>
<author>Christine Felbaum</author>
<author>David Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Introduction to WordNet: An On-Line Lexical Database. ftp//clarity.princeton.eduipub/WordNet/ 5papers.ps.</title>
<date>1993</date>
<contexts>
<context position="3767" citStr="Miller et al., 1993" startWordPosition="565" endWordPosition="568"> links Levin&apos;s (1993) index of alternations with the 160 SCFS to indicate which classes are involved in alternations. 3 Selectional Preference Acquisition Selectional preferences can be obtained for the subject, object and prepositional phrase slots for any specified SCF classes. The input data includes the target verb, SCF and slot along with the noun frequency data and any prepo1493 sition (for PPs). Selectional preferences are represented as Association Tree Cut Models (ATcms) as described by Abe and Li (1996). These are sets of classes which cut across the WordNet hypernym noun hierarchy (Miller et al., 1993) covering all leaves disjointly. Association scores, given by P(*) are calculated for p(c) the classes. These scores are calculated from the frequency of nouns occurring with the target verb and irrespective of the verb. The score indicates the degree of preference between the class (c) and the verb (v) at the specified slot. Part of the ATCM for the direct object slot of build is shown in Figure 1. For another verb a different level for the cut might be required. For example eat might require a cut at the FOOD hyponym of OBJECT. Finding the best set of classes is key to obtaining a good prefe</context>
</contexts>
<marker>Miller, Beckwith, Felbaum, Gross, Miller, 1993</marker>
<rawString>George Miller, Richard Beckwith, Christine Felbaum, David Gross, and Katherine Miller, 1993. Introduction to WordNet: An On-Line Lexical Database. ftp//clarity.princeton.eduipub/WordNet/ 5papers.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesc Ribas</author>
</authors>
<title>On Acquiring Appropriate Selectional Restrictions from Corpora Using a Semantic Taxonomy.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Catalonia.</institution>
<contexts>
<context position="1328" citStr="Ribas, 1995" startWordPosition="188" endWordPosition="189">from corresponding argument slots in the respective frames can be combined to produce a cheaper model than that produced if the data is encoded separately.&apos;. 1 Introduction Diathesis alternations are regular variations in the syntactic expressions of verbal arguments, for example The boy broke the window The window broke. Levin&apos;s (1993) investigation of alternations summarises the research done and demonstrates the utility of alternation information for classifying verbs. Some studies have recently recognised the potential for using diathesis alternations within automatic lexical acquisition (Ribas, 1995; Korhonen, 1997; Briscoe and Carroll, 1997). This paper shows how corpus data can be used to automatically detect which verbs undergo these alternations. Automatic acquisition avoids the costly overheads of a manual approach and allows for the fact that predicate behaviour varies between sublanguages, domains and across time. Subcategorization frames (scFs) are acquired for each verb and 1This work was partially funded by CEC LE1 project &amp;quot;SPARKLE&amp;quot;. We also acknowledge support from UK EPSRC project &amp;quot;PSET: Practical Simplification of English Text&amp;quot;. a hand-crafted classification of diathesis alt</context>
</contexts>
<marker>Ribas, 1995</marker>
<rawString>Francesc Ribas. 1995. On Acquiring Appropriate Selectional Restrictions from Corpora Using a Semantic Taxonomy. Ph.D. thesis, University of Catalonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Modeling by shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<pages>14--465</pages>
<contexts>
<context position="4470" citStr="Rissanen, 1978" startWordPosition="696" endWordPosition="697">c) the classes. These scores are calculated from the frequency of nouns occurring with the target verb and irrespective of the verb. The score indicates the degree of preference between the class (c) and the verb (v) at the specified slot. Part of the ATCM for the direct object slot of build is shown in Figure 1. For another verb a different level for the cut might be required. For example eat might require a cut at the FOOD hyponym of OBJECT. Finding the best set of classes is key to obtaining a good preference model. Abe and Li use MDL to do this. MDL is a principle from information theory (Rissanen, 1978) which states that the best model minimises the sum of i the number of bits to encode the model, and ii the number of bits to encode the data in the model. This makes the compromise between a simple model and one which describes the data efficiently. Abe and Li use a method of encoding tree cut models using estimated frequency and probability distributions for the data description length. The sample size and number of classes in the cut are used for the model description length. They provide a way of obtaining the ATcms using the identity p(c1v) = A(c, v) x p(c). Initially a tree cut model is </context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>J. Rissanen. 1978. Modeling by shortest data description. Automatica, 14:465-471.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>