<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011779">
<title confidence="0.910279">
Cooperative User Models in Statistical Dialog Simulators
</title>
<author confidence="0.687879">
Meritxell Gonz´alez&apos;,2, Silvia Quarteroni&apos;, Giuseppe Riccardi&apos;, Sebastian Varges&apos;
</author>
<affiliation confidence="0.472575">
&apos; DISI - University of Trento, Povo (Trento), Italy
</affiliation>
<address confidence="0.311097">
2 TALP Center - Technical University of Catalonia, Barcelona, Spain
</address>
<email confidence="0.962609">
mgonzalez@lsi.upc.edu, name.lastname@disi.unitn.it
</email>
<sectionHeader confidence="0.992532" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913733333333">
Statistical user simulation is a promis-
ing methodology to train and evaluate the
performance of (spoken) dialog systems.
We work with a modular architecture for
data-driven simulation where the “inten-
tional” component of user simulation in-
cludes a User Model representing user-
specific features. We train a dialog sim-
ulator that combines traits of human be-
havior such as cooperativeness and con-
text with domain-related aspects via the
Expectation-Maximization algorithm. We
show that cooperativeness provides a finer
representation of the dialog context which
directly affects task completion rate.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993478708333333">
Data-driven techniques are a promising approach
to the development of robust (spoken) dialog sys-
tems, particularly when training statistical dialog
managers (Varges et al., 2009). User simulators
have been introduced to cope with the scarcity of
real user conversations and optimize a number of
SDS components (Schatzmann et al., 2006).
In this work, we investigate the combination of
aspects of human behavior with contextual aspects
of conversation in a joint yet modular data-driven
simulation model. For this, we integrate conversa-
tional context representation, centered on a Dialog
Act and a Concept Model, with a User Model rep-
resenting persistent individual features. Our aim
is to evaluate different simulation regimes against
real dialogs to identify any impact of user-specific
features on dialog performance.
In this paper, Section 2 presents our simulator
architecture and Section 3 focuses on our model of
cooperativeness. Our experiments are illustrated
Work partly funded by EU project ADAMACH (022593)
and Spanish project OPENMT-2 (TIN2009-14675-C03).
in Section 4 and conclusions are summarized in
Section 5.
</bodyText>
<sectionHeader confidence="0.976366" genericHeader="method">
2 Simulator Architecture
</sectionHeader>
<bodyText confidence="0.999606864864865">
Data-driven simulation takes place within the rule-
based version of the ADASearch system (Varges
et al., 2009), which uses a taxonomy of 16 dialog
acts and a dozen concepts to deal with three tasks
related to tourism in Trentino (Italy): Lodging En-
quiry, Lodging Reservation and Event Enquiry.
Simulation in our framework occurs at the in-
tention level, where the simulator and the Dia-
log Manager (DM) exchange actions, i.e. or-
dered sequences of dialog acts and a number of
concept-value pairs. In other words, we repre-
sent the DM action as as = {da0,.., da.}, (s
is for “System”) where dad is short for a dialog
act defined over zero or more concept-value pairs,
dad(c0(v0), .., cm(vm)).
In response to the DM action as, the different
modules that compose the User Simulator gener-
ate an N-best list of simulated actions Au(as) =
{a0u, .., a�u }. The probability of each possible ac-
tion being generated after the DM action as is es-
timated based on the conversation context. Such a
context is represented by a User Model, a Dialog
Act Model, a Concept Model and an Error Model
(Quarteroni et al., 2010). The User Model simu-
lates the behavior of an individual user in terms of
goals and other behavioral features such as coop-
erativeness and tendency to hang up. The Dialog
Act Model generates a distribution of M actions
Au = {a0u, .., amu }. Then, one action bu is chosen
out of Au. In order to vary the simulation behav-
ior, the choice of the user action bu is a random
sampling according to the distribution of proba-
bilities therein; making the simulation more realis-
tic. Finally, the Concept Model generates concept
values for bu; and the Error Model simulates the
noisy ASR-SLU channel by “distorting” bu.
These models are derived from the ADASearch
</bodyText>
<subsubsectionHeader confidence="0.676489">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 217–220,
</subsubsectionHeader>
<affiliation confidence="0.890776">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999403">
217
</page>
<bodyText confidence="0.993219">
dataset, containing 74 spoken human-computer
conversations.
</bodyText>
<subsectionHeader confidence="0.887178">
2.1 User Model
</subsectionHeader>
<bodyText confidence="0.998259894736842">
The User Model represents user-specific fea-
tures, both transient and persistent. The
transient feature we focus on in this work is
the user’s goal in the dialog (UG), represented
as a task name and the list of concepts and
values required to fulfill it: an example of
UG is {Activity(EventEnquiry), Time day(2),
Time month(may), Event type(fair), Loca-
tion name(Povo)}.
Persistent features included in our model so far
are: patience, silence (no input) and cooperative-
ness. Patience pat is defined as the tendency
to abandon the conversation (hang up event), i.e.
pat = P(HangUp|as). Similarly, NoInput prob-
ability noi is used to account for user behavior in
noisy environments: noi = P(Nolnput|as). Fi-
nally, cooperativeness coop is a real value repre-
senting the ratio of concepts mentioned in as that
also appear in bu (see Section 3).
</bodyText>
<subsectionHeader confidence="0.995542">
2.2 Dialog Act Model
</subsectionHeader>
<bodyText confidence="0.991100205882353">
We define three Dialog Act (DA) Models: Obedi-
ent (OB), Bigram (BI) and Task-based (TB).
In the Obedient model, total patience and coop-
erativeness are assumed of the user, who will al-
ways respond to each query requiring values for a
set of concepts with an answer concerning exactly
such concepts. Formally, the model responds to a
DM action as with a single user action bu obtained
by consulting a rule table, having probability 1. In
case a request for clarification is issued by the DM,
this model returns a clarifying answer. Any offer
from the DM to continue the conversation will be
either readily met with a new task request or de-
nied at a fixed probability: Au(as) _ {(au, 1)}.
In the Bigram model, first defined in (Eckert et
al., 1997), a transition matrix records the frequen-
cies of transition from DM actions to user actions,
including hang up and no input/no match. Given
a DM action as, the model responds with a list of
M user actions and their probabilities estimated
according to action distribution in the real data:
Au(as) _ {(a°, P(a°|as)), .., (aMu , P(aMu |as))}.
The Task-based model, similarly to the “goal”
model in (Pietquin, 2004), produces an action dis-
tribution containing only the actions observed in
the dataset of dialogs in the context of a spe-
cific task Tk. The TB model divides the dataset
into one partition for each Tk, then creates a
task-specific bigram model, by computing b k:
Au(as) _ {(a°, P(a°|as, Tk)), .., (aMu , P(aMu |as, Tk))}.
As the partition of the dataset reduces the number
of observations, the TB model includes a mech-
anism to back off to the simpler bigram and uni-
gram models.
</bodyText>
<subsectionHeader confidence="0.998285">
2.3 Concept &amp; Error Model
</subsectionHeader>
<bodyText confidence="0.988034117647059">
The Concept Model takes the action bu selected
by the DA Model and attaches values and sam-
pled interpretation confidences to its concepts. In
this work, we adopt a Concept Model which as-
signs the corresponding User Goal values for the
required concepts, which makes the user simulated
responses consistent with the user goal.
The Error Model is responsible of simulating
the noisy communication channel between user
and system; as we simulate the error at SLU level,
errors consist of incorrect concept values. We ex-
periment with a data-driven model where the pre-
cision Pr, obtained by a concept c in the refer-
ence dataset is used to estimate the frequency with
which an error in the true value v� of c will be in-
troduced: P(c(v)|c(v)) = 1 − Pr, (Quarteroni et
al., 2010).
</bodyText>
<sectionHeader confidence="0.991039" genericHeader="method">
3 Modelling Cooperativeness
</sectionHeader>
<bodyText confidence="0.983818956521739">
As in e.g. (Jung et al., 2009), we define coop-
erativeness at the turn level (coops) as a function
of the number of dialog acts in the DM action as
sharing concepts with the dialog acts in the user
action au; at the dialog level, coop is the average
of turn-level cooperativeness.
We discretize coop into a binary variable reflect-
ing high vs low cooperativeness based on whether
or not the dialog cooperativeness exceeds the me-
dian value of coop found in a reference corpus; in
our ADASearch dataset, the median value found
for coop is 0.28; hence, we annotate dialogs as co-
operative if they exceed such a threshold, and as
uncooperative otherwise. Using a corpus thresh-
old allows domain- and population-driven tuning
of cooperativeness rather than a “hard” definition
(as in (Jung et al., 2009)).
We then model cooperativeness as two bigram
models, reflecting the high vs low value of coop.
In practice, given a DM action as and the coop
value (r. = high/low) we obtain a list of user ac-
tions and their probabilities:
Au(as, κ) _ {(au, P(a°|as, κ)), .., (aMu , P(aMu |as, κ))}.
</bodyText>
<page confidence="0.990577">
218
</page>
<subsectionHeader confidence="0.999544">
3.1 Combining cooperativeness and context
</subsectionHeader>
<bodyText confidence="0.9999018">
At this point, the distribution Au(as, n) is lin-
early interpolated with the distribution of actions
Au(as, 0) obtained using the DA model 0 (in the
Task-based DA model; 0 can have three values,
one for each task as explained in Section 2.2):
</bodyText>
<equation confidence="0.996089">
Au(as) = λκ · Au(as, κ) + λψ · Au(as, ψ),
</equation>
<bodyText confidence="0.9961334">
where aκ and Aψ are the weights of each fea-
ture/model and Aψ + aκ = 1.
For each user action aiu, aκ and Aψ are
estimated using the Baum-Welch Expectation-
Maximization algorithm as proposed by (Jelinek
and Mercer, 1980). We use the distributions of ac-
tions obtained from our dataset and we align the
set of actions of the two models. Since we only
have two models, we only need to calculate ex-
pectation for one of the distributions:
</bodyText>
<equation confidence="0.999899666666667">
P(ai u|as, κ)
P(κ|as, ai u) = u
P(aiu|as, κ) + P (aiu|as, ψ) ∀M i=0ai
</equation>
<bodyText confidence="0.955669333333333">
where M is the number of actions. Then, the
weights Aκ and Aψ that maximize the data like-
lihood are calculated as follows:
</bodyText>
<equation confidence="0.789628">
�Mj=0 P(κ|as, aju)
</equation>
<bodyText confidence="0.9990465">
The resulting combined distribution Au(as) is
obtained by factoring the probabilities of each ac-
tion with the weight estimated for the particular
distribution:
</bodyText>
<equation confidence="0.9476725">
Au(as) = {(a0u, λκ·P(a0u|as, κ)), .., (aMu , λκ·P(aMu |as, κ)),
(a0u, λψ · P(a0u|as, ψ)), .., (aMu , λψ · P(aMu |as, ψ))}
</equation>
<subsectionHeader confidence="0.999759">
3.2 Effects of cooperativeness
</subsectionHeader>
<bodyText confidence="0.999945444444444">
To assess the effect of the cooperativeness feature
in the final distribution of actions, we set a 5-fold
cross-validation experiment with the ADASearch
dataset where we average the aκ estimated at each
turn of the dialog. We investigated in which con-
text cooperativeness provides more contribution
by comparing the aκ weights attributed by high
vs. low coop models to user action distributions in
response to Dialog Manager actions.
Figure 1 shows the values achieved by aκ for
several DM actions for high vs low coop regimes.
We can see that aκ achieves high values in case
of uncooperative users in response to DM dialog
acts as [ClarificationRequest] and [Info-request].
In contrast, forward-looking actions, such as the
ones including [Offer], seem to discard the con-
tribution of the low coop model, but to favor the
contribution provided by high coop.
</bodyText>
<figureCaption confidence="0.972342">
Figure 1: Estimated aκ weights in response to se-
lected DM actions in case of high/low coop
</figureCaption>
<sectionHeader confidence="0.998442" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999963">
We evaluate our simulator models using two meth-
ods: first, “offline” statistics are used to assess
how realistic the action estimations by DA Models
are with respect to a dataset of real conversations
(Sec. 4.1); then, “online” statistics (Sec. 4.2) eval-
uate end-to-end simulator performance in terms of
dialog act distributions, error robustness and task
duration and completion rates by comparing real
dialogs with fresh simulated dialogs using action
sampling in the different simulation models.
</bodyText>
<subsectionHeader confidence="0.998364">
4.1 “Offline” statistics
</subsectionHeader>
<bodyText confidence="0.9998198">
In order to compare simulated and real user ac-
tions, we evaluate dialog act Precision (PDA)
and Recall (RDA) following the methodology in
(Schatzmann et al., 2005).
For each DM action as the simulator picks a
user action au from Au(as) and we compare it
with the real user choice au. A simulated dialog
act is correct when it appears in the real action
au. The measurements were obtained using 5-fold
cross-validation on the ADASearch dataset.
</bodyText>
<tableCaption confidence="0.998088">
Table 1: Dialog Act Precision and Recall
</tableCaption>
<table confidence="0.9997732">
Simulation (a,*) Most frequent (a,-)
DA Model PDA RDA PDA RDA
OB 33.8 33.4 33.9 33.5
BI (+coop) 35.6 (35.7) 35.5 (35.8) 49.3 (47.9) 48.8 (47.4)
TB (+coop) 38.2 (39.7) 38.1 (39.4) 51.1 (50.6) 50.6 (50.2)
</table>
<bodyText confidence="0.942888083333333">
Table 1 shows PDA/RDA obtained for the OB,
BI and TB models alone and with cooperative-
ness models (+coop). First, we see that TB is
much better than BI and OB at reproducing real
action selection. This is also visible in both PDA
and RDA obtained by selecting a*, the most fre-
quent user action from the As generated by each
model. By definition, a* maximizes the expected
PDA and RDA, providing an upper bound for our
models; however, to reproduce any possible user
behavior, we need to sample au rather than choos-
ing it by frequency. By now inspecting (+coop)
</bodyText>
<figure confidence="0.932485444444444">
0.6
0.4
0.2
0
[Aplg;ClarifReq] [ClarifReq] [Info-request] [Greet;Offer] [Offer] [Inform;Offer]
high low
λκ =
; λψ = 1 − λκ.
M
</figure>
<page confidence="0.998407">
219
</page>
<bodyText confidence="0.999860625">
values in Table 1, we see that explicit cooperative-
ness models match real dialogs more closely. It
points out that partitioning the reference dataset in
high vs low coop sets allows better data represen-
tation. There is however no improvement in the
a* case: we explain this by the fact that by “slic-
ing” the reference dataset, the cooperative model
augments data sparsity, affecting robustness.
</bodyText>
<subsectionHeader confidence="0.991507">
4.2 “Online” statistics
</subsectionHeader>
<bodyText confidence="0.98738272">
We now discuss online deployment of our sim-
ulation models with different user behaviors and
“fresh” user goals and data. To align with the
ADASearch dataset, we ran 60 simulated dialogs
between the ADASearch DM and each combina-
tion of the Task-based and Bigram models and
high and low values of coop. For each set of simu-
lated dialogs, we measured task duration, defined
as the average number of turns needed to complete
each task, and task completion rate, defined as:
TCR _ number of times a task has been completed .
total number of task requests
Table 2 reports such figures in comparison
to the ones obtained for real dialogs from the
ADASearch dataset. In general, we see that task
duration is closer to real dialogs in the Bigram and
Task-based models when compared to the Obedi-
ent model. Moreover, it can easily be observed
in both BI and TB models that under high-coop
regime (in boldface), the number of turns taken
to complete tasks is lower than under low-coop.
Furthermore, in both TB and BI models, TCR
is higher when cooperativeness is higher, indicat-
ing that cooperative users make dialogs not only
shorter but also more efficient.
</bodyText>
<tableCaption confidence="0.95592">
Table 2: Task duration and TCR in simulated di-
alogs with different regimes vs real dialogs.
</tableCaption>
<table confidence="0.998279375">
Model Lodging Enquiry Lodging Reserv Event Enquiry All
#turns TCR #turns TCR #turns TCR TCR
OB 9.2±0.0 78.1 9.7±1.4 82.4 8.1±2.9 66.7 76.6
BI+low 15.1±4.1 71.4 14.2±3.9 69.4 9.3±1.8 52.2 66.7
BI+high 12.1±2.5 74.6 12.9±3.1 82.9 7.8±1.8 75.0 77.4
TB+low 13.6±4.1 75.8 13.4±3.7 83.3 8.4±3.3 64.7 77.2
TB+high 11.6±2.8 80.0 12.6±3.6 83.7 6.5±1.9 57.1 78.4
Real dialogs 11.1±3.0 71.4 12.7±4.7 69.6 9.3±4.0 85.0 73.4
</table>
<sectionHeader confidence="0.99879" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999921333333333">
In this work, we address data-driven dialog sim-
ulation for the training of statistical dialog man-
agers. Our simulator supports a modular combina-
tion of user-specific features with different models
of dialog act and concept-value estimation, in ad-
dition to ASR/SLU error simulation.
We investigate the effect of joining a model of
user intentions (Dialog Act Model) with a model
of individual user traits (User Model). In partic-
ular, we represent the user’s cooperativeness as
a real-valued feature of the User Model and cre-
ate two separate simulator behaviors, reproducing
high and low cooperativeness. We explore the im-
pact of combining our cooperativeness model with
the Dialog Act model in terms of dialog act accu-
racy and task success.
We find that 1) an explicit modelling of user
cooperativeness contributes to an improved accu-
racy of dialog act estimation when compared to
real conversations; 2) simulated dialogs with high
cooperativeness result in higher task completion
rates than low-cooperativeness dialogs. In future
work, we will study yet more fine-grained and re-
alistic User Model features.
</bodyText>
<sectionHeader confidence="0.999099" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9990972">
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation. In
Proc. IEEE ASRU.
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data.
In Workshop on Pattern Recognition in Practice.
S. Jung, C. Lee, K. Kim, and G. G. Lee. 2009. Hy-
brid approach to user intention modeling for dialog
simulation. In Proc. ACL-IJCNLP.
O. Pietquin. 2004. A Framework for Unsupervised
Learning of Dialogue Strategies. Ph.D. thesis, Fac-
ult´e Polytechnique de Mons, TCTS Lab (Belgique).
S. Quarteroni, M. Gonz´alez, G. Riccardi, and
S. Varges. 2010. Combining user intention and error
modeling for statistical dialog simulators. In Proc.
INTERSPEECH.
J. Schatzmann, K. Georgila, and S. Young. 2005.
Quantitative evaluation of user simulation tech-
niques for spoken dialogue systems. In Proc. SIG-
DIAL.
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of di-
alogue management strategies. Knowl. Eng. Rev.,
21(2):97–126.
S. Varges, S. Quarteroni, G. Riccardi, A. V. Ivanov, and
P. Roberti. 2009. Leveraging POMDPs trained with
user simulations and rule-based dialogue manage-
ment in a spoken dialogue system. In Proc. SIG-
DIAL.
</reference>
<page confidence="0.997543">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.770353">
<title confidence="0.999458">Cooperative User Models in Statistical Dialog Simulators</title>
<author confidence="0.999611">Silvia Giuseppe Sebastian</author>
<affiliation confidence="0.889868">University of Trento, Povo (Trento), Center - Technical University of Catalonia, Barcelona,</affiliation>
<email confidence="0.995855">mgonzalez@lsi.upc.edu,name.lastname@disi.unitn.it</email>
<abstract confidence="0.99959975">Statistical user simulation is a promising methodology to train and evaluate the performance of (spoken) dialog systems. We work with a modular architecture for data-driven simulation where the “intentional” component of user simulation includes a User Model representing userspecific features. We train a dialog simulator that combines traits of human behavior such as cooperativeness and context with domain-related aspects via the Expectation-Maximization algorithm. We show that cooperativeness provides a finer representation of the dialog context which directly affects task completion rate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Eckert</author>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>User modeling for spoken dialogue system evaluation.</title>
<date>1997</date>
<booktitle>In Proc. IEEE ASRU.</booktitle>
<contexts>
<context position="5735" citStr="Eckert et al., 1997" startWordPosition="914" endWordPosition="917">e and cooperativeness are assumed of the user, who will always respond to each query requiring values for a set of concepts with an answer concerning exactly such concepts. Formally, the model responds to a DM action as with a single user action bu obtained by consulting a rule table, having probability 1. In case a request for clarification is issued by the DM, this model returns a clarifying answer. Any offer from the DM to continue the conversation will be either readily met with a new task request or denied at a fixed probability: Au(as) _ {(au, 1)}. In the Bigram model, first defined in (Eckert et al., 1997), a transition matrix records the frequencies of transition from DM actions to user actions, including hang up and no input/no match. Given a DM action as, the model responds with a list of M user actions and their probabilities estimated according to action distribution in the real data: Au(as) _ {(a°, P(a°|as)), .., (aMu , P(aMu |as))}. The Task-based model, similarly to the “goal” model in (Pietquin, 2004), produces an action distribution containing only the actions observed in the dataset of dialogs in the context of a specific task Tk. The TB model divides the dataset into one partition f</context>
</contexts>
<marker>Eckert, Levin, Pieraccini, 1997</marker>
<rawString>W. Eckert, E. Levin, and R. Pieraccini. 1997. User modeling for spoken dialogue system evaluation. In Proc. IEEE ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Workshop on Pattern Recognition in Practice.</booktitle>
<contexts>
<context position="9075" citStr="Jelinek and Mercer, 1980" startWordPosition="1502" endWordPosition="1505">ir probabilities: Au(as, κ) _ {(au, P(a°|as, κ)), .., (aMu , P(aMu |as, κ))}. 218 3.1 Combining cooperativeness and context At this point, the distribution Au(as, n) is linearly interpolated with the distribution of actions Au(as, 0) obtained using the DA model 0 (in the Task-based DA model; 0 can have three values, one for each task as explained in Section 2.2): Au(as) = λκ · Au(as, κ) + λψ · Au(as, ψ), where aκ and Aψ are the weights of each feature/model and Aψ + aκ = 1. For each user action aiu, aκ and Aψ are estimated using the Baum-Welch ExpectationMaximization algorithm as proposed by (Jelinek and Mercer, 1980). We use the distributions of actions obtained from our dataset and we align the set of actions of the two models. Since we only have two models, we only need to calculate expectation for one of the distributions: P(ai u|as, κ) P(κ|as, ai u) = u P(aiu|as, κ) + P (aiu|as, ψ) ∀M i=0ai where M is the number of actions. Then, the weights Aκ and Aψ that maximize the data likelihood are calculated as follows: �Mj=0 P(κ|as, aju) The resulting combined distribution Au(as) is obtained by factoring the probabilities of each action with the weight estimated for the particular distribution: Au(as) = {(a0u</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R. L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Workshop on Pattern Recognition in Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jung</author>
<author>C Lee</author>
<author>K Kim</author>
<author>G G Lee</author>
</authors>
<title>Hybrid approach to user intention modeling for dialog simulation. In</title>
<date>2009</date>
<booktitle>Proc. ACL-IJCNLP.</booktitle>
<contexts>
<context position="7480" citStr="Jung et al., 2009" startWordPosition="1219" endWordPosition="1222">r Goal values for the required concepts, which makes the user simulated responses consistent with the user goal. The Error Model is responsible of simulating the noisy communication channel between user and system; as we simulate the error at SLU level, errors consist of incorrect concept values. We experiment with a data-driven model where the precision Pr, obtained by a concept c in the reference dataset is used to estimate the frequency with which an error in the true value v� of c will be introduced: P(c(v)|c(v)) = 1 − Pr, (Quarteroni et al., 2010). 3 Modelling Cooperativeness As in e.g. (Jung et al., 2009), we define cooperativeness at the turn level (coops) as a function of the number of dialog acts in the DM action as sharing concepts with the dialog acts in the user action au; at the dialog level, coop is the average of turn-level cooperativeness. We discretize coop into a binary variable reflecting high vs low cooperativeness based on whether or not the dialog cooperativeness exceeds the median value of coop found in a reference corpus; in our ADASearch dataset, the median value found for coop is 0.28; hence, we annotate dialogs as cooperative if they exceed such a threshold, and as uncoope</context>
</contexts>
<marker>Jung, Lee, Kim, Lee, 2009</marker>
<rawString>S. Jung, C. Lee, K. Kim, and G. G. Lee. 2009. Hybrid approach to user intention modeling for dialog simulation. In Proc. ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Pietquin</author>
</authors>
<title>A Framework for Unsupervised Learning of Dialogue Strategies.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Facult´e Polytechnique de Mons, TCTS Lab (Belgique).</institution>
<contexts>
<context position="6147" citStr="Pietquin, 2004" startWordPosition="985" endWordPosition="986">om the DM to continue the conversation will be either readily met with a new task request or denied at a fixed probability: Au(as) _ {(au, 1)}. In the Bigram model, first defined in (Eckert et al., 1997), a transition matrix records the frequencies of transition from DM actions to user actions, including hang up and no input/no match. Given a DM action as, the model responds with a list of M user actions and their probabilities estimated according to action distribution in the real data: Au(as) _ {(a°, P(a°|as)), .., (aMu , P(aMu |as))}. The Task-based model, similarly to the “goal” model in (Pietquin, 2004), produces an action distribution containing only the actions observed in the dataset of dialogs in the context of a specific task Tk. The TB model divides the dataset into one partition for each Tk, then creates a task-specific bigram model, by computing b k: Au(as) _ {(a°, P(a°|as, Tk)), .., (aMu , P(aMu |as, Tk))}. As the partition of the dataset reduces the number of observations, the TB model includes a mechanism to back off to the simpler bigram and unigram models. 2.3 Concept &amp; Error Model The Concept Model takes the action bu selected by the DA Model and attaches values and sampled int</context>
</contexts>
<marker>Pietquin, 2004</marker>
<rawString>O. Pietquin. 2004. A Framework for Unsupervised Learning of Dialogue Strategies. Ph.D. thesis, Facult´e Polytechnique de Mons, TCTS Lab (Belgique).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Quarteroni</author>
<author>M Gonz´alez</author>
<author>G Riccardi</author>
<author>S Varges</author>
</authors>
<title>Combining user intention and error modeling for statistical dialog simulators.</title>
<date>2010</date>
<booktitle>In Proc. INTERSPEECH.</booktitle>
<marker>Quarteroni, Gonz´alez, Riccardi, Varges, 2010</marker>
<rawString>S. Quarteroni, M. Gonz´alez, G. Riccardi, and S. Varges. 2010. Combining user intention and error modeling for statistical dialog simulators. In Proc. INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Georgila</author>
<author>S Young</author>
</authors>
<title>Quantitative evaluation of user simulation techniques for spoken dialogue systems.</title>
<date>2005</date>
<booktitle>In Proc. SIGDIAL.</booktitle>
<contexts>
<context position="11464" citStr="Schatzmann et al., 2005" startWordPosition="1898" endWordPosition="1901">line” statistics are used to assess how realistic the action estimations by DA Models are with respect to a dataset of real conversations (Sec. 4.1); then, “online” statistics (Sec. 4.2) evaluate end-to-end simulator performance in terms of dialog act distributions, error robustness and task duration and completion rates by comparing real dialogs with fresh simulated dialogs using action sampling in the different simulation models. 4.1 “Offline” statistics In order to compare simulated and real user actions, we evaluate dialog act Precision (PDA) and Recall (RDA) following the methodology in (Schatzmann et al., 2005). For each DM action as the simulator picks a user action au from Au(as) and we compare it with the real user choice au. A simulated dialog act is correct when it appears in the real action au. The measurements were obtained using 5-fold cross-validation on the ADASearch dataset. Table 1: Dialog Act Precision and Recall Simulation (a,*) Most frequent (a,-) DA Model PDA RDA PDA RDA OB 33.8 33.4 33.9 33.5 BI (+coop) 35.6 (35.7) 35.5 (35.8) 49.3 (47.9) 48.8 (47.4) TB (+coop) 38.2 (39.7) 38.1 (39.4) 51.1 (50.6) 50.6 (50.2) Table 1 shows PDA/RDA obtained for the OB, BI and TB models alone and with </context>
</contexts>
<marker>Schatzmann, Georgila, Young, 2005</marker>
<rawString>J. Schatzmann, K. Georgila, and S. Young. 2005. Quantitative evaluation of user simulation techniques for spoken dialogue systems. In Proc. SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>M Stuttle</author>
<author>S Young</author>
</authors>
<title>A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies.</title>
<date>2006</date>
<journal>Knowl. Eng. Rev.,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1267" citStr="Schatzmann et al., 2006" startWordPosition="173" endWordPosition="176">hat combines traits of human behavior such as cooperativeness and context with domain-related aspects via the Expectation-Maximization algorithm. We show that cooperativeness provides a finer representation of the dialog context which directly affects task completion rate. 1 Introduction Data-driven techniques are a promising approach to the development of robust (spoken) dialog systems, particularly when training statistical dialog managers (Varges et al., 2009). User simulators have been introduced to cope with the scarcity of real user conversations and optimize a number of SDS components (Schatzmann et al., 2006). In this work, we investigate the combination of aspects of human behavior with contextual aspects of conversation in a joint yet modular data-driven simulation model. For this, we integrate conversational context representation, centered on a Dialog Act and a Concept Model, with a User Model representing persistent individual features. Our aim is to evaluate different simulation regimes against real dialogs to identify any impact of user-specific features on dialog performance. In this paper, Section 2 presents our simulator architecture and Section 3 focuses on our model of cooperativeness.</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young. 2006. A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. Knowl. Eng. Rev., 21(2):97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Varges</author>
<author>S Quarteroni</author>
<author>G Riccardi</author>
<author>A V Ivanov</author>
<author>P Roberti</author>
</authors>
<title>Leveraging POMDPs trained with user simulations and rule-based dialogue management in a spoken dialogue system.</title>
<date>2009</date>
<booktitle>In Proc. SIGDIAL.</booktitle>
<contexts>
<context position="1110" citStr="Varges et al., 2009" startWordPosition="148" endWordPosition="151">n simulation where the “intentional” component of user simulation includes a User Model representing userspecific features. We train a dialog simulator that combines traits of human behavior such as cooperativeness and context with domain-related aspects via the Expectation-Maximization algorithm. We show that cooperativeness provides a finer representation of the dialog context which directly affects task completion rate. 1 Introduction Data-driven techniques are a promising approach to the development of robust (spoken) dialog systems, particularly when training statistical dialog managers (Varges et al., 2009). User simulators have been introduced to cope with the scarcity of real user conversations and optimize a number of SDS components (Schatzmann et al., 2006). In this work, we investigate the combination of aspects of human behavior with contextual aspects of conversation in a joint yet modular data-driven simulation model. For this, we integrate conversational context representation, centered on a Dialog Act and a Concept Model, with a User Model representing persistent individual features. Our aim is to evaluate different simulation regimes against real dialogs to identify any impact of user</context>
</contexts>
<marker>Varges, Quarteroni, Riccardi, Ivanov, Roberti, 2009</marker>
<rawString>S. Varges, S. Quarteroni, G. Riccardi, A. V. Ivanov, and P. Roberti. 2009. Leveraging POMDPs trained with user simulations and rule-based dialogue management in a spoken dialogue system. In Proc. SIGDIAL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>