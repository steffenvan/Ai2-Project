<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.172610">
<title confidence="0.995869">
Biomedical Named Entity Recognition Using
Conditional Random Fields and Rich Feature Sets
</title>
<author confidence="0.999317">
Burr Settles
</author>
<affiliation confidence="0.998741">
Department of Computer Sciences
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
</affiliation>
<address confidence="0.869142">
Madison, WI, USA
</address>
<email confidence="0.999429">
bsettles@cs.wisc.edu
</email>
<sectionHeader confidence="0.998472" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984875">
As the wealth of biomedical knowledge in the
form of literature increases, there is a rising need
for effective natural language processing tools
to assist in organizing, curating, and retrieving
this information. To that end, named entity
recognition (the task of identifying words and
phrases in free text that belong to certain classes
of interest) is an important first step for many
of these larger information management goals.
In recent years, much attention has been fo-
cused on the problem of recognizing gene and
protein mentions in biomedical abstracts. This
paper presents a framework for simultaneously
recognizing occurrences of PROTEIN, DNA, RNA,
CELL-LINE, and CELL-TYPE entity classes us-
ing Conditional Random Fields with a variety
of traditional and novel features. I show that
this approach can achieve an overall F1 mea-
sure around 70, which seems to be the current
state of the art.
The system described here was developed as
part of the BioNLP/NLPBA 2004 shared task.
Experiments were conducted on a training and
evaluation set provided by the task organizers.
</bodyText>
<sectionHeader confidence="0.960823" genericHeader="method">
2 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.990555851851852">
Biomedical named entity recognition can be
thought of as a sequence segmentation prob-
lem: each word is a token in a sequence to
be assigned a label (e.g. PROTEIN, DNA, RNA,
CELL-LINE, CELL-TYPE, or OTHER1). Conditional
Random Fields (CRFs) are undirected statisti-
cal graphical models, a special case of which is a
linear chain that corresponds to a conditionally
trained finite-state machine. Such models are
well suited to sequence analysis, and CRFs in
&apos;More accurately, the data is in IOB format. B-DNA
labels the first word of a DNA mention, I-DNA labels all
subsequent words (likewise for other entities), and O la-
bels non-entities. For simplicity, this paper only refers
to the entities, not all the IOB label variants.
particular have been shown to be useful in part-
of-speech tagging (Lafferty et al., 2001), shallow
parsing (Sha and Pereira, 2003), and named en-
tity recognition for newswire data (McCallum
and Li, 2003). They have also just recently been
applied to the more limited task of finding gene
and protein mentions (McDonald and Pereira,
2004), with promising early results.
Let o = (o1, o2, ... , on) be an sequence of
observed words of length n. Let 5 be a set
of states in a finite state machine, each corre-
sponding to a label l E L (e.g. PROTEIN, DNA,
etc.). Let s = (s1, s2, ... , sn) be the sequence
of states in 5 that correspond to the labels as-
signed to words in the input sequence o. Linear-
chain CRFs define the conditional probability of
a state sequence given an input sequence to be:
where Zo is a normalization factor of all state
sequences, fj(si_1, si, o, i) is one of m functions
that describes a feature, and λj is a learned
weight for each such feature function. This pa-
per considers the case of CRFs that use a first-
order Markov independence assumption with
binary feature functions. For example, a fea-
ture may have a value of 0 in most cases, but
given the text “the ATPase” it has the value 1
along the transition where si_1 corresponds to
a state with the label OTHER, si corresponds to a
state with the label PROTEIN, and fj is the fea-
ture function WORD=ATPase E o at position
i in the sequence. Other feature functions that
could have the value 1 along this transition are
CAPITALIZED, MIXEDCASE, and SUFFIX=ase.
Intuitively, the learned feature weight λj
for each feature fj should be positive for fea-
tures that are correlated with the target label,
negative for features that are anti-correlated
with the label, and near zero for relatively
uninformative features. These weights are
</bodyText>
<equation confidence="0.979087">
⎛
n m
P(s|o) =- p �. /_-,Zo ⎝i=1j=1
1
</equation>
<page confidence="0.985586">
104
</page>
<bodyText confidence="0.978270647058824">
set to maximize the conditional log likelihood
of labeled sequences in a training set D
=
f(o, l)(1), ... , (o, l)(n)�:
When the training state sequences are fully
labeled and unambiguous, the objective func-
tion is convex, thus the model is guaranteed
to find the optimal weight settings in terms of
LL(D). Once these settings are found, the la-
beling for an new, unlabeled sequence can be
done using a modified Viterbi algorithm. CRFs
are presented in more complete detail by Laf-
ferty et al. (2001).
These experiments use the MALLET imple-
mentation of CRFs (McCallum, 2002), which
uses a quasi-Newton method called L-BFGS to
find these feature weights efficiently.
</bodyText>
<sectionHeader confidence="0.994483" genericHeader="method">
3 Feature Set
</sectionHeader>
<bodyText confidence="0.999942">
One property that makes feature based statisti-
cal models like CRFs so attractive is that they
reduce the problem to finding an appropriate
feature set. This section outlines the two main
types of features used in these experiments.
</bodyText>
<subsectionHeader confidence="0.996971">
3.1 Orthographic Features
</subsectionHeader>
<bodyText confidence="0.999986192307692">
The simplest and most obvious feature set is the
vocabulary from the training data. Generaliza-
tions over how these words appear (e.g. capital-
ization, affixes, etc.) are also important. The
present model includes training vocabulary, 17
orthographic features based on regular expres-
sions (e.g. ALPHANUMERIC, HASDASH, RO-
MANNUMERAL) as well as prefixes and suffixes
in the character length range [3,5].
Words are also assigned a generalized “word
class” similar to Collins (2002), which replaces
capital letters with ‘A’, lowercase letters with
‘a’, digits with ‘0’, and all other characters
with ‘ ’. There is a similar “brief word class”
feature which collapses consecutive identical
characters into one. Thus the words “IL5”
and “SH3” would both be given the features
WC=AA0 and BWC=A0, while “F-actin” and
“T-cells” would both be assigned WC=A aaaaa
and BWC=A a.
To model local context simply, neighboring
words in the window [-1,1] are also added as
features. For instance, the middle token in the
sequence “human UDG promoter” would have
features WORD=UDG, NEIGHBOR=human and
NEIGHBOR=promoter.
</bodyText>
<subsectionHeader confidence="0.99934">
3.2 Semantic Features
</subsectionHeader>
<bodyText confidence="0.999948145833333">
In addition to orthography, the model could also
benefit from generalized semantic word groups.
If training sequences contain “PML/RAR al-
pha,” “beta 2-M,” and “kappa B-specific DNA
binding protein” all labeled with PROTEIN, the
model might learn that the words “alpha,”
“beta,” and “kappa” are indicative of pro-
teins, but cannot capture the fact that they
are all semantically related because they are
Greek letters. Similarly, words with the feature
WC=Aaa are often part of protein names, such
as “Rab,” “Alu,” and “Gag.” But the model
may have a difficult time setting the weights
for this feature when confronted with words like
“Phe,” “Arg,” and “Cys,” which are amino acid
abbreviations and not often labeled as part of a
protein name.
This sort of semantic domain knowledge can
be provided in the form of lexicons. I pre-
pared a total of 17 such lexicons, which include
7 that were entered by hand (Greek letters,
amino acids, chemical elements, known viruses,
plus abbreviations of all these), and 4 corre-
sponding to genes, chromosome locations, pro-
teins, and cell lines, drawn from online public
databases (Cancer GeneticsWeb,2 BBID,3 Swis-
sProt,4 and the Cell Line Database5). Feature
functions for the lexicons are set to 1 if they
match words in the input sequence exactly. For
lexicon entries that are multi-word, all words
are required to match in the input sequence.
Since no suitable database of terms for the
CELL-TYPE class was found online, a lexicon was
constructed by utilizing Google Sets,6 an online
tool which takes a few seed examples and lever-
ages Google’s web index to return other terms
that appear in similar formatting and context
as the seeds on web pages across the Internet.
Several examples from the training data (e.g.
“lymphocyte” and “neutrophil”) were used as
seeds and new cell types (e.g. “chondroblast,”
which doesn’t even occur in the training data),
were returned. The process was repeated until
the lexicon grew to roughly 50 entries, though
it could probably be more complete.
With all this information at the model’s dis-
posal, it can still be difficult to properly dis-
ambiguate between these entities. For exam-
</bodyText>
<footnote confidence="0.9999164">
2http://www.cancerindex.org/geneweb/
3http://bbid.grc.nia.nih.gov/bbidgene.html
4http://us.expasy.org/sprot/
5http://www.biotech.ist.unige.it/interlab/cldb.html
6http://labs.google.com/sets
</footnote>
<equation confidence="0.955445333333333">
LL(D) = n ( ) m λ2j
i=1 log P (l(i)|o(i)) − E 2σ2.
j=1
</equation>
<page confidence="0.980961">
105
</page>
<bodyText confidence="0.999783129032258">
ple, the acronym “EPC” appears in these static
lexicons both as a protein (“eosinophil cationic
protein” [sic]) and as a cell line (“epithelioma
papulosum cyprini”). Furthermore, a single
word like “transcript” is sometimes all that
disambiguates between RNA and DNA mentions
(e.g. “BMLF1 transcript”). The CRF can learn
weights for these individual words, but it may
help to build general, dynamic keyword lexi-
cons that are associated with each label to assist
in disambiguating between similar classes (and
perhaps boost performance on low-frequency la-
bels, such as RNA and CELL-LINE, for which
training data are sparse).
These keyword lexicons are generated auto-
matically as follows. All of the labeled terms are
extracted from the training set and separated
into five lists (one for each entity class). Stop
words, Greek letters, and digits are filtered, and
remaining words are tallied for raw frequency
counts under each entity class label. These fre-
quencies are then subjected to a x2 test, where
the null hypothesis is that a word’s frequency is
the same for a given entity as it is for any other
entity of interest (i.e. PROTEIN vs. DNA + RNA
+ CELL-LINE + CELL-TYPE, such that there is
only one degree of freedom). All words for which
the null hypothesis is rejected with a p-value
&lt; 0.005 are added to the keyword lexicon for
its majority class. Some example keywords are
listed in table 1.
</bodyText>
<table confidence="0.999459473684211">
Keyword x2 value Lexicon
protein 1121.5 PROTEIN
gene 984.3 DNA
line 618.1 CELL-LINE
promoter 613.4 DNA
factor 563.2 PROTEIN
site 399.8 DNA
receptor 338.7 PROTEIN
complex 312.8 PROTEIN
mRNA 292.2 RNA
sequence 196.5 DNA
peripheral 57.8 CELL-TYPE
lineage 56.1 CELL-TYPE
jurkat 45.2 CELL-LINE
culture 41.3 CELL-LINE
transcript 40.9 RNA
clone 38.1 CELL-LINE
mononuclear 30.2 CELL-TYPE
messenger 12.3 RNA
</table>
<tableCaption confidence="0.99849">
Table 1: A sample of high-ranking semantic key-
words and the lexicons to which they belong.
</tableCaption>
<table confidence="0.9999049375">
Orthographic Features Only
Entity R P F1 L-F1 R-F1
PROTEIN 76.3 68.4 72.1 77.4 79.2
DNA 62.4 68.2 65.2 68.5 73.8
RNA 61.9 62.9 62.4 64.9 75.2
CELL-LINE 53.8 54.0 53.9 58.5 65.1
CELL-TYPE 63.6 78.5 70.3 72.6 80.4
Overall 70.3 69.3 69.8 74.2 77.9
Complete Feature Set
Entity R P F1 L-F1 R-F1
PROTEIN 76.1 68.2 72.0 77.3 79.2
DNA 62.1 67.9 64.9 67.7 74.1
RNA 65.3 64.2 64.7 66.4 73.9
CELL-LINE 57.4 54.1 55.7 59.2 64.2
CELL-TYPE 61.7 78.4 69.1 71.3 79.7
Overall 70.0 69.0 69.5 73.7 77.7
</table>
<tableCaption confidence="0.928754">
Table 2: Detailed performance of the two fea-
tures sets. Relaxed F1-scores using left- and
right-boundary matching are also reported.
</tableCaption>
<sectionHeader confidence="0.991082" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.99992175862069">
Two experiments were completed in the time
allotted: one CRF model using only the ortho-
graphic features described in section 3.1, and a
second system using all the semantic lexicons
from 3.2 as well. Detailed results are presented
in table 2. The orthographic model achieves an
overall F1 measure of 69.8 on the evaluation set
(88.9 on the training set), converging after 230
training iterations and approximately 18 hours
of computation. The complete model, however,
only reached an overall F1 of 69.5 on the evalu-
ation set (86.7 on the training set), converging
after 152 iterations in approximately 9 hours.
The deleterious effect of the semantic lexi-
cons is surprising and puzzling.7 However, even
though semantic lexicons slightly decrease over-
all performance, it is worthwhile to note that
adding lexicons actually improves both recall
and precision for the RNA and CELL-LINE en-
tities. These happen to be the two lowest fre-
quency class labels in the data, together com-
prising less than 10% of the mentions in either
the training or evaluation set. Error analysis
shows that several of the orthographic model’s
false negatives for these entities are of the form
“messenger accumulation” (RNA) or “nonadher-
ent culture” (CELL-LINE). It may be that key-
word lexicons contributed to the model identify-
ing these low frequency terms more accurately.
</bodyText>
<footnote confidence="0.511442666666667">
7Note, however, that these figures are on a single
training/evaluation split without cross-validation, so dif-
ferences are likely not statistically significant.
</footnote>
<page confidence="0.995745">
106
</page>
<bodyText confidence="0.999967071428571">
Also of note is that, in both experiments, the
CRF framework achieves somewhat comparable
performance across all entities. In a previous
attempt to use a Hidden Markov Model to si-
multaneously recognize multiple biomedical en-
tities (Collier et al., 2000), HMM performance
for a particular entity seemed more or less pro-
portional to its frequency in the data. The ad-
vantage of the CRF here may be due to the
fact that HMMs are generative models trained
to learn the joint probability P(o, l) — where
data for l may be sparse — and use Bayes rule
to predict the best label. CRFs are discrimina-
tive models trained to maximize P(l|o) directly.
</bodyText>
<sectionHeader confidence="0.995456" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999925032258065">
In short, I have presented in detail a frame-
work for recognizing multiple entity classes
in biomedical abstracts with Conditional Ran-
dom Fields. I have shown that a CRF-based
model with only simple orthographic features
can achieve performance near the current state
of the art, while using semantic lexicons (as
presented here) do not positively affect perfor-
mance.$
While the system presented here shows
promise, there is still much to be explored.
Richer syntactic information such as shallow
parsing may be useful. The method introduced
in section 3.2 to generate semantic keywords can
also be adapted to generate features for entity-
specific morphology (e.g. affixes) and context,
both linearly (e.g. neighboring words) and hi-
erarchically (e.g. from a parse).
Most interesting, though, might be to inves-
tigate why the lexicons do not generally help.
One explanation is simply an issue of tokeniza-
tion. While one abstract refers to “IL12,” oth-
ers may write “IL-12” or “IL 12.” Similarly,
the generalization of entities to groups (e.g. “x
antibody” vs. “x antibodies”) can cause prob-
lems for these rigid lexicons that require exact
matching. Enumerating all such variants for ev-
ery entry in a lexicon is absurd. Perhaps relax-
ing the matching criteria and standardizing to-
kenization for both the input and lexicons will
improve their utility.
</bodyText>
<footnote confidence="0.924967571428571">
8More recent work (not submitted for evaluation) in-
dicates that lexicons are indeed useful, but mainly when
training data are limited. I have also found that using
orthographic features with part-of-speech tags and only
the RNA and CELL-LINE (rare class) lexicons can boost
overall F1 to 70.3 on the evaluation data, with particu-
lar improvements for the RNA and CELL-LINE entities.
</footnote>
<sectionHeader confidence="0.981986" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.976090833333333">
I would like to thank my advisor Mark Craven for
his advice and guidance, as well as Andrew McCal-
lum and Aron Culotta for answering my questions
about the MALLET system. This work is supported
by NLM training grant 5T15LM007359-02 and NIH
grant R01 LM07050-01.
</bodyText>
<sectionHeader confidence="0.991743" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999782916666666">
Nigel Collier, Chikashi Nobata, and Jun ichi Tsu-
jii. 2000. Extracting the names of genes and gene
products with a hidden markov model. In Pro-
ceedings of the International Conference on Com-
putational Linguistics, pages 201–207. Saarbru-
ucken, Germany.
Michael Collins. 2002. Ranking algorithms for
named-entity extraction: Boosting and the voted
perceptron. In Proceedings of the Association
for Computational Linguistics Conference, pages
489–496. Philadelphia, USA.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning. Williamstown,
MA, USA.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional ran-
dom fields, feature induction and web-enhanced
lexicons. In Proceedings of the Conference on Nat-
ural Language Learning, pages 188–191. Edmon-
ton, Canada.
Andrew McCallum. 2002. Mallet: A
machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2004. Iden-
tifying gene and protein mentions in text us-
ing conditional random fields. In Proceedings of
BioCreative: Critical Assessment for Information
Extraction in Biology. Grenada, Spain.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of the Human Language Technology and North
American Association for Computational Linguis-
tics Conference. Edmonton, Canada.
</reference>
<page confidence="0.998702">
107
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798888">
<title confidence="0.998278">Biomedical Named Entity Recognition Conditional Random Fields and Rich Feature Sets</title>
<author confidence="0.997238">Burr</author>
<affiliation confidence="0.999716">Department of Computer Department of Biostatistics and Medical University of</affiliation>
<address confidence="0.813021">Madison, WI,</address>
<email confidence="0.988267">bsettles@cs.wisc.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nigel Collier</author>
<author>Chikashi Nobata</author>
<author>Jun ichi Tsujii</author>
</authors>
<title>Extracting the names of genes and gene products with a hidden markov model.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>201--207</pages>
<location>Saarbruucken, Germany.</location>
<contexts>
<context position="12709" citStr="Collier et al., 2000" startWordPosition="2063" endWordPosition="2066">tities are of the form “messenger accumulation” (RNA) or “nonadherent culture” (CELL-LINE). It may be that keyword lexicons contributed to the model identifying these low frequency terms more accurately. 7Note, however, that these figures are on a single training/evaluation split without cross-validation, so differences are likely not statistically significant. 106 Also of note is that, in both experiments, the CRF framework achieves somewhat comparable performance across all entities. In a previous attempt to use a Hidden Markov Model to simultaneously recognize multiple biomedical entities (Collier et al., 2000), HMM performance for a particular entity seemed more or less proportional to its frequency in the data. The advantage of the CRF here may be due to the fact that HMMs are generative models trained to learn the joint probability P(o, l) — where data for l may be sparse — and use Bayes rule to predict the best label. CRFs are discriminative models trained to maximize P(l|o) directly. 5 Conclusions and Future Work In short, I have presented in detail a framework for recognizing multiple entity classes in biomedical abstracts with Conditional Random Fields. I have shown that a CRF-based model wit</context>
</contexts>
<marker>Collier, Nobata, Tsujii, 2000</marker>
<rawString>Nigel Collier, Chikashi Nobata, and Jun ichi Tsujii. 2000. Extracting the names of genes and gene products with a hidden markov model. In Proceedings of the International Conference on Computational Linguistics, pages 201–207. Saarbruucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics Conference,</booktitle>
<pages>489--496</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="5363" citStr="Collins (2002)" startWordPosition="887" endWordPosition="888">ing an appropriate feature set. This section outlines the two main types of features used in these experiments. 3.1 Orthographic Features The simplest and most obvious feature set is the vocabulary from the training data. Generalizations over how these words appear (e.g. capitalization, affixes, etc.) are also important. The present model includes training vocabulary, 17 orthographic features based on regular expressions (e.g. ALPHANUMERIC, HASDASH, ROMANNUMERAL) as well as prefixes and suffixes in the character length range [3,5]. Words are also assigned a generalized “word class” similar to Collins (2002), which replaces capital letters with ‘A’, lowercase letters with ‘a’, digits with ‘0’, and all other characters with ‘ ’. There is a similar “brief word class” feature which collapses consecutive identical characters into one. Thus the words “IL5” and “SH3” would both be given the features WC=AA0 and BWC=A0, while “F-actin” and “T-cells” would both be assigned WC=A aaaaa and BWC=A a. To model local context simply, neighboring words in the window [-1,1] are also added as features. For instance, the middle token in the sequence “human UDG promoter” would have features WORD=UDG, NEIGHBOR=human a</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking algorithms for named-entity extraction: Boosting and the voted perceptron. In Proceedings of the Association for Computational Linguistics Conference, pages 489–496. Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<location>Williamstown, MA, USA.</location>
<contexts>
<context position="2195" citStr="Lafferty et al., 2001" startWordPosition="339" endWordPosition="342">E, or OTHER1). Conditional Random Fields (CRFs) are undirected statistical graphical models, a special case of which is a linear chain that corresponds to a conditionally trained finite-state machine. Such models are well suited to sequence analysis, and CRFs in &apos;More accurately, the data is in IOB format. B-DNA labels the first word of a DNA mention, I-DNA labels all subsequent words (likewise for other entities), and O labels non-entities. For simplicity, this paper only refers to the entities, not all the IOB label variants. particular have been shown to be useful in partof-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), and named entity recognition for newswire data (McCallum and Li, 2003). They have also just recently been applied to the more limited task of finding gene and protein mentions (McDonald and Pereira, 2004), with promising early results. Let o = (o1, o2, ... , on) be an sequence of observed words of length n. Let 5 be a set of states in a finite state machine, each corresponding to a label l E L (e.g. PROTEIN, DNA, etc.). Let s = (s1, s2, ... , sn) be the sequence of states in 5 that correspond to the labels assigned to words in the input sequence o. Li</context>
<context position="4449" citStr="Lafferty et al. (2001)" startWordPosition="744" endWordPosition="748">the label, and near zero for relatively uninformative features. These weights are ⎛ n m P(s|o) =- p �. /_-,Zo ⎝i=1j=1 1 104 set to maximize the conditional log likelihood of labeled sequences in a training set D = f(o, l)(1), ... , (o, l)(n)�: When the training state sequences are fully labeled and unambiguous, the objective function is convex, thus the model is guaranteed to find the optimal weight settings in terms of LL(D). Once these settings are found, the labeling for an new, unlabeled sequence can be done using a modified Viterbi algorithm. CRFs are presented in more complete detail by Lafferty et al. (2001). These experiments use the MALLET implementation of CRFs (McCallum, 2002), which uses a quasi-Newton method called L-BFGS to find these feature weights efficiently. 3 Feature Set One property that makes feature based statistical models like CRFs so attractive is that they reduce the problem to finding an appropriate feature set. This section outlines the two main types of features used in these experiments. 3.1 Orthographic Features The simplest and most obvious feature set is the vocabulary from the training data. Generalizations over how these words appear (e.g. capitalization, affixes, etc</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning. Williamstown, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning,</booktitle>
<pages>188--191</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2308" citStr="McCallum and Li, 2003" startWordPosition="357" endWordPosition="360">ich is a linear chain that corresponds to a conditionally trained finite-state machine. Such models are well suited to sequence analysis, and CRFs in &apos;More accurately, the data is in IOB format. B-DNA labels the first word of a DNA mention, I-DNA labels all subsequent words (likewise for other entities), and O labels non-entities. For simplicity, this paper only refers to the entities, not all the IOB label variants. particular have been shown to be useful in partof-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), and named entity recognition for newswire data (McCallum and Li, 2003). They have also just recently been applied to the more limited task of finding gene and protein mentions (McDonald and Pereira, 2004), with promising early results. Let o = (o1, o2, ... , on) be an sequence of observed words of length n. Let 5 be a set of states in a finite state machine, each corresponding to a label l E L (e.g. PROTEIN, DNA, etc.). Let s = (s1, s2, ... , sn) be the sequence of states in 5 that correspond to the labels assigned to words in the input sequence o. Linearchain CRFs define the conditional probability of a state sequence given an input sequence to be: where Zo is </context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proceedings of the Conference on Natural Language Learning, pages 188–191. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="4523" citStr="McCallum, 2002" startWordPosition="758" endWordPosition="759"> ⎛ n m P(s|o) =- p �. /_-,Zo ⎝i=1j=1 1 104 set to maximize the conditional log likelihood of labeled sequences in a training set D = f(o, l)(1), ... , (o, l)(n)�: When the training state sequences are fully labeled and unambiguous, the objective function is convex, thus the model is guaranteed to find the optimal weight settings in terms of LL(D). Once these settings are found, the labeling for an new, unlabeled sequence can be done using a modified Viterbi algorithm. CRFs are presented in more complete detail by Lafferty et al. (2001). These experiments use the MALLET implementation of CRFs (McCallum, 2002), which uses a quasi-Newton method called L-BFGS to find these feature weights efficiently. 3 Feature Set One property that makes feature based statistical models like CRFs so attractive is that they reduce the problem to finding an appropriate feature set. This section outlines the two main types of features used in these experiments. 3.1 Orthographic Features The simplest and most obvious feature set is the vocabulary from the training data. Generalizations over how these words appear (e.g. capitalization, affixes, etc.) are also important. The present model includes training vocabulary, 17 </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Identifying gene and protein mentions in text using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of BioCreative: Critical Assessment for Information Extraction in Biology. Grenada,</booktitle>
<contexts>
<context position="2442" citStr="McDonald and Pereira, 2004" startWordPosition="379" endWordPosition="382">nalysis, and CRFs in &apos;More accurately, the data is in IOB format. B-DNA labels the first word of a DNA mention, I-DNA labels all subsequent words (likewise for other entities), and O labels non-entities. For simplicity, this paper only refers to the entities, not all the IOB label variants. particular have been shown to be useful in partof-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), and named entity recognition for newswire data (McCallum and Li, 2003). They have also just recently been applied to the more limited task of finding gene and protein mentions (McDonald and Pereira, 2004), with promising early results. Let o = (o1, o2, ... , on) be an sequence of observed words of length n. Let 5 be a set of states in a finite state machine, each corresponding to a label l E L (e.g. PROTEIN, DNA, etc.). Let s = (s1, s2, ... , sn) be the sequence of states in 5 that correspond to the labels assigned to words in the input sequence o. Linearchain CRFs define the conditional probability of a state sequence given an input sequence to be: where Zo is a normalization factor of all state sequences, fj(si_1, si, o, i) is one of m functions that describes a feature, and λj is a learned </context>
</contexts>
<marker>McDonald, Pereira, 2004</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2004. Identifying gene and protein mentions in text using conditional random fields. In Proceedings of BioCreative: Critical Assessment for Information Extraction in Biology. Grenada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference.</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2236" citStr="Sha and Pereira, 2003" startWordPosition="345" endWordPosition="348">(CRFs) are undirected statistical graphical models, a special case of which is a linear chain that corresponds to a conditionally trained finite-state machine. Such models are well suited to sequence analysis, and CRFs in &apos;More accurately, the data is in IOB format. B-DNA labels the first word of a DNA mention, I-DNA labels all subsequent words (likewise for other entities), and O labels non-entities. For simplicity, this paper only refers to the entities, not all the IOB label variants. particular have been shown to be useful in partof-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), and named entity recognition for newswire data (McCallum and Li, 2003). They have also just recently been applied to the more limited task of finding gene and protein mentions (McDonald and Pereira, 2004), with promising early results. Let o = (o1, o2, ... , on) be an sequence of observed words of length n. Let 5 be a set of states in a finite state machine, each corresponding to a label l E L (e.g. PROTEIN, DNA, etc.). Let s = (s1, s2, ... , sn) be the sequence of states in 5 that correspond to the labels assigned to words in the input sequence o. Linearchain CRFs define the conditional pro</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference. Edmonton, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>