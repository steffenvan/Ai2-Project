<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000149">
<title confidence="0.971723">
Think Positive: Towards Twitter Sentiment Analysis from Scratch
</title>
<author confidence="0.768945">
Cicero Nogueira dos Santos
</author>
<affiliation confidence="0.6751375">
Brazilian Research Lab
IBM Research
</affiliation>
<email confidence="0.980083">
cicerons@br.ibm.com
</email>
<sectionHeader confidence="0.993556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998818636363636">
In this paper we describe a Deep Convo-
lutional Neural Network (DNN) approach
to perform two sentiment detection tasks:
message polarity classification and con-
textual polarity disambiguation. We apply
the proposed approach for the SemEval-
2014 Task 9: Sentiment Analysis in Twit-
ter. Despite not using any handcrafted
feature or sentiment lexicons, our system
achieves very competitive results for Twit-
ter data.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908272727273">
In this work we apply a recently proposed deep
convolutional neural network (dos Santos and
Gatti, 2014) that exploits from character- to
sentence-level information to perform sentiment
analysis of Twitter messages (tweets). The net-
work proposed by dos Santos and Gatti (2014),
named Character to Sentence Convolutional Neu-
ral Network (CharSCNN), uses two convolutional
layers to extract relevant features from words and
messages of any size.
We evaluate CharSCNN in the unconstrained
track of the SemEval-2014 Task 9: Sentiment
Analysis in Twitter (Rosenthal et al., 2014). Two
subtasks are proposed in the SemEval-2014 Task
9: the contextual polarity disambiguation (Sub-
taskA), which consists in determining the polar-
ity (positive, negative, or neutral) of a marked
word or phrase in a given message; and the
message polarity classification (SubtaskB), which
consists in classifying the polarity of the whole
message. We use the same neural network to per-
form both tasks. The only difference is that in
</bodyText>
<footnote confidence="0.9169415">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.9455932">
SubtaskA, CharSCNN is fed with a text segment
composed by the words in a context window cen-
tered at the target word/phrase. While in Sub-
taskB, CharSCNN is fed with the whole message.
The use of deep neural networks for sentiment
analysis has been the focus of recent research.
However, instead of convolutional neural network,
most investigation has been done in the use of
recursive neural networks (Socher et al., 2011;
Socher et al., 2012; Socher et al., 2013).
</bodyText>
<sectionHeader confidence="0.996465" genericHeader="method">
2 Neural Network Architecture
</sectionHeader>
<bodyText confidence="0.999906">
Given a segment of text (e.g. a tweet), CharSCNN
computes a score for each sentiment label T E
T = {positive, negative, neutral}. In order to
score a text segment, the network takes as input
the sequence of words in the segment, and passes
it through a sequence of layers where features with
increasing levels of complexity are extracted. The
network extracts features from the character-level
up to the sentence-level.
</bodyText>
<subsectionHeader confidence="0.977011">
2.1 Initial Representation Levels
</subsectionHeader>
<bodyText confidence="0.9999808125">
The first layer of the network transforms words
into real-valued feature vectors (embeddings) that
capture morphological, syntactic and semantic in-
formation about the words. We use a fixed-
sized word vocabulary V wrd, and we consider that
words are composed of characters from a fixed-
sized character vocabulary V chr. Given a sen-
tence consisting of N words {w1, w2, ..., wN}, ev-
ery word wn is converted into a vector un =
rrwrd;rwch], which is composed of two sub-
vectors: the word-level embedding rwrd E Rdwrd
and the character-level embedding rwch E Rclu
of wn. While word-level embeddings are meant
to capture syntactic and semantic information,
character-level embeddings capture morphologi-
cal and shape information.
</bodyText>
<page confidence="0.977435">
647
</page>
<note confidence="0.7966375">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 647–651,
Dublin, Ireland, August 23-24, 2014.
</note>
<subsectionHeader confidence="0.728115">
2.1.1 Word-Level Embeddings
</subsectionHeader>
<bodyText confidence="0.955769714285714">
Word-level embeddings are encoded by col-
umn vectors in an embedding matrix Wwrd ∈
Rdwrd×|V wrd|. Each column W wrd
i ∈ Rdwrd cor-
responds to the word-level embedding of the i-th
word in the vocabulary. We transform a word w
into its word-level embedding rwrd by using the
matrix-vector product:
rwrd = Wwrdvw (1)
where vw is a vector of size IV wrdI which has
value 1 at index w and zero in all other positions.
The matrix Wwrd is a parameter to be learned,
and the size of the word-level embedding dwrd is
a hyper-parameter to be chosen by the user.
</bodyText>
<subsectionHeader confidence="0.568108">
2.1.2 Character-Level Embeddings
</subsectionHeader>
<bodyText confidence="0.9245755">
In the task of sentiment analysis of Twitter data,
important information can appear in different parts
of a hash tag (e.g., “#SoSad”, “#ILikeIt”) and
many informative adverbs end with the suffix
“ly” (e.g. “beautifully”, “perfectly” and “badly”).
Therefore, robust methods to extract morphologi-
cal and shape information from this type of tokens
must take into consideration all characters of the
token and select which features are more impor-
tant for sentiment analysis. Like in (dos Santos
and Zadrozny, 2014), we tackle this problem us-
ing a convolutional approach (Waibel et al., 1989),
which works by producing local features around
each character of the word and then combining
them using a max operation to create a fixed-sized
character-level embedding of the word.
Given a word w composed of M characters
{c1, c2, ..., cM}, we first transform each charac-
ter cm into a character embedding rchr
m . Character
embeddings are encoded by column vectors in the
embedding matrix Wchr ∈ Rdchr×|V chr|. Given a
character c, its embedding rchr is obtained by the
matrix-vector product:
rchr = Wchrvc (2)
where vc is a vector of size IV chrI which has value
1 at index c and zero in all other positions. The
input for the convolutional layer is the sequence
of character embeddings {rchr 1,rchr
2 , ...,rchr M }.
The convolutional layer applies a matrix-
vector operation to each window of size
kchr of successive windows in the sequence
{rchr
1 , rchr
2 , ..., rchr
M }. Let us define the vector
zm ∈ Rdchrkchr as the concatenation of the
character embedding m, its (kchr − 1)/2 left
neighbors, and its (kchr − 1)/2 right neighbors:
</bodyText>
<equation confidence="0.953477">
zm = (rm−(kchr−1)/2, ..., rchr
m+(kchr−1)/2
</equation>
<bodyText confidence="0.999611333333333">
The convolutional layer computes the j-th element
of the vector rwch, which is the character-level em-
bedding of w, as follows:
</bodyText>
<equation confidence="0.8423395">
rrwchIj = max [W0zm + b0] (3)
Ir 1&lt;m&lt;M
</equation>
<bodyText confidence="0.999920923076923">
where W0 ∈ Rcl0u×dchrkchr is the weight matrix
of the convolutional layer. The same matrix is
used to extract local features around each charac-
ter window of the given word. Using the max over
all character windows of the word, we extract a
“global” fixed-sized feature vector for the word.
Matrices Wchr and W0, and vector b0 are pa-
rameters to be learned. The size of the char-
acter vector dchr, the number of convolutional
units cl0 u (which corresponds to the size of the
character-level embedding of a word), and the size
of the character context window kchr are hyper-
parameters.
</bodyText>
<subsectionHeader confidence="0.999497">
2.2 Sentence-Level Representation and
Scoring
</subsectionHeader>
<bodyText confidence="0.9942">
Given a text segment x with N words
{w1, w2, ..., wN}, which have been converted to
joint word-level and character-level embedding
{u1, u2, ...,uN}, the next step in CharSCNN
consists in extracting a segment-level represen-
tation rseg
x . Methods to extract a segment-wide
feature set most deal with two main problems:
text segments have different sizes; and important
information can appear at any position in the
segment. A convolutional approach is a good
option to tackle this problems, and therefore
we use a convolutional layer to compute the
segment-wide feature vector rseg. This second
convolutional layer works in a very similar way to
the one used to extract character-level features for
words. This layer produces local features around
each word in the text segment and then combines
them using a max operation to create a fixed-sized
feature vector for the segment.
The second convolutional layer applies a
matrix-vector operation to each window of size
kwrd of successive windows in the sequence
{u1, u2, ..., uN}. Let us define the vector zn ∈
R(dwrd+cl0 u)kwrd as the concatenation of a se-
</bodyText>
<equation confidence="0.798722">
)T
</equation>
<page confidence="0.952753">
648
</page>
<table confidence="0.79003975">
quence of kwrd embeddings, centralized in the n-
th word1:
Taking the log, we arrive at the following con-
ditional log-probability:
�zn = un−(kwrd−1)/2, ..., un+(kwrd−1)/2 )T � �esθ(x)i (7)
log p (τ|x, θ) = sθ(x)τ −log
∀i∈T
The convolutional layer computes the j-th element
of the vector rseg as follows:
Ir seg= max [W1zn + b11j (4)
l 1j1&lt;n&lt;N
where W1 E �u x (dwrd+el�
</table>
<bodyText confidence="0.998554388888889">
][$ )kwrd is the weight
matrix of the convolutional layer. The same ma-
trix is used to extract local features around each
word window of the given segment. Using the max
over all word windows of the segment, we extract
a “global” fixed-sized feature vector for the seg-
ment. Matrix W1 and vector b1 are parameters
to be learned. The number of convolutional units
cl1u (which corresponds to the size of the segment-
level feature vector), and the size of the word con-
text window kwrd are hyper-parameters to be cho-
sen by the user.
Finally, the vector rseg
x , the “global’ feature vec-
tor of text segment x, is processed by two usual
neural network layers, which extract one more
level of representation and compute a score for
each sentiment label τ E T:
</bodyText>
<equation confidence="0.998986">
S(x) = W3h(W2rseg
x + b2) + b3 (5)
</equation>
<bodyText confidence="0.999943">
where matrices W2 E Rhlu×cl� u and W3 E
R|T|×hlu, and vectors b2 E Rhlu and b3 E R|T|
are parameters to be learned. The transfer func-
tion h(.) is the hyperbolic tangent. The size of the
number of hidden units hlu is a hyper-parameter
to be chosen by the user.
</bodyText>
<subsectionHeader confidence="0.992855">
2.3 Network Training
</subsectionHeader>
<bodyText confidence="0.999736">
Our network is trained by minimizing a nega-
tive likelihood over the training set D. Given a
text segment x, the network with parameter set θ
computes a score sθ(x)τ for each sentiment label
τ E T. In order to transform this score into a con-
ditional probability p (τ|x, θ) of the label given the
segment and the set of network parameters θ, we
apply a softmax operation over all tags:
</bodyText>
<equation confidence="0.876642666666667">
esθ(x)τ
p (τ|x, θ) = � (6)
i esθ(x)i
</equation>
<footnote confidence="0.840256">
1We use a special padding token for the words with in-
dices outside of the text segment boundaries.
</footnote>
<bodyText confidence="0.930427">
We use stochastic gradient descent (SGD) to
minimize the negative log-likelihood with respect
to θ:
</bodyText>
<equation confidence="0.9796235">
�θ H −log p(y|x, θ) (8)
(x,y)∈D
</equation>
<bodyText confidence="0.994365444444445">
where (x, y) corresponds to a text segment (e.g. a
tweet) in the training corpus D and y represents its
respective sentiment class label.
We use the backpropagation algorithm to com-
pute the gradients of the network (Lecun et al.,
1998; Collobert, 2011). We implement the
CharSCNN architecture using the automatic dif-
ferentiation capabilities of the Theano library
(Bergstra et al., 2010).
</bodyText>
<sectionHeader confidence="0.998897" genericHeader="method">
3 Experimental Setup and Results
</sectionHeader>
<subsectionHeader confidence="0.9890915">
3.1 Unsupervised Learning of Word-Level
Embeddings
</subsectionHeader>
<bodyText confidence="0.999788347826087">
Unsupervised pre-training of word embeddings
has shown to be an effective approach to improve
model accuracy (Collobert et al., 2011; Luong et
al., 2013; Zheng et al., 2013). In our experiments,
we perform unsupervised learning of word-level
embeddings using the word2vec tool2.
We use two Twitter datasets as sources of un-
labeled data: the Stanford Twitter Sentiment cor-
pus (Go et al., 2009), which contains 1.6 mil-
lion tweets; and a dataset containing 10.4 mil-
lion tweets that were collected in October 2012
for a previous work by the author (Gatti et al.,
2013). We tokenize these corpora using Gimpel et
al.’s (2011) tokenizer, and removed messages that
are less than 5 characters long (including white
spaces) or have less than 3 tokens. Like in (Col-
lobert et al., 2011) and (Luong et al., 2013), we
lowercase all words and substitute each numerical
digit by a 0 (e.g., 1967 becomes 0000). The re-
sulting corpus contains about 12 million tweets.
We do not perform unsupervised learning of
character-level embeddings, which are initial-
ized by randomly sampling each value from an
</bodyText>
<equation confidence="0.639593333333333">
uniform distribution: U (−r, r), where r =
� 6
|V chr |+ dchr . The character vocabulary is
</equation>
<footnote confidence="0.789451">
2https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.997103">
649
</page>
<bodyText confidence="0.999500666666667">
constructed by the (not lowercased) words in the
training set, which allows the neural network to
capture relevant information about capitalization.
</bodyText>
<subsectionHeader confidence="0.999874">
3.2 Sentiment Corpora and Model Setup
</subsectionHeader>
<bodyText confidence="0.996922">
SemEval-2014 Task 9 is a rerun of the SemEval-
2013 Task 2 (Nakov et al., 2013), hence the train-
ing set used in 2014 is the same of the 2013 task.
However, as we downloaded the Twitter training
and development sets in 2014 only, we were not
able to download the complete dataset since some
tweets have been deleted by their respective cre-
ators. In Table 1, we show the number of messages
in our SemEval-2013 Task 2 datasets.
</bodyText>
<table confidence="0.999439">
Dataset SubtaskA SubtaskB
Train 7390 8213
Dev. 904 1415
Twitter2013 (test) 3491 3265
SMS2013 (test) 2,334 2,093
</table>
<tableCaption confidence="0.925423">
Table 1: Number of tweets in our version of
SemEval-2013 Task2 datasets.
</tableCaption>
<bodyText confidence="0.998249571428571">
In SemEval-2014 Task 9, three different test
sets are used: Twitter2014, Twitter2014Sarcarm
and LiveJournal2014. While the two first contain
Twitter messages, the last one contains sentences
from LiveJournal blogs. In Table 2, we show the
number of messages in the SemEval-2014 Task 9
test datasets.
</bodyText>
<table confidence="0.9993855">
Test Dataset SubtaskA SubtaskB
Twitter2014 2597 1939
Twitter2014Sarcasm 124 86
LiveJournal2014 1315 1142
</table>
<tableCaption confidence="0.987826">
Table 2: Number of tweets in the SemEval-2014
Task9 test datasets.
</tableCaption>
<bodyText confidence="0.998629142857143">
We use the copora Twitter2013 (test) and
SMS2013 to tune CharSCNN’s hyper-parameter
values. In Table 3, we show the selected hyper-
parameter values, which are the same for both
SubtaskA and SubtaskB. We concatenate the
SemEval-2013 Task 2 training and development
sets to train the submitted model.
</bodyText>
<subsectionHeader confidence="0.999399">
3.3 Sentiment Prediction Results
</subsectionHeader>
<bodyText confidence="0.99953325">
In Table 4, we present the official results of our
submission to the SemEval-2014 Task9. In Sub-
taskB, CharSCNN’s result for the Twitter2014 test
corpus is the top 11 out of 50 submissions, and is
</bodyText>
<table confidence="0.999922636363636">
Parameter Parameter Name Value
dwrd Word-Level Emb. dim. 100
kwrd Word Context window 3
dchr Char. Emb. dim. 5
kchr Char. Context window 5
cl0 Char. Convol. Units 30
u
cl� Word Convol. Units 100
u
hlu Hidden Units 300
A Learning Rate 0.02
</table>
<tableCaption confidence="0.998452">
Table 3: Neural Network Hyper-Parameters.
</tableCaption>
<bodyText confidence="0.985326357142857">
3.9 F-measure points from the top performing sys-
tem. In the SubtaskA, CharSCNN’s result for the
Twitter2014 test corpus is the top 6 out of 27 sub-
missions. These are very promising results, since
our approach do not use any handcrafted features
or lexicons, all features (representations) are auto-
matically learned from unlabeled and labeled data.
Nevertheless, our system result for the Live-
Journal2014 corpus in SubtaskB is regular. For
this dataset CharSCNN achieves only the top 25
out of 50 submissions, and is 7.9 F-measure points
behind the top performing system. We believe the
main reason for this poor result is the exclusive use
of Twitter data in the unsupervised pre-training.
</bodyText>
<table confidence="0.999666">
Test Subset SubtaskA SubtaskB
Twitter2014 82.05 67.04
Twitter2014Sarcasm 76.74 47.85
LiveJournal2014 80.90 66.96
Twitter2013 88.06 68.15
SMS2013 87.65 63.20
</table>
<tableCaption confidence="0.9811565">
Table 4: Average F-measure of CharSCNN for dif-
ferent test sets.
</tableCaption>
<sectionHeader confidence="0.999033" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999940166666666">
In this work we describe a sentiment analysis
system based on a deep neural network architec-
ture that analyses text at multiple levels, from
character-level to sentence-level. We apply the
proposed system to the SemEval-2014 Task 9 and
achieve very competitive results for Twitter data in
both contextual polarity disambiguation and mes-
sage polarity classification subtasks. As a future
work, we would like to investigate the impact of
the system performance for the LiveJournal2014
corpus when the unsupervised pre-training is per-
formed using in-domain texts.
</bodyText>
<page confidence="0.997199">
650
</page>
<sectionHeader confidence="0.989948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999838072164949">
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy).
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In Proceedings of the Four-
teenth International Conference on Artificial Intelli-
gence and Statistics (AISTATS), pages 224–232.
Cicero Nogueira dos Santos and Maira Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics (COLING), Dublin, Ireland.
Cicero Nogueira dos Santos and Bianca Zadrozny.
2014. Learning character-level representations for
part-of-speech tagging. In Proceedings of the
31st International Conference on Machine Learning
(ICML), JMLR: W&amp;CP volume 32, Beijing, China.
Maira Gatti, Ana Paula Appel, Cicero Nogueira dos
Santos, Claudio Santos Pinhanez, Paulo Rodrigo
Cavalin, and Samuel Martins Barbosa Neto. 2013.
A simulation-based approach to analyze the infor-
mation diffusion in microblogging online social net-
work. In Winter Simulation Conference, pages
1685–1696.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: Short Papers - Volume 2,
pages 42–47.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report, Stanford University.
Yann Lecun, Lon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE,
pages 2278–2324.
Minh-Thang Luong, Richard Socher, and Christo-
pher D. Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Proceedings of the Conference on Computa-
tional Natural Language Learning, Sofia, Bulgaria.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312–
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval’14, Dublin, Ireland.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of theConference on Empirical Meth-
ods in Natural Language Processing, pages 1201–
1211.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1631–1642.
Alexander Waibel, Toshiyuki Hanazawa, Geoffrey
Hinton, Kiyohiro Shikano, and Kevin J. Lang. 1989.
Phoneme recognition using time-delay neural net-
works. IEEE Transactions on Acoustics, Speech and
Signal Processing, 37(3):328–339.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmentation
and pos tagging. In Proceedings of the Conference
on Empirical Methods in NLP, pages 647–657.
</reference>
<page confidence="0.998523">
651
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341733">
<title confidence="0.942428">Think Positive: Towards Twitter Sentiment Analysis from Scratch</title>
<author confidence="0.575156">Cicero Nogueira dos</author>
<affiliation confidence="0.71876">Brazilian Research IBM</affiliation>
<email confidence="0.998597">cicerons@br.ibm.com</email>
<abstract confidence="0.990291083333333">In this paper we describe a Deep Convolutional Neural Network (DNN) approach to perform two sentiment detection tasks: message polarity classification and contextual polarity disambiguation. We apply the proposed approach for the SemEval- 2014 Task 9: Sentiment Analysis in Twitter. Despite not using any handcrafted feature or sentiment lexicons, our system achieves very competitive results for Twitter data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy).</booktitle>
<contexts>
<context position="10267" citStr="Bergstra et al., 2010" startWordPosition="1714" endWordPosition="1717">x)i 1We use a special padding token for the words with indices outside of the text segment boundaries. We use stochastic gradient descent (SGD) to minimize the negative log-likelihood with respect to θ: �θ H −log p(y|x, θ) (8) (x,y)∈D where (x, y) corresponds to a text segment (e.g. a tweet) in the training corpus D and y represents its respective sentiment class label. We use the backpropagation algorithm to compute the gradients of the network (Lecun et al., 1998; Collobert, 2011). We implement the CharSCNN architecture using the automatic differentiation capabilities of the Theano library (Bergstra et al., 2010). 3 Experimental Setup and Results 3.1 Unsupervised Learning of Word-Level Embeddings Unsupervised pre-training of word embeddings has shown to be an effective approach to improve model accuracy (Collobert et al., 2011; Luong et al., 2013; Zheng et al., 2013). In our experiments, we perform unsupervised learning of word-level embeddings using the word2vec tool2. We use two Twitter datasets as sources of unlabeled data: the Stanford Twitter Sentiment corpus (Go et al., 2009), which contains 1.6 million tweets; and a dataset containing 10.4 million tweets that were collected in October 2012 for </context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="10485" citStr="Collobert et al., 2011" startWordPosition="1745" endWordPosition="1748">, θ) (8) (x,y)∈D where (x, y) corresponds to a text segment (e.g. a tweet) in the training corpus D and y represents its respective sentiment class label. We use the backpropagation algorithm to compute the gradients of the network (Lecun et al., 1998; Collobert, 2011). We implement the CharSCNN architecture using the automatic differentiation capabilities of the Theano library (Bergstra et al., 2010). 3 Experimental Setup and Results 3.1 Unsupervised Learning of Word-Level Embeddings Unsupervised pre-training of word embeddings has shown to be an effective approach to improve model accuracy (Collobert et al., 2011; Luong et al., 2013; Zheng et al., 2013). In our experiments, we perform unsupervised learning of word-level embeddings using the word2vec tool2. We use two Twitter datasets as sources of unlabeled data: the Stanford Twitter Sentiment corpus (Go et al., 2009), which contains 1.6 million tweets; and a dataset containing 10.4 million tweets that were collected in October 2012 for a previous work by the author (Gatti et al., 2013). We tokenize these corpora using Gimpel et al.’s (2011) tokenizer, and removed messages that are less than 5 characters long (including white spaces) or have less than</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS),</booktitle>
<pages>224--232</pages>
<contexts>
<context position="10132" citStr="Collobert, 2011" startWordPosition="1697" endWordPosition="1698">en the segment and the set of network parameters θ, we apply a softmax operation over all tags: esθ(x)τ p (τ|x, θ) = � (6) i esθ(x)i 1We use a special padding token for the words with indices outside of the text segment boundaries. We use stochastic gradient descent (SGD) to minimize the negative log-likelihood with respect to θ: �θ H −log p(y|x, θ) (8) (x,y)∈D where (x, y) corresponds to a text segment (e.g. a tweet) in the training corpus D and y represents its respective sentiment class label. We use the backpropagation algorithm to compute the gradients of the network (Lecun et al., 1998; Collobert, 2011). We implement the CharSCNN architecture using the automatic differentiation capabilities of the Theano library (Bergstra et al., 2010). 3 Experimental Setup and Results 3.1 Unsupervised Learning of Word-Level Embeddings Unsupervised pre-training of word embeddings has shown to be an effective approach to improve model accuracy (Collobert et al., 2011; Luong et al., 2013; Zheng et al., 2013). In our experiments, we perform unsupervised learning of word-level embeddings using the word2vec tool2. We use two Twitter datasets as sources of unlabeled data: the Stanford Twitter Sentiment corpus (Go </context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS), pages 224–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cicero Nogueira dos Santos</author>
<author>Maira Gatti</author>
</authors>
<title>Deep convolutional neural networks for sentiment analysis of short texts.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="684" citStr="Santos and Gatti, 2014" startWordPosition="96" endWordPosition="99">ch Cicero Nogueira dos Santos Brazilian Research Lab IBM Research cicerons@br.ibm.com Abstract In this paper we describe a Deep Convolutional Neural Network (DNN) approach to perform two sentiment detection tasks: message polarity classification and contextual polarity disambiguation. We apply the proposed approach for the SemEval2014 Task 9: Sentiment Analysis in Twitter. Despite not using any handcrafted feature or sentiment lexicons, our system achieves very competitive results for Twitter data. 1 Introduction In this work we apply a recently proposed deep convolutional neural network (dos Santos and Gatti, 2014) that exploits from character- to sentence-level information to perform sentiment analysis of Twitter messages (tweets). The network proposed by dos Santos and Gatti (2014), named Character to Sentence Convolutional Neural Network (CharSCNN), uses two convolutional layers to extract relevant features from words and messages of any size. We evaluate CharSCNN in the unconstrained track of the SemEval-2014 Task 9: Sentiment Analysis in Twitter (Rosenthal et al., 2014). Two subtasks are proposed in the SemEval-2014 Task 9: the contextual polarity disambiguation (SubtaskA), which consists in determ</context>
</contexts>
<marker>Santos, Gatti, 2014</marker>
<rawString>Cicero Nogueira dos Santos and Maira Gatti. 2014. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of the 25th International Conference on Computational Linguistics (COLING), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cicero Nogueira dos Santos</author>
<author>Bianca Zadrozny</author>
</authors>
<title>Learning character-level representations for part-of-speech tagging.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML), JMLR: W&amp;CP</booktitle>
<volume>32</volume>
<location>Beijing, China.</location>
<contexts>
<context position="4728" citStr="Santos and Zadrozny, 2014" startWordPosition="739" endWordPosition="742">ze of the word-level embedding dwrd is a hyper-parameter to be chosen by the user. 2.1.2 Character-Level Embeddings In the task of sentiment analysis of Twitter data, important information can appear in different parts of a hash tag (e.g., “#SoSad”, “#ILikeIt”) and many informative adverbs end with the suffix “ly” (e.g. “beautifully”, “perfectly” and “badly”). Therefore, robust methods to extract morphological and shape information from this type of tokens must take into consideration all characters of the token and select which features are more important for sentiment analysis. Like in (dos Santos and Zadrozny, 2014), we tackle this problem using a convolutional approach (Waibel et al., 1989), which works by producing local features around each character of the word and then combining them using a max operation to create a fixed-sized character-level embedding of the word. Given a word w composed of M characters {c1, c2, ..., cM}, we first transform each character cm into a character embedding rchr m . Character embeddings are encoded by column vectors in the embedding matrix Wchr ∈ Rdchr×|V chr|. Given a character c, its embedding rchr is obtained by the matrix-vector product: rchr = Wchrvc (2) where vc </context>
</contexts>
<marker>Santos, Zadrozny, 2014</marker>
<rawString>Cicero Nogueira dos Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML), JMLR: W&amp;CP volume 32, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maira Gatti</author>
</authors>
<title>Ana Paula Appel, Cicero Nogueira dos Santos, Claudio Santos Pinhanez, Paulo Rodrigo Cavalin, and Samuel Martins Barbosa Neto.</title>
<date>2013</date>
<booktitle>In Winter Simulation Conference,</booktitle>
<pages>1685--1696</pages>
<marker>Gatti, 2013</marker>
<rawString>Maira Gatti, Ana Paula Appel, Cicero Nogueira dos Santos, Claudio Santos Pinhanez, Paulo Rodrigo Cavalin, and Samuel Martins Barbosa Neto. 2013. A simulation-based approach to analyze the information diffusion in microblogging online social network. In Winter Simulation Conference, pages 1685–1696.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<volume>2</volume>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="10745" citStr="Go et al., 2009" startWordPosition="1788" endWordPosition="1791">11). We implement the CharSCNN architecture using the automatic differentiation capabilities of the Theano library (Bergstra et al., 2010). 3 Experimental Setup and Results 3.1 Unsupervised Learning of Word-Level Embeddings Unsupervised pre-training of word embeddings has shown to be an effective approach to improve model accuracy (Collobert et al., 2011; Luong et al., 2013; Zheng et al., 2013). In our experiments, we perform unsupervised learning of word-level embeddings using the word2vec tool2. We use two Twitter datasets as sources of unlabeled data: the Stanford Twitter Sentiment corpus (Go et al., 2009), which contains 1.6 million tweets; and a dataset containing 10.4 million tweets that were collected in October 2012 for a previous work by the author (Gatti et al., 2013). We tokenize these corpora using Gimpel et al.’s (2011) tokenizer, and removed messages that are less than 5 characters long (including white spaces) or have less than 3 tokens. Like in (Collobert et al., 2011) and (Luong et al., 2013), we lowercase all words and substitute each numerical digit by a 0 (e.g., 1967 becomes 0000). The resulting corpus contains about 12 million tweets. We do not perform unsupervised learning of</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann Lecun</author>
<author>Lon Bottou</author>
<author>Yoshua Bengio</author>
<author>Patrick Haffner</author>
</authors>
<title>Gradient-based learning applied to document recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>2278--2324</pages>
<contexts>
<context position="10114" citStr="Lecun et al., 1998" startWordPosition="1693" endWordPosition="1696"> θ) of the label given the segment and the set of network parameters θ, we apply a softmax operation over all tags: esθ(x)τ p (τ|x, θ) = � (6) i esθ(x)i 1We use a special padding token for the words with indices outside of the text segment boundaries. We use stochastic gradient descent (SGD) to minimize the negative log-likelihood with respect to θ: �θ H −log p(y|x, θ) (8) (x,y)∈D where (x, y) corresponds to a text segment (e.g. a tweet) in the training corpus D and y represents its respective sentiment class label. We use the backpropagation algorithm to compute the gradients of the network (Lecun et al., 1998; Collobert, 2011). We implement the CharSCNN architecture using the automatic differentiation capabilities of the Theano library (Bergstra et al., 2010). 3 Experimental Setup and Results 3.1 Unsupervised Learning of Word-Level Embeddings Unsupervised pre-training of word embeddings has shown to be an effective approach to improve model accuracy (Collobert et al., 2011; Luong et al., 2013; Zheng et al., 2013). In our experiments, we perform unsupervised learning of word-level embeddings using the word2vec tool2. We use two Twitter datasets as sources of unlabeled data: the Stanford Twitter Sen</context>
</contexts>
<marker>Lecun, Bottou, Bengio, Haffner, 1998</marker>
<rawString>Yann Lecun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, pages 2278–2324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="10505" citStr="Luong et al., 2013" startWordPosition="1749" endWordPosition="1752">x, y) corresponds to a text segment (e.g. a tweet) in the training corpus D and y represents its respective sentiment class label. We use the backpropagation algorithm to compute the gradients of the network (Lecun et al., 1998; Collobert, 2011). We implement the CharSCNN architecture using the automatic differentiation capabilities of the Theano library (Bergstra et al., 2010). 3 Experimental Setup and Results 3.1 Unsupervised Learning of Word-Level Embeddings Unsupervised pre-training of word embeddings has shown to be an effective approach to improve model accuracy (Collobert et al., 2011; Luong et al., 2013; Zheng et al., 2013). In our experiments, we perform unsupervised learning of word-level embeddings using the word2vec tool2. We use two Twitter datasets as sources of unlabeled data: the Stanford Twitter Sentiment corpus (Go et al., 2009), which contains 1.6 million tweets; and a dataset containing 10.4 million tweets that were collected in October 2012 for a previous work by the author (Gatti et al., 2013). We tokenize these corpora using Gimpel et al.’s (2011) tokenizer, and removed messages that are less than 5 characters long (including white spaces) or have less than 3 tokens. Like in (</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recursive neural networks for morphology. In Proceedings of the Conference on Computational Natural Language Learning, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="11833" citStr="Nakov et al., 2013" startWordPosition="1969" endWordPosition="1972">by a 0 (e.g., 1967 becomes 0000). The resulting corpus contains about 12 million tweets. We do not perform unsupervised learning of character-level embeddings, which are initialized by randomly sampling each value from an uniform distribution: U (−r, r), where r = � 6 |V chr |+ dchr . The character vocabulary is 2https://code.google.com/p/word2vec/ 649 constructed by the (not lowercased) words in the training set, which allows the neural network to capture relevant information about capitalization. 3.2 Sentiment Corpora and Model Setup SemEval-2014 Task 9 is a rerun of the SemEval2013 Task 2 (Nakov et al., 2013), hence the training set used in 2014 is the same of the 2013 task. However, as we downloaded the Twitter training and development sets in 2014 only, we were not able to download the complete dataset since some tweets have been deleted by their respective creators. In Table 1, we show the number of messages in our SemEval-2013 Task 2 datasets. Dataset SubtaskA SubtaskB Train 7390 8213 Dev. 904 1415 Twitter2013 (test) 3491 3265 SMS2013 (test) 2,334 2,093 Table 1: Number of tweets in our version of SemEval-2013 Task2 datasets. In SemEval-2014 Task 9, three different test sets are used: Twitter20</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312– 320, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval’14,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1153" citStr="Rosenthal et al., 2014" startWordPosition="165" endWordPosition="168">petitive results for Twitter data. 1 Introduction In this work we apply a recently proposed deep convolutional neural network (dos Santos and Gatti, 2014) that exploits from character- to sentence-level information to perform sentiment analysis of Twitter messages (tweets). The network proposed by dos Santos and Gatti (2014), named Character to Sentence Convolutional Neural Network (CharSCNN), uses two convolutional layers to extract relevant features from words and messages of any size. We evaluate CharSCNN in the unconstrained track of the SemEval-2014 Task 9: Sentiment Analysis in Twitter (Rosenthal et al., 2014). Two subtasks are proposed in the SemEval-2014 Task 9: the contextual polarity disambiguation (SubtaskA), which consists in determining the polarity (positive, negative, or neutral) of a marked word or phrase in a given message; and the message polarity classification (SubtaskB), which consists in classifying the polarity of the whole message. We use the same neural network to perform both tasks. The only difference is that in This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: </context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval’14, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="2218" citStr="Socher et al., 2011" startWordPosition="331" endWordPosition="334">icenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ SubtaskA, CharSCNN is fed with a text segment composed by the words in a context window centered at the target word/phrase. While in SubtaskB, CharSCNN is fed with the whole message. The use of deep neural networks for sentiment analysis has been the focus of recent research. However, instead of convolutional neural network, most investigation has been done in the use of recursive neural networks (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). 2 Neural Network Architecture Given a segment of text (e.g. a tweet), CharSCNN computes a score for each sentiment label T E T = {positive, negative, neutral}. In order to score a text segment, the network takes as input the sequence of words in the segment, and passes it through a sequence of layers where features with increasing levels of complexity are extracted. The network extracts features from the character-level up to the sentence-level. 2.1 Initial Representation Levels The first layer of the network transforms words into real-valued featur</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of theConference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="2239" citStr="Socher et al., 2012" startWordPosition="335" endWordPosition="338">ive Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ SubtaskA, CharSCNN is fed with a text segment composed by the words in a context window centered at the target word/phrase. While in SubtaskB, CharSCNN is fed with the whole message. The use of deep neural networks for sentiment analysis has been the focus of recent research. However, instead of convolutional neural network, most investigation has been done in the use of recursive neural networks (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). 2 Neural Network Architecture Given a segment of text (e.g. a tweet), CharSCNN computes a score for each sentiment label T E T = {positive, negative, neutral}. In order to score a text segment, the network takes as input the sequence of words in the segment, and passes it through a sequence of layers where features with increasing levels of complexity are extracted. The network extracts features from the character-level up to the sentence-level. 2.1 Initial Representation Levels The first layer of the network transforms words into real-valued feature vectors (embeddings</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of theConference on Empirical Methods in Natural Language Processing, pages 1201– 1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="2261" citStr="Socher et al., 2013" startWordPosition="339" endWordPosition="342">on 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ SubtaskA, CharSCNN is fed with a text segment composed by the words in a context window centered at the target word/phrase. While in SubtaskB, CharSCNN is fed with the whole message. The use of deep neural networks for sentiment analysis has been the focus of recent research. However, instead of convolutional neural network, most investigation has been done in the use of recursive neural networks (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). 2 Neural Network Architecture Given a segment of text (e.g. a tweet), CharSCNN computes a score for each sentiment label T E T = {positive, negative, neutral}. In order to score a text segment, the network takes as input the sequence of words in the segment, and passes it through a sequence of layers where features with increasing levels of complexity are extracted. The network extracts features from the character-level up to the sentence-level. 2.1 Initial Representation Levels The first layer of the network transforms words into real-valued feature vectors (embeddings) that capture morphol</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Waibel</author>
<author>Toshiyuki Hanazawa</author>
<author>Geoffrey Hinton</author>
<author>Kiyohiro Shikano</author>
<author>Kevin J Lang</author>
</authors>
<title>Phoneme recognition using time-delay neural networks.</title>
<date>1989</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="4805" citStr="Waibel et al., 1989" startWordPosition="752" endWordPosition="755">2.1.2 Character-Level Embeddings In the task of sentiment analysis of Twitter data, important information can appear in different parts of a hash tag (e.g., “#SoSad”, “#ILikeIt”) and many informative adverbs end with the suffix “ly” (e.g. “beautifully”, “perfectly” and “badly”). Therefore, robust methods to extract morphological and shape information from this type of tokens must take into consideration all characters of the token and select which features are more important for sentiment analysis. Like in (dos Santos and Zadrozny, 2014), we tackle this problem using a convolutional approach (Waibel et al., 1989), which works by producing local features around each character of the word and then combining them using a max operation to create a fixed-sized character-level embedding of the word. Given a word w composed of M characters {c1, c2, ..., cM}, we first transform each character cm into a character embedding rchr m . Character embeddings are encoded by column vectors in the embedding matrix Wchr ∈ Rdchr×|V chr|. Given a character c, its embedding rchr is obtained by the matrix-vector product: rchr = Wchrvc (2) where vc is a vector of size IV chrI which has value 1 at index c and zero in all othe</context>
</contexts>
<marker>Waibel, Hanazawa, Hinton, Shikano, Lang, 1989</marker>
<rawString>Alexander Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J. Lang. 1989. Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech and Signal Processing, 37(3):328–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqing Zheng</author>
<author>Hanyang Chen</author>
<author>Tianyu Xu</author>
</authors>
<title>Deep learning for chinese word segmentation and pos tagging.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in NLP,</booktitle>
<pages>647--657</pages>
<contexts>
<context position="10526" citStr="Zheng et al., 2013" startWordPosition="1753" endWordPosition="1756"> a text segment (e.g. a tweet) in the training corpus D and y represents its respective sentiment class label. We use the backpropagation algorithm to compute the gradients of the network (Lecun et al., 1998; Collobert, 2011). We implement the CharSCNN architecture using the automatic differentiation capabilities of the Theano library (Bergstra et al., 2010). 3 Experimental Setup and Results 3.1 Unsupervised Learning of Word-Level Embeddings Unsupervised pre-training of word embeddings has shown to be an effective approach to improve model accuracy (Collobert et al., 2011; Luong et al., 2013; Zheng et al., 2013). In our experiments, we perform unsupervised learning of word-level embeddings using the word2vec tool2. We use two Twitter datasets as sources of unlabeled data: the Stanford Twitter Sentiment corpus (Go et al., 2009), which contains 1.6 million tweets; and a dataset containing 10.4 million tweets that were collected in October 2012 for a previous work by the author (Gatti et al., 2013). We tokenize these corpora using Gimpel et al.’s (2011) tokenizer, and removed messages that are less than 5 characters long (including white spaces) or have less than 3 tokens. Like in (Collobert et al., 201</context>
</contexts>
<marker>Zheng, Chen, Xu, 2013</marker>
<rawString>Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for chinese word segmentation and pos tagging. In Proceedings of the Conference on Empirical Methods in NLP, pages 647–657.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>